<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="review-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Audiol Res</journal-id><journal-id journal-id-type="iso-abbrev">Audiol Res</journal-id><journal-id journal-id-type="publisher-id">audiolres</journal-id><journal-title-group><journal-title>Audiology Research</journal-title></journal-title-group><issn pub-type="ppub">2039-4330</issn><issn pub-type="epub">2039-4349</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40407670</article-id><article-id pub-id-type="pmc">PMC12101302</article-id>
<article-id pub-id-type="doi">10.3390/audiolres15030056</article-id><article-id pub-id-type="publisher-id">audiolres-15-00056</article-id><article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group></article-categories><title-group><article-title>Prediction of Auditory Performance in Cochlear Implants Using Machine Learning Methods: A Systematic Review</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1339-9490</contrib-id><name><surname>Demirta&#x0015f; Y&#x00131;lmaz</surname><given-names>Beyza</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Eshraghi</surname><given-names>Adrien</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Ocak</surname><given-names>Emre</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-audiolres-15-00056">Department of Audiology, Faculty of Health Sciences, Erciyes University, Kayseri 38039, Turkey; <email>beyzademirtas@erciyes.edu.tr</email>; Tel.: +90-541-714-7207</aff><pub-date pub-type="epub"><day>08</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>6</month><year>2025</year></pub-date><volume>15</volume><issue>3</issue><elocation-id>56</elocation-id><history><date date-type="received"><day>22</day><month>3</month><year>2025</year></date><date date-type="rev-recd"><day>29</day><month>4</month><year>2025</year></date><date date-type="accepted"><day>06</day><month>5</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the author.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p><bold>Background/Objectives:</bold> Cochlear implantation is an advantageous procedure for individuals with severe to profound hearing loss in many aspects related to auditory performance, social communication and quality of life. As machine learning applications have been used in the field of Otorhinolaryngology and Audiology in recent years, signal processing, speech perception and personalised optimisation of cochlear implantation are discussed. <bold>Methods:</bold> A comprehensive literature review was conducted in accordance with the PRISMA guidelines. PubMed, Scopus, Web of Science, Google Scholar and IEEE databases were searched for studies published between 2010 and 2025. We analyzed 59 articles that met the inclusion criteria. Rayyan AI software was used to classify the studies so that the risk of bias was reduced. Study design, machine learning algorithms, and audiological measurements were evaluated in the data analysis. <bold>Results:</bold> Machine learning applications were classified as preoperative evaluation, speech perception, and speech understanding in noise and other studies. The success rates of the articles are presented together with the number of articles changing over the years. It was observed that Random Forest, Decision Trees (96%), Bayesian Linear Regression (96.2%) and Extreme machine learning (99%) algorithms reached high accuracy rates. <bold>Conclusions:</bold> In cochlear implantation applications in the field of audiology, it has been observed that studies have been carried out with a variable number of people and data sets in different subfields. In machine learning applications, it is seen that a high amount of data, data diversity and long training times contribute to achieving high performance. However, more research is needed on deep learning applications in complex problems such as comprehension in noise that require time series processing. <bold>Funding and other resources:</bold> This study was not funded by any institution or organization. No registration was performed for this study.</p></abstract><kwd-group><kwd>machine learning</kwd><kwd>cochlear implants</kwd><kwd>audiology</kwd><kwd>hearing loss</kwd><kwd>speech perception</kwd><kwd>speech in noise</kwd><kwd>cochlear implantation candidates</kwd></kwd-group><funding-group><funding-statement>This study was not funded by any institution or organization. This study was conducted in accordance with the PRISMA 2020 guidelines; however, the compilation protocol was not recorded in a database.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-audiolres-15-00056"><title>1. Introduction</title><p>According to the World Health Organization&#x02019;s 2021 report, by 2050, approximately 2.5 billion people will have some degree of hearing loss, and more than 700 million people will need rehabilitation services [<xref rid="B1-audiolres-15-00056" ref-type="bibr">1</xref>]. With advancing technology, developing programming strategies, and surgical techniques, cochlear implants are used as a safe and effective rehabilitation method [<xref rid="B2-audiolres-15-00056" ref-type="bibr">2</xref>]. Despite the increasing success of cochlear implants in recent years, there may be differences in the performance and satisfaction of individuals with hearing loss. While some individuals may experience significant improvements in hearing performance, others may experience limited improvements in performance [<xref rid="B3-audiolres-15-00056" ref-type="bibr">3</xref>,<xref rid="B4-audiolres-15-00056" ref-type="bibr">4</xref>,<xref rid="B5-audiolres-15-00056" ref-type="bibr">5</xref>]. There are various studies investigating these differences in the literature [<xref rid="B6-audiolres-15-00056" ref-type="bibr">6</xref>,<xref rid="B7-audiolres-15-00056" ref-type="bibr">7</xref>,<xref rid="B8-audiolres-15-00056" ref-type="bibr">8</xref>]. However, identifying and interpreting these predictive factors is difficult even today [<xref rid="B4-audiolres-15-00056" ref-type="bibr">4</xref>]. This shows the necessity and importance of individualized rehabilitation programs. With the changing world and developing technology, machine learning techniques have been applied in recent years to predict performance differences after cochlear implantation and to develop individualized rehabilitation programs [<xref rid="B9-audiolres-15-00056" ref-type="bibr">9</xref>,<xref rid="B10-audiolres-15-00056" ref-type="bibr">10</xref>,<xref rid="B11-audiolres-15-00056" ref-type="bibr">11</xref>,<xref rid="B12-audiolres-15-00056" ref-type="bibr">12</xref>,<xref rid="B13-audiolres-15-00056" ref-type="bibr">13</xref>,<xref rid="B14-audiolres-15-00056" ref-type="bibr">14</xref>].</p><p>The ability of computer systems to imitate human-like thinking, learning, problem-solving and decision-making is the foundation of artificial intelligence. Thanks to algorithms that can identify patterns and make predictions by extracting meaning from data, artificial intelligence gives more comprehensive results day by day. Artificial intelligence, which has taken its place in many fields such as health, education, finance and trade, continues to bring innovations in various fields. Machine learning, which is a functional branch of this broad framework, has come to the fore as a sub-field of automatic learning in order to develop artificial intelligence human-like thinking, learning and decision-making processes [<xref rid="B15-audiolres-15-00056" ref-type="bibr">15</xref>,<xref rid="B16-audiolres-15-00056" ref-type="bibr">16</xref>,<xref rid="B17-audiolres-15-00056" ref-type="bibr">17</xref>].</p><p>These algorithms are based on the principle that computer software programs can analyze data from data sets by training the program rather than coding it with specific formulas [<xref rid="B18-audiolres-15-00056" ref-type="bibr">18</xref>]. Machine learning generally enables data prediction and analysis through two classifications. In one of these classifications, supervised learning, algorithms learn the relationships between the input data and the desired output and make predictions based on new data. In unsupervised machine learning, the algorithm tries to learn the patterns and patterns in the data by making a particular classification or grouping [<xref rid="B19-audiolres-15-00056" ref-type="bibr">19</xref>,<xref rid="B20-audiolres-15-00056" ref-type="bibr">20</xref>]. Deep learning, a more advanced branch of machine learning recently used in many fields, is applied to obtain more meaningful results from large and complex data sets. Deep learning, consisting of artificial neural networks, can learn more abstract and complex situations due to its multi-stage nature.</p><p>Although the foundations of machine learning date back to earlier years in the field of engineering, it is possible to say that there have been developments in the field of Otolaryngology and Audiology in the last two decades. Since the early 2000s, it has been seen that more basic algorithms have been used in the fields of diagnostics, imaging, digital signal processing and automatic diagnostic methods in Otolaryngology and Audiology. In the development of ECAP measurements, machine learning algorithms have been used to perform automatic analysis in cochlear implantation [<xref rid="B21-audiolres-15-00056" ref-type="bibr">21</xref>,<xref rid="B22-audiolres-15-00056" ref-type="bibr">22</xref>,<xref rid="B23-audiolres-15-00056" ref-type="bibr">23</xref>,<xref rid="B24-audiolres-15-00056" ref-type="bibr">24</xref>,<xref rid="B25-audiolres-15-00056" ref-type="bibr">25</xref>].</p><p>Machine learning algorithms for predicting hearing and speech performance of individuals with hearing loss, optimization studies in cochlear implant applications, and determination of electrode placement have been developed and used since the early 2010s [<xref rid="B10-audiolres-15-00056" ref-type="bibr">10</xref>,<xref rid="B26-audiolres-15-00056" ref-type="bibr">26</xref>,<xref rid="B27-audiolres-15-00056" ref-type="bibr">27</xref>,<xref rid="B28-audiolres-15-00056" ref-type="bibr">28</xref>,<xref rid="B29-audiolres-15-00056" ref-type="bibr">29</xref>,<xref rid="B30-audiolres-15-00056" ref-type="bibr">30</xref>,<xref rid="B31-audiolres-15-00056" ref-type="bibr">31</xref>,<xref rid="B32-audiolres-15-00056" ref-type="bibr">32</xref>].</p><p>Deep learning-based solutions, especially in the field of Ear, Nose and Throat and Hearing Science, with developing algorithms, show promising results in auditory signal processing, improvement of speech intelligibility and optimization of postoperative cochlear implant planning [<xref rid="B18-audiolres-15-00056" ref-type="bibr">18</xref>,<xref rid="B33-audiolres-15-00056" ref-type="bibr">33</xref>,<xref rid="B34-audiolres-15-00056" ref-type="bibr">34</xref>,<xref rid="B35-audiolres-15-00056" ref-type="bibr">35</xref>]. Machine learning algorithms are used to predict post-implantation auditory improvements by performing multidimensional analyses of electrophysiological measurements, postoperative speech perception assessments and noise reduction technologies [<xref rid="B36-audiolres-15-00056" ref-type="bibr">36</xref>,<xref rid="B37-audiolres-15-00056" ref-type="bibr">37</xref>]. In this regard, there are methodological differences in the existing studies, the fact that the data sets are quite different from each other, and there are various limitations regarding the reliability of algorithms that have only recently been developed and used in the field of otolaryngology and audiology [<xref rid="B10-audiolres-15-00056" ref-type="bibr">10</xref>,<xref rid="B38-audiolres-15-00056" ref-type="bibr">38</xref>,<xref rid="B39-audiolres-15-00056" ref-type="bibr">39</xref>,<xref rid="B40-audiolres-15-00056" ref-type="bibr">40</xref>]. Considering recent research, it is evident that most of the work has focused on neurotology, and no review has specifically examined the research conducted in audiology. The aim of this systematic review was to evaluate and compare machine learning models used to predict auditory performance in cochlear implant users.</p><p>This study was conducted in accordance with the PRISMA [Preferred Reporting Items for Systematic Reviews and Meta-Analyses] guidelines and aimed to examine in detail the studies on cochlear implantation and machine learning related to audiological outcomes. It focused on the machine learning models used in the research, data types, parameters evaluated and their effects on decision processes in clinical applications. In this context, the aim was to compile the existing literature on individualized rehabilitation approaches for cochlear implant patients and identify potential areas for future research.</p></sec><sec id="sec2-audiolres-15-00056"><title>2. Materials and Methods</title><p>This study was written using the PRISMA 2020 guidelines. Although this study was conducted in accordance with PRISMA 2020 guidelines, the review protocol was not registered in a database.</p><p>This study comprehensively reviewed articles related explicitly to audiological outcomes of cochlear implantation and analyzed their results.</p><sec id="sec2dot1-audiolres-15-00056"><title>2.1. Data Resources and Search Strategy</title><p>The article screening process was carried out between September 2024 and January 2025. Studies conducted between 2010 and 2025 were included in the research. PubMed, Scopus, Web of Science, Google Scholar and IEEE databases were searched for studies. The keywords &#x0201c;Cochlear Implantation, Machine Learning, Hearing Loss, Auditory Performance After Cochlear Implantation, Electrophysiological Measurements, Speech Perception&#x0201d; were used in the searches.</p></sec><sec id="sec2dot2-audiolres-15-00056"><title>2.2. Inclusion and Exclusion Criteria</title><p>Inclusion Criteria</p><list list-type="bullet"><list-item><p>Cochlear implantation studies using machine learning models;</p></list-item><list-item><p>International full-text studies published in peer-reviewed journals.</p></list-item></list><p>Exclusion Criteria</p><list list-type="bullet"><list-item><p>Audiologic studies other than cochlear implantation using machine learning models;</p></list-item><list-item><p>Theoretical studies;</p></list-item><list-item><p>Abstracts published at conferences;</p></list-item><list-item><p>Case reports.</p></list-item></list></sec><sec id="sec2dot3-audiolres-15-00056"><title>2.3. Selection of Studies</title><p>Articles obtained as a result of literature searches from search engines were uploaded to the Rayyan AI software (<uri xlink:href="https://help.rayyan.ai/hc/en-us/articles/4406419348369-What-is-the-version-of-Rayyan">https://help.rayyan.ai/hc/en-us/articles/4406419348369-What-is-the-version-of-Rayyan</uri> accessed on 3 April 2025). Rayyan is an application that uses AI-based algorithms to facilitate the systematic review process. In particular, it can predict article inclusion or exclusion decisions based on the referee&#x02019;s previous selections using machine learning techniques. In addition, it is used to speed up the decision-making process by detecting duplicate or highly similar records through text similarity analysis. These AI-supported features aim to reduce the author&#x02019;s workload, increase consistency, and improve the overall efficiency of study selection. Rayyan is a program that provides researchers with article screening, double-blind evaluation, categorization, and AI-supported recommendations. It can be preferred for time-saving and objective evaluation processes, as it allows the process to be followed in accordance with the PRISMA guide. In this study, Rayyan was used to speed up the literature review, clarify the inclusion and exclusion criteria, and make the research transparent by classifying studies by topic [<xref rid="B41-audiolres-15-00056" ref-type="bibr">41</xref>,<xref rid="B42-audiolres-15-00056" ref-type="bibr">42</xref>]. The filtering and tagging features of the program were used, and articles that met the inclusion and exclusion criteria were identified.</p><p>These tags were applied as &#x0201c;Machine Learning Modeling&#x0201d;, &#x0201c;Clinical practices in Adult Cochlear Implant&#x0201d;, &#x0201c;Clinical practices in Pediatric Cochlear Implant&#x0201d; and &#x0201c;Treatment and Rehabilitation Methods&#x0201d;. First, titles and abstracts were examined, and then, the full text was evaluated. A total of 93 articles were obtained from literature searches. Using Rayyan AI software, 34 duplicated and eliminated articles were identified and removed (<xref rid="audiolres-15-00056-f001" ref-type="fig">Figure 1</xref>). While analyzing the data, the sample size, the machine learning model used, and the performance criteria were recorded. This systematic review analysed studies evaluating auditory performance in cochlear implant users. Since the studies were conducted in different fields, speech discrimination scores and signal-to-noise ratios and similar metrics were considered as success parameters as primary outcome variables. Secondary outcome variables include sensitivity, specificity and F1 score, which indicate the accuracy of machine learning models. All measures obtained from the studies were reviewed, and all auditory performance measures reported in the studies were evaluated. In this systematic review, comparisons between the groups identified in the studies (implant group&#x02013;control group) were evaluated to assess the effectiveness of cochlear implantation and machine learning-based analyses. In this systematic review, variables such as participant age, gender, type of cochlear implant used and follow-up period were analysed but not evaluated because detailed information was not provided in all studies. Studies with missing data were not included in this study. In this review, the results of the individual studies are presented with summary tables including the intervention, participant characteristics and main results of each study. Sample sizes across studies were compared in pie charts, and general trends for the number of studies expected to emerge in this area over the years were summarised in line graphs. In this study, the articles were analysed by a single researcher. In this review, no specific methodology was used to assess reporting biases. The classifications made through the Rayyan application ensured that the screening process was carried out objectively.</p></sec><sec id="sec2dot4-audiolres-15-00056"><title>2.4. Data Analysis</title><p>The machine learning models used in the articles and their success rates were analyzed comparatively. The data obtained are shown in tables using descriptive statistics. In this systematic review, findings from individual studies are described and combined. The results across studies were analysed by grouping them thematically, and similar findings were brought together. Instead of a statistical analysis, the results and methods applied in each study were compared, and general trends and main findings were summarised. In this process, the heterogeneity of the studies was taken into account. In the search process, a total of 93 records were obtained as a result of the searches in the database. Due to duplication and other reasons, 22 studies were excluded. After title and abstract screening, 4 studies that were identified as case reports and technical reports were excluded, and the remaining 59 studies were included in the full text review. The flow diagram is given in <xref rid="audiolres-15-00056-t001" ref-type="table">Table 1</xref>. Small sample sizes and limited dataset information were observed in some studies. However, due to the limited and new nature of the existing studies in the field, these studies were included in the review. It was observed that the included data were analyzed double-blind.</p></sec></sec><sec sec-type="results" id="sec3-audiolres-15-00056"><title>3. Results</title><p>The distribution and application areas of the articles analysed in the study are presented. The analysis of articles published between 2013 and 2025 shows that the use of machine learning techniques in cochlear implantation is increasing. With the widespread use of deep learning models since 2018, the number of studies in this field has increased. In the reviewed articles, machine learning models have achieved successful results in preoperative and postoperative candidacy evaluations, especially in the cleaning and enhancement of speech signals in noisy environments, and machine learning models developed specifically for cochlear implant users have achieved successful results in preoperative and postoperative candidacy evaluations. However, it has been observed that it is aimed to develop automatically optimised applications in accordance with the individual needs of the users [<xref rid="B26-audiolres-15-00056" ref-type="bibr">26</xref>,<xref rid="B41-audiolres-15-00056" ref-type="bibr">41</xref>,<xref rid="B42-audiolres-15-00056" ref-type="bibr">42</xref>,<xref rid="B43-audiolres-15-00056" ref-type="bibr">43</xref>,<xref rid="B44-audiolres-15-00056" ref-type="bibr">44</xref>,<xref rid="B45-audiolres-15-00056" ref-type="bibr">45</xref>].</p><p>Additionally, deep learning clinical based models were frequently used together with traditional methods. However, it has been observed that most of the studies have problems such as limited data sets and lack of clinical validation [<xref rid="B40-audiolres-15-00056" ref-type="bibr">40</xref>,<xref rid="B46-audiolres-15-00056" ref-type="bibr">46</xref>,<xref rid="B47-audiolres-15-00056" ref-type="bibr">47</xref>,<xref rid="B48-audiolres-15-00056" ref-type="bibr">48</xref>,<xref rid="B49-audiolres-15-00056" ref-type="bibr">49</xref>,<xref rid="B50-audiolres-15-00056" ref-type="bibr">50</xref>,<xref rid="B51-audiolres-15-00056" ref-type="bibr">51</xref>,<xref rid="B52-audiolres-15-00056" ref-type="bibr">52</xref>,<xref rid="B53-audiolres-15-00056" ref-type="bibr">53</xref>].</p><p>In <xref rid="audiolres-15-00056-t001" ref-type="table">Table 1</xref> and <xref rid="audiolres-15-00056-f002" ref-type="fig">Figure 2</xref>, the distribution of the articles analysed in the study according to years and application areas is analysed. Sensitivity analysis, assessment of bias due to missing results, and analysis of the level of certainty of evidence were not performed in this review. However, considering the methodological diversity of the studies reviewed and the differences in the machine learning models used, care was taken to present the results as comprehensively as possible. The analysis of studies published between 2013 and 2025 shows a growing interest in the use of machine learning techniques in the field of audiology, particularly in cochlear implantation. As presented in <xref rid="audiolres-15-00056-t001" ref-type="table">Table 1</xref>, there has been a consistent increase in the number of publications over the years. During the 2013&#x02013;2017 period, research mainly focused on early-stage machine learning, deep learning approaches and experimental data analysis. In 2018&#x02013;2022, predictive model development and dataset optimization became more prominent, along with more frequent references to basic artificial intelligence applications. In the most recent period, 2023&#x02013;2025, studies have highlighted integrating multiple machine learning methods, adopting innovative techniques and shifting toward broad-based deep learning applications using advanced algorithms.</p><p>This study presents a comparative analysis of the accuracy rates of machine learning applications utilized in research involving cochlear implant users in different sub-fields.</p><p>The success rates of the machine learning algorithms examined in this study are based on the model evaluation methods specified in the relevant literature. In these studies, it was observed that k-fold cross-validation (mostly 5- or 10-fold) was widely used to increase the generalizability of the model performance. In addition, L1/L2 regularization methods were used, especially in linear and Bayesian regression models. Among the performance metrics used, accuracy is the most common, and in some studies, measures such as F1-score, ROC-AUC and mean squared error (MSE) were also reported. The distribution of these methods according to sample studies is presented in <xref rid="audiolres-15-00056-t002" ref-type="table">Table 2</xref>. These evaluation methods support the reliability and generalizability of the success rates reported in the studies (see <xref rid="audiolres-15-00056-t003" ref-type="table">Table 3</xref>).</p><p>The machine learning models used in the studies included in the review show significant differences in data type, algorithm structure and targeted outputs. In particular, multidimensional data specific to cochlear implant users, such as speech perception test results, electrode placement information, impedance values and user performance evaluations, have been decisive in model selection. While artificial neural networks (ANNs) stand out in black data analysis, more explainable structures, such as support vector machines (SVMs) and decision trees, have generally been preferred in classification tasks. For example, in some studies, users have been classified as &#x0201c;successful/unsuccessful&#x0201d;, while some models have aimed to predict threshold values or satisfaction levels. Ensembling algorithms such as random forest have also been preferred due to their feature selection and advantages in error reduction. The findings are shown in <xref rid="audiolres-15-00056-t003" ref-type="table">Table 3</xref>.</p><p>The sample sizes used in studies on machine learning algorithms are an important factor for the generalizability and reliability of the methods. Depending on the problems considered, the datasets used and the topics, varying sample sizes have been identified. In this regard, the sample sizes of the papers included in this study are shown in <xref rid="audiolres-15-00056-f003" ref-type="fig">Figure 3</xref>.</p><p>Machine learning has been used in the field of audiology and cochlear implants in different years with various approaches and applications. Initially, traditional machine learning methods such as basic decision trees and linear regression were often used, but over time, with the implementation of more complex algorithms and datasets, their use has expanded to EEG signals and biomedical approaches. The results are shown in <xref rid="audiolres-15-00056-t003" ref-type="table">Table 3</xref>.</p><p>In addition, <xref rid="audiolres-15-00056-t004" ref-type="table">Table 4</xref> and <xref rid="audiolres-15-00056-t005" ref-type="table">Table 5</xref> provides a detailed summary of the results of the reviewed articles. Each entry includes information on the machine learning model used, area of application, number of data points, number of participants, accuracy rate and an explanatory statement highlighting the study&#x02019;s key findings.</p><p>The studies span from 2013 to 2024 and include various machine learning models such as support vector machines (SVMs), deep neural networks (DNNs), artificial neural networks (ANNs) and relevance vector machines (RVMs). The areas of application range from speech perception and speech in noise to electrode insertion depth and EEG optimization for cochlear implants. Accuracy rates vary across the studies, with some models achieving high success rates, such as 97.79% accuracy with SVM in speech intelligibility, and others report substantial improvements in noise reduction and speech discrimination.</p></sec><sec sec-type="discussion" id="sec4-audiolres-15-00056"><title>4. Discussion</title><p>This study examined the distribution of machine learning applications in cochlear implant technology in the field of Audiology, application examples and accuracy percentages in different sub-fields, and the place of machine learning methods in clinical practice. When the sub-fields were examined, it was seen that the topics are mostly used in the applicability of automated machine learning methods to the effectiveness of intraoperative and postoperative tests in electrophysiological measurements, with the evaluation of models in pre-op candidacy predictions and speech understanding performance in noise. In addition, it is understood that evaluations have been made in a wide range of applications, from the use of deep learning-based speech recognition models using acoustic simulations instead of human subjects in speech perception evaluations to the development of emotion recognition skills from electroencephalography (EEG) data. Additionally, the effectiveness of cross-modal plasticity, machine learning applications in electrode design, and signal processing methods to enhance music perception in cochlear implant users are examined in detail. In this review, a total of 59 articles on machine learning techniques were analyzed. However, in the discussion section, specific papers were evaluated by excluding studies that were repetitive in terms of methodology, dataset, and performance metrics and findings. The selection of the articles in this section was based on criteria such as the quality of the datasets, the validation processes used and the reliability of the results. However, the discussion section is structured around specific themes to provide a comprehensive and in-depth analysis.</p><sec id="sec4dot1-audiolres-15-00056"><title>4.1. Preoperative Candidacy</title><p>In cochlear implant evaluations, preoperative assessment plays an important role in users&#x02019; postoperative performance and rehabilitation. Otolaryngologists and audiologists complete the candidacy process by evaluating parameters such as the degree and type of hearing loss, neuro-anatomical structure, age at surgery, residual hearing status, hearing aid use and receptive-expressive language age [<xref rid="B57-audiolres-15-00056" ref-type="bibr">57</xref>,<xref rid="B58-audiolres-15-00056" ref-type="bibr">58</xref>,<xref rid="B59-audiolres-15-00056" ref-type="bibr">59</xref>,<xref rid="B60-audiolres-15-00056" ref-type="bibr">60</xref>,<xref rid="B61-audiolres-15-00056" ref-type="bibr">61</xref>].</p><p>In a study of 587 individuals regarding the preoperative evaluation process, a andom Forest machine learning model including demographic and audiologic data was compared to a traditional guideline for the evaluation of individuals with hearing loss. The model was evaluated by cross-validation and reported high sensitivity (92%) and specificity (100%) using speech discrimination scores, patient age and hearing thresholds at specific frequencies. The specificity of the traditional method was reported to be 42%. The study shows that the traditional guideline may be inadequate for new indications, but the developed machine learning model may be more effective by providing individualized assessments [<xref rid="B62-audiolres-15-00056" ref-type="bibr">62</xref>].</p><p>In the context of preoperative candidacy assessment, Zeitler et al. (2024) [<xref rid="B63-audiolres-15-00056" ref-type="bibr">63</xref>] used a random forest machine learning model to predict the most relevant factors affecting postoperative residual hearing levels and determine implant candidacy in 175 cochlear implant users. The Receiver Operating Characteristic Curve (ROC) curve and Matheww Correlation Coefficient (MCC) measures were used to evaluate the performance of the study. Postoperative residual hearing thresholds and preoperative meningitis history were positively associated with preserving residual hearing, whereas sudden hearing loss, noise exposure and ear fullness symptoms were negatively associated. Accuracy measures were provided through ROC and MCC measures. These measures are used in machine learning and statistics to evaluate the performance of classification models. While the ROC measure can take values between zero and one, the MCC can take values between minus one and one. In this study, MCC values ranged between 0.38 and 0.52 and ROC values between 0.73 and 0.83. The study shows that machine learning successfully predicts acoustic hearing preservation [<xref rid="B63-audiolres-15-00056" ref-type="bibr">63</xref>].</p><p>Carlson M. et al. (2024) [<xref rid="B64-audiolres-15-00056" ref-type="bibr">64</xref>] developed a machine learning model to assist in selecting cochlear implant candidacy in adults with demographic data and behavioral audiometry measures in another study of 700 individuals. AzBio and Consonant&#x02013;Nucleus&#x02013;Consonant (CNC) sentence and word recognition tests were used for candidacy prediction. Random Forest, XGBoost (XGB), and Logistic Regression models were used to predict the results obtained from the tests, and a model that can predict cochlear implant candidacy was developed. In the study, it was stated that this model, which was developed with an 87% accuracy rate and 90% sensitivity, can be used in clinics [<xref rid="B64-audiolres-15-00056" ref-type="bibr">64</xref>].</p><p>As is known, there may be missing data in behavioral tests performed in implant candidacy due to limited time, patient test compliance, etc. A study evaluated the accuracy of prediction models to complete the missing audiometric data with 1304 audiograms whose pure tone audiometry data were complete at all frequencies. The performance of the estimation model was evaluated using cross-validation methods with various amounts of missing data and sparsity distributions. The results showed that the Multiple Imputation by Chained Equations (MICEs) machine learning model was able to estimate missing data up to six frequencies in an 11-frequency audiogram. In this study, the MICE machine learning model showed the best performance. More sophisticated machine learning methods (XGB, NN) performed better with more data but not as well as MICE. In this study, the researchers also reported that completing missing data increased the size of the dataset by 5.7 times compared with performing a complete analysis. With models similar to these models, it is stated that studies with more people can be conducted to prevent data loss [<xref rid="B55-audiolres-15-00056" ref-type="bibr">55</xref>].</p><p>When the success rates of these studies on preoperative candidacy assessment are evaluated, they are measured with different metrics. Despite the different measurement methods, it is seen that high success rates are achieved in candidacy prediction, but studies are limited. Although the application evaluated in preoperative evaluations is different, it is seen that the Random Forest method is often associated with high prediction rates.</p></sec><sec id="sec4dot2-audiolres-15-00056"><title>4.2. Intraoperative&#x02013;Postoperative Measurements</title><p>Cochlear implant surgery is a field with very comprehensive stages. In this situation, after the cochlear implant candidacy evaluation, electrode placement, and subsequent programming are critical regarding language development in children, quality of life and effective communication in adults. The most important criteria for successful results are cochlear implant application to the right patient, programming and follow-up [<xref rid="B65-audiolres-15-00056" ref-type="bibr">65</xref>,<xref rid="B66-audiolres-15-00056" ref-type="bibr">66</xref>,<xref rid="B67-audiolres-15-00056" ref-type="bibr">67</xref>,<xref rid="B68-audiolres-15-00056" ref-type="bibr">68</xref>,<xref rid="B69-audiolres-15-00056" ref-type="bibr">69</xref>,<xref rid="B70-audiolres-15-00056" ref-type="bibr">70</xref>,<xref rid="B71-audiolres-15-00056" ref-type="bibr">71</xref>,<xref rid="B72-audiolres-15-00056" ref-type="bibr">72</xref>,<xref rid="B73-audiolres-15-00056" ref-type="bibr">73</xref>].</p><p>There are different studies on machine learning in this field. In a study comparing patients programmed with the Fitting to Outcome eXpert (FOX2G) machine learning software method with patients programmed manually, 47 experienced cochlear implant users took part. The evaluations observed a 10 dB improvement in cochlear implant hearing thresholds at 6000 Hz and a 10% improvement in discrimination tests in patients programmed with FOX software. It was also reported that 89% of users preferred FOX programming to manual programming. This indicates that this software could be used for candidacy assessment [<xref rid="B33-audiolres-15-00056" ref-type="bibr">33</xref>].</p><p>Another study conducted with the artificial intelligence-based FOX algorithm compared the effectiveness of machine learning methods and traditional programming methods. Experienced clinicians with traditional methods first programmed fifty-five adult patients and then reprogrammed them with the FOX algorithm. Performance measurements were made using the CNC word and AzBio sentence tests. In the study, 84% of the patients showed the same performance with FOX programming. No significant difference in mean performance was reported between FOX and traditional methods. It was reported that the software used in this study may help less experienced clinicians to perform effective programming. Regarding patient satisfaction, 82% of the users were satisfied with the sound quality provided by FOX [<xref rid="B38-audiolres-15-00056" ref-type="bibr">38</xref>].</p><p>A study using machine learning models to predict electrode impedances after cochlear implantation in relation to postoperative measurements analyzed data from 80 pediatric patients using the Med-El Flex 28 electrode array to predict electrode impedances at 1, 3, 6 and 12 months post-op. Bayesian Linear Regression (BLR) and neural networks (NNs) gave the best results for most channels. In the study, patient age and intraoperative electrode impedance are important determinants in electrode impedance predictions. It is stated that different electrodes for electrode impedance can be better predicted with different machine learning algorithms. In the study, it was reported that the prediction accuracy of electrode impedance at the 12th postoperative month ranged between 83 and 100% [<xref rid="B36-audiolres-15-00056" ref-type="bibr">36</xref>].</p><p>Many variables affect the results after cochlear implantation. One of these variables is electrode placement and depth. Although electrode placement is evaluated with some electrophysiologic and imaging methods during surgery, unpredictable results may occur. For this purpose, Hafeez et al. (2021) used a robotic system for electrode placement in the cochlea. With this system, electrodes were automatically placed in the scala tympani. The electrode array was placed at a constant speed (0.08 mm/s) and three different angles (medial, middle and lateral), and the magnitude, phase, reactance and resistance components were measured. The effects of different placement routes were analyzed. One hundred thirty-seven electrode placement experiments were performed, and impedance data of eight electrode pairs were recorded in each experiment. Support vector machine and shallow neural networks achieved 86.1% accuracy with partial placement data. The study reported that machine learning methods can provide instant notification during electrode placement, which can help surgeons or robots place the electrode array more accurately [<xref rid="B39-audiolres-15-00056" ref-type="bibr">39</xref>].</p><p>In another study on electrode placement depth and electrode arrays, machine learning was used to predict the location of electrodes using telemetry results. The study included 118 patients, and lateral wall electrode arrays were placed in all patients, and the most successful model was Extremely Randomized Trees. The model successfully predicted the linear depth of electrode placement with an average error of 0.8 mm. Although accuracy rates were not reported in the article, the distance between electrodes was 2.1 mm. The study suggests that this method may be an alternative to computed tomography results, especially in children [<xref rid="B74-audiolres-15-00056" ref-type="bibr">74</xref>].</p><p>Postoperative speech perception results are very important in evaluating the success of cochlear implantation and monitoring the auditory performance of the user. These results can help to plan individualized rehabilitation programs. Changes in speech perception can be monitored and used to determine implant effectiveness and additional interventions when necessary. In a study using supervised machine learning techniques to predict postoperative speech perception results, the Hearing Noise Test results were evaluated using 282 variables (demographic, audiological) in 1604 adult patients. It was reported that the results obtained could be detected with 95.4% accuracy at the end of the post-op 12th month with neural networks (NNs) machine learning model prediction. It was reported that age at surgery, presence of tinnitus, vestibular function, and psychological status of the patient were factors affecting performance [<xref rid="B10-audiolres-15-00056" ref-type="bibr">10</xref>].</p><p>Skidmore et al. (2021) developed an objective prediction model using machine learning and electrophysiological measurements to assess the functionality of the cochlear nerve in cochlear implant users. Electrically Compound Action Potential (ECAP) measurements were used for objective assessments. In the study, an index indicating the state of the cochlear nerve was developed using machine learning methods such as linear regression and support vector machine. The study reported that the algorithms correctly classified patients with hypoplasic and intact nerves in the range of 91&#x02013;95%. The given accuracy rates showed that machine learning algorithms could accurately discriminate between the two patient groups with ECAP data. This study also reported correlation coefficients ranging from 0.49 to 0.73 between cochlear nerve index and speech perception tests [<xref rid="B75-audiolres-15-00056" ref-type="bibr">75</xref>].</p><p>Preservation of residual hearing after cochlear implantation has become very important in recent years, and a study by Zeitler D. et al. investigated the use of machine learning techniques to predict residual hearing after cochlear implantation. Logistic Regression, support vector machines, and Random Forest Extreme Gradient Boosting (XGBoost) methods were compared. Retrospective data of 175 patients were used in this study. The patients&#x02019; pre-op and post-op results were evaluated. The XGBoost machine learning model showed the best performance, with 83%. Preoperative low-frequency hearing levels, history of meningitis and sudden hearing loss were reported to be significantly associated with residual hearing. In the study, especially tobacco use and abnormal anatomical structure were reported to affect hearing preservation negatively. The results of this study suggest that machine learning models can be used to create more realistic expectations for patients after surgery and to plan options such as Electroacoustic Simulation (EAS) [<xref rid="B63-audiolres-15-00056" ref-type="bibr">63</xref>].</p><p>Predicting the results after cochlear implantation in inner ear anomalies is important in determining which patients should receive cochlear implantation. Factors such as anatomical condition, auditory experience of the patient, etc., can significantly affect the results. Weng J et al. used machine learning methods to predict hearing and speech perception performance after cochlear implantation. In this study, the radiological features of the cochlea were examined using 3D segmentation, and a prediction model was developed using the k-Nearest Neighbors machine learning algorithm. This model provides 93.3% accuracy for hearing rehabilitation and 86.7% for speech rehabilitation. The study reported that cochlear volume and canal length are important determinants of post-cochlear implant performance [<xref rid="B12-audiolres-15-00056" ref-type="bibr">12</xref>].</p><p>In addition, cochlear implant success was evaluated in 70 children with hypoplasic nerves and normal cochleae after 2 years of follow-up. Hearing and speech development were evaluated using the categories of auditory performance (CAP), speech intelligence rating (SIR), meaningful auditory integration scale (MAIS), and meaningful use of speech scale (MUSS). The study determined the prediction accuracy of postoperative hearing and speech outcomes as 71% and 93%, respectively, using the support vector machines machine learning model. When the results were evaluated, it was reported that the cochlear nerve area and several nerve bundles were important factors for predicting cochlear implant outcomes. However, the study reported that age at implantation and residual hearing were not associated with cochlear implant outcomes. It has been reported that machine learning models can predict which patients may benefit more from cochlear implantation [<xref rid="B14-audiolres-15-00056" ref-type="bibr">14</xref>].</p><p>When the studies on intraoperative and postoperative measurements are evaluated, it is seen that machine learning techniques are used in subjects such as electrode placement depths, speech perception, preservation of residual hearing and prediction of cochlear nerve function. Different machine learning techniques have come to the forefront of studies. It is seen that the accuracy rates vary between 71 and 100% in the studies. Although the range seems to be exhaustive, most of the studies showed accuracy rates above 80%.</p></sec><sec id="sec4dot3-audiolres-15-00056"><title>4.3. Speech Perception</title><p>With advancing technology, the question of how deep learning models can be used to evaluate the speech perception results of implants has been investigated. Speech perception can be measured as an objective reflection of users&#x02019; effective use of auditory performance. Speech perception tests help to optimize implant settings by reflecting how well sounds can be interpreted and discriminated. In addition, monitoring progression in the postoperative period and guiding the rehabilitation process is of great importance [<xref rid="B76-audiolres-15-00056" ref-type="bibr">76</xref>,<xref rid="B77-audiolres-15-00056" ref-type="bibr">77</xref>,<xref rid="B78-audiolres-15-00056" ref-type="bibr">78</xref>,<xref rid="B79-audiolres-15-00056" ref-type="bibr">79</xref>].</p><p>One study used the speech recognition algorithm of OpenAI&#x02019;s Whisper Model. This model evaluates acoustic distortions and signal processing parameters such as the number of spectral bands, input frequency range and envelope cutoff frequency to simulate the auditory experience of implant users. The study reported that Whisper can be used to optimize implant signal processing parameters. In addition, it was reported that the application also exhibits human-like durability in noisy and quiet environments so that implant simulations can be evaluated effectively [<xref rid="B47-audiolres-15-00056" ref-type="bibr">47</xref>].</p><p>In another study, the speech intelligibility and signal-to-noise ratio performances of the model developed with machine learning to design and validate a digital signal processing plug-in for cochlear implants were evaluated. The study aimed to develop a new signal-processing plug-in to improve the signal-processing performance of cochlear implants. Wavelet neural network (WNN), a machine learning model, is used to create a plug-in for existing filter banks. The existing model is trained in particle swarm optimization. Success rates are evaluated in terms of speech intelligibility using the Short-Time Objective Intelligibility (STOI) metric. STOI is an objective method for evaluating speech intelligibility. It is especially used to measure performance in noisy environments. This metric takes a value between 0 and 1. If it is close to one, speech is considered intelligible, and if it is close to zero, speech intelligibility is considered low. In the study, speech intelligibility in STOI evaluations was measured as 0.834 in the WNN method and 0.762 and 0.813 in the DRNL and SPDN methods, respectively. In signal-to-noise ratio evaluations, it was reported that the WNN method obtained values of 2.470, DRNL, and SPDN methods &#x02212;8189 and &#x02212;1240 in STOI measurements. This new method is stated to perform better in terms of signal-to-noise ratio, speech intelligibility, and other performances compared with the existing digital signal processing techniques (DRNL and SPDN) in cochlear implants [<xref rid="B49-audiolres-15-00056" ref-type="bibr">49</xref>].</p><p>In an article evaluating post-op cochlear implantation outcomes, machine learning models used to assess word recognition performance at 12 months in 2489 post-lingual adult patients were compared. Artificial neural networks, Random Forests, XGBoost and gradient-boosting machine learning models were used. It is stated that the XGBoost model gives the best result. The absolute error rate of XgBoost was determined as 20.81. When data from different clinics were tested in the study, it was reported that the error rate was measured as a maximum of 16%. The main results of the study reported that increasing the amount of data did not increase the model&#x02019;s accuracy. However, comprehensive and high-quality data affected the prediction accuracy of the model [<xref rid="B13-audiolres-15-00056" ref-type="bibr">13</xref>].</p><p>As is known, speech perception can be achieved by combining many factors. Pitch perception also plays an important role for cochlear implant users regarding music perception and speech discrimination. It is known that cochlear implants do not provide near-natural pitch perception due to limited spectral and temporal resolution, and new strategies are being developed for this purpose.</p><p>In a study evaluating pitch perception in cochlear implant users, a computational model was developed using spiking neural networks. This model was trained with digitally generated sounds and pitch cues to see how pitch perception changes in different environments. This model was used to compare a new model developed for cochlear implant users with the traditionally used ACE strategy. The study showed that the machine learning-based models performed better when the place and time cues matched. The results of this study suggest that in future cochlear implant designs, near-natural pitch perception can be achieved using machine learning models [<xref rid="B48-audiolres-15-00056" ref-type="bibr">48</xref>].</p><p>In another study using deep neural networks to estimate pitch perception, researchers developed a deep learning-based model to extract F0 fundamental frequency information from simulated cochlear implant signals. They investigated how cochlear implant signals carry pitch perception by varying the number of electrode channels and pulse rate. They reported that in quiet environments, the F0 accuracy of cochlear implants reached 90% when the number of channels increased above eight. The study reported that these results varied in noisy environments, and the accuracy rate was 75% with 20 channels and 2000 pulse rate, but when the number of channels decreased, this rate dropped below 50%. The study emphasized that deep neural networks can be effective in predicting F0, but the performance depends on parameters such as the number of electrode channels and pulses [<xref rid="B56-audiolres-15-00056" ref-type="bibr">56</xref>].</p><p>In a study where music was remixed using audio source separation algorithms to improve the music experience in cochlear implant users, Deep Convolutional Autoencoder (DCAE), deep recurrent neural network (DRNN), Multilayer Perceptron (MLP) and Non-Negative Matrix Factorization (NMF) methods were evaluated to separate vocals and instruments in songs. The performance of the algorithms was evaluated with metrics such as signal distortion ratio, signal-to-noise ratio and signal artifact ratio. Multilayer Perceptron was the most suitable algorithm for cochlear implant users due to user experience and low processing time [<xref rid="B80-audiolres-15-00056" ref-type="bibr">80</xref>].</p><p>In another study using deep learning-based sound separation technology to enhance the music listening experience of cochlear implant users, a Demucs neural network-based model was developed that separates music into vocal, rhythmic and harmonic accompaniment. The study used a pulsatile vocoder to simulate the hearing experience of cochlear implant users. Evaluations were made with the pulsatile vocoder on 15 individuals with normal hearing. A system that allows cochlear implant users to adjust instrument levels according to their subjective preferences has been developed. The proposed model succeeded more than objective and subjective systems in improving music perception for cochlear implant users. In particular, the personalized audio tuning feature significantly improved the user experience. It was reported that models requiring less computational power could be designed and integrated into cochlear implants in the future [<xref rid="B81-audiolres-15-00056" ref-type="bibr">81</xref>,<xref rid="B82-audiolres-15-00056" ref-type="bibr">82</xref>].</p><p>It has been observed that multiple machine-learning models have been compared in studies on speech perception evaluations. The applied machine learning methods varied, and the measurement results were evaluated using different metrics. Studies have been conducted on voice discrimination technologies, digital signal processing technologies, and pitch perception to improve speech intelligibility in cochlear implants. Machine learning techniques generally yield successful results in the studies.</p></sec><sec id="sec4dot4-audiolres-15-00056"><title>4.4. Speech Perception in Noise</title><p>Understanding speech in noise can make communication difficult, even for individuals with normal hearing. This is a significant issue for cochlear implant users that complicates communication and the quality of daily life. Due to the limited frequency resolution and masking of the speech signal in cochlear implants, speech in noise can be challenging for cochlear implant users. Today, noise reduction technologies can give good results in constant noise. However, results may vary in fluctuating noisy environments [<xref rid="B83-audiolres-15-00056" ref-type="bibr">83</xref>,<xref rid="B84-audiolres-15-00056" ref-type="bibr">84</xref>,<xref rid="B85-audiolres-15-00056" ref-type="bibr">85</xref>,<xref rid="B86-audiolres-15-00056" ref-type="bibr">86</xref>,<xref rid="B87-audiolres-15-00056" ref-type="bibr">87</xref>].</p><p>Goehring et al. (2017) used machine learning methods to develop a speech enhancement algorithm based on neural networks to understand cochlear implant users in noisy environments better. The algorithm was applied to 14 cochlear implant users using three different noises: a speech-dominated noise, a noise that mimics the rhythm of speech and crowd noise. Two different algorithm training methods were created with and without speaker dependence. The study showed that the speech intelligibility of cochlear implant users improved by 5&#x02013;6 dB in the noise that mimics the speech rhythm. However, the speaker-dependent algorithm performed better in all cases. The study suggests that although speaker-dependent models give better results, they need to be retrained for each speaker [<xref rid="B32-audiolres-15-00056" ref-type="bibr">32</xref>].</p><p>Cochlear implant users have difficulty understanding speech not only because of noise but also because of echo. Single and multi-microphone systems and integrated algorithms for the cochlear implant are used to reduce echo. However, the effectiveness of these systems is not one hundred percent in all conditions. Desmond J.M et al. developed a machine learning model to detect echo channel by channel. In the study, artificially generated echo models were evaluated using these algorithms. Machine learning models using Maximum A Posteriori and relevance vector machine were able to detect echo with about 90% accuracy. The study stated that the success rate of this model decreases when noise and echo are present together, but it can be improved with accurate user results [<xref rid="B51-audiolres-15-00056" ref-type="bibr">51</xref>].</p><p>Gaultier C. et al. (2024) also examined speech intelligibility in reverberant environments. They investigated how deep learning applications and the use of multiple microphones would affect the speech understanding of cochlear implant users. In the study, 12,000 different sound environments were simulated and modeled. The researchers tested single-microphone, three-microphone and multi-microphone deep learning-based algorithms. The study showed that using multiple microphones improved speech understanding by up to 10.3 dB. It is emphasized that algorithms should be standardized with different languages and speakers and that faster and lower-latency models should be developed [<xref rid="B88-audiolres-15-00056" ref-type="bibr">88</xref>].</p><p>In another study using a machine learning-based strategy to improve speech intelligibility in reverberation and noisy environments, reverberation is simulated using the Room Impulse Response (RIR) machine learning model. Accuracy rates were classified using the relevance vector machine learning model. Accuracy rates in the 60&#x02013;80% range were reported for reverberant conditions and 30&#x02013;60% for reverberant and noisy conditions [<xref rid="B46-audiolres-15-00056" ref-type="bibr">46</xref>].</p><p>In another study, to improve speech intelligibility for individuals with hearing loss, deep neural networks were used to analyze speech intelligibility for individuals with hearing loss. NC+DDAE, which consists of two main components, a noise classifier and a deep noise reduction autoencoder, was used in the study. While the developed model reached 19.1% of the control group under 5 dB SNR in two-speaker noise, this rate increased to 53% for cochlear implant users. In construction noise, while the control group was 45.4% under 5 dB SNR, this rate increased to 77% for cochlear implant users [<xref rid="B40-audiolres-15-00056" ref-type="bibr">40</xref>].</p><p>In a study aiming to improve speech intelligibility by recognizing environmental conditions in cochlear implant users, three features for environment detection were extracted from the ACE strategy, which is widely used in cochlear implant users, with the aim of enabling cochlear implants to classify environmental conditions automatically. Classification accuracies of the Gaussian Mixture Model, support vector machine model, and neural Networks models were reported to vary between 95 and 97%. The best results were obtained in both noisy and reverberant conditions. Despite limited training data, the support vector machine model was shown to be the most successful model, with a rate of 97.79% [<xref rid="B50-audiolres-15-00056" ref-type="bibr">50</xref>].</p><p>In another study on speech intelligibility, Kang Y. et al. (2021) evaluated deep learning-based speech enhancement algorithms that provide a trade-off between speech distortion and noise residual. This study identified a Long Short Term Memory (LSTM)-based speech enhancement algorithm. The system performance was tested using an envelope-based correlation metric. The study evaluated 10 normal hearing and 19 cochlear implant users. When the Wiener filter, which is considered a traditional method, and LSTM, which is a machine learning model, were compared, it was stated that LSTM performed better. It is stated that the proposed system can help cochlear implant users to understand speech better [<xref rid="B54-audiolres-15-00056" ref-type="bibr">54</xref>].</p><p>Recurrent neural network and SepFormer (Transformer-based advanced model) models were used to test speech intelligibility in 13 cochlear implant users with three different scales in another study investigating the effectiveness of deep learning-based machine learning methods in noisy environments. The SepFormer machine learning model was the most successful in speech intelligibility. Scores ranging from 74.7 to 78.2% were obtained in different noises. This study also evaluated the voice quality of cochlear implant users. In the evaluation method, where a maximum of 5 points can be obtained in the evaluation made with the mean opinion scores criterion, the SepFormer model received values ranging between 4.5 and 4.6 in different noises. This study shows that deep learning-based approaches successfully improve the speech intelligibility of cochlear implant users [<xref rid="B34-audiolres-15-00056" ref-type="bibr">34</xref>].</p><p>Developing a new model called Fused Deep ACE by combining the traditional ACE strategy with deep learning in bilateral cochlear implant users, the researchers aimed to facilitate the exchange of information between the two cochlear implants to improve speech understanding in noise and preserve binaural auditory cues. In this context, the models&#x02019; noise reduction, speech intelligibility, and speech quality performances were compared using speech and noise sounds. The best performance was obtained with the Fused Deep ACE bilateral ACE and deep ACE [<xref rid="B41-audiolres-15-00056" ref-type="bibr">41</xref>].</p><p>In another study using the Deep ACE model, DEEP ACE was the most successful in comparing the ACE model used with the traditional ACE Wiener filter and the models named TasNET+ACE for speech intelligibility in noise. In speech quality evaluations, the DEEP ACE model scored 91.8 in quiet and 75.3&#x02013;79.6 in noisy environments on a 0&#x02013;100 scale using the MUSHRA test. In speech discrimination tests, this rate was between 63.1 and 64.1%. The study shows that a deep learning-based voice coding strategy has excellent potential in cochlear implant users [<xref rid="B42-audiolres-15-00056" ref-type="bibr">42</xref>].</p><p>When the accuracy rates in studies examining speech intelligibility in noise are examined, it is seen that they vary between 30 and 97%. The varying rates indicate that different machine learning-based models should be developed, especially in reverberant environments, and tests should be performed in many noisy environments. However, despite the limited training data, it was observed that the support vector machine algorithm achieved the highest accuracy in noise comprehension test conditions.</p></sec><sec id="sec4dot5-audiolres-15-00056"><title>4.5. Other Studies</title><p>Considering the problems experienced by cochlear implant users, it is known that they have problems with noise and emotional skills, such as emotion recognition. Due to the limited spectral and temporal resolution of cochlear implant technology, it is tough for cochlear implant users to understand musical perception and emotional speech expressions. Acoustic cues such as tone of voice, pitch changes, rhythm and timbre are the basic parameters of emotional perception. Since cochlear implant users cannot wholly and accurately encode these acoustic cues, they have difficulty distinguishing expressions such as surprised, sad, happy or neutral. This can result in difficulties in social adaptation and quality of life, including in patients with tinnitus symptoms [<xref rid="B89-audiolres-15-00056" ref-type="bibr">89</xref>].</p><p>In a study on this subject, the emotion recognition abilities of cochlear implant users were analyzed using machine learning models by analyzing EEG signals. In this study, 63.6% of the users were unilateral users, and 36.4% were bilateral users, and there were 22 users in total. A total of 24 audio and 24 musical expressions were tested with happy, sad and neutral emotion categories. In the study, EEG patterns specific to emotional activity were analyzed with the Random Forest machine learning model. It was reported that the model made 7.5% more successful predictions in the triple classification (happy, sad, neutral), 7.8%, 6.6% and 8.1% more successful predictions in the happy&#x02013;sad, happy&#x02013;neutral, and sad&#x02013;neutral classifications, respectively. In the study, especially in the sad category (1.2&#x02013;1.5%), model accuracy increased at rates ranging from 1.2 to 1.5%, and machine learning models reported promising results in cochlear implant users [<xref rid="B90-audiolres-15-00056" ref-type="bibr">90</xref>].</p><p>As is known, cochlear implant users&#x02019; rehabilitation outcomes vary according to demographic and audiologic factors. One of the reasons for these differences is the adaptation process required for cognitive processes to process and analyze electrical signals. EEG is an important tool for assessing plasticity and evoked potentials in cochlear implant users. As it is known, EEG evaluations cannot be performed in cochlear implant users due to electrical artifacts.</p><p>In this context, a study evaluating the effectiveness of machine learning techniques to remove cochlear-induced artifacts reported that it was detected with 95.4% accuracy in 66 pediatric cochlear implant users using a support vector machine. This study reported that artifact-detected EEG channels could be cleaned by the Ensemble Empirical Mode Decomposition method. As a result of this study, it was stated that artifacts can be removed from EEG data with machine learning models and can be used in rehabilitation methods [<xref rid="B52-audiolres-15-00056" ref-type="bibr">52</xref>].</p><p>In a study evaluating the prediction results using a machine learning model in auditory performance outcomes in cochlear implant users, 13 EEG data from three users were used pre-op and post-op at 3, 6, 12 and 18 months. Auditory, visual and tactile stimuli were used in the study. EEG data were used to investigate how neural data were activated in different cortex regions. Tactile features were found to be the best predictor, with a success rate of 98.8% in the success rates obtained using the support vector machine. Machine learning can be used to evaluate cochlear implant performance [<xref rid="B53-audiolres-15-00056" ref-type="bibr">53</xref>].</p><p>As is known, speech perception and quality of life outcomes show significant variability in cochlear implant users. This study used the K means clustering algorithm to evaluate speech perception and quality of life in 30 adult cochlear implant users. The participants&#x02019; performance was divided into three groups: high achievement group, at-risk group and highest achievement group. The results were evaluated using the Reliable Change Index. As a result of the modeling, it was reported that the best predictors of cochlear implant success were music education and cognitive capacity, while quality of life results were similar [<xref rid="B91-audiolres-15-00056" ref-type="bibr">91</xref>].</p><p>Unlike all these studies, in another study that examined the relationship between phenotyping based on psychological symptoms using machine learning and the presence of tinnitus in adult users of cochlear implants, 99 users were classified as having and not having tinnitus symptoms. The Gaussian Mixture Model machine learning model was used to determine the association of symptoms such as anxiety, depression and insomnia with tinnitus. With this model, subgroups were created from patient data. The relationship between the groups and tinnitus was analyzed with statistical tests. In the study, the rate of tinnitus was found to be lowest in patients with low symptom levels (39.1%), while the rate of tinnitus was higher in phenotypes with high anxiety and insomnia levels [80%]. This study reported that individualized treatments can be beneficial by creating psychological profiles with machine learning in cochlear implant users.</p><p>In the effectiveness of machine learning models, prediction accuracies were determined using Random Forest Regression, Histogram Gradient Boosting, and Elastic-Net regression models for hearing loss progression in Enlarged Vestibular Aqueduct (EVA) anomaly, a type of inner ear anomaly. Audiological, genetic and radiological data were analyzed. Incomplete Partition Type 2 and Endolymphatic sac signal heterogeneity were identified as the most important indicators of hearing loss progression. The study reported that machine learning models remained in similar ranges in prediction success (R2 = 0.26&#x02013;27), while Elastic Net regression reached the highest accuracy with R2 = 0.32. In the study&#x02019;s findings, it is reported that machine learning prediction models are still low in EVA, and early MR scans are important in these patients [<xref rid="B92-audiolres-15-00056" ref-type="bibr">92</xref>].</p><p>In another study by Nobel J et al. (2023), they developed a machine learning model to reduce the energy consumed by cochlear implants while providing nerve stimulation. The study was determined in two stages: developing an auditory neural model and determining energy-optimized waveforms. It was stated that the convolutional neural network (CNN) could mimic the real neural model with an accuracy of over 99% and increased the computation time by five times. At the same time, it was reported that this model consumes 8&#x02013;45% less energy than conventional square waves. The study&#x02019;s findings suggest that machine learning models can be used to increase energy efficiency and reduce the size of cochlear implants [<xref rid="B43-audiolres-15-00056" ref-type="bibr">43</xref>].</p><p>When the studies are evaluated, different areas related to machine learning for cochlear implant surgery in audiology have started to be studied in recent years. Energy optimization, tinnitus applications, emotional emotion recognition and artifact reduction, and inner ear anomalies have also been studied. In artifact reduction and evaluation of EEG data, it is seen that success rates of over 90% have been approached.</p><p>In addition to these diverse application areas of machine learning in cochlear implant surgery, a growing body of systematic reviews has emerged in recent years, aiming to synthesize the predictive use of machine learning models for auditory outcomes. When these review studies are evaluated, important thematic distinctions and methodological variabilities across the literature become evident.</p><p>When the review studies in the literature are examined, it is seen that Crowson et al. (2020) comprehensively evaluated various usage areas (signal processing, surgical support, etc.) of machine learning applications in cochlear implant (CI) processes but did not develop a specific focus on outcome prediction [<xref rid="B9-audiolres-15-00056" ref-type="bibr">9</xref>]. On the other hand, Mo et al. (2025) systematically reviewed studies on predicting auditory functional outcomes in CI users with ML models, but there was a significant diversity in terms of model types, predictor variables and validation methods [<xref rid="B11-audiolres-15-00056" ref-type="bibr">11</xref>]. Patro et al. (2024) developed automated guidance systems for the identification of CI candidates using machine learning, thus focusing on candidacy processes rather than direct auditory performance prediction [<xref rid="B93-audiolres-15-00056" ref-type="bibr">93</xref>]. Shafieibavani et al. (2021) evaluated the predictive power and generalizability of ML models on large multicenter datasets, but showed that the potential for clinical use of the models remained limited due to high error rates [<xref rid="B13-audiolres-15-00056" ref-type="bibr">13</xref>]. Zhang et al. (2024) analyzed the general trends of ML in the field of communication sciences and disorders with bibliometric methods, but did not provide an analysis specific to C&#x00131;s [<xref rid="B94-audiolres-15-00056" ref-type="bibr">94</xref>].</p><p>This systematic review presents a unique and narrow focus on auditory performance prediction in cochlear implant users, in contrast to the broader thematic studies in the existing literature. By minimizing the potential confounding effects of multidimensional variables that may influence cochlear implant outcomes, such as age, etiology, duration of implantation and comorbidities, the effectiveness of machine learning models in predicting auditory performance alone was analyzed with methodological rigor. Thanks to this focused approach, the clinical interpretability of the findings was enhanced, and directly applicable information was generated for model development and standardization processes. The study provides a conceptual framework for the specific use of machine learning algorithms in a complex clinical problem, such as auditory outcome prediction in cochlear implant users and provides a strategic roadmap that can guide future studies in terms of both model selection and dataset standardization. Accordingly, the present systematic review provides both a theoretical foundation and a practical guide for the development of machine learning-based clinical predictions in the field of cochlear implantation.</p></sec><sec id="sec4dot6-audiolres-15-00056"><title>4.6. Evaluation in Terms of Clinical Practice</title><p>Although accuracy rates are frequently emphasized in the evaluation of machine learning models, this alone is insufficient to reflect a model&#x02019;s clinical effectiveness. In addition to accuracy rates, it was observed that different metrics such as sensitivity, specificity, F1 score, area under the ROC curve (AUC), error rate, mean absolute error (MAE) and root mean square error (RMSE) were used in the studies examined in this review. In addition, some studies predicted performance through regression models instead of classification tasks and reported different metrics. However, many of these models did not directly target individual auditory performances of cochlear implant users; instead, they targeted indirect parameters such as preoperative candidate suitability, electrode placement optimization or complication risk prediction. In this context, not only the prediction accuracy of machine learning models but also how the predictions are made, which data are highlighted and the explainability of the model in decision-making processes are of critical importance for audiology practice.</p><p>The integration of machine learning models into clinical applications is affected not only by the model&#x02019;s accuracy but also by the training and practical skills of users (doctors, clinical technicians, etc.) on how to use these results. In this context, supporting models with explainable artificial intelligence (XAI) features will allow healthcare professionals to understand the decisions made by the model and question them when necessary. In addition, it is of great importance for models to be able to make fast and reliable decisions for direct usability in clinical applications. However, difficulties such as data incompatibility, ethical issues, and legal obstacles that may be encountered in the clinical environment should also be taken into account for the development of these applications. Therefore, a multidisciplinary approach and long-term follow-up are required for the adoption of clinical decision support systems in the healthcare field. Explainable artificial intelligence (XAI) refers to the ability of the model to explain its decisions understandably and transparently, especially in healthcare and clinical applications. The main goal of XAI is to overcome the &#x0201c;black box&#x0201d; structure of deep learning and machine learning models and to provide healthcare professionals with information on how these models make predictions, which features are effective in what way, and which criteria are used to make decisions.</p><p>In many studies included in the review, machine learning-based models were tested only on experimental or retrospective data sets but were not systematically validated in clinical settings. This situation significantly limits the integration of these models into clinical practices and poses potential risks to patient safety. Lack of clinical validation directly affects the reliability and internal and external validity of the model, as well as ethical responsibilities in healthcare delivery. The high accuracy rates of machine learning algorithms on experimental data cannot be considered a sufficient indicator for integrating these models into clinical decision-making processes because there may be significant differences between the findings obtained in experimental settings and clinical practices in real-life conditions. These differences are shaped by many factors, such as the diversity of the patient population, variability in data quality, environmental factors and how healthcare professionals interpret the model output. Therefore, models must be tested prospectively in controlled and observational studies for experimental findings to be transformed into clinical validity. In this context, instead of focusing solely on algorithmic accuracy, it is important to conduct multicenter, long-term and application-based studies that evaluate the performance of models in clinical settings. Such studies allow us to assess whether the model works reliably in various patient groups under different clinical protocols and healthcare systems. In addition, issues such as how model outputs are interpreted by healthcare professionals and to what extent they can be integrated into clinical decision-making processes should be investigated. As a result, to strengthen the clinical validity of machine learning-based models and enable the sustainable use of these technologies in healthcare, the gap between experimental and clinical data needs to be systematically addressed. In future studies, it is important to focus on the technical adequacy of the model and clinical validation processes to make real progress in this field.</p><p>In this study, machine learning techniques used in cochlear implant surgery, especially in the field of audiology, were examined. Among the machine learning techniques used in the study, Random Forest (96.2%), Bayesian Linear Regression and Extreme Maching Learning have the highest accuracy rates, while Deep Denois&#x00131;ng Autoencoder (DDAE) (46.8%), Sepformer Seperat&#x00131;on Transformer (59.5%) and recurrent neural network (RNN) (59.5%) have the lowest accuracy rates. It was observed that deep learning methods such as ANNs, DNNs and LSTMs were used especially in complex methods such as speech intelligibility and speech understanding in noise. In machine learning techniques, methods such as Random Forest and Bayesian Linear Regression are thought to be resistant to high accuracy rates, diverse and high numbers of datasets, as well as overfitting (overlearning while training machine learning and obtaining false high accuracy answers). However, it is thought that the low accuracy rates may be due to the difficulties of machine learning methods when processing time series. This suggests that it may be more appropriate to evaluate deep learning methods used in complex skills, such as comprehension in noise, by subjecting them to longer training periods with larger, complex data sets. At the same time, the success criteria of the studies included in our study were evaluated using different metrics and different methodologies. Most of the studies in this review used similar machine learning models (e.g., decision trees and artificial neural networks), which allowed the overall results to be consistent. However, some studies achieved higher accuracy rates using more advanced models. These differences are related to the variety of data sets used and differences in model optimisation techniques.</p><p>Significant differences were observed between studies; some studies tested accuracy with smaller data sets, while others presented more reliable results based on larger and heterogeneous data sets. These differences had a significant impact on the generalisability of the model. Most studies used standard performance metrics such as accuracy and F1 score, which facilitated the comparison of results. However, some studies used only the accuracy metric, which may have led to ignoring class imbalances. This situation is thought to affect the generalizability of the results. In a significant number of the studies analyzed in this review, it is seen that machine learning models were developed with data obtained from a limited number of participants. This fundamental limitation directly affects the accuracy rates and the generalizability of the developed models for clinical applications. Small sample sizes may result from various reasons, such as the difficulty of ethics committee approval processes, restrictions on data confidentiality and security, difficulties in reaching cochlear implant users, and limited access to the sample, especially in studies on pediatric or rare groups.</p><p>Despite these limitations, it is noteworthy that in most existing studies, various methods are used to improve model performance. In particular, the preference for techniques such as cross-validation, data augmentation, oversampling and feature engineering reveals the efforts of researchers to obtain more reliable results when modeling with limited data. However, it should be noted that these techniques are insufficient to ensure the models&#x02019; generalizability.</p><p>Since the accuracy of models developed with small samples is often evaluated only on the training data, it remains unclear how the model will perform when applied to a different population or an independent data set. Therefore, in future studies, clinical validation that models are developed with multicenter and large datasets covering individuals with different socio-demographic characteristics is expected to provide more robust and generalizable results. In addition, replicating existing models in different datasets and testing their external validity is critical for the clinical validity of machine learning-based systems developed in the field.</p><p>In conclusion, although studies conducted with small sample sizes are an important start in the literature, there is a clear need for multicenter, large-scale and highly representative studies in order to make progress in this field. Such studies will facilitate the integration of the developed models into clinical decision support systems and increase their adaptability to different patient groups.</p><p>Using machine learning-based systems in cochlear implant users brings ethical responsibilities not limited to technical accuracy. First, most of the data on which these systems are trained is sensitive medical information; therefore, data anonymization, secure storage, and ethical approval processes are important. In addition, the transparency of the algorithms in the decision-making process is a separate topic of discussion. Since the interpretability of models such as artificial neural networks, which have a &#x0201c;black box&#x0201d; structure, is limited, integrating explainable artificial intelligence approaches is recommended so clinicians can understand algorithmic predictions and question them when necessary. Another important point is that using machine learning systems to support clinical decisions does not mean assigning the role of the final decision-maker to these systems. In this context, it should be emphasized that the final clinical responsibility should remain with healthcare professionals and that algorithmic biases should be recognized. When evaluated from all these perspectives, machine learning applications to be developed in the field of cochlear implants should be evaluated not only in terms of technical performance but also in terms of ethical responsibility. There are some limitations in the review process. Firstly, the articles were analyzed by a single researcher, which may have the potential to make some differences in the decision-making process.</p></sec></sec><sec sec-type="conclusions" id="sec5-audiolres-15-00056"><title>5. Conclusions</title><p>Finally, the diversity of methodological approaches and data sets used across studies may have led to a specific heterogeneity in the results and may have created differences in the findings. In future studies, performance can be examined in more detail by making more standardized measurements with larger data sets. In future studies, how the results will be affected by the combination of different machine learning models can be discussed. This study examines machine learning applications in cochlear implant surgery in detail, focusing on Audiology. Random Forest and Bayesian Linear Regression models can be used more widely in clinical applications. However, further research is needed to use machine learning models in more complex data sets and fields.</p><p>It is known that cochlear implant candidates constitute a highly heterogeneous group in terms of age, hearing loss type, etiology, implantation time and accompanying health conditions. However, the datasets used in existing studies generally do not adequately represent this clinical diversity, which limits the generalizability of the developed machine-learning models and their performance in extreme cases. Integration of various technical approaches is suggested to overcome this problem. First, it may be helpful to use data augmentation methods, especially synthetic sample generation techniques such as SMOTE or ADASYN, to reduce the imbalance between classes. In addition, a more representative and general dataset can be created by integrating data from multicenter and different sources. In order to better manage heterogeneity, it is also important to separate the data into subgroups according to variables such as age group, etiology or cognitive status and train separate models for these groups. However, transfer learning and domain adaptation methods can improve model performance in smaller and rare groups. Domain adaptation enables the model to adapt to new areas by transferring information between different but similar datasets. In order to increase the explainability of the model, the features on which the model makes decisions should be made understandable with methods such as SHAP (Shapley Additive Explanations) and LIME (Local Interpretable Model-Agnostic Explanations). Finally, in order not to be limited to instantaneous assessments, it will be possible to predict the long-term auditory and cognitive outcomes of individuals more accurately by including longitudinal data in the model. Considering these techniques with a holistic approach will significantly increase both the accuracy and clinical applicability of machine learning models.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in this study are included in the article. Further inquiries can be directed to the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The author declares no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-audiolres-15-00056"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chadha</surname><given-names>S.</given-names></name>
<name><surname>Kamenov</surname><given-names>K.</given-names></name>
<name><surname>Cieza</surname><given-names>A.</given-names></name>
</person-group><article-title>The world report on hearing, 2021</article-title><source>Bull. World Health Organ.</source><year>2021</year><volume>99</volume><fpage>242</fpage><lpage>242A</lpage><comment>Available online: <ext-link xlink:href="https://pmc.ncbi.nlm.nih.gov/articles/PMC8085630/" ext-link-type="uri">https://pmc.ncbi.nlm.nih.gov/articles/PMC8085630/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-05-03">(accessed on 3 May 2025)</date-in-citation><pub-id pub-id-type="doi">10.2471/BLT.21.285643</pub-id><pub-id pub-id-type="pmid">33953438</pub-id>
</element-citation></ref><ref id="B2-audiolres-15-00056"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Deep</surname><given-names>N.</given-names></name>
<name><surname>Dowling</surname><given-names>E.</given-names></name>
<name><surname>Jethanamest</surname><given-names>D.</given-names></name>
<name><surname>Carlson</surname><given-names>M.</given-names></name>
</person-group><article-title>Cochlear Implantation: An Overview</article-title><source>J. Neurol. Surg. B Skull Base</source><year>2019</year><volume>80</volume><fpage>169</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1055/s-0038-1669411</pub-id><pub-id pub-id-type="pmid">30931225</pub-id>
</element-citation></ref><ref id="B3-audiolres-15-00056"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Goudey</surname><given-names>B.</given-names></name>
<name><surname>Plant</surname><given-names>K.</given-names></name>
<name><surname>Kiral</surname><given-names>I.</given-names></name>
<name><surname>Jimeno-Yepes</surname><given-names>A.</given-names></name>
<name><surname>Swan</surname><given-names>A.</given-names></name>
<name><surname>Gambhir</surname><given-names>M.</given-names></name>
<name><surname>B&#x000fc;chner</surname><given-names>A.</given-names></name>
<name><surname>Kludt</surname><given-names>E.</given-names></name>
<name><surname>Eikelboom</surname><given-names>R.H.</given-names></name>
<name><surname>Sucher</surname><given-names>C.</given-names></name>
<etal/>
</person-group><article-title>A MultiCenter Analysis of Factors Associated with Hearing Outcome for 2735 Adults with Cochlear Implants</article-title><source>Trends Hear.</source><year>2021</year><volume>25</volume><fpage>23312165211037525</fpage><pub-id pub-id-type="doi">10.1177/23312165211037525</pub-id><pub-id pub-id-type="pmid">34524944</pub-id>
</element-citation></ref><ref id="B4-audiolres-15-00056"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pisoni</surname><given-names>D.B.</given-names></name>
<name><surname>Kronenberger</surname><given-names>W.G.</given-names></name>
<name><surname>Harris</surname><given-names>M.S.</given-names></name>
<name><surname>Moberly</surname><given-names>A.C.</given-names></name>
</person-group><article-title>Three challenges for future research on cochlear implants</article-title><source>World J. Otorhinolaryngol. Head Neck Surg.</source><year>2017</year><volume>3</volume><fpage>240</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1016/j.wjorl.2017.12.010</pub-id><pub-id pub-id-type="pmid">29780970</pub-id>
</element-citation></ref><ref id="B5-audiolres-15-00056"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Boisvert</surname><given-names>I.</given-names></name>
<name><surname>Reis</surname><given-names>M.</given-names></name>
<name><surname>Au</surname><given-names>A.</given-names></name>
<name><surname>Cowan</surname><given-names>R.</given-names></name>
<name><surname>Dowell</surname><given-names>R.C.</given-names></name>
</person-group><article-title>Cochlear implantation outcomes in adults: A scoping review</article-title><source>PLoS ONE</source><year>2020</year><volume>15</volume><elocation-id>e0232421</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0232421</pub-id><pub-id pub-id-type="pmid">32369519</pub-id>
</element-citation></ref><ref id="B6-audiolres-15-00056"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lazard</surname><given-names>D.S.</given-names></name>
<name><surname>Vincent</surname><given-names>C.</given-names></name>
<name><surname>Venail</surname><given-names>F.</given-names></name>
<name><surname>Van de Heyning</surname><given-names>P.</given-names></name>
<name><surname>Truy</surname><given-names>E.</given-names></name>
<name><surname>Sterkers</surname><given-names>O.</given-names></name>
<name><surname>Skarzynski</surname><given-names>P.H.</given-names></name>
<name><surname>Skarzynski</surname><given-names>H.</given-names></name>
<name><surname>Schauwers</surname><given-names>K.</given-names></name>
<name><surname>O&#x02019;Leary</surname><given-names>S.</given-names></name>
<etal/>
</person-group><article-title>Pre-, Per- and Postoperative Factors Affecting Performance of Postlinguistically Deaf Adults Using Cochlear Implants: A New Conceptual Model over Time</article-title><source>PLoS ONE</source><year>2012</year><volume>7</volume><elocation-id>e48739</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0048739</pub-id><pub-id pub-id-type="pmid">23152797</pub-id>
</element-citation></ref><ref id="B7-audiolres-15-00056"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Blamey</surname><given-names>P.</given-names></name>
<name><surname>Artieres</surname><given-names>F.</given-names></name>
<name><surname>Ba&#x0015f;kent</surname><given-names>D.</given-names></name>
<name><surname>Bergeron</surname><given-names>F.</given-names></name>
<name><surname>Beynon</surname><given-names>A.</given-names></name>
<name><surname>Burke</surname><given-names>E.</given-names></name>
<name><surname>Dillier</surname><given-names>N.</given-names></name>
<name><surname>Dowell</surname><given-names>R.</given-names></name>
<name><surname>Fraysse</surname><given-names>B.</given-names></name>
<name><surname>Gall&#x000e9;go</surname><given-names>S.</given-names></name>
<etal/>
</person-group><article-title>Factors Affecting Auditory Performance of Postlinguistically Deaf Adults Using Cochlear Implants: An Update with 2251 Patients</article-title><source>Audiol. Neurotol.</source><year>2013</year><volume>18</volume><fpage>36</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1159/000343189</pub-id></element-citation></ref><ref id="B8-audiolres-15-00056"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Roditi</surname><given-names>R.E.</given-names></name>
<name><surname>Poissant</surname><given-names>S.F.</given-names></name>
<name><surname>Bero</surname><given-names>E.M.</given-names></name>
<name><surname>Lee</surname><given-names>D.J.</given-names></name>
</person-group><article-title>A Predictive Model of Cochlear Implant Performance in Postlingually Deafened Adults</article-title><source>Otol. Neurotol.</source><year>2009</year><volume>30</volume><fpage>449</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1097/MAO.0b013e31819d3480</pub-id><pub-id pub-id-type="pmid">19415041</pub-id>
</element-citation></ref><ref id="B9-audiolres-15-00056"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Crowson</surname><given-names>M.G.</given-names></name>
<name><surname>Lin</surname><given-names>V.</given-names></name>
<name><surname>Chen</surname><given-names>J.M.</given-names></name>
<name><surname>Chan</surname><given-names>T.C.Y.</given-names></name>
</person-group><article-title>Machine Learning and Cochlear Implantation&#x02014;A Structured Review of Opportunities and Challenges</article-title><source>Otol. Neurotol.</source><year>2020</year><volume>41</volume><fpage>E36</fpage><lpage>E45</lpage><pub-id pub-id-type="doi">10.1097/MAO.0000000000002440</pub-id><pub-id pub-id-type="pmid">31644477</pub-id>
</element-citation></ref><ref id="B10-audiolres-15-00056"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Crowson</surname><given-names>M.G.</given-names></name>
<name><surname>Dixon</surname><given-names>P.</given-names></name>
<name><surname>Mahmood</surname><given-names>R.</given-names></name>
<name><surname>Lee</surname><given-names>J.W.</given-names></name>
<name><surname>Shipp</surname><given-names>D.</given-names></name>
<name><surname>Le</surname><given-names>T.</given-names></name>
<name><surname>Lin</surname><given-names>V.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Chan</surname><given-names>T.C.Y.</given-names></name>
</person-group><article-title>Predicting Postoperative Cochlear Implant Performance Using Supervised Machine Learning</article-title><source>Otol. Neurotol.</source><year>2020</year><volume>41</volume><fpage>E1013</fpage><lpage>E1023</lpage><pub-id pub-id-type="doi">10.1097/MAO.0000000000002710</pub-id><pub-id pub-id-type="pmid">32558750</pub-id>
</element-citation></ref><ref id="B11-audiolres-15-00056"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mo</surname><given-names>J.T.</given-names></name>
<name><surname>Chong</surname><given-names>D.S.</given-names></name>
<name><surname>Sun</surname><given-names>C.</given-names></name>
<name><surname>Mohapatra</surname><given-names>N.</given-names></name>
<name><surname>Jiam</surname><given-names>N.T.</given-names></name>
</person-group><article-title>Machine-Learning Predictions of Cochlear Implant Functional Outcomes: A Systematic Review</article-title><source>Ear Hear.</source><year>2025</year><comment>Available online: <ext-link xlink:href="https://pubmed.ncbi.nlm.nih.gov/39876044/" ext-link-type="uri">https://pubmed.ncbi.nlm.nih.gov/39876044/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-05-03">(accessed on 3 May 2025)</date-in-citation></element-citation></ref><ref id="B12-audiolres-15-00056"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Weng</surname><given-names>J.</given-names></name>
<name><surname>Xue</surname><given-names>S.</given-names></name>
<name><surname>Wei</surname><given-names>X.</given-names></name>
<name><surname>Lu</surname><given-names>S.</given-names></name>
<name><surname>Xie</surname><given-names>J.</given-names></name>
<name><surname>Kong</surname><given-names>Y.</given-names></name>
<name><surname>Shen</surname><given-names>M.</given-names></name>
<name><surname>Chen</surname><given-names>B.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Zou</surname><given-names>X.</given-names></name>
<etal/>
</person-group><article-title>Machine learning-based prediction of the outcomes of cochlear implantation in patients with inner ear malformation</article-title><source>Eur. Arch. Oto-Rhino-Laryngol.</source><year>2024</year><volume>281</volume><fpage>3535</fpage><lpage>3545</lpage><pub-id pub-id-type="doi">10.1007/s00405-024-08463-w</pub-id><pub-id pub-id-type="pmid">38353769</pub-id>
</element-citation></ref><ref id="B13-audiolres-15-00056"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shafieibavani</surname><given-names>E.</given-names></name>
<name><surname>Goudey</surname><given-names>B.</given-names></name>
<name><surname>Kiral</surname><given-names>I.</given-names></name>
<name><surname>Zhong</surname><given-names>P.</given-names></name>
<name><surname>Jimeno-Yepes</surname><given-names>A.</given-names></name>
<name><surname>Swan</surname><given-names>A.</given-names></name>
<name><surname>Gambhir</surname><given-names>M.</given-names></name>
<name><surname>Buechner</surname><given-names>A.</given-names></name>
<name><surname>Kludt</surname><given-names>E.</given-names></name>
<name><surname>Eikelboom</surname><given-names>R.H.</given-names></name>
<etal/>
</person-group><article-title>Predictive models for cochlear implant outcomes: Performance, generalizability, and the impact of cohort size</article-title><source>Trends Hear.</source><year>2021</year><volume>25</volume><fpage>23312165211066174</fpage><pub-id pub-id-type="doi">10.1177/23312165211066174</pub-id><pub-id pub-id-type="pmid">34903103</pub-id>
</element-citation></ref><ref id="B14-audiolres-15-00056"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lu</surname><given-names>S.</given-names></name>
<name><surname>Xie</surname><given-names>J.</given-names></name>
<name><surname>Wei</surname><given-names>X.</given-names></name>
<name><surname>Kong</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>B.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Yang</surname><given-names>M.</given-names></name>
<name><surname>Xue</surname><given-names>S.</given-names></name>
<name><surname>Shi</surname><given-names>Y.</given-names></name>
<etal/>
</person-group><article-title>Machine Learning-Based Prediction of the Outcomes of Cochlear Implantation in Patients with Cochlear Nerve Deficiency and Normal Cochlea: A 2-Year Follow-Up of 70 Children</article-title><source>Front. Neurosci.</source><year>2022</year><volume>16</volume><elocation-id>895560</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2022.895560</pub-id><pub-id pub-id-type="pmid">35812216</pub-id>
</element-citation></ref><ref id="B15-audiolres-15-00056"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jiang</surname><given-names>F.</given-names></name>
<name><surname>Jiang</surname><given-names>Y.</given-names></name>
<name><surname>Zhi</surname><given-names>H.</given-names></name>
<name><surname>Dong</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Ma</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Dong</surname><given-names>Q.</given-names></name>
<name><surname>Shen</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Artificial intelligence in healthcare: Past, present and future</article-title><source>Stroke Vasc. Neurol.</source><year>2017</year><volume>2</volume><fpage>230</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1136/svn-2017-000101</pub-id><pub-id pub-id-type="pmid">29507784</pub-id>
</element-citation></ref><ref id="B16-audiolres-15-00056"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hamet</surname><given-names>P.</given-names></name>
<name><surname>Tremblay</surname><given-names>J.</given-names></name>
</person-group><article-title>Artificial intelligence in medicine</article-title><source>Metabolism</source><year>2017</year><volume>69</volume><fpage>S36</fpage><lpage>S40</lpage><pub-id pub-id-type="doi">10.1016/j.metabol.2017.01.011</pub-id></element-citation></ref><ref id="B17-audiolres-15-00056"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Miller</surname><given-names>T.</given-names></name>
</person-group><article-title>Explanation in artificial intelligence: Insights from the social sciences</article-title><source>Artif. Intell.</source><year>2019</year><volume>267</volume><fpage>1</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/j.artint.2018.07.007</pub-id></element-citation></ref><ref id="B18-audiolres-15-00056"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Saeed</surname><given-names>H.S.</given-names></name>
<name><surname>Stivaros</surname><given-names>S.M.</given-names></name>
<name><surname>Saeed</surname><given-names>S.R.</given-names></name>
</person-group><article-title>The potential for machine learning to improve precision medicine in cochlear implantation</article-title><source>Cochlear Implant. Int.</source><year>2019</year><volume>20</volume><fpage>229</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1080/14670100.2019.1631520</pub-id></element-citation></ref><ref id="B19-audiolres-15-00056"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Erickson</surname><given-names>B.J.</given-names></name>
<name><surname>Korfiatis</surname><given-names>P.</given-names></name>
<name><surname>Akkus</surname><given-names>Z.</given-names></name>
<name><surname>Kline</surname><given-names>T.L.</given-names></name>
</person-group><article-title>Machine Learning for Medical Imaging</article-title><source>RadioGraphics</source><year>2017</year><volume>37</volume><fpage>505</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1148/rg.2017160130</pub-id><pub-id pub-id-type="pmid">28212054</pub-id>
</element-citation></ref><ref id="B20-audiolres-15-00056"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Le</surname><given-names>Q.V.</given-names></name>
</person-group><article-title>Building high-level features using large scale unsupervised learning</article-title><source>Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>26&#x02013;31 May 2013</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2013</year><fpage>8595</fpage><lpage>8598</lpage></element-citation></ref><ref id="B21-audiolres-15-00056"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Botros</surname><given-names>A.</given-names></name>
<name><surname>van Dijk</surname><given-names>B.</given-names></name>
<name><surname>Killian</surname><given-names>M.</given-names></name>
</person-group><article-title>AutoNRT<sup>TM</sup>: An automated system that measures ECAP thresholds with the Nucleus<sup>&#x000ae;</sup> Freedom<sup>TM</sup> cochlear implant via machine intelligence</article-title><source>Artif. Intell. Med.</source><year>2007</year><volume>40</volume><fpage>15</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.artmed.2006.06.003</pub-id><pub-id pub-id-type="pmid">16920343</pub-id>
</element-citation></ref><ref id="B22-audiolres-15-00056"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Heman-Ackah</surname><given-names>Y.D.</given-names></name>
</person-group><article-title>Diagnostic tools in laryngology</article-title><source>Curr. Opin. Otolaryngol. Head Neck Surg.</source><year>2004</year><volume>12</volume><fpage>549</fpage><lpage>552</lpage><pub-id pub-id-type="doi">10.1097/01.moo.0000144394.18003.03</pub-id><pub-id pub-id-type="pmid">15548916</pub-id>
</element-citation></ref><ref id="B23-audiolres-15-00056"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Thaler</surname><given-names>E.R.</given-names></name>
<name><surname>Hanson</surname><given-names>C.W.</given-names></name>
</person-group><article-title>Use of an electronic nose to diagnose bacterial sinusitis</article-title><source>Am. J. Rhinol.</source><year>2006</year><volume>20</volume><fpage>170</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1177/194589240602000209</pub-id><pub-id pub-id-type="pmid">16686381</pub-id>
</element-citation></ref><ref id="B24-audiolres-15-00056"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Burgansky-Eliash</surname><given-names>Z.</given-names></name>
<name><surname>Wollstein</surname><given-names>G.</given-names></name>
<name><surname>Chu</surname><given-names>T.</given-names></name>
<name><surname>Ramsey</surname><given-names>J.D.</given-names></name>
<name><surname>Glymour</surname><given-names>C.</given-names></name>
<name><surname>Noecker</surname><given-names>R.J.</given-names></name>
<name><surname>Ishikawa</surname><given-names>H.</given-names></name>
<name><surname>Schuman</surname><given-names>J.S.</given-names></name>
</person-group><article-title>Optical Coherence Tomography Machine Learning Classifiers for Glaucoma Detection: A Preliminary Study</article-title><source>Investig. Opthalmology Vis. Sci.</source><year>2005</year><volume>46</volume><fpage>4147</fpage><pub-id pub-id-type="doi">10.1167/iovs.05-0366</pub-id><pub-id pub-id-type="pmid">16249492</pub-id>
</element-citation></ref><ref id="B25-audiolres-15-00056"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>McCullagh</surname><given-names>P.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Zheng</surname><given-names>H.</given-names></name>
<name><surname>Lightbody</surname><given-names>G.</given-names></name>
<name><surname>McAllister</surname><given-names>G.</given-names></name>
</person-group><article-title>A comparison of supervised classification methods for auditory brainstem response determination</article-title><source>Stud. Health Technol. Inform.</source><year>2007</year><volume>129</volume><issue-part>Pt 2</issue-part><fpage>1289</fpage><lpage>1293</lpage><pub-id pub-id-type="pmid">17911922</pub-id>
</element-citation></ref><ref id="B26-audiolres-15-00056"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Holmes</surname><given-names>A.E.</given-names></name>
<name><surname>Shrivastav</surname><given-names>R.</given-names></name>
<name><surname>Krause</surname><given-names>L.</given-names></name>
<name><surname>Siburt</surname><given-names>H.W.</given-names></name>
<name><surname>Schwartz</surname><given-names>E.</given-names></name>
</person-group><article-title>Speech based optimization of cochlear implants</article-title><source>Int. J. Audiol.</source><year>2012</year><volume>51</volume><fpage>806</fpage><lpage>816</lpage><pub-id pub-id-type="doi">10.3109/14992027.2012.705899</pub-id><pub-id pub-id-type="pmid">22978753</pub-id>
</element-citation></ref><ref id="B27-audiolres-15-00056"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Francart</surname><given-names>T.</given-names></name>
<name><surname>McDermott</surname><given-names>H.J.</given-names></name>
</person-group><article-title>Psychophysics, Fitting, and Signal Processing for Combined Hearing Aid and Cochlear Implant Stimulation</article-title><source>Ear Hear.</source><year>2013</year><volume>34</volume><fpage>685</fpage><lpage>700</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e31829d14cb</pub-id><pub-id pub-id-type="pmid">24165299</pub-id>
</element-citation></ref><ref id="B28-audiolres-15-00056"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Konrad-Martin</surname><given-names>D.</given-names></name>
<name><surname>Reavis</surname><given-names>K.M.</given-names></name>
<name><surname>McMillan</surname><given-names>G.P.</given-names></name>
<name><surname>Dille</surname><given-names>M.F.</given-names></name>
</person-group><article-title>Multivariate DPOAE metrics for identifying changes in hearing: Perspectives from ototoxicity monitoring</article-title><source>Int. J. Audiol.</source><year>2012</year><volume>51</volume><fpage>S51</fpage><lpage>S62</lpage><pub-id pub-id-type="doi">10.3109/14992027.2011.635713</pub-id><pub-id pub-id-type="pmid">22264063</pub-id>
</element-citation></ref><ref id="B29-audiolres-15-00056"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>X.D.</given-names></name>
<name><surname>Wallace</surname><given-names>B.M.</given-names></name>
<name><surname>Gardner</surname><given-names>J.R.</given-names></name>
<name><surname>Ledbetter</surname><given-names>N.M.</given-names></name>
<name><surname>Weinberger</surname><given-names>K.Q.</given-names></name>
<name><surname>Barbour</surname><given-names>D.L.</given-names></name>
</person-group><article-title>Fast, Continuous Audiogram Estimation Using Machine Learning</article-title><source>Ear Hear.</source><year>2015</year><volume>36</volume><fpage>e326</fpage><lpage>e335</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000186</pub-id><pub-id pub-id-type="pmid">26258575</pub-id>
</element-citation></ref><ref id="B30-audiolres-15-00056"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kong</surname><given-names>Y.Y.</given-names></name>
<name><surname>Mullangi</surname><given-names>A.</given-names></name>
<name><surname>Kokkinakis</surname><given-names>K.</given-names></name>
</person-group><article-title>Classification of Fricative Consonants for Speech Enhancement in Hearing Devices</article-title><source>PLoS ONE</source><year>2014</year><volume>9</volume><elocation-id>e95001</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0095001</pub-id><pub-id pub-id-type="pmid">24747721</pub-id>
</element-citation></ref><ref id="B31-audiolres-15-00056"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>L.</given-names></name>
<name><surname>Holland</surname><given-names>S.K.</given-names></name>
<name><surname>Deshpande</surname><given-names>A.K.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Choo</surname><given-names>D.I.</given-names></name>
<name><surname>Lu</surname><given-names>L.J.</given-names></name>
</person-group><article-title>A semi-supervised Support Vector Machine model for predicting the language outcomes following cochlear implantation based on pre-implant brain fMRI imaging</article-title><source>Brain Behav.</source><year>2015</year><volume>5</volume><fpage>e00391</fpage><pub-id pub-id-type="doi">10.1002/brb3.391</pub-id><pub-id pub-id-type="pmid">26807332</pub-id>
</element-citation></ref><ref id="B32-audiolres-15-00056"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Goehring</surname><given-names>T.</given-names></name>
<name><surname>Bolner</surname><given-names>F.</given-names></name>
<name><surname>Monaghan</surname><given-names>J.J.M.</given-names></name>
<name><surname>van Dijk</surname><given-names>B.</given-names></name>
<name><surname>Zarowski</surname><given-names>A.</given-names></name>
<name><surname>Bleeck</surname><given-names>S.</given-names></name>
</person-group><article-title>Speech enhancement based on neural networks improves speech intelligibility in noise for cochlear implant users</article-title><source>Hear Res.</source><year>2017</year><volume>344</volume><fpage>183</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2016.11.012</pub-id><pub-id pub-id-type="pmid">27913315</pub-id>
</element-citation></ref><ref id="B33-audiolres-15-00056"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wathour</surname><given-names>J.</given-names></name>
<name><surname>Govaerts</surname><given-names>P.J.</given-names></name>
<name><surname>Lacroix</surname><given-names>E.</given-names></name>
<name><surname>Na&#x000ef;ma</surname><given-names>D.</given-names></name>
</person-group><article-title>Effect of a CI Programming Fitting Tool with Artificial Intelligence in Experienced Cochlear Implant Patients</article-title><source>Otol. Neurotol.</source><year>2023</year><volume>44</volume><fpage>209</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1097/MAO.0000000000003810</pub-id><pub-id pub-id-type="pmid">36728126</pub-id>
</element-citation></ref><ref id="B34-audiolres-15-00056"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Borjigin</surname><given-names>A.</given-names></name>
<name><surname>Kokkinakis</surname><given-names>K.</given-names></name>
<name><surname>Bharadwaj</surname><given-names>H.M.</given-names></name>
<name><surname>Stohl</surname><given-names>J.S.</given-names></name>
</person-group><article-title>Deep learning restores speech intelligibility in multi-talker interference for cochlear implant users</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>13241</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-63675-8</pub-id><pub-id pub-id-type="pmid">38853168</pub-id>
</element-citation></ref><ref id="B35-audiolres-15-00056"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Koyama</surname><given-names>H.</given-names></name>
</person-group><article-title>Machine learning application in otology</article-title><source>Auris Nasus Larynx</source><year>2024</year><volume>51</volume><fpage>666</fpage><lpage>673</lpage><pub-id pub-id-type="doi">10.1016/j.anl.2024.04.003</pub-id><pub-id pub-id-type="pmid">38704894</pub-id>
</element-citation></ref><ref id="B36-audiolres-15-00056"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alohali</surname><given-names>Y.A.</given-names></name>
<name><surname>Fayed</surname><given-names>M.S.</given-names></name>
<name><surname>Abdelsamad</surname><given-names>Y.</given-names></name>
<name><surname>Almuhawas</surname><given-names>F.</given-names></name>
<name><surname>Alahmadi</surname><given-names>A.</given-names></name>
<name><surname>Mesallam</surname><given-names>T.</given-names></name>
<name><surname>Hagr</surname><given-names>A.</given-names></name>
</person-group><article-title>Machine Learning and Cochlear Implantation: Predicting the Post-Operative Electrode Impedances</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>2720</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12122720</pub-id></element-citation></ref><ref id="B37-audiolres-15-00056"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schuerch</surname><given-names>K.</given-names></name>
<name><surname>Wimmer</surname><given-names>W.</given-names></name>
<name><surname>Dalbert</surname><given-names>A.</given-names></name>
<name><surname>Rummel</surname><given-names>C.</given-names></name>
<name><surname>Caversaccio</surname><given-names>M.</given-names></name>
<name><surname>Mantokoudis</surname><given-names>G.</given-names></name>
<name><surname>Gawliczek</surname><given-names>T.</given-names></name>
<name><surname>Weder</surname><given-names>S.</given-names></name>
</person-group><article-title>An intracochlear electrocochleography dataset&#x02014;From raw data to objective analysis using deep learning</article-title><source>Sci. Data</source><year>2023</year><volume>10</volume><fpage>157</fpage><pub-id pub-id-type="doi">10.1038/s41597-023-02055-9</pub-id><pub-id pub-id-type="pmid">36949075</pub-id>
</element-citation></ref><ref id="B38-audiolres-15-00056"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Waltzman</surname><given-names>S.B.</given-names></name>
<name><surname>Kelsall</surname><given-names>D.C.</given-names></name>
</person-group><article-title>The Use of Artificial Intelligence to Program Cochlear Implants</article-title><source>Otol. Neurotol.</source><year>2020</year><volume>41</volume><fpage>452</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1097/MAO.0000000000002566</pub-id><pub-id pub-id-type="pmid">32176123</pub-id>
</element-citation></ref><ref id="B39-audiolres-15-00056"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hafeez</surname><given-names>N.</given-names></name>
<name><surname>Du</surname><given-names>X.</given-names></name>
<name><surname>Boulgouris</surname><given-names>N.</given-names></name>
<name><surname>Begg</surname><given-names>P.</given-names></name>
<name><surname>Irving</surname><given-names>R.</given-names></name>
<name><surname>Coulson</surname><given-names>C.</given-names></name>
<name><surname>Tourrel</surname><given-names>G.</given-names></name>
</person-group><article-title>Electrical impedance guides electrode array in cochlear implantation using machine learning and robotic feeder</article-title><source>Hear Res.</source><year>2021</year><volume>412</volume><fpage>108371</fpage><pub-id pub-id-type="doi">10.1016/j.heares.2021.108371</pub-id><pub-id pub-id-type="pmid">34689069</pub-id>
</element-citation></ref><ref id="B40-audiolres-15-00056"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lai</surname><given-names>Y.H.</given-names></name>
<name><surname>Tsao</surname><given-names>Y.</given-names></name>
<name><surname>Lu</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>F.</given-names></name>
<name><surname>Su</surname><given-names>Y.T.</given-names></name>
<name><surname>Chen</surname><given-names>K.C.</given-names></name>
<name><surname>Chen</surname><given-names>Y.H.</given-names></name>
<name><surname>Chen</surname><given-names>L.C.</given-names></name>
<name><surname>Li</surname><given-names>L.P.H.</given-names></name>
<name><surname>Lee</surname><given-names>C.H.</given-names></name>
</person-group><article-title>Deep learning-based noise reduction approach to improve speech intelligibility for cochlear implant recipients</article-title><source>Ear Hear.</source><year>2018</year><volume>39</volume><fpage>795</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000537</pub-id><pub-id pub-id-type="pmid">29360687</pub-id>
</element-citation></ref><ref id="B41-audiolres-15-00056"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gajecki</surname><given-names>T.</given-names></name>
<name><surname>Nogueira</surname><given-names>W.</given-names></name>
</person-group><article-title>A Fused Deep Denoising Sound Coding Strategy for Bilateral Cochlear Implants</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2024</year><volume>71</volume><fpage>2232</fpage><lpage>2242</lpage><pub-id pub-id-type="doi">10.1109/TBME.2024.3367530</pub-id><pub-id pub-id-type="pmid">38376983</pub-id>
</element-citation></ref><ref id="B42-audiolres-15-00056"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gajecki</surname><given-names>T.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Nogueira</surname><given-names>W.</given-names></name>
</person-group><article-title>A Deep Denoising Sound Coding Strategy for Cochlear Implants</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2023</year><volume>70</volume><fpage>2700</fpage><lpage>2709</lpage><pub-id pub-id-type="doi">10.1109/TBME.2023.3262677</pub-id><pub-id pub-id-type="pmid">37030808</pub-id>
</element-citation></ref><ref id="B43-audiolres-15-00056"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>de Nobel</surname><given-names>J.</given-names></name>
<name><surname>Kononova</surname><given-names>A.V.</given-names></name>
<name><surname>Briaire</surname><given-names>J.J.</given-names></name>
<name><surname>Frijns</surname><given-names>J.H.M.</given-names></name>
<name><surname>B&#x000e4;ck</surname><given-names>T.H.W.</given-names></name>
</person-group><article-title>Optimizing stimulus energy for cochlear implants with a machine learning model of the auditory nerve</article-title><source>Hear Res.</source><year>2023</year><volume>432</volume><fpage>108741</fpage><pub-id pub-id-type="doi">10.1016/j.heares.2023.108741</pub-id><pub-id pub-id-type="pmid">36972636</pub-id>
</element-citation></ref><ref id="B44-audiolres-15-00056"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mamun</surname><given-names>N.</given-names></name>
<name><surname>Khorram</surname><given-names>S.</given-names></name>
<name><surname>Hansen</surname><given-names>J.H.L.</given-names></name>
</person-group><article-title>Convolutional Neural Network-Based Speech Enhancement for Cochlear Implant Recipients</article-title><source>Proceedings of the Interspeech 2019</source><conf-loc>Graz, Austria</conf-loc><conf-date>15&#x02013;19 September 2019</conf-date><publisher-name>ISCA</publisher-name><publisher-loc>Graz, Austria</publisher-loc><year>2019</year><fpage>4265</fpage><lpage>4269</lpage></element-citation></ref><ref id="B45-audiolres-15-00056"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Henry</surname><given-names>F.</given-names></name>
<name><surname>Parsi</surname><given-names>A.</given-names></name>
<name><surname>Glavin</surname><given-names>M.</given-names></name>
<name><surname>Jones</surname><given-names>E.</given-names></name>
</person-group><article-title>Experimental Investigation of Acoustic Features to Optimize Intelligibility in Cochlear Implants</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>7553</elocation-id><pub-id pub-id-type="doi">10.3390/s23177553</pub-id><pub-id pub-id-type="pmid">37688009</pub-id>
</element-citation></ref><ref id="B46-audiolres-15-00056"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chu</surname><given-names>K.</given-names></name>
<name><surname>Throckmorton</surname><given-names>C.</given-names></name>
<name><surname>Collins</surname><given-names>L.</given-names></name>
<name><surname>Mainsah</surname><given-names>B.</given-names></name>
</person-group><article-title>Using machine learning to mitigate the effects of reverberation and noise in cochlear implants</article-title><source>Proc. Mtgs. Acoust.</source><year>2018</year><volume>33</volume><fpage>050003</fpage></element-citation></ref><ref id="B47-audiolres-15-00056"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sinha</surname><given-names>R.</given-names></name>
<name><surname>Azadpour</surname><given-names>M.</given-names></name>
</person-group><article-title>Employing deep learning model to evaluate speech information in acoustic simulations of Cochlear implants</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>24056</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-73173-6</pub-id><pub-id pub-id-type="pmid">39402071</pub-id>
</element-citation></ref><ref id="B48-audiolres-15-00056"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Erfanian Saeedi</surname><given-names>N.</given-names></name>
<name><surname>Blamey</surname><given-names>P.J.</given-names></name>
<name><surname>Burkitt</surname><given-names>A.N.</given-names></name>
<name><surname>Grayden</surname><given-names>D.B.</given-names></name>
</person-group><article-title>An integrated model of pitch perception incorporating place and temporal pitch codes with application to cochlear implant research</article-title><source>Hear Res.</source><year>2017</year><volume>344</volume><fpage>135</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2016.11.005</pub-id><pub-id pub-id-type="pmid">27845260</pub-id>
</element-citation></ref><ref id="B49-audiolres-15-00056"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hajiaghababa</surname><given-names>F.</given-names></name>
<name><surname>Marateb</surname><given-names>H.R.</given-names></name>
<name><surname>Kermani</surname><given-names>S.</given-names></name>
</person-group><article-title>The design and validation of a hybrid digital-signal-processing plug-in for traditional cochlear implant speech processors</article-title><source>Comput. Methods Programs Biomed.</source><year>2018</year><volume>159</volume><fpage>103</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2018.03.003</pub-id><pub-id pub-id-type="pmid">29650304</pub-id>
</element-citation></ref><ref id="B50-audiolres-15-00056"><label>50.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hazrati</surname><given-names>O.</given-names></name>
<name><surname>Sadjadi</surname><given-names>S.O.</given-names></name>
<name><surname>Hansen</surname><given-names>J.H.L.</given-names></name>
</person-group><article-title>Robust and efficient environment detection for adaptive speech enhancement in cochlear implants</article-title><source>Proceedings of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Florence, Italy</conf-loc><conf-date>4&#x02013;9 May 2014</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2014</year><fpage>900</fpage><lpage>904</lpage></element-citation></ref><ref id="B51-audiolres-15-00056"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Desmond</surname><given-names>J.M.</given-names></name>
<name><surname>Collins</surname><given-names>L.M.</given-names></name>
<name><surname>Throckmorton</surname><given-names>C.S.</given-names></name>
</person-group><article-title>Using channel-specific statistical models to detect reverberation in cochlear implant stimuli</article-title><source>J. Acoust. Soc. Am.</source><year>2013</year><volume>134</volume><fpage>1112</fpage><lpage>1120</lpage><pub-id pub-id-type="doi">10.1121/1.4812273</pub-id><pub-id pub-id-type="pmid">23927111</pub-id>
</element-citation></ref><ref id="B52-audiolres-15-00056"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>Q.</given-names></name>
<name><surname>Wu</surname><given-names>Y.</given-names></name>
<name><surname>Zhu</surname><given-names>J.</given-names></name>
<name><surname>Cao</surname><given-names>L.</given-names></name>
<name><surname>Bai</surname><given-names>Y.</given-names></name>
<name><surname>Ni</surname><given-names>G.</given-names></name>
</person-group><article-title>Cochlear Implant Artifacts Removal in EEG-Based Objective Auditory Rehabilitation Assessment</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2024</year><volume>32</volume><fpage>2854</fpage><lpage>2863</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2024.3438149</pub-id><pub-id pub-id-type="pmid">39102322</pub-id>
</element-citation></ref><ref id="B53-audiolres-15-00056"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kyong</surname><given-names>J.S.</given-names></name>
<name><surname>Suh</surname><given-names>M.W.</given-names></name>
<name><surname>Han</surname><given-names>J.J.</given-names></name>
<name><surname>Park</surname><given-names>M.K.</given-names></name>
<name><surname>Noh</surname><given-names>T.S.</given-names></name>
<name><surname>Oh</surname><given-names>S.H.</given-names></name>
<name><surname>Lee</surname><given-names>J.H.</given-names></name>
</person-group><article-title>Cross-Modal Cortical Activity in the Brain Can Predict Cochlear Implantation Outcome in Adults: A Machine Learning Study</article-title><source>J. Int. Adv. Otol.</source><year>2021</year><volume>17</volume><fpage>380</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.5152/iao.2021.9337</pub-id><pub-id pub-id-type="pmid">34617886</pub-id>
</element-citation></ref><ref id="B54-audiolres-15-00056"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kang</surname><given-names>Y.</given-names></name>
<name><surname>Zheng</surname><given-names>N.</given-names></name>
<name><surname>Meng</surname><given-names>Q.</given-names></name>
</person-group><article-title>Deep Learning-Based Speech Enhancement With a Loss Trading Off the Speech Distortion and the Noise Residue for Cochlear Implants</article-title><source>Front. Med.</source><year>2021</year><volume>8</volume><elocation-id>740123</elocation-id><pub-id pub-id-type="doi">10.3389/fmed.2021.740123</pub-id></element-citation></ref><ref id="B55-audiolres-15-00056"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pavelchek</surname><given-names>C.</given-names></name>
<name><surname>Michelson</surname><given-names>A.P.</given-names></name>
<name><surname>Walia</surname><given-names>A.</given-names></name>
<name><surname>Ortmann</surname><given-names>A.</given-names></name>
<name><surname>Herzog</surname><given-names>J.</given-names></name>
<name><surname>Buchman</surname><given-names>C.A.</given-names></name>
<name><surname>Shew</surname><given-names>M.A.</given-names></name>
</person-group><article-title>Imputation of missing values for cochlear implant candidate audiometric data and potential applications</article-title><source>PLoS ONE</source><year>2023</year><volume>18</volume><elocation-id>e0281337</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0281337</pub-id><pub-id pub-id-type="pmid">36745652</pub-id>
</element-citation></ref><ref id="B56-audiolres-15-00056"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ashihara</surname><given-names>T.</given-names></name>
<name><surname>Furukawa</surname><given-names>S.</given-names></name>
<name><surname>Kashino</surname><given-names>M.</given-names></name>
</person-group><article-title>Estimating Pitch Information From Simulated Cochlear Implant Signals With Deep Neural Networks</article-title><source>Trends Hear.</source><year>2024</year><volume>28</volume><fpage>23312165241298606</fpage><pub-id pub-id-type="doi">10.1177/23312165241298606</pub-id><pub-id pub-id-type="pmid">39569552</pub-id>
</element-citation></ref><ref id="B57-audiolres-15-00056"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Prentiss</surname><given-names>S.</given-names></name>
<name><surname>Snapp</surname><given-names>H.</given-names></name>
<name><surname>Zwolan</surname><given-names>T.</given-names></name>
</person-group><article-title>Audiology Practices in the Preoperative Evaluation and Management of Adult Cochlear Implant Candidates</article-title><source>JAMA Otolaryngol.&#x02013;Head Neck Surg.</source><year>2020</year><volume>146</volume><fpage>136</fpage><pub-id pub-id-type="doi">10.1001/jamaoto.2019.3760</pub-id><pub-id pub-id-type="pmid">31830215</pub-id>
</element-citation></ref><ref id="B58-audiolres-15-00056"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Holder</surname><given-names>J.T.</given-names></name>
<name><surname>Reynolds</surname><given-names>S.M.</given-names></name>
<name><surname>Sunderhaus</surname><given-names>L.W.</given-names></name>
<name><surname>Gifford</surname><given-names>R.H.</given-names></name>
</person-group><article-title>Current Profile of Adults Presenting for Preoperative Cochlear Implant Evaluation</article-title><source>Trends Hear.</source><year>2018</year><volume>22</volume><fpage>2331216518755288</fpage><pub-id pub-id-type="doi">10.1177/2331216518755288</pub-id><pub-id pub-id-type="pmid">29441835</pub-id>
</element-citation></ref><ref id="B59-audiolres-15-00056"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sennaroglu</surname><given-names>L.</given-names></name>
<name><surname>Saatci</surname><given-names>I.</given-names></name>
<name><surname>Aralasmak</surname><given-names>A.</given-names></name>
<name><surname>Gursel</surname><given-names>B.</given-names></name>
<name><surname>Turan</surname><given-names>E.</given-names></name>
</person-group><article-title>Magnetic resonance imaging versus computed tomography in pre-operative evaluation of cochlear implant candidates with congenital hearing loss</article-title><source>J. Laryngol. Otol.</source><year>2002</year><volume>116</volume><fpage>804</fpage><lpage>810</lpage><pub-id pub-id-type="doi">10.1258/00222150260293619</pub-id><pub-id pub-id-type="pmid">12437835</pub-id>
</element-citation></ref><ref id="B60-audiolres-15-00056"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Verschuur</surname><given-names>C.</given-names></name>
<name><surname>Hellier</surname><given-names>W.</given-names></name>
<name><surname>Teo</surname><given-names>C.</given-names></name>
</person-group><article-title>An evaluation of hearing preservation outcomes in routine cochlear implant care: Implications for candidacy</article-title><source>Cochlear Implants Int.</source><year>2016</year><volume>17</volume><issue>(Suppl. 1)</issue><fpage>62</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1080/14670100.2016.1152007</pub-id><pub-id pub-id-type="pmid">27099115</pub-id>
</element-citation></ref><ref id="B61-audiolres-15-00056"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cinar</surname><given-names>B.C.</given-names></name>
<name><surname>&#x000d6;zses</surname><given-names>M.</given-names></name>
</person-group><article-title>How differ eCAP types in cochlear implants users with and without inner ear malformations: Amplitude growth function, spread of excitation, refractory recovery function</article-title><source>Eur. Arch. Oto-Rhino-Laryngol.</source><year>2025</year><volume>282</volume><fpage>731</fpage><lpage>742</lpage><pub-id pub-id-type="doi">10.1007/s00405-024-08971-9</pub-id><pub-id pub-id-type="pmid">39284939</pub-id>
</element-citation></ref><ref id="B62-audiolres-15-00056"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Patro</surname><given-names>A.</given-names></name>
<name><surname>Perkins</surname><given-names>E.L.</given-names></name>
<name><surname>Ortega</surname><given-names>C.A.</given-names></name>
<name><surname>Lindquist</surname><given-names>N.R.</given-names></name>
<name><surname>Dawant</surname><given-names>B.M.</given-names></name>
<name><surname>Gifford</surname><given-names>R.</given-names></name>
<name><surname>Haynes</surname><given-names>D.S.</given-names></name>
<name><surname>Chowdhury</surname><given-names>N.</given-names></name>
</person-group><article-title>Machine Learning Approach for Screening Cochlear Implant Candidates: Comparing with the 60/60 Guideline</article-title><source>Otol. Neurotol.</source><year>2023</year><volume>44</volume><fpage>E486</fpage><lpage>E491</lpage><pub-id pub-id-type="doi">10.1097/MAO.0000000000003927</pub-id><pub-id pub-id-type="pmid">37400135</pub-id>
</element-citation></ref><ref id="B63-audiolres-15-00056"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zeitler</surname><given-names>D.M.</given-names></name>
<name><surname>Buchlak</surname><given-names>Q.D.</given-names></name>
<name><surname>Ramasundara</surname><given-names>S.</given-names></name>
<name><surname>Farrokhi</surname><given-names>F.</given-names></name>
<name><surname>Esmaili</surname><given-names>N.</given-names></name>
</person-group><article-title>Predicting Acoustic Hearing Preservation Following Cochlear Implant Surgery Using Machine Learning</article-title><source>Laryngoscope</source><year>2024</year><volume>134</volume><fpage>926</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.1002/lary.30894</pub-id><pub-id pub-id-type="pmid">37449725</pub-id>
</element-citation></ref><ref id="B64-audiolres-15-00056"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Carlson</surname><given-names>M.L.</given-names></name>
<name><surname>Carducci</surname><given-names>V.</given-names></name>
<name><surname>Deep</surname><given-names>N.L.</given-names></name>
<name><surname>DeJong</surname><given-names>M.D.</given-names></name>
<name><surname>Poling</surname><given-names>G.L.</given-names></name>
<name><surname>Brufau</surname><given-names>S.R.</given-names></name>
</person-group><article-title>AI model for predicting adult cochlear implant candidacy using routine behavioral audiometry</article-title><source>Am. J. Otolaryngol.&#x02014;Head Neck Med. Surg.</source><year>2024</year><volume>45</volume><fpage>104337</fpage><pub-id pub-id-type="doi">10.1016/j.amjoto.2024.104337</pub-id></element-citation></ref><ref id="B65-audiolres-15-00056"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>&#x00130;kiz Bozsoy</surname><given-names>M.</given-names></name>
<name><surname>Parlak Kocabay</surname><given-names>A.</given-names></name>
<name><surname>Koska</surname><given-names>B.</given-names></name>
<name><surname>Demirta&#x0015f; Y&#x00131;lmaz</surname><given-names>B.</given-names></name>
<name><surname>&#x000d6;zses</surname><given-names>M.</given-names></name>
<name><surname>Avc&#x00131;</surname><given-names>N.B.</given-names></name>
<name><surname>Akkaplan</surname><given-names>S.</given-names></name>
<name><surname>Ate&#x0015f;</surname><given-names>Z.B.</given-names></name>
<name><surname>&#x000c7;&#x00131;nar</surname><given-names>B.&#x000c7;.</given-names></name>
<name><surname>Yaral&#x00131;</surname><given-names>M.</given-names></name>
<etal/>
</person-group><article-title>Intraoperative impedance and ECAP results in cochlear implant recipients with inner ear malformations and normal cochlear anatomy: A retrospective analysis</article-title><source>Acta Otolaryngol.</source><year>2025</year><volume>145</volume><fpage>222</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1080/00016489.2025.2452346</pub-id></element-citation></ref><ref id="B66-audiolres-15-00056"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Y&#x00131;ld&#x00131;r&#x00131;m G&#x000f6;kay</surname><given-names>N.</given-names></name>
<name><surname>Demirta&#x0015f;</surname><given-names>B.</given-names></name>
<name><surname>&#x000d6;zbal Batuk</surname><given-names>M.</given-names></name>
<name><surname>Y&#x000fc;cel</surname><given-names>E.</given-names></name>
<name><surname>Sennaro&#x0011f;lu</surname><given-names>G.</given-names></name>
</person-group><article-title>Auditory performance and language skills in children with auditory brainstem implants and cochlear implants</article-title><source>Eur. Arch. Oto-Rhino-Laryngol.</source><year>2024</year><volume>281</volume><fpage>4153</fpage><lpage>4159</lpage><pub-id pub-id-type="doi">10.1007/s00405-024-08594-0</pub-id><pub-id pub-id-type="pmid">38573512</pub-id>
</element-citation></ref><ref id="B67-audiolres-15-00056"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Budak</surname><given-names>Z.</given-names></name>
<name><surname>Batuk</surname><given-names>M.O.</given-names></name>
<name><surname>D&#x02019;Alessandro</surname><given-names>H.D.</given-names></name>
<name><surname>Sennaroglu</surname><given-names>G.</given-names></name>
</person-group><article-title>Hearing-related quality of life assessment of pediatric cochlear implant users with inner ear malformations</article-title><source>Int. J. Pediatr. Otorhinolaryngol.</source><year>2022</year><volume>160</volume><fpage>111243</fpage><pub-id pub-id-type="doi">10.1016/j.ijporl.2022.111243</pub-id><pub-id pub-id-type="pmid">35853403</pub-id>
</element-citation></ref><ref id="B68-audiolres-15-00056"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hawthorne</surname><given-names>G.</given-names></name>
<name><surname>Hogan</surname><given-names>A.</given-names></name>
<name><surname>Giles</surname><given-names>E.</given-names></name>
<name><surname>Stewart</surname><given-names>M.</given-names></name>
<name><surname>Kethel</surname><given-names>L.</given-names></name>
<name><surname>White</surname><given-names>K.</given-names></name>
<name><surname>Plaith</surname><given-names>B.</given-names></name>
<name><surname>Pedley</surname><given-names>K.</given-names></name>
<name><surname>Rushbrooke</surname><given-names>E.</given-names></name>
<name><surname>Taylor</surname><given-names>A.</given-names></name>
</person-group><article-title>Evaluating the health-related quality of life effects of cochlear implants: A prospective study of an adult cochlear implant program</article-title><source>Int. J. Audiol.</source><year>2004</year><volume>43</volume><fpage>183</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1080/14992020400050026</pub-id><pub-id pub-id-type="pmid">15250122</pub-id>
</element-citation></ref><ref id="B69-audiolres-15-00056"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vaerenberg</surname><given-names>B.</given-names></name>
<name><surname>Smits</surname><given-names>C.</given-names></name>
<name><surname>De Ceulaer</surname><given-names>G.</given-names></name>
<name><surname>Zir</surname><given-names>E.</given-names></name>
<name><surname>Harman</surname><given-names>S.</given-names></name>
<name><surname>Jaspers</surname><given-names>N.</given-names></name>
<name><surname>D&#x02019;Hondt</surname><given-names>C.</given-names></name>
<name><surname>Frijns</surname><given-names>J.H.M.</given-names></name>
<name><surname>De Beukelaer</surname><given-names>C.</given-names></name>
<name><surname>Govaerts</surname><given-names>P.J.</given-names></name>
</person-group><article-title>Cochlear implant programming: A global survey on the state of the art</article-title><source>Sci. World J.</source><year>2014</year><volume>2014</volume><fpage>501738</fpage><pub-id pub-id-type="doi">10.1155/2014/501738</pub-id><pub-id pub-id-type="pmid">24688394</pub-id>
</element-citation></ref><ref id="B70-audiolres-15-00056"><label>70.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Wolfe</surname><given-names>J.</given-names></name>
<name><surname>Schafer</surname><given-names>E.C.</given-names></name>
</person-group><source>Programming Cochlear Implants</source><edition>2nd ed.</edition><publisher-name>Plural Publishing</publisher-name><publisher-loc>San Diego, CA, USA</publisher-loc><year>2015</year><size units="pages">408p</size></element-citation></ref><ref id="B71-audiolres-15-00056"><label>71.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Thangavelu</surname><given-names>K.</given-names></name>
<name><surname>Nitzge</surname><given-names>M.</given-names></name>
<name><surname>Wei&#x000df;</surname><given-names>R.M.</given-names></name>
<name><surname>Mueller-Mazzotta</surname><given-names>J.</given-names></name>
<name><surname>Stuck</surname><given-names>B.A.</given-names></name>
<name><surname>Reimann</surname><given-names>K.</given-names></name>
</person-group><article-title>Role of cochlear reserve in adults with cochlear implants following post-lingual hearing loss</article-title><source>Eur. Arch. Oto-Rhino-Laryngol.</source><year>2023</year><volume>280</volume><fpage>1063</fpage><lpage>1071</lpage><pub-id pub-id-type="doi">10.1007/s00405-022-07558-6</pub-id></element-citation></ref><ref id="B72-audiolres-15-00056"><label>72.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sawaf</surname><given-names>T.</given-names></name>
<name><surname>Vovos</surname><given-names>R.</given-names></name>
<name><surname>Hadford</surname><given-names>S.</given-names></name>
<name><surname>Woodson</surname><given-names>E.</given-names></name>
<name><surname>Anne</surname><given-names>S.</given-names></name>
</person-group><article-title>Utility of intraoperative neural response telemetry in pediatric cochlear implants</article-title><source>Int. J. Pediatr. Otorhinolaryngol.</source><year>2022</year><volume>162</volume><fpage>111298</fpage><pub-id pub-id-type="doi">10.1016/j.ijporl.2022.111298</pub-id><pub-id pub-id-type="pmid">36088734</pub-id>
</element-citation></ref><ref id="B73-audiolres-15-00056"><label>73.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Demirta&#x0015f;</surname><given-names>B.</given-names></name>
<name><surname>&#x000d6;zbal Batuk</surname><given-names>M.</given-names></name>
<name><surname>Din&#x000e7;er D&#x02019;Alessandro</surname><given-names>H.</given-names></name>
<name><surname>Sennaro&#x0011f;lu</surname><given-names>G.</given-names></name>
</person-group><article-title>The Audiological Profile and Rehabilitation of Patients with Incomplete Partition Type II and Large Vestibular Aqueducts</article-title><source>J. Int. Adv. Otol.</source><year>2024</year><volume>20</volume><fpage>196</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.5152/iao.2024.231372</pub-id><pub-id pub-id-type="pmid">39128043</pub-id>
</element-citation></ref><ref id="B74-audiolres-15-00056"><label>74.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schraivogel</surname><given-names>S.</given-names></name>
<name><surname>Weder</surname><given-names>S.</given-names></name>
<name><surname>Mantokoudis</surname><given-names>G.</given-names></name>
<name><surname>Caversaccio</surname><given-names>M.</given-names></name>
<name><surname>Wimmer</surname><given-names>W.</given-names></name>
</person-group><article-title>Predictive Models for Radiation-Free Localization of Cochlear Implants&#x02019; Most Basal Electrode using Impedance Telemetry</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2024</year><volume>72</volume><fpage>1453</fpage><lpage>1464</lpage><pub-id pub-id-type="doi">10.1109/TBME.2024.3509527</pub-id></element-citation></ref><ref id="B75-audiolres-15-00056"><label>75.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Skidmore</surname><given-names>J.</given-names></name>
<name><surname>Xu</surname><given-names>L.</given-names></name>
<name><surname>Chao</surname><given-names>X.</given-names></name>
<name><surname>Riggs</surname><given-names>W.J.</given-names></name>
<name><surname>Pellittieri</surname><given-names>A.</given-names></name>
<name><surname>Vaughan</surname><given-names>C.</given-names></name>
<name><surname>Ning</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>R.</given-names></name>
<name><surname>Luo</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>S.</given-names></name>
</person-group><article-title>Prediction of the Functional Status of the Cochlear Nerve in Individual Cochlear Implant Users Using Machine Learning and Electrophysiological Measures</article-title><source>Ear Hear.</source><year>2021</year><volume>42</volume><fpage>180</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000916</pub-id><pub-id pub-id-type="pmid">32826505</pub-id>
</element-citation></ref><ref id="B76-audiolres-15-00056"><label>76.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Calmels</surname><given-names>M.N.</given-names></name>
<name><surname>Saliba</surname><given-names>I.</given-names></name>
<name><surname>Wanna</surname><given-names>G.</given-names></name>
<name><surname>Cochard</surname><given-names>N.</given-names></name>
<name><surname>Fillaux</surname><given-names>J.</given-names></name>
<name><surname>Deguine</surname><given-names>O.</given-names></name>
<name><surname>Fraysse</surname><given-names>B.</given-names></name>
</person-group><article-title>Speech perception and speech intelligibility in children after cochlear implantation</article-title><source>Int. J. Pediatr. Otorhinolaryngol.</source><year>2004</year><volume>68</volume><fpage>347</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1016/j.ijporl.2003.11.006</pub-id><pub-id pub-id-type="pmid">15129946</pub-id>
</element-citation></ref><ref id="B77-audiolres-15-00056"><label>77.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dillon</surname><given-names>M.T.</given-names></name>
<name><surname>Buss</surname><given-names>E.</given-names></name>
<name><surname>Adunka</surname><given-names>M.C.</given-names></name>
<name><surname>King</surname><given-names>E.R.</given-names></name>
<name><surname>Pillsbury</surname><given-names>H.C.</given-names></name>
<name><surname>Adunka</surname><given-names>O.F.</given-names></name>
<name><surname>Buchman</surname><given-names>C.A.</given-names></name>
</person-group><article-title>Long-term Speech Perception in Elderly Cochlear Implant Users</article-title><source>JAMA Otolaryngol.&#x02013;Head Neck Surg.</source><year>2013</year><volume>139</volume><fpage>279</fpage><pub-id pub-id-type="doi">10.1001/jamaoto.2013.1814</pub-id><pub-id pub-id-type="pmid">23657352</pub-id>
</element-citation></ref><ref id="B78-audiolres-15-00056"><label>78.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yildirim G&#x000f6;kay</surname><given-names>N.</given-names></name>
<name><surname>G&#x000fc;nd&#x000fc;z</surname><given-names>B.</given-names></name>
<name><surname>Karamert</surname><given-names>R.</given-names></name>
<name><surname>Tutar</surname><given-names>H.</given-names></name>
</person-group><article-title>Postoperative Auditory Progress in Cochlear-Implanted Children with Auditory Neuropathy</article-title><source>Am. J. Audiol.</source><year>2025</year><volume>34</volume><fpage>29</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1044/2024_AJA-24-00168</pub-id><pub-id pub-id-type="pmid">39509703</pub-id>
</element-citation></ref><ref id="B79-audiolres-15-00056"><label>79.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Y&#x00131;ld&#x00131;r&#x00131;m G&#x000f6;kay</surname><given-names>N.</given-names></name>
<name><surname>Y&#x000fc;cel</surname><given-names>E.</given-names></name>
</person-group><article-title>Bilateral cochlear implantation: An assessment of language sub-skills and phoneme recognition in school-aged children</article-title><source>Eur. Arch. Oto-Rhino-Laryngol.</source><year>2021</year><volume>278</volume><fpage>2093</fpage><lpage>2100</lpage><pub-id pub-id-type="doi">10.1007/s00405-020-06493-8</pub-id><pub-id pub-id-type="pmid">33231756</pub-id>
</element-citation></ref><ref id="B80-audiolres-15-00056"><label>80.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gaj&#x00119;cki</surname><given-names>T.</given-names></name>
<name><surname>Nogueira</surname><given-names>W.</given-names></name>
</person-group><article-title>Deep learning models to remix music for cochlear implant users</article-title><source>J. Acoust. Soc. Am.</source><year>2018</year><volume>143</volume><fpage>3602</fpage><lpage>3615</lpage><pub-id pub-id-type="doi">10.1121/1.5042056</pub-id><pub-id pub-id-type="pmid">29960485</pub-id>
</element-citation></ref><ref id="B81-audiolres-15-00056"><label>81.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chang</surname><given-names>Y.J.</given-names></name>
<name><surname>Han</surname><given-names>J.Y.</given-names></name>
<name><surname>Chu</surname><given-names>W.C.</given-names></name>
<name><surname>Li</surname><given-names>L.P.H.</given-names></name>
<name><surname>Lai</surname><given-names>Y.H.</given-names></name>
</person-group><article-title>Enhancing music recognition using deep learning-powered source separation technology for cochlear implant users</article-title><source>J. Acoust. Soc. Am.</source><year>2024</year><volume>155</volume><fpage>1694</fpage><lpage>1703</lpage><pub-id pub-id-type="doi">10.1121/10.0025057</pub-id><pub-id pub-id-type="pmid">38426839</pub-id>
</element-citation></ref><ref id="B82-audiolres-15-00056"><label>82.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Y&#x000fc;ksel</surname><given-names>M.</given-names></name>
<name><surname>Ta&#x0015f;demir</surname><given-names>&#x00130;.</given-names></name>
<name><surname>&#x000c7;iprut</surname><given-names>A.</given-names></name>
</person-group><article-title>Listening Effort in Prelingual Cochlear Implant Recipients: Effects of Spectral and Temporal Auditory Processing and Contralateral Acoustic Hearing</article-title><source>Otol. Neurotol.</source><year>2022</year><volume>43</volume><fpage>e1077</fpage><lpage>e1084</lpage><pub-id pub-id-type="doi">10.1097/MAO.0000000000003690</pub-id><pub-id pub-id-type="pmid">36099588</pub-id>
</element-citation></ref><ref id="B83-audiolres-15-00056"><label>83.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Caldwell</surname><given-names>A.</given-names></name>
<name><surname>Nittrouer</surname><given-names>S.</given-names></name>
</person-group><article-title>Speech Perception in Noise by Children With Cochlear Implants</article-title><source>J. Speech Lang. Hear. Res.</source><year>2013</year><volume>56</volume><fpage>13</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1044/1092-4388(2012/11-0338)</pub-id><pub-id pub-id-type="pmid">22744138</pub-id>
</element-citation></ref><ref id="B84-audiolres-15-00056"><label>84.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dunn</surname><given-names>C.C.</given-names></name>
<name><surname>Noble</surname><given-names>W.</given-names></name>
<name><surname>Tyler</surname><given-names>R.S.</given-names></name>
<name><surname>Kordus</surname><given-names>M.</given-names></name>
<name><surname>Gantz</surname><given-names>B.J.</given-names></name>
<name><surname>Ji</surname><given-names>H.</given-names></name>
</person-group><article-title>Bilateral and Unilateral Cochlear Implant Users Compared on Speech Perception in Noise</article-title><source>Ear Hear.</source><year>2010</year><volume>31</volume><fpage>296</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e3181c12383</pub-id><pub-id pub-id-type="pmid">19858720</pub-id>
</element-citation></ref><ref id="B85-audiolres-15-00056"><label>85.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Oxenham</surname><given-names>A.J.</given-names></name>
<name><surname>Kreft</surname><given-names>H.A.</given-names></name>
</person-group><article-title>Speech Perception in Tones and Noise via Cochlear Implants Reveals Influence of Spectral Resolution on Temporal Processing</article-title><source>Trends Hear.</source><year>2014</year><volume>18</volume><fpage>2331216514553783</fpage><pub-id pub-id-type="doi">10.1177/2331216514553783</pub-id><pub-id pub-id-type="pmid">25315376</pub-id>
</element-citation></ref><ref id="B86-audiolres-15-00056"><label>86.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Srinivasan</surname><given-names>A.G.</given-names></name>
<name><surname>Padilla</surname><given-names>M.</given-names></name>
<name><surname>Shannon</surname><given-names>R.V.</given-names></name>
<name><surname>Landsberger</surname><given-names>D.M.</given-names></name>
</person-group><article-title>Improving speech perception in noise with current focusing in cochlear implant users</article-title><source>Hear Res.</source><year>2013</year><volume>299</volume><fpage>29</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2013.02.004</pub-id><pub-id pub-id-type="pmid">23467170</pub-id>
</element-citation></ref><ref id="B87-audiolres-15-00056"><label>87.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Atay</surname><given-names>G.</given-names></name>
<name><surname>Tellio&#x0011f;lu</surname><given-names>B.</given-names></name>
<name><surname>Tellio&#x0011f;lu</surname><given-names>H.T.</given-names></name>
<name><surname>Avc&#x00131;</surname><given-names>N.B.</given-names></name>
<name><surname>&#x000c7;&#x00131;nar</surname><given-names>B.&#x000c7;.</given-names></name>
<name><surname>&#x0015e;ekero&#x0011f;lu</surname><given-names>H.T.</given-names></name>
</person-group><article-title>Evaluation of auditory pathways and comorbid inner ear malformations in pediatric patients with Duane retraction syndrome</article-title><source>Int. J. Pediatr. Otorhinolaryngol.</source><year>2025</year><volume>188</volume><fpage>112207</fpage><pub-id pub-id-type="doi">10.1016/j.ijporl.2024.112207</pub-id><pub-id pub-id-type="pmid">39732049</pub-id>
</element-citation></ref><ref id="B88-audiolres-15-00056"><label>88.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gaultier</surname><given-names>C.</given-names></name>
<name><surname>Goehring</surname><given-names>T.</given-names></name>
</person-group><article-title>Recovering speech intelligibility with deep learning and multiple microphones in noisy-reverberant situations for people using cochlear implants</article-title><source>J. Acoust. Soc. Am.</source><year>2024</year><volume>155</volume><fpage>3833</fpage><lpage>3847</lpage><pub-id pub-id-type="doi">10.1121/10.0026218</pub-id><pub-id pub-id-type="pmid">38884525</pub-id>
</element-citation></ref><ref id="B89-audiolres-15-00056"><label>89.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ocak</surname><given-names>E.</given-names></name>
<name><surname>Kocaoz</surname><given-names>D.</given-names></name>
<name><surname>Acar</surname><given-names>B.</given-names></name>
<name><surname>Topcuoglu</surname><given-names>M.</given-names></name>
</person-group><article-title>Radiological Evaluation of Inner Ear with Computed Tomography in Patients with Unilateral Non-Pulsatile Tinnitus</article-title><source>J. Int. Adv. Otol.</source><year>2018</year><volume>14</volume><fpage>273</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.5152/iao.2017.3727</pub-id><pub-id pub-id-type="pmid">29283099</pub-id>
</element-citation></ref><ref id="B90-audiolres-15-00056"><label>90.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Paquette</surname><given-names>S.</given-names></name>
<name><surname>Gouin</surname><given-names>S.</given-names></name>
<name><surname>Lehmann</surname><given-names>A.</given-names></name>
</person-group><article-title>Improving emotion perception in cochlear implant users: Insights from machine learning analysis of EEG signals</article-title><source>BMC Neurol.</source><year>2024</year><volume>24</volume><elocation-id>115</elocation-id><pub-id pub-id-type="doi">10.1186/s12883-024-03616-0</pub-id><pub-id pub-id-type="pmid">38589815</pub-id>
</element-citation></ref><ref id="B91-audiolres-15-00056"><label>91.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Patro</surname><given-names>A.</given-names></name>
<name><surname>Lawrence</surname><given-names>P.J.</given-names></name>
<name><surname>Tamati</surname><given-names>T.N.</given-names></name>
<name><surname>Ning</surname><given-names>X.</given-names></name>
<name><surname>Moberly</surname><given-names>A.C.</given-names></name>
</person-group><article-title>Using Machine Learning and Multifaceted Preoperative Measures to Predict Adult Cochlear Implant Outcomes: A Prospective Pilot Study</article-title><source>Ear Hear.</source><year>2025</year><volume>46</volume><fpage>543</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000001593</pub-id></element-citation></ref><ref id="B92-audiolres-15-00056"><label>92.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Saeed</surname><given-names>H.S.</given-names></name>
<name><surname>Fergie</surname><given-names>M.</given-names></name>
<name><surname>Mey</surname><given-names>K.</given-names></name>
<name><surname>West</surname><given-names>N.</given-names></name>
<name><surname>Bille</surname><given-names>M.</given-names></name>
<name><surname>Caye-Thomasen</surname><given-names>P.</given-names></name>
<name><surname>Nash</surname><given-names>R.</given-names></name>
<name><surname>Saeed</surname><given-names>S.R.</given-names></name>
<name><surname>Stivaros</surname><given-names>S.M.</given-names></name>
<name><surname>Black</surname><given-names>G.</given-names></name>
<etal/>
</person-group><article-title>Enlarged Vestibular Aqueduct and Associated Inner Ear Malformations: Hearing Loss Prognostic Factors and Data Modeling from an International Cohort</article-title><source>J. Int. Adv. Otol.</source><year>2023</year><volume>19</volume><fpage>454</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.5152/iao.2023.231044</pub-id><pub-id pub-id-type="pmid">38088316</pub-id>
</element-citation></ref><ref id="B93-audiolres-15-00056"><label>93.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Patro</surname><given-names>A.</given-names></name>
<name><surname>Freeman</surname><given-names>M.H.</given-names></name>
<name><surname>Haynes</surname><given-names>D.S.</given-names></name>
</person-group><article-title>Machine Learning to Predict Adult Cochlear Implant Candidacy</article-title><source>Curr. Otorhinolaryngol. Rep.</source><year>2024</year><volume>12</volume><fpage>45</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1007/s40136-024-00511-7</pub-id></element-citation></ref><ref id="B94-audiolres-15-00056"><label>94.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Tang</surname><given-names>E.</given-names></name>
<name><surname>Ding</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Artificial Intelligence and the Future of Communication Sciences and Disorders: A Bibliometric and Visualization Analysis</article-title><source>J. Speech Lang. Hear. Res.</source><year>2024</year><volume>67</volume><fpage>4369</fpage><lpage>4390</lpage><pub-id pub-id-type="doi">10.1044/2024_JSLHR-24-00157</pub-id><pub-id pub-id-type="pmid">39418583</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="audiolres-15-00056-f001"><label>Figure 1</label><caption><p>PRISMA Flow Date.</p></caption><graphic xlink:href="audiolres-15-00056-g001" position="float"/></fig><fig position="float" id="audiolres-15-00056-f002"><label>Figure 2</label><caption><p>Changes in the number of articles by year. <bold>Footnote:</bold> Continuous lines indicate the current number of articles, while dashed lines indicate the number expected to reach in the future depending on the rate of increase.</p></caption><graphic xlink:href="audiolres-15-00056-g002" position="float"/></fig><fig position="float" id="audiolres-15-00056-f003"><label>Figure 3</label><caption><p>Data set characteristics.</p></caption><graphic xlink:href="audiolres-15-00056-g003" position="float"/></fig><table-wrap position="float" id="audiolres-15-00056-t001"><object-id pub-id-type="pii">audiolres-15-00056-t001_Table 1</object-id><label>Table 1</label><caption><p>Distribution of machine learning applications in the field of audiology by year.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Year </th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Articles </th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Featured Topics and Methods</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">2013&#x02013;2017</td><td align="center" valign="middle" rowspan="1" colspan="1">12</td><td align="center" valign="middle" rowspan="1" colspan="1">Early machine learning applications<break/>Early deep learning approaches, experimental data analysis</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2018&#x02013;2022</td><td align="center" valign="middle" rowspan="1" colspan="1">22</td><td align="center" valign="middle" rowspan="1" colspan="1">Predictive models, dataset optimizations<break/>Increasing use of &#x0201c;machine learning&#x0201d;, basic AI applications</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2023&#x02013;2025</td><td align="center" valign="middle" rowspan="1" colspan="1">25</td><td align="center" valign="middle" rowspan="1" colspan="1">Integration of multiple machine learning methods, innovative techniques<break/>Transition to broad-based deep learning applications, advanced algorithms</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="audiolres-15-00056-t002"><object-id pub-id-type="pii">audiolres-15-00056-t002_Table 2</object-id><label>Table 2</label><caption><p><bold>A Classification of Artificial Intelligence Methods Based on Algorithm Type, Application Domain, and Evaluation Metrics.</bold>&#x000a0;</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Algorithm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Using Field</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Evaluation Methods</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Regularization</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Performance Metrics</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>MAP</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10-fold CV</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>RVM</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Train-Test Split</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>GMM</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5-fold CV</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SVM</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Post-Op Speech Perception</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10-fold CV</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L1/L2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy, F1-score</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>ANN</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Electrode Design</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10-fold CV</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>DNN</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Train-Test Split</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout, L2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy, MSE</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>KNN</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Speech Perception</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5-fold CV</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Random Forest</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Electrophysiological Measurements</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10-fold CV</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy, ROC-AUC</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>LSTM</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10-fold CV</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy, F1-score</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CNN</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Electrode Placement</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Train-Test Split</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Batch Norm, Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>DDAE</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5-fold CV</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSE, Accuracy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>MLP</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Signal Processing</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Train-Test Split</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SepFormer</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10-fold CV</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>ELM</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Electrode Design</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Train-Test Split</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Bayesian LR</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Electrode Impedance</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bayesian prior</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSE, Accuracy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>GBM</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Candidacy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10-fold CV</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy, AUC</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>RNN</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5-fold CV</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td></tr></tbody></table></table-wrap><table-wrap position="float" id="audiolres-15-00056-t003"><object-id pub-id-type="pii">audiolres-15-00056-t003_Table 3</object-id><label>Table 3</label><caption><p>Accuracy ratings of machine learning techniques based on the relevant fields of study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy Ratio Range</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Field of Use</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Decision trees</td><td align="center" valign="middle" rowspan="1" colspan="1">91&#x02013;96%</td><td align="center" valign="middle" rowspan="1" colspan="1">Intra-Post-Op Measurement</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Maksimum A Posteriori [MAP]</td><td align="center" valign="middle" rowspan="1" colspan="1">91.7&#x02013;96.2%</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Relevance Vector Machine [RVM]</td><td align="center" valign="middle" rowspan="1" colspan="1">91.7&#x02013;96.2%</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Gaussian Mixture Model [GMM]</td><td align="center" valign="middle" rowspan="1" colspan="1">95.13&#x02013;97.79%</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Support Vector Machine [SVM]</td><td align="center" valign="middle" rowspan="1" colspan="1">76&#x02013;97.79%</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise, Post-Op Speech Perception</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Artificial Neural Networks [ANN]</td><td align="center" valign="middle" rowspan="1" colspan="1">65&#x02013;89%</td><td align="center" valign="middle" rowspan="1" colspan="1">Electrode Design, Speech Perception</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deep Neural Networks [DNN]</td><td align="center" valign="middle" rowspan="1" colspan="1">18.2&#x02013;44.4% [improved speech intelligibility]</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">K-Nearest Neighbors [KNN]</td><td align="center" valign="middle" rowspan="1" colspan="1">80.7&#x02013;96.52%</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech Perception-Quality of Life</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Random Forest</td><td align="center" valign="middle" rowspan="1" colspan="1">73.3&#x02013;96.2%</td><td align="center" valign="middle" rowspan="1" colspan="1">Electrophysiological Measurements,</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Linear Regression</td><td align="center" valign="middle" rowspan="1" colspan="1">78.9&#x02013;96.52%</td><td align="center" valign="middle" rowspan="1" colspan="1">Programming, Speech Detection</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LSTM [Long Short-Term Memory]</td><td align="center" valign="middle" rowspan="1" colspan="1">71.1&#x02013;82.9%</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Convolutional Neural Networks [CNN]</td><td align="center" valign="middle" rowspan="1" colspan="1">54&#x02013;99%</td><td align="center" valign="middle" rowspan="1" colspan="1">Electrode Placement, Speech Perception</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deep Denoising Autoencoder [DDAE]</td><td align="center" valign="middle" rowspan="1" colspan="1">46.8&#x02013;77%</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Multilayer Perceptron [MLP]</td><td align="center" valign="middle" rowspan="1" colspan="1">75&#x02013;80%</td><td align="center" valign="middle" rowspan="1" colspan="1">Music Perception/Signal Processing</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SepFormer [Separation Transformer]</td><td align="center" valign="middle" rowspan="1" colspan="1">59.5&#x02013;74.7%</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Extreme Learning Machine [ELM]</td><td align="center" valign="middle" rowspan="1" colspan="1">90&#x02013;99%</td><td align="center" valign="middle" rowspan="1" colspan="1">Electrode Design</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Bayesian Linear Regression</td><td align="center" valign="middle" rowspan="1" colspan="1">83&#x02013;99%</td><td align="center" valign="middle" rowspan="1" colspan="1">Electrode Impedance Prediction</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Gradient Boosting Machines [GBM]</td><td align="center" valign="middle" rowspan="1" colspan="1">87&#x02013;93%</td><td align="center" valign="middle" rowspan="1" colspan="1">Preoperative Candidacy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recurrent Neural Network [RNN]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.5&#x02013;74.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Speech in Noise</td></tr></tbody></table></table-wrap><table-wrap position="float" id="audiolres-15-00056-t004"><object-id pub-id-type="pii">audiolres-15-00056-t004_Table 4</object-id><label>Table 4</label><caption><p>Highlights and practices.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Period</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Highlights and Practices</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Machine Learning</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Areas of Use in Audiology</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Early Machine Learning Approaches</td><td align="center" valign="middle" rowspan="1" colspan="1">Use of basic machine learning methods such as basic decision trees and linear regression.</td><td align="center" valign="middle" rowspan="1" colspan="1">First experimental applications in the field of cochlear implant, especially post-op measurements</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Early Deep Learning Approaches, Experimental Data Analyses</td><td align="center" valign="middle" rowspan="1" colspan="1">The introduction of more complex models, such as artificial neural networks (ANN) and support vector machines (SVM).</td><td align="center" valign="middle" rowspan="1" colspan="1">Early studies to improve cochlear implant performance by analysing experimental data.</td></tr><tr><td rowspan="2" align="center" valign="middle" colspan="1">Predictive Models, Data Set Optimizations</td><td align="center" valign="middle" rowspan="1" colspan="1">Use of predictive models (e.g., Random Forest, Gradient Boosting)</td><td rowspan="2" align="center" valign="middle" colspan="1">Prediction models in areas such as language development and speech perception after cochlear implantation.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Data set optimizations and studies to improve model accuracy.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Rising Use of &#x02018;Machine Learning&#x02019;, Basic Artificial Intelligence Applications</td><td align="center" valign="middle" rowspan="1" colspan="1">Expansion of machine learning methods in areas such as CI programming, electrode design and speech in noise.</td><td align="center" valign="middle" rowspan="1" colspan="1">Using basic AI applications (e.g., FOX system) to improve speech intelligibility of cochlear implant users</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Increasing Emphasis on &#x02018;Artificial Intelligence&#x02019;, Model Comparisons</td><td align="center" valign="middle" rowspan="1" colspan="1">An increased emphasis on artificial intelligence (AI) and comparison of different models (SVM, ANN, Random Forest).</td><td align="center" valign="middle" rowspan="1" colspan="1">Using various AI models to predict hearing and speech performance after cochlear implant.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Integration of Multiple Machine Learning Methods, Innovative Techniques</td><td align="center" valign="middle" rowspan="1" colspan="1">Integrating multiple machine learning methods (e.g., LSTM, CNN, RNN).</td><td align="center" valign="middle" rowspan="1" colspan="1">Comprehension problems in noise with innovative techniques (e.g., deep learning-based noise reduction). Improving speech intelligibility of cochlear implant users.</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">The Transition to Comprehensive Deep Learning Approaches, Advanced Algorithms</td><td align="center" valign="middle" rowspan="1" colspan="1">Common use of deep learning models (e.g., Transformer, SepFormer) in CI.</td><td align="center" valign="middle" rowspan="1" colspan="1">Using deep learning models for the analysis of EEG signals and other biomedical data.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Working with advanced algorithms (e.g., Multi-Task Learning, Deep ACE).</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Improving cochlear implant users&#x02019; music perception and speech understanding in noise.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="audiolres-15-00056-t005"><object-id pub-id-type="pii">audiolres-15-00056-t005_Table 5</object-id><label>Table 5</label><caption><p>Analysing the studies in different fields in detail.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Authors</bold>
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Years</bold>
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Machine Learning Model</bold>
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Area of Use</bold>
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Number of Data</bold>
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Number of Participants</bold>
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Accuracy Rate</bold>
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Explanatory Statement</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">Desmond, J.M. et al. [<xref rid="B51-audiolres-15-00056" ref-type="bibr">51</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2013</td><td align="center" valign="middle" rowspan="1" colspan="1">Maksimum A Posteriori (MAP), Relevance Vector Machine</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" rowspan="1" colspan="1">simulation-</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">91.7&#x02013;96.2%</td><td align="center" valign="middle" rowspan="1" colspan="1">Machine learning models were able to distinguish echo from other types of noise. The algorithms showed durability against different room and cochlear implant parameters.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">Hazrati, O. et al. [<xref rid="B50-audiolres-15-00056" ref-type="bibr">50</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2014</td><td align="center" valign="middle" rowspan="1" colspan="1">Gaussian Mixture Model (GMM), support vector machine (SVM), Neural Network (NN)</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" rowspan="1" colspan="1">720</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1"> 95.13&#x02013;97.79%</td><td align="center" valign="middle" rowspan="1" colspan="1">SVM model in speech intelligibility (showed the highest success with 97.79% accuracy rate).</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">Saeedi, N.E. et al. [<xref rid="B48-audiolres-15-00056" ref-type="bibr">48</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2017</td><td align="center" valign="middle" rowspan="1" colspan="1">Artificial neural network&#x02014;ANN, Spiking Neural Network&#x02014;SNN</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech Perception</td><td align="center" valign="middle" rowspan="1" colspan="1">116</td><td align="center" valign="middle" rowspan="1" colspan="1">29</td><td align="center" valign="middle" rowspan="1" colspan="1">65&#x02013;89%</td><td align="center" valign="middle" rowspan="1" colspan="1">Artificial neural network (ANN) has shown the best pitch ranking success when it uses spatial and temporal information together.Models using only spatial or temporal codes have lower performance.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">Chu, K. et al. [<xref rid="B46-audiolres-15-00056" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">Relevance Vector Machine (RVM) </td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">10% improvement in reverberant environments, deterioration when noise and reverberation are combined</td><td align="center" valign="middle" rowspan="1" colspan="1">Partially successful machine learning applications</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">Lai, Y.H. et al. [<xref rid="B40-audiolres-15-00056" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">Deep Learning/NC + DDAE (Noise Classifier + Deep Denoising Autoencoder)</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" rowspan="1" colspan="1">320</td><td align="center" valign="middle" rowspan="1" colspan="1">9</td><td align="center" valign="middle" rowspan="1" colspan="1">Noise classification success rate 99.6% noise reduction: 67.3%</td><td align="center" valign="middle" rowspan="1" colspan="1">NC + DDAE gives at least 2 times, sometimes up to 4 times, better results compared to classical noise reduction methods</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">Hajiaghababa, F. et al. [<xref rid="B49-audiolres-15-00056" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">Wavelet Neural Networks (WNNs), Infinite Impulse Response Filter Banks (IIR FBs), Dual Resonance Nonlinear (DRNL), Simple Dual Path Nonlinear (SDPN)</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech Intelligibility</td><td align="center" valign="middle" rowspan="1" colspan="1">120</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">Wavelet Neural Networks (WNNs) showed the highest performance in both test and training sets.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">Waltzman, S.B. &#x00026; Kelsall, D.C. [<xref rid="B38-audiolres-15-00056" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">FOX</td><td align="center" valign="middle" rowspan="1" colspan="1">Electrophysiological-Programming-Speech Perception</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">55</td><td align="center" valign="middle" rowspan="1" colspan="1">No statistically significant difference between manual programming and fox <italic toggle="yes">p</italic> = 0.65, and 0.47</td><td align="center" valign="middle" rowspan="1" colspan="1">With FOX, standardised rehabilitation, equal performance and improved patient experience have been found.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kang, Y. et al. [<xref rid="B54-audiolres-15-00056" ref-type="bibr">54</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LSTM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Deep learning based machine learning method for voice enhancement for speech understanding in noise</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">9</td><td align="center" valign="middle" rowspan="1" colspan="1">Hafeez, N. et al. [<xref rid="B39-audiolres-15-00056" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1"> Support vector machine (SVM), Shallow Neural Network (SNN), k-Nearest Neighbors (KNN)</td><td align="center" valign="middle" rowspan="1" colspan="1">Electrode Insertion Depth-Intra Op</td><td align="center" valign="middle" rowspan="1" colspan="1">137</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1"> 86.1&#x02013;97.1%</td><td align="center" valign="middle" rowspan="1" colspan="1">Highly accurate classification of EA using different insertion measurements during the electrode array placement process</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">Gajecki, T. et al. [<xref rid="B42-audiolres-15-00056" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">Deep Neural Networks&#x02014;DNN, Deep ACE, Fully-Convolutional Time-Domain Audio Separation Network, Adam, Binary Cross-Entropy</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">SRT speech discrimination 63.1</td><td align="center" valign="middle" rowspan="1" colspan="1">The best model for noise reduction: Deep ACE</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">11</td><td align="center" valign="middle" rowspan="1" colspan="1">Pavelchek, C. et al. [<xref rid="B55-audiolres-15-00056" ref-type="bibr">55</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">Univariate Imputation (UI)Interpolation (INT), Multiple Imputation by Chained Equations (MICE), k-Nearest Neighbors (KNN), Gradient Boosted Trees (XGB), Neural Networks (NN)</td><td align="center" valign="middle" rowspan="1" colspan="1">Cochlear implant candidacy-Behavioural Tests</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">1304</td><td align="center" valign="middle" rowspan="1" colspan="1">93%</td><td align="center" valign="middle" rowspan="1" colspan="1">In real-world hearing tests, it has been shown that missing data can be safely filled in. In particular, RMSE = 7.83 dB was achieved, below the clinically significant error threshold of 10 dB.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">12</td><td align="center" valign="middle" rowspan="1" colspan="1">de Nobel, J. et al. [<xref rid="B43-audiolres-15-00056" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">Convolutional Neural Network (CNN), Evolutionary Algorithm (EA), Polynomial Elastic Net (PEN), Random Forest (RF), Gradient Boosting (GB), Multilayer Perceptron (MLP)</td><td align="center" valign="middle" rowspan="1" colspan="1">1,466,189 simulation samples, 12,441,600 different excitation waveforms</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">Accuracy&#x02014;54&#x02013;99%</td><td align="center" valign="middle" rowspan="1" colspan="1">The energy savings of these new waveforms may contribute to longer operation of CI devices with smaller batteries.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">13</td><td align="center" valign="middle" rowspan="1" colspan="1">Zheng, Q. et al. [<xref rid="B52-audiolres-15-00056" ref-type="bibr">52</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">SVM&#x02014;Support vector machine-EEMD-ICA</td><td align="center" valign="middle" rowspan="1" colspan="1">EEG-Optimisation</td><td align="center" valign="middle" rowspan="1" colspan="1">8448</td><td align="center" valign="middle" rowspan="1" colspan="1">91</td><td align="center" valign="middle" rowspan="1" colspan="1">95.44%</td><td align="center" valign="middle" rowspan="1" colspan="1">The SVM-based classification algorithm achieved 95.44% accuracy in automatically identifying channels containing cochlear implant artifacts.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">14</td><td align="center" valign="middle" rowspan="1" colspan="1">Gajecki, T. &#x00026; Nogueira, W. [<xref rid="B41-audiolres-15-00056" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">Bilateral ACE, Bilateral Deep ACE, Fused Deep ACE</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech in Noise</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">168</td><td align="center" valign="middle" rowspan="1" colspan="1">Speech intelligibility: 45&#x02013;82%, noise reduction: 72&#x02013;90%</td><td align="center" valign="middle" rowspan="1" colspan="1">Fused deep ACE model is the most successful model, 20&#x02013;30% better speech understanding</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ashihara, T. et al. [<xref rid="B56-audiolres-15-00056" ref-type="bibr">56</xref>] </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Deep Neural Network (DNN) </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Speech Perception</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Machine learning to improve speech perception in cochlear implant users</td></tr></tbody></table></table-wrap></floats-group></article>