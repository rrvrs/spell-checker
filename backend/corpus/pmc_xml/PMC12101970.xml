<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Nat Mach Intell</journal-id><journal-id journal-id-type="iso-abbrev">Nat Mach Intell</journal-id><journal-title-group><journal-title>Nature Machine Intelligence</journal-title></journal-title-group><issn pub-type="ppub">2522-5839</issn><issn pub-type="epub">2522-5839</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC12101970</article-id><article-id pub-id-type="publisher-id">1035</article-id><article-id pub-id-type="doi">10.1038/s42256-025-01035-5</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A personalized time-resolved 3D mesh generative model for unveiling normal heart dynamics</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5157-1079</contrib-id><name><surname>Qiao</surname><given-names>Mengyun</given-names></name><address><email>m.qiao21@imperial.ac.uk</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>McGurk</surname><given-names>Kathryn A.</given-names></name><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Shuo</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1619-8328</contrib-id><name><surname>Matthews</surname><given-names>Paul M.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff6">6</xref><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author"><name><surname>O&#x02019;Regan</surname><given-names>Declan P.</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2943-7698</contrib-id><name><surname>Bai</surname><given-names>Wenjia</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff8">8</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/041kmwe10</institution-id><institution-id institution-id-type="GRID">grid.7445.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 2113 8111</institution-id><institution>Department of Brain Sciences, </institution><institution>Imperial College London, </institution></institution-wrap>London, UK </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/041kmwe10</institution-id><institution-id institution-id-type="GRID">grid.7445.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 2113 8111</institution-id><institution>Data Science Institute, </institution><institution>Imperial College London, </institution></institution-wrap>London, UK </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/041kmwe10</institution-id><institution-id institution-id-type="GRID">grid.7445.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 2113 8111</institution-id><institution>MRC Laboratory of Medical Sciences, </institution><institution>Imperial College London, </institution></institution-wrap>London, UK </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/041kmwe10</institution-id><institution-id institution-id-type="GRID">grid.7445.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 2113 8111</institution-id><institution>National Heart and Lung Institute, </institution><institution>Imperial College London, </institution></institution-wrap>London, UK </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/013q1eq08</institution-id><institution-id institution-id-type="GRID">grid.8547.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 0125 2443</institution-id><institution>Digital Medical Research Center, School of Basic Medical Sciences, </institution><institution>Fudan University and Shanghai Key Laboratory of MICCAI, </institution></institution-wrap>Shanghai, China </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/041kmwe10</institution-id><institution-id institution-id-type="GRID">grid.7445.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 2113 8111</institution-id><institution>UK Dementia Research Institute, </institution><institution>Imperial College London, </institution></institution-wrap>London, UK </aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01djcs087</institution-id><institution-id institution-id-type="GRID">grid.507854.b</institution-id><institution>Rosalind Franklin Institute, </institution><institution>Harwell Science and Innovation Campus, </institution></institution-wrap>Didcot, UK </aff><aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/041kmwe10</institution-id><institution-id institution-id-type="GRID">grid.7445.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 2113 8111</institution-id><institution>Department of Computing, </institution><institution>Imperial College London, </institution></institution-wrap>London, UK </aff></contrib-group><pub-date pub-type="epub"><day>19</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>19</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="ppub"><year>2025</year></pub-date><volume>7</volume><issue>5</issue><fpage>800</fpage><lpage>811</lpage><history><date date-type="received"><day>20</day><month>9</month><year>2024</year></date><date date-type="accepted"><day>7</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Understanding the structure and motion of the heart is crucial for diagnosing and managing cardiovascular diseases, the leading cause of global death. There is wide variation in cardiac shape and motion patterns, influenced by demographic, anthropometric and disease factors. Unravelling normal patterns of shape and motion, and understanding how each individual deviates from the norm, would facilitate accurate diagnosis and personalized treatment strategies. Here, to this end, we developed a conditional generative model, MeshHeart, to learn the distribution of shape and motion patterns for the left and right ventricles of the heart. To model the high-dimensional spatio-temporal mesh data, MeshHeart uses a geometric encoder to represent cardiac meshes in a latent space and a temporal transformer to model the motion dynamics of latent representations. Based on MeshHeart, we investigate the latent space of 3D + t cardiac mesh sequences and propose a distance metric, latent delta, which quantifies the deviation of a real heart from its personalized normative pattern. Here, 3D + t refers to three-dimensional data evolving over time. In experiments using a large cardiac magnetic resonance image dataset of 38,309 participants from the UK Biobank, MeshHeart demonstrates high performance in cardiac mesh sequence reconstruction and generation. Latent space features are discriminative for cardiac disease classification, whereas latent delta exhibits strong correlations with clinical phenotypes in phenome-wide association studies.</p></abstract><abstract id="Abs2" abstract-type="web-summary"><p id="Par2">MeshHeart, a conditional generative model for time-resolved 3D heart mesh generation, is proposed by Qiao et al. to unravel heart motion patterns. Their findings could advance diagnosis and treatment strategies for cardiovascular diseases.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Cardiovascular diseases</kwd><kwd>Magnetic resonance imaging</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100000266</institution-id><institution>RCUK | Engineering and Physical Sciences Research Council (EPSRC)</institution></institution-wrap></funding-source><award-id>EP/W01842X/1</award-id><award-id>EP/W01842X/1</award-id><principal-award-recipient><name><surname>Qiao</surname><given-names>Mengyun</given-names></name><name><surname>Bai</surname><given-names>Wenjia</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100000274</institution-id><institution>British Heart Foundation (BHF)</institution></institution-wrap></funding-source><award-id>FS/IPBSRF/22/27059</award-id><award-id>RE/18/4/34215</award-id><award-id>RG/19/6/34387, RE/24/130023, CH/P/23/80008</award-id><principal-award-recipient><name><surname>McGurk</surname><given-names>Kathryn A.</given-names></name><name><surname>O&#x02019;Regan</surname><given-names>Declan P.</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100009962</institution-id><institution>Shanghai International Science and Technology</institution></institution-wrap></funding-source><award-id>23410710400</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Shuo</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100000320</institution-id><institution>Alzheimer's Society</institution></institution-wrap></funding-source></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100000265</institution-id><institution>RCUK | Medical Research Council (MRC)</institution></institution-wrap></funding-source><award-id>MC_UP_1605/13</award-id><principal-award-recipient><name><surname>O&#x02019;Regan</surname><given-names>Declan P.</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Main</title><p id="Par3">The heart is one of the most important and vital organs within the human body<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. It is composed of four morphologically distinct chambers that function in a coordinated manner. The shape of the heart is governed by genetic and environmental factors<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR3">3</xref></sup>, as well as a remodelling process observed in response to myocardial infarction, pressure overload and cardiac diseases<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>. The motion of the heart follows a periodic nonlinear pattern modulated by the underlying molecular, electrophysiological and biophysical processes<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Unveiling the complex patterns of cardiac shape and motion will provide important insights for assessing the status of cardiac health in both clinical diagnosis and cardiovascular research<sup><xref ref-type="bibr" rid="CR7">7</xref>&#x02013;<xref ref-type="bibr" rid="CR10">10</xref></sup>.</p><p id="Par4">The current state of the art for assessing cardiac shape and motion is to perform analyses of cardiac images, for example, cardiac magnetic resonance (MR) images, and extract imaging-derived phenotypes of cardiac chambers<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. Most imaging phenotypes, such as chamber volumes or ejection fractions, provide a global and simplistic measure of the complex three-dimensional (3D)&#x02013;temporal (3D + t) geometry of cardiac chambers<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>. However, these global volumetric measures may not fully capture the dynamics and variations of cardiac function across individuals. Recent studies have shown that mesh-based cardiac shape and motion analyses can provide more detailed and clinically relevant insights<sup><xref ref-type="bibr" rid="CR13">13</xref>&#x02013;<xref ref-type="bibr" rid="CR16">16</xref></sup>. For example, Piras et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> proposed to use spatio-temporal motion analysis to identify myocardial infarction. Gilbert et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> highlighted stronger associations between cardiac risk factors and mesh-derived metrics in the UK Biobank dataset. Mauger et al.<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> showed that mesh-based motion metrics could independently predict adverse cardiac events. This underscores the importance of establishing a precise computational model of cardiac status to define what a normal heart looks like and moves like. Nevertheless, it is a non-trivial task to describe the normative pattern of the 3D shape or even 3D + t motion of the heart, due to the complexity in representing high-dimensional spatio-temporal data.</p><p id="Par5">Recently, machine learning techniques have received increasing attention for cardiac shape and motion analysis<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>. Most existing research focuses on developing discriminative machine learning models, that is, training a model to perform classification tasks between different shapes or motion patterns<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup>. However, discriminative models offer only classification results and do not explicitly explain what the normative pattern of cardiac shape or motion looks like<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. By contrast, generative machine learning models provide an alternative route. Generative models are capable of describing distributions of high-dimensional data, such as images<sup><xref ref-type="bibr" rid="CR22">22</xref>&#x02013;<xref ref-type="bibr" rid="CR24">24</xref></sup>, geometric shapes<sup><xref ref-type="bibr" rid="CR25">25</xref>&#x02013;<xref ref-type="bibr" rid="CR27">27</xref></sup> or molecules<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>, which allow the representation of normative data patterns in the latent space of the model. In terms of generative modelling of the heart, recent developments focus on shape reconstruction and virtual population synthesis<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR30">30</xref>&#x02013;<xref ref-type="bibr" rid="CR34">34</xref></sup>. For example, Xia et al. proposed a method that integrates statistical shape priors with deep learning for four-chamber cardiac shape reconstruction from images<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. Gaggion et al. introduced HybridVNet, which combines convolutional neural networks with graph convolutions to perform shape reconstruction from multiview images<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. Dou et al. proposed a conditional flow-based variational autoencoder (VAE) for synthesizing virtual populations of cardiac anatomy<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> and later developed a compositional generative model for multipart anatomical structures<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. Beetz et al. introduced a variational mesh autoencoder that models population-wide variations in cardiac shapes with a hierarchical structure<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> and investigated the interpretability of the latent space extracted from a point-cloud VAE<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. Although generative models have been explored for cardiac shape reconstruction<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup>, shape modelling<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR37">37</xref>&#x02013;<xref ref-type="bibr" rid="CR39">39</xref></sup>, image and video generation<sup><xref ref-type="bibr" rid="CR41">41</xref>&#x02013;<xref ref-type="bibr" rid="CR43">43</xref></sup> and data augmentation<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, their application to personalized normative modelling of the heart from population data remains underexplored.</p><p id="Par6">Here, we provide an endeavour to create a personalized normative model of 3D + t cardiac shape and motion, leveraging deep generative modelling techniques. Cardiac shape and motion are represented by a dynamic sequence of 3D surface meshes across a cardiac cycle. A geometric deep generative model, named MeshHeart, is developed to model the distribution of 3D + t cardiac mesh sequences. MeshHeart uses a graph convolutional network (GCN)<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> to learn the latent features of the mesh geometry and a transformer to learn the temporal dynamics of the latent features during cardiac motion. This integration enables MeshHeart to model the distributions across both spatial and temporal dimensions. MeshHeart functions as a conditional generative model, accounting for major clinical variables such as sex and age as the generation factor. This enables the model to describe personalized normative patterns, generating synthetic healthy cardiac mesh sequences for a specific patient or a specific subpopulation.</p><p id="Par7">We train the proposed generative model, MeshHeart (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>), on a large-scale population-level imaging dataset with 38,309 participants from the UK Biobank<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>. After training the model, for each individual heart, we can generate a personalized 3D + t cardiac mesh model that describes the normative pattern for this particular subpopulation that has the same clinical factors as the input heart, as shown in Fig. <xref rid="Fig1" ref-type="fig">1c</xref>. In qualitative and quantitative experiments, we demonstrate that MeshHeart achieves high accuracy in generating the personalized heart model. Furthermore, we investigate the clinical relevance of the latent vector <italic>z</italic> of the model and propose a distance metric (latent delta &#x00394;<italic>z</italic>), which measures the deviation of the input heart from its personalized normative pattern (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>). We demonstrate that the latent vector and latent delta have a highly discriminative value for the disease classification task, and they are associated with a range of clinical features in phenome-wide association studies (PheWAS).<fig id="Fig1"><label>Fig. 1</label><caption><title>An overview of the MeshHeart model.</title><p><bold>a</bold>, Model architecture: MeshHeart encodes a sequence of cardiac meshes using a mesh encoder <italic>M</italic><sub>enc</sub> and encodes clinical factors using a conditional encoder <italic>C</italic><sub>enc</sub>. The encoder outputs across the time frames and along with distribution tokens <italic>&#x003bc;</italic><sub>token</sub> and <italic>&#x003a3;</italic><sub>token</sub> are processed by a temporal transformer encoder <italic>T</italic><sub>enc</sub>. A transformer decoder <italic>T</italic><sub>dec</sub> and a mesh decoder <italic>M</italic><sub>dec</sub> generate a 3D cardiac mesh sequence based on clinical factors. <bold>b</bold>, Given a set of clinical factors, an example of the generated mesh sequence across time frames. <bold>c</bold>, Conceptual framework: MeshHeart constructs a normative cardiac mesh sequence using personal information including age, sex, weight and height. A real heart can be compared with its personalized norm by the latent vector. The latent delta &#x00394;<italic>z</italic> is a distance defined between the latent vector of a synthetic normal heart (dark-blue dot) and that of the real heart (red dot).</p></caption><graphic xlink:href="42256_2025_1035_Fig1_HTML" id="d33e565"/></fig></p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>MeshHeart learns spatio-temporal mesh characteristics</title><p id="Par8">We first assessed the reconstruction capability of MeshHeart for 3D + t cardiac mesh sequences. The experiments used a dataset of 4,000 test participants, with details of the dataset described in Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>. Each input mesh sequence was encoded into latent representation and then decoded to reconstruct the mesh sequence. Reconstruction performance was evaluated using two metrics, the Hausdorff distance (HD) and the average symmetric surface distance (ASSD), which measure the difference between the input and reconstructed meshes. The HD metric quantifies the maximum distance between points in two sets, highlighting the maximum discrepancy between the original and reconstructed heart meshes. ASSD computes the average distance between the surfaces of two meshes, providing a more holistic evaluation of the model&#x02019;s accuracy. Evaluation was performed for three anatomical structures: the left ventricle (LV), the myocardium (Myo) and the right ventricle (RV). We compared the performance of MeshHeart with three baseline mesh generative models: Action2Motion<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, ACTOR<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> and CHeart<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref> presents the architecture comparison.</p><p id="Par9">Figure <xref rid="Fig2" ref-type="fig">2a</xref> and Supplementary Table <xref rid="MOESM1" ref-type="media">3</xref> report the reconstruction accuracy of MeshHeart, compared with other generative models. The metrics are reported as the average across all time frames, as well as at two representative time frames of cardiac motion: the end-diastolic (ED) frame and the end-systolic (ES) frame. Overall, MeshHeart achieves the best reconstruction accuracy, outperforming other generative models, with the lowest HD of 4.163&#x02009;mm and ASSD of 1.934&#x02009;mm averaged across the time frames and across anatomical structures. In addition, Fig. <xref rid="Fig2" ref-type="fig">2b</xref> visualizes examples of the reconstructed meshes, with vertex-wise reconstruction errors overlaid, at different frames of the cardiac cycle (<italic>t</italic>&#x02009;(time) =&#x02009;0, 10 and 19 out of 50 frames in total). MeshHeart achieves lower reconstruction errors compared with the other models and maintains the smoothness of reconstructed meshes. We further conducted ablation studies to assess the contribution of each component to the model performance. These components are described in the <xref rid="Sec8" ref-type="sec">Methods</xref>, and the detailed results are reported in Supplementary Table <xref rid="MOESM1" ref-type="media">6</xref>. Replacing GCN by linear layers results in an increased HD from 4.163&#x02009;mm to 5.707&#x02009;mm, while replacing GCN by convolutional neural network results in a HD of 5.268&#x02009;mm, highlighting GCN&#x02019;s superiority in encoding mesh geometry. Substituting the transformer with gated recurrent units (GRUs) or long short-term memory networks (LSTMs) leads to an increased HD of 4.720&#x02009;mm or 5.015&#x02009;mm, respectively, which demonstrates the advantage of using the transformer for modelling long-range temporal dependencies. Other components such as the smoothness loss term and the distribution parameter tokens also contribute to the model performance. These results highlight MeshHeart&#x02019;s capability in learning spatial&#x02013;temporal characteristics of cardiac mesh sequences.<fig id="Fig2"><label>Fig. 2</label><caption><title>Evaluation of the mesh reconstruction accuracy of MeshHeart, compared with three other methods; Action2Motion, ACTOR and CHeart.</title><p><bold>a</bold>, Plots of the HD and ASSD metrics. The metrics are calculated as the mean across all time frames, as well as at the ED frame and the ES frame. They are reported for the LV, the Myo and the RV and averaged across the anatomical structures. Lower values indicate better performance. Each boxplot represents results over <italic>n</italic>&#x02009;=&#x02009;4,000 UK Biobank participants from the held-out test set, treated as biological replicates. Each participant contributes one sample per method; no technical replicates were used. The unit of analysis is the individual participant. Box plots represent the distribution of the data as follows: the centre line indicates the median; box limits represent the 25th and 75th percentiles (interquartile range); whiskers extend to 1.5&#x000d7; the interquartile range from the box limits; and points beyond this range are plotted individually as outliers. <bold>b</bold>, Visualization of the reconstructed cardiac mesh sequence, coloured by the reconstruction error (in red) between the input mesh and reconstructed mesh. The mesh is visualized in three different imaging planes.</p></caption><graphic xlink:href="42256_2025_1035_Fig2_HTML" id="d33e627"/></fig></p></sec><sec id="Sec4"><title>MeshHeart resembles real data distribution</title><p id="Par10">Utilizing the latent representations learned by MeshHeart, we assessed the ability of the model to generate new synthetic cardiac mesh sequences that mimic real heart dynamics. To evaluate the fidelity and diversity of the generation, we calculated the similarity between the distributions of real meshes and generated synthetic meshes. For each real heart in the test set (<italic>n</italic>&#x02009;=&#x02009;4,000), we applied MeshHeart to generate synthetic mesh sequences using the same clinical factors (age, sex, weight and height) as the individual as the model input. During the generation stage, we chose 20 random samples from the Gaussian distribution of the latent space and generated the corresponding mesh sequences. For both real and synthetic meshes, clinically relevant metrics for cardiac structure and function were derived, including left ventricular ED volume (LVEDV), left ventricular ES volume (LVESV), left ventricular ejection fraction (LVEF), left ventricular myocardial mass (LVM), right ventricular ED volume (RVEDV), right ventricular ES volume (RVESV) and right ventricular ejection fraction (RVEF). For each metric <italic>m</italic>, its probability distributions against age <italic>P</italic>(<italic>m</italic>&#x02223;<italic>c</italic>&#x02009;=&#x02009;age) and against sex <italic>P</italic>(<italic>m</italic>&#x02223;<italic>c</italic>&#x02009;=&#x02009;sex) were calculated. The similarity between real and synthetic probability distributions was quantified using the Kullback&#x02013;Leibler (KL) divergence<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> and the Wasserstein distance (WD)<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, with a lower value denoting a higher similarity, that is, better generation performance. KL divergence is a metric from information theory that evaluates the dissimilarity between two probability mass functions. Similarly, WD measures the dissimilarity between two probability distributions. MeshHeart&#x02019;s ability to replicate real data distributions is quantitatively demonstrated in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>. MeshHeart achieves lower KL and WD scores compared with existing methods, as shown by radar plots with the smallest area, suggesting that the synthetic data generated by the proposed model align closely with the real data distribution for clinically relevant metrics. Supplementary Tables <xref rid="MOESM1" ref-type="media">4</xref> and <xref rid="MOESM1" ref-type="media">5</xref> report the detailed KL divergence and WD scores for different methods.<fig id="Fig3"><label>Fig. 3</label><caption><title>Evaluation of the generation performance of MeshHeart.</title><p><bold>a</bold>, Spider charts for the WD and the KL divergence metrics, which quantify the distance between the generated and real data distributions. The data distribution is calculated as the histogram of a cardiac imaging phenotype (LVEDV, LVESV, LVEF, LVM, RVEDV, RVESV and RVEF) against a clinical factor (age or sex). The metrics are plotted over a polar coordinate system, colour-coded by different methods. The smaller the metric (closer to the centre), the greater the similarity between the generated and real data distributions. <bold>b</bold>, Examples of generated 3D + t cardiac meshes with different generating factors, including age (<italic>a</italic>), sex (f and m for female and male, respectively), weight (<italic>w</italic>) and height (<italic>h</italic>). <bold>c</bold>, A side-by-side comparison of a real heart versus the generated synthetic heart and the difference map between them.</p></caption><graphic xlink:href="42256_2025_1035_Fig3_HTML" id="d33e700"/></fig></p><p id="Par11">For qualitative assessment, Fig. <xref rid="Fig3" ref-type="fig">3b</xref> shows four instances of synthetic cardiac mesh sequences for different personal factors (age, sex, weight and height). For brevity, only two frames (<italic>t</italic>&#x02009;=&#x02009;0 and 20) are shown. The figure demonstrates that MeshHeart can mimic authentic cardiac movements, showing contractions across time from diastole to systole. Figure <xref rid="Fig3" ref-type="fig">3c</xref> compares a real heart with a synthetic normal heart, at different time frames (<italic>t</italic>&#x02009;=&#x02009;0, 5, 15 and 19), demonstrating the capability of MeshHeart in replicating both the real cardiac structure as well as typical motion patterns.</p><p id="Par12">We also examined the latent representation learnt by MeshHeart using <italic>t</italic>-distributed stochastic neighbour embedding visualization<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> as illustrated in Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>. The <italic>t</italic>-distributed stochastic neighbour embedding plot projects the 64-dimensional latent representation of a mesh, extracted from the last hidden layer of the transformer encoder <italic>T</italic><sub>enc</sub>, onto a two-dimensional space, with each point denoting a mesh. It shows ten sample sequences. For each sample, the latent representations of the meshes across time frames form a circular pattern that resembles the rhythmic beating of the heart<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>.</p></sec><sec id="Sec5"><title>Latent vector aids cardiovascular disease classification</title><p id="Par13">After demonstrating the generative capability of MeshHeart, we explore its potential for clinical applications, in particular using its latent space, which provides a low-dimensional representation of cardiac shape and motion. The latent feature analyses were conducted on 17,309 participants. More than half (58.5%) had a reported diagnosis of at least one disease. We use the latent vector <italic>z</italic> of each mesh sequence, a 64-dimensional vector, as the feature for correlation analysis and for cardiovascular disease classification. Figure <xref rid="Fig5" ref-type="fig">5a</xref> shows that the latent vector exhibits strong correlations with conventional imaging phenotypes, such as LVM, LVEDV and RVEDV. Figure <xref rid="Fig4" ref-type="fig">4</xref> and Supplementary Table <xref rid="MOESM1" ref-type="media">7</xref> compare the classification performance of six cardiac diseases when using different feature sets. The three evaluated feature sets include &#x02018;phenotypes + confounders (age, sex, weight, height)&#x02019;, &#x02018;latent vector + confounders&#x02019; and &#x02018;phenotypes + latent vector + confounders&#x02019;. The classification performance is evaluated using the area under the curve (AUC) scores for three different classifiers: AdaBoost, linear discriminant analysis (LDA) and support vector machine (SVM). The six cardiovascular diseases include myocardial infarction (ICD-10 code I21), ischaemic heart diseases (I24), paroxysmal tachycardia (I47), atrial fibrillation and flutter (I48), hypertension (I10) and cardiac disease (I51). Figure <xref rid="Fig4" ref-type="fig">4</xref> shows that using imaging phenotypes alone led to moderate AUC scores (for example, 0.8361 and 0.8201 for myocardial infarction and ischaemic heart diseases using with AdaBoost). Using the latent vector resulted in increased AUC scores (0.8557 and 0.8453). Combining both imaging phenotypes and the latent vector further improved the AUC scores (0.8762 and 0.8472), indicating the usefulness of the latent vector for cardiovascular disease classification. These results demonstrate the model&#x02019;s ability to discriminate not only between normal and abnormal cardiac states but also among specific disease conditions.<fig id="Fig4"><label>Fig. 4</label><caption><title>Comparison of disease classification performance, in terms of the AUC scores, when different feature sets are used.</title><p>The three feature sets being compared include &#x02018;phenotypes + confounders (age, sex, weight, height)&#x02019;, &#x02018;latent vector + confounders&#x02019; and &#x02018;phenotypes + latent vector + confounders&#x02019;, with each feature set represented by a unique colour in the plot. The three subplots show the performance of three different classifiers, AdaBoost, LDA and SVM. The <italic>x</italic> axis denotes the disease type, and the <italic>y</italic> axis denotes the AUC score. AUC values are averaged across five random train&#x02013;test splits. For the three feature sets, each feature set is compared with one of the other two features sets using the two-sided DeLong&#x02019;s test. A single asterisk denotes a notable difference (<italic>P</italic> &#x0003c; 0.05), indicating that a feature set outperforms another feature set markedly, while a double asterisk indicates that a feature set outperforms both of the other two feature sets substantially. Exact <italic>P</italic> values are provided in Supplementary Table <xref rid="MOESM1" ref-type="media">S7</xref>. No multiple testing correction was applied.</p></caption><graphic xlink:href="42256_2025_1035_Fig4_HTML" id="d33e782"/></fig></p><p id="Par14">For the AdaBoost classifier, using feature sets comprising the latent vector, as well as the combination of phenotypes and the latent vector, consistently outperformed the performance of the phenotypes set alone (for example, 0.8291 and 0.8316 for cardiac disease using latent vector and combined feature sets), implying that incorporating the latent vector improved the classification accuracy. The trend was particularly noticeable for myocardial infarction, hypertension and cardiac diseases, where the combined phenotypes and latent vector feature set substantially improved the AUC scores (0.8762, 0.7738 and 0.8316 for myocardial infarction, hypertension and cardiac disease). While the model was trained using a normal healthy heart dataset, it has learned a rich latent representation to encode diverse shape and motion patterns for different subpopulations in this large dataset. The resulting latent vector captures deviations in the latent space that are indicative of specific disease outcomes, as demonstrated by the experimental results. The LDA and SVM classifiers demonstrated that, among the three feature sets, the combined phenotypes and latent vector feature set achieved the highest AUC scores (for example, 0.6728 and 0.6479 for hypertension with LDA and SVM). However, for certain diseases such as ischaemic heart disease, classifiers using only phenotypes (for example, 0.7381 and 0.7123 for ischaemic heart diseases with LDA and SVM) outperformed those that used only the latent vector (0.7277 and 0.6975) but still fell short of their combination (0.7492 and 0.7214). Overall, the results show that, integrating imaging phenotypes, the latent vector along with confounders provides the best discriminative feature set for classification.</p></sec><sec id="Sec6"><title>Latent delta for PheWAS</title><p id="Par15">For each individual heart, we use MeshHeart to generate a normal synthetic heart using the same clinical factors as this individual. This synthetic heart can be regarded as a personalized normative model learned from a specific subpopulation. We define the latent delta &#x00394;<italic>z</italic> to be the difference between the latent vectors of an individual heart and its personalized norm, quantified using the Euclidean distance. The latent delta characterizes the deviation of the shape and motion patterns of an individual heart from the normal pattern for a subpopulation with the same clinical factors (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>). A PheWAS was performed to explore the clinical relevance of &#x00394;<italic>z</italic>, as shown in Fig. <xref rid="Fig5" ref-type="fig">5b</xref>. The PheWAS revealed notable associations between the latent delta &#x00394;<italic>z</italic> and an unbiased set of clinical outcomes, including circulatory system diseases, endocrine and metabolic diseases, genitourinary diseases, musculoskeletal diseases and neoplasms.<fig id="Fig5"><label>Fig. 5</label><caption><title>Association studies for the latent vector and the latent delta with imaging-derived phenotypes and clinical features.</title><p><bold>a</bold>, A heatmap of the Pearson correlation coefficients between imaging phenotypes and the 64-dimensional latent vector. The intensity of the colour reflects the magnitude and direction of the correlation, where blue signifies negative (Neg) correlations and red signifies positive (Pos) correlations. Darker shades indicate a stronger correlation between the vector and the phenotypes. <bold>b</bold>, PheWAS between the latent delta &#x00394;<italic>z</italic> and unbiased categories of clinical features. The <italic>y</italic> axis lists the clinical outcomes where a signficiant association was identified. The <italic>x</italic> axis uses different colours to represent different disease categories. Each triangle denotes a notable PheWAS association, adjusted for multiple comparisons using the Bonferroni correction for 1,163 clinical features analysed. These clinical features encompass both clinical outcomes (for example, diseases and diagnoses) and phenotypes, covering a wide range of characteristics and measurements. The shape of each triangle indicates the direction of the effect. This analysis included 17,000 participants.</p></caption><graphic xlink:href="42256_2025_1035_Fig5_HTML" id="d33e827"/></fig></p><p id="Par16">The latent delta has been shown to correlate with phenotypes such as LVM and LVEF (Fig. <xref rid="Fig5" ref-type="fig">5a</xref>), which serve as indicators of cardiac structure and function. Conditions such as hypertension, lipid and cholesterol abnormalities and diabetes can induce changes in these cardiac phenotypes. For example, hypertension probably results in an increased LVM and may be linked to a reduced LVEF due to the heart&#x02019;s adaptation to prolonged high blood pressure. In a similar vein, diabetes can exert metabolic stress on the heart, which can lead to changes in cardiac volume and ejection fraction. These modifications in the structure and motion patterns of the heart, as captured by the latent delta, provide a mechanistic explanation for the associations observed in the PheWAS results.</p><p id="Par17">In Fig. <xref rid="Fig5" ref-type="fig">5b</xref>, the direction of effect shows the relationship between &#x00394;<italic>z</italic> and the clinical outcome. A positive effect indicates that an increase in &#x00394;<italic>z</italic> is associated with a higher probability of the outcome. By contrast, a negative effect indicates that a higher &#x00394;<italic>z</italic> reduces the likelihood of the outcome. For example, a negative effect for birth trauma suggests that a higher &#x00394;<italic>z</italic> is associated with a reduced likelihood of birth trauma. These directional effects provide insight into how deviations in cardiac structure and function relate to specific clinical outcomes, highlighting potential associations for further in-depth clinical investigation.</p></sec></sec><sec id="Sec7" sec-type="discussion"><title>Discussion</title><p id="Par18">This work contributes to the growing field of generative artificial intelligence for science, with a specific application in cardiac imaging. The proposed MeshHeart model is a generative model that can facilitate improved understanding of the complexities of 3D + t cardiac shape and motion. In this study, we made four major contributions. First, we developed MeshHeart using a dataset of 38,309 participants from a large UK population<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>, capturing the variation in cardiac structures and clinical characteristics. Second, we demonstrated MeshHeart&#x02019;s capability to generate a normal heart, accounting for clinical factors such as age, sex, weight and height. This established a personalized normative model for cardiac anatomy. Third, we investigated the latent vector of MeshHeart and demonstrated its associations with conventional imaging phenotypes and usefulness for enhancing disease classification performance. Finally, we propose a latent delta (&#x00394;<italic>z</italic>) metric. This metric provides a way for quantifying the difference between an individual heart and the normative model, as well as for investigating the associations between the spatial&#x02013;temporal characteristics of the heart and various health outcomes.</p><p id="Par19">MeshHeart&#x02019;s reconstruction capability was assessed using HD and ASSD metrics. Using these two metrics, we compared the model with other models along with an ablation study. Using geometric convolutions and a temporal transformer, the model reconstructed more accurate cardiac mesh sequences compared with other state-of-the-art models. This is is due to the reason that geometric convolutions are proficient in encoding mesh geometry, and the transformer is effective in capturing long-range temporal dependencies. The ablation study confirms the essential role of geometric convolutions and the temporal transformer in increasing the performance of the model, as detailed in Supplementary Table <xref rid="MOESM1" ref-type="media">6</xref>. We also compared MeshHeart against a previous work CHeart<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. CHeart uses segmentation as a representation method for the cardiac structure, whereas MeshHeart uses the mesh representation. The results show that mesh provides a powerful representation for modelling the 3D geometry as well for tracking temporal motion, as it essentially allows the movement of each individual point to be monitored over time.</p><p id="Par20">The generative capabilities of MeshHeart, as illustrated by the results in Fig. <xref rid="Fig3" ref-type="fig">3</xref> and Supplementary Tables <xref rid="MOESM1" ref-type="media">3</xref> and <xref rid="MOESM1" ref-type="media">4</xref>, demonstrate its proficiency as a generative model, able to replicate a normal heart on the basis of certain clinical factors including demographics (age and sex) and anthropometrics (weight and height). These four factors have shown strong correlations with heart structure and function across various individuals<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR52">52</xref>,<xref ref-type="bibr" rid="CR53">53</xref></sup>. They form a reliable basis for constructing a normal heart model for an individual, as shown in Fig. <xref rid="Fig3" ref-type="fig">3b</xref>. Our analysis in Fig. <xref rid="Fig3" ref-type="fig">3a</xref> and Supplementary Tables <xref rid="MOESM1" ref-type="media">3</xref> and <xref rid="MOESM1" ref-type="media">4</xref> focused on age and sex, using WD and KL divergence to assess the similarity between the real and synthetic data distributions. Lower WD and KL metrics suggest that MeshHeart effectively represents demographic diversity, making the synthetic data beneficial for potential clinical and research purposes. The incorporation of additional clinical variables in the future, such as blood pressure and medical history, could improve the representation of cardiac health and diseases, thus enabling more potential applications for downstream tasks.</p><p id="Par21">The latent vector obtained from the MeshHeart demonstrated its discriminative power for disease classification tasks. Incorporating the latent vector as feature substantially improves the classification accuracy for a range of cardiovascular conditions, as illustrated in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. Although conventional imaging phenotypes can also be used as a feature set for the classification model, their classification performance was surpassed by the augmented feature set that also includes the latent vector, suggesting that the latent vector may contain some information not provided by the imaging phenotypes. Combining imaging phenotypes with the latent vector and confounders consistently achieved the best classification performance, regardless of the classification model used, demonstrating the benefit of integrating multiple data sources to represent the status of the heart. Some dimensions of the latent vector exhibit high correlations with conventional cardiac phenotypes, which are essential for assessing cardiovascular disease risk. The high correlation with the latent vector underscores their clinical analysis potential.</p><p id="Par22">PheWAS uses a data-driven approach to uncover unbiased associations between cardiac deviations and disease diagnoses. Our analysis found that greater deviations in heart function are linked to increased risks of endocrine, metabolic and circulatory diseases. These cardiac diseases suggest underlying metabolic problems such as insulin resistance or metabolic disturbances observed in diabetes and obesity, which affect the structure and performance of the heart<sup><xref ref-type="bibr" rid="CR54">54</xref>,<xref ref-type="bibr" rid="CR55">55</xref></sup>. Likewise, they indicate wider circulatory conditions such as hypertension and atherosclerosis, which can lead to heart failure and ischaemic heart disease<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>. Understanding these relationships is crucial for risk stratification, personalized medicine and prevention strategies, highlighting the need for thorough cardiac evaluations in clinical management<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>.</p><p id="Par23">Although this work advances the science in personalized cardiac modelling, there are several limitations. First, the personalized normative model relies on a restricted range of generating factors, including age, sex, weight and height, as we aim to develop a standard healthy heart. Including additional elements in the future such as diseases or environmental factors such as air pollution and noise<sup><xref ref-type="bibr" rid="CR58">58</xref></sup> could improve our understanding of their impacts on cardiac anatomy and function. Second, the model uses a cross-sectional dataset from the UK Biobank for both training and testing purposes. However, it does not include a benchmark for the progression of cardiac ageing, which could be addressed by using a longitudinal dataset to evaluate the model. Repeated scans are expected in the near future from the UK Biobank. Third, this study focuses on modelling the dynamic mesh sequence to describe cardiac shape and motion. It does not aim to model the underlying electrophysiology or biomechanics of the heart, which are also essential for cardiac modelling and understanding cardiac function<sup><xref ref-type="bibr" rid="CR59">59</xref>,<xref ref-type="bibr" rid="CR60">60</xref></sup>. In addition, the explainability of latent vectors could be explored, as understanding the specific information each latent dimension captures is crucial for clinical interpretation and validation. Finally, our method does not incorporate long-axis images, which limits its ability to capture the mitral, tricuspid or aortic valves for assessing valvular function. Mauger et al.<sup><xref ref-type="bibr" rid="CR61">61</xref></sup> used two-chamber and four-chamber long-axis images to identify tricuspid and mitral valve points, so that the motion of the valve points can be tracked and modelled using principal component analysis.</p><p id="Par24">In conclusion, this study presents MeshHeart, a generative model for cardiac shape modelling. By training and evaluating the model on a population-level dataset from the UK Biobank, we demonstrate that MeshHeart not only achieves a high reconstruction accuracy but also excels in generating synthetic cardiac mesh sequences that closely resemble the real heart. The latent vector of the generative model and the latent delta metric provide new avenues of research to improve disease classification and personalized healthcare. These findings pave the way for future research on cardiac modelling and may inspire the development of generative modelling techniques for other types of biomedical data.</p></sec><sec id="Sec8"><title>Methods</title><sec id="Sec9"><title>Generative model architecture</title><p id="Par25">Figure <xref rid="Fig1" ref-type="fig">1a</xref> illustrates the architecture of the proposed generative model, MeshHeart. Given a set of clinical conditions <italic>c</italic>, our goal is to develop a model that can generate a dynamic 3D cardiac mesh sequence, <italic>X</italic><sub>0:<italic>T</italic>&#x02212;1</sub>&#x02009;=&#x02009;{<italic>x</italic><sub>0</sub>, <italic>x</italic><sub>1</sub>, &#x022ef;, <italic>x</italic><sub><italic>T</italic>&#x02212;1</sub>}, where <italic>T</italic> denotes the number of time frames, that corresponds to the conditions <italic>c</italic>. Figure <xref rid="Fig1" ref-type="fig">1b</xref> shows an example of the input conditions and the generated mesh sequence. Without losing generality, we take age, sex, weight and height as conditions <italic>c</italic> in this work. Age, weight and height are continuous variables, whereas sex is a binary variable. Each cardiac mesh <italic>x</italic><sub><italic>t</italic></sub>&#x02009;=&#x02009;(<italic>v</italic><sub><italic>t</italic></sub>, <italic>e</italic><sub><italic>t</italic></sub>) is a graph with a set of vertices <italic>v</italic> and a set of edges <italic>e</italic> connecting them.</p><p id="Par26">The proposed generative model consists of a mesh encoder <italic>M</italic><sub>enc</sub>, a transformer encoder <italic>T</italic><sub>enc</sub>, a condition encoder <italic>C</italic><sub>enc</sub>, a transformer decoder <italic>T</italic><sub>dec</sub> and a mesh decoder <italic>M</italic><sub>dec</sub>. These components are designed to work together to learn the probability distribution <italic>p</italic><sub><italic>&#x003b8;</italic></sub>(<italic>x</italic>&#x02223;<italic>z</italic><sub><italic>c</italic></sub>) of the cardiac mesh sequence conditioned on clinical attributes, where <italic>&#x003b8;</italic> represents the decoder parameters and <italic>z</italic><sub><italic>c</italic></sub> denotes the condition latent vector. The condition encoder <italic>C</italic><sub>enc</sub>, implemented as a multilayer perceptron (MLP), maps the clinical conditions <italic>c</italic> into a condition latent vector <italic>z</italic><sub><italic>c</italic></sub>.</p><p id="Par27">The mesh encoder <italic>M</italic><sub>enc</sub>, implemented as a GCN, processes the input cardiac mesh sequence <italic>x</italic><sub>0:<italic>T</italic>&#x02212;1</sub>. It extracts latent representations <italic>z</italic><sub>0:<italic>T</italic>&#x02212;1</sub>, where each vector <italic>z</italic><sub><italic>t</italic></sub> corresponds to a latent representation of the cardiac mesh at time frame <italic>t</italic>. These latent vectors serve as intermediate representations of the cardiac mesh sequence.</p><p id="Par28">The latent vectors <italic>z</italic><sub>0:<italic>T</italic>&#x02212;1</sub> from the mesh encoder are concatenated with the condition latent vector <italic>z</italic><sub><italic>c</italic></sub> to form a sequence of input tokens to the transformer encoder <italic>T</italic><sub>enc</sub>. The transformer encoder <italic>T</italic><sub>enc</sub> captures temporal dependencies across the sequence, which comprises <italic>L</italic> layers of alternating blocks of multihead self-attention (MSA) and MLP. To ensure stability and effective learning, LayerNorm (LN) is applied before each block and residual connections are applied after each block. Similar to the class token in the vision transformer<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>, we append the input tokens <italic>z</italic><sub>0:<italic>T</italic>&#x02212;1</sub> with two learnable parameters <italic>&#x003bc;</italic><sub>token</sub> and <italic>&#x003a3;</italic><sub>token</sub>, named distribution parameter tokens, which parameterize a Gaussian distribution over the latent space. In the transformer output layer, we extract the outputs from the distribution parameter tokens as distribution parameters <italic>&#x003bc;</italic> and <italic>&#x003a3;</italic>. We then use the reparameterization trick<sup><xref ref-type="bibr" rid="CR63">63</xref></sup> to derive the latent <italic>z</italic><sub><italic>a</italic></sub> from <italic>&#x003bc;</italic> and <italic>&#x003a3;</italic>, as shown in Fig. <xref rid="Fig1" ref-type="fig">1a</xref>. The encoding process is formulated as<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e1188">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{rcl}{z}_{{\mathrm{input}}}&#x00026;=&#x00026;[{\mu }_{{\mathrm{token}}};{\Sigma }_{{\mathrm{token}}};{z}_{0};{z}_{1};\ldots ;{z}_{T-1}]\\ {z}^{{\prime} l}&#x00026;=&#x00026;{\mathrm{MSA}}\left({\mathrm{LN}}\left({z}^{l-1}\right)\right)+{z}^{l-1},l=1,\ldots ,L\\ {z}^{l}&#x00026;=&#x00026;{\mathrm{LN}}\left[{\mathrm{MLP}}\left.\right({\mathrm{LN}}\left({z}^{{\prime} l}\right)\right]\\ {z}_{a}&#x00026;=&#x00026;\mu +\epsilon\Sigma ,\epsilon \sim {\mathcal{N}}(0,{\bf{1}})\end{array}.$$\end{document}</tex-math><mml:math id="d33e1194"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">input</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">token</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">token</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal">MSA</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="normal">LN</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal">LN</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:mi mathvariant="normal">MLP</mml:mi><mml:mfenced close="("><mml:mrow/></mml:mfenced><mml:mi mathvariant="normal">LN</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>&#x003bc;</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003f5;</mml:mi><mml:mi mathvariant="normal">&#x003a3;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003f5;</mml:mi><mml:mo>~</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">1</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="42256_2025_1035_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where ~ means distributed as, indicating that the random variable <italic>&#x003b5;</italic> follows a normal distribution, where the bold <bold>1</bold> denotes the identity matrix. The resulting latent vector <italic>z</italic><sub><italic>a</italic></sub>, derived after the reparameterization step, captures the information about the distribution of the mesh sequence. This vector is concatenated with the condition latent vector <italic>z</italic><sub><italic>c</italic></sub> to form the input to the transformer decoder <italic>T</italic><sub>dec</sub>. The decoder uses these concatenated vectors as keys and values in the self-attention layer, while sinusoidal temporal positional encodings<sup><xref ref-type="bibr" rid="CR62">62</xref></sup> serve as queries to incorporate temporal information. The temporal positional encoding <italic>p</italic><sub><italic>t</italic></sub> at time frame <italic>t</italic> is defined using the sinusoidal function with the same dimension <italic>d</italic> as <italic>z</italic><sub><italic>a</italic></sub>:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e1396">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{p}_{t}}^{(i)}=\left\{\begin{array}{ll}\sin \left(t/\text{10,000}^{2i/d}\right),\quad &#x00026;\,\text{if}\,\,i=2k\\ \cos \left(t/\text{10,000}^{2i/d}\right),\quad &#x00026;\,\text{if}\,\,i=2k+1\end{array}\right.,$$\end{document}</tex-math><mml:math id="d33e1402"><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi>sin</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mstyle><mml:mtext>10,000</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1.0em"/></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>if</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>k</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>cos</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mstyle><mml:mtext>10,000</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1.0em"/></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>if</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="42256_2025_1035_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>i</italic> denotes the dimension index. The transformer decoder outputs a sequence of latent vectors, each corresponding to a mesh representation at a timepoint of the cardiac cycle. The latent vectors generated by the transformer decoder are passed through the mesh decoder <italic>M</italic><sub>dec</sub>, composed of fully connected (FC) layers, to reconstruct the 3D + t cardiac mesh sequence <inline-formula id="IEq1"><alternatives><tex-math id="d33e1490">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X}_{0:T-1}^{{\prime} }$$\end{document}</tex-math><mml:math id="d33e1495"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq1.gif"/></alternatives></inline-formula>.</p></sec><sec id="Sec10"><title>Probabilistic modelling and optimization</title><p id="Par29">Following the VAE formulation<sup><xref ref-type="bibr" rid="CR63">63</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>, we assume a prior distribution <italic>p</italic>(<italic>z</italic><sub><italic>a</italic></sub>) over the latent variable <italic>z</italic><sub><italic>a</italic></sub>. The prior <italic>p</italic>(<italic>z</italic><sub><italic>a</italic></sub>), together with the decoder (constructed by <italic>T</italic><sub>dec</sub> and <italic>M</italic><sub>dec</sub>), defines the joint distribution <italic>p</italic>(<italic>x</italic>, <italic>z</italic><sub><italic>a</italic></sub>&#x02223;<italic>z</italic><sub><italic>c</italic></sub>). To train the model and perform inference, we need to compute the posterior distribution <italic>p</italic>(<italic>z</italic><sub><italic>a</italic></sub>&#x02223;<italic>x</italic>, <italic>z</italic><sub><italic>c</italic></sub>), which is generally intractable. To turn the intractable posterior inference problem <italic>p</italic>(<italic>z</italic><sub><italic>a</italic></sub>&#x02223;<italic>x</italic>, <italic>z</italic><sub><italic>c</italic></sub>) into a tractable problem, we introduce a parametric encoder model (constructed by <italic>C</italic><sub>enc</sub>, <italic>M</italic><sub>enc</sub> and <italic>T</italic><sub>enc</sub>) <italic>q</italic><sub><italic>&#x003d5;</italic></sub>(<italic>z</italic><sub><italic>a</italic></sub>&#x02223;<italic>x</italic>, <italic>z</italic><sub><italic>c</italic></sub>) with <italic>&#x003d5;</italic> to be the variational parameters, which approximates the true but intractable posterior distribution <italic>p</italic>(<italic>z</italic><sub><italic>a</italic></sub>&#x02223;<italic>x</italic>, <italic>z</italic><sub><italic>c</italic></sub>) of the generative model, given an input <italic>x</italic> and conditions <italic>c</italic>:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e1673">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${q}_{\phi }({z}_{a}| x,{z}_{c})\approx {p}_{\theta }({z}_{a}| x,{z}_{c}),$$\end{document}</tex-math><mml:math id="d33e1679"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02248;</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="42256_2025_1035_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <italic>q</italic><sub><italic>&#x003d5;</italic></sub>(<italic>z</italic><sub><italic>a</italic></sub>&#x02223;<italic>x</italic>, <italic>z</italic><sub><italic>c</italic></sub>) often adopts a simpler form, for example the Gaussian distribution<sup><xref ref-type="bibr" rid="CR63">63</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>. By introducing the approximate posterior <italic>q</italic><sub><italic>&#x003d5;</italic></sub>(<italic>z</italic><sub><italic>a</italic></sub>&#x02223;<italic>x</italic>, <italic>z</italic><sub><italic>c</italic></sub>), the log-likelihood of the conditional distribution <italic>p</italic><sub><italic>&#x003b8;</italic></sub>(<italic>x</italic>&#x02223;<italic>z</italic><sub><italic>c</italic></sub>) for input data <italic>x</italic>, also known as evidence, can be formulated as<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="d33e1800">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{rcl}\log {p}_{\theta }(x| {z}_{c})&#x00026;=&#x00026;{{\mathbb{E}}}_{{z}_{a} \sim {q}_{\phi }({z}_{a}| x,{z}_{c})}\log \left[{p}_{\theta }(x| {z}_{c})\right]\\ &#x00026;=&#x00026;{{\mathbb{E}}}_{{z}_{a} \sim {q}_{\phi }({z}_{a}| x,{z}_{c})}\log \left[\frac{{p}_{\theta }(x,{z}_{a}| {z}_{c})}{{q}_{\phi }({z}_{a}| x,{z}_{c})}\right]+{{\mathbb{E}}}_{{z}_{a} \sim {q}_{\phi }({z}_{a}| x,{z}_{c})}\log \left[\frac{{q}_{\phi }({z}_{a}| x,{z}_{c})}{{p}_{\theta }({z}_{a}| x,{z}_{c})}\right]\end{array},$$\end{document}</tex-math><mml:math id="d33e1806"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi>log</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"/><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="42256_2025_1035_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where the second term denotes the KL divergence <italic>D</italic><sub>KL</sub>(<italic>q</italic><sub><italic>&#x003d5;</italic></sub>&#x02225;<italic>p</italic><sub><italic>&#x003b8;</italic></sub>) between <italic>q</italic><sub><italic>&#x003d5;</italic></sub>(<italic>z</italic><sub><italic>a</italic></sub>&#x02223;<italic>x</italic>, <italic>z</italic><sub><italic>c</italic></sub>) and <italic>p</italic><sub><italic>&#x003b8;</italic></sub>(<italic>z</italic><sub><italic>a</italic></sub>&#x02223;<italic>x</italic>, <italic>z</italic><sub><italic>c</italic></sub>)<sup><xref ref-type="bibr" rid="CR63">63</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>. It is non-negative and zero only if the approximate posterior <italic>q</italic><sub><italic>&#x003d5;</italic></sub>(<italic>z</italic><sub><italic>a</italic></sub>&#x02223;<italic>x</italic>, <italic>z</italic><sub><italic>c</italic></sub>) equals the true posterior distribution <italic>p</italic><sub><italic>&#x003b8;</italic></sub>(<italic>z</italic><sub><italic>a</italic></sub>&#x02223;<italic>x</italic>, <italic>z</italic><sub><italic>c</italic></sub>). Due to the non-negativity of the KL divergence, the first term in equation (<xref rid="Equ4" ref-type="disp-formula">4</xref>) is the lower bound of the evidence <inline-formula id="IEq2"><alternatives><tex-math id="d33e2173">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log [{p}_{\theta }(x| {z}_{c})]$$\end{document}</tex-math><mml:math id="d33e2178"><mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq2.gif"/></alternatives></inline-formula>, known as the evidence lower bound (ELBO). Instead of optimizing the evidence <inline-formula id="IEq3"><alternatives><tex-math id="d33e2203">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log [{p}_{\theta }(x| {z}_{c})]$$\end{document}</tex-math><mml:math id="d33e2208"><mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq2.gif"/></alternatives></inline-formula>, which is often intractable, we optimize the ELBO as follows:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="d33e2233">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathop{\min }\limits_{\theta ,\phi }{\mathrm{ELBO}}=-\log [{p}_{\theta }(x| {z}_{c})]+{D}_{{\mathrm{KL}}}.$$\end{document}</tex-math><mml:math id="d33e2239"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:munder><mml:mi mathvariant="normal">ELBO</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">KL</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="42256_2025_1035_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec11"><title>Training loss function</title><p id="Par30">Based on the ELBO, we define the concrete training loss function, which combines the mesh reconstruction loss <inline-formula id="IEq4"><alternatives><tex-math id="d33e2286">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{{\mathrm{R}}}$$\end{document}</tex-math><mml:math id="d33e2291"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq4.gif"/></alternatives></inline-formula>, the KL loss <inline-formula id="IEq5"><alternatives><tex-math id="d33e2301">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{{\mathrm{KL}}}$$\end{document}</tex-math><mml:math id="d33e2306"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">KL</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq5.gif"/></alternatives></inline-formula> and a mesh smoothing loss <inline-formula id="IEq6"><alternatives><tex-math id="d33e2316">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{{\mathrm{S}}}$$\end{document}</tex-math><mml:math id="d33e2321"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq6.gif"/></alternatives></inline-formula>. The mesh reconstruction loss <inline-formula id="IEq7"><alternatives><tex-math id="d33e2331">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{{\mathrm{R}}}$$\end{document}</tex-math><mml:math id="d33e2336"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq4.gif"/></alternatives></inline-formula> is defined as the Chamfer distance between the reconstructed mesh sequence <inline-formula id="IEq8"><alternatives><tex-math id="d33e2346">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X}_{0:T-1}^{{\prime} }=({V}^{{\prime} },{E}^{{\prime} })$$\end{document}</tex-math><mml:math id="d33e2351"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq8.gif"/></alternatives></inline-formula> and the ground truth <italic>X</italic><sub>0:<italic>T</italic>&#x02212;1</sub>&#x02009;=&#x02009;(<italic>V</italic>, <italic>E</italic>), formulated as <inline-formula id="IEq9"><alternatives><tex-math id="d33e2397">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{{\mathrm{R}}}=\frac{1}{T}\mathop{\sum }\nolimits_{t = 0}^{T-1}{D}_{{\mathrm{cham}}}({V}_{t}^{{\prime} },{V}_{t})$$\end{document}</tex-math><mml:math id="d33e2402"><mml:mrow><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">cham</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq9.gif"/></alternatives></inline-formula>, where <italic>D</italic><sub>cham</sub> denotes the Chamber distance<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>, <inline-formula id="IEq10"><alternatives><tex-math id="d33e2462">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${V}_{t}^{{\prime} }$$\end{document}</tex-math><mml:math id="d33e2467"><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq10.gif"/></alternatives></inline-formula> and <italic>V</italic><sub><italic>t</italic></sub> denote the mesh vertex coordinates for the reconstruction and the ground truth, respectively:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="d33e2483">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${D}_{{\mathrm{cham}}}({V}_{t},{V}_{t}^{{\prime} })=\frac{1}{\left\vert {V}_{t}\right\vert }\sum _{{v}_{t}\in {V}_{t}}\mathop{\min }\limits_{{v}_{t}^{{\prime} }\in {V}_{t}^{{\prime} }}{\left\Vert {v}_{t}-{v}_{t}^{{\prime} }\right\Vert }_{2}+\frac{1}{\left\vert {V}_{t}^{{\prime} }\right\vert }\sum _{{v}_{t}^{{\prime} }\in {V}_{t}^{{\prime} }}\mathop{\min }\limits_{{v}_{t}\in {V}_{t}}{\left\Vert {v}_{t}^{{\prime} }-{v}_{t}\right\Vert }_{2}.$$\end{document}</tex-math><mml:math id="d33e2489"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">cham</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced close="&#x02223;" open="&#x02223;"><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mfenced close="&#x02225;" open="&#x02225;"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced close="&#x02223;" open="&#x02223;"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mfenced close="&#x02225;" open="&#x02225;"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="42256_2025_1035_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>In the VAE, the distribution of the latent space for <italic>z</italic><sub><italic>a</italic></sub> is encouraged to be close to a prior Gaussian distribution. The KL divergence is defined between the latent distribution and the Gaussian prior distribution. To control the trade-off between distribution fitting and diversity, we adopt the <italic>&#x003b2;</italic>-VAE formulation<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>. The KL loss <inline-formula id="IEq11"><alternatives><tex-math id="d33e2664">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{{\mathrm{KL}}}$$\end{document}</tex-math><mml:math id="d33e2669"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">KL</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq5.gif"/></alternatives></inline-formula> is formulated as<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="d33e2679">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{{\mathrm{KL}}}=\beta \cdot {\mathrm{KL}}({\mathcal{N}}(\;\mu ,\Sigma )\parallel {\mathcal{N}}(0,{\bf{1}})),$$\end{document}</tex-math><mml:math id="d33e2685"><mml:mrow><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">KL</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi mathvariant="normal">KL</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mspace width="0.16em"/><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">&#x003a3;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02225;</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">1</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="42256_2025_1035_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>which encourages the latent space <inline-formula id="IEq12"><alternatives><tex-math id="d33e2728">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{N}}(\;\mu ,\Sigma )$$\end{document}</tex-math><mml:math id="d33e2733"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mspace width="0.16em"/><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">&#x003a3;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq12.gif"/></alternatives></inline-formula> to be close to the prior Gaussian distribution <inline-formula id="IEq13"><alternatives><tex-math id="d33e2748">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{N}}(0,{\bf{I}})$$\end{document}</tex-math><mml:math id="d33e2753"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq13.gif"/></alternatives></inline-formula>.</p><p id="Par31">The Laplacian smoothing loss penalizes the difference between neighbouring vertices such as sharp changes on the mesh<sup><xref ref-type="bibr" rid="CR66">66</xref>,<xref ref-type="bibr" rid="CR67">67</xref></sup>. It is defined as<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="d33e2776">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{rcl}{{\mathcal{L}}}_{{\mathrm{S}}}&#x00026;=&#x00026;\frac{1}{T}\mathop{\sum }\limits_{t=0}^{T-1}{D}_{{\mathrm{smooth}}}({V}_{t}^{{\prime} },{E}_{t}^{{\prime} })\\ {D}_{{\mathrm{smooth}}}(V,E)&#x00026;=&#x00026;\mathop{\sum}\limits _{{v}_{i}\in V}\frac{1}{| V| }{\left\Vert\mathop{\sum}\limits_{j\in {N}_{i}}\frac{1}{| {N}_{i}| }({v}_{j}-{v}_{i})\right\Vert }_{2}\end{array},$$\end{document}</tex-math><mml:math id="d33e2782"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">smooth</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">smooth</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:munder><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02223;</mml:mo><mml:mi>V</mml:mi><mml:mo>&#x02223;</mml:mo></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mfenced close="&#x02225;" open="&#x02225;"><mml:mrow><mml:munder><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="42256_2025_1035_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <italic>N</italic><sub><italic>i</italic></sub> denotes the neighbouring vertices adjacent to <italic>v</italic><sub><italic>i</italic></sub>. The total loss function <italic>L</italic> is a weighted sum of the three loss terms<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="d33e2936">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{L}}={{\mathcal{L}}}_{{\mathrm{R}}}+{{\mathcal{L}}}_{{\mathrm{KL}}}+{\lambda }_{{\mathrm{s}}}\cdot {{\mathcal{L}}}_{{\mathrm{S}}}.$$\end{document}</tex-math><mml:math id="d33e2942"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">KL</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="42256_2025_1035_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par32">In terms of implementation, the mesh encoder <italic>M</italic><sub>enc</sub> has three GCN layers and one FC layer. The mesh decoder <italic>M</italic><sub>dec</sub> is composed of five FC layers. The transformer encoder <italic>T</italic><sub>enc</sub> and decoder <italic>T</italic><sub>dec</sub> consist of two layers, four attention heads, a feed-forward size of 1,024 and a dropout rate of 0.1. The latent vector dimensions for the mesh and condition were set to 64 and 32, respectively. The model contains approximately 69.71 million parameters and was trained on an NVIDIA RTX A6000 graphics processing unit (48&#x02009;GB) using the Adam optimizer with a fixed learning rate of 10<sup>&#x02212;4</sup> for 300 epochs. Training was performed with a batch size of one cardiac mesh sequence, consisting of 50 time frames. The cardiac mesh at each time frame consists of 22,043 vertices and 43,840 faces. The weights <italic>&#x003b2;</italic> and <italic>&#x003bb;</italic><sub>s</sub> in the loss function were empirically set to 0.01 and 1.</p></sec><sec id="Sec12"><title>Personalized normative model, latent vector and delta</title><p id="Par33">MeshHeart is trained on a large population of asymptomatic hearts. Once trained, it can be used as a personalized normative model to generate a synthetic mesh sequence of a normal heart with certain attributes <italic>c</italic>, including age, sex, weight and height. For each real heart, we can then compare the real cardiac mesh sequence with the synthetic normal mesh sequence of the same attributes, to understand the deviation of the real heart from its personalized normative pattern.</p><p id="Par34">To represent a cardiac mesh sequence in a low-dimensional latent space, we extract a latent vector after the transformer encoder <italic>T</italic><sub>enc</sub> but before the reparameterization step. The latent vector is calculated as the mean of the latent vectors at the transformer encoder output layer across 50 time frames. For calculating the latent delta, we quantify the deviation of the latent vector of the real heart to the latent vector of a group of synthetic hearts of the same attributes. Given conditions <italic>c</italic>, 100 samples of the latent variable <italic>z</italic><sub><italic>a</italic></sub> are drawn from a standard Gaussian distribution, <inline-formula id="IEq14"><alternatives><tex-math id="d33e3029">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${z}_{a} \sim {\mathcal{N}}({\bf{0}},{\bf{I}})$$\end{document}</tex-math><mml:math id="d33e3034"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">0</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="42256_2025_1035_Article_IEq14.gif"/></alternatives></inline-formula>, where <italic>z</italic><sub><italic>a</italic></sub> denotes the latent space after reparameterization in the VAE formulation. Each sample <italic>z</italic><sub><italic>a</italic></sub> is concatenated with the condition latent vector <italic>z</italic><sub><italic>c</italic></sub> and passed through the transformer decoder and mesh decoder to generate a synthetic cardiac mesh sequence. After synthetic mesh generation, each synthetic mesh sequence is provided to the mesh encoder <italic>M</italic><sub>enc</sub> and transformer encoder <italic>T</italic><sub>enc</sub>, to generate latent vectors across 50 time frames at the transformer output later, subsequently averaged to form the latent vector <italic>z</italic><sup>synth</sup>. The real heart mesh sequence is provided to the mesh encoder <italic>M</italic><sub>enc</sub> and transformer encoder <italic>T</italic><sub>enc</sub> for calculating the latent vector <italic>z</italic><sup>real</sup> in the same manner.</p><p id="Par35">With the latent vector <italic>z</italic><sup>real</sup> for the real heart and the latent vector <italic>z</italic><sup>synth</sup> for the synthetic heart, we define the latent vector as the Euclidean distance between <italic>z</italic><sup>real</sup> and <italic>z</italic><sup>synth</sup>. As we draw 100 synthetic samples to represent a subpopulation with the same attributes, the latent delta &#x00394;<italic>z</italic> is defined as<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="d33e3122">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta z={\left\Vert {z}^{{\rm{real}}}-\frac{1}{100}\mathop{\sum }\limits_{i = 1}^{100}{z}_{i}^{{\rm{synth}}}\right\Vert }_{2},$$\end{document}</tex-math><mml:math id="d33e3128"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mfenced close="&#x02225;" open="&#x02225;"><mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">real</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">synth</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="42256_2025_1035_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>where <italic>i</italic> denotes the sample index. The latent delta &#x00394;<italic>z</italic> provides a robust metric to evaluate individual differences in cardiac structure and motion, quantifying the deviation of the real heart from its personalized normative model.</p></sec><sec id="Sec13"><title>Data and experiments</title><p id="Par36">This study used a dataset of 38,309 participants obtained from the UK Biobank<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. Each participant underwent cine cardiac MR (CMR) imaging scans. From the cine CMR images, a 3D mesh sequence is derived to describe the shape and motion of the heart. The mesh sequence covers three anatomical structures, LV, Myo and RV. Each sequence contains 50 time frames over the course of a cardiac cycle. To derive cardiac meshes from the CMR images, automated segmentation<sup><xref ref-type="bibr" rid="CR68">68</xref></sup> was applied to the images. The resulting segmentations were enhanced using an atlas-base approach<sup><xref ref-type="bibr" rid="CR69">69</xref></sup>, by registering multiple high-resolution cardiac atlases<sup><xref ref-type="bibr" rid="CR69">69</xref>,<xref ref-type="bibr" rid="CR70">70</xref></sup> onto the segmentations followed by label fusion, resulting in high-resolution segmentations. A 3D template mesh<sup><xref ref-type="bibr" rid="CR70">70</xref></sup> was then fitted to the high-resolution segmentations at the ED and ES frames using non-rigid image registration, generating ED and ES cardiac meshes. Subsequently, motion tracking was performed using Deepali<sup><xref ref-type="bibr" rid="CR71">71</xref></sup>, a graphics-processing-unit-accelerated version of the non-rigid registration toolbox MIRTK<sup><xref ref-type="bibr" rid="CR72">72</xref></sup>, on cardiac segmentations across the time frames. Deformation fields were derived using a free-form deformation model with a control point spacing of [8, 8, 8]. The registration objective function included Dice similarity as the primary similarity metric and B-spline bending energy regularization with a weight of 0.01. The deformation fields were derived between time frames and applied to propagate the ED mesh and ES mesh across the cardiac cycle. The proposed meshes were averaged using weighted interpolation based on temporal proximity to ED and ES<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> to ensure temporal smoothness of the resulting mesh sequence. All cardiac meshes maintained the same geometric structure.</p><p id="Par37">The dataset was partitioned into training, validation and test sets for developing the MeshHeart model and a clinical analysis set for evaluating its performance for disease classification task. In brief, MeshHeart was trained on 15,000 healthy participants from the Cheadle imaging centre. For parameter tuning and performance evaluation, MeshHeart was evaluated on a validation set of 2,000 and a test set of 4,000 healthy participants, from three different sites, Cheadle, Reading and Newcastle centres. For clinical analysis, including performing the disease classification study and latent delta PheWAS, we used a separate set of 17,309 participants from the three imaging centres, including 7,178 healthy participants and 10,131 participants with cardiac diseases and hypertension. PheWAS was undertaken using the PheWAS R package with clinical outcomes and coded phenotypes converted to 1,163 categorical PheCodes. <italic>P</italic> values were deemed significant with Bonferroni adjustment for the number of PheCodes. The details of the dataset split and the definition of disease code are described in Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>.</p></sec><sec id="Sec14"><title>Method comparison</title><p id="Par38">To compare the generation performance of MeshHeart, we adapt three state-of-the-art generative models originally proposed for other tasks: (1) Action2Motion<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, originally developed for human motion generation; (2) ACTOR<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, developed for human pose and motion generation; and (3) CHeart<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, developed for the generation of cardiac segmentation maps, instead of cardiac meshes. We modified these models to adapt to the cardiac mesh generation task.</p></sec></sec><sec id="Sec15" sec-type="supplementary-material"><title>Supplementary information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="42256_2025_1035_MOESM1_ESM.pdf"><label>Supplementary Information</label><caption><p>Supplementary Tables 1&#x02013;8 and Figs. 1&#x02013;4.</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s42256-025-01035-5.</p></sec><ack><title>Acknowledgements</title><p>This research was conducted using the UK Biobank Resource under Application Number 18545. Images were reproduced with kind permission of UK Biobank. We thank all UK Biobank participants and staff. We also thank W. Zhang for helpful discussions on the methodology. This work is supported by EPSRC DeepGeM Grant (grant no. EP/W01842X/1) and the BHF New Horizons Grant (grant no. NH/F/23/70013). K.A.M. is supported by the British Heart Foundation (grant nos. FS/IPBSRF/22/27059 and RE/18/4/34215) and the NIHR Imperial College Biomedical Research Centre. S.W. is supported by Shanghai Sailing Program (grant no. 22YF1409300), CCF-Baidu Open Fund (grant no. CCF-BAIDU 202316) and International Science and Technology Cooperation Program under the 2023 Shanghai Action Plan for Science (grant no. 23410710400). P.M.M. acknowledges generous personal support from the Edmond J. Safra Foundation and L. Safra, an NIHR Senior Investigator Award and the UK Dementia Research Institute, which is funded predominantly by UKRI Medical Research Council. D.P.O. is supported by the Medical Research Council (grant no. MC_UP_1605/13), National Institute for Health Research (NIHR) Imperial College Biomedical Research Centre and the British Heart Foundation (grant nos. RG/19/6/34387, RE/24/130023 and CH/P/23/80008).</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>M.Q. and W.B. conceived the study. M.Q. conducted the experiments shown in Figs. 2&#x02013;5 (mesh reconstruction accuracy, generation performance, disease classification and latent vector analysis). K.A.M. conducted the experiment described in Fig. 5b (PheWAS). M.Q. and W.B. analysed the results. D.P.O. and P.M.M. provided data resources. All authors reviewed the manuscript.</p></notes><notes notes-type="peer-review"><title>Peer review</title><sec id="FPar1"><title>Peer review information</title><p id="Par39"><italic>Nature Machine Intelligence</italic> thanks Tanveer Syeda-Mahmood, Alistair Young and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p></sec></notes><notes notes-type="data-availability"><title>Data availability</title><p>The raw imaging data and non-imaging participant characteristics are available from UK Biobank to approved researchers via a standard application process at <ext-link ext-link-type="uri" xlink:href="http://www.ukbiobank.ac.uk/register-apply">http://www.ukbiobank.ac.uk/register-apply</ext-link>.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The code for this research is available via GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/MengyunQ/MeshHeart">https://github.com/MengyunQ/MeshHeart</ext-link> and via Zenodo at 10.5281/zenodo.15122485 (ref. <sup><xref ref-type="bibr" rid="CR73">73</xref></sup>).</p></notes><notes id="FPar2" notes-type="COI-statement"><title>Competing interests</title><p id="Par40">D.P.O. has consulted for Bayer AG and Bristol-Myers-Squibb. K.A.M. has consulted for Checkpoint Capital LP. None of these activities is directly related to the work presented here. P.M.M. has received consultancy or speaker fees from Roche, Merck, Biogen, Rejuveron, Sangamo, Nodthera and Novartis. P.M.M. has received research or educational funds from Biogen, Novartis, Merck and GlaxoSmithKline. The other authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Sanz</surname><given-names>J</given-names></name><name><surname>Fayad</surname><given-names>ZA</given-names></name></person-group><article-title>Imaging of atherosclerotic cardiovascular disease</article-title><source>Nature</source><year>2008</year><volume>451</volume><fpage>953</fpage><lpage>957</lpage><pub-id pub-id-type="doi">10.1038/nature06803</pub-id><pub-id pub-id-type="pmid">18288186</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Sanz, J. &#x00026; Fayad, Z. A. Imaging of atherosclerotic cardiovascular disease. <italic>Nature</italic><bold>451</bold>, 953&#x02013;957 (2008).<pub-id pub-id-type="pmid">18288186</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Aung</surname><given-names>N</given-names></name><etal/></person-group><article-title>Genome-wide analysis of left ventricular image-derived phenotypes identifies fourteen loci associated with cardiac morphogenesis and heart failure development</article-title><source>Circulation</source><year>2019</year><volume>140</volume><fpage>1318</fpage><lpage>1330</lpage><pub-id pub-id-type="doi">10.1161/CIRCULATIONAHA.119.041161</pub-id><pub-id pub-id-type="pmid">31554410</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Aung, N. et al. Genome-wide analysis of left ventricular image-derived phenotypes identifies fourteen loci associated with cardiac morphogenesis and heart failure development. <italic>Circulation</italic><bold>140</bold>, 1318&#x02013;1330 (2019).<pub-id pub-id-type="pmid">31554410</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Bonazzola</surname><given-names>R</given-names></name><etal/></person-group><article-title>Unsupervised ensemble-based phenotyping enhances discoverability of genes related to left-ventricular morphology</article-title><source>Nat. Mach. Intell.</source><year>2024</year><volume>6</volume><fpage>291</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1038/s42256-024-00801-1</pub-id><pub-id pub-id-type="pmid">38523678</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Bonazzola, R. et al. Unsupervised ensemble-based phenotyping enhances discoverability of genes related to left-ventricular morphology. <italic>Nat. Mach. Intell.</italic><bold>6</bold>, 291&#x02013;306 (2024).<pub-id pub-id-type="pmid">38523678</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>HV</given-names></name><etal/></person-group><article-title>Genetic and functional insights into the fractal structure of the heart</article-title><source>Nature</source><year>2020</year><volume>584</volume><fpage>589</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2635-8</pub-id><pub-id pub-id-type="pmid">32814899</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Meyer, H. V. et al. Genetic and functional insights into the fractal structure of the heart. <italic>Nature</italic><bold>584</bold>, 589&#x02013;594 (2020).<pub-id pub-id-type="pmid">32814899</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>GH</given-names></name><name><surname>Uriel</surname><given-names>N</given-names></name><name><surname>Burkhoff</surname><given-names>D</given-names></name></person-group><article-title>Reverse remodelling and myocardial recovery in heart failure</article-title><source>Nat. Rev. Cardiol.</source><year>2018</year><volume>15</volume><fpage>83</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1038/nrcardio.2017.139</pub-id><pub-id pub-id-type="pmid">28933783</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Kim, G. H., Uriel, N. &#x00026; Burkhoff, D. Reverse remodelling and myocardial recovery in heart failure. <italic>Nat. Rev. Cardiol.</italic><bold>15</bold>, 83&#x02013;96 (2018).<pub-id pub-id-type="pmid">28933783</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Bello, G. A. et al. Deep-learning cardiac motion analysis for human survival prediction. <italic>Nat. Mach. Intell.</italic>10.1038/s42256-019-0019-2 (2019).</mixed-citation></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Puyol-Anton</surname><given-names>E</given-names></name><etal/></person-group><article-title>A multimodal spatiotemporal cardiac motion atlas from MR and ultrasound data</article-title><source>Med. Image Anal.</source><year>2017</year><volume>40</volume><fpage>94</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.media.2017.06.002</pub-id></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Puyol-Anton, E. et al. A multimodal spatiotemporal cardiac motion atlas from MR and ultrasound data. <italic>Med. Image Anal.</italic><bold>40</bold>, 94&#x02013;110 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Duchateau, N., King, A. P. &#x00026; De Craene, M. Machine learning approaches for myocardial motion and deformation analysis. <italic>Front. Cardiovasc. Med.</italic>10.3389/fcvm.2019.00190 (2020).</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Bai, W. et al. A population-based phenome-wide association study of cardiac and aortic structure and function. <italic>Nat. Med.</italic>10.1038/s41591-020-1009-y (2020).</mixed-citation></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Corral-Acero</surname><given-names>J</given-names></name><etal/></person-group><article-title>The &#x02018;Digital Twin&#x02019; to enable the vision of precision cardiology</article-title><source>Eur. Heart J.</source><year>2020</year><volume>41</volume><fpage>4556</fpage><lpage>4564</lpage><pub-id pub-id-type="doi">10.1093/eurheartj/ehaa159</pub-id><pub-id pub-id-type="pmid">32128588</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Corral-Acero, J. et al. The &#x02018;Digital Twin&#x02019; to enable the vision of precision cardiology. <italic>Eur. Heart J.</italic><bold>41</bold>, 4556&#x02013;4564 (2020).<pub-id pub-id-type="pmid">32128588</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Schulz-Menger, J. et al. Standardized image interpretation and post-processing in cardiovascular magnetic resonance-2020 update: Society for Cardiovascular Magnetic Resonance (SCMR): Board of Trustees Task Force on Standardized Post-processing. <italic>J. Cardiovasc. Magn. Reson.</italic>10.1186/s12968-020-00610-6 (2020).</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Hundley, W. G. et al. Society for Cardiovascular Magnetic Resonance (SCMR) guidelines for reporting cardiovascular magnetic resonance examinations. <italic>J. Cardiovasc. Magn. Reson.</italic>10.1186/s12968-021-00827-z (2022).</mixed-citation></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Gasparovici</surname><given-names>A</given-names></name><name><surname>Serban</surname><given-names>A</given-names></name></person-group><article-title>Generative 3D cardiac shape modelling for in-silico trials</article-title><source>Stud. Health Technol. Informatics</source><year>2024</year><volume>321</volume><fpage>190</fpage><lpage>194</lpage></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Gasparovici, A. &#x00026; Serban, A. Generative 3D cardiac shape modelling for in-silico trials. <italic>Stud. Health Technol. Informatics</italic><bold>321</bold>, 190&#x02013;194 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Piras</surname><given-names>P</given-names></name><etal/></person-group><article-title>Morphologically normalized left ventricular motion indicators from mri feature tracking characterize myocardial infarction</article-title><source>Sci. Rep.</source><year>2017</year><volume>25</volume><fpage>12259</fpage><pub-id pub-id-type="doi">10.1038/s41598-017-12539-5</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Piras, P. et al. Morphologically normalized left ventricular motion indicators from mri feature tracking characterize myocardial infarction. <italic>Sci. Rep.</italic><bold>25</bold>, 12259 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>K</given-names></name><etal/></person-group><article-title>Independent left ventricular morphometric atlases show consistent relationships with cardiovascular risk factors: a UK Biobank study</article-title><source>Sci. Rep.</source><year>2019</year><volume>9</volume><fpage>1130</fpage><pub-id pub-id-type="doi">10.1038/s41598-018-37916-6</pub-id><pub-id pub-id-type="pmid">30718635</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Gilbert, K. et al. Independent left ventricular morphometric atlases show consistent relationships with cardiovascular risk factors: a UK Biobank study. <italic>Sci. Rep.</italic><bold>9</bold>, 1130 (2019).<pub-id pub-id-type="pmid">30718635</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Mauger</surname><given-names>CA</given-names></name><etal/></person-group><article-title>Multi-ethnic study of atherosclerosis: relationship between left ventricular shape at cardiac mri and 10-year outcomes</article-title><source>Radiology</source><year>2022</year><volume>306</volume><fpage>e220122</fpage><pub-id pub-id-type="doi">10.1148/radiol.220122</pub-id><pub-id pub-id-type="pmid">36125376</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Mauger, C. A. et al. Multi-ethnic study of atherosclerosis: relationship between left ventricular shape at cardiac mri and 10-year outcomes. <italic>Radiology</italic><bold>306</bold>, e220122 (2022).<pub-id pub-id-type="pmid">36125376</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Qi</surname><given-names>H</given-names></name><etal/></person-group><article-title>Non-rigid respiratory motion estimation of whole-heart coronary mr images using unsupervised deep learning</article-title><source>IEEE Trans. Med. Imag.</source><year>2020</year><volume>40</volume><fpage>444</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1109/TMI.2020.3029205</pub-id></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Qi, H. et al. Non-rigid respiratory motion estimation of whole-heart coronary mr images using unsupervised deep learning. <italic>IEEE Trans. Med. Imag.</italic><bold>40</bold>, 444&#x02013;454 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>M</given-names></name><etal/></person-group><article-title>Sequencemorph: a unified unsupervised learning framework for motion tracking on cardiac image sequences</article-title><source>IEEE Trans. Pattern. Anal. Mach. Intell.</source><year>2023</year><volume>45</volume><fpage>10409</fpage><lpage>10426</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2023.3243040</pub-id><pub-id pub-id-type="pmid">37022840</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">Ye, M. et al. Sequencemorph: a unified unsupervised learning framework for motion tracking on cardiac image sequences. <italic>IEEE Trans. Pattern. Anal. Mach. Intell.</italic><bold>45</bold>, 10409&#x02013;10426 (2023).<pub-id pub-id-type="pmid">37022840</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Suinesiaputra</surname><given-names>A</given-names></name><etal/></person-group><article-title>Statistical shape modeling of the left ventricle: myocardial infarct classification challenge</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2017</year><volume>22</volume><fpage>503</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2017.2652449</pub-id><pub-id pub-id-type="pmid">28103561</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Suinesiaputra, A. et al. Statistical shape modeling of the left ventricle: myocardial infarct classification challenge. <italic>IEEE J. Biomed. Health Inform.</italic><bold>22</bold>, 503&#x02013;515 (2017).<pub-id pub-id-type="pmid">28103561</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>Q</given-names></name><name><surname>Delingette</surname><given-names>H</given-names></name><name><surname>Ayache</surname><given-names>N</given-names></name></person-group><article-title>Explainable cardiac pathology classification on cine MRI with motion characterization by semi-supervised learning of apparent flow</article-title><source>Med. Image Anal.</source><year>2019</year><volume>56</volume><fpage>80</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1016/j.media.2019.06.001</pub-id><pub-id pub-id-type="pmid">31200290</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">Zheng, Q., Delingette, H. &#x00026; Ayache, N. Explainable cardiac pathology classification on cine MRI with motion characterization by semi-supervised learning of apparent flow. <italic>Med. Image Anal.</italic><bold>56</bold>, 80&#x02013;95 (2019).<pub-id pub-id-type="pmid">31200290</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Kawel-Boehm</surname><given-names>N</given-names></name><etal/></person-group><article-title>Reference ranges (&#x02018;normal values&#x02019;) for cardiovascular magnetic resonance (cmr) in adults and children: 2020 update</article-title><source>J. Cardiovasc. Magn. Reson.</source><year>2020</year><volume>22</volume><fpage>63</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1186/s12968-020-00683-3</pub-id><pub-id pub-id-type="pmid">32892751</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Kawel-Boehm, N. et al. Reference ranges (&#x02018;normal values&#x02019;) for cardiovascular magnetic resonance (cmr) in adults and children: 2020 update. <italic>J. Cardiovasc. Magn. Reson.</italic><bold>22</bold>, 63&#x02013;87 (2020).<pub-id pub-id-type="pmid">32892751</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>J</given-names></name><etal/></person-group><article-title>Get3d: a generative model of high quality 3D textured shapes learned from images</article-title><source>Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>31841</fpage><lpage>31854</lpage></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Gao, J. et al. Get3d: a generative model of high quality 3D textured shapes learned from images. <italic>Neural Inf. Process. Syst.</italic><bold>35</bold>, 31841&#x02013;31854 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Xue, Y., Li, Y., Singh, K. K. &#x00026; Lee, Y. J. Giraffe HD: a high-resolution 3D-aware generative model. In <italic>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 18440&#x02013;18449 (IEEE, 2022).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Kim, G. &#x00026; Chun, S. Y. DATID-3D: diversity-preserved domain adaptation using text-to-image diffusion for 3D generative model. In <italic>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 14203&#x02013;14213 (IEEE, 2023).</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Petrovich, M., Black, M. J. &#x00026; Varol, G. TEMOS: Generating diverse human motions from textual descriptions. In <italic>European Conference on Computer Vision</italic> 480&#x02013;497 (Springer, 2022).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Athanasiou, N., Petrovich, M., Black, M. J. &#x00026; Varol, G. TEACH: temporal action composition for 3D humans. In <italic>International Conference on 3D Vision</italic> 414&#x02013;423 (IEEE, 2022).</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Petrovich, M., Black, M. J. &#x00026; Varol, G. Action-conditioned 3D human motion synthesis with Transformer VAE. In <italic>International Conference on Computer Vision</italic> 10985&#x02013;10995 (IEEE, 2021).</mixed-citation></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Swanson</surname><given-names>K</given-names></name><etal/></person-group><article-title>Generative AI for designing and validating easily synthesizable and structurally novel antibiotics</article-title><source>Nat. Mach. Intell.</source><year>2024</year><volume>6</volume><fpage>338</fpage><lpage>353</lpage><pub-id pub-id-type="doi">10.1038/s42256-024-00809-7</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Swanson, K. et al. Generative AI for designing and validating easily synthesizable and structurally novel antibiotics. <italic>Nat. Mach. Intell.</italic><bold>6</bold>, 338&#x02013;353 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Jiang, Y. et al. Pocketflow is a data-and-knowledge-driven structure-based molecular generative model. <italic>Nat. Mach. Intell.</italic>10.1038/s42256-024-00808-8 (2024).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Kong, F. et al. Sdf4chd: Generative modeling of cardiac anatomies with congenital heart defects. <italic>Med. Image Anal.</italic>10.1016/j.media.2024.103293 (2024).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Wang, S. et al. Deep generative model-based quality control for cardiac MRI segmentation. In <italic>Medical Image Computing and Computer Assisted Intervention&#x02013;MICCAI 2020: 23rd International Conference, Lima, Peru, October 4&#x02013;8, 2020, Proceedings, Part IV 23</italic> 88&#x02013;97 (Springer, 2020).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Vukadinovic, M., Kwan, A. C., Li, D. &#x00026; Ouyang, D. GANcMRI: cardiac magnetic resonance video generation and physiologic guidance using latent space prompting. In <italic>Machine Learning for Health (ML4H)</italic> 594&#x02013;606 (PMLR, 2023).</mixed-citation></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>G&#x000f3;mez</surname><given-names>S</given-names></name><name><surname>Romo-Bucheli</surname><given-names>D</given-names></name><name><surname>Mart&#x000ed;nez</surname><given-names>F</given-names></name></person-group><article-title>A digital cardiac disease biomarker from a generative progressive cardiac cine-MRI representation</article-title><source>Biomed. Eng. Lett.</source><year>2022</year><volume>12</volume><fpage>75</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1007/s13534-021-00212-w</pub-id><pub-id pub-id-type="pmid">35186361</pub-id>
</element-citation><mixed-citation id="mc-CR33" publication-type="journal">G&#x000f3;mez, S., Romo-Bucheli, D. &#x00026; Mart&#x000ed;nez, F. A digital cardiac disease biomarker from a generative progressive cardiac cine-MRI representation. <italic>Biomed. Eng. Lett.</italic><bold>12</bold>, 75&#x02013;84 (2022).<pub-id pub-id-type="pmid">35186361</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Muffoletto, M. et al. Combining generative modelling and semi-supervised domain adaptation for whole heart cardiovascular magnetic resonance angiography segmentation. <italic>J. Cardiovasc. Magn. Reson.</italic>10.1186/s12968-023-00981-6 (2023).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Xia, Y. et al. Automatic 3D+t four-chamber CMR quantification of the UK Biobank: integrating imaging and non-imaging data priors at scale. <italic>Med. Image Anal.</italic>10.1016/j.media.2022.102498 (2022).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Gaggion, N. et al. Multi-view hybrid graph convolutional network for volume-to-mesh reconstruction in cardiovascular MRI. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2311.13706">https://arxiv.org/abs/2311.13706</ext-link> (2023).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Dou, H., Ravikumar, N. &#x00026; Frangi, A. F. A conditional flow variational autoencoder for controllable synthesis of virtual populations of anatomy. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic> 142&#x02013;152 (Springer, 2023).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Dou, H., Virtanen, S., Ravikumar, N. &#x00026; Frangi, A. F. A generative shape compositional framework to synthesize populations of virtual chimeras. <italic>IEEE Transactions on Neural Networks and Learning Systems</italic><bold>36</bold>, 4750&#x02013;4764 (2024).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Beetz, M. et al. Interpretable cardiac anatomy modeling using variational mesh autoencoders. <italic>Front. Cardiovasc. Med.</italic>10.3389/fcvm.2022.983868 (2022).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Beetz, M., Banerjee, A. &#x00026; Grau, V. Generating subpopulation-specific biventricular anatomy models using conditional point cloud variational autoencoders. In <italic>International Workshop on Statistical Atlases and Computational Models of the Heart</italic> 75&#x02013;83 (Springer, 2021).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Campello, V. M. et al. Cardiac aging synthesis from cross-sectional data with conditional generative adversarial networks. <italic>Front. Cardiovasc. Med.</italic>10.3389/fcvm.2022.983091 (2022).</mixed-citation></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>M</given-names></name><etal/></person-group><article-title>CHeart: a conditional spatio-temporal generative model for cardiac anatomy</article-title><source>IEEE Trans. Med. Imag.</source><year>2024</year><volume>43</volume><fpage>1259</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1109/TMI.2023.3331982</pub-id></element-citation><mixed-citation id="mc-CR42" publication-type="journal">Qiao, M. et al. CHeart: a conditional spatio-temporal generative model for cardiac anatomy. <italic>IEEE Trans. Med. Imag.</italic><bold>43</bold>, 1259&#x02013;1269 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Reynaud, H. et al. D&#x02019;ARTAGNAN: counterfactual video generation. In <italic>Medical Image Computing and Computer Assisted Intervention</italic> 599&#x02013;609 (Springer, 2022).</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Gilbert, A. et al. Generating synthetic labeled data from existing anatomical models: an example with echocardiography segmentation. <italic>IEEE Trans. Med. Imag.</italic>10.1109/tmi.2021.3051806 (2021).</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Kipf, T. N. &#x00026; Welling, M. Semi-supervised classification with graph convolutional networks. In <italic>International Conference on Learning Representations 2017</italic><ext-link ext-link-type="uri" xlink:href="https://openreview.net/pdf?id=SJU4ayYgl">https://openreview.net/pdf?id=SJU4ayYgl</ext-link> (ICLR, 2017).</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Petersen, S. E. et al. UK Biobank&#x02019;s cardiovascular magnetic resonance protocol. <italic>J. Cardiovasc. Magn. Reson.</italic>10.1186/s12968-016-0227-4 (2015).</mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Guo, C. et al. Action2Motion: conditioned generation of 3D human motions. In <italic>Proc.</italic><italic>28th ACM International Conference on Multimedia</italic>10.1145/3394171.3413635 (ACM, 2020).</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Cover, T. M. <italic>Elements of Information Theory</italic> (John Wiley &#x00026; Sons, 1999).</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Arjovsky, M., Chintala, S. &#x00026; Bottou, L. Wasserstein generative adversarial networks. In <italic>International Conference on Machine Learning</italic> 214&#x02013;223 (PMLR, 2017).</mixed-citation></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Maaten</surname><given-names>L</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Visualizing data using t-SNE</article-title><source>J. Mach. Learn. Res.</source><year>2008</year><volume>9</volume><fpage>2579</fpage><lpage>2605</lpage></element-citation><mixed-citation id="mc-CR50" publication-type="journal">Van der Maaten, L. &#x00026; Hinton, G. Visualizing data using t-SNE. <italic>J. Mach. Learn. Res.</italic><bold>9</bold>, 2579&#x02013;2605 (2008).</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name><surname>Fukuta</surname><given-names>H</given-names></name><name><surname>Little</surname><given-names>WC</given-names></name></person-group><article-title>The cardiac cycle and the physiologic basis of left ventricular contraction, ejection, relaxation, and filling</article-title><source>Heart Fail. Clin.</source><year>2008</year><volume>4</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.hfc.2007.10.004</pub-id><pub-id pub-id-type="pmid">18313620</pub-id>
</element-citation><mixed-citation id="mc-CR51" publication-type="journal">Fukuta, H. &#x00026; Little, W. C. The cardiac cycle and the physiologic basis of left ventricular contraction, ejection, relaxation, and filling. <italic>Heart Fail. Clin.</italic><bold>4</bold>, 1&#x02013;11 (2008).<pub-id pub-id-type="pmid">18313620</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Nethononda, R. M. et al. Gender specific patterns of age-related decline in aortic stiffness: a cardiovascular magnetic resonance study including normal ranges. <italic>J. Cardiovasc. Magn. Reson.</italic>10.1186/s12968-015-0126-0 (2015).</mixed-citation></ref><ref id="CR53"><label>53.</label><citation-alternatives><element-citation id="ec-CR53" publication-type="journal"><person-group person-group-type="author"><name><surname>Heckbert</surname><given-names>SR</given-names></name><etal/></person-group><article-title>Traditional cardiovascular risk factors in relation to left ventricular mass, volume, and systolic function by cardiac magnetic resonance imaging: the multiethnic study of atherosclerosis</article-title><source>J. Am. Coll. Cardiol.</source><year>2006</year><volume>48</volume><fpage>2285</fpage><lpage>2292</lpage><pub-id pub-id-type="doi">10.1016/j.jacc.2006.03.072</pub-id><pub-id pub-id-type="pmid">17161261</pub-id>
</element-citation><mixed-citation id="mc-CR53" publication-type="journal">Heckbert, S. R. et al. Traditional cardiovascular risk factors in relation to left ventricular mass, volume, and systolic function by cardiac magnetic resonance imaging: the multiethnic study of atherosclerosis. <italic>J. Am. Coll. Cardiol.</italic><bold>48</bold>, 2285&#x02013;2292 (2006).<pub-id pub-id-type="pmid">17161261</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR54"><label>54.</label><citation-alternatives><element-citation id="ec-CR54" publication-type="journal"><person-group person-group-type="author"><name><surname>Ortega</surname><given-names>FB</given-names></name><name><surname>Lavie</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>SN</given-names></name></person-group><article-title>Obesity and cardiovascular disease</article-title><source>Circ. Res.</source><year>2016</year><volume>118</volume><fpage>1752</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1161/CIRCRESAHA.115.306883</pub-id><pub-id pub-id-type="pmid">27230640</pub-id>
</element-citation><mixed-citation id="mc-CR54" publication-type="journal">Ortega, F. B., Lavie, C. J. &#x00026; Blair, S. N. Obesity and cardiovascular disease. <italic>Circ. Res.</italic><bold>118</bold>, 1752&#x02013;1770 (2016).<pub-id pub-id-type="pmid">27230640</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR55"><label>55.</label><citation-alternatives><element-citation id="ec-CR55" publication-type="journal"><person-group person-group-type="author"><name><surname>Ormazabal</surname><given-names>V</given-names></name><etal/></person-group><article-title>Association between insulin resistance and the development of cardiovascular disease</article-title><source>Cardiovasc. Diabetol.</source><year>2018</year><volume>17</volume><fpage>122</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1186/s12933-018-0762-4</pub-id><pub-id pub-id-type="pmid">30170598</pub-id>
</element-citation><mixed-citation id="mc-CR55" publication-type="journal">Ormazabal, V. et al. Association between insulin resistance and the development of cardiovascular disease. <italic>Cardiovasc. Diabetol.</italic><bold>17</bold>, 122&#x02013;136 (2018).<pub-id pub-id-type="pmid">30170598</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR56"><label>56.</label><citation-alternatives><element-citation id="ec-CR56" publication-type="journal"><person-group person-group-type="author"><name><surname>Ford</surname><given-names>ES</given-names></name><name><surname>Greenlund</surname><given-names>KJ</given-names></name><name><surname>Hong</surname><given-names>Y</given-names></name></person-group><article-title>Ideal cardiovascular health and mortality from all causes and diseases of the circulatory system among adults in the united states</article-title><source>Circulation</source><year>2012</year><volume>125</volume><fpage>987</fpage><lpage>995</lpage><pub-id pub-id-type="doi">10.1161/CIRCULATIONAHA.111.049122</pub-id><pub-id pub-id-type="pmid">22291126</pub-id>
</element-citation><mixed-citation id="mc-CR56" publication-type="journal">Ford, E. S., Greenlund, K. J. &#x00026; Hong, Y. Ideal cardiovascular health and mortality from all causes and diseases of the circulatory system among adults in the united states. <italic>Circulation</italic><bold>125</bold>, 987&#x02013;995 (2012).<pub-id pub-id-type="pmid">22291126</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Binu, A. J. et al. The heart of the matter: cardiac manifestations of endocrine disease. <italic>Ind. J. Endocrinol. Metab.</italic><bold>21</bold>, 919&#x02013;925 (2017).</mixed-citation></ref><ref id="CR58"><label>58.</label><citation-alternatives><element-citation id="ec-CR58" publication-type="journal"><person-group person-group-type="author"><name><surname>Bhatnagar</surname><given-names>A</given-names></name></person-group><article-title>Environmental determinants of cardiovascular disease</article-title><source>Circ. Res.</source><year>2017</year><volume>121</volume><fpage>162</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1161/CIRCRESAHA.117.306458</pub-id><pub-id pub-id-type="pmid">28684622</pub-id>
</element-citation><mixed-citation id="mc-CR58" publication-type="journal">Bhatnagar, A. Environmental determinants of cardiovascular disease. <italic>Circ. Res.</italic><bold>121</bold>, 162&#x02013;180 (2017).<pub-id pub-id-type="pmid">28684622</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR59"><label>59.</label><citation-alternatives><element-citation id="ec-CR59" publication-type="journal"><person-group person-group-type="author"><name><surname>Mann</surname><given-names>DL</given-names></name><name><surname>Bristow</surname><given-names>MR</given-names></name></person-group><article-title>Mechanisms and models in heart failure: the biomechanical model and beyond</article-title><source>Circulation</source><year>2005</year><volume>111</volume><fpage>2837</fpage><lpage>2849</lpage><pub-id pub-id-type="doi">10.1161/CIRCULATIONAHA.104.500546</pub-id><pub-id pub-id-type="pmid">15927992</pub-id>
</element-citation><mixed-citation id="mc-CR59" publication-type="journal">Mann, D. L. &#x00026; Bristow, M. R. Mechanisms and models in heart failure: the biomechanical model and beyond. <italic>Circulation</italic><bold>111</bold>, 2837&#x02013;2849 (2005).<pub-id pub-id-type="pmid">15927992</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR60"><label>60.</label><citation-alternatives><element-citation id="ec-CR60" publication-type="journal"><person-group person-group-type="author"><name><surname>Trayanova</surname><given-names>NA</given-names></name></person-group><article-title>Whole-heart modeling: applications to cardiac electrophysiology and electromechanics</article-title><source>Circ. Res.</source><year>2011</year><volume>108</volume><fpage>113</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1161/CIRCRESAHA.110.223610</pub-id><pub-id pub-id-type="pmid">21212393</pub-id>
</element-citation><mixed-citation id="mc-CR60" publication-type="journal">Trayanova, N. A. Whole-heart modeling: applications to cardiac electrophysiology and electromechanics. <italic>Circ. Res.</italic><bold>108</bold>, 113&#x02013;128 (2011).<pub-id pub-id-type="pmid">21212393</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="other">Mauger, C. et al. Right ventricular shape and function: cardiovascular magnetic resonance reference morphology and biventricular risk factor morphometrics in UK Biobank. <italic>J. Cardiovasc. Magn. Reson.</italic>10.1186/s12968-019-0551-6 (2019).</mixed-citation></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="other">Dosovitskiy, A. et al. An image is worth 16 &#x000d7; 16 words: transformers for image recognition at scale. In <italic>International Conference on Learning Representations</italic><italic>2021</italic><ext-link ext-link-type="uri" xlink:href="https://openreview.net/pdf?id=YicbFdNTTy">https://openreview.net/pdf?id=YicbFdNTTy</ext-link> (ICLR, 2021).</mixed-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Kingma, D. P. &#x00026; Welling, M. Auto-encoding variational Bayes. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</ext-link> (2013).</mixed-citation></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="other">Higgins, I. et al. <italic>&#x003b2;</italic>-VAE: learning basic visual concepts with a constrained variational framework. In <italic>International Conference on Learning Representations 2017</italic><ext-link ext-link-type="uri" xlink:href="https://openreview.net/pdf?id=Sy2fzU9gl">https://openreview.net/pdf?id=Sy2fzU9gl</ext-link> (ICLR, 2017).</mixed-citation></ref><ref id="CR65"><label>65.</label><mixed-citation publication-type="other">Fan, H., Su, H. &#x00026; Guibas, L. J. A point set generation network for 3D object reconstruction from a single image. In <italic>IEEE Conference on Computer Vision and Pattern Recognition</italic>10.1109/CVPR.2017.264 (IEEE, 2017).</mixed-citation></ref><ref id="CR66"><label>66.</label><mixed-citation publication-type="other">Nealen, A., Igarashi, T., Sorkine, O. &#x00026; Alexa, M. Laplacian mesh optimization. In <italic>International Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast Asia</italic> 381&#x02013;389 (ACM, 2006).</mixed-citation></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="other">Desbrun, M., Meyer, M., Schr&#x000f6;der, P. &#x00026; Barr, A. H. Implicit fairing of irregular meshes using diffusion and curvature flow. In <italic>SIGGRAPH '99: Proc. 26th Annual Conference on Computer Graphics and Interactive Techniques</italic>10.1145/311535.311576 (ACM, 1999).</mixed-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="other">Bai, W. et al. Automated cardiovascular magnetic resonance image analysis with fully convolutional networks. <italic>J. Cardiovasc. Magn. Reson.</italic>10.1186/s12968-018-0471-x (2018).</mixed-citation></ref><ref id="CR69"><label>69.</label><citation-alternatives><element-citation id="ec-CR69" publication-type="journal"><person-group person-group-type="author"><name><surname>Duan</surname><given-names>J</given-names></name><etal/></person-group><article-title>Automatic 3D bi-ventricular segmentation of cardiac images by a shape-refined multi-task deep learning approach</article-title><source>IEEE Trans. Med. Imag.</source><year>2019</year><volume>38</volume><fpage>2151</fpage><lpage>2164</lpage><pub-id pub-id-type="doi">10.1109/TMI.2019.2894322</pub-id></element-citation><mixed-citation id="mc-CR69" publication-type="journal">Duan, J. et al. Automatic 3D bi-ventricular segmentation of cardiac images by a shape-refined multi-task deep learning approach. <italic>IEEE Trans. Med. Imag.</italic><bold>38</bold>, 2151&#x02013;2164 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR70"><label>70.</label><citation-alternatives><element-citation id="ec-CR70" publication-type="journal"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>W</given-names></name><etal/></person-group><article-title>A bi-ventricular cardiac atlas built from 1000+ high resolution MR images of healthy subjects and an analysis of shape and motion</article-title><source>Med. Image Anal.</source><year>2015</year><volume>26</volume><fpage>133</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1016/j.media.2015.08.009</pub-id><pub-id pub-id-type="pmid">26387054</pub-id>
</element-citation><mixed-citation id="mc-CR70" publication-type="journal">Bai, W. et al. A bi-ventricular cardiac atlas built from 1000+ high resolution MR images of healthy subjects and an analysis of shape and motion. <italic>Med. Image Anal.</italic><bold>26</bold>, 133&#x02013;145 (2015).<pub-id pub-id-type="pmid">26387054</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="other">Schuh, A., Qiu, H. &#x00026; HeartFlow Research. deepali: Image, Point Set, and Surface Registration in PyTorch. <italic>GitHub</italic><ext-link ext-link-type="uri" xlink:href="https://biomedia.github.io/deepali/index.html">https://biomedia.github.io/deepali/index.html</ext-link> (2024).</mixed-citation></ref><ref id="CR72"><label>72.</label><citation-alternatives><element-citation id="ec-CR72" publication-type="journal"><person-group person-group-type="author"><name><surname>Rueckert</surname><given-names>D</given-names></name><etal/></person-group><article-title>Nonrigid registration using free-form deformations: application to breast MR images</article-title><source>IEEE Trans. Med. Imag.</source><year>1999</year><volume>18</volume><fpage>712</fpage><lpage>721</lpage><pub-id pub-id-type="doi">10.1109/42.796284</pub-id></element-citation><mixed-citation id="mc-CR72" publication-type="journal">Rueckert, D. et al. Nonrigid registration using free-form deformations: application to breast MR images. <italic>IEEE Trans. Med. Imag.</italic><bold>18</bold>, 712&#x02013;721 (1999).</mixed-citation></citation-alternatives></ref><ref id="CR73"><label>73.</label><mixed-citation publication-type="other">Qiao, M. et al. MeshHeart: version0. <italic>Zenodo</italic>10.5281/zenodo.15122485 (2025).</mixed-citation></ref></ref-list></back></article>