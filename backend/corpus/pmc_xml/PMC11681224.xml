<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Heliyon</journal-id><journal-id journal-id-type="iso-abbrev">Heliyon</journal-id><journal-title-group><journal-title>Heliyon</journal-title></journal-title-group><issn pub-type="epub">2405-8440</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11681224</article-id><article-id pub-id-type="pii">S2405-8440(24)00117-8</article-id><article-id pub-id-type="doi">10.1016/j.heliyon.2024.e24086</article-id><article-id pub-id-type="publisher-id">e24086</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Computer vision-based instantaneous speed tracking system for measuring the subtask speed in the 100-meter sprinter: Development and concurrent validity study</article-title></title-group><contrib-group><contrib contrib-type="author" id="au1"><name><surname>Kamnardsiri</surname><given-names>Teerawat</given-names></name><xref rid="aff1" ref-type="aff">a</xref><xref rid="aff4" ref-type="aff">d</xref></contrib><contrib contrib-type="author" id="au2"><name><surname>Boripuntakul</surname><given-names>Sirinun</given-names></name><email>sirinun.b@cmu.ac.th</email><xref rid="aff2" ref-type="aff">b</xref><xref rid="cor2" ref-type="corresp">&#x02217;&#x02217;</xref></contrib><contrib contrib-type="author" id="au3"><name><surname>Kaiket</surname><given-names>Chinawat</given-names></name><email>chinawat.k@cmu.ac.th</email><xref rid="aff3" ref-type="aff">c</xref><xref rid="aff4" ref-type="aff">d</xref><xref rid="cor1" ref-type="corresp">&#x02217;</xref></contrib><aff id="aff1"><label>a</label>Department of Digital Game, College of Arts, Media and Technology, Chiang Mai University, 50200, Chiang Mai, Thailand</aff><aff id="aff2"><label>b</label>Department of Physical Therapy, Faculty of Associated Medical Sciences, Chiang Mai University, 50200, Chiang Mai, Thailand</aff><aff id="aff3"><label>c</label>Department of Vocational Education and Wellness Promotion, Faculty of Education, Chiang Mai University, 50200, Chiang Mai, Thailand</aff><aff id="aff4"><label>d</label>Department of Sport Sciences, Graduate School, Chiang Mai University, 50200, Chiang Mai, Thailand</aff></contrib-group><author-notes><corresp id="cor1"><label>&#x02217;</label>Corresponding author. Department of Vocational Education and Wellness Promotion, Faculty of Education, Chiang Mai University, 50200, Chiang Mai, Thailand. <email>chinawat.k@cmu.ac.th</email></corresp><corresp id="cor2"><label>&#x02217;&#x02217;</label>Corresponding author. Department of Physical Therapy, Faculty of Associated Medical Sciences, Chiang Mai University, 50200, Chiang Mai, Thailand. <email>sirinun.b@cmu.ac.th</email></corresp></author-notes><pub-date pub-type="pmc-release"><day>06</day><month>1</month><year>2024</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.--><pub-date pub-type="collection"><day>30</day><month>1</month><year>2024</year></pub-date><pub-date pub-type="epub"><day>06</day><month>1</month><year>2024</year></pub-date><volume>10</volume><issue>2</issue><elocation-id>e24086</elocation-id><history><date date-type="received"><day>12</day><month>9</month><year>2023</year></date><date date-type="rev-recd"><day>19</day><month>12</month><year>2023</year></date><date date-type="accepted"><day>3</day><month>1</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 The Authors</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p></license></permissions><abstract id="abs0010"><p>The 100-m sprint is one of the track events, and the pace of the runner can be measured using a variety of tools, such as a hand stopwatch, timing gate, laser device, radar device, photocell timing, etc. The data measured is the mean travel time. Nonetheless, monitoring an individual sprinter's instantaneous speed tracking is essential for assisting staff trainers in developing an appropriate training schedule for the individual sprinter. The purpose of this study was to construct a computer vision-based system for assessing the sprinting speed of the 100-m subtasks and also to investigate the concurrent validity of video analysis software among athletes. Five athletes participated in the research to determine its validity. Over the course of two trials, the sprinting pace of each participant's subtasks (a 100-m split to 10&#x000a0;m for each subtask) was measured. The application of the computer vision-based system to video analysis software was validated using the Pearson correlation coefficient. The agreement between the two measurement systems was quantified using Bland-Altman plots. The results revealed a significant relationship between the two systems and all 100-m subtask sprinting speeds (r&#x000a0;=&#x000a0;0.961&#x02013;1.000, p 0.0001). The Bland-Altman analyses indicated that the mean differences in 100-m subtask speeds were consistently close to zero, falling within the 95&#x000a0;% limits of agreement. The scatter plot distribution showed symmetry. The computer vision-based system proved to be a valid tool, suggesting its potential value in measuring and monitoring the 100-m subtask sprinting speed of athletes.</p></abstract><kwd-group id="kwrds0010"><title>Keywords</title><kwd>Speed detection</kwd><kwd>Computer vision</kwd><kwd>Human detection</kwd><kwd>Sprinter</kwd><kwd>Athletics</kwd><kwd>Image processing</kwd><kwd>Video analysis</kwd><kwd>Speed test</kwd><kwd>100-M sprint</kwd></kwd-group></article-meta></front><body><sec id="sec1"><label>1</label><title>Introduction</title><p id="p0010">Athletics is one of the most prevalent exercise options [<xref rid="bib1" ref-type="bibr">[1]</xref>, <xref rid="bib2" ref-type="bibr">[2]</xref>, <xref rid="bib3" ref-type="bibr">[3]</xref>, <xref rid="bib4" ref-type="bibr">[4]</xref>]. An estimated one hundred million individuals are enthusiastic spectators and participants of athletics, with the sport enjoying regional prominence in the Americas, Asia, Africa, Europe, Oceania, North and Central America, the Caribbean, and South America [<xref rid="bib2" ref-type="bibr">2</xref>]. The Edudwar website's report on the 15 most popular sports worldwide in 2023 revealed that athletics is adored by an audience of around 300 million people [<xref rid="bib3" ref-type="bibr">3</xref>]. There are two different categories of events in Track and Field. Track events involve short-running, medium-running, and long-distance running, whereas field events consist of jumping and throwing [<xref rid="bib5" ref-type="bibr">5</xref>]. The 100-m sprint is a track event that can be categorized into three primary phases: (1) the phase of acceleration, (2) the phase of maximum velocity, and (3) the phase of deceleration [<xref rid="bib6" ref-type="bibr">[6]</xref>, <xref rid="bib7" ref-type="bibr">[7]</xref>, <xref rid="bib8" ref-type="bibr">[8]</xref>, <xref rid="bib9" ref-type="bibr">[9]</xref>]. Moreover, sprint running involves a cyclical motion; the supporting phase and the non-supporting phase are the two primary phases of the cyclic running movement [<xref rid="bib10" ref-type="bibr">10</xref>]. If sprinter needs to make good speed, they have to run as fast as possible. Therefore, the performances of the sprinters are enhanced by utilizing essential sprint metrics, including split times, split distance, step length, step length, and ground contact time. Previous research and coaching practices have utilized many sprint characteristics in order to enhance sprint performance in both training and competitive settings [<xref rid="bib11" ref-type="bibr">[11]</xref>, <xref rid="bib12" ref-type="bibr">[12]</xref>, <xref rid="bib13" ref-type="bibr">[13]</xref>, <xref rid="bib14" ref-type="bibr">[14]</xref>, <xref rid="bib15" ref-type="bibr">[15]</xref>, <xref rid="bib16" ref-type="bibr">[16]</xref>].</p><p id="p0015">Besides, numerous specific training strategies, such as biomechanics, strength, speed, and endurance, are considered for enhancing the performance of athletes. Furthermore, sports science and technology have an effect on athletes, such as sensor monitoring technology, computer vision techniques, high-speed video cameras, digital image and signal processing, etc. There are still challenges and numerous research opportunities for enhancing the athletic performance of each individual.</p><p id="p0020">The biggest problem with staff coaches is that general coaches are unable to determine immediate and maximum speed during athletes&#x02019; running using only their eyes and time from a hand stopwatch. The staff coach only understands how to catch average speed time from a hand stopwatch, which has more mistakes than the gold standard system. As a result, instructors may be unable to recommend and construct practice plans for particular athletes. Furthermore, it could be the secret to increasing athletes' performance.</p><p id="p0025">This study makes significant contributions to the existing literature in a crucial field, emphasizing the development of a computer vision-based system for instantly tracking the speed of an individual sprint athlete. The objective is to establish a computer vision system specifically designed for monitoring the immediate speed of 100-m sprinters. Ultimately, the system's performance needs to be validated against an already established tool.</p></sec><sec id="sec2"><label>2</label><title>Literature review</title><p id="p0030">Nowadays, the development of digital equipment supports the use of technology to produce automatic applications. In computer science research, computer vision techniques were used to analyze the human movement.</p><p id="p0035">Fast speed detection, the speed of the human running can be measured with several tools, such as a timing gate [<xref rid="bib17" ref-type="bibr">17</xref>], hand stopwatch [<xref rid="bib18" ref-type="bibr">18</xref>], laser device [<xref rid="bib19" ref-type="bibr">[19]</xref>, <xref rid="bib20" ref-type="bibr">[20]</xref>, <xref rid="bib21" ref-type="bibr">[21]</xref>, <xref rid="bib22" ref-type="bibr">[22]</xref>], radar device [<xref rid="bib23" ref-type="bibr">23</xref>], motion capture system [<xref rid="bib24" ref-type="bibr">[24]</xref>, <xref rid="bib25" ref-type="bibr">[25]</xref>, <xref rid="bib26" ref-type="bibr">[26]</xref>, <xref rid="bib27" ref-type="bibr">[27]</xref>, <xref rid="bib28" ref-type="bibr">[28]</xref>], photocell timing [<xref rid="bib25" ref-type="bibr">25</xref>,<xref rid="bib29" ref-type="bibr">29</xref>], video timing [<xref rid="bib24" ref-type="bibr">24</xref>,<xref rid="bib26" ref-type="bibr">26</xref>,<xref rid="bib30" ref-type="bibr">[30]</xref>, <xref rid="bib31" ref-type="bibr">[31]</xref>, <xref rid="bib32" ref-type="bibr">[32]</xref>, <xref rid="bib33" ref-type="bibr">[33]</xref>, <xref rid="bib34" ref-type="bibr">[34]</xref>, <xref rid="bib35" ref-type="bibr">[35]</xref>, <xref rid="bib36" ref-type="bibr">[36]</xref>, <xref rid="bib37" ref-type="bibr">[37]</xref>], Global Positioning Systems (GPS) [<xref rid="bib17" ref-type="bibr">17</xref>,<xref rid="bib38" ref-type="bibr">38</xref>,<xref rid="bib39" ref-type="bibr">39</xref>], Global Navigation Satellite System (GNSS) [<xref rid="bib40" ref-type="bibr">40</xref>], radio-based tracking system [<xref rid="bib41" ref-type="bibr">41</xref>], and Inertial Measurement Unit (IMU) [<xref rid="bib42" ref-type="bibr">[42]</xref>, <xref rid="bib43" ref-type="bibr">[43]</xref>, <xref rid="bib44" ref-type="bibr">[44]</xref>].</p><p id="p0040">The action recognition system aims to automatically analyze the human movements made by a human agent during the performance of a task into actions. The typical action recognition system consists of feature extraction, action learning, action classification, and action segmentation [<xref rid="bib45" ref-type="bibr">45</xref>]. For instance, Eveland et al. [<xref rid="bib46" ref-type="bibr">46</xref>] employed background modeling for the segmentation of video-rate stereo sequences. And also, Kamnardsiri et al. [<xref rid="bib26" ref-type="bibr">26</xref>] developed a speed detection system for measuring the instantaneous walking speed of older adults. Nagano et al. [<xref rid="bib24" ref-type="bibr">24</xref>] studied a new low-cost and time-efficient methodology for determining instantaneous horizontal walking and running speed. Kamnardsiri [<xref rid="bib33" ref-type="bibr">33</xref>] designed a system to detect speed, measuring the immediate running speed of seven sprinters. The setup involved a video camera recording a 25-m running distance. The findings revealed a highly significant correlation between the two systems, with Spearman's Rho Correlation exceeding 0.99&#x000a0;at a p-value below 0.0001.</p><p id="p0045">The previous study conducted by the video analysis software (Dartfish) [<xref rid="bib30" ref-type="bibr">30</xref>] provided evidence that the latter measuring approach was valid within the constraints of its accuracy. The instruments have a precision of around &#x000b1;0.01&#x000a0;s. The video data were captured by a video camera (Sony DCRPC105E) and then calculated velocities that ted a near-perfect correlation (r&#x000a0;=&#x000a0;0.99) after analyzing the data [<xref rid="bib31" ref-type="bibr">31</xref>]. The time was compared with photocells [<xref rid="bib32" ref-type="bibr">32</xref>], which documented the use of an Intraclass Correlation Coefficient (ICC). When the velocity data were evaluated using a statistical analysis, a correlation coefficient of 0.98 was obtained. Cameras with frame rates of 50&#x000a0;Hz and 100&#x000a0;Hz.</p><p id="p0050">Moreover, Kamnardsiri et al. [<xref rid="bib47" ref-type="bibr">47</xref>] developed a Knowledge-Based Smart Trainer (KBST) system for coaching in long jump, utilizing computer vision and image processing methods. They examined the effectiveness of the system in assessing the performance of long jump students and evaluated the transfer of knowledge from coaches to students. The study utilized two video cameras to record the speed and jump angle of twenty-two long jumpers. The KBST system proved to be successful in enhancing the performance of long jump students and facilitating the transfer of knowledge from coaches to the students.</p><p id="p0055">The study designed the approach [<xref rid="bib48" ref-type="bibr">48</xref>] that produced the number of steps, stride length, and speed transition from a film of a 100-m sprint using OpenPose. The results demonstrated the method's correctness by comparing it to data measured using standard methods. Furthermore, the accuracy of estimating the number of steps and displaying visualized runners' steps and speed transitions. Stenum et al. [<xref rid="bib49" ref-type="bibr">49</xref>] compared spatiotemporal and sagittal kinematic gait parameters derived from OpenPose with those obtained using 3D motion capture during overground walking in healthy adults. The results presented that the described method effectively and accurately computes spatiotemporal gait parameters, as well as hip and knee angles. This capacity is well-suited for detecting alterations in the gait pattern. Yang et al. [<xref rid="bib50" ref-type="bibr">50</xref>] employed OpenCV to recognize and track middle-distance runners in a 1500&#x000a0;m race video, determining their actual coordinates, running distance, and speed. Using a 4&#x000a0;K high-definition camera and AutoCAD for playground mapping, OpenCV is utilized for image recognition and tracking. Results show discrepancies in the runner's actual distance covered compared to the official distance, but speed calculations using object tracking align closely with manual video analysis. Despite challenges, this pioneering use of object tracking in 1500&#x000a0;m middle-distance running offers valuable insights for future race analysis.</p><p id="p0060">Garc&#x000ed;a-Pinillos et al. [<xref rid="bib51" ref-type="bibr">51</xref>] evaluated the precision of two wearable devices (Stryd&#x02122; and RunScribe&#x02122;) in gauging spatiotemporal parameters while running on a treadmill, as compared to high-speed video analysis conducted at a rate of 1000&#x000a0;Hz. The study involved forty-nine endurance runners, and the parameters under scrutiny included step length, step frequency, contact time, and flight time. The results indicated that both foot pods were considered reliable for measuring spatiotemporal parameters during treadmill running at a comfortable pace. Notably, the RunScribe&#x02122; system demonstrated greater accuracy in temporal parameters and SL compared to the Stryd&#x02122; system, taking into account the agreement limits with high-speed VA.</p><p id="p0065">Jafarzadeh et al. [<xref rid="bib52" ref-type="bibr">52</xref>] introduced a real-time 2D athlete pose estimation system using OpenPose, capturing body part positions in a hurdles athlete, and tested it with videos from a top-tier hurdler. The study assessed the performance of various feature extractors in OpenPose, revealing that the system with the extended version of VGG19 as the feature extractor demonstrated superior accuracy compared to other algorithms.</p><p id="p0070">Felipe et al. [<xref rid="bib53" ref-type="bibr">53</xref>] demonstrated the concordance between a multi-camera tracking system (Mediacoach&#x000ae;) and a GPS system for real-time monitoring of movements in elite football players. The findings indicated strong agreement with high Intraclass Correlation Coefficient (ICC) values (&#x0003e;0.75) and substantial correlations (r&#x000a0;&#x0003e;&#x000a0;0.70) across all variables. However, the video-based system exhibited a tendency to overestimate results in various speed zones, number of sprints, and maximum speed. The study concluded that, particularly for total distance and distances covered at medium speeds, Mediacoach&#x000ae; is comparable in accuracy to a GPS system, providing real-time, objective data tailored to the physical and movement requirements of elite football.</p><p id="p0075">The literature review highlights the potential application of computer vision and image processing techniques in monitoring the immediate speed of individual sprint athletes, aiding coaches in devising tailored training plans. Thus, the study focuses on developing a computer vision-based system for tracking the instantaneous speed of 100-m sprinters. The primary objective is to evaluate the simultaneous accuracy of this system in measuring the speed of each subtask within the 100-m sprint. Additionally, the study evaluates the concurrent validity of each subtask's sprint performance using video analysis software, Tracker 6.1.3 [<xref rid="bib54" ref-type="bibr">54</xref>], as the analysis and modeling tool.</p></sec><sec id="sec3"><label>3</label><title>Materials and methods</title><sec id="sec3.1"><label>3.1</label><title>Study design</title><p id="p0080">This validity study was a prospective 100-m speed trial, a cross-sectional study design, with two trials of 100-m sprinting to evaluate the concurrent validity between the computer vision-based system and video analysis software. The research procedure received approval from the Human Ethical Review Board at the institution of the principal investigator (approval number: AMSEC-65EX-017). Prior to enrollment, all participants provided written consent after receiving relevant information.</p></sec><sec id="sec3.2"><label>3.2</label><title>Recruitment and participants</title><p id="p0085">Five sprint athletes were recruited from Chiang Mai University, Chiang Mai, Thailand. Participants performed sprinting 100-m as fast as possible two times to evaluate sprinting performance. The inclusion criteria were: (1) Thai national healthy sprint athletes age 18&#x02013;25 years; (2) able to do running fast as 4&#x000a0;m&#x000a0;s<sup>&#x02212;1</sup> or more 100&#x000a0;m; and (3) able to comprehend instructions and willing to participate. Exclusion criteria were having: (1) The presence of physical conditions such as a musculoskeletal injury that preclude participants from completing the testing protocol; (2) Taking alcohol 6&#x000a0;h before testing or using drug regimens that affect running performance.</p></sec><sec id="sec3.3"><label>3.3</label><title>Equipment and experimental setup of the proposed system</title><p id="p0090">The subtask speed of the 100-m speed (m.s<sup>&#x02212;1</sup>) was separated into 10&#x000a0;m for each subtask. It was evaluated using the computer vision-based system with four standard web cameras (Logitech C920 HD Pro Webcam, Logitech&#x000ae;, China). The computer vision-based system generated positional data for each participant as a 2D coordinate system (X-axis and Y-axis). Each video camera was set at 30&#x000a0;Hz. The resolution of each video was 640&#x000a0;&#x000d7;&#x000a0;360 pixels. Four video clips of each speed test were collected using the multi-camera video capture software (MultiCam Capture 2.0, Corel&#x000ae; Corporation, Canada). Furthermore, the development of the computer vision-based system involved utilizing MATLAB&#x000ae; 2015a from MathWorks, Inc., Natick, Massachusetts, USA. Moreover, the Computer Vision and Image Processing Toolbox were employed to detect the center of the human body. The collected data from all sources was combined with the interpolate missing values function (fillmiss.m, Kirill K. Pankratov, MIT, USA) and then filtered with a smooth function. The markers were placed at intervals of 100&#x000a0;m, starting from (M5) and extending to the end of the 100-m running track (M1), as depicted in <xref rid="fig1" ref-type="fig">Fig. 1</xref>A. Furthermore, each webcam was positioned at a height of 1.2&#x000a0;m above the ground and located approximately 20&#x000a0;m away from the running track used for testing. Data collection was conducted at the main stadium of Chiang Mai University, Chiang Mai, Thailand. The experimental configuration is illustrated in <xref rid="fig1" ref-type="fig">Fig. 1</xref>B.<fig id="fig1"><label>Fig. 1</label><caption><p>Experimental setup of the 100-m sprint test (A) Sprint test of the participant and the position of markers (M1 to M5), and (B) The position of web cameras (Camera 1 to Camera 4) and the computer with installed the MultiCam capture software.</p></caption><alt-text id="alttext0015">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig></p></sec><sec id="sec3.4"><label>3.4</label><title>Assessment of the concurrent validity of the proposed system</title><p id="p0095">Participant preparation: The eligible participants were informed about the study's purposes before signing informed consent. And then, all participants warm up for the body, including stretching, dynamic exercises, and jogging, to prepare their muscles around 30&#x02013;45&#x000a0;min before the test. Additionally, before participating in the 100-m activity, participants were instructed to don a black, long-sleeved sports shirt and black athletic jogger pants. After that, they performed the sprint on the synthetic running track, flat and dry. All participants performed the sprint test on two attempts. After the first attempt, they were asked to take a rest for around 15&#x000a0;min and then test the second attempt, respectively.</p></sec><sec id="sec3.5"><label>3.5</label><title>Computer vision-based for processing the 100-m speed test</title><p id="p0100">The processing steps for calculating the speeds of the 100-m subtasks in our computer vision-based system were segmented into seven stages, as illustrated in <xref rid="fig2" ref-type="fig">Fig. 2</xref>.<fig id="fig2"><label>Fig. 2</label><caption><p>The diagram of the processing in the proposed system for detecting the speed of the 100-m subtask (A) The 100-m speed test of each participant, (B) Calibration of the capture volume with positions of M1 to M5 markers, (C) Detection of the human body location of each Source 1 to Source 4, (D) Tracking of the body location of four sources, (E) Combination of the speed information into the 100-m distance data, (F) Tracking of the instantaneous speed of the participant, and (G) Segmentation of the 100-m subtask speed of each 10&#x000a0;m.</p></caption><alt-text id="alttext0020">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig></p><sec id="sec3.5.1"><label>3.5.1</label><title>The 100-m speed test</title><p id="p0105">In this stage, the participant prepared to begin at the starting point. The contestant received the starting signal (whistle sound) from the starting referee. The participant completed the 100-m run as quickly as possible to the endpoint. During the run, all video recordings were collected by four conventional web cameras using the Corel&#x000ae; MultiCam Capture 2.0 software that was loaded on the laptop. The starting and ending points of all video files were synchronized, as shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>A.</p></sec><sec id="sec3.5.2"><label>3.5.2</label><title>Calibration of the capture volume</title><p id="p0110">The marker was placed every 25&#x000a0;m from the starting point (0-m) to the end of the running test point (100-m) to calibrate the proposed system. <xref rid="fig2" ref-type="fig">Fig. 2</xref>B shows how these markers were used to determine the capture volume of the 100-m in the horizontal plane (x-axis).</p></sec><sec id="sec3.5.3"><label>3.5.3</label><title>Detection of the human body location</title><p id="p0115">In this step, the human body (participant) location was detected by employing the background subtraction technique (motion segmentation and feature extraction) within the video frames of each video file that presents in <xref rid="fig2" ref-type="fig">Fig. 2</xref>C. The foreground detection <inline-formula><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> as given in equation <xref rid="fd1" ref-type="disp-formula">(1)</xref>:<disp-formula id="fd1"><label>(1)</label><mml:math id="M2" altimg="si2.svg" alttext="Equation 1."><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M3" altimg="si3.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the foreground pixel value, <inline-formula><mml:math id="M4" altimg="si4.svg"><mml:mrow><mml:mi>B</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the background pixel value that was captured before testing, and <inline-formula><mml:math id="M5" altimg="si5.svg"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the foreground detection pixel value in each video frame. The foreground and background were set as grayscale values (0&#x02013;255).</p><p id="p0120">To remove the background from each image, use the Thresholding technique and then convert to a grayscale image as given in equations <xref rid="fd2" ref-type="disp-formula">(2)</xref>, <xref rid="fd3" ref-type="disp-formula">(3)</xref>, <xref rid="fd4" ref-type="disp-formula">(4)</xref>, <xref rid="fd5" ref-type="disp-formula">(5)</xref>:<disp-formula id="fd2"><label>(2)</label><mml:math id="M6" altimg="si6.svg" alttext="Equation 2."><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">&#x0003e;</mml:mo><mml:mspace width="0.25em"/><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>255</mml:mn><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">&#x02264;</mml:mo><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M7" altimg="si7.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the human body pixel value, <inline-formula><mml:math id="M8" altimg="si8.svg"><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:math></inline-formula> is a Threshold value. In this study the Threshold was set as <inline-formula><mml:math id="M9" altimg="si9.svg"><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math></inline-formula> in any initial experimental results. This threshold value was used for all video frames in this study.</p><p id="p0125">To remove noise, a 2D median filtering (3&#x000a0;&#x000d7;&#x000a0;3 adjacent pixels) was used to eliminate noises from <inline-formula><mml:math id="M10" altimg="si10.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of each image. Subsequently, the resulting image was converted to a binary representation, where black is denoted by <italic>&#x02018;0&#x02032;</italic> and white is represented by <italic>&#x02018;1&#x02032;</italic> as outlined in equation <xref rid="fd3" ref-type="disp-formula">(3)</xref>:<disp-formula id="fd3"><label>(3)</label><mml:math id="M11" altimg="si11.svg" alttext="Equation 3."><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">&#x0003e;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M12" altimg="si12.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the human body binary image.</p><p id="p0130">The Bounding Box of a BLOB Analysis [<xref rid="bib55" ref-type="bibr">55</xref>] was used to locate the geometric center of mass (COM) location. The centroid of each participant was marked to identify their position. <inline-formula><mml:math id="M13" altimg="si13.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M14" altimg="si14.svg"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M15" altimg="si15.svg"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="M16" altimg="si16.svg"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> pixels placed at location <inline-formula><mml:math id="M17" altimg="si17.svg"><mml:mrow><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and position <inline-formula><mml:math id="M18" altimg="si18.svg"><mml:mrow><mml:mi>y</mml:mi><mml:mo>:</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as given in equations <xref rid="fd4" ref-type="disp-formula">(4)</xref>, <xref rid="fd5" ref-type="disp-formula">(5)</xref>:<disp-formula id="fd4"><label>(4)</label><mml:math id="M19" altimg="si19.svg" alttext="Equation 4."><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="fd5"><label>(5)</label><mml:math id="M20" altimg="si20.svg" alttext="Equation 5."><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M21" altimg="si16.svg"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> represents the number of pixels in the <inline-formula><mml:math id="M22" altimg="si12.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> detected by a BLOB Analysis, and <inline-formula><mml:math id="M23" altimg="si21.svg"><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id="M24" altimg="si22.svg"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M25" altimg="si23.svg"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula> coordinates of the pixels, respectively.</p></sec><sec id="sec3.5.4"><label>3.5.4</label><title>Tracking of the human body location</title><p id="p0135">To track the human body centroid position, real centroid of the human body in pixel value (<inline-formula><mml:math id="M26" altimg="si24.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) was considered to calculate and also calibrated values of the 100-m position (M1), the 75-m position (M2), the 50-m position (M3), the 25-m position (M4), and the 0-m position (M5) were performed for tracking the real position of the human body COM location (<inline-formula><mml:math id="M27" altimg="si25.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) as given in equations <xref rid="fd6" ref-type="disp-formula">(6)</xref>, <xref rid="fd7" ref-type="disp-formula">(7)</xref>, <xref rid="fd8" ref-type="disp-formula">(8)</xref>, <xref rid="fd9" ref-type="disp-formula">(9)</xref>:<disp-formula id="fd6"><label>(6)</label><mml:math id="M28" altimg="si26.svg" alttext="Equation 6."><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="fd7"><label>(7)</label><mml:math id="M29" altimg="si27.svg" alttext="Equation 7."><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.25em"/></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi>M</mml:mi><mml:mtext>end</mml:mtext></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="fd8"><label>(8)</label><mml:math id="M30" altimg="si28.svg" alttext="Equation 8."><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>25</mml:mn><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="fd9"><label>(9)</label><mml:math id="M31" altimg="si29.svg" alttext="Equation 9."><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mtext>start</mml:mtext></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi>M</mml:mi><mml:mtext>end</mml:mtext></mml:msub></mml:mrow></mml:math></disp-formula>where, <inline-formula><mml:math id="M32" altimg="si30.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the value of distance of each pixel, <inline-formula><mml:math id="M33" altimg="si14.svg"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the human body centroid position, <inline-formula><mml:math id="M34" altimg="si31.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the distance value between <inline-formula><mml:math id="M35" altimg="si32.svg"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mtext>start</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M36" altimg="si33.svg"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mtext>end</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> , <inline-formula><mml:math id="M37" altimg="si32.svg"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mtext>start</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> represents the start point of the video image, <inline-formula><mml:math id="M38" altimg="si33.svg"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mtext>end</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> represents the end point of the video image, and 25 is the actual distance between the start point marker (M1) and the endpoint marker (M2) (for instance, the distance between M1 and M2 is comparable to 25&#x000a0;m of video from Source 4).</p><p id="p0140">Following that, data from videos Source 1 (markers M4 and M5), Source 2 (markers M3 and M4), Source 3 (markers M2 and M3), and Source 4 (markers M1 and M2) were calculated to track the actual position of the human body COM location that presents in <xref rid="fig2" ref-type="fig">Fig. 2</xref>D.</p></sec><sec id="sec3.5.5"><label>3.5.5</label><title>Combination of the speed information</title><p id="p0145">Each period of data from the video source was calculated independently for tracking the human body's COM location. As a result, in order to integrate the human body location, these data were combined into a single data set (distance 0&#x000a0;m&#x02013;100&#x000a0;m). <xref rid="fig2" ref-type="fig">Fig. 2</xref>E illustrates the combination of the human body COM location data (<inline-formula><mml:math id="M39" altimg="si34.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) as given in equation <xref rid="fd10" ref-type="disp-formula">(10)</xref>:<disp-formula id="fd10"><label>(10)</label><mml:math id="M40" altimg="si35.svg" alttext="Equation 10."><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></disp-formula>where,<disp-formula id="ufd1"><mml:math id="M41" altimg="si36.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mspace width="0.5em"/><mml:mn>0</mml:mn><mml:mo linebreak="goodbreak">&#x0003c;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mo linebreak="goodbreak">&#x02264;</mml:mo><mml:mn>25</mml:mn><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="ufd2"><mml:math id="M42" altimg="si37.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mspace width="0.5em"/><mml:mn>25</mml:mn><mml:mo linebreak="goodbreak">&#x0003c;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mo linebreak="goodbreak">&#x02264;</mml:mo><mml:mn>50</mml:mn><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="ufd3"><mml:math id="M43" altimg="si38.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mspace width="0.25em"/></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mspace width="0.5em"/><mml:mn>50</mml:mn><mml:mo linebreak="goodbreak">&#x0003c;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mo linebreak="goodbreak">&#x02264;</mml:mo><mml:mn>75</mml:mn><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="ufd4"><mml:math id="M44" altimg="si39.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mspace width="0.5em"/><mml:mn>75</mml:mn><mml:mo linebreak="goodbreak">&#x0003c;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mo linebreak="goodbreak">&#x02264;</mml:mo><mml:mn>100</mml:mn><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="ufd5"><mml:math id="M45" altimg="si40.svg"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mfrac><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="ufd6"><mml:math id="M46" altimg="si41.svg"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mfrac><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="ufd7"><mml:math id="M47" altimg="si42.svg"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub></mml:mfrac><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="ufd8"><mml:math id="M48" altimg="si43.svg"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula>and, <inline-formula><mml:math id="M49" altimg="si44.svg"><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula> represents the human body's centroid location, <inline-formula><mml:math id="M50" altimg="si45.svg"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula> represents the position of frame of the video, <inline-formula><mml:math id="M51" altimg="si46.svg"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mi>b</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> represent a constant of each distance, <inline-formula><mml:math id="M52" altimg="si47.svg"><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> represent the difference of frame <inline-formula><mml:math id="M53" altimg="si48.svg"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M54" altimg="si49.svg"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> , <inline-formula><mml:math id="M55" altimg="si50.svg"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> , <inline-formula><mml:math id="M56" altimg="si51.svg"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> divide by the difference of location <inline-formula><mml:math id="M57" altimg="si52.svg"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> , <inline-formula><mml:math id="M58" altimg="si53.svg"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> , <inline-formula><mml:math id="M59" altimg="si54.svg"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> , <inline-formula><mml:math id="M60" altimg="si55.svg"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> of each distance, respectively. And also, when a missing value appears at the frame a <inline-formula><mml:math id="M61" altimg="si56.svg"><mml:mrow><mml:mi>N</mml:mi><mml:mi>a</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> value will be added in the <inline-formula><mml:math id="M62" altimg="si57.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The tracking location process had some missing values. As a result, a data cleaning process was carried out in order to recover missing values from the human body COM position data (<inline-formula><mml:math id="M63" altimg="si34.svg"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). For recovering missing values, the interpolates missing values function (fillmiss.m, Kirill K. Pankratov, MIT, USA) that presented in Data Analysis: Filtering, Cross-correlation, Coherence and Applications to Geophysical Data using Matlab [<xref rid="bib56" ref-type="bibr">56</xref>] and then filtered with a smooth function were employed, as shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>E.</p></sec><sec id="sec3.5.6"><label>3.5.6</label><title>Tracking of the instantaneous speed</title><p id="p0150">The human body's COM location in the coordinate system was collected as an array in order to track the velocity of the human. The horizontal plane velocity (<inline-formula><mml:math id="M64" altimg="si58.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) was calculated for each of the four frames (<inline-formula><mml:math id="M65" altimg="si59.svg"><mml:mrow><mml:mi>D</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>) in the video image, following the equations provided in equation <xref rid="fd11" ref-type="disp-formula">(11)</xref>, <xref rid="fd12" ref-type="disp-formula">(12)</xref>, <xref rid="fd13" ref-type="disp-formula">(13)</xref>:<disp-formula id="fd11"><label>(11)</label><mml:math id="M66" altimg="si60.svg" alttext="Equation 11."><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="fd12"><label>(12)</label><mml:math id="M67" altimg="si61.svg" alttext="Equation 12."><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>D</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="fd13"><label>(13)</label><mml:math id="M68" altimg="si62.svg" alttext="Equation 13."><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>D</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>where, <inline-formula><mml:math id="M69" altimg="si63.svg"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the difference between current distance (<inline-formula><mml:math id="M70" altimg="si64.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) and the distance at the next 4 frames (<inline-formula><mml:math id="M71" altimg="si65.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), and <inline-formula><mml:math id="M72" altimg="si66.svg"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the difference between current (<inline-formula><mml:math id="M73" altimg="si67.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) time and time at the next 4 frames (<inline-formula><mml:math id="M74" altimg="si68.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>).</p><p id="p0155"><xref rid="fig2" ref-type="fig">Fig. 2</xref>F illustrates that the 100-m subtask velocity segmentation was split into 10 subtasks, including.<list list-type="simple" id="olist0010"><list-item id="o0010"><label>a.</label><p id="p0160"><bold>Sprinting meter 0 to 10 (V1)</bold> was computed using the horizontal running distance of the human body and the quantity of frames within the range from meter 0 to meter 10.</p></list-item><list-item id="o0015"><label>b.</label><p id="p0165"><bold>Sprinting meter 10 to 20 (V2)</bold> was computed using the horizontal running distance of the human body and the quantity of frames within the range from meter 10 to meter 20.</p></list-item><list-item id="o0020"><label>c.</label><p id="p0170"><bold>Sprinting meter 20 to 30 (V3)</bold> was computed using the horizontal running distance of the human body and the quantity of frames within the range from meter 20 to meter 30.</p></list-item><list-item id="o0025"><label>d.</label><p id="p0175"><bold>Sprinting meter 30 to 40 (V4)</bold> was computed using the horizontal running distance of the human body and the quantity of frames within the range from meter 30 to meter 40.</p></list-item><list-item id="o0030"><label>e.</label><p id="p0180"><bold>Sprinting meter 40 to 50 (V5)</bold> was computed using the horizontal running distance of the human body and the quantity of frames within the range from meter 40 to meter 50.</p></list-item><list-item id="o0035"><label>f.</label><p id="p0185"><bold>Sprinting meter 50 to 60 (V6)</bold> was computed using the horizontal running distance of the human body and the quantity of frames within the range from meter 50 to meter 60.</p></list-item><list-item id="o0040"><label>g.</label><p id="p0190"><bold>Sprinting meter 60 to 70 (V7)</bold> was computed using the horizontal running distance of the human body and the quantity of frames within the range from meter 60 to meter 70.</p></list-item><list-item id="o0045"><label>h.</label><p id="p0195"><bold>Sprinting meter 70 to 80 (V8)</bold> was computed using the horizontal running distance of the human body and the quantity of frames within the range from meter 70 to meter 80.</p></list-item><list-item id="o0050"><label>i.</label><p id="p0200"><bold>Sprinting meter 80 to 90 (V9)</bold> was computed using the horizontal running distance of the human body and the quantity of frames within the range from meter 80 to meter 90.</p></list-item><list-item id="o0055"><label>j.</label><p id="p0205"><bold>Sprinting meter 90 to 100 (V10)</bold> was computed using the horizontal running distance of the human body and the quantity of frames within the range from meter 90 to meter 100.</p></list-item><list-item id="o0060"><label>7.</label><p id="p0210">Segmentation of the 100-m subtask speed:</p></list-item></list></p><p id="p0215">To report the performance test, the mean sprint velocity was split every 10&#x000a0;m to display the participant's performance [<xref rid="bib10" ref-type="bibr">10</xref>,<xref rid="bib57" ref-type="bibr">57</xref>,<xref rid="bib58" ref-type="bibr">58</xref>]. The average velocity data from V1 to V10 was used to compute the velocity values from V1 to V10. Finally, the bar graph was depicted the 100-m subtask speed test as given in equation <xref rid="fd14" ref-type="disp-formula">(14)</xref>, <xref rid="fd15" ref-type="disp-formula">(15)</xref>:<disp-formula id="fd14"><label>(14)</label><mml:math id="M75" altimg="si69.svg" alttext="Equation 14."><mml:mrow><mml:mi>V</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msubsup><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="fd15"><label>(15)</label><mml:math id="M76" altimg="si70.svg" alttext="Equation 15."><mml:mrow><mml:mi>N</mml:mi><mml:mi>V</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msubsup><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">&#x02265;</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">&#x02229;</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">&#x02264;</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>.</mml:mo><mml:mspace width="0.25em"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula>where, <inline-formula><mml:math id="M77" altimg="si71.svg"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> represents the average of velocity of each period, <inline-formula><mml:math id="M78" altimg="si72.svg"><mml:mrow><mml:mi>N</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> represents the number of <inline-formula><mml:math id="M79" altimg="si73.svg"><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula> values of each period, <inline-formula><mml:math id="M80" altimg="si74.svg"><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> represents the size of the <inline-formula><mml:math id="M81" altimg="si73.svg"><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula> array, <inline-formula><mml:math id="M82" altimg="si75.svg"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the location of the human body, <inline-formula><mml:math id="M83" altimg="si76.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the starting point of each period, and <inline-formula><mml:math id="M84" altimg="si77.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the endpoint of each period <xref rid="fig2" ref-type="fig">Fig. 2</xref>G.</p><p id="p0220">The speed of tracking for each participant and the segmentation of the 100-m subtasks were computed using a MATLAB&#x000ae; 2015a script, incorporating the computer vision and image processing toolbox (The MathWorks, Inc., Natick, Massachusetts, USA). The data capture and computation were carried out on a laptop computer from ASUSTek&#x000ae; Computer Inc., Taipei, Taiwan with Intel&#x000ae; Core (TM) i5-8265U 8th Gen <email>CPU@1.60</email> GHz, 2&#x000a0;GB NVIDIA graphic card 2&#x000a0;GB RAM, and 8&#x000a0;GB DDR4 RAM.</p></sec></sec><sec id="sec3.6"><label>3.6</label><title>Video analysis software for processing the 100-m speed test</title><p id="p0225">The Video Analysis and Modeling Tool (Tracker), which is built on the open source physics software [<xref rid="bib59" ref-type="bibr">59</xref>], is employed to track moving objects and provide their position and velocity. Tracker provides powerful analysis tools that make tracking the human velocity from a video source simple. Several prior research used Tracker to analyze the movement for running the 100-m [<xref rid="bib60" ref-type="bibr">60</xref>] and to detect the speed of the 25-m sprinter [<xref rid="bib33" ref-type="bibr">33</xref>]. Therefore, in this study, the expert performed Tracker 6.1.3 as a ground truth of the speed information. Tracker's processing of the 100-m speed test was divided into five parts, as follows.</p><sec id="sec3.6.1"><label>3.6.1</label><title>Loading video data</title><p id="p0230">Each participant's captured video was downloaded onto the laptop. Furthermore, the starting frame, finish frame, and step size (4 frames) were all specified to compute the speed of each video source.</p></sec><sec id="sec3.6.2"><label>3.6.2</label><title>Calibration</title><p id="p0235">Each collected video's capture volume was calibrated using the calibration stick. For example, the starting point (M5) and endpoint (M4) of the calibration stick were marked, and then 25&#x000a0;m were entered into the calibration stick label. Furthermore, the coordinate axes were placed at the starting position to establish the axes' origins as (<inline-formula><mml:math id="M85" altimg="si78.svg"><mml:mrow><mml:mi>x</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true">)</mml:mo><mml:mspace width="0.25em"/><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec3.6.3"><label>3.6.3</label><title>Setup the point mass</title><p id="p0240">In this step, the centroid of the human body was tracked using the point mass from the beginning to the end of each of the four frames.</p></sec><sec id="sec3.6.4"><label>3.6.4</label><title>Calculating and exporting the velocity</title><p id="p0245">The values from the point mass were calculated and then displayed in the Tracker software's table. The velocity information for each was then exported into a. txt file format.</p></sec><sec id="sec3.6.5"><label>3.6.5</label><title>Segmentation of the 100-m subtask speed</title><p id="p0250">Finally, all of the txt files were imported into Microsoft&#x000ae; Excel 2016 Spreadsheet software for the combination of all sources of each participant, and then the video data (100-m speed test) was separated into V1 to V10, accordingly.</p></sec></sec><sec id="sec3.7"><label>3.7</label><title>Statistical analysis</title><p id="p0255">The concurrent validity of the proposed system in detecting velocities for the 100-m subtasks was evaluated through the Pearson correlation coefficient (<inline-formula><mml:math id="M86" altimg="si79.svg"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>), with a specified probability level of 0.05 to determine statistical significance when compared to the Tracker. The assessment of the relative agreement between the systems was explained as follows: if <inline-formula><mml:math id="M87" altimg="si80.svg"><mml:mrow><mml:mi>r</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&#x0003c;</mml:mo><mml:mn>0.50</mml:mn></mml:mrow></mml:math></inline-formula> it signified low agreement; if <inline-formula><mml:math id="M88" altimg="si81.svg"><mml:mrow><mml:mi>r</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&#x0003e;</mml:mo><mml:mn>0.50</mml:mn><mml:mspace width="0.25em"/><mml:mtext>to</mml:mtext><mml:mspace width="0.25em"/><mml:mn>0.69</mml:mn></mml:mrow></mml:math></inline-formula> indicated moderate agreement, <inline-formula><mml:math id="M89" altimg="si82.svg"><mml:mrow><mml:mi>r</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&#x0003e;</mml:mo><mml:mn>0.70</mml:mn><mml:mspace width="0.25em"/><mml:mtext>to</mml:mtext><mml:mspace width="0.25em"/><mml:mn>0.89</mml:mn></mml:mrow></mml:math></inline-formula> indicated high agreement, and <inline-formula><mml:math id="M90" altimg="si83.svg"><mml:mrow><mml:mi>r</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&#x0003e;</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula> indicated very high agreement [<xref rid="bib61" ref-type="bibr">61</xref>]. Bland-Altman plots were employed to assess the inherent variability of the data, where the scatter plot and narrow limit of agreement (LOA) signify a high degree of stability [<xref rid="bib62" ref-type="bibr">62</xref>]. In addition, IBM&#x000ae; SPSS (version 21.0, IBM Corporation, Armonk, NY, USA) was utilized to conduct statistical analyses. This study was conducted at the main stadium of Chiang Mai University, Thailand. The time for 100-m sprint testing was around 9:00 a.m. to 12:00 a.m. on June 25, 2022.</p></sec></sec><sec id="sec4"><label>4</label><title>Results</title><sec id="sec4.1"><label>4.1</label><title>Participant characteristics</title><p id="p0260">All participants were recruited from Chiang Mai University in Chiang Mai, Thailand. Five participants (3 males and 2 females). The mean age of the participants was 21.20 (1.10) years, the mean BMI was 21.78 (2.79) kg.m<sup>&#x02212;2</sup>, the mean time of the first trial 100-m test was 16.682 (2.843) s, with an average speed 6.124 (0.953) m.s<sup>&#x02212;1</sup>. And also, the mean time of the second trial test was 17.225 (2.213) s, with an average speed 5.887 (0.801) m.s<sup>&#x02212;1</sup>. The participants successfully completed two tests without incident. The participants' demographic characteristics are shown in <xref rid="tbl1" ref-type="table">Table 1</xref>.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Participants' demographic characteristics.</p></caption><alt-text id="alttext0035">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="4">Participants</th><th colspan="9">Characteristics<hr/></th></tr><tr><th rowspan="3">Gender</th><th rowspan="3">Age (years)</th><th rowspan="3">Height (cm)</th><th rowspan="3">Weight (kg)</th><th rowspan="3">BMI (kg.m<sup>&#x02212;2</sup>)</th><th colspan="4">100-m test<hr/></th></tr><tr><th colspan="2">Time (sec)<hr/></th><th colspan="2">Speed (m.s<sup>&#x02212;1</sup>)<hr/></th></tr><tr><th>First Trial</th><th>Second Trial</th><th>First Trial</th><th>Second Trial</th></tr></thead><tbody><tr><td align="left">Subject 1</td><td align="left">Male</td><td align="left">20</td><td align="left">168</td><td align="left">68</td><td align="left">24.1</td><td align="left">14.211</td><td align="left">13.955</td><td align="left">7.037</td><td align="left">7.166</td></tr><tr><td align="left">Subject 2</td><td align="left">Male</td><td align="left">20</td><td align="left">171</td><td align="left">72</td><td align="left">24.6</td><td align="left">14.317</td><td align="left">17.680</td><td align="left">6.985</td><td align="left">5.656</td></tr><tr><td align="left">Subject 3</td><td align="left">Male</td><td align="left">22</td><td align="left">180</td><td align="left">73</td><td align="left">22.5</td><td align="left">16.724</td><td align="left">16.876</td><td align="left">5.980</td><td align="left">5.925</td></tr><tr><td align="left">Subject 4</td><td align="left">Female</td><td align="left">22</td><td align="left">151</td><td align="left">43</td><td align="left">18.9</td><td align="left">21.219</td><td align="left">20.143</td><td align="left">4.713</td><td align="left">4.964</td></tr><tr><td align="left">Subject 5</td><td align="left">Female</td><td align="left">22</td><td align="left">160</td><td align="left">48</td><td align="left">18.8</td><td align="left">16.937</td><td align="left">17.470</td><td align="left">5.904</td><td align="left">5.724</td></tr><tr><td colspan="2" align="left">Means (SD)</td><td align="left">21.20 (1.10)</td><td align="left">166.00 (11.02)</td><td align="left">60.80 (14.20)</td><td align="left">21.78 (2.79)</td><td align="left">16.682 (2.843)</td><td align="left">17.225 (2.213)</td><td align="left">6.124 (0.953)</td><td align="left">5.887 (0.801)</td></tr></tbody></table></table-wrap></p></sec><sec id="sec4.2"><label>4.2</label><title>The 100-m performance of participants</title><p id="p0265">The overall speed of all participants with two trials was collected by hand stopwatch. <xref rid="fig3" ref-type="fig">Fig. 3</xref> illustrates the plotting of a sprinter's velocity-distance trajectory within 100&#x000a0;m for all trials from the computer vision-based system. At the same time, the collected data from web cameras was calculated by the computer vision-based system. Five participants performed two 100-m tests. The results indicated that the majority of the participants' speed slightly declined in the second trial.<fig id="fig3"><label>Fig. 3</label><caption><p>The sprint velocity curves of the participants (S1 to S5) with two trials of the 100-m speed test. Trial 1 shows the sprint velocity of each participant (solid line), and Trial 2 shows the sprint velocity of each participant (dash line).</p></caption><alt-text id="alttext0025">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig></p></sec><sec id="sec4.3"><label>4.3</label><title>Concurrent validity</title><p id="p0270">The association between the proposed system and the video analysis software (Tracker) during the ten subtasks ranged from 0.961 to 1.000, indicating a strong relationship. The lowest correlation was observed in the meter 0 to 10 (V1) subtask (r&#x000a0;=&#x000a0;0.961, p&#x000a0;&#x0003c;&#x000a0;0.0001), while the highest agreement was found in the meter 10 to 20 (V2) subtask (r&#x000a0;=&#x000a0;1.000, p&#x000a0;&#x0003c;&#x000a0;0.0001). <xref rid="tbl2" ref-type="table">Table 2</xref> presents the correlation between two systems and the mean speed value for each 100-m subtask.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Correlation for the 100-m subtasks measures, as determined by the computer vision-based system, against those obtained from the video analysis software (Tracker).</p></caption><alt-text id="alttext0040">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">100-m subtasks (m)</th><th colspan="2">Speed (m.s<sup>&#x02212;1</sup>)<hr/></th><th rowspan="2">r (95&#x000a0;% CI)</th><th rowspan="2">p-value<sup>a</sup></th></tr><tr><th>Tracker<break/>Mean (SD)</th><th>Computer vision-based<break/>Mean (SD)</th></tr></thead><tbody><tr><td align="left">V1 (0&#x02013;10)</td><td align="left">4.819 (0.517)</td><td align="left">4.736 (0.510)</td><td align="left">0.961</td><td align="left">0.0001</td></tr><tr><td align="left">V2 (10&#x02013;20)</td><td align="left">6.678 (1.003)</td><td align="left">6.695 (0.994)</td><td align="left">1.000</td><td align="left">0.0001</td></tr><tr><td align="left">V3 (20&#x02013;30)</td><td align="left">7.209 (1.229)</td><td align="left">6.758 (1.111)</td><td align="left">0.996</td><td align="left">0.0001</td></tr><tr><td align="left">V4 (30&#x02013;40)</td><td align="left">7.049 (1.194)</td><td align="left">6.991 (1.170)</td><td align="left">0.999</td><td align="left">0.0001</td></tr><tr><td align="left">V5 (40&#x02013;50)</td><td align="left">6.483 (1.107)</td><td align="left">6.253 (1.137)</td><td align="left">0.995</td><td align="left">0.0001</td></tr><tr><td align="left">V6 (50&#x02013;60)</td><td align="left">6.660 (1.125)</td><td align="left">6.554 (1.170)</td><td align="left">0.998</td><td align="left">0.0001</td></tr><tr><td align="left">V7 (60&#x02013;70)</td><td align="left">6.842 (1.148)</td><td align="left">6.850 (1.140)</td><td align="left">0.999</td><td align="left">0.0001</td></tr><tr><td align="left">V8 (70&#x02013;80)</td><td align="left">6.857 (1.140)</td><td align="left">6.861 (1.076)</td><td align="left">0.992</td><td align="left">0.0001</td></tr><tr><td align="left">V9 (80&#x02013;90)</td><td align="left">6.533 (1.110)</td><td align="left">6.543 (1.080)</td><td align="left">0.998</td><td align="left">0.0001</td></tr><tr><td align="left">V10 (90&#x02013;100)</td><td align="left">5.688 (1.070)</td><td align="left">5.716 (1.068)</td><td align="left">0.999</td><td align="left">0.0001</td></tr></tbody></table><table-wrap-foot><fn id="tspara0020"><p>r&#x000a0;=&#x000a0;Correlation coefficients, CI = Confidence interval.</p></fn><fn id="tspara0025"><p><sup>a</sup> Significant difference at p&#x000a0;&#x0003c;&#x000a0;0.0001.</p></fn></table-wrap-foot></table-wrap></p><p id="p0275">Additionally, <xref rid="fig4" ref-type="fig">Fig. 4</xref> displays the Bland-Altman plots for the subtasks of the 100-m. For the majority of participants throughout the various subtasks of the 100-m event as reported in <xref rid="fig4" ref-type="fig">Fig. 4</xref> (B&#x02013;F, H, I, J), the mean differences observed between the two systems were found to be near zero, falling within the limits of agreement (95&#x000a0;% LOA). Additionally, the scatter plot distribution exhibited symmetry, indicating a high level of agreement between the systems. There were two outliers (10&#x000a0;%) across ten subtasks (meter 0 to 10, <xref rid="fig4" ref-type="fig">Fig. 4</xref>A) and (meter 60 to 70, <xref rid="fig4" ref-type="fig">Fig. 4</xref>G) in the 100-m speed test.<fig id="fig4"><label>Fig. 4</label><caption><p>The Bland-Altman plots assessing concurrent validity in the context of the 100-m sprint test. The plots demonstrate the agreement between the computer vision-based system and the video analysis software (Tracker) for the measurement of the subtask speed of the 100-m test. (A) Sprint meter 0 to 10, (B) sprint meter 10 to 20, (C) sprint meter 20 to 30, (D) sprint meter 30 to 40, (E) sprint meter 40 to 50, (F) sprint meter 50 to 60, (G) sprint meter 60 to 70, (H) sprint meter 70 to 80, (I) sprint meter 80 to 90, and (J) sprint meter 90 to 100. The horizontal axis depicts the average values, while the vertical axis shows the average difference between the two systems for each subtask in the 100-m sprint test. Reference lines are utilized to illustrate the mean difference between the two systems (solid line) and the 95&#x000a0;% limit of agreement (LOA) for the mean difference (dashed line).</p></caption><alt-text id="alttext0030">Fig. 4</alt-text><graphic xlink:href="gr4"/></fig></p></sec></sec><sec id="sec5"><label>5</label><title>Discussion</title><p id="p0280">The purpose of this study was to create a computer vision-based system capable of accurately measuring the instantaneous speed of a sprinter. Additionally, this study aims to evaluate the concurrent validity of the computer vision-based system that was built for the purpose of assessing the 100-m sprinting speeds of ten subtasks in athletes. We hypothesized that the computer vision-based system developed would have high concurrent validity in all subtask sprinting speeds of 0&#x02013;100&#x000a0;m. In accordance with expectations, all of the subtasks' sprinting speeds presented a high degree of association between the two systems. Based on the available information, this research is the first investigation into the concurrent validity of using the video analysis software (Tracker) to accurately measure the sprinting speed of individual subtasks during the 100-m event among athletes.</p><p id="p0285">The gold standard for quantifying 100-m subtask sprint speeds was Tracker. In the present investigation, the 100-m speed of each subtask was calculated using 90 to 150 instantaneous speed values from the computer vision-based system and Tracker. This study demonstrated that the correlation between the proposed system and Tracker was very high across all subtasks and trials of the 100-m sprint test. In addition, a visual examination of the Bland-Altman plots for concurrent validity revealed that the two systems were in good agreement. Our findings are consistent with prior research, which found that the computer vision-based approach outperformed video analysis tools in estimating instantaneous sprint speed in athletes [<xref rid="bib17" ref-type="bibr">17</xref>,<xref rid="bib32" ref-type="bibr">32</xref>,<xref rid="bib33" ref-type="bibr">33</xref>,<xref rid="bib48" ref-type="bibr">48</xref>]. Furthermore, the previous studies agreed that the video camera-based system had a higher degree of correlation than the force distribution measurement platform while evaluating 100-m speed in sprint athletes [<xref rid="bib31" ref-type="bibr">31</xref>,<xref rid="bib32" ref-type="bibr">32</xref>].</p><sec id="sec5.1"><label>5.1</label><title>Limitations of the study</title><p id="p0290">There were a few limitations to this study. The first relates to the background subtraction technique, which influences motion segmentation and feature extraction. Tracking the COM of a human body necessitates resolving significant environmental issues such as sky luminance, moving clouds, large drak objects, tree leaves, several moving small objects, and moving shadows, as well as the color of the clothing. Therefore, dark gray or black is the appropriate color for an athlete's suit, while other colors are prohibited. The second is capturing the volume of the system, which is dependent on camera placement and camera lens; for example, if the capture volume of the athlete racing around was 25&#x000a0;m, the camera must be located about 20&#x02013;25&#x000a0;m away from the athlete's body. The third is the intersection between sources 1 and 4 from the actual location camera. As presented in <xref rid="fig3" ref-type="fig">Fig. 3</xref>, the velocity data are not consistent. It may pose a problem for the location detection algorithm. Finally, the small sample size may have an effect on statistical power; therefore, future studies with larger sample sizes would improve the power analysis.</p></sec><sec id="sec5.2"><label>5.2</label><title>Future works</title><p id="p0295">Future studies need to develop an autonomous system for tracking location using deep learning techniques [<xref rid="bib48" ref-type="bibr">48</xref>,<xref rid="bib49" ref-type="bibr">49</xref>,<xref rid="bib63" ref-type="bibr">[63]</xref>, <xref rid="bib64" ref-type="bibr">[64]</xref>, <xref rid="bib65" ref-type="bibr">[65]</xref>, <xref rid="bib66" ref-type="bibr">[66]</xref>] (object detection algorithms) to detect the speed and spatiotemporal of human movement (split times, split distance, step length, step length, and ground contact time) and machine learning techniques [<xref rid="bib67" ref-type="bibr">67</xref>] to predict an individual athlete's training plan. Additionally, the system needs to be implemented with professional athletes, such as the Thai National Team.</p></sec></sec><sec id="sec6"><label>6</label><title>Conclusion</title><p id="p0300">In the present study, it was demonstrated that the computer vision-based system had high concurrent validity with the video analysis software (Tracker) for measuring 100-m subtask sprinting speeds. In addition, the Bland-Altman plots established that the computer vision-based system had good agreement with the Tracker and high temporal stability for measuring 100-m subtask sprinting speeds among athletes.</p></sec><sec sec-type="data-availability" id="sec7"><title>Data availability statement</title><p id="p0305">No data was used for the research described in the article.</p></sec><sec id="sec8"><title>Lead contact website</title><p id="p0310"><xref rid="au1" ref-type="contrib">Teerawat Kamnardsiri</xref><ext-link ext-link-type="uri" xlink:href="https://www.camt.cmu.ac.th/index.php/en/about-us/list-of-camt-s-staff.html" id="intref0015">https://www.camt.cmu.ac.th/index.php/en/about-us/list-of-camt-s-staff.html</ext-link>.</p></sec><sec id="sec9"><title>CRediT authorship contribution statement</title><p id="p0315"><bold>Teerawat Kamnardsiri:</bold> Writing &#x02013; review &#x00026; editing, Writing &#x02013; original draft, Visualization, Validation, Software, Resources, Project administration, Methodology, Investigation, Funding acquisition, Formal analysis, Data curation, Conceptualization. <bold>Sirinun Boripuntakul:</bold> Writing &#x02013; review &#x00026; editing, Validation, Supervision, Project administration, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. <bold>Chinawat Kaiket:</bold> Writing &#x02013; review &#x00026; editing, Visualization, Validation, Supervision, Project administration, Methodology, Investigation, Formal analysis, Data curation, Conceptualization.</p></sec><sec sec-type="COI-statement"><title>Declaration of competing interest</title><p id="p0320">The authors declare the following financial interests/personal relationships which may be considered as potential competing interests:Chinawat Kaiket reports financial support was provided by <funding-source id="gs4"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100002842</institution-id><institution>Chiang Mai University</institution></institution-wrap></funding-source>.</p></sec></body><back><ref-list id="cebib0010"><title>References</title><ref id="bib1"><label>1</label><element-citation publication-type="other" id="sref1"><person-group person-group-type="author"><name><surname>Gurung</surname><given-names>R.</given-names></name></person-group><article-title>Top 16 most popular sports in the world 2023</article-title><ext-link ext-link-type="uri" xlink:href="https://playersbio.com/most-popular-sports-in-the-world/" id="intref0025">https://playersbio.com/most-popular-sports-in-the-world/</ext-link><year>2023</year></element-citation></ref><ref id="bib2"><label>2</label><element-citation publication-type="other" id="sref2"><person-group person-group-type="author"><name><surname>Babu Tendu</surname><given-names>C.A.</given-names></name><name><surname>Adrianna</surname><given-names>S.</given-names></name></person-group><article-title>Top 20 most popular sports in the world revealed as of 2023</article-title><ext-link ext-link-type="uri" xlink:href="https://sportsbrief.com/other-sports/16715-revealed-top-popular-sports-world/" id="intref0030">https://sportsbrief.com/other-sports/16715-revealed-top-popular-sports-world/</ext-link><year>2023</year></element-citation></ref><ref id="bib3"><label>3</label><element-citation publication-type="other" id="sref3"><person-group person-group-type="author"><name><surname>Verma</surname><given-names>M.</given-names></name></person-group><article-title>Top 15 most popular sports in the world 2023</article-title><ext-link ext-link-type="uri" xlink:href="https://www.edudwar.com/most-popular-sports-in-the-world/" id="intref0035">https://www.edudwar.com/most-popular-sports-in-the-world/</ext-link><year>2023</year></element-citation></ref><ref id="bib4"><label>4</label><element-citation publication-type="book" id="sref4"><person-group person-group-type="author"><name><surname>Staff</surname><given-names>T.</given-names></name></person-group><part-title>World's Top 30 Most Popular Sports</part-title><year>2022</year><ext-link ext-link-type="uri" xlink:href="https://www.totalsportal.com/list/most-popular-sports/#10_Athletics" id="intref0040">https://www.totalsportal.com/list/most-popular-sports/#10_Athletics</ext-link></element-citation></ref><ref id="bib5"><label>5</label><element-citation publication-type="book" id="sref5"><person-group person-group-type="author"><name><surname>Carr</surname><given-names>G.A.</given-names></name></person-group><part-title>Fundamentals of Track and Field</part-title><year>1999</year><publisher-name>Human Kinetics</publisher-name></element-citation></ref><ref id="bib6"><label>6</label><element-citation publication-type="journal" id="sref6"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>R.</given-names></name><name><surname>Bezodis</surname><given-names>I.</given-names></name><name><surname>Thompson</surname><given-names>A.</given-names></name></person-group><article-title>Coaching sprinting: expert coaches' perception of race phases and technical constructs</article-title><source>Int. J. Sports Sci. Coach.</source><volume>4</volume><year>2009</year><fpage>385</fpage><lpage>396</lpage></element-citation></ref><ref id="bib7"><label>7</label><element-citation publication-type="journal" id="sref7"><person-group person-group-type="author"><name><surname>Ae</surname><given-names>M.</given-names></name><name><surname>Ito</surname><given-names>A.</given-names></name><name><surname>Suzuki</surname><given-names>M.</given-names></name></person-group><article-title>The men's 100 metres</article-title><source>New Stud. Athl.</source><volume>7</volume><issue>1</issue><year>1992</year><fpage>47</fpage><lpage>52</lpage></element-citation></ref><ref id="bib8"><label>8</label><element-citation publication-type="journal" id="sref8"><person-group person-group-type="author"><name><surname>Mackala</surname><given-names>K.</given-names></name></person-group><article-title>Optimisation of performance through kinematic analysis of the different phases of the 100 metres</article-title><source>New Stud. Athl.</source><volume>22</volume><issue>2</issue><year>2007</year><fpage>7</fpage></element-citation></ref><ref id="bib9"><label>9</label><element-citation publication-type="book" id="sref9"><person-group person-group-type="author"><collab>ATHLETICS</collab></person-group><part-title>The International Track and Field Annual</part-title><year>2021</year><publisher-name>ASSN OF TRACK &#x00026; FIELD STA</publisher-name></element-citation></ref><ref id="bib10"><label>10</label><element-citation publication-type="book" id="sref10"><person-group person-group-type="author"><name><surname>Gerhardt</surname><given-names>S.</given-names></name></person-group><part-title>Walking and Running</part-title><year>1983</year><publisher-name>Sportverlag Berlin</publisher-name></element-citation></ref><ref id="bib11"><label>11</label><element-citation publication-type="journal" id="sref11"><person-group person-group-type="author"><name><surname>Mattes</surname><given-names>K.</given-names></name><name><surname>Wolff</surname><given-names>S.</given-names></name><name><surname>Alizadeh</surname><given-names>S.</given-names></name><name><surname>S</surname></name></person-group><article-title>Kinematic stride characteristics of maximal sprint running of elite sprinters&#x02013;verification of the &#x0201c;swing-pull technique&#x0201d;</article-title><source>J. Hum. Kinet.</source><volume>77</volume><year>2021</year><fpage>15</fpage><lpage>24</lpage><pub-id pub-id-type="pmid">34168688</pub-id>
</element-citation></ref><ref id="bib12"><label>12</label><element-citation publication-type="journal" id="sref12"><person-group person-group-type="author"><name><surname>Manzer</surname><given-names>S.</given-names></name><name><surname>Mattes</surname><given-names>K.</given-names></name><name><surname>Holl&#x000e4;nder</surname><given-names>K.</given-names></name></person-group><article-title>Kinematic analysis of sprinting pickup acceleration versus maximum sprinting speed</article-title><source>Biol. Sport</source><volume>12</volume><issue>2</issue><year>2016</year><fpage>55</fpage><lpage>67</lpage></element-citation></ref><ref id="bib13"><label>13</label><element-citation publication-type="journal" id="sref13"><person-group person-group-type="author"><name><surname>Debaere</surname><given-names>S.</given-names></name><name><surname>Jonkers</surname><given-names>I.</given-names></name><name><surname>Delecluse</surname><given-names>C.</given-names></name></person-group><article-title>The contribution of step characteristics to sprint running performance in high-level male and female athletes</article-title><source>J. Strength Condit Res.</source><volume>27</volume><issue>1</issue><year>2013</year><fpage>116</fpage><lpage>124</lpage></element-citation></ref><ref id="bib14"><label>14</label><element-citation publication-type="journal" id="sref14"><person-group person-group-type="author"><name><surname>Mero</surname><given-names>A.</given-names></name><name><surname>Komi</surname><given-names>P.</given-names></name><name><surname>Gregor</surname><given-names>R.</given-names></name></person-group><article-title>Biomechanics of sprint running: a review</article-title><source>Sports Med.</source><volume>13</volume><issue>6</issue><year>1992</year><fpage>376</fpage><lpage>392</lpage><pub-id pub-id-type="pmid">1615256</pub-id>
</element-citation></ref><ref id="bib15"><label>15</label><element-citation publication-type="journal" id="sref15"><person-group person-group-type="author"><name><surname>Young</surname><given-names>M.</given-names></name><name><surname>Choice</surname><given-names>C.</given-names></name></person-group><article-title>Maximal velocity sprint mechanics</article-title><source>Track Coach</source><volume>179</volume><year>2007</year><fpage>5723</fpage><lpage>5729</lpage></element-citation></ref><ref id="bib16"><label>16</label><element-citation publication-type="journal" id="sref16"><person-group person-group-type="author"><name><surname>Ma&#x00107;ka&#x00142;a</surname><given-names>K.</given-names></name><name><surname>Fostiak</surname><given-names>M.</given-names></name><name><surname>Kowalski</surname><given-names>K.</given-names></name></person-group><article-title>Selected determinants of acceleration in the 100m sprint</article-title><source>J. Hum. Kinet.</source><volume>45</volume><year>2015</year><fpage>135</fpage><lpage>148</lpage><pub-id pub-id-type="pmid">25964817</pub-id>
</element-citation></ref><ref id="bib17"><label>17</label><element-citation publication-type="journal" id="sref17"><person-group person-group-type="author"><name><surname>Haugen</surname><given-names>T.</given-names></name><name><surname>Buchheit</surname><given-names>M.</given-names></name></person-group><article-title>Sprint running performance monitoring: methodological and practical considerations</article-title><source>Sports Med.</source><volume>46</volume><issue>5</issue><year>2016</year><fpage>641</fpage><lpage>656</lpage><pub-id pub-id-type="pmid">26660758</pub-id>
</element-citation></ref><ref id="bib18"><label>18</label><element-citation publication-type="book" id="sref18"><person-group person-group-type="author"><name><surname>Quercetani</surname><given-names>R.</given-names></name></person-group><part-title>World History of Sprint Racing (1850-2005): the Stellar Events</part-title><year>2006</year><publisher-name>Sep Editrice</publisher-name></element-citation></ref><ref id="bib19"><label>19</label><element-citation publication-type="journal" id="sref19"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>R.A.</given-names></name><name><surname>Pua</surname><given-names>Y.H.</given-names></name><name><surname>Bower</surname><given-names>K.J.</given-names></name></person-group><article-title>Validity of a low-cost laser with freely available software for improving measurement of walking and running speed</article-title><source>J. Sci. Med. Sport</source><volume>22</volume><issue>2</issue><year>2019</year><fpage>212</fpage><lpage>216</lpage><pub-id pub-id-type="pmid">30029889</pub-id>
</element-citation></ref><ref id="bib20"><label>20</label><element-citation publication-type="journal" id="sref20"><person-group person-group-type="author"><name><surname>Arsac</surname><given-names>L.M.</given-names></name><name><surname>Locatelli</surname><given-names>E.</given-names></name></person-group><article-title>Modeling the energetics of 100-m running by using speed curves of world champions</article-title><source>J. Appl. Physiol.</source><volume>92</volume><issue>5</issue><year>2002</year><fpage>1781</fpage><lpage>1788</lpage><pub-id pub-id-type="pmid">11960924</pub-id>
</element-citation></ref><ref id="bib21"><label>21</label><element-citation publication-type="journal" id="sref21"><person-group person-group-type="author"><name><surname>Bezodis</surname><given-names>N.E.</given-names></name><name><surname>Salo</surname><given-names>A.I.</given-names></name><name><surname>Trewartha</surname><given-names>G.</given-names></name></person-group><article-title>Measurement error in estimates of sprint velocity from a laser displacement measurement device</article-title><source>Int. J. Sports Med.</source><volume>33</volume><issue>6</issue><year>2012</year><fpage>439</fpage><lpage>444</lpage><pub-id pub-id-type="pmid">22450882</pub-id>
</element-citation></ref><ref id="bib22"><label>22</label><element-citation publication-type="journal" id="sref22"><person-group person-group-type="author"><name><surname>&#x00160;tuhec</surname><given-names>S.</given-names></name><name><surname>Planj&#x00161;ek</surname><given-names>P.</given-names></name><name><surname>Ptak</surname><given-names>M.</given-names></name><name><surname>&#x0010c;oh</surname><given-names>M.</given-names></name><name><surname>Mackala</surname><given-names>K.</given-names></name></person-group><article-title>Application of the laser linear distance-speed-acceleration measurement system and sport kinematic analysis software</article-title><source>Sensors</source><volume>22</volume><issue>15</issue><year>2022</year><fpage>5876</fpage><pub-id pub-id-type="pmid">35957434</pub-id>
</element-citation></ref><ref id="bib23"><label>23</label><element-citation publication-type="journal" id="sref23"><person-group person-group-type="author"><name><surname>Morin</surname><given-names>J.B.</given-names></name><name><surname>Jeannin</surname><given-names>T.</given-names></name><name><surname>Chevallier</surname><given-names>B.</given-names></name><name><surname>Belli</surname><given-names>A.</given-names></name></person-group><article-title>Spring-mass model characteristics during sprint running: correlation with performance and fatigue-induced changes</article-title><source>Int. J. Sports Med.</source><volume>27</volume><issue>2</issue><year>2005</year><fpage>158</fpage><lpage>165</lpage></element-citation></ref><ref id="bib24"><label>24</label><element-citation publication-type="journal" id="sref24"><person-group person-group-type="author"><name><surname>Nagano</surname><given-names>A.</given-names></name><name><surname>Fujimoto</surname><given-names>M.</given-names></name><name><surname>Kudo</surname><given-names>S.</given-names></name><name><surname>Akaguma</surname><given-names>R.</given-names></name></person-group><article-title>An image-processing based technique to obtain instantaneous horizontal walking and running speed</article-title><source>Gait Posture</source><volume>51</volume><year>2017</year><fpage>7</fpage><lpage>9</lpage><pub-id pub-id-type="pmid">27693807</pub-id>
</element-citation></ref><ref id="bib25"><label>25</label><element-citation publication-type="journal" id="sref25"><person-group person-group-type="author"><name><surname>Feser</surname><given-names>E.H.</given-names></name><name><surname>Neville</surname><given-names>J.</given-names></name><name><surname>Wells</surname><given-names>D.</given-names></name><name><surname>Diewald</surname><given-names>S.</given-names></name><name><surname>Kameda</surname><given-names>M.</given-names></name><name><surname>Bezodis</surname><given-names>N.E.</given-names></name><name><surname>Clark</surname><given-names>K.</given-names></name><name><surname>Nagahara</surname><given-names>R.</given-names></name><name><surname>Macadam</surname><given-names>P.</given-names></name><name><surname>Uthoff</surname><given-names>A.M.</given-names></name><name><surname>Tinwala</surname><given-names>F.</given-names></name></person-group><article-title>Lower-limb wearable resistance overloads joint angular velocity during early acceleration sprint running</article-title><source>J. Sports Sci.</source><volume>41</volume><issue>4</issue><year>2023</year><fpage>326</fpage><lpage>332</lpage><pub-id pub-id-type="pmid">37183445</pub-id>
</element-citation></ref><ref id="bib26"><label>26</label><element-citation publication-type="book" id="sref26"><person-group person-group-type="author"><name><surname>Kamnardsiri</surname><given-names>T.</given-names></name><name><surname>Khuwuthyakorn</surname><given-names>P.</given-names></name><name><surname>Boripuntakul</surname><given-names>S.</given-names></name></person-group><part-title>The development of a gait speed detection system for older adults using video-based processing</part-title><source>Proceedings of the 2019 4th International Conference on Biomedical Imaging</source><year>2019, October</year><publisher-name>Signal Processing</publisher-name><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="bib27"><label>27</label><element-citation publication-type="journal" id="sref27"><person-group person-group-type="author"><name><surname>Kamnardsiri</surname><given-names>T.</given-names></name><name><surname>Thawinchai</surname><given-names>N.</given-names></name><name><surname>Parameyong</surname><given-names>A.</given-names></name></person-group><article-title>Conventional video-based system for measuring the subtask speed of the Timed up and Go Test in older adults: validity and reliability study</article-title><source>PLoS One</source><volume>18</volume><year>2023</year><object-id pub-id-type="publisher-id">e0286574</object-id></element-citation></ref><ref id="bib28"><label>28</label><element-citation publication-type="journal" id="sref28"><person-group person-group-type="author"><name><surname>Linke</surname><given-names>D.</given-names></name><name><surname>Link</surname><given-names>D.</given-names></name><name><surname>Lames</surname><given-names>M.</given-names></name></person-group><article-title>Football-specific validity of TRACAB's optical video tracking systems</article-title><source>PLoS One</source><volume>15</volume><issue>3</issue><year>2020</year><object-id pub-id-type="publisher-id">e0230179</object-id></element-citation></ref><ref id="bib29"><label>29</label><element-citation publication-type="journal" id="sref29"><person-group person-group-type="author"><name><surname>Stewart</surname><given-names>F.</given-names></name><name><surname>Turner</surname><given-names>P.N.</given-names></name><name><surname>Miller</surname><given-names>A.C.</given-names></name><name><surname>S</surname></name></person-group><article-title>Reliability, factorial validity, and interrelationships of five commonly used change of direction speed tests</article-title><source>Scand. J. Med. Sci. Sports</source><volume>24</volume><issue>3</issue><year>2014</year><fpage>500</fpage><lpage>506</lpage><pub-id pub-id-type="pmid">23176602</pub-id>
</element-citation></ref><ref id="bib30"><label>30</label><element-citation publication-type="journal" id="sref30"><person-group person-group-type="author"><name><surname>Haugen</surname><given-names>A.</given-names></name><name><surname>T&#x000f8;nnessen</surname><given-names>T.E.</given-names></name><name><surname>Seiler</surname><given-names>K.</given-names></name><name><surname>S</surname></name></person-group><article-title>The difference is in the start: impact of timing and start procedure on sprint running performance</article-title><source>J. Strength Condit Res.</source><volume>26</volume><issue>2</issue><year>2012</year><fpage>473</fpage><lpage>479</lpage><comment>2012</comment></element-citation></ref><ref id="bib31"><label>31</label><element-citation publication-type="journal" id="sref31"><person-group person-group-type="author"><name><surname>Chelly</surname><given-names>S.</given-names></name><name><surname>M</surname></name><name><surname>Fathloun</surname><given-names>M.</given-names></name><name><surname>Cherif</surname><given-names>N.</given-names></name></person-group><article-title>Effects of a back squat training program on leg power, jump, and sprint performances in junior soccer players</article-title><source>J. Strength Condit Res.</source><volume>23</volume><issue>8</issue><year>2009</year><fpage>2241</fpage><lpage>2249</lpage></element-citation></ref><ref id="bib32"><label>32</label><element-citation publication-type="journal" id="sref32"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>J.</given-names></name><name><surname>A</surname></name><name><surname>Jensen</surname><given-names>L.</given-names></name><name><surname>R</surname></name><name><surname>Donoghue</surname><given-names>O.</given-names></name></person-group><article-title>A comparison of laser and video techniques for determining displacement and velocity during running</article-title><source>Meas. Phys. Educ. Exerc.</source><volume>9</volume><issue>4</issue><year>2005</year><fpage>219</fpage><lpage>231</lpage></element-citation></ref><ref id="bib33"><label>33</label><element-citation publication-type="journal" id="sref33"><person-group person-group-type="author"><name><surname>Kamnardsiri</surname><given-names>T.</given-names></name></person-group><article-title>Assessment the speed-up detection system for testing sprinters' performances using computer vision techniques</article-title><source>Eurasian. J. Anal. Chem.</source><volume>13</volume><issue>6</issue><year>2018</year><fpage>581</fpage><lpage>587</lpage></element-citation></ref><ref id="bib34"><label>34</label><element-citation publication-type="journal" id="sref34"><person-group person-group-type="author"><name><surname>Dobre</surname><given-names>A.D.</given-names></name><name><surname>Gheorghe</surname><given-names>C.</given-names></name></person-group><article-title>The optimization of the running technique using video analysis method</article-title><source>J. Phys.: Conf. Ser.</source><volume>1746</volume><issue>1</issue><year>2021</year><object-id pub-id-type="publisher-id">012086</object-id></element-citation></ref><ref id="bib35"><label>35</label><element-citation publication-type="journal" id="sref35"><person-group person-group-type="author"><name><surname>Jha</surname><given-names>S.</given-names></name><name><surname>Seo</surname><given-names>C.</given-names></name><name><surname>Yang</surname><given-names>E.</given-names></name><name><surname>Joshi</surname><given-names>G.P.</given-names></name></person-group><article-title>Real time object detection and trackingsystem for video surveillance system</article-title><source>Multimed. Tool. Appl.</source><volume>80</volume><year>2021</year><fpage>3981</fpage><lpage>3996</lpage></element-citation></ref><ref id="bib36"><label>36</label><element-citation publication-type="journal" id="sref36"><person-group person-group-type="author"><name><surname>Balamuralidhar</surname><given-names>N.</given-names></name><name><surname>Tilon</surname><given-names>S.</given-names></name><name><surname>Nex</surname><given-names>F.</given-names></name></person-group><article-title>MultEYE: monitoring system for real-time vehicle detection, tracking and speed estimation from UAV imagery on edge-computing platforms</article-title><source>Rem. Sens.</source><volume>13</volume><issue>4</issue><year>2021</year><fpage>573</fpage></element-citation></ref><ref id="bib37"><label>37</label><element-citation publication-type="book" id="sref37"><person-group person-group-type="author"><name><surname>Einfalt</surname><given-names>M.</given-names></name><name><surname>Lienhart</surname><given-names>R.</given-names></name></person-group><part-title>Decoupling video and human motion: towards practical event detection in athlete recordings</part-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</source><year>2020, June</year><fpage>892</fpage><lpage>893</lpage></element-citation></ref><ref id="bib38"><label>38</label><element-citation publication-type="journal" id="sref38"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>W.</given-names></name><name><surname>Talpey</surname><given-names>B.W.</given-names></name><name><surname>S</surname></name><name><surname>James</surname><given-names>P.</given-names></name><name><surname>L</surname></name></person-group><article-title>Common high-speed running thresholds likely do not correspond to high-speed running in field sports</article-title><source>J. Strength Condit Res.</source><volume>37</volume><issue>7</issue><year>2023</year><fpage>1411</fpage><lpage>1418</lpage></element-citation></ref><ref id="bib39"><label>39</label><element-citation publication-type="journal" id="sref39"><person-group person-group-type="author"><name><surname>Pons</surname><given-names>E.</given-names></name><name><surname>Garc&#x000ed;a-Calvo</surname><given-names>T.</given-names></name><name><surname>Resta</surname><given-names>R.</given-names></name><name><surname>Blanco</surname><given-names>H.</given-names></name><name><surname>L&#x000f3;pez del Campo</surname><given-names>R.</given-names></name><name><surname>D&#x000ed;az Garc&#x000ed;a</surname><given-names>J.</given-names></name><name><surname>Pulido</surname><given-names>J.J.</given-names></name></person-group><article-title>A comparison of a GPS device and a multi-camera video technology during official soccer matches: agreement between systems</article-title><source>PLoS One</source><volume>14</volume><issue>8</issue><year>2019</year><object-id pub-id-type="publisher-id">e0220729</object-id></element-citation></ref><ref id="bib40"><label>40</label><element-citation publication-type="journal" id="sref40"><person-group person-group-type="author"><name><surname>Makar</surname><given-names>P.</given-names></name><name><surname>Silva</surname><given-names>F.</given-names></name><name><surname>A</surname></name><name><surname>Oliveira</surname><given-names>R.</given-names></name></person-group><article-title>Assessing the agreement between a global navigation satellite system and an optical-tracking system for measuring total, high-speed running, and sprint distances in official soccer matches</article-title><source>Sci. Prog.</source><volume>106</volume><issue>3</issue><year>2023</year><object-id pub-id-type="publisher-id">00368504231187501</object-id></element-citation></ref><ref id="bib41"><label>41</label><element-citation publication-type="journal" id="sref41"><person-group person-group-type="author"><name><surname>Seidl</surname><given-names>T.</given-names></name><name><surname>Russomanno</surname><given-names>G.</given-names></name><name><surname>T</surname></name><name><surname>St&#x000f6;ckl</surname><given-names>M.</given-names></name><name><surname>Lames</surname><given-names>M.</given-names></name></person-group><article-title>Assessment of sprint parameters in top speed interval in 100 m sprint&#x02014;a pilot study under field conditions</article-title><source>Front. Sports Act. Living.</source><volume>3</volume><year>2021</year><object-id pub-id-type="publisher-id">689341</object-id></element-citation></ref><ref id="bib42"><label>42</label><element-citation publication-type="journal" id="sref42"><person-group person-group-type="author"><name><surname>Miranda-Oliveira</surname><given-names>P.</given-names></name><name><surname>Branco</surname><given-names>M.</given-names></name><name><surname>Fernandes</surname><given-names>O.</given-names></name></person-group><article-title>Accuracy and interpretation of the acceleration from an inertial measurement unit when applied to the sprint performance of track and field athletes</article-title><source>Sensors</source><volume>23</volume><issue>4</issue><year>2023</year><fpage>1761</fpage><pub-id pub-id-type="pmid">36850357</pub-id>
</element-citation></ref><ref id="bib43"><label>43</label><element-citation publication-type="journal" id="sref43"><person-group person-group-type="author"><name><surname>Feletti</surname><given-names>F.</given-names></name><name><surname>Bracco</surname><given-names>C.</given-names></name><name><surname>Takeko Molisso</surname><given-names>M.</given-names></name><name><surname>Bova</surname><given-names>L.</given-names></name><name><surname>Aliverti</surname><given-names>A.</given-names></name></person-group><article-title>Analysis of fluency of movement in parkour using a video and inertial measurement unit technology</article-title><source>J. Hum. Kinet.</source><volume>89</volume><year>2023</year></element-citation></ref><ref id="bib44"><label>44</label><element-citation publication-type="journal" id="sref44"><person-group person-group-type="author"><name><surname>de Ruiter</surname><given-names>C.J.</given-names></name><name><surname>Wilmes</surname><given-names>E.</given-names></name><name><surname>Brouwers</surname><given-names>S.A.</given-names></name><name><surname>Jagers</surname><given-names>E.C.</given-names></name><name><surname>van Die&#x000eb;n</surname><given-names>J.H.</given-names></name></person-group><article-title>Concurrent validity of an easy-to-use inertial measurement unit-system to evaluate sagittal plane segment kinematics during overground sprinting at different speeds</article-title><source>Sports BioMech.</source><year>2022</year><fpage>1</fpage><lpage>14</lpage></element-citation></ref><ref id="bib45"><label>45</label><element-citation publication-type="journal" id="sref45"><person-group person-group-type="author"><name><surname>Weinland</surname><given-names>D.</given-names></name><name><surname>Ronfard</surname><given-names>R.</given-names></name><name><surname>Boyer</surname><given-names>E.</given-names></name></person-group><article-title>A survey of vision-based methods for action representation, segmentation and recognition</article-title><source>Comput. Vis. Image Underst.</source><volume>115</volume><issue>2</issue><year>2011</year><fpage>224</fpage><lpage>241</lpage></element-citation></ref><ref id="bib46"><label>46</label><element-citation publication-type="book" id="sref46"><person-group person-group-type="author"><name><surname>Eveland</surname><given-names>C.</given-names></name><name><surname>Konolige</surname><given-names>K.</given-names></name><name><surname>C</surname></name><name><surname>Bolles</surname><given-names>R.</given-names></name></person-group><part-title>Background modeling for segmentation of video-rate stereo sequences</part-title><source>Proceedings of 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat</source><year>1998, June</year><publisher-name>No. 98CB36231)</publisher-name><fpage>266</fpage><lpage>271</lpage></element-citation></ref><ref id="bib47"><label>47</label><element-citation publication-type="journal" id="sref47"><person-group person-group-type="author"><name><surname>Kamnardsiri</surname><given-names>T.</given-names></name><name><surname>Khuwuthyakorn</surname><given-names>P.</given-names></name><name><surname>Boripuntakul</surname><given-names>S.</given-names></name><name><surname>Janchai</surname><given-names>W.</given-names></name></person-group><article-title>A knowledge-based Smart trainer system for transferring knowledge from coaches to long jump students</article-title><source>Front. Educ.</source><volume>6</volume><year>2021</year><object-id pub-id-type="publisher-id">609114</object-id></element-citation></ref><ref id="bib48"><label>48</label><element-citation publication-type="book" id="sref48"><person-group person-group-type="author"><name><surname>Yagi</surname><given-names>K.</given-names></name><name><surname>Hasegawa</surname><given-names>K.</given-names></name><name><surname>Sugiura</surname><given-names>Y.</given-names></name><name><surname>Saito</surname><given-names>H.</given-names></name></person-group><part-title>Estimation of runners' number of steps, stride length and speed transition from video of a 100-meter race</part-title><source>Proceedings of the 1st International Workshop on Multimedia Content Analysis in Sports</source><year>2018, October</year><fpage>87</fpage><lpage>95</lpage></element-citation></ref><ref id="bib49"><label>49</label><element-citation publication-type="journal" id="sref49"><person-group person-group-type="author"><name><surname>Stenum</surname><given-names>J.</given-names></name><name><surname>Rossi</surname><given-names>C.</given-names></name><name><surname>Roemmich</surname><given-names>R.T.</given-names></name></person-group><article-title>Two-dimensional video-based analysis of human gait using pose estimation</article-title><source>PLoS Comput. Biol.</source><volume>17</volume><issue>4</issue><year>2021</year><object-id pub-id-type="publisher-id">e1008935</object-id></element-citation></ref><ref id="bib50"><label>50</label><element-citation publication-type="journal" id="sref50"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>Lee</surname><given-names>X.</given-names></name><name><surname>Enomoto</surname><given-names>Y.</given-names></name></person-group><article-title>Implementing the tracking of 1500 m runners using Open CV</article-title><source>J. Phys. Educ. Sport.</source><volume>23</volume><issue>7</issue><year>2023</year><fpage>1698</fpage><lpage>1705</lpage></element-citation></ref><ref id="bib51"><label>51</label><element-citation publication-type="journal" id="sref51"><person-group person-group-type="author"><name><surname>Garc&#x000ed;a-Pinillos</surname><given-names>F.</given-names></name><name><surname>Latorre-Rom&#x000e1;n</surname><given-names>P.&#x000c1;.</given-names></name><name><surname>Soto-Hermoso</surname><given-names>V.M.</given-names></name><name><surname>P&#x000e1;rraga-Montilla</surname><given-names>J.A.</given-names></name><name><surname>Pantoja-Vallejo</surname><given-names>A.</given-names></name><name><surname>Ram&#x000ed;rez-Campillo</surname><given-names>R.</given-names></name><name><surname>Roche-Seruendo</surname><given-names>L.E.</given-names></name></person-group><article-title>Agreement between the spatiotemporal gait parameters from two different wearable devices and high-speed video analysis</article-title><source>PLoS One</source><volume>14</volume><issue>9</issue><year>2019</year><object-id pub-id-type="publisher-id">e0222872</object-id></element-citation></ref><ref id="bib52"><label>52</label><element-citation publication-type="book" id="sref52"><person-group person-group-type="author"><name><surname>Jafarzadeh</surname><given-names>P.</given-names></name><name><surname>Virjonen</surname><given-names>P.</given-names></name><name><surname>Nevalainen</surname><given-names>P.</given-names></name><name><surname>Farahnakian</surname><given-names>F.</given-names></name><name><surname>Heikkonen</surname><given-names>J.</given-names></name></person-group><part-title>Pose estimation of hurdles athletes using openpose</part-title><source>Proceedings of 2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering</source><year>2021, October</year><publisher-name>ICECCME)</publisher-name><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="bib53"><label>53</label><element-citation publication-type="journal" id="sref53"><person-group person-group-type="author"><name><surname>Felipe</surname><given-names>J.L.</given-names></name><name><surname>Garcia-Unanue</surname><given-names>J.</given-names></name><name><surname>Viejo-Romero</surname><given-names>D.</given-names></name><name><surname>Navandar</surname><given-names>A.</given-names></name><name><surname>S&#x000e1;nchez-S&#x000e1;nchez</surname><given-names>J.</given-names></name></person-group><article-title>Validation of a video-based performance analysis system (Mediacoach&#x000ae;) to analyze the physical demands during matches in LaLiga</article-title><source>Sensors</source><volume>19</volume><issue>19</issue><year>2019</year><fpage>4113</fpage><pub-id pub-id-type="pmid">31547591</pub-id>
</element-citation></ref><ref id="bib54"><label>54</label><element-citation publication-type="book" id="sref54"><person-group person-group-type="author"><name><surname>Hockicko</surname><given-names>P.</given-names></name></person-group><part-title>Video Analysis of Motions</part-title><year>2014</year><publisher-name>Department of Physics Faculty of Electrical Engineering University of Zilina</publisher-name><publisher-loc>Slovakia</publisher-loc></element-citation></ref><ref id="bib55"><label>55</label><element-citation publication-type="book" id="sref55"><person-group person-group-type="author"><name><surname>Moeslund</surname><given-names>B.</given-names></name><name><surname>T</surname></name><name><surname>Moeslund</surname><given-names>B.</given-names></name><name><surname>T</surname></name></person-group><part-title>BLOB Analysis. Introduction to Video and Image Processing: Building Real Systems and Applications</part-title><year>2012</year><publisher-name>Springer Science and Business Media</publisher-name><fpage>103</fpage><lpage>115</lpage></element-citation></ref><ref id="bib56"><label>56</label><element-citation publication-type="book" id="sref56"><person-group person-group-type="author"><name><surname>dos Santos Mesquita</surname><given-names>M.</given-names></name><name><surname>Halld&#x000f3;rsd&#x000f3;ttir</surname><given-names>S.</given-names></name></person-group><part-title>Data Analysis: Filtering, Crosscorrelation, Coherence and Applications to Geophysical Data Using Matlab</part-title><year>2005</year></element-citation></ref><ref id="bib57"><label>57</label><element-citation publication-type="journal" id="sref57"><person-group person-group-type="author"><name><surname>Matsumura</surname><given-names>T.</given-names></name><name><surname>Tomoo</surname><given-names>K.</given-names></name><name><surname>Sugimoto</surname><given-names>T.</given-names></name></person-group><article-title>Acute effect of caffeine supplementation on 100-m sprint running performance: a field test</article-title><source>Med. Sci. Sports Exerc.</source><volume>55</volume><issue>3</issue><year>2023</year><fpage>525</fpage><pub-id pub-id-type="pmid">36251383</pub-id>
</element-citation></ref><ref id="bib58"><label>58</label><element-citation publication-type="journal" id="sref58"><person-group person-group-type="author"><name><surname>Healy</surname><given-names>R.</given-names></name><name><surname>Kenny</surname><given-names>I.C.</given-names></name><name><surname>Harrison</surname><given-names>A.J.</given-names></name></person-group><article-title>Profiling elite male 100-m sprint performance: the role of maximum velocity and relative acceleration</article-title><source>J. Sport Health Sci.</source><volume>11</volume><issue>1</issue><year>2022</year><fpage>75</fpage><lpage>84</lpage><pub-id pub-id-type="pmid">35151419</pub-id>
</element-citation></ref><ref id="bib59"><label>59</label><element-citation publication-type="book" id="sref59"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>D.</given-names></name><name><surname>Christian</surname><given-names>W.</given-names></name><name><surname>Hanson</surname><given-names>M.</given-names></name><name><surname>Tracker</surname><given-names>R.</given-names></name></person-group><part-title>Video Analysis and Modeling Tool</part-title><year>2022</year><ext-link ext-link-type="uri" xlink:href="https://physlets.org/tracker/" id="intref0045">https://physlets.org/tracker/</ext-link></element-citation></ref><ref id="bib60"><label>60</label><element-citation publication-type="journal" id="sref60"><person-group person-group-type="author"><name><surname>Castaneda</surname><given-names>A.</given-names></name></person-group><article-title>Rectilinear movement and functions through the analysis of videos with Tracker</article-title><source>Phys. Teach.</source><volume>57</volume><issue>7</issue><year>2019</year><fpage>506</fpage><lpage>507</lpage></element-citation></ref><ref id="bib61"><label>61</label><element-citation publication-type="journal" id="sref61"><person-group person-group-type="author"><name><surname>Mukaka</surname><given-names>M.M.</given-names></name></person-group><article-title>A guide to appropriate use of correlation coefficient in medical research</article-title><source>Malawi Med. J.</source><volume>24</volume><issue>3</issue><year>2012</year><fpage>69</fpage><lpage>71</lpage><pub-id pub-id-type="pmid">23638278</pub-id>
</element-citation></ref><ref id="bib62"><label>62</label><element-citation publication-type="journal" id="sref62"><person-group person-group-type="author"><name><surname>Bland</surname><given-names>J.M.</given-names></name><name><surname>Altman</surname><given-names>D.</given-names></name></person-group><article-title>Statistical methods for assessing agreement between two methods of clinical measurement</article-title><source>Lancet</source><volume>327</volume><issue>8476</issue><year>1986</year><fpage>307</fpage><lpage>310</lpage></element-citation></ref><ref id="bib63"><label>63</label><element-citation publication-type="journal" id="sref63"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>H.</given-names></name></person-group><article-title>Sports video athlete detection based on deep learning</article-title><source>Neural Comput. Appl.</source><volume>35</volume><issue>6</issue><year>2023</year><fpage>4201</fpage><lpage>4210</lpage></element-citation></ref><ref id="bib64"><label>64</label><element-citation publication-type="journal" id="sref64"><person-group person-group-type="author"><name><surname>Mhalla</surname><given-names>A.</given-names></name><name><surname>Chateau</surname><given-names>T.</given-names></name><name><surname>Amara</surname><given-names>N.E.B.</given-names></name></person-group><article-title>Spatio-temporal object detection by deep learning: video-interlacing to improve multi-object tracking, Image Vis</article-title><source>Comput. Times</source><volume>88</volume><year>2019</year><fpage>120</fpage><lpage>131</lpage></element-citation></ref><ref id="bib65"><label>65</label><element-citation publication-type="book" id="sref65"><person-group person-group-type="author"><name><surname>Einfalt</surname><given-names>M.</given-names></name><name><surname>Dampeyrou</surname><given-names>C.</given-names></name><name><surname>Zecha</surname><given-names>D.</given-names></name></person-group><part-title>Frame-level event detection in athletics videos with pose-based convolutional sequence networks</part-title><source>Proceedings Proceedings of the 2nd International Workshop on Multimedia Content Analysis in Sports</source><year>2019, October</year><fpage>42</fpage><lpage>50</lpage></element-citation></ref><ref id="bib66"><label>66</label><element-citation publication-type="journal" id="sref66"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>R.</given-names></name><name><surname>Amores</surname><given-names>J.</given-names></name></person-group><article-title>Improving detection speed in video by exploiting frame correlation</article-title><source>Pattern Recogn. Lett.</source><volume>112</volume><year>2018</year><fpage>303</fpage><lpage>309</lpage></element-citation></ref><ref id="bib67"><label>67</label><element-citation publication-type="journal" id="sref67"><person-group person-group-type="author"><name><surname>Richter</surname><given-names>C.</given-names></name><name><surname>O'Reilly</surname><given-names>M.</given-names></name><name><surname>Delahunt</surname><given-names>E.</given-names></name></person-group><article-title>Machine learning in sports science: challenges and opportunities</article-title><source>Sports BioMech.</source><year>2021</year><fpage>1</fpage><lpage>7</lpage></element-citation></ref></ref-list><sec id="appsec1" sec-type="supplementary-material"><label>Appendix A</label><title>Supplementary data</title><p id="p0330">The following is the Supplementary data to this article.<supplementary-material content-type="local-data" id="mmc1"><caption><title>Multimedia component 1</title></caption><media xlink:href="mmc1.pdf"><alt-text>Multimedia component 1</alt-text></media></supplementary-material></p></sec><ack id="ack0010"><title>Acknowledgements</title><p id="p0325">The authors would like to acknowledge Chiang Mai University. This work was supported by the <funding-source id="gs1"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100002842</institution-id><institution>CMU Junior Research Fellowship Program, Chiang Mai University, Thailand</institution></institution-wrap></funding-source> (no. <award-id award-type="grant" rid="gs1">JRCMU2564_074</award-id>). And also, <funding-source id="gs3">Motion Capture and Motion Analysis Laboratory, College of Arts, Media and Technology</funding-source> for the majority of the project.</p></ack><fn-group><fn id="appsec2" fn-type="supplementary-material"><label>Appendix A</label><p id="p0335">Supplementary data to this article can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.heliyon.2024.e24086" id="intref0020">https://doi.org/10.1016/j.heliyon.2024.e24086</ext-link>.</p></fn></fn-group></back></article>