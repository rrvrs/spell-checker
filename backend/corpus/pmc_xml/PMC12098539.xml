<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Oncol</journal-id><journal-id journal-id-type="iso-abbrev">Front Oncol</journal-id><journal-id journal-id-type="publisher-id">Front. Oncol.</journal-id><journal-title-group><journal-title>Frontiers in Oncology</journal-title></journal-title-group><issn pub-type="epub">2234-943X</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC12098539</article-id><article-id pub-id-type="doi">10.3389/fonc.2025.1577198</article-id><article-categories><subj-group subj-group-type="heading"><subject>Oncology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>A transformation uncertainty and multi-scale contrastive learning-based semi-supervised segmentation method for oral cavity-derived cancer</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Ran</given-names></name><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Lyu</surname><given-names>Chengqi</given-names></name><xref rid="fn001" ref-type="author-notes">
<sup>*</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/3026818/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/resources/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Yu</surname><given-names>Lvfeng</given-names></name><xref rid="fn001" ref-type="author-notes">
<sup>*</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/2938075/overview"/><role content-type="https://credit.niso.org/contributor-roles/funding-acquisition/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/resources/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib></contrib-group><aff id="aff1">
<institution>Department of Stomatology, Shanghai Sixth People&#x02019;s Hospital Affiliated to Shanghai Jiao Tong University School of Medicine</institution>, <addr-line>Shanghai</addr-line>, <country>China</country>
</aff><author-notes><fn fn-type="edited-by"><p>Edited by: Abhijit Chakraborty, University of Texas MD Anderson Cancer Center, United States</p></fn><fn fn-type="edited-by"><p>Reviewed by: Abhinava Mishra, University of California, Santa Barbara, United States</p><p>Aseem Rai Bhatnagar, Henry Ford Health - Cancer, United States</p></fn><corresp id="fn001">*Correspondence: Chengqi Lyu, <email xlink:href="mailto:loner_lcq@sjtu.edu.cn">loner_lcq@sjtu.edu.cn</email>; Lvfeng Yu, <email xlink:href="mailto:lfyu@sjtu.edu.cn">lfyu@sjtu.edu.cn</email>
</corresp></author-notes><pub-date pub-type="epub"><day>09</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><elocation-id>1577198</elocation-id><history><date date-type="received"><day>15</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>14</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025 Wang, Lyu and Yu</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Wang, Lyu and Yu</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><sec><title>Objectives</title><p>Oral cavity-derived cancer pathological images (OPI) are crucial for diagnosing oral squamous cell carcinoma (OSCC), but existing deep learning methods for OPI segmentation rely heavily on large, accurately labeled datasets, which are labor- and resource-intensive to obtain. This paper presents a semi-supervised segmentation method for OPI to mitigate the limitations of scarce labeled data by leveraging both labeled and unlabeled data.</p></sec><sec><title>Materials and methods</title><p>We use the Hematoxylin and Eosin (H&#x00026;E)-stained oral cavity-derived cancer dataset (OCDC), which consists of 451 images with tumor regions annotated and verified by pathologists. Our method combines transformation uncertainty and multi-scale contrastive learning. The transformation uncertainty estimation evaluates the model&#x02019;s confidence on data transformed via different methods, reducing discrepancies between the teacher and student models. Multi-scale contrastive learning enhances class similarity and separability while reducing teacher-student model similarity, encouraging diverse feature representations. Additionally, a boundary-aware enhanced U-Net is proposed to capture boundary information and improve segmentation accuracy.</p></sec><sec><title>Results</title><p>Experimental results on the OCDC dataset demonstrate that our method outperforms both fully supervised and existing semi-supervised approaches, achieving superior segmentation performance.</p></sec><sec><title>Conclusions</title><p>Our semi-supervised method, integrating transformation uncertainty, multi-scale contrastive learning, and a boundary-aware enhanced U-Net, effectively addresses data scarcity and improves segmentation accuracy. This approach reduces the dependency on large labeled datasets, promoting the application of AI in OSCC detection and improving the efficiency and accuracy of clinical diagnoses for OSCC.</p></sec></abstract><kwd-group><kwd>pathological image segmentation</kwd><kwd>semi-supervised learning</kwd><kwd>oral cavity-derived cancer</kwd><kwd>contrastive learning</kwd><kwd>uncertainty estimation</kwd></kwd-group><funding-group><award-group><funding-source id="cn001"><institution-wrap><institution>National Natural Science Foundation of China
</institution><institution-id institution-id-type="doi">10.13039/501100001809</institution-id></institution-wrap></funding-source><award-id award-type="contract" rid="cn001">82071160, 82370933, 82470946, 82271027</award-id></award-group><funding-statement>The author(s) declare that financial support was received for the research and/or publication of this article. The authors gratefully acknowledge the financial support of the National Natural Science Foundation of China (82071160, 82370933, 82470946, 82271027), the Advanced Research Program of Shanghai Jiaotong University Affiliated Sixth People&#x02019;s Hospital (LY33.X-5596).</funding-statement></funding-group><counts><fig-count count="9"/><table-count count="6"/><equation-count count="22"/><ref-count count="39"/><page-count count="15"/><word-count count="7825"/></counts><custom-meta-group><custom-meta><meta-name>section-in-acceptance</meta-name><meta-value>Head and Neck Cancer</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="s1"><label>1</label><title>Introduction</title><p>According to the World Cancer Research Fund&#x02019;s International Report, over 377,700 cases of oral cavity-derived cancer were reported globally in 2020, ranking 16th among all cancers (<xref rid="B1" ref-type="bibr">1</xref>). Oral squamous cell carcinoma (OSCC) is a common and aggressive oral tumor, with a five-year survival rate of only around 50% (<xref rid="B2" ref-type="bibr">2</xref>). Pathological images are considered the gold standard for diagnosing and grading cancer (<xref rid="B3" ref-type="bibr">3</xref>), and their accurate interpretation is crucial for OSCC treatment and control. OSCC histopathological evaluation typically involves processes like formalin fixation, sectioning, paraffin embedding, and hematoxylin and eosin staining, followed by microscopic examination by trained pathologists (<xref rid="B4" ref-type="bibr">4</xref>). Pathologists use standardized criteria to assess the tumor&#x02019;s presence, subtype, and other histological features.</p><p>Recent advancements in computer-aided systems, driven by high-precision imaging and computational power, have accelerated the development of automated methods for histopathological image analysis. Deep learning, in particular, has shown great promise in the automated segmentation of oral cavity-derived cancer images (OPI) (<xref rid="B5" ref-type="bibr">5</xref>&#x02013;<xref rid="B8" ref-type="bibr">8</xref>). While these methods have shown promising results, they typically require large annotated datasets. However, pathological images, compared to other medical imaging modalities like MRI and CT, often have high spatial resolution, making accurate labeling more challenging. Additionally, the labeling process requires specialized knowledge and extensive diagnostic experience, making it difficult to obtain sufficient labeled data, which limits the broader application of deep learning methods for OPI segmentation.</p><p>Semi-supervised learning (SSL) addresses the challenge of limited labeled data by combining a small amount of labeled data with a large volume of unlabeled data. In medical image segmentation, consistency regularization methods are commonly used, assuming that small perturbations should not significantly change the model&#x02019;s outputs. These methods introduce perturbations in data, model, and task, enforcing consistency across them. For data perturbation, techniques like Gaussian noise (<xref rid="B9" ref-type="bibr">9</xref>) and affine transformations (<xref rid="B10" ref-type="bibr">10</xref>) are often used. In model perturbation, methods such as Mean Teacher (MT) (<xref rid="B11" ref-type="bibr">11</xref>) have been effective, where Dropout operations in the teacher-student network and exponential moving averages (EMA) of model weights are used to improve model accuracy. To enhance prediction quality, uncertainty estimation techniques, such as prediction entropy (<xref rid="B12" ref-type="bibr">12</xref>), evidence theory (<xref rid="B13" ref-type="bibr">13</xref>), and KL divergence (<xref rid="B14" ref-type="bibr">14</xref>), have been incorporated. Multi-task consistency methods, such as reconstruction (<xref rid="B15" ref-type="bibr">15</xref>), boundary perception (<xref rid="B16" ref-type="bibr">16</xref>), and distance map tasks (<xref rid="B17" ref-type="bibr">17</xref>), are also used to better utilize unlabeled data.</p><p>The lack of large sample labels is the starting point for semi-supervised learning. In classical MT networks, the teacher model&#x02019;s predictions are often used as pseudo-labels to guide the optimization of the student model. However, the substantial semantic gap between the pseudo-labels generated by the teacher model and the true labels can seriously impact the student model&#x02019;s performance. Considering that pathologists typically rotate, flip, and otherwise transform pathological images in clinical practice to make a comprehensive evaluation, and that we desire deep networks to exhibit invariance (such as the ability to recognize objects under translation, rotation, scaling, or varying lighting conditions), we design a transformation-based uncertainty estimation (TB-UE) method. Building on UA-MT (<xref rid="B12" ref-type="bibr">12</xref>), we combine multiple data transformation methods to estimate uncertainty by measuring the model&#x02019;s predictions for the same data point under different transformations. This method incorporates both data uncertainty and per-pixel entropy information, mitigating the detrimental effects of noisy pseudo-labels on the student model. However, this approach may lead to high similarity between the teacher and student models. To address this, we propose a multi-scale contrastive learning (MS-CL) method, which computes the average feature vectors of different categories from both the teacher and student models, using contrastive loss to pull together feature vectors of the same class and push apart those of different classes. This method not only alleviates the over-similarity problem between teacher and student models but also improves intra-class similarity and inter-class separability, resulting in more diverse feature representations. Additionally, we propose a boundary-aware enhanced U-Net (BAE-U-Net), which adds a boundary perception enhancement branch to the original U-Net (<xref rid="B18" ref-type="bibr">18</xref>), enabling the capture of boundary information in OSCC pathological images. In our BAE-U-Net, we design a channel-attention-based boundary-spatial feature fusion module (BSFM) that combines the boundary information extracted by the enhancement branch with the spatial information from U-Net, facilitating more comprehensive feature representation.</p><p>In summary, the contributions of this paper are as follows:</p><list list-type="order"><list-item><p>A semi-supervised segmentation method for Oral Cavity-derived Cancer pathological images is proposed, based on transformation uncertainty and multi-scale contrastive learning, and is designed to alleviate the limitations imposed by the scarcity of labeled data.</p></list-item><list-item><p>A transformation-based uncertainty estimation method is introduced, in which pixel uncertainty is estimated by evaluating the model&#x02019;s predictions on data transformed using different methods.</p></list-item><list-item><p>A multi-scale contrastive learning method is presented, which improves intra-class similarity and inter-class separability while mitigating the over-similarity problem between the teacher and student models.</p></list-item><list-item><p>A boundary-aware enhanced U-Net is proposed, which integrates boundary information with spatial information to facilitate more comprehensive feature learning.</p></list-item><list-item><p>Extensive experiments on the dataset demonstrate the superiority of the proposed method compared to other approaches, highlighting its potential in addressing the issue of labeled data scarcity.</p></list-item></list></sec><sec id="s2"><label>2</label><title>Related work</title><sec id="s2_1"><label>2.1</label><title>Semi-supervised segmentation of medical images</title><p>The main goal of semi-supervised learning is to utilize a large amount of unlabeled data to improve supervised learning performance. In medical image segmentation, consistency regularization and pseudo-labeling are two major paradigms for semi-supervised learning.</p><p>Pseudo-labeling methods typically involve training a model on a labeled dataset, then using this trained model to assign pseudo-labels with confidence scores to unlabeled data. High-confidence pseudo-labels are added to the labeled set to enhance model performance. Self-training and co-training are two common approaches in pseudo-labeling. Self-training focuses on refining pseudo-labels using various strategies to make them closer to true labels. For instance, Bai et&#x000a0;al. (<xref rid="B19" ref-type="bibr">19</xref>) optimized pseudo-labels with conditional random fields, while Zeng et&#x000a0;al. (<xref rid="B20" ref-type="bibr">20</xref>) selected high-confidence samples by combining class information and prediction entropy. However, single-branch self-training methods can be unstable due to variations in pseudo-label quality. Co-training, derived from multi-view learning, uses multiple complementary views of the data for multi-branch training. High-confidence predictions are added to other branches&#x02019; data or consistency methods are applied to guide interaction between branches. Examples of multi-view approaches include adversarial learning to generate multiple views (<xref rid="B21" ref-type="bibr">21</xref>), multi-modality data for multi-view samples (<xref rid="B22" ref-type="bibr">22</xref>), and multi-branch Transformer-CNN structures for feature extraction (<xref rid="B23" ref-type="bibr">23</xref>). In particular, CNN-based branches are widely used due to their powerful local feature extraction capability, which complements global dependencies captured by Transformer modules, and improves the model&#x02019;s robustness to variations in tissue structures and staining.</p><p>Consistency regularization methods are based on the smoothness assumption, adding perturbations to data points in terms of data, model, and task, and enforcing consistency. Significant progress has been made in data perturbation consistency, with methods like patch-shuffling (<xref rid="B24" ref-type="bibr">24</xref>), cut-paste augmentation (<xref rid="B25" ref-type="bibr">25</xref>), and Copy-Paste (<xref rid="B26" ref-type="bibr">26</xref>). For model perturbation consistency, besides the Mean Teacher (MT) network, multi-decoder structures (<xref rid="B27" ref-type="bibr">27</xref>&#x02013;<xref rid="B29" ref-type="bibr">29</xref>) are also effective. These structures use a shared encoder and multiple decoders, which either learn from each other or minimize statistical differences between decoders to reduce model uncertainty. Other model perturbation methods include multi-scale consistency (<xref rid="B30" ref-type="bibr">30</xref>), complementary consistency (<xref rid="B31" ref-type="bibr">31</xref>), and the use of anatomical prior knowledge (<xref rid="B32" ref-type="bibr">32</xref>).</p></sec><sec id="s2_2"><label>2.2</label><title>Pathological image segmentation</title><p>The goal of pathological image segmentation is to divide the image into different components, such as cell nuclei, glands, or tissue regions, which is essential for clinical diagnosis. By linking morphological features to clinical outcomes, segmentation provides an objective and quantitative analysis that helps guide treatment decisions. Deep learning-based methods have shown great promise in the segmentation of pathological images, but they typically rely on pixel-level labels, which are time-consuming and expensive to obtain.</p><p>In breast cancer pathological image analysis, Li et&#x000a0;al. (<xref rid="B33" ref-type="bibr">33</xref>) proposed DeepTree, a deep learning architecture based on a tree-structured diagnostic strategy. This method represents relationships between different pathological categories and establishes a new framework for segmentation in pathological regions of interest (ROI). In lung cancer diagnosis, Chen et&#x000a0;al. (<xref rid="B34" ref-type="bibr">34</xref>) introduced a weakly supervised learning method using a deep generative model to convert fluorescent tissue images into virtual H&#x00026;E stained images, followed by a multi-instance learning model for segmentation. This approach leverages weak supervision to mitigate the need for large labeled datasets. For bladder cancer analysis, He et&#x000a0;al. (<xref rid="B35" ref-type="bibr">35</xref>) developed MultiTrans, a framework that enhances segmentation accuracy through multi-scale feature fusion, aiding in the segmentation of head-and-neck at-risk organs.</p><p>These studies showcase the application of CNNs, graph convolutions, and Transformers in pathological image segmentation, as well as the growing use of weakly supervised and unsupervised methods to reduce the reliance on large annotated datasets.</p></sec></sec><sec id="s3"><label>3</label><title>Methods</title><sec id="s3_1"><label>3.1</label><title>Overview</title><p>The OPI semi-supervised segmentation task aims to jointly train a model using a large amount of unlabeled data and a small amount of labeled data to improve model performance. We use a dataset <inline-formula>
<mml:math id="im1" display="inline" overflow="scroll"><mml:mi>D</mml:mi></mml:math>
</inline-formula> consisting of <inline-formula>
<mml:math id="im2" display="inline" overflow="scroll"><mml:mi>M</mml:mi></mml:math>
</inline-formula> labeled samples and NN unlabeled samples, where <inline-formula>
<mml:math id="im3" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x0226a;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math>
</inline-formula>. The labeled dataset is defined as <inline-formula>
<mml:math id="im4" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mi>L</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>, and the unlabeled dataset is defined as <inline-formula>
<mml:math id="im5" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mi>U</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math>
</inline-formula>, where <inline-formula>
<mml:math id="im6" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> represents a pathological image of height H, width W, and C channels, and <inline-formula>
<mml:math id="im7" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> represents the corresponding label map for <inline-formula>
<mml:math id="im8" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mtext>i</mml:mtext></mml:msup></mml:mrow></mml:math>
</inline-formula>. The goal of the semi-supervised segmentation task is to learn a student model <inline-formula>
<mml:math id="im9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> parameterized by <inline-formula>
<mml:math id="im10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> from the dataset D, such that each pixel in the input image is mapped to its correct class.</p><p>
<xref rid="f1" ref-type="fig">
<bold>Figure&#x000a0;1</bold>
</xref> illustrates the OPI semi-supervised segmentation method based on transformation uncertainty and multi-scale contrastive learning, which is proposed in this paper. This method aims to jointly train on a small amount of labeled data and a large amount of unlabeled data to mitigate the limitation caused by the shortage of labeled data in OPI segmentation models. The method follows the approach of MT (<xref rid="B10" ref-type="bibr">10</xref>). Specifically, the network is divided into a teacher model and a student model, both of which share the same network architecture. The parameters <inline-formula>
<mml:math id="im11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> of the student network are updated using the gradient backpropagation algorithm. For the teacher model, the parameters <inline-formula>
<mml:math id="im12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> are updated using the EMA method, formulated as:</p><fig position="float" id="f1"><label>Figure&#x000a0;1</label><caption><p>Overall framework of the proposed method for OPI semi-supervised segmentation based on transformation uncertainty and multi-scale contrastive learning.</p></caption><graphic xlink:href="fonc-15-1577198-g001" position="float"/></fig><disp-formula id="eq1">
<label>(1)</label>
<mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>T</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math>
</disp-formula><p>where <inline-formula>
<mml:math id="im13" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula> denotes the parameters of the teacher model after the t-th iteration, <inline-formula>
<mml:math id="im14" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula> denotes the parameters of the student model after the t-th iteration, and <inline-formula>
<mml:math id="im15" display="inline" overflow="scroll"><mml:mi>&#x003b1;</mml:mi></mml:math>
</inline-formula> is the EMA decay coefficient controlling the rate at which the teacher model parameters are updated. To better utilize the multi-scale information of pathological images, we design a contrastive learning method by optimizing the multi-scale contrastive loss function <inline-formula>
<mml:math id="im16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>C</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> to distinguish the multi-scale class features of OPI. Additionally, we incorporate an uncertainty estimation method that combines data transformation and entropy, building on MT, to reduce the gap between teacher model predictions and true labels. Based on this, the consistency loss <inline-formula>
<mml:math id="im17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is optimized to enhance the prediction accuracy of the student model. Below, we provide a detailed explanation of the proposed BAE-U-Net, the TB-UE method, and the MS-CL method.</p></sec><sec id="s3_2"><label>3.2</label><title>BAE-U-Net</title><p>To extract boundary information for OPI and combine it with the spatial information extracted by the U-Net, we propose the BAE-U-Net. This network consists of the classical U-Net, a boundary-enhancement branch, and a BSFM. In this paper, the final convolutional layer of the U-Net is referred to as the &#x0201c;seg-head,&#x0201d; while the remaining parts are referred to as the &#x0201c;seg-net.&#x0201d; The input data pass through the seg-net to obtain the spatial features <inline-formula>
<mml:math id="im18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>. The structure of the boundary-aware enhancement branch is shown in <xref rid="f1" ref-type="fig">
<bold>Figure&#x000a0;1</bold>
</xref>. After passing through this branch, the enhanced boundary features <inline-formula>
<mml:math id="im19" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> are obtained. This structure consists of boundary-aware and boundary-enhancement components, aiming to capture a more comprehensive boundary feature representation of the pathological image.</p><list list-type="order"><list-item><p>Boundary-Aware Module: This module consists of two Sobel operators, as illustrated in <xref rid="f2" ref-type="fig">
<bold>Figure&#x000a0;2</bold>
</xref>. These operators are used to extract the horizontal and vertical boundary information of the image. The parameter <inline-formula>
<mml:math id="im20" display="inline" overflow="scroll"><mml:mi>&#x003b1;</mml:mi></mml:math>
</inline-formula> is a learnable parameter.</p></list-item><list-item><p>Boundary-Enhancement Module: This module consists of five feature extraction layers, each of which is formed by sequentially connecting a convolutional layer, a batch normalization layer, and a ReLU activation layer. The kernel sizes of the convolutional layers are 5, 5, 3, 3, and 1. This module retains a significant amount of boundary detail and further reduces the impact of noise and artifacts in the image, achieving the goal of refining and enhancing the image boundary features.</p></list-item></list><fig position="float" id="f2"><label>Figure&#x000a0;2</label><caption><p>Sobel operator used in the boundary-aware module.</p></caption><graphic xlink:href="fonc-15-1577198-g002" position="float"/></fig><p>To better fuse boundary features with spatial features and avoid information loss caused by traditional addition or multiplication methods, we propose a BSFM, the structure of which is shown in <xref rid="f3" ref-type="fig">
<bold>Figure&#x000a0;3</bold>
</xref>. Considering the semantic gap between boundary information and global spatial information, we implement cross-modal fusion of spatial and boundary information in the form of channel attention. The fused features can be formulated as (<xref rid="eq2" ref-type="disp-formula">Equation 2</xref>):</p><fig position="float" id="f3"><label>Figure&#x000a0;3</label><caption><p>BSFM.</p></caption><graphic xlink:href="fonc-15-1577198-g003" position="float"/></fig><disp-formula id="eq2">
<label>(2)</label>
<mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>&#x02295;</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</disp-formula><p>where <inline-formula>
<mml:math id="im21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> denotes the boundary-spatial joint features, and <inline-formula>
<mml:math id="im22" display="inline" overflow="scroll"><mml:mi>&#x003b1;</mml:mi></mml:math>
</inline-formula> is a parameter that adjusts the importance of boundary information and spatial information. <inline-formula>
<mml:math id="im23" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="im24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> represent the weights of spatial and boundary information, which are obtained through the cross-modal attention mechanism. As shown in <xref rid="f3" ref-type="fig">
<bold>Figure&#x000a0;3</bold>
</xref>, the cross-modal attention mechanism structure includes an activation function (<inline-formula>
<mml:math id="im25" display="inline" overflow="scroll"><mml:mi>&#x003c3;</mml:mi></mml:math>
</inline-formula>), max-pooling (MP), average-pooling (AP), shared convolution (CC), and multi-layer perceptron (MLP). The process of obtaining the boundary-spatial joint weights <inline-formula>
<mml:math id="im26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>E</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> can be represented as (<xref rid="eq3" ref-type="disp-formula">Equation 3</xref>):</p><disp-formula id="eq3">
<label>(3)</label>
<mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>E</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mtext>=MLP</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</disp-formula><p>For both boundary features and spatial features, we first apply the sigmoid activation function, followed by MP and AP for channel attention, and use shared convolutions for processing. For the convolutional features, we initially fuse the two feature types using the concatenation operation (Concat). To capture the complex relationships between channels and the interactions of features, we apply MLP to the fused features, resulting in the boundary-spatial joint weights. Since <inline-formula>
<mml:math id="im27" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> in this paper, the effectively compressed and fused feature weights are split into equal-sized <inline-formula>
<mml:math id="im28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="im29" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> to distinguish the importance of spatial and boundary information.</p></sec><sec id="s3_3"><label>3.3</label><title>TB-UE</title><p>To further mitigate the issue of incorrect predictions in the student model due to noisy labels, we propose an uncertainty estimation method based on data transformations, building upon the UA-MT method. This method effectively combines pixel entropy information with data transformation invariance, specifically divided into pixel entropy estimation and transformation invariance estimation.</p><list list-type="order"><list-item><p>Pixel Entropy Estimation: We adopt the method from UA-MT to calculate the entropy of each pixel in the pseudo-labels. First, we perform T forward passes of the data through the teacher model to simulate Monte Carlo sampling. Let <inline-formula>
<mml:math id="im30" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula> represent the predicted probability for class c at pixel i during the t-th forward pass. Then, the sum of probabilities over all classes is: <inline-formula>
<mml:math id="im31" display="inline" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math>
</inline-formula>, where n is the total number of classes. The average predicted probability for class cc across the T forward passes is: <inline-formula>
<mml:math id="im32" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>The entropy at pixel ii is then calculated as: <inline-formula>
<mml:math id="im33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>. Next, we filter the high-confidence pixels based on the entropy values. We define a Boolean function <inline-formula>
<mml:math id="im34" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>, which outputs 1 when the condition is true, otherwise 0. The high-confidence entropy mask <inline-formula>
<mml:math id="im35" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> is defined as: <inline-formula>
<mml:math id="im36" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>, where <inline-formula>
<mml:math id="im37" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> is the uncertainty threshold for pixel entropy, which varies over iterations.</p></list-item><list-item><p>Transformation Uncertainty Estimation: To better estimate transformation uncertainty, we design <inline-formula>
<mml:math id="im38" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math>
</inline-formula> data transformation methods, which include rotation (90&#x000b0;, 180&#x000b0;, and 270&#x000b0;), flipping (horizontal and vertical), patching (<xref rid="B36" ref-type="bibr">36</xref>), and color channel transformations. <xref rid="f4" ref-type="fig">
<bold>Figure&#x000a0;4</bold>
</xref> illustrates the results of the different transformation methods. The transformed data can be represented as: <inline-formula>
<mml:math id="im39" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>7</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>, where <inline-formula>
<mml:math id="im40" display="inline" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> denotes the j-th data transformation method. After applying these transformations to the data, we pass them through the teacher model and reverse the transformations to obtain the transformed predictions, represented as: <inline-formula>
<mml:math id="im41" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>, where <inline-formula>
<mml:math id="im42" display="inline" overflow="scroll"><mml:mrow><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> represents the inverse transformation corresponding to <inline-formula>
<mml:math id="im43" display="inline" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>, and <inline-formula>
<mml:math id="im44" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula> denotes the predicted labels after the inverse transformation. We then compute the transformation confidence map for each pixel, defined as: <inline-formula>
<mml:math id="im45" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>T</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">/</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>, where <inline-formula>
<mml:math id="im46" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> represents the number of times that pixel i is predicted as class cc across the transformations. The high-confidence transformation mask <inline-formula>
<mml:math id="im47" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> is defined as (<xref rid="eq4" ref-type="disp-formula">Equation 4</xref>):</p></list-item></list><fig position="float" id="f4"><label>Figure&#x000a0;4</label><caption><p>Illustration of different types of data transformations applied to the original data.</p></caption><graphic xlink:href="fonc-15-1577198-g004" position="float"/></fig><disp-formula id="eq4">
<label>(4)</label>
<mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mtext>&#x02009;</mml:mtext><mml:mo>&#x02228;</mml:mo><mml:mtext>&#x02009;</mml:mtext><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>T</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>&#x02228;</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>T</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math>
</disp-formula><p>where <inline-formula>
<mml:math id="im48" display="inline" overflow="scroll"><mml:mo>&#x02228;</mml:mo></mml:math>
</inline-formula> denotes the pixel-wise OR operation, and <inline-formula>
<mml:math id="im49" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> is the uncertainty threshold for transformation invariance.</p><p>We calculate the consistency loss <inline-formula>
<mml:math id="im50" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> between the teacher and student models using the obtained masks <inline-formula>
<mml:math id="im51" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="im52" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> (<xref rid="eq5" ref-type="disp-formula">Equation 5</xref>):</p><disp-formula id="eq5">
<label>(5)</label>
<mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</disp-formula><p>where <inline-formula>
<mml:math id="im53" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="im54" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula> represent the predictions from the student and teacher models, respectively. The mask <inline-formula>
<mml:math id="im55" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:math>
</inline-formula> is the pixel-wise product of <inline-formula>
<mml:math id="im56" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="im57" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>, which integrates both pixel entropy and transformation invariance information. This combined mask effectively constrains the pseudo-labels, alleviating the performance degradation caused by semantic gaps between pseudo-labels and true labels.</p></sec><sec id="s3_4"><label>3.4</label><title>MS-CL</title><p>The consistency loss method proposed in Section 3.3 works well for constraining the similarity between the predictions of the student and teacher models, but it lacks a mechanism to enforce intra-class compactness and inter-class separability, which can lead to over-mixing of the features between the student and teacher models. Therefore, in this section, we propose a MS-CL method to enhance the intra-class similarity and inter-class separability of features, while also de-mixing the student and teacher models.</p><p>The flow of the MS-CL method is shown in <xref rid="f1" ref-type="fig">
<bold>Figure&#x000a0;1</bold>
</xref>. We calculate the multi-scale class contrastive learning loss <inline-formula>
<mml:math id="im58" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>C</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> based on the outputs from the last <inline-formula>
<mml:math id="im59" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math>
</inline-formula> layers of the seg-net. For the output features at scale d, <inline-formula>
<mml:math id="im60" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:mi>d</mml:mi></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>, we first apply a convolution operation with a kernel size of 1 to transform them into features <inline-formula>
<mml:math id="im61" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:msup><mml:mi>d</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> is the output channel size. For the teacher model, we compute the average feature vector for each class (<xref rid="eq6" ref-type="disp-formula">Equation 6</xref>):</p><disp-formula id="eq6">
<label>(6)</label>
<mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>c</mml:mi><mml:mi>d</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mtext>d</mml:mtext></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:msup><mml:mi>d</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:msubsup><mml:msubsup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mtext>d</mml:mtext></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</disp-formula><p>where <inline-formula>
<mml:math id="im62" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>c</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula> represents the average feature vector for class cc at scale d, and <inline-formula>
<mml:math id="im63" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula> is the probability that pixel n at scale dd belongs to class c, which is obtained by applying the softmax function to <inline-formula>
<mml:math id="im64" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:mi>d</mml:mi></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>. Similarly, for the student model, we compute the average feature vector <inline-formula>
<mml:math id="im65" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>c</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>. Since our dataset contains only background and target (oral tumor region) classes, the average feature vectors at scale dd for both the teacher and student models are denoted as <inline-formula>
<mml:math id="im66" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="im67" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>, <inline-formula>
<mml:math id="im68" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="im69" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>. As shown in <xref rid="f1" ref-type="fig">
<bold>Figure&#x000a0;1</bold>
</xref>, to calculate the multi-scale contrastive loss, we first compute the contrastive loss <inline-formula>
<mml:math id="im70" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula> at each scale.</p><p>Since our goal is to bring the feature vectors of the same class closer and push the feature vectors of different classes apart, we use the InfoNCE loss function. The contrastive loss at scale d isdefined in <xref rid="eq10" ref-type="disp-formula">Equation 10</xref>. where <inline-formula>
<mml:math id="im71" display="inline" overflow="scroll"><mml:mi>&#x003c4;</mml:mi></mml:math>
</inline-formula> is the temperature parameter, and <inline-formula>
<mml:math id="im72" display="inline" overflow="scroll"><mml:mrow><mml:mtext>sim</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</inline-formula> represents the similarity between vectors a and b. The multi-scale contrastive loss is defined as (<xref rid="eq7" ref-type="disp-formula">Equation 7</xref>):</p><disp-formula id="eq7">
<label>(7)</label>
<mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>C</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>&#x003b2;</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</disp-formula></sec><sec id="s3_5"><label>3.5</label><title>In this paper, <inline-formula>
<mml:math id="im73" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b2;</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math>
</inline-formula> represents the weight of the loss function at different scales, with the constraint: <inline-formula>
<mml:math id="im74" display="inline" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>&#x003b2;</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula> In our method, we set <inline-formula>
<mml:math id="im75" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b2;</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>&#x003b2;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>&#x003b2;</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math>
</inline-formula>Loss function</title><p>The loss function in this paper is composed of two parts: the supervised loss <inline-formula>
<mml:math id="im76" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> and the unsupervised loss <inline-formula>
<mml:math id="im77" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>. The supervised loss is calculated by averaging the cross-entropy loss and Dice loss between the student model&#x02019;s predictions and the true labels over the labeled dataset <inline-formula>
<mml:math id="im78" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>. The unsupervised loss consists of the consistency loss <inline-formula>
<mml:math id="im79" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> and the multi-scale contrastive loss <inline-formula>
<mml:math id="im80" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>C</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>. The unsupervised loss <inline-formula>
<mml:math id="im81" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> can be expressed as (<xref rid="eq8" ref-type="disp-formula">Equation 8</xref>):</p><disp-formula id="eq8">
<label>(8)</label>
<mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003c9;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>C</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</disp-formula><p>where <inline-formula>
<mml:math id="im82" display="inline" overflow="scroll"><mml:mi>&#x003c9;</mml:mi></mml:math>
</inline-formula> is a Gaussian weighting function defined as: <inline-formula>
<mml:math id="im83" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c9;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</inline-formula>. Here, t denotes the current iteration, and <inline-formula>
<mml:math id="im84" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> represents the maximum number of iterations. Finally, the total loss function for our method can be written as (<xref rid="eq9" ref-type="disp-formula">Equation 9</xref>):</p><disp-formula id="eq9">
<label>(9)</label>
<mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow></mml:math>
</disp-formula><p>where <inline-formula>
<mml:math id="im85" display="inline" overflow="scroll"><mml:mi>&#x003bb;</mml:mi></mml:math>
</inline-formula> is a pre-defined weight that balances the supervised loss and the consistency loss.</p><disp-formula id="eq10">
<label>(10)</label>
<mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mi>e</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>log</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>sim</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">/</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>sim</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">/</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>sim</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">/</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>sim</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">/</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>sim</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">/</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>sim</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">/</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>sim</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">/</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math>
</disp-formula></sec></sec><sec id="s4"><label>4</label><title>Experiments</title><sec id="s4_1"><label>4.1</label><title>Dataset</title><p>In this study, we use the Hematoxylin and Eosin (H&#x00026;E) stained oral cavity-derived cancer dataset (OCDC) collected in (<xref rid="B5" ref-type="bibr">5</xref>). The tumor regions in this dataset have been manually annotated by experts and verified by pathologists. The OCDC dataset consists of 1,020 histological images with a size of 640&#x000d7;640 pixels, which include fully annotated tumor regions for segmentation purposes. All histological images were digitized at a 20&#x000d7; magnification. Since our experiment focuses on segmenting tumor regions, we excluded 569 images that contained no tumor areas, as confirmed by pathologists&#x02019; gold-standard annotations. The remaining 451 images were used for the experiments.</p></sec><sec id="s4_2"><label>4.2</label><title>Evaluation metrics</title><p>To ensure a fair comparison of the proposed method with other methods, we used five common evaluation metrics to assess the performance of the proposed model and other approaches on the same test set: Overall Accuracy (OA), Average Accuracy (AA), Dice Similarity Coefficient (DSC), and Jaccard Index. The results for each method were summarized, and the average and standard deviation for each metric were reported in the table. OA measures the proportion of correctly predicted samples out of the total samples. AA is the average accuracy across all classes, emphasizing class balance. DSC and Jaccard evaluate the similarity between the segmentation results and the ground truth. The formulas for the four metrics are as follows (<xref rid="eq11" ref-type="disp-formula">Equations 11</xref>&#x02013;<xref rid="eq14" ref-type="disp-formula">14</xref>):</p><disp-formula id="eq11">
<label>(11)</label>
<mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mi>O</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</disp-formula><disp-formula id="eq12">
<label>(12)</label>
<mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</disp-formula><disp-formula id="eq13">
<label>(13)</label>
<mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x02229;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</disp-formula><disp-formula id="eq14">
<label>(14)</label>
<mml:math id="M14" display="block" overflow="scroll"><mml:mrow><mml:mi>J</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x02229;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x0222a;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</disp-formula><p>where TP represents the number of samples correctly predicted as positive, TN represents the number of samples correctly predicted as negative, FP refers to the number of negative samples incorrectly predicted as positive (false positives), and FN refers to the number of positive samples incorrectly predicted as negative (false negatives). A is the foreground pixel set in the ground truth, and B is the foreground pixel set in the predicted result. <inline-formula>
<mml:math id="im86" display="inline" overflow="scroll"><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x02229;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:math>
</inline-formula> represents the number of pixels in the intersection of the ground truth and predicted results, while <inline-formula>
<mml:math id="im87" display="inline" overflow="scroll"><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x0222a;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:math>
</inline-formula> represents the number of pixels in their union. <inline-formula>
<mml:math id="im88" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="im89" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula> denote the number of foreground pixels in the ground truth and prediction, respectively. In addition, we also used 95% Hausdorff distance (HD95) to quantitatively evaluate the segmentation of the boundaries.</p></sec><sec id="s4_3"><label>4.3</label><title>Implementation details</title><p>In this study, the proposed network was implemented using the PyTorch framework and trained on an NVIDIA GeForce RTX 3090 GPU. We used the SGD optimizer with a learning rate of 0.01 and a momentum coefficient of 0.9. 10% of the data was used as the test set, and 5-fold cross-validation was performed on the remaining 90%. The model was trained for 100 epochs. The model with the highest DSC on the validation set was selected as the final model for testing. During training, the batch size for both labeled and unlabeled data was set to 1, with equal proportions of labeled and unlabeled data. Based on the findings in MT (<xref rid="B10" ref-type="bibr">10</xref>), we set the EMA coefficient &#x003b1;in <xref rid="eq1" ref-type="disp-formula">Equation 1</xref> to 0.99.</p></sec><sec id="s4_4"><label>4.4</label><title>Comparison experiments</title><sec id="s4_4_1"><label>4.4.1</label><title>Comparison with fully supervised methods</title><p>To verify that our method can leverage unlabeled data to improve model segmentation performance, we trained the model using 10%, 20%, and 30% labeled data along with the corresponding proportion of unlabeled data, and compared the results with fully supervised methods. The quantitative experimental results are reported in <xref rid="T1" ref-type="table">
<bold>Table&#x000a0;1</bold>
</xref>. The data in <xref rid="T1" ref-type="table">
<bold>Table&#x000a0;1</bold>
</xref> shows that the proposed method outperforms the fully supervised methods in all five average metrics when using the corresponding proportions of labeled data. Specifically, the DSC improved by 2.39%, 3.74%, and 2.75%, respectively. Notably, when using 30% labeled data, the proposed method performed better than using 100% labeled data, indicating that the method significantly reduces the need for labeled data. <xref rid="f5" ref-type="fig">
<bold>Figure&#x000a0;5</bold>
</xref> shows the visualization of OA and DSC. From the figure, it is clear that as the proportion of labeled data increases from 10% to 30%, the evaluation metrics show significant improvement in the fully supervised method. However, from 30% to 100%, the improvement in the metrics is not as pronounced, highlighting that simply increasing the amount of labeled data does not significantly improve model performance, further emphasizing the importance of semi-supervised learning on the OCDC dataset. To visually demonstrate the improvement of our method over the fully supervised approach, we show the visual output results in <xref rid="f6" ref-type="fig">
<bold>Figure&#x000a0;6</bold>
</xref>. It can be seen that when only 10% labeled data is used, the fully supervised method fails to recognize a significant portion of the tumor region, while our method successfully identifies more tumor areas. Furthermore, the segmentation results from our method more accurately capture the tumor boundaries compared to the fully supervised method, demonstrating theaccuracy of the proposed boundary-aware enhancement module. This also explains why our method consistently outperforms the corresponding fully supervised methods in terms of HD95.</p><table-wrap position="float" id="T1"><label>Table&#x000a0;1</label><caption><p>Quantitative results of supervised and proposed methods using various proportions of labeled data on the OCDC dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="middle" rowspan="2" align="left" colspan="1">Method</th><th valign="middle" colspan="2" align="left" rowspan="1">Samples used</th><th valign="middle" colspan="4" align="center" rowspan="1">Metrics</th><th valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><th valign="middle" align="left" rowspan="1" colspan="1">M</th><th valign="middle" align="left" rowspan="1" colspan="1">N</th><th valign="middle" align="center" rowspan="1" colspan="1">OA(%) &#x02191;</th><th valign="middle" align="center" rowspan="1" colspan="1">AA(%) &#x02191;</th><th valign="middle" align="center" rowspan="1" colspan="1">DSC(%) &#x02191;</th><th valign="middle" align="center" rowspan="1" colspan="1">Jaccard(%)&#x02191;</th><th valign="top" align="center" rowspan="1" colspan="1">HD<sub>95</sub> &#x02193;</th></tr></thead><tbody><tr><td valign="middle" rowspan="4" align="left" colspan="1">SL</td><td valign="middle" align="left" rowspan="1" colspan="1">324</td><td valign="middle" align="left" rowspan="1" colspan="1">0</td><td valign="middle" align="center" rowspan="1" colspan="1">90.12</td><td valign="middle" align="center" rowspan="1" colspan="1">84.59</td><td valign="middle" align="center" rowspan="1" colspan="1">83.61</td><td valign="middle" align="center" rowspan="1" colspan="1">73.94</td><td valign="top" align="center" rowspan="1" colspan="1">11.48</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">96</td><td valign="middle" align="left" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">88.35</td><td valign="top" align="center" rowspan="1" colspan="1">84.06</td><td valign="top" align="center" rowspan="1" colspan="1">81.56</td><td valign="top" align="center" rowspan="1" colspan="1">71.00</td><td valign="top" align="center" rowspan="1" colspan="1">12.55</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">65</td><td valign="middle" align="left" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">85.96</td><td valign="top" align="center" rowspan="1" colspan="1">81.56</td><td valign="top" align="center" rowspan="1" colspan="1">78.49</td><td valign="top" align="center" rowspan="1" colspan="1">67.78</td><td valign="top" align="center" rowspan="1" colspan="1">13.60</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">32</td><td valign="middle" align="left" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">83.03</td><td valign="top" align="center" rowspan="1" colspan="1">78.99</td><td valign="top" align="center" rowspan="1" colspan="1">75.39</td><td valign="top" align="center" rowspan="1" colspan="1">63.79</td><td valign="top" align="center" rowspan="1" colspan="1">14.69</td></tr><tr><td valign="middle" rowspan="3" align="left" colspan="1">Ours</td><td valign="middle" align="left" rowspan="1" colspan="1">96</td><td valign="middle" align="left" rowspan="1" colspan="1">228</td><td valign="top" align="center" rowspan="1" colspan="1">90.27</td><td valign="top" align="center" rowspan="1" colspan="1">88.84</td><td valign="top" align="center" rowspan="1" colspan="1">84.31</td><td valign="top" align="center" rowspan="1" colspan="1">74.52</td><td valign="top" align="center" rowspan="1" colspan="1">11.82</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">65</td><td valign="middle" align="left" rowspan="1" colspan="1">259</td><td valign="top" align="center" rowspan="1" colspan="1">88.72</td><td valign="top" align="center" rowspan="1" colspan="1">88.26</td><td valign="top" align="center" rowspan="1" colspan="1">82.23</td><td valign="top" align="center" rowspan="1" colspan="1">75.82</td><td valign="top" align="center" rowspan="1" colspan="1">12.10</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">32</td><td valign="middle" align="left" rowspan="1" colspan="1">292</td><td valign="top" align="center" rowspan="1" colspan="1">85.12</td><td valign="top" align="center" rowspan="1" colspan="1">86.61</td><td valign="top" align="center" rowspan="1" colspan="1">77.78</td><td valign="top" align="center" rowspan="1" colspan="1">66.27</td><td valign="top" align="center" rowspan="1" colspan="1">13.93</td></tr></tbody></table><table-wrap-foot><fn><p>The results report the average from five-fold cross-validation experiments.</p></fn></table-wrap-foot></table-wrap><fig position="float" id="f5"><label>Figure&#x000a0;5</label><caption><p>Visualization of OA and DSC using different proportions of labeled data on the OCDC dataset.</p></caption><graphic xlink:href="fonc-15-1577198-g005" position="float"/></fig><fig position="float" id="f6"><label>Figure&#x000a0;6</label><caption><p>Visualization comparison of supervised and proposed methods using various proportions of labeled data on the OCDC dataset. Yellow regions represent tumor areas, and the rest are background.</p></caption><graphic xlink:href="fonc-15-1577198-g006" position="float"/></fig></sec><sec id="s4_4_2"><label>4.4.2</label><title>Comparison with other semi-supervised methods</title><p>To prove the effectiveness of the proposed method in semi-supervised scenarios, we conducted comparison experiments on the GIN and CCA datasets using 10% and 20% labeled data. We compared our method with six state-of-the-art semi-supervised methods: MT (<xref rid="B10" ref-type="bibr">10</xref>), UA-MT (<xref rid="B11" ref-type="bibr">11</xref>), CCT (<xref rid="B37" ref-type="bibr">37</xref>), CPS (<xref rid="B38" ref-type="bibr">38</xref>), DMMT (<xref rid="B31" ref-type="bibr">31</xref>), and SPCL (<xref rid="B39" ref-type="bibr">39</xref>). The quantitative experimental results are reported in <xref rid="T2" ref-type="table">
<bold>Table&#x000a0;2</bold>
</xref>.</p><table-wrap position="float" id="T2"><label>Table&#x000a0;2</label><caption><p>Quantitative results of different methods using various proportions of labeled data on the OCDC dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="middle" rowspan="2" align="left" colspan="1">Method</th><th valign="middle" colspan="2" align="left" rowspan="1">Samples used</th><th valign="middle" colspan="4" align="center" rowspan="1">Metrics</th><th valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><th valign="middle" align="left" rowspan="1" colspan="1">M</th><th valign="middle" align="left" rowspan="1" colspan="1">N</th><th valign="middle" align="center" rowspan="1" colspan="1">OA(%) &#x02191;</th><th valign="middle" align="center" rowspan="1" colspan="1">AA(%) &#x02191;</th><th valign="middle" align="center" rowspan="1" colspan="1">DSC(%) &#x02191;</th><th valign="middle" align="center" rowspan="1" colspan="1">Jaccard(%)&#x02191;</th><th valign="top" align="center" rowspan="1" colspan="1">HD<sub>95</sub> &#x02193;</th></tr></thead><tbody><tr><td valign="middle" rowspan="3" align="left" colspan="1">SL</td><td valign="middle" align="left" rowspan="1" colspan="1">324</td><td valign="middle" align="left" rowspan="1" colspan="1">0</td><td valign="middle" align="center" rowspan="1" colspan="1">90.12</td><td valign="middle" align="center" rowspan="1" colspan="1">84.59</td><td valign="middle" align="center" rowspan="1" colspan="1">83.61</td><td valign="middle" align="center" rowspan="1" colspan="1">73.94</td><td valign="top" align="center" rowspan="1" colspan="1">11.48</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">65</td><td valign="middle" align="left" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">85.96</td><td valign="top" align="center" rowspan="1" colspan="1">81.56</td><td valign="top" align="center" rowspan="1" colspan="1">78.49</td><td valign="top" align="center" rowspan="1" colspan="1">67.78</td><td valign="top" align="center" rowspan="1" colspan="1">13.60</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">32</td><td valign="middle" align="left" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">83.03</td><td valign="top" align="center" rowspan="1" colspan="1">78.99</td><td valign="top" align="center" rowspan="1" colspan="1">75.39</td><td valign="top" align="center" rowspan="1" colspan="1">63.79</td><td valign="top" align="center" rowspan="1" colspan="1">14.69</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">MT</td><td valign="middle" align="left" rowspan="1" colspan="1">32</td><td valign="middle" align="left" rowspan="1" colspan="1">292</td><td valign="top" align="center" rowspan="1" colspan="1">82.67</td><td valign="top" align="center" rowspan="1" colspan="1">78.66</td><td valign="top" align="center" rowspan="1" colspan="1">74.04</td><td valign="top" align="center" rowspan="1" colspan="1">62.07</td><td valign="top" align="center" rowspan="1" colspan="1">14.36</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">UA-MT</td><td valign="top" align="left" rowspan="1" colspan="1">32</td><td valign="top" align="left" rowspan="1" colspan="1">292</td><td valign="top" align="center" rowspan="1" colspan="1">82.00</td><td valign="top" align="center" rowspan="1" colspan="1">77.88</td><td valign="top" align="center" rowspan="1" colspan="1">73.56</td><td valign="top" align="center" rowspan="1" colspan="1">61.39</td><td valign="top" align="center" rowspan="1" colspan="1">14.49</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">CPS</td><td valign="top" align="left" rowspan="1" colspan="1">32</td><td valign="top" align="left" rowspan="1" colspan="1">292</td><td valign="top" align="center" rowspan="1" colspan="1">82.57</td><td valign="top" align="center" rowspan="1" colspan="1">78.97</td><td valign="top" align="center" rowspan="1" colspan="1">74.80</td><td valign="top" align="center" rowspan="1" colspan="1">62.90</td><td valign="top" align="center" rowspan="1" colspan="1">14.16</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">CCT</td><td valign="top" align="left" rowspan="1" colspan="1">32</td><td valign="top" align="left" rowspan="1" colspan="1">292</td><td valign="top" align="center" rowspan="1" colspan="1">80.90</td><td valign="top" align="center" rowspan="1" colspan="1">76.72</td><td valign="top" align="center" rowspan="1" colspan="1">72.77</td><td valign="top" align="center" rowspan="1" colspan="1">60.12</td><td valign="top" align="center" rowspan="1" colspan="1">14.50</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">DMMT</td><td valign="top" align="left" rowspan="1" colspan="1">32</td><td valign="top" align="left" rowspan="1" colspan="1">292</td><td valign="top" align="center" rowspan="1" colspan="1">81.43</td><td valign="top" align="center" rowspan="1" colspan="1">77.92</td><td valign="top" align="center" rowspan="1" colspan="1">72.51</td><td valign="top" align="center" rowspan="1" colspan="1">60.00</td><td valign="top" align="center" rowspan="1" colspan="1">14.03</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">SPCL</td><td valign="top" align="left" rowspan="1" colspan="1">32</td><td valign="top" align="left" rowspan="1" colspan="1">292</td><td valign="top" align="center" rowspan="1" colspan="1">83.09</td><td valign="top" align="center" rowspan="1" colspan="1">78.81</td><td valign="top" align="center" rowspan="1" colspan="1">75.66</td><td valign="top" align="center" rowspan="1" colspan="1">63.76</td><td valign="top" align="center" rowspan="1" colspan="1">14.55</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">Ours</td><td valign="top" align="left" rowspan="1" colspan="1">32</td><td valign="top" align="left" rowspan="1" colspan="1">292</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>85.12</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>86.61</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>77.78</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>66.27</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">13.93</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">MT</td><td valign="middle" align="left" rowspan="1" colspan="1">65</td><td valign="middle" align="left" rowspan="1" colspan="1">259</td><td valign="top" align="center" rowspan="1" colspan="1">87.03</td><td valign="top" align="center" rowspan="1" colspan="1">82.66</td><td valign="top" align="center" rowspan="1" colspan="1">80.14</td><td valign="top" align="center" rowspan="1" colspan="1">69.15</td><td valign="top" align="center" rowspan="1" colspan="1">13.18</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">UA-MT</td><td valign="middle" align="left" rowspan="1" colspan="1">65</td><td valign="top" align="left" rowspan="1" colspan="1">259</td><td valign="top" align="center" rowspan="1" colspan="1">85.51</td><td valign="top" align="center" rowspan="1" colspan="1">80.89</td><td valign="top" align="center" rowspan="1" colspan="1">77.43</td><td valign="top" align="center" rowspan="1" colspan="1">66.38</td><td valign="top" align="center" rowspan="1" colspan="1">13.58</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">CPS</td><td valign="middle" align="left" rowspan="1" colspan="1">65</td><td valign="top" align="left" rowspan="1" colspan="1">259</td><td valign="top" align="center" rowspan="1" colspan="1">87.10</td><td valign="top" align="center" rowspan="1" colspan="1">82.76</td><td valign="top" align="center" rowspan="1" colspan="1">79.69</td><td valign="top" align="center" rowspan="1" colspan="1">68.87</td><td valign="top" align="center" rowspan="1" colspan="1">13.26</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">CCT</td><td valign="middle" align="left" rowspan="1" colspan="1">65</td><td valign="top" align="left" rowspan="1" colspan="1">259</td><td valign="top" align="center" rowspan="1" colspan="1">83.82</td><td valign="top" align="center" rowspan="1" colspan="1">80.12</td><td valign="top" align="center" rowspan="1" colspan="1">76.44</td><td valign="top" align="center" rowspan="1" colspan="1">65.25</td><td valign="top" align="center" rowspan="1" colspan="1">12.87</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">DMMT</td><td valign="middle" align="left" rowspan="1" colspan="1">65</td><td valign="top" align="left" rowspan="1" colspan="1">259</td><td valign="top" align="center" rowspan="1" colspan="1">86.41</td><td valign="top" align="center" rowspan="1" colspan="1">82.55</td><td valign="top" align="center" rowspan="1" colspan="1">79.75</td><td valign="top" align="center" rowspan="1" colspan="1">68.62</td><td valign="top" align="center" rowspan="1" colspan="1">13.08</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">SPCL</td><td valign="middle" align="left" rowspan="1" colspan="1">65</td><td valign="top" align="left" rowspan="1" colspan="1">259</td><td valign="top" align="center" rowspan="1" colspan="1">86.15</td><td valign="top" align="center" rowspan="1" colspan="1">80.64</td><td valign="top" align="center" rowspan="1" colspan="1">76.88</td><td valign="top" align="center" rowspan="1" colspan="1">66.06</td><td valign="top" align="center" rowspan="1" colspan="1">13.30</td></tr><tr><td valign="middle" align="left" rowspan="1" colspan="1">Ours</td><td valign="middle" align="left" rowspan="1" colspan="1">65</td><td valign="top" align="left" rowspan="1" colspan="1">259</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>88.72</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>88.26</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>82.23</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>75.82</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">12.10</td></tr></tbody></table><table-wrap-foot><fn><p>The results report the average from five-fold cross-validation experiments, with the optimal results bolded and the suboptimal results underlined.</p></fn></table-wrap-foot></table-wrap><p>The table shows that when using 10% labeled data for training, the six semi-supervised methods do not achieve significant improvements compared to the fully supervised method, especially the teacher-student network-based models (MT, UA-MT, etc.), whose segmentation results are worse than the fully supervised approach. Our method outperforms the second-best method across all five evaluation metrics by 2.03%, 7.64%, 2.12%, 3.37% and 0.2, respectively. In this setup, the SPCL method, which utilizes contrastive learning, achieves relatively good results among the other semi-supervised methods. However, when the labeled data increases to 20%, this advantage does not persist. In contrast, our method achieves the best results in both settings. <xref rid="f7" ref-type="fig">
<bold>Figures&#x000a0;7</bold>
</xref>, <xref rid="f8" ref-type="fig">
<bold>8</bold>
</xref> show the visual segmentation results for 10% and 20% labeled data. From the figures, it is evident that our method&#x02019;s segmentation results are much closer to the ground truth, especially in terms of accurately delineating boundaries. Compared to the UA-MT method, which only uses entropy for uncertainty estimation, our method significantly reduces false positive samples, demonstrating that the proposed uncertainty estimation based on data transformation effectively reduces the teacher model&#x02019;s prediction errors.</p><fig position="float" id="f7"><label>Figure&#x000a0;7</label><caption><p>Visualization comparison of the proposed method with other semi-supervised methods using 10% labeled data on the OCDC dataset. Yellow regions represent tumor areas, and the rest are background.</p></caption><graphic xlink:href="fonc-15-1577198-g007" position="float"/></fig><fig position="float" id="f8"><label>Figure&#x000a0;8</label><caption><p>Visualization comparison of the proposed method with other semi-supervised methods using 20% labeled data on the OCDC dataset. Yellow regions represent tumor areas, and the rest are background.</p></caption><graphic xlink:href="fonc-15-1577198-g008" position="float"/></fig><p>To further validate the effectiveness of the proposed method under limited annotation conditions, we conducted paired t-tests between our method and several baseline approaches (including fully supervised and representative semi-supervised methods) under two training scenarios using 10% and 20% labeled data. The significance testing results are summarized in <xref rid="T3" ref-type="table">
<bold>Table&#x000a0;3</bold>
</xref>. As observed, all p-values are less than 0.05, indicating that the performance improvements of our method over the baselines are statistically significant. These results demonstrate the robustness and superiority of the proposed method in low-label regimes.</p><table-wrap position="float" id="T3"><label>Table&#x000a0;3</label><caption><p>P-values of paired t-tests between the proposed method and other methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="middle" align="center" rowspan="1" colspan="1">Samples used</th><th valign="middle" align="center" rowspan="1" colspan="1">Metrics</th><th valign="middle" align="center" rowspan="1" colspan="1">SL</th><th valign="middle" align="center" rowspan="1" colspan="1">MT</th><th valign="middle" align="center" rowspan="1" colspan="1">UA-MT</th><th valign="middle" align="center" rowspan="1" colspan="1">CPS</th><th valign="middle" align="center" rowspan="1" colspan="1">CCT</th><th valign="middle" align="center" rowspan="1" colspan="1">DMMT</th><th valign="middle" align="center" rowspan="1" colspan="1">SPCL</th></tr></thead><tbody><tr><td valign="middle" rowspan="2" align="center" colspan="1">10%</td><td valign="top" align="center" rowspan="1" colspan="1">OA</td><td valign="top" align="center" rowspan="1" colspan="1">0.013</td><td valign="top" align="center" rowspan="1" colspan="1">0.027</td><td valign="top" align="center" rowspan="1" colspan="1">0.025</td><td valign="top" align="center" rowspan="1" colspan="1">0.019</td><td valign="top" align="center" rowspan="1" colspan="1">0.023</td><td valign="top" align="center" rowspan="1" colspan="1">0.036</td><td valign="top" align="center" rowspan="1" colspan="1">0.042</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">DSC</td><td valign="top" align="center" rowspan="1" colspan="1">0.020</td><td valign="top" align="center" rowspan="1" colspan="1">0.031</td><td valign="top" align="center" rowspan="1" colspan="1">0.029</td><td valign="top" align="center" rowspan="1" colspan="1">0.022</td><td valign="top" align="center" rowspan="1" colspan="1">0.022</td><td valign="top" align="center" rowspan="1" colspan="1">0.041</td><td valign="top" align="center" rowspan="1" colspan="1">0.038</td></tr><tr><td valign="middle" rowspan="2" align="center" colspan="1">20%</td><td valign="top" align="center" rowspan="1" colspan="1">OA</td><td valign="top" align="center" rowspan="1" colspan="1">0.010</td><td valign="top" align="center" rowspan="1" colspan="1">0.029</td><td valign="top" align="center" rowspan="1" colspan="1">0.037</td><td valign="top" align="center" rowspan="1" colspan="1">0.039</td><td valign="top" align="center" rowspan="1" colspan="1">0.038</td><td valign="top" align="center" rowspan="1" colspan="1">0.046</td><td valign="top" align="center" rowspan="1" colspan="1">0.029</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">DSC</td><td valign="top" align="center" rowspan="1" colspan="1">0.018</td><td valign="top" align="center" rowspan="1" colspan="1">0.035</td><td valign="top" align="center" rowspan="1" colspan="1">0.044</td><td valign="top" align="center" rowspan="1" colspan="1">0.040</td><td valign="top" align="center" rowspan="1" colspan="1">0.032</td><td valign="top" align="center" rowspan="1" colspan="1">0.042</td><td valign="top" align="center" rowspan="1" colspan="1">0.033</td></tr></tbody></table></table-wrap><p>In addition, we evaluated the model complexity and inference time of all compared methods, and the results are reported in <xref rid="T4" ref-type="table">
<bold>Table&#x000a0;4</bold>
</xref>. Since the SL, MT, UA-MT, CPS, and SPCL methods all use a standard 2D U-Net as their feature extractor, they have similar numbers of parameters and inference times. In contrast, our method introduces the BAEM module, which slightly increases the model size and inference time.</p><table-wrap position="float" id="T4"><label>Table&#x000a0;4</label><caption><p>Parameter count and inference time on the entire dataset for the proposed method and comparative methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="middle" align="left" rowspan="1" colspan="1"/><th valign="middle" align="center" rowspan="1" colspan="1">Ours</th><th valign="middle" align="center" rowspan="1" colspan="1">SL</th><th valign="middle" align="center" rowspan="1" colspan="1">MT</th><th valign="middle" align="center" rowspan="1" colspan="1">UA-MT</th><th valign="middle" align="center" rowspan="1" colspan="1">CPS</th><th valign="middle" align="center" rowspan="1" colspan="1">CCT</th><th valign="middle" align="center" rowspan="1" colspan="1">DMMT</th><th valign="middle" align="center" rowspan="1" colspan="1">SPCL</th></tr></thead><tbody><tr><td valign="middle" align="left" rowspan="1" colspan="1">Params(M)</td><td valign="top" align="center" rowspan="1" colspan="1">1.83</td><td valign="top" align="center" rowspan="1" colspan="1">1.81</td><td valign="top" align="center" rowspan="1" colspan="1">1.81</td><td valign="top" align="center" rowspan="1" colspan="1">1.81</td><td valign="top" align="center" rowspan="1" colspan="1">1.81</td><td valign="top" align="center" rowspan="1" colspan="1">1.82</td><td valign="top" align="center" rowspan="1" colspan="1">1.71</td><td valign="top" align="center" rowspan="1" colspan="1">1.81</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Times(s)</td><td valign="top" align="center" rowspan="1" colspan="1">92.23</td><td valign="top" align="center" rowspan="1" colspan="1">67.85</td><td valign="top" align="center" rowspan="1" colspan="1">69.02</td><td valign="top" align="center" rowspan="1" colspan="1">67.94</td><td valign="top" align="center" rowspan="1" colspan="1">68.43</td><td valign="top" align="center" rowspan="1" colspan="1">73.21</td><td valign="top" align="center" rowspan="1" colspan="1">73.32</td><td valign="top" align="center" rowspan="1" colspan="1">69.21</td></tr></tbody></table></table-wrap></sec></sec><sec id="s4_5"><label>4.5</label><title>Ablation study</title><p>In this section, we design experiments to validate the proposed methods, including the TB-UE approach, the MS-CL method, and the MAEM. The quantitative results of the experiments are reported in <xref rid="T5" ref-type="table">
<bold>Table&#x000a0;5</bold>
</xref>. Firstly, we constructed a baseline model, named &#x0201c;Basic&#x0201d;, which is based on the MT network, by removing the aforementioned three methods. As the uncertainty estimation methods, we removed consists of two parts: entropy- based uncertainty estimation (EU) and data transformation-baseduncertainty estimation (TU), we respectively added EU and TU to the Basic model. Notably, when the EU module is added, the network turns into the UA-MT model. The results in <xref rid="T5" ref-type="table">
<bold>Table&#x000a0;5</bold>
</xref> show a noticeable reduction in evaluation metrics when the EU module is added compared to the MT model. This suggests that relying solely on pixel entropy for uncertainty estimation is insufficient for the complex scenarios encountered in oral pathology image (OPI) segmentation. However, when the TU module is added on its own or in combination with the EU module, the evaluation metrics show significant improvement. This demonstrates that the TU module enhances the accuracy of pseudo-labels generated by the teacher model, and it confirms the complementarity between TU and EU modules.</p><table-wrap position="float" id="T5"><label>Table&#x000a0;5</label><caption><p>Quantitative results of ablation experiments using 20% labeled data on the OCDC dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1">Basic</th><th valign="top" align="center" rowspan="1" colspan="1">EU</th><th valign="top" align="center" rowspan="1" colspan="1">TU</th><th valign="top" align="center" rowspan="1" colspan="1">BAEM</th><th valign="top" align="center" rowspan="1" colspan="1">MS-CL</th><th valign="middle" align="center" rowspan="1" colspan="1">OA(%)&#x02191;</th><th valign="middle" align="center" rowspan="1" colspan="1">AA(%)&#x02191;</th><th valign="middle" align="center" rowspan="1" colspan="1">DSC(%)&#x02191;</th><th valign="middle" align="center" rowspan="1" colspan="1">Jaccard(%)&#x02191;</th><th valign="top" align="center" rowspan="1" colspan="1">HD<sub>95</sub> &#x02193;</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Model1</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">87.03</td><td valign="top" align="center" rowspan="1" colspan="1">82.66</td><td valign="top" align="center" rowspan="1" colspan="1">80.14</td><td valign="top" align="center" rowspan="1" colspan="1">69.15</td><td valign="top" align="center" rowspan="1" colspan="1">13.18</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Model2</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">85.51</td><td valign="top" align="center" rowspan="1" colspan="1">80.89</td><td valign="top" align="center" rowspan="1" colspan="1">77.43</td><td valign="top" align="center" rowspan="1" colspan="1">66.38</td><td valign="top" align="center" rowspan="1" colspan="1">13.58</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Model3</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">87.16</td><td valign="top" align="center" rowspan="1" colspan="1">82.59</td><td valign="top" align="center" rowspan="1" colspan="1">80.70</td><td valign="top" align="center" rowspan="1" colspan="1">70.17</td><td valign="top" align="center" rowspan="1" colspan="1">13.05</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Model4</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">87.84</td><td valign="top" align="center" rowspan="1" colspan="1">84.27</td><td valign="top" align="center" rowspan="1" colspan="1">81.55</td><td valign="top" align="center" rowspan="1" colspan="1">70.94</td><td valign="top" align="center" rowspan="1" colspan="1">12.98</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Model5</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">88.05</td><td valign="top" align="center" rowspan="1" colspan="1">83.48</td><td valign="top" align="center" rowspan="1" colspan="1">82.32</td><td valign="top" align="center" rowspan="1" colspan="1">71.53</td><td valign="top" align="center" rowspan="1" colspan="1">12.33</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Model6</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">88.62</td><td valign="top" align="center" rowspan="1" colspan="1">83.62</td><td valign="top" align="center" rowspan="1" colspan="1">82.13</td><td valign="top" align="center" rowspan="1" colspan="1">71.66</td><td valign="top" align="center" rowspan="1" colspan="1">12.71</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Model7</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<italic>&#x0221a;</italic>
</td><td valign="top" align="center" rowspan="1" colspan="1">88.72</td><td valign="top" align="center" rowspan="1" colspan="1">88.26</td><td valign="top" align="center" rowspan="1" colspan="1">82.23</td><td valign="top" align="center" rowspan="1" colspan="1">75.82</td><td valign="top" align="center" rowspan="1" colspan="1">12.10</td></tr></tbody></table><table-wrap-foot><fn><p>The results report the average from five-fold cross-validation experiments.</p></fn></table-wrap-foot></table-wrap><p>Subsequently, we added the BAEM and MS-CL modules, leading to improvements in all five evaluation metrics, proving the effectiveness of these two methods. Notably, after incorporating the BAEM module, the HD95 decreased from 12.98 to 12.33, indicating its effectiveness in improving boundary segmentation accuracy. Finally, the best performance was achieved when all three modules were integrated into the basic model. To further illustrate the contribution of the proposed MS-CL module to class-specific feature discrimination, we visualized the feature representations obtained before the final output layer using UMAP, as shown in <xref rid="f9" ref-type="fig">
<bold>Figure&#x000a0;9</bold>
</xref>. The visualizations correspond to Model 6 and Model 7. It can be observed that Model 7, which includes the MS-CL module, exhibits more compact intra-class clustering and clearer inter-class boundaries. These results suggest that MS-CL effectively enhances intra-class consistency and inter-class separability in the learned representations.</p><fig position="float" id="f9"><label>Figure&#x000a0;9</label><caption><p>Feature visualization diagrams of model6 and model7.</p></caption><graphic xlink:href="fonc-15-1577198-g009" position="float"/></fig><p>It is worth noting that the increase in the Jaccard index is more significant than that of the DSC, which can be attributed to the fact that the values reported in the table are the average results from five-fold cross-validation, rather than a single trial. From the formulas for DSC and Jaccard, we can derive the conversion formula between DSC and Jaccard for a single trial (<xref rid="eq15" ref-type="disp-formula">Equation 15</xref>):</p><disp-formula id="eq15">
<label>(15)</label>
<mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mi>J</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>J</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</disp-formula><p>Thus, the relationship can be expressed as the following function: <inline-formula>
<mml:math id="im90" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>. If we have n points <inline-formula>
<mml:math id="im91" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> the function value at the mean of these points is: <inline-formula>
<mml:math id="im92" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>, and the mean of the function values can be expressed as: <inline-formula>
<mml:math id="im93" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>. Thus, the difference <inline-formula>
<mml:math id="im94" display="inline" overflow="scroll"><mml:mi>&#x00394;</mml:mi></mml:math>
</inline-formula> is defined as (<xref rid="eq16" ref-type="disp-formula">Equation 16</xref>):</p><disp-formula id="eq16">
<label>(16)</label>
<mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:mi>&#x00394;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</disp-formula><p>The function <inline-formula>
<mml:math id="im95" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula> is an increasing convex function, and therefore satisfies Jensen&#x02019;s inequality (<xref rid="eq17" ref-type="disp-formula">Equation 17</xref>):</p><disp-formula id="eq17">
<label>(17)</label>
<mml:math id="M17" display="block" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:math>
</disp-formula><p>Thus, the difference <inline-formula>
<mml:math id="im96" display="inline" overflow="scroll"><mml:mi>&#x00394;</mml:mi></mml:math>
</inline-formula> is non-negative. Next, we perform a first and second-order Taylor expansion for each <inline-formula>
<mml:math id="im97" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula> (<xref rid="eq18" ref-type="disp-formula">Equation 18</xref>):</p><disp-formula id="eq18">
<label>(18)</label>
<mml:math id="M18" display="block" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02248;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>f</mml:mi><mml:mo>&#x02033;</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math>
</disp-formula><p>where <inline-formula>
<mml:math id="im98" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>. Substituting these expansions into the difference calculation (<xref rid="eq19" ref-type="disp-formula">Equation 19</xref>):</p><disp-formula id="eq19">
<label>(19)</label>
<mml:math id="M19" display="block" overflow="scroll"><mml:mrow><mml:mi>&#x00394;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>f</mml:mi><mml:mo>&#x02033;</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</disp-formula><p>Since <inline-formula>
<mml:math id="im99" display="inline" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math>
</inline-formula>, we obtain (<xref rid="eq20" ref-type="disp-formula">Equation 20</xref>):</p><disp-formula id="eq20">
<label>(20)</label>
<mml:math id="M20" display="block" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>&#x00394;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mstyle><mml:msup><mml:mi>f</mml:mi><mml:mo>&#x02033;</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x02003;</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>f</mml:mi><mml:mo>&#x02033;</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula><p>Recognizing that <inline-formula>
<mml:math id="im100" display="inline" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula> is the sum of the squared deviations of the samples, which is related to the standard deviation <inline-formula>
<mml:math id="im101" display="inline" overflow="scroll"><mml:mi>&#x003c3;</mml:mi></mml:math>
</inline-formula> as follows (<xref rid="eq21" ref-type="disp-formula">Equation 21</xref>):</p><disp-formula id="eq21">
<label>(21)</label>
<mml:math id="M21" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</disp-formula><p>Thus, the difference <inline-formula>
<mml:math id="im102" display="inline" overflow="scroll"><mml:mi>&#x00394;</mml:mi></mml:math>
</inline-formula> can be expressed as (<xref rid="eq22" ref-type="disp-formula">Equation 22</xref>):</p><disp-formula id="eq22">
<label>(22)</label>
<mml:math id="M22" display="block" overflow="scroll"><mml:mrow><mml:mi>&#x00394;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>f</mml:mi><mml:mo>&#x02033;</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math>
</disp-formula><p>This implies that the difference <inline-formula>
<mml:math id="im103" display="inline" overflow="scroll"><mml:mi>&#x00394;</mml:mi></mml:math>
</inline-formula> is directly proportional to the square of the standard deviation (i.e., variance), and also proportional to the sample size n. In the quantitative results presented in this paper, we report the average results from five-fold cross-validation. Therefore, the smaller the difference in DSC and Jaccard, the lower the variance of the method. Based on the above analysis, we conclude that the incorporation of the three methods proposed in this paper significantly improves the model&#x02019;s generalization performance.</p></sec><sec id="s4_6"><label>4.6</label><title>Hyperparameter Study</title><p>To explore the effect of different scales and scale weights on segmentation performance in MS-CL, we conducted experiments with D values ranging from 1 to 5. Considering that smaller scales have higher resolution, the scale weights <inline-formula>
<mml:math id="im104" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b2;</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1,5</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>, should be negatively correlated with ii. The quantitative results for different hyperparameter settings are summarized in <xref rid="T6" ref-type="table">
<bold>Table&#x000a0;6</bold>
</xref>. The performance was better when <inline-formula>
<mml:math id="im105" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math>
</inline-formula> compared to single-scale models, indicating that multi-scale information can help the model capture more diverse features. However, when D &#x0003e; 3, the performance dropped below that of single-scale models, suggesting that the bottleneck layer and high-scale information were not fully leveraged in the contrastive learning setup.</p><table-wrap position="float" id="T6"><label>Table&#x000a0;6</label><caption><p>Quantitative results of hyperparameter experiments using 20% labeled data on the OCDC dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">D</th><th valign="top" align="center" rowspan="1" colspan="1">&#x003b2;<sup>1</sup>
</th><th valign="top" align="center" rowspan="1" colspan="1">&#x003b2;<sup>2</sup>
</th><th valign="top" align="center" rowspan="1" colspan="1">&#x003b2;<sup>3</sup>
</th><th valign="top" align="center" rowspan="1" colspan="1">&#x003b2;<sup>6</sup>
</th><th valign="top" align="center" rowspan="1" colspan="1">&#x003b2;<sup>5</sup>
</th><th valign="middle" align="center" rowspan="1" colspan="1">OA(%)&#x02191;</th><th valign="middle" align="center" rowspan="1" colspan="1">AA(%)&#x02191;</th><th valign="middle" align="center" rowspan="1" colspan="1">DSC(%)&#x02191;</th><th valign="middle" align="center" rowspan="1" colspan="1">Jaccard(%)&#x02191;</th><th valign="top" align="center" rowspan="1" colspan="1">HD<sub>95</sub> &#x02193;</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">1</td><td valign="top" align="center" rowspan="1" colspan="1">1</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">88.42</td><td valign="top" align="center" rowspan="1" colspan="1">84.42</td><td valign="top" align="center" rowspan="1" colspan="1">81.64</td><td valign="top" align="center" rowspan="1" colspan="1">70.59</td><td valign="top" align="center" rowspan="1" colspan="1">12.39</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">0.7</td><td valign="top" align="center" rowspan="1" colspan="1">0.3</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">88.25</td><td valign="top" align="center" rowspan="1" colspan="1">87.56</td><td valign="top" align="center" rowspan="1" colspan="1">81.34</td><td valign="top" align="center" rowspan="1" colspan="1">70.82</td><td valign="top" align="center" rowspan="1" colspan="1">12.22</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">0.7</td><td valign="top" align="center" rowspan="1" colspan="1">0.2</td><td valign="top" align="center" rowspan="1" colspan="1">0.1</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">88.26</td><td valign="top" align="center" rowspan="1" colspan="1">87.92</td><td valign="top" align="center" rowspan="1" colspan="1">81.69</td><td valign="top" align="center" rowspan="1" colspan="1">71.20</td><td valign="top" align="center" rowspan="1" colspan="1">12.29</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">0.6</td><td valign="top" align="center" rowspan="1" colspan="1">0.3</td><td valign="top" align="center" rowspan="1" colspan="1">0.1</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">88.72</td><td valign="top" align="center" rowspan="1" colspan="1">88.26</td><td valign="top" align="center" rowspan="1" colspan="1">82.23</td><td valign="top" align="center" rowspan="1" colspan="1">75.82</td><td valign="top" align="center" rowspan="1" colspan="1">12.10</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">0.4</td><td valign="top" align="center" rowspan="1" colspan="1">0.3</td><td valign="top" align="center" rowspan="1" colspan="1">0.2</td><td valign="top" align="center" rowspan="1" colspan="1">0.1</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">88.17</td><td valign="top" align="center" rowspan="1" colspan="1">82.43</td><td valign="top" align="center" rowspan="1" colspan="1">81.10</td><td valign="top" align="center" rowspan="1" colspan="1">70.00</td><td valign="top" align="center" rowspan="1" colspan="1">12.44</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">5</td><td valign="top" align="center" rowspan="1" colspan="1">0.4</td><td valign="top" align="center" rowspan="1" colspan="1">0.2</td><td valign="top" align="center" rowspan="1" colspan="1">0.2</td><td valign="top" align="center" rowspan="1" colspan="1">0.1</td><td valign="top" align="center" rowspan="1" colspan="1">0.1</td><td valign="top" align="center" rowspan="1" colspan="1">87.44</td><td valign="top" align="center" rowspan="1" colspan="1">83.44</td><td valign="top" align="center" rowspan="1" colspan="1">82.36</td><td valign="top" align="center" rowspan="1" colspan="1">71.72</td><td valign="top" align="center" rowspan="1" colspan="1">12.03</td></tr></tbody></table><table-wrap-foot><fn><p>The results report the average from five-fold cross-validation experiments.</p></fn></table-wrap-foot></table-wrap></sec></sec><sec sec-type="conclusions" id="s5"><label>5</label><title>Conclusions</title><p>This paper proposes a semi-supervised segmentation method for OPI based on transformation uncertainty and multi-scale contrastive learning. The method leverages a small amount of labeled data and a large amount of unlabeled data to jointly train the model, addressing the limitation of label scarcity and improving segmentation performance for OPI. In our method, we design a TB-UE approach that evaluates the model&#x02019;s confidence on predictions for data transformed using different methods. This approach effectively mitigates the impact of semantic discrepancies between teacher model predictions and ground truth labels. Furthermore, we introduce a MS-CL approach, which enhances intra-class similarity and inter-class separability, while reducing the similarity between the teacher and student models, fostering more diverse feature representations. Additionally, we propose a boundary-aware U-Net model to capture the boundary information of OPI and integrate it with spatial features to improve segmentation accuracy. Extensive experiments on the OCDC dataset demonstrate the superiority of our method over fully supervised and other semi-supervised methods, providing new insights for alleviating data scarcity in pathology image segmentation.</p><p>Although the focus of this work is on methodological innovation, it is worth noting the potential clinical implications of the proposed model. Accurate and automated segmentation of pathological structures can provide critical support for pathologists by highlighting tumor boundaries and reducing diagnostic subjectivity. Furthermore, the proposed method is compatible with visual explanation tools such as class activation maps (CAMs), attention heatmaps, or uncertainty visualizations, which may enhance interpretability and foster trust in clinical practice. Integrating such models into digital pathology workflows could assist in pre-screening, prioritization, and quality assurance tasks. Future work may explore user studies or expert feedback to further validate the model&#x02019;s utility in real-world diagnostic settings.</p><p>We also acknowledge several limitations of the current study. First, while our method achieves higher segmentation accuracy, it introduces additional computational cost due to the inclusion of the TB-UE and MS-CL modules. This results in increased model parameters and inference time. Future work will explore lightweight architectures or model compression strategies to reduce computational overhead while maintaining performance. Second, although the proposed approach is designed for oral squamous cell carcinoma, we have not yet verified its generalizability to other cancer sites. Evaluating the model&#x02019;s transferability to other histopathological datasets&#x02014;such as those related to lung, breast, or prostate cancer&#x02014;will be a key direction in our future research. Lastly, while this paper emphasizes pixel-level annotation efficiency through semi-supervised learning, we recognize that obtaining fine-grained pathology annotations remains labor-intensive. To further reduce annotation costs, we plan to investigate weaker forms of supervision, such as image-level labels, scribbles, or pathologist sketches, potentially combined with active learning techniques.</p></sec></body><back><sec sec-type="data-availability" id="s6"><title>Data availability statement</title><p>Publicly available datasets were analyzed in this study. This data can be found here: <uri xlink:href="https://data.mendeley.com/datasets/9bsc36jyrt/1">https://data.mendeley.com/datasets/9bsc36jyrt/1</uri>.</p></sec><sec sec-type="author-contributions" id="s7"><title>Author contributions</title><p>RW: Conceptualization, Formal Analysis, Methodology, Software, Visualization, Writing &#x02013; original draft. CL: Conceptualization, Formal Analysis, Investigation, Resources, Writing &#x02013; review &#x00026; editing. LY: Funding acquisition, Project administration, Resources, Supervision, Writing &#x02013; review &#x00026; editing.</p></sec><sec sec-type="COI-statement" id="s9"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="ai-statement" id="s10"><title>Generative AI statement</title><p>The author(s) declare that no Generative AI was used in the creation of this manuscript.</p></sec><sec sec-type="disclaimer" id="s11"><title>Publisher&#x02019;s note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><ref-list><title>References</title><ref id="B1"><label>1</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Sung</surname><given-names>H</given-names></name><name><surname>Ferlay</surname><given-names>J</given-names></name><name><surname>Siegel</surname><given-names>RL</given-names></name><name><surname>Laversanne</surname><given-names>M</given-names></name><name><surname>Soerjomataram</surname><given-names>I</given-names></name><name><surname>Jemal</surname><given-names>A</given-names></name><etal/></person-group>. <article-title>Global cancer statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries</article-title>. <source>CA Cancer J Clin</source>. (<year>2021</year>) <volume>71</volume>:<page-range>209&#x02013;49</page-range>. doi:&#x000a0;<pub-id pub-id-type="doi">10.3322/caac.21660</pub-id>
</mixed-citation></ref><ref id="B2"><label>2</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ibrahim</surname><given-names>SA</given-names></name><name><surname>Ahmed</surname><given-names>ANA</given-names></name><name><surname>Elsersy</surname><given-names>HA</given-names></name><name><surname>Darahem</surname><given-names>IMH</given-names></name></person-group>. <article-title>Elective neck dissection in T1/T2 oral squamous cell carcinoma with N0 neck: essential or not</article-title>? <source>A systematic Rev meta-analysis Eur Arch Oto-Rhino-Laryngology</source>. (<year>2020</year>) <volume>277</volume>:<page-range>1741&#x02013;52</page-range>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1007/s00405-020-05866-3</pub-id>
</mixed-citation></ref><ref id="B3"><label>3</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Ventura-Diaz</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>He</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Weir</surname><given-names>A</given-names></name><etal/></person-group>. <article-title>An explainable longitudinal multi-modal fusion model for predicting neoadjuvant therapy response in women with breast cancer</article-title>. <source>Nat Commun</source>. (<year>2024</year>) <volume>15</volume>:<fpage>9613</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1038/s41467-024-53450-8</pub-id>
<pub-id pub-id-type="pmid">39511143</pub-id>
</mixed-citation></ref><ref id="B4"><label>4</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Omar</surname><given-names>M</given-names></name><name><surname>Alexanderani</surname><given-names>MK</given-names></name><name><surname>Valencia</surname><given-names>I</given-names></name><name><surname>Loda</surname><given-names>M</given-names></name><name><surname>Marchionni</surname><given-names>L</given-names></name></person-group>. <article-title>Applications of digital pathology in cancer: A comprehensive review</article-title>. <source>Annu Rev Cancer Biol</source>. (<year>2024</year>) <volume>8</volume>:<page-range>245&#x02013;68</page-range>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1146/annurev-cancerbio-062822-010523</pub-id>
</mixed-citation></ref><ref id="B5"><label>5</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Dos Santos</surname><given-names>DFD</given-names></name><name><surname>de Faria</surname><given-names>PR</given-names></name><name><surname>Traven&#x000e7;olo</surname><given-names>BAN</given-names></name><name><surname>do Nascimento</surname><given-names>MZ</given-names></name></person-group>. <article-title>Influence of data augmentation strategies on the segmentation of Oral histological images using fully convolutional neural networks</article-title>. <source>J Digit Imaging</source>. (<year>2023</year>) <volume>36</volume>:<page-range>1608&#x02013;23</page-range>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1007/s10278-023-00814-z</pub-id>
</mixed-citation></ref><ref id="B6"><label>6</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ahmed</surname><given-names>IA</given-names></name><name><surname>Senan</surname><given-names>EM</given-names></name><name><surname>Shatnawi</surname><given-names>HSA</given-names></name></person-group>. <article-title>Analysis of histopathological images for early diagnosis of oral squamous cell carcinoma by hybrid systems based on CNN fusion features</article-title>. <source>Int J Intelligent Syst</source>. (<year>2023</year>) <volume>2023</volume>:<fpage>2662719</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1155/2023/2662719</pub-id>
</mixed-citation></ref><ref id="B7"><label>7</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Guo</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name><etal/></person-group>. <article-title>FD-net: feature distillation network for oral squamous cell carcinoma lymph node segmentation in hyperspectral imagery</article-title>. <source>IEEE J BioMed Health Inform</source>. (<year>2024</year>) <volume>28</volume>:<page-range>1552&#x02013;63</page-range>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1109/JBHI.2024.3350245</pub-id>
</mixed-citation></ref><ref id="B8"><label>8</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Qin</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name></person-group>. <article-title>URCA: Uncertainty-based region clipping algorithm for semi-supervised medical image segmentation</article-title>. <source>Comput Methods Programs BioMed</source>. (<year>2024</year>) <volume>254</volume>:<fpage>108278</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.cmpb.2024.108278</pub-id>
<pub-id pub-id-type="pmid">38878360</pub-id>
</mixed-citation></ref><ref id="B9"><label>9</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Gleber-Netto</surname><given-names>FO</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Jin</surname><given-names>KW</given-names></name><name><surname>Yang</surname><given-names>DM</given-names></name><name><surname>Gillenwater</surname><given-names>AM</given-names></name><etal/></person-group>. <article-title>A deep learning onion peeling approach to measure oral epithelium layer number</article-title>. <source>Cancers (Basel)</source>. (<year>2023</year>) <volume>15</volume>:<fpage>3891</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.3390/cancers15153891</pub-id>
<pub-id pub-id-type="pmid">37568707</pub-id>
</mixed-citation></ref><ref id="B10"><label>10</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Gai</surname><given-names>D</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Min</surname><given-names>W</given-names></name><name><surname>Geng</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Zhu</surname><given-names>M</given-names></name><etal/></person-group>. <article-title>SDMI-Net: Spatially Dependent Mutual Information Network for semi-supervised medical image segmentation</article-title>. <source>Comput Biol Med</source>. (<year>2024</year>) <volume>174</volume>:<fpage>108374</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.compbiomed.2024.108374</pub-id>
<pub-id pub-id-type="pmid">38582003</pub-id>
</mixed-citation></ref><ref id="B11"><label>11</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Tarvainen</surname><given-names>A</given-names></name><name><surname>Valpola</surname><given-names>H</given-names></name></person-group>. <article-title>Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</article-title>. In: <conf-name>Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS&#x02019;17)</conf-name>. (<year>2017</year>) pp. <page-range>1195&#x02013;204</page-range>.</mixed-citation></ref><ref id="B12"><label>12</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Yu</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Fu</surname><given-names>CW</given-names></name><name><surname>Heng</surname><given-names>PA</given-names></name></person-group>. <article-title>Uncertainty-aware self-ensembling model for semi-supervised 3D left atrium segmentation</article-title>. In: <person-group person-group-type="editor"><name><surname>Shen</surname><given-names>D</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Peters</surname><given-names>TM</given-names></name><name><surname>Staib</surname><given-names>LH</given-names></name><name><surname>Essert</surname><given-names>C</given-names></name><name><surname>Zhou</surname><given-names>S</given-names></name><etal/></person-group> editors. <source>Medical Image Computing and Computer-Assisted Intervention &#x02013; MICCAI 2019</source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name> (<year>2019</year>). p. <fpage>605</fpage>&#x02013;<lpage>13</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-030-32245-8_67</pub-id>
</mixed-citation></ref><ref id="B13"><label>13</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lu</surname><given-names>S</given-names></name><name><surname>Yan</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>W</given-names></name><name><surname>Cheng</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>G</given-names></name></person-group>. <article-title>Dual consistency regularization with subjective logic for semi-supervised medical image segmentation</article-title>. <source>Comput Biol Med</source>. (<year>2024</year>) <volume>170</volume>:<fpage>107991</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.compbiomed.2024.107991</pub-id>
<pub-id pub-id-type="pmid">38242016</pub-id>
</mixed-citation></ref><ref id="B14"><label>14</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lu</surname><given-names>L</given-names></name><name><surname>Yin</surname><given-names>M</given-names></name><name><surname>Fu</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>F</given-names></name></person-group>. <article-title>Uncertainty-aware pseudo-label and consistency for semi-supervised medical image segmentation</article-title>. <source>BioMed Signal Process Control</source>. (<year>2023</year>) <volume>79</volume>:<fpage>104203</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.bspc.2022.104203</pub-id>
</mixed-citation></ref><ref id="B15"><label>15</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lyu</surname><given-names>J</given-names></name><name><surname>Sui</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Dou</surname><given-names>Q</given-names></name><name><surname>Qin</surname><given-names>J</given-names></name></person-group>. <article-title>Adaptive feature aggregation based multi-task learning for uncertainty-guided semi-supervised medical image segmentation</article-title>. <source>Expert Syst Appl</source>. (<year>2023</year>) <volume>232</volume>:<fpage>120836</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.eswa.2023.120836</pub-id>
</mixed-citation></ref><ref id="B16"><label>16</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Y</given-names></name><name><surname>Liao</surname><given-names>K</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name><name><surname>Guo</surname><given-names>X</given-names></name></person-group>. <article-title>Boundary attention with multi-task consistency constraints for semi-supervised 2D echocardiography segmentation</article-title>. <source>Comput Biol Med</source>. (<year>2024</year>) <volume>171</volume>:<fpage>108100</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.compbiomed.2024.108100</pub-id>
<pub-id pub-id-type="pmid">38340441</pub-id>
</mixed-citation></ref><ref id="B17"><label>17</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Ma</surname><given-names>Y</given-names></name><name><surname>Mei</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Fu</surname><given-names>Z</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name></person-group>. <article-title>Triple-task mutual consistency for semi-supervised 3D medical image segmentation</article-title>. <source>Comput Biol Med</source>. (<year>2024</year>) <volume>175</volume>:<fpage>108506</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.compbiomed.2024.108506</pub-id>
<pub-id pub-id-type="pmid">38688127</pub-id>
</mixed-citation></ref><ref id="B18"><label>18</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group>. <article-title>U-Net: Convolutional networks for biomedical image segmentation</article-title>. In: <person-group person-group-type="editor"><name><surname>Navab</surname><given-names>N</given-names></name><name><surname>Hornegger</surname><given-names>J</given-names></name><name><surname>Wells</surname><given-names>WM</given-names></name><name><surname>Frangi</surname><given-names>AF</given-names></name></person-group> editors. <source>Medical Image Computing and Computer-Assisted Intervention &#x02013; MICCAI 2015</source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name> (<year>2015</year>). p. <fpage>234</fpage>&#x02013;<lpage>41</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id>
</mixed-citation></ref><ref id="B19"><label>19</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wenjia</surname><given-names>B</given-names></name><name><surname>Matthew</surname><given-names>S</given-names></name><name><surname>Giacomo</surname><given-names>T</given-names></name><name><surname>Ozan</surname><given-names>O</given-names></name><name><surname>Martin</surname><given-names>R</given-names></name><name><surname>Ghislain</surname><given-names>V</given-names></name><etal/></person-group>. <article-title>Automated cardiovascular magnetic resonance image analysis with fully convolutional networks</article-title>. <source>J Cardiovasc Magnetic Resonance</source>. (<year>2017</year>) <volume>20</volume>:<page-range>65&#x02013;</page-range>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1186/s12968-018-0471-x</pub-id>
</mixed-citation></ref><ref id="B20"><label>20</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zeng</surname><given-names>L-L</given-names></name><name><surname>Gao</surname><given-names>K</given-names></name><name><surname>Hu</surname><given-names>D</given-names></name><name><surname>Feng</surname><given-names>Z</given-names></name><name><surname>Hou</surname><given-names>C</given-names></name><name><surname>Rong</surname><given-names>P</given-names></name><etal/></person-group>. <article-title>SS-TBN: A semi-supervised tri-branch network for COVID-19 screening and lesion segmentation</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. (<year>2023</year>) <volume>45</volume>:<page-range>10427&#x02013;42</page-range>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1109/TPAMI.2023.3240886</pub-id>
</mixed-citation></ref><ref id="B21"><label>21</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Peng</surname><given-names>J</given-names></name><name><surname>Estrada</surname><given-names>G</given-names></name><name><surname>Pedersoli</surname><given-names>M</given-names></name><name><surname>Desrosiers</surname><given-names>C</given-names></name></person-group>. <article-title>Deep co-training for semi-supervised image segmentation</article-title>. <source>Pattern Recognit</source>. (<year>2020</year>) <volume>107</volume>:<fpage>107269</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.patcog.2020.107269</pub-id>
</mixed-citation></ref><ref id="B22"><label>22</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Han</surname><given-names>M</given-names></name></person-group>. <article-title>3D medical image segmentation based on semi-supervised learning using deep co-training</article-title>. <source>Appl Soft Comput</source>. (<year>2024</year>) <volume>159</volume>:<fpage>111641</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.asoc.2024.111641</pub-id>
</mixed-citation></ref><ref id="B23"><label>23</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Lyu</surname><given-names>J</given-names></name><name><surname>Cai</surname><given-names>Q</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><etal/></person-group>. <article-title>Cross co-teaching for semi-supervised medical image segmentation</article-title>. <source>Pattern Recognit</source>. (<year>2024</year>) <volume>152</volume>:<fpage>110426</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.patcog.2024.110426</pub-id>
</mixed-citation></ref><ref id="B24"><label>24</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Peng</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>M</given-names></name></person-group>. <article-title>Patch-shuffle-based semi-supervised segmentation of bone computed tomography via consistent learning</article-title>. <source>BioMed Signal Process Control</source>. (<year>2023</year>) <volume>80</volume>:<fpage>104239</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.bspc.2022.104239</pub-id>
</mixed-citation></ref><ref id="B25"><label>25</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Yap</surname><given-names>BP</given-names></name><name><surname>Ng</surname><given-names>BK</given-names></name></person-group>. <article-title>Cut-paste consistency learning for semi-supervised lesion segmentation</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</conf-name>. (<year>2023</year>). p.<page-range>6160&#x02013;9</page-range>.</mixed-citation></ref><ref id="B26"><label>26</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Bai</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Shen</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group>. <article-title>Bidirectional copy-paste for semi-supervised medical image segmentation</article-title>. In: <conf-name>Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>. (<year>2023</year>) pp. <page-range>11514&#x02013;24</page-range>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1109/CVPR52729.2023.01108</pub-id>
</mixed-citation></ref><ref id="B27"><label>27</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhong</surname><given-names>L</given-names></name><name><surname>Luo</surname><given-names>X</given-names></name><name><surname>Liao</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>G</given-names></name></person-group>. <article-title>Semi-supervised pathological image segmentation via cross distillation of multiple attentions and Seg-CAM consistency</article-title>. <source>Pattern Recognit</source>. (<year>2024</year>) <volume>152</volume>:<fpage>110492</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.patcog.2024.110492</pub-id>
</mixed-citation></ref><ref id="B28"><label>28</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Yao</surname><given-names>Y</given-names></name><name><surname>Duan</surname><given-names>X</given-names></name><name><surname>Qu</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name></person-group>. <article-title>DFCG: A Dual-Frequency Cascade Graph model for semi-supervised ultrasound image segmentation with diffusion model</article-title>. <source>Knowl Based Syst</source>. (<year>2024</year>) <volume>300</volume>:<fpage>112261</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.knosys.2024.112261</pub-id>
</mixed-citation></ref><ref id="B29"><label>29</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group>. <article-title>Uncertainty-aware representation calibration for semi-supervised medical imaging segmentation</article-title>. <source>Neurocomputing</source>. (<year>2024</year>) <volume>595</volume>:<fpage>127912</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.neucom.2024.127912</pub-id>
</mixed-citation></ref><ref id="B30"><label>30</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Lv</surname><given-names>Q</given-names></name><name><surname>Lee</surname><given-names>CH</given-names></name><name><surname>Shen</surname><given-names>L</given-names></name></person-group>. <article-title>Segmenting medical images with limited data</article-title>. <source>Neural Networks</source>. (<year>2024</year>) <volume>177</volume>:<fpage>106367</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.neunet.2024.106367</pub-id>
<pub-id pub-id-type="pmid">38754215</pub-id>
</mixed-citation></ref><ref id="B31"><label>31</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Lu</surname><given-names>M</given-names></name><name><surname>Zou</surname><given-names>Y</given-names></name></person-group>. <article-title>Complementary consistency semi-supervised learning for 3D left atrial image segmentation</article-title>. <source>Comput Biol Med</source>. (<year>2023</year>) <volume>165</volume>:<fpage>107368</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.107368</pub-id>
<pub-id pub-id-type="pmid">37611420</pub-id>
</mixed-citation></ref><ref id="B32"><label>32</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Bai</surname><given-names>Y</given-names></name><name><surname>Shen</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Yu</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group>. <article-title>MagicNet: Semi-supervised multi-organ segmentation via magic-cube partition and recovery</article-title>. In: <conf-name>Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>. (<year>2023</year>). p. <page-range>23869&#x02013;78</page-range>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1109/CVPR52729.2023.02286</pub-id>
</mixed-citation></ref><ref id="B33"><label>33</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Cheng</surname><given-names>J</given-names></name><name><surname>Meng</surname><given-names>L</given-names></name><name><surname>Yan</surname><given-names>H</given-names></name><name><surname>He</surname><given-names>Y</given-names></name><name><surname>Shi</surname><given-names>H</given-names></name><etal/></person-group>. <article-title>DeepTree: pathological image classification through imitating tree-like strategies of pathologists</article-title>. <source>IEEE Trans Med Imaging</source>. (<year>2024</year>) <volume>43</volume>:<page-range>1501&#x02013;12</page-range>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1109/TMI.2023.3341846</pub-id>
</mixed-citation></ref><ref id="B34"><label>34</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Wong</surname><given-names>IHM</given-names></name><name><surname>Dai</surname><given-names>W</given-names></name><name><surname>Lo</surname><given-names>CTK</given-names></name><name><surname>Wong</surname><given-names>TTW</given-names></name></person-group>. <article-title>Lung cancer diagnosis on virtual histologically stained tissue using weakly supervised learning</article-title>. <source>Mod Pathol</source>. (<year>2024</year>) <volume>37</volume>(<issue>6</issue>):<fpage>100487</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.modpat.2024.100487</pub-id>
<pub-id pub-id-type="pmid">38588884</pub-id>
</mixed-citation></ref><ref id="B35"><label>35</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>He</surname><given-names>Y</given-names></name><name><surname>Song</surname><given-names>F</given-names></name><name><surname>Wu</surname><given-names>W</given-names></name><name><surname>Tian</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>MultiTrans: Multi-scale feature fusion transformer with transfer learning strategy for multiple organs segmentation of head and neck CT images</article-title>. <source>Med Nov Technol Devices</source>. (<year>2023</year>) <volume>18</volume>:<fpage>100235</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.medntd.2023.100235</pub-id>
</mixed-citation></ref><ref id="B36"><label>36</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Noroozi</surname><given-names>M</given-names></name><name><surname>Favaro</surname><given-names>P</given-names></name></person-group>. <article-title>Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles</article-title>. In: <person-group person-group-type="editor"><name><surname>Leibe</surname><given-names>B</given-names></name><name><surname>Matas</surname><given-names>J</given-names></name><name><surname>Sebe</surname><given-names>N</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group>, editors. <source>Computer Vision &#x02013; ECCV 2016</source>. <publisher-name>Springer International Publishing</publisher-name>, <publisher-loc>Cham</publisher-loc> (<year>2016</year>). p. <fpage>69</fpage>&#x02013;<lpage>84</lpage>.</mixed-citation></ref><ref id="B37"><label>37</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Ouali</surname><given-names>Y</given-names></name><name><surname>Hudelot</surname><given-names>C</given-names></name><name><surname>Tami</surname><given-names>M</given-names></name></person-group>. (<year>2020</year>). <article-title>Semi-supervised semantic segmentation with cross-consistency training</article-title>, in: <conf-name>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>, . pp. <page-range>12671&#x02013;81</page-range>.</mixed-citation></ref><ref id="B38"><label>38</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Yuan</surname><given-names>Y</given-names></name><name><surname>Zeng</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name></person-group>. <article-title>Semi-supervised semantic segmentation with cross pseudo supervision</article-title>. In: <conf-name>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>. (<year>2021</year>). p. <page-range>2613&#x02013;22</page-range>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1109/CVPR46437.2021.00264</pub-id>
</mixed-citation></ref><ref id="B39"><label>39</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Gao</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Cao</surname><given-names>X</given-names></name><name><surname>Zhu</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>P</given-names></name></person-group>. <article-title>Semi-supervised seg-mentation of hyperspectral pathological imagery based on shape priors and contrastive learning</article-title>. <source>BioMed Signal Process Control</source>. (<year>2024</year>) <volume>91</volume>:<fpage>105881</fpage>. doi:&#x000a0;<pub-id pub-id-type="doi">10.1016/j.bspc.2023.105881</pub-id>
</mixed-citation></ref></ref-list></back></article>