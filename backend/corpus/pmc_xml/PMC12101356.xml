<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Audiol Res</journal-id><journal-id journal-id-type="iso-abbrev">Audiol Res</journal-id><journal-id journal-id-type="publisher-id">audiolres</journal-id><journal-title-group><journal-title>Audiology Research</journal-title></journal-title-group><issn pub-type="ppub">2039-4330</issn><issn pub-type="epub">2039-4349</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40407674</article-id><article-id pub-id-type="pmc">PMC12101356</article-id>
<article-id pub-id-type="doi">10.3390/audiolres15030060</article-id><article-id pub-id-type="publisher-id">audiolres-15-00060</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Objective Detection of Auditory Steady-State Responses (ASSRs) Based on Mutual Information: Receiver Operating Characteristics and Performance Across Modulation Rates and Levels <xref rid="fn1-audiolres-15-00060" ref-type="author-notes">&#x02020;</xref></article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1821-3261</contrib-id><name><surname>Bidelman</surname><given-names>Gavin M.</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-audiolres-15-00060" ref-type="aff">1</xref><xref rid="af2-audiolres-15-00060" ref-type="aff">2</xref><xref rid="af3-audiolres-15-00060" ref-type="aff">3</xref><xref rid="c1-audiolres-15-00060" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Horn</surname><given-names>Claire McElwain</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af4-audiolres-15-00060" ref-type="aff">4</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Beynon</surname><given-names>Andy J.</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Merch&#x000e1;n</surname><given-names>Miguel &#x000c1;ngel</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-audiolres-15-00060"><label>1</label>Department of Speech, Language and Hearing Sciences, Indiana University, Bloomington, IN 47408, USA</aff><aff id="af2-audiolres-15-00060"><label>2</label>Program in Neuroscience, Indiana University, Bloomington, IN 47405, USA</aff><aff id="af3-audiolres-15-00060"><label>3</label>Cognitive Science Program, Indiana University, Bloomington, IN 47405, USA</aff><aff id="af4-audiolres-15-00060"><label>4</label>School of Communication Sciences &#x00026; Disorders, University of Memphis, Memphis, TN 38152, USA</aff><author-notes><corresp id="c1-audiolres-15-00060"><label>*</label>Correspondence: <email>gbidel@iu.edu</email></corresp><fn id="fn1-audiolres-15-00060"><label>&#x02020;</label><p>Preliminary versions of this work were disseminated as a preprint.</p></fn></author-notes><pub-date pub-type="epub"><day>15</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>6</month><year>2025</year></pub-date><volume>15</volume><issue>3</issue><elocation-id>60</elocation-id><history><date date-type="received"><day>09</day><month>3</month><year>2025</year></date><date date-type="rev-recd"><day>28</day><month>3</month><year>2025</year></date><date date-type="accepted"><day>13</day><month>5</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p><bold>Background</bold>: Auditory steady-state responses (ASSRs) are sustained potentials used to assess the physiological integrity of the auditory pathway and objectively estimate hearing thresholds. ASSRs are typically analyzed using statistical procedures to remove the subjective bias of human operators. Knowing when to terminate signal averaging in ASSR testing is critical for making efficient clinical decisions and obtaining high-quality data in empirical research. Here, we report on stimulus-specific (frequency, level) properties and operating ranges of a novel ASSR detection metric based on mutual information (MI). <bold>Methods</bold>: ASSRs were measured in <italic toggle="yes">n</italic> = 10 normal-hearing listeners exposed to various stimuli varying in modulation rate (40, 80 Hz) and level (80&#x02013;20 dB SPL). <bold>Results</bold>: MI-based classifiers applied to ASSR recordings showed that the accuracy of ASSR detection ranged from ~75 to 99% and was better for 40 compared to 80 Hz responses and for higher compared to lower stimulus levels. Receiver operating characteristics (ROCs) were used to establish normative ranges for MI for reliable ASSR detection across levels and rates (MI = 0.9&#x02013;1.6). Relative to current statistics for ASSR identification (F-test), MI was a more efficient metric for determining the stopping criterion for signal averaging. <bold>Conclusions</bold>: Our results confirm that MI can be applied across a broad range of ASSR stimuli and might offer improvements to conventional objective techniques for ASSR detection.</p></abstract><kwd-group><kwd>auditory evoked potentials (AEPs)</kwd><kwd>auditory steady-state response (ASSR)</kwd><kwd>evoked potential classification</kwd><kwd>F-test</kwd><kwd>objective audiometry</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-audiolres-15-00060"><title>1. Introduction</title><p>Auditory steady-state responses (ASSRs) are sustained evoked potentials typically elicited by amplitude or frequency modulated signals. ASSRs offer a rapid physiological assessment of hearing function and can be used to estimate full audiogram thresholds simultaneously in both ears [<xref rid="B1-audiolres-15-00060" ref-type="bibr">1</xref>,<xref rid="B2-audiolres-15-00060" ref-type="bibr">2</xref>,<xref rid="B3-audiolres-15-00060" ref-type="bibr">3</xref>]. ASSRs are preferred over other electrophysiological measures (e.g., auditory brainstem response, ABR) because response detection is based on a statistical comparison between signal and noise power in the evoked potential average rather than human waveform inspection [<xref rid="B1-audiolres-15-00060" ref-type="bibr">1</xref>,<xref rid="B4-audiolres-15-00060" ref-type="bibr">4</xref>,<xref rid="B5-audiolres-15-00060" ref-type="bibr">5</xref>,<xref rid="B6-audiolres-15-00060" ref-type="bibr">6</xref>]. This objectivity is beneficial as it avoids subjective operator interpretation and bias in determining the presence/absence of a response and the quality of the auditory evoked potential (AEP) recording [<xref rid="B6-audiolres-15-00060" ref-type="bibr">6</xref>,<xref rid="B7-audiolres-15-00060" ref-type="bibr">7</xref>,<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>].</p><p>Current approaches to analyzing ASSRs typically involve frequency domain measures where a statistic is applied to the response spectrum in order to determine the significance of the signal&#x02019;s amplitude relative to the surrounding noise floor [<xref rid="B1-audiolres-15-00060" ref-type="bibr">1</xref>,<xref rid="B4-audiolres-15-00060" ref-type="bibr">4</xref>,<xref rid="B5-audiolres-15-00060" ref-type="bibr">5</xref>,<xref rid="B6-audiolres-15-00060" ref-type="bibr">6</xref>]. Several statistics have been proposed in the literature including the <italic toggle="yes">F</italic>-test, magnitude-squared coherence (MSC) [<xref rid="B4-audiolres-15-00060" ref-type="bibr">4</xref>,<xref rid="B9-audiolres-15-00060" ref-type="bibr">9</xref>], circular <italic toggle="yes">T</italic><sup>2</sup> statistic [<xref rid="B10-audiolres-15-00060" ref-type="bibr">10</xref>], <italic toggle="yes">Fcric</italic> [<xref rid="B11-audiolres-15-00060" ref-type="bibr">11</xref>], or <italic toggle="yes">Fmp</italic> statistic implemented in the Interacoustics-Eclipse system (<uri xlink:href="https://tinyurl.com/28r8s4ry">https://tinyurl.com/28r8s4ry</uri>; Accessed 25 February 2025). In all cases, these statistics become more powerful with an increasing number of trials. As such, a stopping rule can be applied when a criterion value or significance level is achieved (e.g., <italic toggle="yes">p</italic> &#x0003c; 0.05). Such metrics are currently available in several commercial AEP systems. However, it remains unclear if these are the most optimal statistics for characterizing sustained AEPs. Arguably, metrics like the <italic toggle="yes">F</italic>-test are somewhat limited because they are usable only on specific features of the stimulus (e.g., power at the modulation frequency, <italic toggle="yes">f<sub>m</sub></italic>). Consequently, these metrics cannot be broadly applied to sustained AEPs elicited by more complex sounds (e.g., multi-frequency, time-varying stimuli) that have proven more useful in characterizing central disorders of the auditory nervous system [<xref rid="B2-audiolres-15-00060" ref-type="bibr">2</xref>,<xref rid="B12-audiolres-15-00060" ref-type="bibr">12</xref>,<xref rid="B13-audiolres-15-00060" ref-type="bibr">13</xref>,<xref rid="B14-audiolres-15-00060" ref-type="bibr">14</xref>,<xref rid="B15-audiolres-15-00060" ref-type="bibr">15</xref>]. Novel statistical approaches might offer higher sensitivity and/or flexibility for detecting ASSRs and other sustained AEPs.</p><p>To this end, we began developing a novel statistical method for detecting sustained auditory potentials based on mutual information (MI) [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>], a metric adopted from information theory and image processing (for review, see (for review, see [<xref rid="B16-audiolres-15-00060" ref-type="bibr">16</xref>]). The aim of our approach is to compare the spectrographic representations of the stimulus signal to that of the neural response [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>]. While promising, our previous investigations were only proof of concept, testing MI using only a single suprathreshold stimulus. Thus, it remains unclear if MI can be more broadly applied to detect ASSRs elicited under a range of stimulus parameters including different modulation rates and levels. Furthermore, the criterion threshold for MI we used previously was estimated via computational modeling [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>]. Thus, it is not clear from our previous studies whether this is the most appropriate criterion for detecting ASSRs evoked by different stimulus levels and rates or if it was idiosyncratic to the one stimulus in our prior report. New normative data reported here allowed us to address these open questions and characterize ranges for the MI metric based on its performance (e.g., sensitivity/specificity) in detecting a wider variety of ASSR responses. Understanding the performance of MI detection across different stimulus settings is critical if the response is to be eventually used for objective audiometry [<xref rid="B1-audiolres-15-00060" ref-type="bibr">1</xref>,<xref rid="B2-audiolres-15-00060" ref-type="bibr">2</xref>,<xref rid="B3-audiolres-15-00060" ref-type="bibr">3</xref>].</p><p>The present study aimed to more fully characterize the performance of an MI-based classifier for detecting ASSRs across a broader range of stimulus parameters. We assessed ASSR detection for responses recorded at different modulation frequencies (40 Hz, 80 Hz) to assess the metric&#x02019;s dependence on stimulus modulation rate (and thus the putative site of the ASSR generator) (e.g., cortex vs. brainstem: [<xref rid="B17-audiolres-15-00060" ref-type="bibr">17</xref>]). Additionally, we parametrically varied the stimulus level across a large dynamic range (80&#x02013;20 dB SPL) to evaluate the level dependence of MI in detecting ASSRs. This latter manipulation is important given the application of ASSRs for threshold estimation and objective audiometry [<xref rid="B5-audiolres-15-00060" ref-type="bibr">5</xref>,<xref rid="B18-audiolres-15-00060" ref-type="bibr">18</xref>]. Receiver operating characteristics (ROCs) allowed us to characterize how different choices of MI criterion values affect ASSR detection and thus allowed us to establish a normative operating range for the metric and guide its future implementation. Lastly, we evaluated the efficacy of the MI algorithm by comparing its application as a stopping criterion for signal ongoing averaging against other &#x0201c;gold-standard&#x0201d; statistical approaches (i.e., F-test) [<xref rid="B1-audiolres-15-00060" ref-type="bibr">1</xref>].</p></sec><sec id="sec2-audiolres-15-00060"><title>2. Materials and Methods</title><sec sec-type="subjects" id="sec2dot1-audiolres-15-00060"><title>2.1. Participants</title><p>Ten young, normal-hearing listeners (5 male, 5 female; age: 23.7 &#x000b1; 1.94 years) participated in the experiment This sample size is consistent with the sample sizes of other recent ASSR signal detection studies on normal-hearing listeners [<xref rid="B11-audiolres-15-00060" ref-type="bibr">11</xref>,<xref rid="B19-audiolres-15-00060" ref-type="bibr">19</xref>]. However, it is important to note that for the development of a statistical classifier which aims to segregate signal from noise, it is the unique number of datapoints that is most critical. In this regard, the aggregate data we use for statistical analysis and characterizing the parameter space of MI (e.g., see Figure 2) included a substantial (100,000 = 10 subjects &#x000d7; 4 levels &#x000d7; 2500 sweeps) number of unique data epochs. All participants had normal hearing thresholds (&#x02264;15 dB HL, octave frequencies 250&#x02013;8000 Hz) bilaterally, were right handed [<xref rid="B20-audiolres-15-00060" ref-type="bibr">20</xref>], and were native speakers of American English. Participants gave written informed consent in compliance with a protocol approved by the University of Memphis Institutional Review Board (Protocol #2370).</p></sec><sec id="sec2dot2-audiolres-15-00060"><title>2.2. Stimuli</title><p>ASSRs were evoked by sinusoidal amplitude-modulated (SAM) tones with a carrier frequency (<italic toggle="yes">f<sub>c</sub></italic>) of 1000 Hz and modulation frequencies (<italic toggle="yes">f<sub>m</sub></italic>) of 40 Hz or 80 Hz (100% modulation depth). Stimulus duration was 200 ms (including 5 ms onset/offset ramping to minimize onset components) following our previous report [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>,<xref rid="B21-audiolres-15-00060" ref-type="bibr">21</xref>]. Stimuli were delivered binaurally via ER-2 insert earphones (Etymotic Research) at levels of 80, 60, 40, and 20 dB SPL using alternating polarity. In addition to these stimulus conditions, sham recordings were obtained by presenting stimuli with the inserts removed from participants&#x02019; ears [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>,<xref rid="B22-audiolres-15-00060" ref-type="bibr">22</xref>]. Shams provided baseline, control recordings of &#x0201c;neural noise&#x0201d; [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>,<xref rid="B21-audiolres-15-00060" ref-type="bibr">21</xref>].</p></sec><sec id="sec2dot3-audiolres-15-00060"><title>2.3. Electrophysiological Recordings</title><p>ASSR recording procedures and stimuli were similar to those in our previous report [<xref rid="B21-audiolres-15-00060" ref-type="bibr">21</xref>]. EEGs were recorded between Ag/AgCl disc electrodes placed on the scalp at the high forehead by the hairline, referenced to linked mastoids (A1/A2) (mid-forehead = ground). Interelectrode impedances were &#x02264;5 k&#x003a9;. Continuous EEG signals were digitized at 10 kHz (SynAmps RT amplifiers; Compumedics Neuroscan). EEGs were windowed [0&#x02013;200 ms], filtered (30&#x02013;1000 Hz), and averaged in the time domain to obtain ASSR waveforms for each stimulus. Listeners heard 2500 repetitions of the stimulus token presented at an interstimulus interval of 5 ms. Post-processing and analyses were performed using custom routines coded in MATLAB<sup>&#x000ae;</sup> (v2024a; The MathWorks, Inc., Natick, MA, USA).</p></sec><sec id="sec2dot4-audiolres-15-00060"><title>2.4. Mutual Information (MI) Detection Metric</title><p>We computed the mutual information (MI) between spectrographic representations of the stimulus and the neural response to index the degree to which neural responses captured spectrotemporal details of the acoustic input. Details of this metric are fully elaborated in our previous study&#x02019;s bench testing MI for AEP detection [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>,<xref rid="B21-audiolres-15-00060" ref-type="bibr">21</xref>]. MI is a dimensionless quantity (measured in bits) which measures the degree of linear and nonlinear dependence between two signals (A and B). In our specific case where A and B are two spectrograms, MI computes the dependence or similarity between the two images [<xref rid="B16-audiolres-15-00060" ref-type="bibr">16</xref>]. Effectively, the resulting value describes the similarity between the ASSR and stimulus spectrograms. We use this value to decide, statistically speaking, if the ASSR response is present or absent in the EEG recording.</p><p>MI was computed between the stimulus and each neural response spectrogram, allowing us to assess the degree to which neural responses reflect spectrotemporal properties of the evoking stimulus [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>]. Spectrograms were computed using the &#x0201c;spectrogram&#x0201d; routine in MATLAB and converted to grayscale images. This routine computed a 2<sup>14</sup> point FFT in consecutive 50 ms segments (Hamming windowed) computed every 3 ms [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>] (Window length changes the spectral resolution of the resulting spectrogram, which could impact the computation of MI when comparing the stimulus and response spectrograms. In initial analyses, we varied the sliding window length parametrically from 25 ms to 100 ms. However, in pilot testing, we found no appreciable changes in the accuracy of response detection for different window lengths. Consequently, we adopted a 50 ms window, equivalent to a spectral resolution of 20 Hz. This is appropriate for resolving both 40 Hz and 80 Hz components and is consistent with our previous studies [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>,<xref rid="B15-audiolres-15-00060" ref-type="bibr">15</xref>].). Time waveforms were zero-padded to minimize edge effects and ensure that spectrograms ran to the end of the signal&#x02019;s duration. Identical parameters were used to compute both the stimulus and response spectrograms. SAM tone stimulus spectrograms were squared prior to computing MI to account for the half-wave rectification that is applied during the cochlear transduction process [<xref rid="B21-audiolres-15-00060" ref-type="bibr">21</xref>,<xref rid="B23-audiolres-15-00060" ref-type="bibr">23</xref>,<xref rid="B24-audiolres-15-00060" ref-type="bibr">24</xref>].</p></sec><sec id="sec2dot5-audiolres-15-00060"><title>2.5. Receiver Operating Characteristics (ROCs) for the MI Classifier</title><p>After determining a criterion (i.e., decision rule) for MI statistically from our data (see the Results Section), we then applied this threshold (MI<sub>&#x003b8;</sub>) as a binary classifier to ASSR and sham recordings. Recordings yielding MI &#x02265; MI<sub>&#x003b8;</sub> were classified as neural responses, whereas recordings with MI &#x0003c; MI<sub>&#x003b8;</sub> were considered noise (i.e., no response) [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>]. Classifier performance was evaluated by computing standard signal detection theory and ROC metrics including true and false positive rates. ROC analyses also allowed us to determine the acceptable range of MI values that yielded above-chance detection of ASSRs from noise. For a given value of MI, sensitivity was computed as the percentage of actual ASSR recordings correctly identified; false positive rate was computed as the percentage of sham recordings (i.e., &#x0201c;neural noise&#x0201d;) erroneously classified as a biological ASSR response. ROC curves were constructed for each modulation rate (40 Hz, 80 Hz) and level (80&#x02013;20 dB SPL) to characterize the overall performance of the MI classifier across the different stimulus settings.</p></sec><sec id="sec2dot6-audiolres-15-00060"><title>2.6. Comparison of MI to F-Test</title><p>To test the efficiency of MI as a stopping criterion for signal averaging, we computed MI on a sweep-by-sweep basis as accumulating trials were added to the ongoing ASSR average. This was repeated separately for each level and modulation rate. Similarly, we compared the &#x0201c;online&#x0201d; development of MI against the well-known F-test [<xref rid="B1-audiolres-15-00060" ref-type="bibr">1</xref>,<xref rid="B4-audiolres-15-00060" ref-type="bibr">4</xref>] used in commercial ASSR recording systems (e.g., Bio-logic MASTER II; Intelligent Hearing Systems SmartEP-ASSR). While other detection metrics are available (e.g., MSC), we have previously shown that MI is most comparable in detection performance to the F-test (MSC performs more poorly) [<xref rid="B21-audiolres-15-00060" ref-type="bibr">21</xref>] and thus represents a stringent benchmark comparison. The underlying assumption of the F-test approach is that in the spectral domain, ASSR energy should be localized to a frequency bin near the stimulus modulation frequency; activity in adjacent bins contains only random noise, with zero mean and variance distributed equally across the noise bins [<xref rid="B1-audiolres-15-00060" ref-type="bibr">1</xref>]. The ratio of signal power to the sum of the powers in N adjacent frequency bins is distributed according to an F distribution with 2 and 2N-1 degrees of freedom [<xref rid="B1-audiolres-15-00060" ref-type="bibr">1</xref>]. In the current study, we used N = 12 frequency bins surrounding the target signal. We then compared our measured F-ratio against the critical F-value with 2 and 23 degrees of freedom and obtained a corresponding <italic toggle="yes">p</italic>-value for response detection. Traces yielding <italic toggle="yes">p</italic> &#x0003c; 0.05 were deemed to have response energy at the <italic toggle="yes">fm</italic> frequency that was significantly above the surrounding noise floor. Comparison between the MI and F-test statistical metrics allowed us to relate their performance and determine differences in their stopping rule for signal averaging, i.e., the number of trials where each measure detected the presence of ASSRs.</p></sec></sec><sec sec-type="results" id="sec3-audiolres-15-00060"><title>3. Results</title><sec id="sec3dot1-audiolres-15-00060"><title>3.1. ASSR Responses</title><p>ASSR time waveforms and spectra are shown for actual and sham recordings in <xref rid="audiolres-15-00060-f001" ref-type="fig">Figure 1</xref>. The spectra illustrate response energy at the modulation rates (40 Hz or 80 Hz) and their upper harmonics for ASSR but not sham recordings (gray trace). These findings confirm that ASSRs contained robust phase-locked neural activity, whereas sham recordings contained no ASSR response (nor stimulus artifact) and are thus suitable for use as &#x0201c;catch trials&#x0201d; in testing our MI detection metric [<xref rid="B21-audiolres-15-00060" ref-type="bibr">21</xref>]. As expected, ASSR amplitudes also decreased with decreasing stimulus level and were only weakly above the noise floor at 20 dB SPL.</p></sec><sec id="sec3dot2-audiolres-15-00060"><title>3.2. Performance and ROC Characteristics of MI Classifier</title><p>Examples of MI computed between the 40 Hz SAM stimulus and responses are shown for different stimulus levels in <xref rid="audiolres-15-00060-f002" ref-type="fig">Figure 2</xref>A. MI decreases at lower stimulus levels, indicating weaker dependence between the stimulus and ASSR response. At high intensities (80 dB SPL), ASSR spectrograms show strong dependence on the evoking stimulus spectrogram and MI is large. Nearer the threshold (20 dB SPL), ASSRs are dominated by background EEG noise, implying that the averaged neural response shares less information with the stimulus, which consequently yields a low MI (Note that non-zero MI is observed even for sham recordings, suggesting some shared time&#x02013;frequency information between the stimulus and neural noise. We attribute this to myogenic noise of the EEG which is strong for frequencies &#x0003c;40 Hz. The SAM tone stimulus also has significant low-frequency energy below &#x0003c;40 Hz. Thus, even in the absence of a stimulus, spectral energy below &#x0003c;40 Hz in both the stimulus and &#x0201c;neural noise&#x0201d; can produce non-zero MI. This can be taken as the floor of the metric.).</p><p>Our first aim was to statistically determine a decision rule for MI for use in detecting ASSR responses. To this end, signal detection theory was used to determine an optimal criterion (MI<italic toggle="yes"><sub>&#x003b8;</sub></italic>) for the MI classifier from EEG recordings. <xref rid="audiolres-15-00060-f002" ref-type="fig">Figure 2</xref>B shows the probability density functions of MI values for all trials and subjects for the 40 Hz and 80 Hz ASSR (pooling across levels) and sham recordings. On average, MI values range from ~1 to 1.5 across all stimulus combinations. All ASSRs were, to varying degrees, linearly separable along the MI decision axis compared to sham recordings, which elicited weak MI. In the current study, MI<italic toggle="yes"><sub>&#x003b8;</sub></italic> = 0.93 was taken as the criterion value because 95% of the data (i.e., MIs for ASSR responses) fell above this threshold; the false positive rate was 5%. From a signal detection standpoint, this implies that any arbitrary recording for which MI &#x0003e; MI<italic toggle="yes"><sub>&#x003b8;</sub></italic> will be predicted to contain a true ASSR response, whereas recordings with MI &#x0003c; MI<italic toggle="yes"><sub>&#x003b8;</sub></italic> are considered noise (no response). MI<italic toggle="yes"><sub>&#x003b8;</sub></italic> = 0.93 was determined to be the optimal decision rule for ASSR detection, though we show later that a range of values around MI = 1 are also sufficient.</p><p>Classifier performance of the MI metric is shown in <xref rid="audiolres-15-00060-f003" ref-type="fig">Figure 3</xref> as ROC curves. Each panel represents the true (sensitivity) vs. false positive (1-specificity) rate for distinguishing ASSRs from sham recordings at different stimulus levels. The bowing of the ROC curve toward the upper left corner is indicative of robust sensitivity in segregating signal from noise (i.e., higher <italic toggle="yes">d</italic>-prime). Each individual datapoint represents the true/false positive rate for a different choice of MI<italic toggle="yes"><sub>&#x003b8;</sub></italic>. A criterion located at the maximum curvature of the ROC curve represents the optimal decision rule for classification, one which produces the highest sensitivity while minimizing false positive detection (i.e., erroneously labeling a noise recording as an ASSR).</p><p>With decreasing levels, ASSRs become more difficult to segregate from EEG noise, as evident by the ROC curves approaching chance performance (dotted lines) at 20 dB SPL. Overall classification accuracy for the 40 Hz responses is near ceiling (99%) at 80 dB SPL, as indicated by the area under the curve (AUC) [<xref rid="B25-audiolres-15-00060" ref-type="bibr">25</xref>]. Classification accuracy weakens with decreasing level, indicating that discriminating ASSRs from noise is more difficult nearer to the threshold. Nevertheless, classification remains high (74%) for the 40 Hz responses at 20 dB SPL. Accuracy in detecting 80 Hz responses is 10&#x02013;15% poorer compared to 40 Hz responses but remains well above chance (73%) even at the lowest intensity tested. These operating characteristics demonstrate that the MI between a stimulus and neural response provides an objective means for detecting ASSRs across various levels and modulation rates.</p></sec><sec id="sec3dot3-audiolres-15-00060"><title>3.3. Acceptable Ranges of MI for ASSR Detection</title><p>While the statistically derived criterion MI<sub>&#x003b8;</sub> = 0.93 represents the optimal threshold for detecting ASSRs (5% false positive), our ROC characterizations reveal that there is a range of acceptable MI values around a convenient MI &#x02248; 1 that could be used to reliably detect neural responses. <xref rid="audiolres-15-00060-f004" ref-type="fig">Figure 4</xref> shows the overall accuracy of detecting ASSRs from shams for different choices of MI for 40 Hz (<xref rid="audiolres-15-00060-f004" ref-type="fig">Figure 4</xref>A) and 80 Hz (<xref rid="audiolres-15-00060-f004" ref-type="fig">Figure 4</xref>B) responses. Each family of functions shows the overall accuracy in correctly distinguishing ASSRs from noise using different MI cutoffs. The reduction in peak accuracy across curves indicates a level-dependent effect in classification accuracy. Consistent with ROC results, MI is less robust at detecting ASSRs evoked by weaker stimulus levels. Nevertheless, there is a range of MI values that allow for above-chance detection of the response (<xref rid="audiolres-15-00060-f004" ref-type="fig">Figure 4</xref>C).</p><p>MI ranges were extracted from the width of each level-dependent accuracy function shown in panels A and B and show the acceptable range of MI cutoff thresholds that allow for above-chance detection (<xref rid="audiolres-15-00060-f004" ref-type="fig">Figure 4</xref>C). For the 40 Hz response, acceptable values of MI range from 0.9 to 1.6 for high-level (80 dB) stimuli. This allowable range is reduced with decreasing level; 40 Hz responses are detectable at 20 dB with MI values between 0.9 and 1.3. Similar results were obtained for the 80 Hz responses, although the acceptable MI range was reduced at both high- (0.9&#x02013;1.4) and low-level (0.9&#x02013;1.2) stimuli.</p><p>The corresponding 95% confidence intervals for MI across levels were similar for 40 Hz ([0.99 1.14]) and 80 Hz responses ([0.99 1.13]). Notably, both 95% CIs included 1.0, suggesting that a simple value of MI = 1 could be applied to ASSRs for response detection. MI = 1 is also convenient in terms of its information&#x02013;theoretic interpretation: MI = 1 means that ASSR detection is achieved when &#x0201c;1 bit&#x0201d; of information is transferred (is shared) between the stimulus and EEG response spectrograms.</p><p>MI values corresponding to maximum classification accuracy (i.e., peak of functions in <xref rid="audiolres-15-00060-f004" ref-type="fig">Figure 4</xref>A,B) are shown in <xref rid="audiolres-15-00060-f004" ref-type="fig">Figure 4</xref>D. Maximum accuracy is obtained with MI &#x02248; 1 (cf. MI<italic toggle="yes"><sub>&#x003b8;</sub></italic>). Collectively, these results help provide a normative tolerance range and optimal choice of MI values for using it as an ASSR classifier.</p><p>Typically, the performance of a diagnostic or detection method is evaluated by considering the sensitivity and specificity of the measure. However, it is also useful to evaluate a diagnostic&#x02019;s true positive rate (i.e., sensitivity) for a fixed false positive rate (e.g., 5%). <xref rid="audiolres-15-00060-f004" ref-type="fig">Figure 4</xref>E,F show the overall accuracy of the MI metric (AUC) and sensitivity at a fixed 5% false positive rate (i.e., 95% specificity) across stimulus levels and modulation rates. For 40 Hz ASSRs, performance ranges from 100/95% sensitivity/specificity at 80 dB SPL to 20/95% sensitivity/specificity at 20 dB SPL. For 80 Hz ASSRs, performance ranges from 55/95% sensitivity/specificity at 80 dB SPL to 20/95% sensitivity/specificity at 20 dB SPL.</p></sec><sec id="sec3dot4-audiolres-15-00060"><title>3.4. MI as a Criterion for Terminating Signal Averaging</title><p>In addition to detection, an objective metric should be suitable as a stopping criterion for online signal averaging. <xref rid="audiolres-15-00060-f005" ref-type="fig">Figure 5</xref> shows the growth in MI and the conventional <italic toggle="yes">F</italic>-test [<xref rid="B1-audiolres-15-00060" ref-type="bibr">1</xref>,<xref rid="B4-audiolres-15-00060" ref-type="bibr">4</xref>] metric during ASSR recordings as a function of the number of trials in the ongoing average. In this figure, we set the MI threshold to the convenient value of MI = 1, the approximate midpoint of the metric&#x02019;s 95% CI (see <xref rid="sec3dot3-audiolres-15-00060" ref-type="sec">Section 3.3</xref>). In general, each metric improves with additional trials and asymptotes as the running AEP stabilizes. Response growth is faster for 40 Hz relative to 80 Hz responses and at higher (80 dB SPL) compared to lower (20 dB SPL) intensities. For high-level 40 Hz ASSRs, responses exceed the MI and F-test stopping criteria (MI = 1.0; F-test: <italic toggle="yes">p</italic> = 0.05) by ~50 and 750 sweeps, respectively, corresponding to &#x0003c;1 min (MI) vs. 2.5 min (F-test) of recording time. More extended recording durations (sweeps) are needed for detecting low-level ASSRs and 80 Hz responses, which sometimes do not achieve the criterion thresholds (e.g., <xref rid="audiolres-15-00060-f005" ref-type="fig">Figure 5</xref>D). As an expected control, MI remains invariant sweep to sweep for noise sham recordings.</p><p>We conducted a mixed-model ANOVA (lme4 package in R [<xref rid="B26-audiolres-15-00060" ref-type="bibr">26</xref>]) on the stopping sweep number to assess differences in the recording time needed under each metric. The fixed effects were rate, level, and test type (subject served as a random effect). Not all recordings reached the stopping criteria, especially for low-level, 80 Hz stimuli (e.g., <xref rid="audiolres-15-00060-f005" ref-type="fig">Figure 5</xref>D). These missing observations were coded as 2500, the maximum number of sweeps in our recording session. The ANOVA revealed a significant three-way rate x level x test interaction [<italic toggle="yes">F</italic>(3, 144) = 2.53, <italic toggle="yes">p</italic> = 0.059, <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> = 0.05] (all lower-order two-way and one-way effects were also significant) (<xref rid="audiolres-15-00060-f006" ref-type="fig">Figure 6</xref>). Tukey-corrected multiple contrasts revealed that the interaction was attributed to better performance of MI over the F-test in all but the 40 Hz, 80 dB SPL stimulus condition. These results indicate that MI outperformed the F-test across decreasing stimulus levels and modulation rates and detected the ASSR response in a fewer number of trials.</p></sec></sec><sec sec-type="discussion" id="sec4-audiolres-15-00060"><title>4. Discussion</title><p>There have been few developments in the ASSR literature over the past decade outside of industry manufacturers [<xref rid="B27-audiolres-15-00060" ref-type="bibr">27</xref>]. The majority of independent research efforts have focused on simultaneously recording the response with other evoked potentials (e.g., brainstem + cortical AEPs) [<xref rid="B28-audiolres-15-00060" ref-type="bibr">28</xref>,<xref rid="B29-audiolres-15-00060" ref-type="bibr">29</xref>,<xref rid="B30-audiolres-15-00060" ref-type="bibr">30</xref>,<xref rid="B31-audiolres-15-00060" ref-type="bibr">31</xref>], efficient protocol development and the use of the response as a diagnostic in certain clinical populations [<xref rid="B32-audiolres-15-00060" ref-type="bibr">32</xref>,<xref rid="B33-audiolres-15-00060" ref-type="bibr">33</xref>], or optimizing stimulus selection for enhancing the response [<xref rid="B34-audiolres-15-00060" ref-type="bibr">34</xref>]&#x02014;the so-called &#x0201c;Next-generation&#x0201d; ASSR using chirp sounds [<xref rid="B35-audiolres-15-00060" ref-type="bibr">35</xref>,<xref rid="B36-audiolres-15-00060" ref-type="bibr">36</xref>,<xref rid="B37-audiolres-15-00060" ref-type="bibr">37</xref>].</p><p>Here, we report on the tolerance and operating range of an objective statistical approach to detect ASSRs based on mutual information (MI) [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>]. The technique quantifies the quality of ASSRs by considering the linear and nonlinear dependences between the rich time&#x02013;frequency information provided by the signal and response spectrograms. Our previous report&#x02019;s bench testing of the MI classifier demonstrates its superiority over &#x0201c;gold-standard&#x0201d; (i.e., visual subjective) judgments of human observers [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>] and other objective techniques for ASSR detection (e.g., MSC, <italic toggle="yes">F</italic>-test) [<xref rid="B21-audiolres-15-00060" ref-type="bibr">21</xref>]. Here, we extend these previous findings by showing that MI can be used for response detection across a broader range of ASSR-evoking stimuli including different combinations of levels and modulation rates that are used in the audiological clinic. Given that the MI tracks with level, it can be used to guide threshold searches. We also found that MI yields higher efficiency in detecting ASSRs in shorter recording times than conventional statistical algorithms [<xref rid="B21-audiolres-15-00060" ref-type="bibr">21</xref>]. The present results reaffirm the multiple benefits of MI and suggest that the statistic might be a useful tool to reduce valuable recording time during auditory electrophysical testing in the clinic.</p><p>We found that the overall performance accuracy in distinguishing true neurobiological responses from noise using our MI metric was &#x0003e;90% for high-level stimuli (80 dB SPL) and remained well above chance (73%) for levels nearer to auditory threshold (20 dB SPL). More importantly, our results establish a normative tolerance range for MI criterion values (MI = 0.9&#x02013;1.6) that allow for robust detection of ASSRs across different modulation rates and intensities. However, as determined by ROC analyses, the most optimal classification of ASSRs is achieved with the criterion MI<italic toggle="yes"><sub>&#x003b8;</sub></italic> = 0.93. Though for the sake of convenience, MI = 1.0 would suffice in robustly identifying ASSRs from EEG noise (<xref rid="audiolres-15-00060-f004" ref-type="fig">Figure 4</xref>), such that &#x0201c;1 bit&#x0201d; of information must be transferred between the stimulus and EEG response to confirm that an ASSR is present. Lastly, we showed that MI increases monotonically with an increasing number of stimulus presentations (i.e., trials) and can, for some stimulus conditions, detect ASSRs in a fewer number of trials compared to conventional ASSR detection procedures, i.e., the F-test (i.e., F-test; [<xref rid="B1-audiolres-15-00060" ref-type="bibr">1</xref>,<xref rid="B4-audiolres-15-00060" ref-type="bibr">4</xref>]).</p><p>The more comprehensive stimulus set used in this study compared to our previous reports [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>,<xref rid="B21-audiolres-15-00060" ref-type="bibr">21</xref>] allowed for a more complete characterization of MI&#x02019;s effectiveness as an ASSR classifier. Several observations are worth noting regarding the metric&#x02019;s performance and limits. First, while MI can successfully detect the presence of ASSRs at different modulation rates (<xref rid="audiolres-15-00060-f003" ref-type="fig">Figure 3</xref>), we found that overall accuracy was generally higher for 40 Hz compared to 80 Hz responses. The higher classification for 40 Hz compared to 80 Hz suggests that MI-based detection may be more sensitive to cortical rather than subcortical auditory phase-locked activity [<xref rid="B17-audiolres-15-00060" ref-type="bibr">17</xref>,<xref rid="B38-audiolres-15-00060" ref-type="bibr">38</xref>]. Critically, however, MI detection was still well above chance in all cases (<xref rid="audiolres-15-00060-f004" ref-type="fig">Figure 4</xref>) and overall testing time was reduced compared to the F-test (<xref rid="audiolres-15-00060-f006" ref-type="fig">Figure 6</xref>). The more optimal performance at 40 vs. 80 Hz is a common bias in the ASSR literature [<xref rid="B39-audiolres-15-00060" ref-type="bibr">39</xref>] and is likely due to the higher signal-to-noise ratios and more robust amplitudes of ASSRs with low vs. high-frequency modulation rates, as shown in the present study in <xref rid="audiolres-15-00060-f002" ref-type="fig">Figure 2</xref> [<xref rid="B15-audiolres-15-00060" ref-type="bibr">15</xref>,<xref rid="B40-audiolres-15-00060" ref-type="bibr">40</xref>,<xref rid="B41-audiolres-15-00060" ref-type="bibr">41</xref>]. Indeed, by early adolescence, the 40 Hz response is nearly twice the amplitude of the 80 Hz response [<xref rid="B42-audiolres-15-00060" ref-type="bibr">42</xref>]. Moreover, unlike their 80 Hz counterparts, 40 Hz responses are highly dependent on subject state: low <italic toggle="yes">fm</italic> responses are reliably recorded only in awake individuals [<xref rid="B41-audiolres-15-00060" ref-type="bibr">41</xref>,<xref rid="B43-audiolres-15-00060" ref-type="bibr">43</xref>,<xref rid="B44-audiolres-15-00060" ref-type="bibr">44</xref>] and are eradicated with anesthesia [<xref rid="B38-audiolres-15-00060" ref-type="bibr">38</xref>,<xref rid="B40-audiolres-15-00060" ref-type="bibr">40</xref>]. Despite this typical low-frequency bias in the ASSR, we found that the overall testing time was reduced using MI compared to the F-test (<xref rid="audiolres-15-00060-f006" ref-type="fig">Figure 6</xref>). Our results therefore suggest that MI could offer a means to collect sustained ASSR/AEP data in a more time-optimized manner and reduce valuable recording time in the clinic (present study; [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>,<xref rid="B21-audiolres-15-00060" ref-type="bibr">21</xref>]).</p><p>Secondly, we found that MI has a smaller useable range (<xref rid="audiolres-15-00060-f004" ref-type="fig">Figure 4</xref>C) and lower accuracy/sensitivity (<xref rid="audiolres-15-00060-f004" ref-type="fig">Figure 4</xref>D) for low-level, 80 Hz stimuli. This would limit the metric&#x02019;s application for threshold testing [<xref rid="B45-audiolres-15-00060" ref-type="bibr">45</xref>], particularly in infants [<xref rid="B46-audiolres-15-00060" ref-type="bibr">46</xref>]. Additionally, neural generators of ASSRs are dependent on the frequency of the stimulus modulation rate; high frequencies (80 Hz) evoke brainstem generators, whereas low frequencies (40 Hz) recruit cortical sources [<xref rid="B17-audiolres-15-00060" ref-type="bibr">17</xref>,<xref rid="B38-audiolres-15-00060" ref-type="bibr">38</xref>]. Thus, the fact that we observe superior performance for 40 Hz stimuli across the board implies that MI might be more useful for monitoring cortical rather than subcortical neural activity.</p><p>Lastly, sweep-by-sweep tracking of MI confirmed the metric&#x02019;s efficiency as a stopping rule for ASSR signal averaging. In this regard, we found that MI was able to detect 40 Hz ASSRs within ~1 min, corresponding to &#x0003c;50 stimulus trials. In contrast, using the F-test required considerably more stimulus presentations; ~750 sweeps (2.5 min of testing) were needed to detect the 40 Hz response at 80 dB SPL and ~1500 sweeps at 60 dB SPL. Moreover, our 80 Hz ASSRs never achieved the F-test criterion, indicating that more than 2500 trials would be needed to detect those responses. While our data show that MI can offer a more efficient stopping rule for terminating averaging compared to the <italic toggle="yes">F</italic>-test, from a practical standpoint, this improvement in testing time (1&#x02013;2 min) is probably negligible. Nevertheless, our data indicate that under certain stimulus conditions, MI can detect ASSRs in half the number of trials (i.e., twice as efficient) as the gold-standard <italic toggle="yes">F</italic>-test.</p><p>More broadly, MI is an information&#x02013;theoretic measure that is &#x0201c;distribution free&#x0201d; and therefore requires fewer assumptions than other statistical approaches (e.g., <italic toggle="yes">F</italic>-test) which utilize parametric (distribution-based) statistics. This is another benefit as auditory responses tend to be highly nonlinear. Thus, MI might offer an improvement upon older and other more recent ASSR detection statistics for use in objective audiometry [<xref rid="B11-audiolres-15-00060" ref-type="bibr">11</xref>,<xref rid="B19-audiolres-15-00060" ref-type="bibr">19</xref>]. However, unlike other metrics, MI can be easily applied to time-varying signals beyond the ASSR [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>]. Thus, in addition to clinical application as a stopping criterion for ASSR audiometry and threshold testing, it can be readily applied to electrophysiological responses like the speech-evoked FFR [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>]. This would be another major clinical benefit as FFRs have become popular in assessing complex auditory function including central auditory processing disorders [<xref rid="B12-audiolres-15-00060" ref-type="bibr">12</xref>], aging [<xref rid="B47-audiolres-15-00060" ref-type="bibr">47</xref>,<xref rid="B48-audiolres-15-00060" ref-type="bibr">48</xref>], cognitive decline [<xref rid="B13-audiolres-15-00060" ref-type="bibr">13</xref>], and learning-related plasticity [<xref rid="B49-audiolres-15-00060" ref-type="bibr">49</xref>,<xref rid="B50-audiolres-15-00060" ref-type="bibr">50</xref>].</p></sec><sec sec-type="conclusions" id="sec5-audiolres-15-00060"><title>5. Conclusions</title><p>The application of the MI metric to electrical response audiometry and ASSRs may provide clinicians and researchers with a more robust tool to objectively evaluate the presence and quality of sustained auditory AEPs, including fast testing time. The calculation of MI could be easily incorporated into commercially available AEP systems similar to other statistical detection metrics (e.g., <italic toggle="yes">F-test</italic>, <italic toggle="yes">F<sub>sp</sub></italic>, <italic toggle="yes">Fmp</italic>, <italic toggle="yes">MSC</italic>) already available in clinical hardware (MASTER system, Interacoustic Eclipse, Intelligent Hearing Systems). Implementation would require relatively straightforward signal processing to first compute the time&#x02013;frequency spectrograms of the (i) stimulus and (ii) response epoch followed by MI between i and ii. MI can then be tracked in a running manner during online averaging as trials are accumulated (e.g., as in <xref rid="audiolres-15-00060-f005" ref-type="fig">Figure 5</xref>). Signal averaging could then be terminated immediately once the criterion value (established here: M &#x02248; 1) is exceeded, saving valuable clinic time and guess work. Still, future studies are warranted to assess the performance of MI in clinical populations (e.g., infants, hearing loss) where testing times can be much longer than in listeners with normal hearing [<xref rid="B51-audiolres-15-00060" ref-type="bibr">51</xref>].</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, methodology, formal analysis, G.M.B.; data curation, formal analysis, C.M.H.; writing&#x02014;original draft preparation and reviewing/editing, G.M.B. and C.M.H. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>This study was conducted in accordance with the Declaration of Helsinki and approved by the Institutional Review Board at the University of Memphis (protocol # 2370, approved 10/3/2012).</p></notes><notes><title>Informed Consent Statement</title><p>Written informed consent was obtained from all subjects involved in this study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are only available on request from the corresponding author due to privacy reasons.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>G.M.B. serves on the Editorial Board of <italic toggle="yes">Audiology Research</italic>. The authors declare no other conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-audiolres-15-00060"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>John</surname><given-names>M.S.</given-names></name>
<name><surname>Picton</surname><given-names>T.W.</given-names></name>
</person-group><article-title>MASTER: A Windows program for recording multiple auditory steady-state responses</article-title><source>Comput. Methods Programs Biomed.</source><year>2000</year><volume>61</volume><fpage>125</fpage><lpage>150</lpage><pub-id pub-id-type="doi">10.1016/S0169-2607(99)00035-8</pub-id><pub-id pub-id-type="pmid">10661398</pub-id>
</element-citation></ref><ref id="B2-audiolres-15-00060"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cone-Wesson</surname><given-names>B.</given-names></name>
<name><surname>Dowell</surname><given-names>R.C.</given-names></name>
<name><surname>Tomlin</surname><given-names>D.</given-names></name>
<name><surname>Rance</surname><given-names>G.</given-names></name>
<name><surname>Ming</surname><given-names>W.J.</given-names></name>
</person-group><article-title>The auditory steady-state response: Comparisons with the auditory brainstem response</article-title><source>J. Am. Acad. Audiol.</source><year>2002</year><volume>13</volume><fpage>173</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1055/s-0040-1715962</pub-id><pub-id pub-id-type="pmid">12025894</pub-id>
</element-citation></ref><ref id="B3-audiolres-15-00060"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Picton</surname><given-names>T.W.</given-names></name>
<name><surname>Durieux-Smith</surname><given-names>A.</given-names></name>
<name><surname>Champagne</surname><given-names>S.C.</given-names></name>
<name><surname>Whittingham</surname><given-names>J.</given-names></name>
<name><surname>Moran</surname><given-names>L.M.</given-names></name>
<name><surname>Giguere</surname><given-names>C.</given-names></name>
<name><surname>Beauregard</surname><given-names>Y.</given-names></name>
</person-group><article-title>Objective evaluation of aided thresholds using auditory steady-state responses</article-title><source>J. Am. Acad. Audiol.</source><year>1998</year><volume>9</volume><fpage>315</fpage><lpage>331</lpage><pub-id pub-id-type="pmid">9806406</pub-id>
</element-citation></ref><ref id="B4-audiolres-15-00060"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dobie</surname><given-names>R.A.</given-names></name>
<name><surname>Wilson</surname><given-names>M.J.</given-names></name>
</person-group><article-title>A comparison of <italic toggle="yes">t</italic> test, F test, and coherence methods of detecting steady-state auditory-evoked potentials, distortion-product otoacoustic emissions, or other sinusoids</article-title><source>J. Acoust. Soc. Am.</source><year>1996</year><volume>100</volume><fpage>2236</fpage><lpage>2246</lpage><pub-id pub-id-type="doi">10.1121/1.417933</pub-id><pub-id pub-id-type="pmid">8865632</pub-id>
</element-citation></ref><ref id="B5-audiolres-15-00060"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sturzebecher</surname><given-names>E.</given-names></name>
<name><surname>Cebulla</surname><given-names>M.</given-names></name>
</person-group><article-title>Automated auditory response detection: Improvement of the statistical test strategy</article-title><source>Int. J. Audiol.</source><year>2013</year><volume>52</volume><fpage>861</fpage><lpage>864</lpage><pub-id pub-id-type="doi">10.3109/14992027.2013.822995</pub-id><pub-id pub-id-type="pmid">24219121</pub-id>
</element-citation></ref><ref id="B6-audiolres-15-00060"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vidler</surname><given-names>M.</given-names></name>
<name><surname>Parker</surname><given-names>D.</given-names></name>
</person-group><article-title>Auditory brainstem response threshold estimation: Subjective threshold estimation by experienced clinicians in a computer simulation of a clinical test</article-title><source>Int. J. Audiol.</source><year>2004</year><volume>43</volume><fpage>417</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1080/14992020400050053</pub-id><pub-id pub-id-type="pmid">15515641</pub-id>
</element-citation></ref><ref id="B7-audiolres-15-00060"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bogaerts</surname><given-names>S.</given-names></name>
<name><surname>Clements</surname><given-names>J.D.</given-names></name>
<name><surname>Sullivan</surname><given-names>J.M.</given-names></name>
<name><surname>Oleskevich</surname><given-names>S.</given-names></name>
</person-group><article-title>Automated threshold detection for auditory brainstem responses: Comparison with visual estimation in a stem cell transplantation study</article-title><source>BMC Neurosci.</source><year>2009</year><volume>10</volume><elocation-id>104</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2202-10-104</pub-id><pub-id pub-id-type="pmid">19706195</pub-id>
</element-citation></ref><ref id="B8-audiolres-15-00060"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bidelman</surname><given-names>G.M.</given-names></name>
</person-group><article-title>Objective information-theoretic algorithm for detecting brainstem evoked responses to complex stimuli</article-title><source>J. Am. Acad. Audiol.</source><year>2014</year><volume>25</volume><fpage>711</fpage><lpage>722</lpage><pub-id pub-id-type="doi">10.3766/jaaa.25.8.2</pub-id></element-citation></ref><ref id="B9-audiolres-15-00060"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Champlin</surname><given-names>C.A.</given-names></name>
</person-group><article-title>Methods for detecting auditory steady-state potentials recorded from humans</article-title><source>Hear Res.</source><year>1992</year><volume>58</volume><fpage>63</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1016/0378-5955(92)90009-C</pub-id><pub-id pub-id-type="pmid">1559907</pub-id>
</element-citation></ref><ref id="B10-audiolres-15-00060"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Victor</surname><given-names>J.D.</given-names></name>
<name><surname>Mast</surname><given-names>J.</given-names></name>
</person-group><article-title>A new statistic for steady-state evoked potentials</article-title><source>Electroencephalogr. Clin. Neurophysiol.</source><year>1991</year><volume>78</volume><fpage>378</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(91)90099-P</pub-id><pub-id pub-id-type="pmid">1711456</pub-id>
</element-citation></ref><ref id="B11-audiolres-15-00060"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Norouzpour</surname><given-names>A.</given-names></name>
<name><surname>Roberts</surname><given-names>T.L.</given-names></name>
</person-group><article-title>Fcirc statistic for steady-state evoked potentials; a generalized version of Tcirc2 statistic</article-title><source>Biomed. Signal Process. Control</source><year>2024</year><volume>87</volume><elocation-id>105549</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2023.105549</pub-id></element-citation></ref><ref id="B12-audiolres-15-00060"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rocha-Muniz</surname><given-names>C.N.</given-names></name>
<name><surname>Befi-Lopes</surname><given-names>D.M.</given-names></name>
<name><surname>Schochat</surname><given-names>E.</given-names></name>
</person-group><article-title>Investigation of auditory processing disorder and language impairment using the speech-evoked auditory brainstem response</article-title><source>Hear Res.</source><year>2012</year><volume>294</volume><fpage>143</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2012.08.008</pub-id><pub-id pub-id-type="pmid">22974503</pub-id>
</element-citation></ref><ref id="B13-audiolres-15-00060"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bidelman</surname><given-names>G.M.</given-names></name>
<name><surname>Lowther</surname><given-names>J.E.</given-names></name>
<name><surname>Tak</surname><given-names>S.H.</given-names></name>
<name><surname>Alain</surname><given-names>C.</given-names></name>
</person-group><article-title>Mild cognitive impairment is characterized by deficient hierarchical speech coding between auditory brainstem and cortex</article-title><source>J. Neurosci.</source><year>2017</year><volume>37</volume><fpage>3610</fpage><lpage>3620</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3700-16.2017</pub-id><pub-id pub-id-type="pmid">28270574</pub-id>
</element-citation></ref><ref id="B14-audiolres-15-00060"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Johnson</surname><given-names>K.L.</given-names></name>
<name><surname>Nicol</surname><given-names>T.G.</given-names></name>
<name><surname>Kraus</surname><given-names>N.</given-names></name>
</person-group><article-title>Brain stem response to speech: A biological marker of auditory processing</article-title><source>Ear Hear</source><year>2005</year><volume>26</volume><fpage>424</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1097/01.aud.0000179687.71662.6e</pub-id><pub-id pub-id-type="pmid">16230893</pub-id>
</element-citation></ref><ref id="B15-audiolres-15-00060"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Purcell</surname><given-names>D.W.</given-names></name>
<name><surname>John</surname><given-names>S.M.</given-names></name>
<name><surname>Schneider</surname><given-names>B.A.</given-names></name>
<name><surname>Picton</surname><given-names>T.W.</given-names></name>
</person-group><article-title>Human temporal auditory acuity as assessed by envelope following responses</article-title><source>J. Acoust. Soc. Am.</source><year>2004</year><volume>116</volume><fpage>3581</fpage><lpage>3593</lpage><pub-id pub-id-type="doi">10.1121/1.1798354</pub-id><pub-id pub-id-type="pmid">15658709</pub-id>
</element-citation></ref><ref id="B16-audiolres-15-00060"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pluim</surname><given-names>J.P.</given-names></name>
<name><surname>Maintz</surname><given-names>J.B.</given-names></name>
<name><surname>Viergever</surname><given-names>M.A.</given-names></name>
</person-group><article-title>Mutual-information-based registration of medical images: A survey</article-title><source>IEEE Trans. Med. Imaging</source><year>2003</year><volume>22</volume><fpage>986</fpage><lpage>1004</lpage><pub-id pub-id-type="doi">10.1109/TMI.2003.815867</pub-id><pub-id pub-id-type="pmid">12906253</pub-id>
</element-citation></ref><ref id="B17-audiolres-15-00060"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Herdman</surname><given-names>A.T.</given-names></name>
<name><surname>Lins</surname><given-names>O.</given-names></name>
<name><surname>van Roon</surname><given-names>P.</given-names></name>
<name><surname>Stapells</surname><given-names>D.R.</given-names></name>
<name><surname>Scherg</surname><given-names>M.</given-names></name>
<name><surname>Picton</surname><given-names>T.</given-names></name>
</person-group><article-title>Intracerebral sources of human auditory steady-state responses</article-title><source>Brain Topogr.</source><year>2002</year><volume>15</volume><fpage>69</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1023/A:1021470822922</pub-id><pub-id pub-id-type="pmid">12537303</pub-id>
</element-citation></ref><ref id="B18-audiolres-15-00060"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Johnson</surname><given-names>T.A.</given-names></name>
<name><surname>Brown</surname><given-names>C.J.</given-names></name>
</person-group><article-title>Threshold prediction using the auditory steady-state response and the tone burst auditory brain stem response: A within-subject comparison</article-title><source>Ear Hear</source><year>2005</year><volume>26</volume><fpage>559</fpage><lpage>576</lpage><pub-id pub-id-type="doi">10.1097/01.aud.0000188105.75872.a3</pub-id><pub-id pub-id-type="pmid">16377993</pub-id>
</element-citation></ref><ref id="B19-audiolres-15-00060"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Noordanus</surname><given-names>E.</given-names></name>
<name><surname>Van Opstal</surname><given-names>A.J.</given-names></name>
</person-group><article-title>Towards real-time detection of auditory steady-state responses: A comparative study</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>108975</fpage><lpage>108991</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2021.3100157</pub-id></element-citation></ref><ref id="B20-audiolres-15-00060"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Oldfield</surname><given-names>R.C.</given-names></name>
</person-group><article-title>The assessment and analysis of handedness: The Edinburgh inventory</article-title><source>Neuropsychologia</source><year>1971</year><volume>9</volume><fpage>97</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id><pub-id pub-id-type="pmid">5146491</pub-id>
</element-citation></ref><ref id="B21-audiolres-15-00060"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bidelman</surname><given-names>G.M.</given-names></name>
<name><surname>Bhagat</surname><given-names>S.P.</given-names></name>
</person-group><article-title>Objective detection of auditory steady-state evoked potentials based on mutual information</article-title><source>Int. J. Audiol.</source><year>2016</year><volume>55</volume><fpage>313</fpage><lpage>319</lpage><pub-id pub-id-type="doi">10.3109/14992027.2016.1141246</pub-id><pub-id pub-id-type="pmid">26924597</pub-id>
</element-citation></ref><ref id="B22-audiolres-15-00060"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aiken</surname><given-names>S.J.</given-names></name>
<name><surname>Picton</surname><given-names>T.W.</given-names></name>
</person-group><article-title>Envelope and spectral frequency-following responses to vowel sounds</article-title><source>Hear Res.</source><year>2008</year><volume>245</volume><fpage>35</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2008.08.004</pub-id><pub-id pub-id-type="pmid">18765275</pub-id>
</element-citation></ref><ref id="B23-audiolres-15-00060"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Oxenham</surname><given-names>A.J.</given-names></name>
<name><surname>Bernstein</surname><given-names>J.G.W.</given-names></name>
<name><surname>Penagos</surname><given-names>H.</given-names></name>
</person-group><article-title>Correct tonotopic representation is necessary for complex pitch perception</article-title><source>Proc. Natl. Acad. Sci. USA</source><year>2004</year><volume>101</volume><fpage>1421</fpage><lpage>1425</lpage><pub-id pub-id-type="doi">10.1073/pnas.0306958101</pub-id><pub-id pub-id-type="pmid">14718671</pub-id>
</element-citation></ref><ref id="B24-audiolres-15-00060"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lins</surname><given-names>O.G.</given-names></name>
<name><surname>Picton</surname><given-names>P.E.</given-names></name>
<name><surname>Picton</surname><given-names>T.W.</given-names></name>
<name><surname>Champagn</surname><given-names>S.C.</given-names></name>
<name><surname>Durieux-Smith</surname><given-names>A.</given-names></name>
</person-group><article-title>Auditory steady-state responses to tones amplitude-modulated at 80-110 Hz</article-title><source>J. Acoust. Soc. Am.</source><year>1995</year><volume>97</volume><fpage>3051</fpage><lpage>3063</lpage><pub-id pub-id-type="doi">10.1121/1.411869</pub-id><pub-id pub-id-type="pmid">7759645</pub-id>
</element-citation></ref><ref id="B25-audiolres-15-00060"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hanley</surname><given-names>J.A.</given-names></name>
<name><surname>McNeil</surname><given-names>B.J.</given-names></name>
</person-group><article-title>A method of comparing the areas under receiver operating characteristic curves derived from the same cases</article-title><source>Radiology</source><year>1983</year><volume>148</volume><fpage>839</fpage><lpage>843</lpage><pub-id pub-id-type="doi">10.1148/radiology.148.3.6878708</pub-id><pub-id pub-id-type="pmid">6878708</pub-id>
</element-citation></ref><ref id="B26-audiolres-15-00060"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bates</surname><given-names>D.</given-names></name>
<name><surname>M&#x000e4;chler</surname><given-names>M.</given-names></name>
<name><surname>Bolker</surname><given-names>B.</given-names></name>
<name><surname>Walker</surname><given-names>S.</given-names></name>
</person-group><article-title>Fitting linear mixed-effects models using lme4</article-title><source>J. Stat. Softw.</source><year>2015</year><volume>67</volume><fpage>1</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></element-citation></ref><ref id="B27-audiolres-15-00060"><label>27.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>British Society of Audiology</collab>
</person-group><article-title>Auditory Steady State Response (ASSR) Testing</article-title><year>2023</year><comment>Available online: <ext-link xlink:href="https://www.thebsa.org.uk/wp-content/uploads/2023/10/OD104-115-Practice-Guidance-for-ASSR-Testing-1.pdf" ext-link-type="uri">https://www.thebsa.org.uk/wp-content/uploads/2023/10/OD104-115-Practice-Guidance-for-ASSR-Testing-1.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-07">(accessed on 7 March 2025)</date-in-citation></element-citation></ref><ref id="B28-audiolres-15-00060"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Slugocki</surname><given-names>C.</given-names></name>
<name><surname>Bosnyak</surname><given-names>D.</given-names></name>
<name><surname>Trainor</surname><given-names>L.</given-names></name>
</person-group><article-title>Simultaneously-evoked auditory potentials (SEAP): A new method for concurrent measurement of cortical and subcortical auditory-evoked activity</article-title><source>Hear Res.</source><year>2017</year><volume>345</volume><fpage>30</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2016.12.014</pub-id><pub-id pub-id-type="pmid">28043881</pub-id>
</element-citation></ref><ref id="B29-audiolres-15-00060"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bidelman</surname><given-names>G.M.</given-names></name>
</person-group><article-title>Towards an optimal paradigm for simultaneously recording cortical and brainstem auditory evoked potentials</article-title><source>J. Neurosci. Methods</source><year>2015</year><volume>241</volume><fpage>94</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2014.12.019</pub-id><pub-id pub-id-type="pmid">25561397</pub-id>
</element-citation></ref><ref id="B30-audiolres-15-00060"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>MacLean</surname><given-names>J.</given-names></name>
<name><surname>Drobny</surname><given-names>E.</given-names></name>
<name><surname>Rizzi</surname><given-names>R.</given-names></name>
<name><surname>Bidelman</surname><given-names>G.M.</given-names></name>
</person-group><article-title>Musicianship modulates cortical effects of attention on processing musical triads</article-title><source>Brain Sci.</source><year>2024</year><volume>14</volume><elocation-id>1079</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci14111079</pub-id><pub-id pub-id-type="pmid">39595842</pub-id>
</element-citation></ref><ref id="B31-audiolres-15-00060"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lai</surname><given-names>J.</given-names></name>
<name><surname>Alain</surname><given-names>C.</given-names></name>
<name><surname>Bidelman</surname><given-names>G.M.</given-names></name>
</person-group><article-title>Cortical-brainstem interplay during speech perception in older adults with and without hearing loss</article-title><source>Front. Neurosci.</source><year>2023</year><volume>17</volume><elocation-id>1075368</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2023.1075368</pub-id><pub-id pub-id-type="pmid">36816123</pub-id>
</element-citation></ref><ref id="B32-audiolres-15-00060"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sininger</surname><given-names>Y.S.</given-names></name>
<name><surname>Hunter</surname><given-names>L.L.</given-names></name>
<name><surname>Roush</surname><given-names>P.A.</given-names></name>
<name><surname>Windmill</surname><given-names>S.</given-names></name>
<name><surname>Hayes</surname><given-names>D.</given-names></name>
<name><surname>Uhler</surname><given-names>K.M.</given-names></name>
</person-group><article-title>Protocol for Rapid, Accurate, Electrophysiologic, Auditory Assessment of Infants and Toddlers</article-title><source>J. Am. Acad. Audiol.</source><year>2020</year><volume>31</volume><fpage>455</fpage><lpage>468</lpage><pub-id pub-id-type="doi">10.3766/jaaa.19046</pub-id><pub-id pub-id-type="pmid">31870467</pub-id>
</element-citation></ref><ref id="B33-audiolres-15-00060"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sugiyama</surname><given-names>S.</given-names></name>
<name><surname>Ohi</surname><given-names>K.</given-names></name>
<name><surname>Kuramitsu</surname><given-names>A.</given-names></name>
<name><surname>Takai</surname><given-names>K.</given-names></name>
<name><surname>Muto</surname><given-names>Y.</given-names></name>
<name><surname>Taniguchi</surname><given-names>T.</given-names></name>
<name><surname>Kinukawa</surname><given-names>T.</given-names></name>
<name><surname>Takeuchi</surname><given-names>N.</given-names></name>
<name><surname>Motomura</surname><given-names>E.</given-names></name>
<name><surname>Nishihara</surname><given-names>M.</given-names></name>
<etal/>
</person-group><article-title>The Auditory Steady-State Response: Electrophysiological Index for Sensory Processing Dysfunction in Psychiatric Disorders</article-title><source>Front. Psychiatry</source><year>2021</year><volume>12</volume><elocation-id>644541</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyt.2021.644541</pub-id><pub-id pub-id-type="pmid">33776820</pub-id>
</element-citation></ref><ref id="B34-audiolres-15-00060"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Guo</surname><given-names>D.</given-names></name>
<name><surname>Sheng</surname><given-names>Y.</given-names></name>
<name><surname>Ke</surname><given-names>Y.</given-names></name>
<name><surname>An</surname><given-names>X.</given-names></name>
<name><surname>He</surname><given-names>F.</given-names></name>
<name><surname>Ming</surname><given-names>D.</given-names></name>
</person-group><article-title>Enhanced Auditory Steady-State Response Using an Optimized Chirp Stimulus-Evoked Paradigm</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>748</elocation-id><pub-id pub-id-type="doi">10.3390/s19030748</pub-id><pub-id pub-id-type="pmid">30759874</pub-id>
</element-citation></ref><ref id="B35-audiolres-15-00060"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sininger</surname><given-names>Y.S.</given-names></name>
<name><surname>Hunter</surname><given-names>L.L.</given-names></name>
<name><surname>Hayes</surname><given-names>D.</given-names></name>
<name><surname>Roush</surname><given-names>P.A.</given-names></name>
<name><surname>Uhler</surname><given-names>K.M.</given-names></name>
</person-group><article-title>Evaluation of speed and accuracy of next-generation auditory steady state response and auditory brainstem response audiometry in children with normal hearing and hearing loss</article-title><source>Ear Hear</source><year>2018</year><volume>39</volume><fpage>1207</fpage><lpage>1223</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000580</pub-id><pub-id pub-id-type="pmid">29624540</pub-id>
</element-citation></ref><ref id="B36-audiolres-15-00060"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hamad</surname><given-names>H.</given-names></name>
<name><surname>Washnik</surname><given-names>N.J.</given-names></name>
<name><surname>Suresh</surname><given-names>C.H.</given-names></name>
</person-group><article-title>Next-generation auditory steady-state responses in normal-hearing adults: A pilot test&#x02013;retest reliability study</article-title><source>J. Otorhinolaryngol. Hear. Balance Med.</source><year>2023</year><volume>4</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.3390/ohbm4020006</pub-id></element-citation></ref><ref id="B37-audiolres-15-00060"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ehrmann-M&#x000fc;ller</surname><given-names>D.</given-names></name>
<name><surname>Shehata-Dieler</surname><given-names>W.</given-names></name>
<name><surname>Alzoubi</surname><given-names>A.</given-names></name>
<name><surname>Hagen</surname><given-names>R.</given-names></name>
<name><surname>Cebulla</surname><given-names>M.</given-names></name>
</person-group><article-title>Using ASSR with narrow-band chirps to evaluate hearing in children and adults</article-title><source>Eur. Arch. Otorhinolaryngol.</source><year>2021</year><volume>278</volume><fpage>49</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1007/s00405-020-06053-0</pub-id><pub-id pub-id-type="pmid">32449020</pub-id>
</element-citation></ref><ref id="B38-audiolres-15-00060"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kuwada</surname><given-names>S.</given-names></name>
<name><surname>Anderson</surname><given-names>J.S.</given-names></name>
<name><surname>Batra</surname><given-names>R.</given-names></name>
<name><surname>Fitzpatrick</surname><given-names>D.C.</given-names></name>
<name><surname>Teissier</surname><given-names>N.</given-names></name>
<name><surname>D&#x02019;Angelo</surname><given-names>W.R.</given-names></name>
</person-group><article-title>Sources of the scalp-recorded amplitude-modulation following response</article-title><source>J. Am. Acad. Audiol.</source><year>2002</year><volume>13</volume><fpage>188</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1055/s-0040-1715963</pub-id><pub-id pub-id-type="pmid">12025895</pub-id>
</element-citation></ref><ref id="B39-audiolres-15-00060"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Van Maanen</surname><given-names>A.</given-names></name>
<name><surname>Stapells</surname><given-names>D.R.</given-names></name>
</person-group><article-title>Comparison of multiple auditory steady-state responses (80 versus 40 Hz) and slow cortical potentials for threshold estimation in hearing-impaired adults</article-title><source>Int. J. Audiol.</source><year>2005</year><volume>44</volume><fpage>613</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1080/14992020500258628</pub-id><pub-id pub-id-type="pmid">16379489</pub-id>
</element-citation></ref><ref id="B40-audiolres-15-00060"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Galambos</surname><given-names>R.</given-names></name>
<name><surname>Makeig</surname><given-names>S.</given-names></name>
<name><surname>Talmachoff</surname><given-names>P.</given-names></name>
</person-group><article-title>A 40-Hz auditory potential recorded from the human scalp</article-title><source>Proc. Natl. Acad. Sci. USA</source><year>1981</year><volume>78</volume><fpage>2643</fpage><lpage>2647</lpage><pub-id pub-id-type="doi">10.1073/pnas.78.4.2643</pub-id><pub-id pub-id-type="pmid">6941317</pub-id>
</element-citation></ref><ref id="B41-audiolres-15-00060"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Korczak</surname><given-names>P.</given-names></name>
<name><surname>Smart</surname><given-names>J.</given-names></name>
<name><surname>Delgado</surname><given-names>R.</given-names></name>
<name><surname>Strobel</surname><given-names>T.M.</given-names></name>
<name><surname>Bradford</surname><given-names>C.</given-names></name>
</person-group><article-title>Auditory steady-state responses</article-title><source>J. Am. Acad. Audiol.</source><year>2012</year><volume>23</volume><fpage>146</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.3766/jaaa.23.3.3</pub-id><pub-id pub-id-type="pmid">22436114</pub-id>
</element-citation></ref><ref id="B42-audiolres-15-00060"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pethe</surname><given-names>J.</given-names></name>
<name><surname>Muhler</surname><given-names>R.</given-names></name>
<name><surname>Siewert</surname><given-names>K.</given-names></name>
<name><surname>von Specht</surname><given-names>H.</given-names></name>
</person-group><article-title>Near-threshold recordings of amplitude modulation following responses (AMFR) in children of different ages</article-title><source>Int. J. Audiol.</source><year>2004</year><volume>43</volume><fpage>339</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1080/14992020400050043</pub-id><pub-id pub-id-type="pmid">15457816</pub-id>
</element-citation></ref><ref id="B43-audiolres-15-00060"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>L.T.</given-names></name>
<name><surname>Rickards</surname><given-names>F.W.</given-names></name>
<name><surname>Clark</surname><given-names>G.M.</given-names></name>
</person-group><article-title>A comparison of steady-state evoked potentials to modulated tones in awake and sleeping humans</article-title><source>J. Acoust. Soc. Am.</source><year>1991</year><volume>90</volume><fpage>2467</fpage><lpage>2479</lpage><pub-id pub-id-type="doi">10.1121/1.402050</pub-id><pub-id pub-id-type="pmid">1774415</pub-id>
</element-citation></ref><ref id="B44-audiolres-15-00060"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kuwada</surname><given-names>S.</given-names></name>
<name><surname>Batra</surname><given-names>R.</given-names></name>
<name><surname>Maher</surname><given-names>V.L.</given-names></name>
</person-group><article-title>Scalp potentials of normal and hearing-impaired subjects in response to sinusoidally amplitude-modulated tones</article-title><source>Hear Res.</source><year>1986</year><volume>21</volume><fpage>179</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/0378-5955(86)90038-9</pub-id><pub-id pub-id-type="pmid">3700256</pub-id>
</element-citation></ref><ref id="B45-audiolres-15-00060"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Picton</surname><given-names>T.W.</given-names></name>
<name><surname>Dimitrijevic</surname><given-names>A.</given-names></name>
<name><surname>Perez-Abalo</surname><given-names>M.C.</given-names></name>
<name><surname>Van Roon</surname><given-names>P.</given-names></name>
</person-group><article-title>Estimating audiometric thresholds using auditory steady-state responses</article-title><source>J. Am. Acad. Audiol.</source><year>2005</year><volume>16</volume><fpage>140</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.3766/jaaa.16.3.3</pub-id><pub-id pub-id-type="pmid">15844740</pub-id>
</element-citation></ref><ref id="B46-audiolres-15-00060"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Stroebel</surname><given-names>D.</given-names></name>
<name><surname>Swanepoel</surname><given-names>W.</given-names></name>
<name><surname>Groenewald</surname><given-names>E.</given-names></name>
</person-group><article-title>Aided auditory steady-state responses in infants</article-title><source>Int. J. Audiol.</source><year>2007</year><volume>46</volume><fpage>287</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1080/14992020701212630</pub-id><pub-id pub-id-type="pmid">17530513</pub-id>
</element-citation></ref><ref id="B47-audiolres-15-00060"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bidelman</surname><given-names>G.M.</given-names></name>
<name><surname>Villafuerte</surname><given-names>J.W.</given-names></name>
<name><surname>Moreno</surname><given-names>S.</given-names></name>
<name><surname>Alain</surname><given-names>C.</given-names></name>
</person-group><article-title>Age-related changes in the subcortical-cortical encoding and categorical perception of speech</article-title><source>Neurobiol. Aging</source><year>2014</year><volume>35</volume><fpage>2526</fpage><lpage>2540</lpage><pub-id pub-id-type="doi">10.1016/j.neurobiolaging.2014.05.006</pub-id><pub-id pub-id-type="pmid">24908166</pub-id>
</element-citation></ref><ref id="B48-audiolres-15-00060"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Anderson</surname><given-names>S.</given-names></name>
<name><surname>Bieber</surname><given-names>R.</given-names></name>
<name><surname>Schloss</surname><given-names>A.</given-names></name>
</person-group><article-title>Peripheral deficits and phase-locking declines in aging adults</article-title><source>Hear Res.</source><year>2021</year><volume>403</volume><fpage>108188</fpage><pub-id pub-id-type="doi">10.1016/j.heares.2021.108188</pub-id><pub-id pub-id-type="pmid">33581668</pub-id>
</element-citation></ref><ref id="B49-audiolres-15-00060"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Reetzke</surname><given-names>R.</given-names></name>
<name><surname>Xie</surname><given-names>Z.</given-names></name>
<name><surname>Llanos</surname><given-names>F.</given-names></name>
<name><surname>Chandrasekaran</surname><given-names>B.</given-names></name>
</person-group><article-title>Tracing the trajectory of sensory plasticity across different stages of speech learning in adulthood</article-title><source>Curr. Biol.</source><year>2018</year><volume>28</volume><fpage>1419</fpage><lpage>1427.e1414</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.03.026</pub-id><pub-id pub-id-type="pmid">29681473</pub-id>
</element-citation></ref><ref id="B50-audiolres-15-00060"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>MacLean</surname><given-names>J.</given-names></name>
<name><surname>Stirn</surname><given-names>J.</given-names></name>
<name><surname>Sisson</surname><given-names>A.</given-names></name>
<name><surname>Bidelman</surname><given-names>G.M.</given-names></name>
</person-group><article-title>Short- and long-term neuroplasticity interact during the perceptual learning of concurrent speech</article-title><source>Cereb. Cortex</source><year>2024</year><volume>34</volume><fpage>bhad543</fpage><pub-id pub-id-type="doi">10.1093/cercor/bhad543</pub-id><pub-id pub-id-type="pmid">38212291</pub-id>
</element-citation></ref><ref id="B51-audiolres-15-00060"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Venail</surname><given-names>F.</given-names></name>
<name><surname>Artaud</surname><given-names>J.P.</given-names></name>
<name><surname>Blanchet</surname><given-names>C.</given-names></name>
<name><surname>Uziel</surname><given-names>A.</given-names></name>
<name><surname>Mondain</surname><given-names>M.</given-names></name>
</person-group><article-title>Refining the audiological assessment in children using narrow-band CE-Chirp-evoked auditory steady state responses</article-title><source>Int. J. Audiol.</source><year>2015</year><volume>54</volume><fpage>106</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.3109/14992027.2014.935496</pub-id><pub-id pub-id-type="pmid">25036002</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="audiolres-15-00060-f001"><label>Figure 1</label><caption><p>Auditory steady-state response (ASSR) waveforms and spectra elicited by 40 Hz (<bold>left</bold>) and 80 Hz (<bold>right</bold>) SAM tones (<italic toggle="yes">fc</italic> = 1 kHz) showing the level dependence of responses. ASSR waveforms show phase locking at the stimulus modulation rate and first few harmonics which progressively weakens with decreasing level, approaching the noise floor at ~20 dB SPL. Gray traces, sham recording in which the earphone was removed from the ear canal (i.e., EEG noise floor). &#x025bc; = response energy at the <italic toggle="yes">fm</italic>.</p></caption><graphic xlink:href="audiolres-15-00060-g001" position="float"/></fig><fig position="float" id="audiolres-15-00060-f002"><label>Figure 2</label><caption><p>Characterizing the quality of ASSR recordings using mutual information (MI). (<bold>A</bold>) (<italic toggle="yes">left</italic>) A rectified stimulus spectrogram for a 40 Hz SAM tone stimulus. (<italic toggle="yes">right</italic>) Spectrograms of ASSRs recorded for a descending-level series. The inset of each panel indicates the MI computed between each response spectrogram and that of the stimulus [<xref rid="B8-audiolres-15-00060" ref-type="bibr">8</xref>,<xref rid="B21-audiolres-15-00060" ref-type="bibr">21</xref>]. With decreasing level, time&#x02013;frequency representations of the neural ASSRs show less correspondence with those of the stimulus, as indicated by decreasing values of MI. (<bold>B</bold>) Signal detection theory analysis to determine an optimal criterion (MI<italic toggle="yes"><sub>&#x003b8;</sub></italic>) for the MI response classifier. Shown here is the distribution (probability density functions) of MI values for 40 and 80 Hz ASSRs (pooled across stimulus levels) and sham recordings. MI is always larger for true vs. sham recordings. The criterion MI<italic toggle="yes"><sub>&#x003b8;</sub></italic> = 0.93 segregates 95% of suprathreshold ASSRs from sham noise. From a classifier perspective, recordings containing MI &#x0003e; MI<italic toggle="yes"><sub>&#x003b8;</sub></italic> (=0.93) are predicted to contain a true neural ASSR response, whereas recordings with MI &#x0003c; MI<italic toggle="yes"><sub>&#x003b8;</sub></italic> are considered noise (no response).</p></caption><graphic xlink:href="audiolres-15-00060-g002" position="float"/></fig><fig position="float" id="audiolres-15-00060-f003"><label>Figure 3</label><caption><p>Receiver operating characteristic (ROC) curves for ASSRs evoked by different modulation rates and levels. ROC curves measure tradeoff in sensitivity and specificity with different MI cutoffs (marked by different points along each curve). Individual points denote the true positive (sensitivity) vs. false positive (1-specificity) rates for various values of MI in distinguishing true from sham recordings based on 2500 sweeps. Dotted lines correspond with chance performance (i.e., <italic toggle="yes">d</italic>&#x02032; = 0). With an optimal value of MI for signal&#x02013;noise segregation (MI<italic toggle="yes"><sub>&#x003b8;</sub></italic> = 0.93; see <xref rid="audiolres-15-00060-f002" ref-type="fig">Figure 2</xref>), classification accuracy (AUC) is 99% for 40 Hz ASSRs at 80 dB SPL and 90% for 80 Hz ASSRs. Classification accuracy is better for low (40 Hz) compared to high (80 Hz) modulation rates and high- vs. low-level stimuli.</p></caption><graphic xlink:href="audiolres-15-00060-g003" position="float"/></fig><fig position="float" id="audiolres-15-00060-f004"><label>Figure 4</label><caption><p>Acceptable MI values for detecting suprathreshold ASSRs across stimulus levels and modulation rates. (<bold>A</bold>,<bold>B</bold>) Level-dependent classification accuracy for the 40 Hz (<bold>A</bold>) and 80 Hz (<bold>B</bold>) responses. Each family of functions shows the overall accuracy in correctly detecting ASSRs from noise using different MI cutoffs. (<bold>C</bold>) Range of acceptable MI values for classifying 40 and 80 Hz ASSRs above chance levels. Ranges were extracted from the width of each level-dependent accuracy function shown in panels A and B. Color scheme, same as in panel A. (<bold>D</bold>) Max accuracy for detecting ASSRs and the corresponding MI. Max accuracy was extracted from the peak of each level-dependent accuracy function of panel A and B. Note that some points for the 80 Hz responses overlap. (<bold>E</bold>) Overall accuracy across stimulus levels and modulation rates. Accuracies were extracted from ROC functions (e.g., <xref rid="audiolres-15-00060-f003" ref-type="fig">Figure 3</xref>) as the area under the curve (AUC). (<bold>F</bold>) Sensitivity of the MI metric for a fixed false positive rate of 5%. Response detection accuracy and sensitivity are better for 40 Hz compared to 80 Hz responses, decrease with decreasing stimulus level, but remain well above chance. Error bars = &#x02212;95% CI.</p></caption><graphic xlink:href="audiolres-15-00060-g004" position="float"/></fig><fig position="float" id="audiolres-15-00060-f005"><label>Figure 5</label><caption><p>A comparison of the growth in MI with the <italic toggle="yes">F</italic>-test during online ASSR recording. Sweep-by-sweep ASSR detection based on MI for the 40 Hz (<bold>A</bold>) and 80 Hz (<bold>B</bold>) responses at each level. (<bold>C</bold>,<bold>D</bold>) Improvements in ASSR detection using the conventional <italic toggle="yes">F</italic>-test procedure [<xref rid="B1-audiolres-15-00060" ref-type="bibr">1</xref>]. Dotted lines denote the criterion threshold for ASSR detection under each metric (MI = 1.0; <italic toggle="yes">F</italic>-test: <italic toggle="yes">p</italic> &#x0003c; 0.05). The abscissa shows both the number of sweeps and corresponding recording time for online ASSR averaging. Shading = &#x000b1;1 s.e.m.</p></caption><graphic xlink:href="audiolres-15-00060-g005" position="float"/></fig><fig position="float" id="audiolres-15-00060-f006"><label>Figure 6</label><caption><p>A comparison of the stopping time (# of sweeps) for ASSR recordings between the conventional F-test and MI (MI = 1). Longer recording times are needed for lower-level stimuli. MI generally outperforms the F-test with earlier response detection (i.e., faster recording times). Dotted line = max (ceiling) number of sweeps in the recording session. Error bars = &#x02212;95% CI.</p></caption><graphic xlink:href="audiolres-15-00060-g006" position="float"/></fig></floats-group></article>