<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006438</article-id><article-id pub-id-type="pmc">PMC11859670</article-id><article-id pub-id-type="doi">10.3390/s25041209</article-id><article-id pub-id-type="publisher-id">sensors-25-01209</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Preprocessing Method for Insulation Pull Rod Defect Dataset Based on the YOLOv5s Object Detection Network</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-4736-9315</contrib-id><name><surname>Li</surname><given-names>Xuetong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-01209" ref-type="aff">1</xref><xref rid="fn1-sensors-25-01209" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><name><surname>Cong</surname><given-names>Meng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af2-sensors-25-01209" ref-type="aff">2</xref><xref rid="fn1-sensors-25-01209" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Bo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af2-sensors-25-01209" ref-type="aff">2</xref><xref rid="fn1-sensors-25-01209" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><name><surname>Fan</surname><given-names>Xianhao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01209" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0890-2220</contrib-id><name><surname>Qin</surname><given-names>Weiqi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01209" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2849-9574</contrib-id><name><surname>Liang</surname><given-names>Fangwei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01209" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Chuanyang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><xref rid="af1-sensors-25-01209" ref-type="aff">1</xref><xref rid="c1-sensors-25-01209" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>He</surname><given-names>Jinliang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-01209" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Smolka</surname><given-names>Bogdan</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01209"><label>1</label>Department of Electrical Engineering, Tsinghua University, Beijing 100084, China; <email>lixt23@mails.tsinghua.edu.cn</email> (X.L.); <email>xianhaofan@tsinghua.edu.cn</email> (X.F.); <email>qinwq@mail.tsinghua.edu.cn</email> (W.Q.); <email>liangfangwei@tsinghua.edu.cn</email> (F.L.); <email>hejl@tsinghua.edu.cn</email> (J.H.)</aff><aff id="af2-sensors-25-01209"><label>2</label>Shandong Taikai Electrical Insulation Co., Ltd., Taian 271000, China; <email>congm@sdtaikai.com</email> (M.C.); <email>liubo@sdtaikai.com</email> (B.L.)</aff><author-notes><corresp id="c1-sensors-25-01209"><label>*</label>Correspondence: <email>chuanyang_li@tsinghua.edu.cn</email></corresp><fn id="fn1-sensors-25-01209"><label>&#x02020;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>17</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1209</elocation-id><history><date date-type="received"><day>27</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>13</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>14</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Insulation pull rods used in gas-insulated switchgear (GIS) inevitably contain the micro defects generated during production. The intelligent identification method, which requires large datasets with a balanced distribution of defect types, is regarded as the prevailing way to avoid insulation faults. However, the number of defective pull rods is limited, and the occurrence of different types of defects is highly imbalanced in actual production, leading to poor recognition performance. Thus, this work proposes a data preprocessing method for the insulation pull rod defect feature dataset. In this work, the YOLOv5s algorithm is used to detect defects in insulation pull rod images, creating a dataset with five defect categories. Two preprocessing methods for impurities and bubbles are introduced, including copy&#x02013;paste within images and bounding box corrections for hair-like impurities. The results show that these two methods can specifically enhance small-sized defect targets while maintaining the detection performance for other types of targets. In contrast, the proposed method integrates copy&#x02013;paste within images with Mosaic data augmentation and corrects bounding boxes for hair-like impurities significantly improving the model&#x02019;s performance.</p></abstract><kwd-group><kwd>pull rod</kwd><kwd>defect detection</kwd><kwd>YOLOv5s</kwd><kwd>copy&#x02013;paste</kwd><kwd>bounding box</kwd><kwd>Mosaic</kwd><kwd>data augmentation</kwd></kwd-group><funding-group><award-group><funding-source>Science Fund Program for Distinguished Young Scholars (Overseas)</funding-source><award-id>23FAA00813</award-id></award-group><funding-statement>This research was funded by the Science Fund Program for Distinguished Young Scholars (Overseas) under grant number 23FAA00813.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01209"><title>1. Introduction</title><p>Gas-insulated switchgear (GIS) has been widely used in power grids [<xref rid="B1-sensors-25-01209" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01209" ref-type="bibr">2</xref>], in which the insulation pull rod is employed as a key insulating component in GIS and is responsible for transmitting mechanical motion. Insulation pull rods are typically made from fiber-reinforced epoxy composites, and defects can occur during production, transportation, and installation. The presence of defects may lead to insulation breakdown and further affect the reliable operation of GIS equipment [<xref rid="B3-sensors-25-01209" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01209" ref-type="bibr">4</xref>]. Therefore, accurate detection of insulation pull rod defects is essential to reduce the operational risks of GIS devices and ensure the high reliability of power grid operations.</p><p>In recent years, with the rapid development of artificial intelligence and deep learning technologies, deep learning-based object detection techniques have made significant progress. The YOLO (You Only Look Once) series is a typical single-stage detector widely used for object detection. In the power industry, YOLO-based detectors have been applied in various scenarios. Feng et al. [<xref rid="B5-sensors-25-01209" ref-type="bibr">5</xref>] introduced the K-means clustering method into the YOLOv5x model to achieve automated detection of insulator defects in power transmission lines. Liu et al. [<xref rid="B6-sensors-25-01209" ref-type="bibr">6</xref>] incorporated Ghost convolution and model pruning into the YOLOv5s network to reduce model parameters. This approach significantly improved detection speed without compromising accuracy and was used to detect whether safety harnesses were properly worn during high-altitude operations by power workers. Nguyen [<xref rid="B7-sensors-25-01209" ref-type="bibr">7</xref>] proposed using AI-powered drones to automate the inspection process of power facilities. Based on the YOLOv5s network, the system monitored the condition of five types of electrical equipment: crossarms, isolators, insulators, poles, and transformers. P&#x000e9;rez-Aguilar et al. [<xref rid="B8-sensors-25-01209" ref-type="bibr">8</xref>] developed an automated computer vision mechanism based on YOLOv5 to detect hot spots in thermal images of electrical substations. Li et al. [<xref rid="B9-sensors-25-01209" ref-type="bibr">9</xref>] introduced an image enhancement method based on illumination correction and compensation to overcome lighting challenges. Combined with the YOLOv5 model, this method was used to detect insulator defects. Liang et al. [<xref rid="B10-sensors-25-01209" ref-type="bibr">10</xref>] proposed a lightweight non-destructive testing algorithm for welding defect surfaces based on attention mechanism, which solves the problem of it being difficult to efficiently and accurately detect complex defects in the ultrasonic welding process. However, current research mainly focuses on the recognition of large targets, such as insulators and human bodies, and the detection accuracy still needs to be improved when dealing with small-sized defects. The insulation rod defects that need to be detected in this study are generally small-sized defects, and these targeted detection methods need to be studied.</p><p>Another study by Li et al. [<xref rid="B11-sensors-25-01209" ref-type="bibr">11</xref>] leveraged the YOLOv5s model to detect typical defects in insulation pull rods. They demonstrated that YOLOv5s has a faster inference speed and higher recognition accuracy compared to other mainstream recognition models, including Faster R-CNN, Mask R-CNN, SSD, YOLOv4, etc., in identifying insulation rod defects. However, the dataset primarily consisted of insulation pull rods with highly distinctive features, resulting in decreased detection accuracy for rods with less prominent features encountered in real production procedures.</p><p>In practical applications, to maximize the accuracy and efficiency of a model, it is essential to ensure a large and diverse dataset. However, for defect detection in insulation pull rods, directly collecting a large number of images of typical defects is labor-intensive and incurs high production costs. Even if this data collection is feasible, the dataset may still be insufficient for model training. Therefore, data augmentation techniques are required to increase both the quantity and diversity of the dataset. For image datasets, various augmentation methods are available, including image scaling, random cropping, random flipping, color dithering, random erasing, Mixup, CutMix, Mosaic, and others to enhance data diversity [<xref rid="B12-sensors-25-01209" ref-type="bibr">12</xref>]. Among these, Mosaic data augmentation is one of the most commonly used techniques for defect detection in insulation pull rod images. In fact, the Mosaic data augmentation method can improve data utilization efficiency, enhancing the diversity and robustness of the dataset, especially when the dataset is small. It has been widely applied in the processing of various image or video datasets [<xref rid="B13-sensors-25-01209" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01209" ref-type="bibr">14</xref>].</p><p>However, this data augmentation method has some drawbacks. The generated images tend to have uniform sizes. Since there is an upper limit on the number of pixels in the images used for training, this results in a pixel limit for the images produced by Mosaic data augmentation. Consequently, the clarity of the original images is inevitably reduced, and the image size is decreased. Defect sizes in insulation pull rods vary, with defects such as bubbles and impurities being smaller in size. After applying Mosaic data augmentation, many labeled defects have a size smaller than 3 pixels, leading to poor correspondence between anchor boxes and the defect locations, which significantly hampers the accuracy of the training.</p><p>As for the typical defects observed in GIS, each individual defect conveys unique features and contours. White spots, scratches, and cracks are relatively larger in size and have more samples. The size of bubble defects is smaller, with a typical diameter of around 5 mm [<xref rid="B15-sensors-25-01209" ref-type="bibr">15</xref>]. Impurity defects, on the other hand, vary greatly in size due to the wide variety of impurities, which include metal particles, hair, threads, and other contaminants [<xref rid="B11-sensors-25-01209" ref-type="bibr">11</xref>]. Bubbles and metal impurities have very small sizes, with diameters ranging from 1 mm to 5 mm, and occupy a small proportion of the image in insulation pull rod pictures. Most image processing methods are designed for targets that occupy a relatively larger portion of the image, so when performing data augmentation, it is difficult to specifically enhance these small defects. In fact, the small size of bubbles and impurities may even lead to semantic loss in the image. Semantic loss refers to the degradation of small target features caused by downsampling or excessive enhancement in data augmentation.</p><p>In addition, due to the uneven distribution of defects in the actual production of pull rods, the proportion of defects such as white spots is much higher than that of defects such as bubbles. The proportion of imbalanced targets in the original data will be further imbalanced after expansion, resulting in a significant decrease in recognition performance. Kim et al. [<xref rid="B16-sensors-25-01209" ref-type="bibr">16</xref>] indicated that the dataset significantly affects the accuracy of object recognition. Issues such as a small dataset size and an imbalanced distribution of different object types can lead to a decline in the model&#x02019;s recognition performance. Khan et al. [<xref rid="B17-sensors-25-01209" ref-type="bibr">17</xref>] proposed that data imbalance is a common problem in many practical applications, where the number of samples in most classes far exceeds that in a few classes. This imbalance can cause the classifier to lean towards the majority class, thereby affecting the recognition performance of the minority class. Therefore, when processing dataset augmentation, it is also necessary to consider the proportion balance of various targets.</p><p>Additionally, hair-like impurities are long in length but extremely narrow in width, with complex shapes. When labeling bounding boxes, a large portion of the area in the box is invalid. Therefore, the expanded areas are mostly invalid during data augmentation, which may result in distortion of semantic information.</p><p>To address the faced challenges, this study proposes a data preprocessing method that combines Mosaic data augmentation with in-image copy&#x02013;pasting to enhance the defect features of insulation pull rods. Additionally, the bounding boxes for hair-like impurities are corrected, enabling the diverse shapes of these impurities to be standardized into two forms: diagonal and arc-shaped, thus improving detection accuracy for hair-like impurities. The YOLOv5s model, trained on the augmented dataset using this method, is employed to recognize defects in insulation pull rod images. The results show that, compared to using only Mosaic data augmentation, the model trained on the dataset expanded with this method and improved its mAP@0.5:0.95 from 0.618 to 0.789.</p></sec><sec id="sec2-sensors-25-01209"><title>2. YOLOv5s Network Architecture and Procedure</title><sec id="sec2dot1-sensors-25-01209"><title>2.1. Overview of YOLOv5s Network</title><p>In this study, the YOLOv5 model is used for defect detection, as it is a classic object detection model. Compared to other traditional object detection models, such as SSD and Faster R-CNN, YOLOv5 offers advantages, including fast detection speed, high accuracy, ease of training, and lightweight architecture [<xref rid="B18-sensors-25-01209" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-01209" ref-type="bibr">19</xref>]. YOLOv5 is also tailored for different application scenarios and is divided into four variants based on depth and feature map width: YOLOv5s, YOLOv5m, YOLOv5l, and YOLOv5x. Among these, the YOLOv5s model has the smallest depth and feature map width, while also providing faster frame rates and inference speeds [<xref rid="B20-sensors-25-01209" ref-type="bibr">20</xref>]. Given that insulation pull rod defect detection requires both high accuracy and fast detection speed, the YOLOv5s model is the most suitable for this task. The YOLOv5s model consists of three main components: the backbone network which is responsible for feature extraction; the neck network which is responsible for feature fusion; and the prediction network. The procedure is shown in <xref rid="sensors-25-01209-f001" ref-type="fig">Figure 1</xref>.</p><p>The default training procedure for the YOLOv5s network is as follows: First, the input image is processed by resizing it to the preset dimensions of the network. In this study, the default size used is 640 &#x000d7; 640 pixels. The resized image is then split into its three RGB channels, and each pixel&#x02019;s information is normalized to a floating-point number in the range of 0 to 1. Next, based on the requirements, the model can optionally use the default Mosaic data augmentation to expand the dataset, enhancing its robustness and diversity. Additionally, during the dataset preprocessing phase, the model calculates adaptive anchor boxes based on the pre-labeled bounding boxes in the dataset in order to determine the optimal anchor boxes for that particular dataset. Once the preprocessing steps are completed, the model begins several epochs of training. In each training epoch, the YOLO algorithm uses the original anchor boxes as references, calculates the differences between the predicted anchor boxes and the actual target locations, and updates the weight matrix using backpropagation. After training is finished, the model outputs the weight matrix corresponding to the best-performing round, which is then used for object detection.</p></sec><sec id="sec2dot2-sensors-25-01209"><title>2.2. Three Parts of YOLOv5s Network</title><sec id="sec2dot2dot1-sensors-25-01209"><title>2.2.1. Backbone Feature Extraction Network</title><p>The main task of the backbone network is to extract features from images, converting the input raw image into multi-level feature maps. This part consists of the Conv module, C3_X module, and SPPF (Spatial Pyramid Pooling Fusion) module. The Conv module includes convolutional layers, batch normalization (BN) layers, and activation functions. The formula for the BN layer is shown in Equation (1), where the subscript <italic toggle="yes">k</italic> represents the <italic toggle="yes">k</italic>-th dimension of the sample, <italic toggle="yes">x<sub>k</sub></italic> and <italic toggle="yes">y<sub>k</sub></italic> represent the input and output samples of the BN layer, respectively; <italic toggle="yes">&#x003b2;<sub>k</sub></italic> and <italic toggle="yes">&#x003b3;<sub>k</sub></italic> represent the shift and scale parameters, which are not hyperparameters but are learned through the network; and <italic toggle="yes">&#x003bc;<sub>k</sub></italic> and <italic toggle="yes">&#x003c3;<sub>k</sub></italic> represent the mean and standard deviation of the input samples [<xref rid="B21-sensors-25-01209" ref-type="bibr">21</xref>].<disp-formula id="FD1-sensors-25-01209"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000a0;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The convolutional layers extract local features from the image, the BN layers normalize the extracted features, and the activation functions provide the network with nonlinear properties. The C3_X module stacks multiple convolutional operations with residual connections to efficiently extract multi-level information from the input features while reducing information loss during the feature extraction process. The SPPF module is introduced at the end of the backbone network to apply pooling operations of different scales to the convolutional feature maps, then fuses the pooled results to produce output feature maps with a consistent size, which can effectively expand the receptive field of the feature maps [<xref rid="B22-sensors-25-01209" ref-type="bibr">22</xref>].</p></sec><sec id="sec2dot2dot2-sensors-25-01209"><title>2.2.2. Neck Feature Fusion Network</title><p>The neck network adopts a structure combining FPN (Feature Pyramid Network) and PAN (Path Aggregation Network) [<xref rid="B23-sensors-25-01209" ref-type="bibr">23</xref>]. FPN uses a top-down approach to upsample the final feature map from the backbone network and fuses it with feature maps of the corresponding scales from earlier layers, enriching the feature representation. After repeating this process, the resulting fused feature maps have dimensions of 40 &#x000d7; 40 &#x000d7; 512 and 80 &#x000d7; 80 &#x000d7; 256. PAN uses a bottom-up strategy to process the lower-level feature maps of FPN through convolution operations, then fuses the results with feature maps of the same scale from FPN. This process is repeated until it reaches the top layer of FPN, producing fused feature maps of 40 &#x000d7; 40 &#x000d7; 256 and 20 &#x000d7; 20 &#x000d7; 512. FPN propagates high-level semantic information downwards, while PAN propagates positional information upwards. Together, they perform feature aggregation at different detection layers.</p></sec><sec id="sec2dot2dot3-sensors-25-01209"><title>2.2.3. Prediction Network</title><p>The YOLOv5 model&#x02019;s loss function is divided into three parts: confidence, classification, and localization loss functions. The training process is essentially the process of minimizing the loss function. The localization loss function uses the GIOU loss function [<xref rid="B24-sensors-25-01209" ref-type="bibr">24</xref>].</p><p>The formula for the YOLOv5 confidence loss function is shown in Equation (2) [<xref rid="B25-sensors-25-01209" ref-type="bibr">25</xref>]. Here, <italic toggle="yes">o</italic> represents the true confidence value, which is 1 when sample <italic toggle="yes">i</italic> is a positive sample and 0 otherwise; <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> represents the predicted confidence value, indicating the probability that the model predicts sample <italic toggle="yes">i</italic> as a positive sample; and <italic toggle="yes">N</italic> is the total number of positive and negative samples.<disp-formula id="FD2-sensors-25-01209"><label>(2)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">conf</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ln</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ln</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mi mathvariant="italic">Sigmoid</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The formula for the YOLOv5 localization loss function is shown in Equation (3). In this formula, (<italic toggle="yes">t<sub>x</sub></italic>, <italic toggle="yes">t<sub>y</sub></italic>, <italic toggle="yes">t<sub>w</sub></italic><sub>,</sub>
<italic toggle="yes">t<sub>h</sub></italic>) represents the predicted offset of sample <italic toggle="yes">i</italic> relative to the anchor box, where the coordinate values are not processed by the Sigmoid activation function; <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> represents the predicted offset of sample <italic toggle="yes">i</italic> after being processed by the Sigmoid function; (<inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula><sub>,</sub>&#x000a0;<inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula><sub>,</sub>&#x000a0;<inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) represents the true center coordinates and width or height of sample <italic toggle="yes">i</italic>; (<inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula><sub>,</sub>&#x000a0;<inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula><sub>,</sub>&#x000a0;<inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) represents the center coordinates and width or height of the anchor box <italic toggle="yes">i</italic>; and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> represents the true offset of sample <italic toggle="yes">i</italic> relative to the anchor box.<disp-formula id="FD4-sensors-25-01209"><label>(3)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">loc</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="italic">pos</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">GIoU</mml:mi><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pos</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The prediction network outputs three feature maps of different scales to detect large, medium, and small objects. YOLOv5s divides the 640 &#x000d7; 640 &#x000d7; 3 input image into <italic toggle="yes">N</italic> &#x000d7; <italic toggle="yes">N</italic> grid cells and predicts information for each cell, including the bounding box center coordinates, width, height, classification probabilities, and confidence scores. The bounding box coordinates and size represent the precise position and size of the predicted object, while the confidence score indicates the likelihood that the object exists in the grid cell, with higher values suggesting a higher probability of the object&#x02019;s presence. The classification probabilities determine the classification information of the predicted object.</p><p>First, the confidence score of each predicted bounding box is compared with a set threshold. Predictions with confidence scores below the threshold are discarded, while those above the threshold are considered to contain targets, with the position and size of the detected targets obtained. Next, the Non-Maximum Suppression (NMS) algorithm is applied to filter duplicate bounding boxes for the same target [<xref rid="B26-sensors-25-01209" ref-type="bibr">26</xref>]. Finally, the target&#x02019;s index and category are determined based on the highest classification probability. The detection process of YOLOv5s network is shown in <xref rid="sensors-25-01209-f002" ref-type="fig">Figure 2</xref>.</p></sec></sec></sec><sec id="sec3-sensors-25-01209"><title>3. Two Preprocessing Methods for Insulation Pull Rod Defect Dataset</title><sec id="sec3dot1-sensors-25-01209"><title>3.1. Copy&#x02013;Paste Data Augmentation</title><p>For small-sized bubble and impurity defects, directly applying Mosaic data augmentation leads to a significant reduction in the size of the defects in the generated images, which in turn results in a marked decrease in training performance. In this study, we use the copy&#x02013;paste method for augmenting the defect dataset. The approach involves copying a portion of the image from within the bounding box of the original image and pasting it randomly multiple times within the same image, thereby increasing the number of corresponding targets and enhancing the diversity of the dataset. In the original image, the insulation pull rod typically occupies only a central portion, while the top and bottom areas are mostly background, so the pasted regions are likely to fall into the previously inactive areas, ensuring that the defect regions are not obscured.</p><p>This data augmentation method is applied only to bubble and impurity defects. For each image, if there are multiple bubble or impurity defects, the copy&#x02013;paste operation is applied to each defect individually. Other types of defects are left untouched. The same original image can be augmented into multiple processed images, effectively increasing the number of bubble and impurity defect samples.</p><p>After performing copy&#x02013;paste on the image, the resulting images are combined with the original images for Mosaic data augmentation. The Mosaic-augmented images are then mixed with images that only underwent copy&#x02013;paste augmentation, forming the training dataset for the network. In the neck network, FPN upsamples from top to bottom, while PAN applies convolution to the lower-level feature maps of FPN in a bottom-up manner, and then fuses the convolution results with feature maps of the same scale from FPN. This generates three fused feature maps of sizes 20 &#x000d7; 20 &#x000d7; 1024, 40 &#x000d7; 40 &#x000d7; 512, and 80 &#x000d7; 80 &#x000d7; 256. The procedure of Copy&#x02013;Paste Data Augmentation Method is shown in <xref rid="sensors-25-01209-f003" ref-type="fig">Figure 3</xref>.</p><p>This method is particularly effective for small-sized impurity and bubble defects. After data augmentation, it preserves the large features in the dataset while also retaining the small-sized features introduced by Mosaic. In other words, it adds large and medium-sized defects of the corresponding types to the dataset, significantly increasing the number of these two types of defects, and leading to a more balanced distribution of different defect types in the dataset.</p></sec><sec id="sec3dot2-sensors-25-01209"><title>3.2. Correcting Bounding Boxes for Hair-like Impurities</title><p>Hair and threads are typical impurities that may occasionally fall into the raw materials during the production of insulation pull rods. These impurities typically feature very fine widths, and long lengths, and are prone to bending into various shapes. During training, YOLOv5s uses a horizontal rectangular bounding box format, which cannot rotate, making it prone to labeling a large number of irrelevant areas when annotating hair-like impurities. This significantly deteriorates the training results.</p><p>To address this issue, this study proposes a correction to the annotation method for hair-like impurities. Instead of annotating a single continuous hair strand, the hair is divided into multiple segments. These segments take two main shapes: diagonal and arc-shaped. A diagonal-shaped segment extends from one vertex of the bounding box to the opposite vertex, while an arc-shaped segment extends from one vertex to another vertex on the same side of the box. This method simplifies the diverse shapes of hair-like impurities into two more regular forms, making them more typical and easier to identify. Moreover, during detection, even if a small part of the hair impurity is not detected, the presence of any detected region is sufficient to consider the defect recognized.</p><p>The principles of hair impurity bounding box correction include the following: (1) The angle difference between diagonal hair impurities and the diagonal body should not exceed &#x000b1;15 degrees; (2) The radius of the annotation box for arched hair impurities should not exceed 90 degrees of the corresponding curvature circle; and (3) To avoid the annotation box being too small, the number of segments should not exceed six every 360 degrees.</p><p>The process begins by applying the above annotation method to the hair-like impurities in the original defect images of the insulation pull rods. Then, the annotated hair-like impurities are subject to the copy&#x02013;paste augmentation, and the augmented images are combined with the original images for Mosaic data augmentation. These Mosaic-augmented images, along with the images that only underwent copy&#x02013;paste augmentation, are then mixed to form the training dataset for the network. After passing through the FPN and PAN feature extraction layers in the YOLOv5s neck network, three fused feature maps with sizes 20 &#x000d7; 20 &#x000d7; 1024, 40 &#x000d7; 40 &#x000d7; 512 and 80 &#x000d7; 80 &#x000d7; 256 are generated. This approach operates solely on hair-like impurities based on the copy&#x02013;paste augmentation, without making any additional adjustments for other types of defects. The procedure of bounding boxes of hair-like impurities bounding boxes is shown in <xref rid="sensors-25-01209-f004" ref-type="fig">Figure 4</xref>.</p></sec></sec><sec id="sec4-sensors-25-01209"><title>4. Experiment Results and Analysis</title><sec id="sec4dot1-sensors-25-01209"><title>4.1. Overview</title><p>In this study, the YOLOv5s model was trained on multiple datasets to compare its detection performance and validate the effectiveness of the preprocessing methods described earlier. A total of seven datasets were set up, named Dataset 1 through Dataset 7. Each dataset contains 8000 images, with the detection targets being five types of insulation pull rod defects: white spots, impurities, bubbles, cracks, and scratches.</p><p>However, in the original dataset, the proportion of various types of defects is very unbalanced, and the specific quantities and proportions of the five types of defects are shown in <xref rid="sensors-25-01209-t001" ref-type="table">Table 1</xref>.</p><p>Before data augmentation, there were a total of 397 original images, of which 350 images were taken as the expanded basis for the training set, including five types of defects. The remaining 47 images will be kept as the validation set. After data augmentation, the original 350 images were expanded to 8000 images. Each target type defect was copied and pasted five times.</p><p>As shown in <xref rid="sensors-25-01209-t002" ref-type="table">Table 2</xref>, these datasets were used to evaluate the performance of the model with different preprocessing techniques.</p><p>To evaluate the effectiveness of the data preprocessing methods, mAP@0.5 and mAP@0.5:0.95 are used as evaluation metrics for detection accuracy, where mAP (mean average precision) represents the average detection accuracy across all target types, and AP (average precision) represents the average detection accuracy for a specific target type. The values of mAP and AP can be computed using Equations (4) and (5):<disp-formula id="FD5-sensors-25-01209"><label>(4)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>mAP</mml:mi><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x02211;</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-01209"><label>(5)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>AP</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x0222b;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo><mml:mi>d</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In Equation (5), the average precision is the area under the P-R (Precision-Recall) curve, where P(Precision) and R(Recall) are defined by Equations (6) and (7):<disp-formula id="FD7-sensors-25-01209"><label>(6)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-25-01209"><label>(7)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>T<sub>P</sub> (True Positive) refers to cases where the actual label is positive, and the prediction is also positive. T<sub>N</sub> (True Negative) refers to cases where the actual label is negative, and the prediction is also negative. F<sub>P</sub> (False Positive) refers to cases where the actual label is negative, but the prediction is positive. F<sub>N</sub> (False Negative) refers to cases where the actual label is positive, but the prediction is negative.</p><p>F1 score is another important parameter for evaluating the effectiveness of a model, which can be used to comprehensively assess the balance between P and R. The value of F1 score can be computed using Equation (8):<disp-formula id="FD9-sensors-25-01209"><label>(8)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi>Score</mml:mi></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>&#x022c5;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>PR</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Based on these definitions, precision reflects the model&#x02019;s ability to correctly detect positive instances, and recall reflects the proportion of actual positive instances correctly predicted by the model. In this study, the values of mAP@0.5, mAP@0.5:0.95, P, R and F1 score for each defect type are used as the evaluation criteria. Specifically, mAP@0.5 refers to the mAP value when the Intersection over Union (IoU) threshold is set to 0.5, while mAP@0.5:0.95 refers to the average mAP value when the IoU threshold is varied from 0.05 to 0.95 in steps of 0.05.</p><p>The Python version used in this study is 3.9.19, and the PyTorch version is 2.3.1.</p></sec><sec id="sec4dot2-sensors-25-01209"><title>4.2. Validation of the Effectiveness of Two Methods</title><sec id="sec4dot2dot1-sensors-25-01209"><title>4.2.1. Validation of the Effectiveness of the Copy&#x02013;Paste Method</title><p>In this study, seven training datasets were used. By training with Dataset 1, Dataset 2, and Dataset 5, in which no correction to the bounding boxes of hair-like impurities was applied, we can compare whether combining the copy&#x02013;paste method with Mosaic data augmentation improves the defect detection performance of insulation pull rods. The training was conducted using the same parameters; the starting weight file was the default YOLOv5s model weight file, and the number of training epochs was set to 100. Since the datasets had already undergone data augmentation, no further augmentation was applied during training. The training results are shown in <xref rid="sensors-25-01209-t003" ref-type="table">Table 3</xref>.</p><p><xref rid="sensors-25-01209-t003" ref-type="table">Table 3</xref> presents the training results for three datasets, listing the number of annotated defects of each type after data augmentation, along with the corresponding mAP@0.5 and mAP@0.5:0.95 values. From the table, it can be observed that when all images in the dataset undergo Mosaic data augmentation, the total number of defect samples is the highest, but the distribution is uneven. This is because, in the original data, the number of white spots and impurities is already high; therefore, after augmentation, these two defect types account for 75% of the total samples. Although the number of crack and scratch samples is relatively small, these defects are larger in size and have more distinct features, leading to better detection results. In contrast, the bubble defects, which originally constituted only 4.8% of the dataset, are smaller in size, leading to poorer training results. The detection performance for impurities is also relatively poor.</p><p>As the number of augmented images using the copy&#x02013;paste method increases, the total number of samples decreases, but the numbers of bubble and impurity defect samples decrease less or even increase. The distribution of defects becomes more balanced. It can be observed that the detection performance for bubble defects has significantly improved, while the detection of the other defect types has not noticeably declined. Although the recognition accuracy for white spots, cracks, and scratches in Datasets 2 and 4 is slightly lower than in Dataset 1, this decline is closely related to the reduction in number of samples. In future work, increasing the sample size for these two defect types can help improve their recognition performance.</p><p>Based on the Precision, Recall, and F1 score metrics, the precision values of all three datasets are relatively high; however, in Dataset 1, the recall values for bubble and impurity defects are comparatively low. This indicates that in the dataset trained using default Mosaic data augmentation, most of the identified bubble and impurity defects are correctly recognized, but a significant proportion of defects are still missed. In contrast, after applying in-image copy&#x02013;paste augmentation for bubble and impurity defects, as the number of augmented images increases, the recall for bubbles significantly improves, while the increase in recall for impurities is relatively modest. Consequently, the F1 scores for both bubbles and impurities improve with the increasing number of copy&#x02013;paste augmented images. Specifically, the F1 score for bubbles increases from 0.585 to 0.913 (<xref rid="sensors-25-01209-t003" ref-type="table">Table 3</xref>, Dataset 1 and Dataset 5, &#x0201c;Bubble&#x0201d; column), which is comparable to the F1 scores of other defect types; however, the F1 score for impurities remains significantly lower at only 0.793&#x02014;more than 0.1 below that of other types (<xref rid="sensors-25-01209-t003" ref-type="table">Table 3</xref>, Dataset 1 and Dataset 5, &#x0201c;Impurity&#x0201d; column).</p><p>This suggests that the enhancement in impurity detection is limited, indicating that the proposed method is less effective for irregular defects like impurities, and that alternative approaches are needed to further improve impurity detection performance.</p></sec><sec id="sec4dot2dot2-sensors-25-01209"><title>4.2.2. Validation of the Effectiveness of Hair-like Impurity Bounding Boxes Correction</title><p>By comparing Datasets 2, 3, and 4, and separately comparing Datasets 5, 6, and 7, we can evaluate whether the bounding box correction of hair-like impurities improves the defect detection performance of the insulation pull rod. The same training parameters were used, with the starting weight file being the default YOLOv5s model weights. The number of training epochs was set to 100, and since each dataset had already undergone data augmentation, no further augmentation was applied during training. The training results are shown in <xref rid="sensors-25-01209-t004" ref-type="table">Table 4</xref> and <xref rid="sensors-25-01209-t005" ref-type="table">Table 5</xref>.</p><p>From the results shown in the tables, it is evident that, under otherwise identical conditions, correcting the bounding boxes for hair-like impurities can significantly improve the recognition accuracy of impurity-related defects.</p><p>For Dataset 2, after correcting the bounding boxes for hair-like impurities and applying the copy&#x02013;paste augmentation method, the mAP@0.5:0.95 for impurity defects increased by 8.3%. If the images with corrected bounding boxes are further subjected to Mosaic data augmentation, combined with copy&#x02013;paste augmentation, the mAP@0.5:0.95 for impurity defects increased by 23.2%, with the mAP@0.5 surpassing 90%.</p><p>For Dataset 5, after correcting the bounding boxes for hair-like impurities and applying only the copy&#x02013;paste augmentation, the mAP@0.5:0.95 for impurity defects increased by 3.3%. When the corrected images were further augmented using Mosaic data augmentation combined with copy&#x02013;paste method, the mAP@0.5:0.95 for impurity defects increased by 13.8%, and the mAP@0.5 approached 95%.</p><p>Based on the Precision, Recall, and F1 score metrics, under otherwise identical conditions, correcting the bounding boxes for hair-like impurities can effectively improve the recall for impurity defects, thereby also enhancing their F1 score. For Dataset 2, the F1 score for impurity defects increased from 0.678 to 0.889 (<xref rid="sensors-25-01209-t004" ref-type="table">Table 4</xref>, Dataset 2 and Dataset 4, &#x0201c;Impurity&#x0201d; column); for Dataset 5, it increased from 0.793 to 0.913 (<xref rid="sensors-25-01209-t005" ref-type="table">Table 5</xref>, Dataset 5 and Dataset 7, &#x0201c;Impurity&#x0201d; column). Meanwhile, the Precision, Recall, and F1 scores for other defect types were not significantly affected. Therefore, it can be concluded that correcting the bounding boxes for hair-like impurities enables the model to more accurately detect these impurities, ultimately improving the detection rate of impurity defects.</p><p>The results show that correcting the bounding boxes for hair-like impurities, followed by Mosaic data augmentation and combined with copy&#x02013;paste, performs significantly better than applying only copy&#x02013;paste after the bounding box correction, while still using the original images for Mosaic data augmentation. Moreover, this method not only improves the recognition accuracy of impurity defects but also enhances the detection of other types of defects to a certain extent. For Dataset 2, the overall mAP@0.5:0.95 for all defect types increased by 13.6%, Precision for all defect types increased by 0.5%, Recall for all defect types increased by 14.2%, and the F1 Score for all defect types increased by 8.2% (<xref rid="sensors-25-01209-t004" ref-type="table">Table 4</xref>, Dataset 2 and Dataset 4, &#x0201c;All&#x0201d; column). For Dataset 5, the overall mAP@0.5:0.95 for all defect types increased by 8.2%, Precision for all defect types increased by 1.3%, Recall for all defect types increased by 9.7%, and F1 Score for all defect types increased by 5.9% (<xref rid="sensors-25-01209-t005" ref-type="table">Table 5</xref>, Dataset 5 and Dataset 7, &#x0201c;All&#x0201d; column). The comparison of mAP@0.5:0.95 performance and P/R/F1 score performance cross different data augmentation methods, as <xref rid="sensors-25-01209-f005" ref-type="fig">Figure 5</xref> shows.</p><p><xref rid="sensors-25-01209-f005" ref-type="fig">Figure 5</xref> shows a comprehensive comparison of the best-performing datasets in each group. It can be observed that correcting the bounding boxes for hair-like impurities, followed by Mosaic data augmentation combined with copy&#x02013;paste, resulted in a significant improvement in detection performance compared to the model trained solely with Mosaic data augmentation. The overall mAP@0.5:0.95 for all defect types increased from 61.8% to 78.9%, and a corresponding improvement of 17.1% is observed as well. The overall F1 score for all defect types increased from 0.816 to 0.951, and a corresponding improvement of 13.5% is observed as well (<xref rid="sensors-25-01209-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-01209-t005" ref-type="table">Table 5</xref>, Dataset 1 and Dataset 7, &#x0201c;All&#x0201d; column).</p><p>The model complexity during training is 7,033,114 parameters, 16.0 GFLOPs. The inference time and GPU memory consumption are listed in <xref rid="sensors-25-01209-t006" ref-type="table">Table 6</xref>.</p><p>It can be found that, after copying and pasting in the image and modifying the hair impurity bounding box, the inference time is not affected much, and the GPU memory consumption is almost unchanged.</p></sec></sec><sec id="sec4dot3-sensors-25-01209"><title>4.3. Comparison of Detection Results on Real Photos</title><p>To further evaluate the effectiveness of the two data preprocessing methods in improving the defect detection accuracy of insulation rods, <xref rid="sensors-25-01209-f006" ref-type="fig">Figure 6</xref> presents the detection results of models trained on different datasets using the same set of images. Three test images were selected for comparison. The first image contains multiple defects of white spots and impurities; the second image contains multiple bubble defects; and the third image includes both a large and small hair-like impurity. The models trained on datasets 1, 5, 4, and 7 were named A, B, C, and D, respectively.</p><p>From the detection results of the first image, it can be observed that models A and B failed to detect all the impurity defects, with model A having noticeably lower detection confidence. Model C detected all the defects but made a false positive, misclassifying a white spot as both a white spot and an impurity. Model D did not make any false positives, but missed a small impurity defect, and its confidence was slightly lower than that of Model C. In the second image, Model A failed to detect all the bubble defects, while Models B, C, and D successfully detected all bubble defects, with Models C and D showing similar confidence, both higher than Model B. In the third image, Model A completely failed to detect the larger hair impurity, while Models B and C missed the smaller hair-like impurity. Model D successfully detected both hair-like impurities, with better bounding box coverage and higher confidence than Model C.</p><p>Comparing these results, it is clear that using only Mosaic data augmentation leads to poorer detection performance. In contrast, combining image copy-pasting with Mosaic augmentation significantly improves the model&#x02019;s detection accuracy. Additionally, correcting the bounding boxes for hair-like impurities enhances the model&#x02019;s accuracy in detecting this type of defect. However, the ratio of image copy-pasting to Mosaic-augmented images should not be excessively high, as an overly large number of copy&#x02013;pasted images may not significantly improve the detection accuracy and could even lead to a decrease in performance.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01209"><title>5. Conclusions</title><p>The traditional inspection methods used for insulation rods after production are often subjective, time-consuming, and labor-intensive, which may result in missed detections or errors. This study proposes using a YOLOv5s-based image recognition system for defect detection in insulation pull rods and introduces a combined data augmentation method involving in-image copy &#x00026; paste and Mosaic augmentation. Additionally, a framework for correcting bounding boxes for hair-like impurities is proposed to improve the model&#x02019;s accuracy in detecting defects. Based on the above work, the following conclusions can be drawn:<list list-type="simple"><list-item><label>(1)</label><p>Comparing with traditional Mosaic data augmentation, the in-image copy&#x02013;paste technique significantly improves the detection accuracy of small bubble defects. This approach increases the mAP@0.5 for bubble defects from 0.531 to 0.904, increases the mAP@0.5:0.95 from 0.329 to 0.766 and increases the F1 score for bubbles from 0.585 to 0.913 (<xref rid="sensors-25-01209-t003" ref-type="table">Table 3</xref>, Dataset 1 and Dataset 5, &#x0201c;Bubble&#x0201d; column) It also raises the overall mAP@0.5 from 0.797 to 0.897, raises the overall mAP@0.5:0.95 from 0.618 to 0.707 and raises the overall F1 score from 0.816 to 0.892 (<xref rid="sensors-25-01209-t003" ref-type="table">Table 3</xref>, Dataset 1 and Dataset 5, &#x0201c;All&#x0201d; column).</p></list-item><list-item><label>(2)</label><p>Correcting the bounding boxes for typical hair-like impurities improves the detection accuracy of these defects. Standardizing the diverse shapes of hair-like impurities into two regular forms-arched-type and diagonal-type, combined with the in-image copy&#x02013;paste and Mosaic data augmentation approach, improves the detection of impurity defects. The mAP@0.5 for hair-like impurities increases from 0.777 to 0.946, mAP@0.5:0.95 rises from 0.593 to 0.731, and the F1 score rises from 0.793 to 0.913(<xref rid="sensors-25-01209-t005" ref-type="table">Table 5</xref>, Dataset 5 and Dataset 7, &#x0201c;Impurity&#x0201d; column). Meanwhile, the overall mAP@0.5 improves from 0.897 to 0.976, overall mAP@0.5:0.95 increases from 0.707 to 0.789, and the overall F1 score rises from 0.892 to 0.951(<xref rid="sensors-25-01209-t005" ref-type="table">Table 5</xref>, Dataset 5 and Dataset 7, &#x0201c;All&#x0201d; column).</p></list-item><list-item><label>(3)</label><p>The proportion of images generated through the in-image copy&#x02013;paste method should not be too high, as an excessive number of such images may negatively impact detection accuracy.</p></list-item></list></p><p>There are still several promising directions remaining for future work. Introducing Generative Adversarial Networks (GANs) to synthesize high-resolution defect samples and further alleviate data imbalance is a feasible option [<xref rid="B27-sensors-25-01209" ref-type="bibr">27</xref>]. Meanwhile, investigating feature fusion enhancements or hybrid detection approaches will also benefit the improvement of the model.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors thank the anonymous reviewers and academic editors for their constructive comments and helpful suggestions.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, X.L. and C.L.; software, X.L., M.C. and B.L.; validation, X.L., M.C. and B.L.; writing&#x02014;original draft preparation, X.L. and M.C.; writing&#x02014;review and editing, X.F., W.Q., F.L. and J.H. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Due to the nature of this research, study participants did not agree for their data to be shared publicly, so supporting data are not available.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Authors Meng Cong and Bo Liu were employed by the company Shandong Taikai Electrical Insulation Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01209"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Lin</surname><given-names>C.</given-names></name>
<name><surname>Chen</surname><given-names>G.</given-names></name>
<name><surname>Tu</surname><given-names>Y.</given-names></name>
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>Q.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
<name><surname>He</surname><given-names>J.</given-names></name>
</person-group><article-title>Field-Dependent Charging Phenomenon of HVDC Spacers Based on Dominant Charge Behaviors</article-title><source>Appl. Phys. Lett.</source><year>2019</year><volume>114</volume><fpage>203501</fpage><pub-id pub-id-type="doi">10.1063/1.5096228</pub-id></element-citation></ref><ref id="B2-sensors-25-01209"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
<name><surname>Hu</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>Q.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
<name><surname>He</surname><given-names>J.</given-names></name>
</person-group><article-title>Charge Cluster Triggers Unpredictable Insulation Surface Flashover in Pressurized SF6</article-title><source>J. Phys. D Appl. Phys.</source><year>2020</year><volume>54</volume><fpage>015308</fpage><pub-id pub-id-type="doi">10.1088/1361-6463/abb38f</pub-id></element-citation></ref><ref id="B3-sensors-25-01209"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
<name><surname>Xu</surname><given-names>G.</given-names></name>
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
<name><surname>Jia</surname><given-names>M.</given-names></name>
<name><surname>Zhong</surname><given-names>S.</given-names></name>
<name><surname>Gao</surname><given-names>Y.</given-names></name>
<name><surname>Park</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<etal/>
</person-group><article-title>Insulating Materials for Realising Carbon Neutrality: Opportunities, Remaining Issues and Challenges</article-title><source>High Voltage</source><year>2022</year><volume>7</volume><fpage>610</fpage><lpage>632</lpage><pub-id pub-id-type="doi">10.1049/hve2.12232</pub-id></element-citation></ref><ref id="B4-sensors-25-01209"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xing</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>L.</given-names></name>
<name><surname>Xu</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Zhou</surname><given-names>F.</given-names></name>
<name><surname>He</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>C.</given-names></name>
</person-group><article-title>Defects and Failure Types of Solid Insulation in Gas-Insulated Switchgear: In Situ Study and Case Analysis</article-title><source>High Voltage</source><year>2022</year><volume>7</volume><fpage>158</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1049/hve2.12127</pub-id></element-citation></ref><ref id="B5-sensors-25-01209"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Feng</surname><given-names>Z.</given-names></name>
<name><surname>Guo</surname><given-names>L.</given-names></name>
<name><surname>Huang</surname><given-names>D.</given-names></name>
<name><surname>Li</surname><given-names>R.</given-names></name>
</person-group><article-title>Electrical Insulator Defects Detection Method Based on YOLOv5</article-title><source>Proceedings of the 2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS)</source><conf-loc>Suzhou, China</conf-loc><conf-date>14&#x02013;16 May 2021</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2021</year><fpage>979</fpage><lpage>984</lpage></element-citation></ref><ref id="B6-sensors-25-01209"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>L.</given-names></name>
<name><surname>Huang</surname><given-names>K.</given-names></name>
<name><surname>Bai</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Q.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
</person-group><article-title>Real-Time Detection Model of Electrical Work Safety Belt Based on Lightweight Improved YOLOv5</article-title><source>J. Real-Time Image Process.</source><year>2024</year><volume>21</volume><fpage>151</fpage><pub-id pub-id-type="doi">10.1007/s11554-024-01533-6</pub-id></element-citation></ref><ref id="B7-sensors-25-01209"><label>7.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Nguyen</surname><given-names>R.</given-names></name>
</person-group><article-title>Object Detection with YOLOv5 for Electric Utility Asset Inspection Using UAVs</article-title><source>Master&#x02019;s Thesis</source><publisher-name>California State University</publisher-name><publisher-loc>Long Beach, CA, USA</publisher-loc><year>2023</year></element-citation></ref><ref id="B8-sensors-25-01209"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>P&#x000e9;rez-Aguilar</surname><given-names>D.A.</given-names></name>
<name><surname>P&#x000e9;rez-Aguilar</surname><given-names>J.M.</given-names></name>
<name><surname>P&#x000e9;rez-Aguilar</surname><given-names>A.P.</given-names></name>
<name><surname>Risco-Ramos</surname><given-names>R.H.</given-names></name>
<name><surname>Malpica-Rodr&#x000ed;guez</surname><given-names>M.E.</given-names></name>
</person-group><article-title>Electric Substation Inspection: YOLOv5 in Hotspot Detection Through Thermal Imaging</article-title><source>Ingenius</source><year>2024</year><volume>31</volume><fpage>43</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.17163/ings.n31.2024.04</pub-id></element-citation></ref><ref id="B9-sensors-25-01209"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Ni</surname><given-names>M.</given-names></name>
<name><surname>Lu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Insulator Defect Detection for Power Grid Based on Light Correction Enhancement and YOLOv5 Model</article-title><source>Energy Rep.</source><year>2022</year><volume>8</volume><fpage>807</fpage><lpage>814</lpage><pub-id pub-id-type="doi">10.1016/j.egyr.2022.08.027</pub-id></element-citation></ref><ref id="B10-sensors-25-01209"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liang</surname><given-names>F.</given-names></name>
<name><surname>Zhao</surname><given-names>L.</given-names></name>
<name><surname>Ren</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>To</surname><given-names>S.</given-names></name>
<name><surname>Abbas</surname><given-names>Z.</given-names></name>
<name><surname>Islam</surname><given-names>M.S.</given-names></name>
</person-group><article-title>LAD-Net: A lightweight welding defect surface non-destructive detection algorithm based on the attention mechanism</article-title><source>Comput. Ind.</source><year>2024</year><volume>161</volume><fpage>104109</fpage><pub-id pub-id-type="doi">10.1016/j.compind.2024.104109</pub-id></element-citation></ref><ref id="B11-sensors-25-01209"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Hua</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
</person-group><article-title>Intelligent Identification Method of Insulation Pull Rod Defects Based on Intactness-Aware Mosaic Data Augmentation and Fusion of YOLOv5s</article-title><source>High Voltage</source><year>2024</year><volume>9</volume><fpage>1171</fpage><lpage>1182</lpage><pub-id pub-id-type="doi">10.1049/hve2.12447</pub-id></element-citation></ref><ref id="B12-sensors-25-01209"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nisa</surname><given-names>U.</given-names></name>
</person-group><article-title>Image Augmentation Approaches for Small and Tiny Object Detection in Aerial Images: A Review</article-title><source>Multimed. Tools Appl.</source><year>2024</year><pub-id pub-id-type="doi">10.1007/s11042-024-19768-7</pub-id></element-citation></ref><ref id="B13-sensors-25-01209"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Teterja</surname><given-names>D.</given-names></name>
<name><surname>Garcia Rodriguez</surname><given-names>J.</given-names></name>
<name><surname>Azorin-Lopez</surname><given-names>J.</given-names></name>
<name><surname>Sebastian-Gonzalez</surname><given-names>E.</given-names></name>
<name><surname>Nedic</surname><given-names>D.</given-names></name>
<name><surname>Lekovic</surname><given-names>D.</given-names></name>
<name><surname>Knezevic</surname><given-names>P.</given-names></name>
<name><surname>Drajic</surname><given-names>D.</given-names></name>
<name><surname>Vukobratovic</surname><given-names>D.</given-names></name>
</person-group><article-title>A Video Mosaicing-Based Sensing Method for Chicken Behavior Recognition on Edge Computing Devices</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>3409</elocation-id><pub-id pub-id-type="doi">10.3390/s24113409</pub-id><pub-id pub-id-type="pmid">38894200</pub-id>
</element-citation></ref><ref id="B14-sensors-25-01209"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Deng</surname><given-names>L.</given-names></name>
<name><surname>Yan</surname><given-names>R.</given-names></name>
</person-group><article-title>Surround Sensing Technique for Trucks Based on Multi-Features and Improved Yolov5 Algorithm</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>2112</elocation-id><pub-id pub-id-type="doi">10.3390/s24072112</pub-id><pub-id pub-id-type="pmid">38610324</pub-id>
</element-citation></ref><ref id="B15-sensors-25-01209"><label>15.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Dai</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>G.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>C.</given-names></name>
<name><surname>Tu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Electric Field Distribution of Micro Surface Scratch Defects in GIS Insulation Pull Rods</article-title><source>Proceedings of the 2024 IEEE International Conference on High Voltage Engineering and Applications (ICHVE), Berlin, Germany, 18&#x02013;22 August 2024</source><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><isbn>979-8-3503-7498-8</isbn></element-citation></ref><ref id="B16-sensors-25-01209"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>J.-H.</given-names></name>
<name><surname>Kim</surname><given-names>N.</given-names></name>
<name><surname>Park</surname><given-names>Y.W.</given-names></name>
<name><surname>Won</surname><given-names>C.S.</given-names></name>
</person-group><article-title>Object Detection and Classification Based on YOLO-V5 with Improved Maritime Dataset</article-title><source>J. Mar. Sci. Eng.</source><year>2022</year><volume>10</volume><elocation-id>377</elocation-id><pub-id pub-id-type="doi">10.3390/jmse10030377</pub-id></element-citation></ref><ref id="B17-sensors-25-01209"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hasib</surname><given-names>K.M.</given-names></name>
<name><surname>Iqbal</surname><given-names>M.S.</given-names></name>
<name><surname>Shah</surname><given-names>F.M.</given-names></name>
<name><surname>Mahmud</surname><given-names>J.A.</given-names></name>
<name><surname>Popel</surname><given-names>M.H.</given-names></name>
<name><surname>Showrov</surname><given-names>M.I.</given-names></name>
<name><surname>Ahmed</surname><given-names>S.</given-names></name>
<name><surname>Rahman</surname><given-names>O.</given-names></name>
</person-group><article-title>A Survey of Methods for Managing the Classification and Solution of Data Imbalance Problem</article-title><source>J. Comput. Sci.</source><year>2020</year><volume>16</volume><fpage>1546</fpage><lpage>1557</lpage><pub-id pub-id-type="doi">10.3844/jcssp.2020.1546.1557</pub-id></element-citation></ref><ref id="B18-sensors-25-01209"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Redmon</surname><given-names>J.</given-names></name>
<name><surname>Divvala</surname><given-names>S.</given-names></name>
<name><surname>Girshick</surname><given-names>R.</given-names></name>
<name><surname>Farhadi</surname><given-names>A.</given-names></name>
</person-group><article-title>You Only Look Once: Unified, Realtime Object Detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Las Vegas, NV, USA</publisher-loc><year>2016</year><fpage>779</fpage><lpage>788</lpage></element-citation></ref><ref id="B19-sensors-25-01209"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bochkovskiy</surname><given-names>A.</given-names></name>
<name><surname>Wang</surname><given-names>C.Y.</given-names></name>
<name><surname>Liao</surname><given-names>H.Y.M.</given-names></name>
</person-group><article-title>YOLOv4: Optimal Speed and Accuracy of Object Detection</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2004.10934</pub-id></element-citation></ref><ref id="B20-sensors-25-01209"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Hu</surname><given-names>C.</given-names></name>
<name><surname>Dai</surname><given-names>F.</given-names></name>
<name><surname>Xing</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>E.</given-names></name>
<name><surname>Du</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Guo</surname><given-names>W.</given-names></name>
</person-group><article-title>Study on the Detection of Defoliation Effect of an Improved YOLOv5x Cotton</article-title><source>Agriculture</source><year>2022</year><volume>12</volume><elocation-id>1583</elocation-id><pub-id pub-id-type="doi">10.3390/agriculture12101583</pub-id></element-citation></ref><ref id="B21-sensors-25-01209"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ioffe</surname><given-names>S.</given-names></name>
<name><surname>Szegedy</surname><given-names>C.</given-names></name>
</person-group><article-title>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1502.03167</pub-id></element-citation></ref><ref id="B22-sensors-25-01209"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2015</year><volume>37</volume><fpage>1904</fpage><lpage>1916</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2015.2389824</pub-id><pub-id pub-id-type="pmid">26353135</pub-id>
</element-citation></ref><ref id="B23-sensors-25-01209"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Qi</surname><given-names>L.</given-names></name>
<name><surname>Qin</surname><given-names>H.</given-names></name>
<name><surname>Shi</surname><given-names>J.</given-names></name>
<name><surname>Jia</surname><given-names>J.</given-names></name>
</person-group><article-title>Path Aggregation Network for Instance Segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#x02013;22 June 2018</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Salt Lake City, UT, USA</publisher-loc><year>2018</year></element-citation></ref><ref id="B24-sensors-25-01209"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Rezatofighi</surname><given-names>H.</given-names></name>
<name><surname>Tsoi</surname><given-names>N.</given-names></name>
<name><surname>Gwak</surname><given-names>J.Y.</given-names></name>
<name><surname>Sadeghian</surname><given-names>A.</given-names></name>
<name><surname>Reid</surname><given-names>I.</given-names></name>
<name><surname>Savarese</surname><given-names>S.</given-names></name>
</person-group><article-title>Generalized Intersection over Union: A Metric and a Loss for Bounding Box Regression</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#x02013;20 June 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Long Beach, CA, USA</publisher-loc><year>2019</year></element-citation></ref><ref id="B25-sensors-25-01209"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Redmon</surname><given-names>J.</given-names></name>
<name><surname>Farhadi</surname><given-names>A.</given-names></name>
</person-group><article-title>YOLOv3: An Incremental Improvement</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1804.02767</pub-id></element-citation></ref><ref id="B26-sensors-25-01209"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qiu</surname><given-names>S.</given-names></name>
<name><surname>Wen</surname><given-names>G.</given-names></name>
<name><surname>Deng</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Fan</surname><given-names>Y.</given-names></name>
</person-group><article-title>Accurate Non-Maximum Suppression for Object Detection in High-Resolution Remote Sensing Images</article-title><source>Remote Sens. Lett.</source><year>2018</year><volume>9</volume><fpage>237</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1080/2150704X.2017.1415473</pub-id></element-citation></ref><ref id="B27-sensors-25-01209"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rezazadeh</surname><given-names>N.</given-names></name>
<name><surname>Perfetto</surname><given-names>D.</given-names></name>
<name><surname>Polverino</surname><given-names>A.</given-names></name>
<name><surname>De Luca</surname><given-names>A.</given-names></name>
<name><surname>Lamanna</surname><given-names>G.</given-names></name>
</person-group><article-title>Guided wave-driven machine learning for damage classification with limited dataset in aluminum panel</article-title><source>Struct. Health Monit.</source><year>2024</year><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1177/14759217241268394</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01209-f001"><label>Figure 1</label><caption><p>Structural diagram of defect detection model of insulating tie rod based on YOLOv5s.</p></caption><graphic xlink:href="sensors-25-01209-g001" position="float"/></fig><fig position="float" id="sensors-25-01209-f002"><label>Figure 2</label><caption><p>Detection process of YOLOv5s network.</p></caption><graphic xlink:href="sensors-25-01209-g002" position="float"/></fig><fig position="float" id="sensors-25-01209-f003"><label>Figure 3</label><caption><p>Illustration of the Copy&#x02013;Paste Data Augmentation Method. The area marked with a red box indicates the defect target area.</p></caption><graphic xlink:href="sensors-25-01209-g003" position="float"/></fig><fig position="float" id="sensors-25-01209-f004"><label>Figure 4</label><caption><p>Correction to bounding boxes of hair-like impurities.</p></caption><graphic xlink:href="sensors-25-01209-g004" position="float"/></fig><fig position="float" id="sensors-25-01209-f005"><label>Figure 5</label><caption><p>Comparison of mAP@0.5:0.95 Performance and P/R/F1 score performance across different data augmentation methods. (<bold>a</bold>) Comparison of mAP@0.5:0.95 performance across different data augmentation methods; (<bold>b</bold>) Comparison of P/R/F1 score performance across different data augmentation methods.</p></caption><graphic xlink:href="sensors-25-01209-g005" position="float"/></fig><fig position="float" id="sensors-25-01209-f006"><label>Figure 6</label><caption><p>Comparison of detection results on real photos: (<bold>1a</bold>) Pull Rod 1, Dataset 1; (<bold>1b</bold>) Pull Rod 1, Dataset 5; (<bold>1c</bold>) Pull Rod 1, Dataset 4; (<bold>1d</bold>) Pull Rod 1, Dataset 7; (<bold>2a</bold>) Pull Rod, Dataset 1; (<bold>2b</bold>) Pull Rod 2, Dataset 5; (<bold>2c</bold>) Pull Rod 2, Dataset 4; (<bold>2d</bold>) Pull Rod 2, Dataset 7; (<bold>3a</bold>) Pull Rod 3, Dataset 1; (<bold>3b</bold>) Pull Rod 3, Dataset 5; (<bold>3c</bold>) Pull Rod 3, Dataset 4; (<bold>3d</bold>) Pull Rod 3, Dataset 7.</p></caption><graphic xlink:href="sensors-25-01209-g006" position="float"/></fig><table-wrap position="float" id="sensors-25-01209-t001"><object-id pub-id-type="pii">sensors-25-01209-t001_Table 1</object-id><label>Table 1</label><caption><p>The specific quantity and proportion of the five types of defects.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Defect Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Quantity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Proportion</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">White spot</td><td align="center" valign="middle" rowspan="1" colspan="1">483</td><td align="center" valign="middle" rowspan="1" colspan="1">58.2%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Impurity</td><td align="center" valign="middle" rowspan="1" colspan="1">138</td><td align="center" valign="middle" rowspan="1" colspan="1">16.6%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Bubble</td><td align="center" valign="middle" rowspan="1" colspan="1">40</td><td align="center" valign="middle" rowspan="1" colspan="1">4.8%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Crack</td><td align="center" valign="middle" rowspan="1" colspan="1">61</td><td align="center" valign="middle" rowspan="1" colspan="1">7.4%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Scratch</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">108</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.0%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01209-t002"><object-id pub-id-type="pii">sensors-25-01209-t002_Table 2</object-id><label>Table 2</label><caption><p>Composition of datasets and preprocessing methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Copy&#x02013;Paste</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hair-like Impurities Bounding Box Correction</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mosaic with Hair-like Impurities Bounding Box Correction</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mosaic without Hair-like Impurities Bounding Box Correction</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 1</td><td align="center" valign="middle" rowspan="1" colspan="1">Adopted/Not Adopted</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Quantity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>/</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8000</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 2</td><td align="center" valign="middle" rowspan="1" colspan="1">Adopted/Not Adopted</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Quantity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6000</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 3</td><td align="center" valign="middle" rowspan="1" colspan="1">Adopted/Not Adopted</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Quantity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6000</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 4</td><td align="center" valign="middle" rowspan="1" colspan="1">Adopted/Not Adopted</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Quantity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 5</td><td align="center" valign="middle" rowspan="1" colspan="1">Adopted/Not Adopted</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Quantity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4000</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 6</td><td align="center" valign="middle" rowspan="1" colspan="1">Adopted/Not Adopted</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Quantity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4000</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 7</td><td align="center" valign="middle" rowspan="1" colspan="1">Adopted/Not Adopted</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Quantity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01209-t003"><object-id pub-id-type="pii">sensors-25-01209-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of training results for different data augmentation methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">White Spot</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Impurity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Bubble</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Crack</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scratch</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">All</th></tr></thead><tbody><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 1</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of instances</td><td align="center" valign="middle" rowspan="1" colspan="1">38,855</td><td align="center" valign="middle" rowspan="1" colspan="1">11,234</td><td align="center" valign="middle" rowspan="1" colspan="1">3186</td><td align="center" valign="middle" rowspan="1" colspan="1">4769</td><td align="center" valign="middle" rowspan="1" colspan="1">8714</td><td align="center" valign="middle" rowspan="1" colspan="1">66,758</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.958</td><td align="center" valign="middle" rowspan="1" colspan="1">0.664</td><td align="center" valign="middle" rowspan="1" colspan="1">0.531</td><td align="center" valign="middle" rowspan="1" colspan="1">0.907</td><td align="center" valign="middle" rowspan="1" colspan="1">0.928</td><td align="center" valign="middle" rowspan="1" colspan="1">0.797</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5:0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.796</td><td align="center" valign="middle" rowspan="1" colspan="1">0.437</td><td align="center" valign="middle" rowspan="1" colspan="1">0.329</td><td align="center" valign="middle" rowspan="1" colspan="1">0.752</td><td align="center" valign="middle" rowspan="1" colspan="1">0.775</td><td align="center" valign="middle" rowspan="1" colspan="1">0.618</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">0.948</td><td align="center" valign="middle" rowspan="1" colspan="1">0.894</td><td align="center" valign="middle" rowspan="1" colspan="1">0.888</td><td align="center" valign="middle" rowspan="1" colspan="1">0.966</td><td align="center" valign="middle" rowspan="1" colspan="1">0.962</td><td align="center" valign="middle" rowspan="1" colspan="1">0.931</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">0.913</td><td align="center" valign="middle" rowspan="1" colspan="1">0.555</td><td align="center" valign="middle" rowspan="1" colspan="1">0.436</td><td align="center" valign="middle" rowspan="1" colspan="1">0.855</td><td align="center" valign="middle" rowspan="1" colspan="1">0.877</td><td align="center" valign="middle" rowspan="1" colspan="1">0.727</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.930</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.685</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.585</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.907</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.918</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.816</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 2</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of instances</td><td align="center" valign="middle" rowspan="1" colspan="1">30,021</td><td align="center" valign="middle" rowspan="1" colspan="1">10,402</td><td align="center" valign="middle" rowspan="1" colspan="1">4740</td><td align="center" valign="middle" rowspan="1" colspan="1">3520</td><td align="center" valign="middle" rowspan="1" colspan="1">6641</td><td align="center" valign="middle" rowspan="1" colspan="1">55,324</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.955</td><td align="center" valign="middle" rowspan="1" colspan="1">0.659</td><td align="center" valign="middle" rowspan="1" colspan="1">0.77</td><td align="center" valign="middle" rowspan="1" colspan="1">0.904</td><td align="center" valign="middle" rowspan="1" colspan="1">0.933</td><td align="center" valign="middle" rowspan="1" colspan="1">0.844</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5:0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.774</td><td align="center" valign="middle" rowspan="1" colspan="1">0.428</td><td align="center" valign="middle" rowspan="1" colspan="1">0.531</td><td align="center" valign="middle" rowspan="1" colspan="1">0.723</td><td align="center" valign="middle" rowspan="1" colspan="1">0.746</td><td align="center" valign="middle" rowspan="1" colspan="1">0.641</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">0.948</td><td align="center" valign="middle" rowspan="1" colspan="1">0.921</td><td align="center" valign="middle" rowspan="1" colspan="1">0.971</td><td align="center" valign="middle" rowspan="1" colspan="1">0.959</td><td align="center" valign="middle" rowspan="1" colspan="1">0.969</td><td align="center" valign="middle" rowspan="1" colspan="1">0.954</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">0.907</td><td align="center" valign="middle" rowspan="1" colspan="1">0.536</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7</td><td align="center" valign="middle" rowspan="1" colspan="1">0.841</td><td align="center" valign="middle" rowspan="1" colspan="1">0.873</td><td align="center" valign="middle" rowspan="1" colspan="1">0.771</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.927</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.678</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.814</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.896</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.918</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.853</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 5</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of instances</td><td align="center" valign="middle" rowspan="1" colspan="1">20,858</td><td align="center" valign="middle" rowspan="1" colspan="1">9666</td><td align="center" valign="middle" rowspan="1" colspan="1">6424</td><td align="center" valign="middle" rowspan="1" colspan="1">2340</td><td align="center" valign="middle" rowspan="1" colspan="1">4494</td><td align="center" valign="middle" rowspan="1" colspan="1">43,782</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.777</td><td align="center" valign="middle" rowspan="1" colspan="1">0.904</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91</td><td align="center" valign="middle" rowspan="1" colspan="1">0.943</td><td align="center" valign="middle" rowspan="1" colspan="1">0.897</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5:0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.738</td><td align="center" valign="middle" rowspan="1" colspan="1">0.593</td><td align="center" valign="middle" rowspan="1" colspan="1">0.766</td><td align="center" valign="middle" rowspan="1" colspan="1">0.702</td><td align="center" valign="middle" rowspan="1" colspan="1">0.737</td><td align="center" valign="middle" rowspan="1" colspan="1">0.707</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">0.954</td><td align="center" valign="middle" rowspan="1" colspan="1">0.924</td><td align="center" valign="middle" rowspan="1" colspan="1">0.981</td><td align="center" valign="middle" rowspan="1" colspan="1">0.956</td><td align="center" valign="middle" rowspan="1" colspan="1">0.966</td><td align="center" valign="middle" rowspan="1" colspan="1">0.956</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">0.903</td><td align="center" valign="middle" rowspan="1" colspan="1">0.695</td><td align="center" valign="middle" rowspan="1" colspan="1">0.853</td><td align="center" valign="middle" rowspan="1" colspan="1">0.853</td><td align="center" valign="middle" rowspan="1" colspan="1">0.874</td><td align="center" valign="middle" rowspan="1" colspan="1">0.836</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.928</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.793</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.913</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.902</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.918</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.892</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01209-t004"><object-id pub-id-type="pii">sensors-25-01209-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison of training results for datasets with and without hair-like impurity bounding box correction (Datasets 2, 3 and 4).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">White Spot</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Impurity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Bubble</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Crack</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scratch</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">All</th></tr></thead><tbody><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 2</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of instances</td><td align="center" valign="middle" rowspan="1" colspan="1">30,021</td><td align="center" valign="middle" rowspan="1" colspan="1">10,402</td><td align="center" valign="middle" rowspan="1" colspan="1">4740</td><td align="center" valign="middle" rowspan="1" colspan="1">3520</td><td align="center" valign="middle" rowspan="1" colspan="1">6641</td><td align="center" valign="middle" rowspan="1" colspan="1">55,324</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.955</td><td align="center" valign="middle" rowspan="1" colspan="1">0.659</td><td align="center" valign="middle" rowspan="1" colspan="1">0.77</td><td align="center" valign="middle" rowspan="1" colspan="1">0.904</td><td align="center" valign="middle" rowspan="1" colspan="1">0.933</td><td align="center" valign="middle" rowspan="1" colspan="1">0.844</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5:0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.774</td><td align="center" valign="middle" rowspan="1" colspan="1">0.428</td><td align="center" valign="middle" rowspan="1" colspan="1">0.531</td><td align="center" valign="middle" rowspan="1" colspan="1">0.723</td><td align="center" valign="middle" rowspan="1" colspan="1">0.746</td><td align="center" valign="middle" rowspan="1" colspan="1">0.641</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">0.948</td><td align="center" valign="middle" rowspan="1" colspan="1">0.921</td><td align="center" valign="middle" rowspan="1" colspan="1">0.971</td><td align="center" valign="middle" rowspan="1" colspan="1">0.959</td><td align="center" valign="middle" rowspan="1" colspan="1">0.969</td><td align="center" valign="middle" rowspan="1" colspan="1">0.954</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">0.907</td><td align="center" valign="middle" rowspan="1" colspan="1">0.536</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7</td><td align="center" valign="middle" rowspan="1" colspan="1">0.841</td><td align="center" valign="middle" rowspan="1" colspan="1">0.873</td><td align="center" valign="middle" rowspan="1" colspan="1">0.771</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.927</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.678</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.814</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.896</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.918</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.853</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 3</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of instances</td><td align="center" valign="middle" rowspan="1" colspan="1">29,684</td><td align="center" valign="middle" rowspan="1" colspan="1">11,004</td><td align="center" valign="middle" rowspan="1" colspan="1">4728</td><td align="center" valign="middle" rowspan="1" colspan="1">3520</td><td align="center" valign="middle" rowspan="1" colspan="1">6632</td><td align="center" valign="middle" rowspan="1" colspan="1">55,748</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.956</td><td align="center" valign="middle" rowspan="1" colspan="1">0.726</td><td align="center" valign="middle" rowspan="1" colspan="1">0.787</td><td align="center" valign="middle" rowspan="1" colspan="1">0.913</td><td align="center" valign="middle" rowspan="1" colspan="1">0.934</td><td align="center" valign="middle" rowspan="1" colspan="1">0.863</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5:0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.775</td><td align="center" valign="middle" rowspan="1" colspan="1">0.511</td><td align="center" valign="middle" rowspan="1" colspan="1">0.635</td><td align="center" valign="middle" rowspan="1" colspan="1">0.737</td><td align="center" valign="middle" rowspan="1" colspan="1">0.765</td><td align="center" valign="middle" rowspan="1" colspan="1">0.684</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">0.953</td><td align="center" valign="middle" rowspan="1" colspan="1">0.917</td><td align="center" valign="middle" rowspan="1" colspan="1">0.943</td><td align="center" valign="middle" rowspan="1" colspan="1">0.964</td><td align="center" valign="middle" rowspan="1" colspan="1">0.971</td><td align="center" valign="middle" rowspan="1" colspan="1">0.95</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">0.909</td><td align="center" valign="middle" rowspan="1" colspan="1">0.626</td><td align="center" valign="middle" rowspan="1" colspan="1">0.717</td><td align="center" valign="middle" rowspan="1" colspan="1">0.858</td><td align="center" valign="middle" rowspan="1" colspan="1">0.871</td><td align="center" valign="middle" rowspan="1" colspan="1">0.796</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.930</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.744</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.815</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.908</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.918</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.866</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 4</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of instances</td><td align="center" valign="middle" rowspan="1" colspan="1">29,389</td><td align="center" valign="middle" rowspan="1" colspan="1">12,245</td><td align="center" valign="middle" rowspan="1" colspan="1">4966</td><td align="center" valign="middle" rowspan="1" colspan="1">3591</td><td align="center" valign="middle" rowspan="1" colspan="1">6739</td><td align="center" valign="middle" rowspan="1" colspan="1">57,380</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.991</td><td align="center" valign="middle" rowspan="1" colspan="1">0.922</td><td align="center" valign="middle" rowspan="1" colspan="1">0.925</td><td align="center" valign="middle" rowspan="1" colspan="1">0.992</td><td align="center" valign="middle" rowspan="1" colspan="1">0.991</td><td align="center" valign="middle" rowspan="1" colspan="1">0.964</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5:0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.817</td><td align="center" valign="middle" rowspan="1" colspan="1">0.66</td><td align="center" valign="middle" rowspan="1" colspan="1">0.733</td><td align="center" valign="middle" rowspan="1" colspan="1">0.835</td><td align="center" valign="middle" rowspan="1" colspan="1">0.837</td><td align="center" valign="middle" rowspan="1" colspan="1">0.777</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">0.969</td><td align="center" valign="middle" rowspan="1" colspan="1">0.931</td><td align="center" valign="middle" rowspan="1" colspan="1">0.937</td><td align="center" valign="middle" rowspan="1" colspan="1">0.978</td><td align="center" valign="middle" rowspan="1" colspan="1">0.981</td><td align="center" valign="middle" rowspan="1" colspan="1">0.959</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">0.964</td><td align="center" valign="middle" rowspan="1" colspan="1">0.851</td><td align="center" valign="middle" rowspan="1" colspan="1">0.833</td><td align="center" valign="middle" rowspan="1" colspan="1">0.957</td><td align="center" valign="middle" rowspan="1" colspan="1">0.962</td><td align="center" valign="middle" rowspan="1" colspan="1">0.913</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.966</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.889</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.882</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.967</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.971</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.935</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01209-t005"><object-id pub-id-type="pii">sensors-25-01209-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of training results for datasets with and without hair-like impurity bounding box correction (Datasets 5, 6, and 7).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">White Spot</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Impurity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Bubble</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Crack</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scratch</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">All</th></tr></thead><tbody><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 5</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of instances</td><td align="center" valign="middle" rowspan="1" colspan="1">20,858</td><td align="center" valign="middle" rowspan="1" colspan="1">9666</td><td align="center" valign="middle" rowspan="1" colspan="1">6424</td><td align="center" valign="middle" rowspan="1" colspan="1">2340</td><td align="center" valign="middle" rowspan="1" colspan="1">4494</td><td align="center" valign="middle" rowspan="1" colspan="1">43,782</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.777</td><td align="center" valign="middle" rowspan="1" colspan="1">0.904</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91</td><td align="center" valign="middle" rowspan="1" colspan="1">0.943</td><td align="center" valign="middle" rowspan="1" colspan="1">0.897</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5:0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.738</td><td align="center" valign="middle" rowspan="1" colspan="1">0.593</td><td align="center" valign="middle" rowspan="1" colspan="1">0.766</td><td align="center" valign="middle" rowspan="1" colspan="1">0.702</td><td align="center" valign="middle" rowspan="1" colspan="1">0.737</td><td align="center" valign="middle" rowspan="1" colspan="1">0.707</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">0.954</td><td align="center" valign="middle" rowspan="1" colspan="1">0.924</td><td align="center" valign="middle" rowspan="1" colspan="1">0.981</td><td align="center" valign="middle" rowspan="1" colspan="1">0.956</td><td align="center" valign="middle" rowspan="1" colspan="1">0.966</td><td align="center" valign="middle" rowspan="1" colspan="1">0.956</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">0.903</td><td align="center" valign="middle" rowspan="1" colspan="1">0.695</td><td align="center" valign="middle" rowspan="1" colspan="1">0.853</td><td align="center" valign="middle" rowspan="1" colspan="1">0.853</td><td align="center" valign="middle" rowspan="1" colspan="1">0.874</td><td align="center" valign="middle" rowspan="1" colspan="1">0.836</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.928</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.793</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.913</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.902</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.918</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.892</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 6</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of instances</td><td align="center" valign="middle" rowspan="1" colspan="1">20,661</td><td align="center" valign="middle" rowspan="1" colspan="1">10,830</td><td align="center" valign="middle" rowspan="1" colspan="1">6400</td><td align="center" valign="middle" rowspan="1" colspan="1">2340</td><td align="center" valign="middle" rowspan="1" colspan="1">4476</td><td align="center" valign="middle" rowspan="1" colspan="1">44,707</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.805</td><td align="center" valign="middle" rowspan="1" colspan="1">0.899</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91</td><td align="center" valign="middle" rowspan="1" colspan="1">0.939</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5:0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.744</td><td align="center" valign="middle" rowspan="1" colspan="1">0.626</td><td align="center" valign="middle" rowspan="1" colspan="1">0.771</td><td align="center" valign="middle" rowspan="1" colspan="1">0.703</td><td align="center" valign="middle" rowspan="1" colspan="1">0.737</td><td align="center" valign="middle" rowspan="1" colspan="1">0.716</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">0.967</td><td align="center" valign="middle" rowspan="1" colspan="1">0.941</td><td align="center" valign="middle" rowspan="1" colspan="1">0.987</td><td align="center" valign="middle" rowspan="1" colspan="1">0.97</td><td align="center" valign="middle" rowspan="1" colspan="1">0.979</td><td align="center" valign="middle" rowspan="1" colspan="1">0.969</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">0.891</td><td align="center" valign="middle" rowspan="1" colspan="1">0.715</td><td align="center" valign="middle" rowspan="1" colspan="1">0.849</td><td align="center" valign="middle" rowspan="1" colspan="1">0.833</td><td align="center" valign="middle" rowspan="1" colspan="1">0.859</td><td align="center" valign="middle" rowspan="1" colspan="1">0.83</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.927</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.813</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.913</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.896</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.915</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.894</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Dataset 7</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of instances</td><td align="center" valign="middle" rowspan="1" colspan="1">20,583</td><td align="center" valign="middle" rowspan="1" colspan="1">11,659</td><td align="center" valign="middle" rowspan="1" colspan="1">6565</td><td align="center" valign="middle" rowspan="1" colspan="1">2309</td><td align="center" valign="middle" rowspan="1" colspan="1">4522</td><td align="center" valign="middle" rowspan="1" colspan="1">45,638</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" rowspan="1" colspan="1">0.946</td><td align="center" valign="middle" rowspan="1" colspan="1">0.969</td><td align="center" valign="middle" rowspan="1" colspan="1">0.991</td><td align="center" valign="middle" rowspan="1" colspan="1">0.985</td><td align="center" valign="middle" rowspan="1" colspan="1">0.976</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP@0.5:0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.798</td><td align="center" valign="middle" rowspan="1" colspan="1">0.731</td><td align="center" valign="middle" rowspan="1" colspan="1">0.816</td><td align="center" valign="middle" rowspan="1" colspan="1">0.799</td><td align="center" valign="middle" rowspan="1" colspan="1">0.804</td><td align="center" valign="middle" rowspan="1" colspan="1">0.789</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">0.971</td><td align="center" valign="middle" rowspan="1" colspan="1">0.934</td><td align="center" valign="middle" rowspan="1" colspan="1">0.981</td><td align="center" valign="middle" rowspan="1" colspan="1">0.976</td><td align="center" valign="middle" rowspan="1" colspan="1">0.981</td><td align="center" valign="middle" rowspan="1" colspan="1">0.969</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">0.958</td><td align="center" valign="middle" rowspan="1" colspan="1">0.893</td><td align="center" valign="middle" rowspan="1" colspan="1">0.919</td><td align="center" valign="middle" rowspan="1" colspan="1">0.946</td><td align="center" valign="middle" rowspan="1" colspan="1">0.948</td><td align="center" valign="middle" rowspan="1" colspan="1">0.933</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.964</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.913</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.949</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.961</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.964</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.951</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01209-t006"><object-id pub-id-type="pii">sensors-25-01209-t006_Table 6</object-id><label>Table 6</label><caption><p>The inference time and GPU memory consumption of each dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference Time (ms)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GPU Memory Consumption</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dataset 1</td><td align="center" valign="middle" rowspan="1" colspan="1">22.9</td><td align="center" valign="middle" rowspan="1" colspan="1">7.35 G</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dataset 2</td><td align="center" valign="middle" rowspan="1" colspan="1">23.0</td><td align="center" valign="middle" rowspan="1" colspan="1">7.34 G</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dataset 3</td><td align="center" valign="middle" rowspan="1" colspan="1">22.5</td><td align="center" valign="middle" rowspan="1" colspan="1">7.35 G</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dataset 4</td><td align="center" valign="middle" rowspan="1" colspan="1">25.1</td><td align="center" valign="middle" rowspan="1" colspan="1">7.35 G</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dataset 5</td><td align="center" valign="middle" rowspan="1" colspan="1">23.6</td><td align="center" valign="middle" rowspan="1" colspan="1">7.35 G</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dataset 6</td><td align="center" valign="middle" rowspan="1" colspan="1">23.4</td><td align="center" valign="middle" rowspan="1" colspan="1">7.35 G</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dataset 7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.34 G</td></tr></tbody></table></table-wrap></floats-group></article>