<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Bioeng Biotechnol</journal-id><journal-id journal-id-type="iso-abbrev">Front Bioeng Biotechnol</journal-id><journal-id journal-id-type="publisher-id">Front. Bioeng. Biotechnol.</journal-id><journal-title-group><journal-title>Frontiers in Bioengineering and Biotechnology</journal-title></journal-title-group><issn pub-type="epub">2296-4185</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40013307</article-id><article-id pub-id-type="pmc">PMC11861201</article-id><article-id pub-id-type="publisher-id">1490919</article-id><article-id pub-id-type="doi">10.3389/fbioe.2025.1490919</article-id><article-categories><subj-group subj-group-type="heading"><subject>Bioengineering and Biotechnology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Facial expression recognition through muscle synergies and estimation of facial keypoint displacements through a skin-musculoskeletal model using facial sEMG signals</article-title><alt-title alt-title-type="left-running-head">Shu et al.</alt-title><alt-title alt-title-type="right-running-head">
<ext-link xlink:href="https://doi.org/10.3389/fbioe.2025.1490919" ext-link-type="uri">10.3389/fbioe.2025.1490919</ext-link>
</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Shu</surname><given-names>Lun</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/2769453/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/><role content-type="https://credit.niso.org/contributor-roles/Writing - review &#x00026; editing/"/></contrib><contrib contrib-type="author"><name><surname>Barradas</surname><given-names>Victor R.</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/1492799/overview"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/Writing - review &#x00026; editing/"/></contrib><contrib contrib-type="author"><name><surname>Qin</surname><given-names>Zixuan</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/1263912/overview"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/Writing - review &#x00026; editing/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Koike</surname><given-names>Yasuharu</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><xref rid="c001" ref-type="corresp">*</xref><uri xlink:href="https://loop.frontiersin.org/people/686183/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/funding-acquisition/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/Writing - review &#x00026; editing/"/></contrib></contrib-group><aff id="aff1">
<sup>1</sup>
<institution>Department of Information and Communications Engineering</institution>, <institution>Institute of Science Tokyo</institution>, <addr-line>Yokohama</addr-line>, <country>Japan</country>
</aff><aff id="aff2">
<sup>2</sup>
<institution>Institute of Integrated Research, Institute of Science Tokyo</institution>, <addr-line>Yokohama</addr-line>, <country>Japan</country>
</aff><author-notes><fn fn-type="edited-by"><p>
<bold>Edited by:</bold>
<ext-link xlink:href="https://loop.frontiersin.org/people/471376/overview" ext-link-type="uri">Wei Meng</ext-link>, Wuhan University of Technology, China</p></fn><fn fn-type="edited-by"><p>
<bold>Reviewed by:</bold>
<ext-link xlink:href="https://loop.frontiersin.org/people/2266681/overview" ext-link-type="uri">Riccardo Collu</ext-link>, University of Cagliari, Italy</p><p>
<ext-link xlink:href="https://loop.frontiersin.org/people/2647174/overview" ext-link-type="uri">Sanjeev Gupta</ext-link>, Manav Rachna International Institute of Research and Studies (MRIIRS), India</p><p>
<ext-link xlink:href="https://loop.frontiersin.org/people/2839346/overview" ext-link-type="uri">Shoukun Wang</ext-link>, Beijing Institute of Technology, China</p></fn><corresp id="c001">*Correspondence: Yasuharu Koike, <email>koike@pi.titech.ac.jp</email>
</corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>13</volume><elocation-id>1490919</elocation-id><history><date date-type="received"><day>04</day><month>9</month><year>2024</year></date><date date-type="accepted"><day>20</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025 Shu, Barradas, Qin and Koike.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Shu, Barradas, Qin and Koike</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>The development of facial expression recognition (FER) and facial expression generation (FEG) systems is essential to enhance human-robot interactions (HRI). The facial action coding system is widely used in FER and FEG tasks, as it offers a framework to relate the action of facial muscles and the resulting facial motions to the execution of facial expressions. However, most FER and FEG studies are based on measuring and analyzing facial motions, leaving the facial muscle component relatively unexplored. This study introduces a novel framework using surface electromyography (sEMG) signals from facial muscles to recognize facial expressions and estimate the displacement of facial keypoints during the execution of the expressions. For the facial expression recognition task, we studied the coordination patterns of seven muscles, expressed as three muscle synergies extracted through non-negative matrix factorization, during the execution of six basic facial expressions. Muscle synergies are groups of muscles that show coordinated patterns of activity, as measured by their sEMG signals, and are hypothesized to form the building blocks of human motor control. We then trained two classifiers for the facial expressions based on extracted features from the sEMG signals and the synergy activation coefficients of the extracted muscle synergies, respectively. The accuracy of both classifiers outperformed other systems that use sEMG to classify facial expressions, although the synergy-based classifier performed marginally worse than the sEMG-based one (classification accuracy: synergy-based 97.4%, sEMG-based 99.2%). However, the extracted muscle synergies revealed common coordination patterns between different facial expressions, allowing a low-dimensional quantitative visualization of the muscle control strategies involved in human facial expression generation. We also developed a skin-musculoskeletal model enhanced by linear regression (SMSM-LRM) to estimate the displacement of facial keypoints during the execution of a facial expression based on sEMG signals. Our proposed approach achieved a relatively high fidelity in estimating these displacements (NRMSE 0.067). We propose that the identified muscle synergies could be used in combination with the SMSM-LRM model to generate motor commands and trajectories for desired facial displacements, potentially enabling the generation of more natural facial expressions in social robotics and virtual reality.</p></abstract><kwd-group><kwd>facial expression recognition</kwd><kwd>sEMG</kwd><kwd>muscle synergy</kwd><kwd>musculoskeletal model</kwd><kwd>facial keypoints estimation</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that financial support was received for the research, authorship, and/or publication of this article. This work was supported by the Japan Society for the Promotion of Science (JSPS) Grant KAKENHI 19H05728 and 23K28123 (to YK).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Biomechanics</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><title>1 Introduction</title><p>In human-human interactions, facial expressions are often more effective than verbal methods and body language in conveying affective information (<xref rid="B57" ref-type="bibr">Mehrabian and Ferris, 1967</xref>; <xref rid="B77" ref-type="bibr">Scherer and Wallbott, 1994</xref>), which is essential in social interactions (<xref rid="B8" ref-type="bibr">Blair, 2003</xref>; <xref rid="B24" ref-type="bibr">Ekman, 1984</xref>; <xref rid="B51" ref-type="bibr">Levenson, 1994</xref>; <xref rid="B18" ref-type="bibr">Darwin, 1872</xref>; <xref rid="B25" ref-type="bibr">Ekman and Friesen, 1969</xref>). The importance of facial expressions has also been shown in human-robot interactions (HRI) (<xref rid="B70" ref-type="bibr">Rawal and Stock-Homburg, 2022</xref>; <xref rid="B81" ref-type="bibr">Stock-Homburg, 2022</xref>; <xref rid="B91" ref-type="bibr">Yang et al., 2008</xref>; <xref rid="B5" ref-type="bibr">Bennett and &#x00160;abanovi&#x00107;, 2014</xref>; <xref rid="B29" ref-type="bibr">Fu et al., 2023</xref>; <xref rid="B76" ref-type="bibr">Saunderson and Nejat, 2019</xref>), which are poised to become widespread in service industries (<xref rid="B32" ref-type="bibr">Gonzalez-Aguirre et al., 2021</xref>; <xref rid="B64" ref-type="bibr">Paluch et al., 2020</xref>), educational areas (<xref rid="B4" ref-type="bibr">Belpaeme et al., 2018</xref>; <xref rid="B82" ref-type="bibr">Takayuki Kanda et al., 2004</xref>), and healthcare domains (<xref rid="B47" ref-type="bibr">Kyrarini et al., 2021</xref>; <xref rid="B41" ref-type="bibr">Johanson et al., 2021</xref>). In such social settings, the use of facial expressions in robots can influence the users&#x02019; cognitive framing towards the robots, providing perceptions of intelligence, friendliness, and likeability (<xref rid="B42" ref-type="bibr">Johanson et al., 2020</xref>; <xref rid="B31" ref-type="bibr">Gonsior et al., 2011</xref>; <xref rid="B10" ref-type="bibr">Cameron et al., 2018</xref>). Expressive robots can also promote user engagement (<xref rid="B30" ref-type="bibr">Ghorbandaei Pour et al., 2018</xref>; <xref rid="B84" ref-type="bibr">Tapus et al., 2012</xref>) and enhance collaboration (<xref rid="B60" ref-type="bibr">Moshkina, 2021</xref>; <xref rid="B29" ref-type="bibr">Fu et al., 2023</xref>), improving performance in a given task (<xref rid="B71" ref-type="bibr">Reyes et al., 2019</xref>; <xref rid="B16" ref-type="bibr">Cohen et al., 2017</xref>). Therefore, the development of robots with the ability to recognize and generate rich facial expressions could facilitate the application of social robots in daily life.</p><p>Due to the visual nature of facial expressions, most facial expression recognition (FER) systems use computer vision to detect faces and determine the presence of facial expressions (<xref rid="B53" ref-type="bibr">Liu et al., 2014</xref>; <xref rid="B96" ref-type="bibr">Zhang et al., 2018</xref>; <xref rid="B92" ref-type="bibr">Yu et al., 2018</xref>; <xref rid="B9" ref-type="bibr">Boughida et al., 2022</xref>). These systems have achieved high accuracy (<xref rid="B92" ref-type="bibr">Yu et al., 2018</xref>; <xref rid="B9" ref-type="bibr">Boughida et al., 2022</xref>) in the recognition of predefined expressions, but suffer from robustness issues stemming from the sensitivity of vision systems to environmental variables such as illumination, occlusion and head pose (<xref rid="B96" ref-type="bibr">Zhang et al., 2018</xref>; <xref rid="B52" ref-type="bibr">Li and Deng, 2020</xref>). On the other hand, robots (<xref rid="B85" ref-type="bibr">Toan et al., 2022</xref>; <xref rid="B7" ref-type="bibr">Berns and Hirth, 2006</xref>; <xref rid="B27" ref-type="bibr">Faraj et al., 2021</xref>; <xref rid="B66" ref-type="bibr">Pumarola et al., 2020</xref>; <xref rid="B2" ref-type="bibr">Asheber et al., 2016</xref>) and animation software (<xref rid="B93" ref-type="bibr">Zhang et al., 2022a</xref>; <xref rid="B35" ref-type="bibr">Guo et al., 2019</xref>; <xref rid="B33" ref-type="bibr">Gonzalez-Franco et al., 2020</xref>) have been developed to generate facial expressions for HRI systems, although it is unclear what features of the generated expressions are important for successful HRI, and the goodness of the expressions has not been evaluated systematically (<xref rid="B27" ref-type="bibr">Faraj et al., 2021</xref>). Research on the recognition and generation of facial expressions in robotics has heavily relied on the facial action coding system (FACS), which is a framework that catalogs facial expressions as combinations of action units (AUs), which relate facial movements to the actions of individual muscles or groups of muscles (<xref rid="B36" ref-type="bibr">Hager et al., 2002</xref>). However, FACS only provides a qualitative relationship between facial motions and muscle activations. Therefore, <italic>ad hoc</italic> methods based on empirical measurements and calculations are required to define the precise temporal and spatial characteristics of facial points (<xref rid="B83" ref-type="bibr">Tang et al., 2023</xref>).</p><p>The measurement of surface electromyography (sEMG) of facial muscles offers an alternative to understand the temporal and spatial aspects of facial expressions in detail, as it provides information about the activation of muscles. Some studies have explored the use of sEMG signals for FER, although the resulting performance is not yet comparable to the performance of established computer vision methods due to limitations in the collection of facial sEMG (<xref rid="B55" ref-type="bibr">Lou et al., 2020</xref>; <xref rid="B37" ref-type="bibr">Hamedi et al., 2016</xref>; <xref rid="B11" ref-type="bibr">Cha et al., 2020</xref>; <xref rid="B45" ref-type="bibr">Kehri et al., 2019</xref>; <xref rid="B59" ref-type="bibr">Mithbavkar and Shah, 2021</xref>; <xref rid="B13" ref-type="bibr">Chen et al., 2015</xref>; <xref rid="B34" ref-type="bibr">Gruebler and Suzuki, 2010</xref>; <xref rid="B23" ref-type="bibr">Egger et al., 2019</xref>). Furthermore, given that facial expressions result from the coordinated action of different muscles (AUs as described by FACS), muscle synergy analysis offers tools to analyze these coordinated actions when measuring facial sEMG. A muscle synergy is a group of muscles that shows a pattern of coordinated activation during the execution of a motor task. Similar to the concept of AUs in the facial expression domain, muscle synergies are hypothesized to serve as the building blocks of motor behaviors (<xref rid="B21" ref-type="bibr">d&#x02019;Avella et al., 2003</xref>). In practice, muscle synergies are identified through dimensionality reduction methods applied on the sEMG data, with non-negative matrix factorization (NMF) being favored for its interpretability of the identified synergies, as it organizes synergies into a spatial component containing the contribution of individual muscles, and a temporal component that dictates the non-negative activation coefficients of each synergy during the task (<xref rid="B68" ref-type="bibr">Rabbi et al., 2020</xref>; <xref rid="B48" ref-type="bibr">Lambert-Shirzad and Van der Loos, 2017</xref>). Surprisingly, there is little research using muscle synergies for FER related tasks (<xref rid="B65" ref-type="bibr">Perusquia-Hernandez et al., 2020</xref>; <xref rid="B22" ref-type="bibr">Delis et al., 2016</xref>; <xref rid="B73" ref-type="bibr">Root and Stephens, 2003</xref>; <xref rid="B15" ref-type="bibr">Chiovetto et al., 2018</xref>). Here, we propose using muscle synergy analysis for the FER task by extracting muscle synergies from sEMG and using features of the synergy activation coefficients to classify different facial expressions.</p><p>sEMG can also be applied in facial expression generation tasks, as granular spatial and temporal information about the action of facial muscles can inform the design of robotic systems capable of generating facial expressions. In particular, sEMG can be exploited to build musculoskeletal models (MSM) that estimate a physical output such as muscle force or joint torque based on sEMG measurements. Such models have been leveraged to build controllers for robotic upper and lower limbs (<xref rid="B94" ref-type="bibr">Zhang et al., 2022b</xref>; <xref rid="B6" ref-type="bibr">Bennett et al., 2022</xref>; <xref rid="B54" ref-type="bibr">Lloyd and Besier, 2003</xref>; <xref rid="B69" ref-type="bibr">Rajagopal et al., 2016</xref>; <xref rid="B67" ref-type="bibr">Qin et al., 2022</xref>; <xref rid="B58" ref-type="bibr">Mithbavkar and Shah, 2019</xref>). However, in the problem of the generation of facial expressions, the output of interest is the deformation of skin caused by muscle action. The field of computer graphics has excelled in modeling facial skin deformations to generate 3D models of facial expressions (<xref rid="B46" ref-type="bibr">Kim et al., 2020</xref>; <xref rid="B79" ref-type="bibr">Sifakis et al., 2005</xref>; <xref rid="B95" ref-type="bibr">Zhang et al., 2001</xref>; <xref rid="B50" ref-type="bibr">Lee et al., 1995</xref>; <xref rid="B43" ref-type="bibr">K&#x000e4;hler et al., 2001</xref>). However, these advanced CG models do not address the relationship between sEMG signals and facial deformations, which are necessary to inform the design of expressive robots. Other studies have used convolutional neural networks to predict the position of facial landmarks from sEMG signals to generate facial expressions in a virtual reality (VR) environment, but these methods treat the relationship between muscle activity and facial motions as a black box, forgoing the functional relationship between them (<xref rid="B89" ref-type="bibr">Wu et al., 2021</xref>). Here, we combine techniques developed by the robotics and the computer graphics fields by modeling both muscles (<xref rid="B78" ref-type="bibr">Shin et al., 2009</xref>; <xref rid="B39" ref-type="bibr">He et al., 2022</xref>) and skin (<xref rid="B95" ref-type="bibr">Zhang et al., 2001</xref>) as coupled non-linear springs, allowing us to estimate the displacement of facial points based on muscle activations. Additionally, we found that combining the skin-musculoskeletal model with a linear regression model enhanced the estimation performance when compared to the performance of both models in isolation.</p><p>The paper is organized as follows: in <xref rid="s2" ref-type="sec">Section 2</xref>, we present the Materials and Methods for developing the FER systems and the facial keypoint displacement estimation system. <xref rid="s2-1" ref-type="sec">Sections 2.1</xref>&#x02013;<xref rid="s2-4" ref-type="sec">2.4</xref> describe the experimental protocol we used to collect the sEMG signals in a facial expression task. <xref rid="s2-5" ref-type="sec">Section 2.5</xref> provides details on the development of two FER systems based on individual muscle and muscle synergies, respectively. <xref rid="s2-6" ref-type="sec">Section 2.6</xref> describes the development of the skin-musculoskeletal model (SMSM) and the SMSM enhanced with a linear regression model (SMSM-LRM) for estimating the displacement of facial keypoints. <xref rid="s3" ref-type="sec">Section 3</xref> outlines the results of the proposed approaches in the FER and facial keypoint displacement estimation. In <xref rid="s4" ref-type="sec">Section 4</xref>, we discuss the results of the muscle synergy analysis, FER analysis, and the estimation of displacement of facial points. Finally, <xref rid="s5" ref-type="sec">Section 5</xref> provides conclusions and prospects for future work.</p></sec><sec sec-type="materials|methods" id="s2"><title>2 Materials and Methods</title><p>We propose a methodology to use facial sEMG signals to recognize facial expressions and estimate the displacement of facial keypoints (<xref rid="F1" ref-type="fig">Figure 1</xref>). We extracted muscle synergies from processed sEMG data of facial muscles, used their synergy activation coefficients for feature extraction, and used a random forest classifier for the classification of facial expressions. For comparison, we also built an RF classifier based on the same features, but extracted from processed sEMG signals. Then, we used the processed sEMG data and the displacement of facial keypoints measured from video data to develop and compare the performance of three models (SMSM, LRM, and SMSM-LRM) in estimating the displacements based on sEMG.</p><fig position="float" id="F1"><label>FIGURE 1</label><caption><p>Methodology for recognition of facial expressions and estimation of displacement of facial points. Facial expression recognition: sEMG signals are measured through electrodes and bandpass filtered, rectified, normalized, and low-pass filtered. Subsequently, muscle synergies are extracted using non-negative matrix factorization (NMF), with the extracted synergy activation coefficients used for feature extraction. These features are then employed for facial expression recognition using a random forest classifier. Estimation of facial points displacement: displacements of facial points are measured using DeepLabCut, and these measurements, along with downsampled sEMG signals, are used to train the SMSM (Skin-Muscle-Skeletal Model), LRM (Linear Regression Model), and SMSM-LRM models.</p></caption><graphic xlink:href="fbioe-13-1490919-g001" position="float"/></fig><sec id="s2-1"><title>2.1 Participants</title><p>Ten participants (5 men), aged from 23 to 29&#x000a0;years old (mean age: 25.7&#x000a0;years (SD 2.7)), participated in the study after providing written informed consent. All the research procedures complied with the ethics committee of the Tokyo Institute of Technology and were conducted in accordance with the Declaration of Helsinki.</p></sec><sec id="s2-2"><title>2.2 Experimental setup</title><p>Participants sat on a chair in front of a laptop computer and faced a webcam (resolution: <inline-formula id="inf1">
<mml:math id="m1" overflow="scroll"><mml:mrow><mml:mn>1280</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>720</mml:mn></mml:mrow></mml:math>
</inline-formula>, frame rate: 25&#x000a0;Hz). Participants were asked to face the webcam for the length of the experiment. We asked participants to sit upright and lean on the back of the chair during all experimental trials to keep a fixed distance from the webcam. During the experimental sessions, illustrations of target facial expressions were displayed on the laptop screen and participants were asked to replicate the target expression.</p><p>We recorded sEMG signals from seven facial muscle regions associated with the target facial expressions: inner frontalis region (IF), outer frontalis region (OF), corrugator supercilii region (CS), levator labii superioris alaeque nasi region (LLSAN), zygomaticus major region (ZM), depressor anguli oris region (DAO), and mentalis (Me). Hereafter, muscle names refer to their respective regions. Active bipolar electrodes were used to record EMG activity wirelessly (Trigno mini sensors, Trigno wireless system, Delsys) at a sampling rate of 1,024&#x000a0;Hz. <xref rid="F2" ref-type="fig">Figure 2</xref> shows the general placement of the electrodes. The placement process was meticulously standardized to ensure consistent and accurate signal collection across all participants. While healthcare professionals were not involved in this process, the accuracy of muscle identification and electrode placement was ensured through the use of established handbooks, anatomical atlases, and prior research literature. Additionally, the operator underwent extensive training under the guidance of experienced faculty members specializing in biotechnology and bio-interfaces, including theoretical sessions using established handbooks, anatomical atlases, and prior research literature. Initially, we identified the general areas, ranges, and actions of the targeted facial muscles (<xref rid="B17" ref-type="bibr">Cohn and Ekman, 2005</xref>; <xref rid="B61" ref-type="bibr">Mueller et al., 2022</xref>; <xref rid="B28" ref-type="bibr">Fridlund and Cacioppo, 1986</xref>; <xref rid="B36" ref-type="bibr">Hager et al., 2002</xref>; <xref rid="B44" ref-type="bibr">Kawai and Harajima, 2005</xref>). Especially, there is an atlas of EMG electrode placements for recording over major facial muscles (<xref rid="B28" ref-type="bibr">Fridlund and Cacioppo, 1986</xref>). Then, participants were asked to perform specific facial actions that engaged the targeted muscles while we manually palpated the expected location of each muscle, allowing us to pinpoint suitable sites for electrode placement. Before fixing the electrode placement, we temporarily placed electrodes at the identified sites and asked participants to perform the facial actions again. This step allowed us to monitor the EMG signals during each action to ensure that the muscle contraction produces measured EMG signals that match expectations. The positions were marked, compared with the atlas of EMG electrode placements (<xref rid="B28" ref-type="bibr">Fridlund and Cacioppo, 1986</xref>), and electrodes were then securely attached. This standardized procedure ensured that the data collected were both reliable and consistent. Because of electrode size and individual differences in participants&#x02019; facial structure, the electrodes were distributed differently for each participant. That is, in general, the electrodes were placed on the target muscle on different sides of the face, except for the muscle pairs IF and OF, and ZM and DAO, which were always placed on the same side of the face. The participants&#x02019; skin was cleaned before electrode placement to optimize the interface between electrodes and the skin.</p><fig position="float" id="F2"><label>FIGURE 2</label><caption><p>Experimental setup. <bold>(A)</bold> sEMG electrode placement. We used seven facial muscles: &#x02460; inner part of frontalis (inner frontalis, IF), &#x02461; outer part of frontalis (outer frontalis, OF), &#x02462; corrugator supercilii (CS), &#x02463; levator labii superioris alaeque nasi (LLSAN), &#x02464; zygomaticus major (ZM), &#x02465; depressor anguli oris (DAO), and &#x02466; mentalis (Me). <bold>(B)</bold> Experimental setup. <bold>(C)</bold> Facial keypoints. We tracked the displacement of five facial keypoints during the experiment: outer eyebrow (red), inner eyebrow (orange), superior end of the nasolabial fold (green), mouth corner (dark blue), and chin (blue).</p></caption><graphic xlink:href="fbioe-13-1490919-g002" position="float"/></fig><p>The sEMG signal data was transferred to a laptop computer (Dell Precision 7510). Video of the participants&#x02019; facial expressions during the experiment was recorded using the laptop&#x02019;s webcam to track the displacement of facial keypoints. We attached stickers on five facial keypoints (outer eyebrow, inner eyebrow, superior end of the nasolabial fold, mouth corner, chin) to use computer vision-based object-tracking software (DeepLabCut) to track their positions. <xref rid="F2" ref-type="fig">Figure 2C</xref> shows the general position of the facial keypoints. These specific locations were standardized across all participants using a combination of the FACS (<xref rid="B36" ref-type="bibr">Hager et al., 2002</xref>), previous research (<xref rid="B61" ref-type="bibr">Mueller et al., 2022</xref>; <xref rid="B28" ref-type="bibr">Fridlund and Cacioppo, 1986</xref>; <xref rid="B44" ref-type="bibr">Kawai and Harajima, 2005</xref>) and established facial landmarks detection maps (<xref rid="B97" ref-type="bibr">K&#x000f6;stinger et al., 2011</xref>; <xref rid="B74" ref-type="bibr">Sagonas et al., 2016</xref>). The stickers were attached according to the electrode placement, such that the distribution of stickers across participants also varied (except for the outer and inner eyebrow points which were always attached to the same side of the face). The Lab Streaming Layer software (<xref rid="B80" ref-type="bibr">Stenner et al., 2023</xref>) was used to synchronize the sEMG and video data. The experimental routines were created using MATLAB (MathWorks, United States).</p></sec><sec id="s2-3"><title>2.3 Experimental protocol</title><p>Before the main experiment, participants underwent comprehensive training to accurately perform six different facial expressions derived from the FACS system (anger, disgust, fear, happiness, sadness, surprise) as depicted in <xref rid="F3" ref-type="fig">Figure 3B</xref> (<xref rid="B36" ref-type="bibr">Hager et al., 2002</xref>). These basic six facial expressions are universally common in different cultures (<xref rid="B63" ref-type="bibr">Ortony and Turner, 1990</xref>), although more recent research opposes this view (<xref rid="B40" ref-type="bibr">Jack et al., 2012</xref>). Nonetheless, these expressions have been extensively analyzed in both academic (<xref rid="B88" ref-type="bibr">Wolf, 2015</xref>; <xref rid="B98" ref-type="bibr">K&#x000fc;ntzler et al., 2021</xref>) and applied settings (<xref rid="B70" ref-type="bibr">Rawal and Stock-Homburg, 2022</xref>; <xref rid="B83" ref-type="bibr">Tang et al., 2023</xref>) due to their role in human emotional communication. Therefore, our study uses this background to facilitate the comparison of results with past and future research. The training session consisted of the introduction stage and guided practice. In the introduction stage, we showed participants illustrations (<xref rid="F3" ref-type="fig">Figure 3B</xref>) of each target facial expression, alongside verbal explanations on how to move different parts of the face to express the target facial expression. In the guided practice, participants were guided through each expression, receiving verbal cues to adjust their facial movements on how to correct the facial movement. During the training, we also monitored the EMG signals to ensure that only the muscles involved in the desired expression were activated. If we detected erroneous facial actions or EMG signals, we provided verbal cues to the participants to correct the action. It is a combination of subjective and more or less objective procedures. We evaluated that the expected Action Units (AUs) are moving, and that the expected EMG signals for the AUs are activated (without other AUs activating significantly).</p><fig position="float" id="F3"><label>FIGURE 3</label><caption><p>Structure of experimental tasks. <bold>(A)</bold> MVC task. <inline-formula id="inf2">
<mml:math id="m2" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2,3,4,5</mml:mn></mml:mrow></mml:math>
</inline-formula>, denotes the action index. In the <inline-formula id="inf3">
<mml:math id="m3" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:math>
</inline-formula> trial, the participant started from the neutral state, performed the <inline-formula id="inf4">
<mml:math id="m4" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:math>
</inline-formula> action, and returned to the neutral state. This was repeated five times. The screen displayed the target action to be performed. For the neutral state, participants were instructed to relax their masseter muscles, as indicated by the marks in the figure <bold>(B)</bold> Facial expression task. <inline-formula id="inf5">
<mml:math id="m5" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2,3,4,5,6</mml:mn></mml:mrow></mml:math>
</inline-formula>, denotes the six facial expressions separately: 1). anger, 2) disgust, 3) fear, 4) happiness, 5) sadness, and 6) surprise. In the <inline-formula id="inf6">
<mml:math id="m6" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:math>
</inline-formula> trial, the participant started from the neutral expression, performed the <inline-formula id="inf7">
<mml:math id="m7" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:math>
</inline-formula> target facial expression, and returned to the neutral expression. This was repeated 12 times. The screen displayed the target facial expression.</p></caption><graphic xlink:href="fbioe-13-1490919-g003" position="float"/></fig><p>In our study, the data acquisition and processing protocols were rigorously developed based on established methodologies within the field. To ensure the robustness of our procedures, we adhered closely to the protocols described in previous studies (<xref rid="B55" ref-type="bibr">Lou et al., 2020</xref>; <xref rid="B37" ref-type="bibr">Hamedi et al., 2016</xref>; <xref rid="B11" ref-type="bibr">Cha et al., 2020</xref>; <xref rid="B45" ref-type="bibr">Kehri et al., 2019</xref>; <xref rid="B13" ref-type="bibr">Chen et al., 2015</xref>; <xref rid="B61" ref-type="bibr">Mueller et al., 2022</xref>; <xref rid="B28" ref-type="bibr">Fridlund and Cacioppo, 1986</xref>). The main experiment consisted of two parts: the maximum voluntary contraction (MVC) task, and the facial expression task. In the MVC task, participants were asked to perform five different actions as intensely as possible to obtain MVC values for the recorded muscles. We used five actions to measure MVC: eyebrow elevation, eyebrow furrowing and nose elevation, eyelid closure, elevation of mouth corners, and depression of mouth corners (<xref rid="B17" ref-type="bibr">Cohn and Ekman, 2005</xref>). <xref rid="F3" ref-type="fig">Figure 3A</xref> illustrates the structure of the MVC task. At the beginning of the task, the screen displayed a neutral expression, which participants maintained for 4&#x000a0;s. Then, the first trial started. A trial consisted of cycles of neutral expression and target actions. At the beginning of the first trial, the screen displayed the neutral expression for 4&#x000a0;s. Next, the screen displayed an illustration of the target action for 4&#x000a0;s. Participants were instructed to replicate the facial actions that were displayed on the screen at all times. This cycle was repeated 5 times within a single trial. The duration of a trial was 40&#x000a0;s. There were five trials in total. Each trial was associated to a different facial action.</p><p>In the facial expression task, participants were asked to perform six basic facial expressions included in the Facial Action Coding System: anger, disgust, fear, happiness, sadness, and surprise (<xref rid="B36" ref-type="bibr">Hager et al., 2002</xref>). Participants rested around 10&#x000a0;min between the MVC and facial expression task. <xref rid="F3" ref-type="fig">Figure 3B</xref> illustrates the structure of the facial expression task. At the beginning of the task, the screen displayed the image of a neutral expression, which participants maintained for 7&#x000a0;s. Then the first trial started. A trial consisted of cycles of neutral and target facial expressions. First, the screen displayed the neutral expression for 5&#x000a0;s. Next, the screen displayed an illustration of the target expression for 5&#x000a0;s. Finally, the screen displayed the neutral expression again for 1&#x000a0;s. Participants were instructed to replicate the facial expression that was displayed on the screen at all times. This cycle was repeated 12 times within a single trial. The duration of a trial was 132&#x000a0;s. There were six trials in total. Each trial was associated to a different facial expression. In order to prevent muscle fatigue, participants were asked to rest 2&#x000a0;min between trials, and 10&#x000a0;min between two experiment tasks. After data collection, we reviewed the footage post-trial to identify and exclude any instances where the expressions were incorrectly performed.</p></sec><sec id="s2-4"><title>2.4 Data processing</title><p>The sEMG data from both the MVC and facial expression tasks was filtered using a 20&#x02013;450&#x000a0;Hz band-pass filter, rectified, and low-pass filtered using a 2&#x000a0;Hz cutoff frequency (<xref rid="B55" ref-type="bibr">Lou et al., 2020</xref>; <xref rid="B37" ref-type="bibr">Hamedi et al., 2016</xref>; <xref rid="B11" ref-type="bibr">Cha et al., 2020</xref>; <xref rid="B45" ref-type="bibr">Kehri et al., 2019</xref>; <xref rid="B59" ref-type="bibr">Mithbavkar and Shah, 2021</xref>; <xref rid="B13" ref-type="bibr">Chen et al., 2015</xref>; <xref rid="B34" ref-type="bibr">Gruebler and Suzuki, 2010</xref>; <xref rid="B23" ref-type="bibr">Egger et al., 2019</xref>). Additionally, before low-pass filtering, the sEMG data from the facial expression task was normalized using the maximum values of sEMG obtained in the MVC task.</p><p>We used the DeepLabCut (DLC) software (<xref rid="B56" ref-type="bibr">Mathis et al., 2018</xref>) to track five different facial keypoints: the inner eyebrow, outer eyebrow, nose, mouth corner, and jaw, to which we attached stickers (<xref rid="F2" ref-type="fig">Figure 2C</xref>). First, we extracted 275 frames from a video sample (corresponding to 11&#x000a0;s) of each participant and manually labeled the stickers for training DLC. We also manually labeled one of the medial canthi and the upper point of both ears to use them as reference points, as they are immobile with respect to each other. This allowed us to compute a linear transformation that consistently aligned the extracted facial points in each frame to a canonical frame of reference (frontal view of the face), using the methods described in (<xref rid="B89" ref-type="bibr">Wu et al., 2021</xref>). Next, we used the trained DLC to track the facial keypoints in the rest of the video data and extracted the x and y coordinates of the facial keypoints. Finally, we calculated the displacement of the five facial keypoints during the task with respect to the canonical frame.</p><p>Because of the differing sampling rates between the sEMG signal (1&#x000a0;KHz) and the video data (25&#x000a0;Hz), we resampled the sEMG data to 25&#x000a0;Hz for the training of the model relating to sEMG and displacement of the facial keypoints. This resampling involved synchronizing the sEMG and video data using Lab Streaming Layer software. The experimental routines emitted trigger signals at the start and end of each expression, ensuring precise alignment. The sEMG signals were then downsampled by selecting one sample point every 40&#x000a0;ms based on these trigger points, reducing the sampling rate to match that of the video data. Additionally, there is noisy data caused by friction between the skin and the electrodes during the transitions between the neutral and target expressions. To eliminate this noise, we discarded 1&#x000a0;s of the data adjacent to the transitions. This applied to both the neutral and expression segments of both the sEMG and displacement data. Finally, to train the facial expression recognition system and the models for facial keypoint displacement estimation, we randomly selected one target expression from each trial. These periods were combined to create 12 reordered trials. Therefore, each reordered trial contained six different facial expressions in a randomized sequence.</p></sec><sec id="s2-5"><title>2.5 Facial expression recognition system</title><p>We developed a facial expression recognition system that classifies participants&#x02019; expressions based on the synergy activation coefficients of muscle synergies extracted from the recorded facial muscles. This system relies on two procedures: muscle synergy extraction and facial expression classification.</p><sec id="s2-5-1"><title>2.5.1 Muscle synergy extraction</title><p>We used the non-negative matrix factorization (NMF) method (<xref rid="B49" ref-type="bibr">Lee and Seung, 1999</xref>) to obtain muscle synergies and their synergy activation coefficients according to:<disp-formula id="e1">
<mml:math id="m8" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mrow></mml:math>
<label>(1)</label>
</disp-formula>where <inline-formula id="inf8">
<mml:math id="m9" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow></mml:math>
</inline-formula> is a <inline-formula id="inf9">
<mml:math id="m10" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math>
</inline-formula> matrix (<inline-formula id="inf10">
<mml:math id="m11" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:math>
</inline-formula> denotes the number of muscles, <inline-formula id="inf11">
<mml:math id="m12" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math>
</inline-formula> denotes the number of samples) of recorded sEMG signals in all trials for each participant, <inline-formula id="inf12">
<mml:math id="m13" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is a <inline-formula id="inf13">
<mml:math id="m14" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math>
</inline-formula> matrix of muscle synergies (<inline-formula id="inf14">
<mml:math id="m15" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math>
</inline-formula> denotes the number of synergies), <inline-formula id="inf15">
<mml:math id="m16" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:math>
</inline-formula> is a <inline-formula id="inf16">
<mml:math id="m17" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math>
</inline-formula> matrix of synergy activation coefficients and <inline-formula id="inf17">
<mml:math id="m18" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mrow></mml:math>
</inline-formula> is a <inline-formula id="inf18">
<mml:math id="m19" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math>
</inline-formula> matrix of unexplained variation in the muscle activations. In NMF, the number of muscle synergies <inline-formula id="inf19">
<mml:math id="m20" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math>
</inline-formula> is a hyperparameter. We selected <inline-formula id="inf20">
<mml:math id="m21" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math>
</inline-formula> so that the variance accounted for (VAF) by the extracted synergies <inline-formula id="inf21">
<mml:math id="m22" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> reached at least 90%, which is a commonly used criteria in muscle synergy research (<xref rid="B20" ref-type="bibr">d&#x02019;Avella et al., 2006</xref>; <xref rid="B86" ref-type="bibr">Turpin et al., 2021</xref>; <xref rid="B1" ref-type="bibr">Antuvan et al., 2016</xref>). We used the NMF implementation provided in the scikit-learn (Version: 1.3.2) package with the coordinate descent solver and Frobenius norm objective function, with multiple random initializations.</p></sec><sec id="s2-5-2"><title>2.5.2 Facial expression classification</title><p>We used a random forest classifier to classify facial expressions based on features derived from the muscle synergy activation coefficients <inline-formula id="inf22">
<mml:math id="m23" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:math>
</inline-formula>. We extracted features from isolated segments of the <inline-formula id="inf23">
<mml:math id="m24" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:math>
</inline-formula> synergy activation coefficients obtained by a sliding window of 150&#x000a0;ms (<xref rid="B62" ref-type="bibr">Nakayama et al., 2017</xref>) with a step of 40&#x000a0;ms. The 40&#x000a0;ms step was chosen to match the period defined by the 25&#x000a0;Hz video frame rate. The data contained in the sliding window was used to compute the classification features: root mean square (RMS), which measures the signal&#x02019;s average power; variance (VAR), which measures signal fluctuation; mean absolute value (MAV), which measures the amplitude of the signal; and integrated EMG (IEMG), which measures increases in signal power and amplitude (<xref rid="B58" ref-type="bibr">Mithbavkar and Shah, 2019</xref>). The features in one sliding window are defined as <xref rid="e2" ref-type="disp-formula">Equations 2</xref>&#x02013;<xref rid="e5" ref-type="disp-formula">5</xref>:<disp-formula id="e2">
<mml:math id="m25" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:math>
<label>(2)</label>
</disp-formula>
<disp-formula id="e3">
<mml:math id="m26" overflow="scroll"><mml:mrow><mml:mi>V</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
<label>(3)</label>
</disp-formula>
<disp-formula id="e4">
<mml:math id="m27" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math>
<label>(4)</label>
</disp-formula>
<disp-formula id="e5">
<mml:math id="m28" overflow="scroll"><mml:mrow><mml:mi>I</mml:mi><mml:mi>E</mml:mi><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math>
<label>(5)</label>
</disp-formula>where <inline-formula id="inf24">
<mml:math id="m29" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> represents the value of the synergy activation coefficient of muscle synergy <inline-formula id="inf25">
<mml:math id="m30" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2,3</mml:mn></mml:mrow></mml:math>
</inline-formula> (or sEMG signal of muscle <inline-formula id="inf26">
<mml:math id="m31" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math>
</inline-formula>) and <inline-formula id="inf27">
<mml:math id="m32" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:math>
</inline-formula> represents the number of sampled points in one sliding window (here, <inline-formula id="inf28">
<mml:math id="m33" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>125</mml:mn></mml:mrow></mml:math>
</inline-formula>). <inline-formula id="inf29">
<mml:math id="m34" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:math>
</inline-formula> represents the mean value of synergy activation coefficient of muscle synergy <inline-formula id="inf30">
<mml:math id="m35" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2,3</mml:mn></mml:mrow></mml:math>
</inline-formula> (or mean value of sEMG signal of muscle <inline-formula id="inf31">
<mml:math id="m36" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math>
</inline-formula>) in one sliding window. We also built a classifier that uses sEMG signals directly to classify the facial expressions using the same procedure as described above, with the values of the synergy activation coefficients replaced by the values of the EMG signals. Therefore, the input to the random forest classifier is an array of the features extracted from synergy activation coefficients or sEMG signal, that is, an array of <inline-formula id="inf32">
<mml:math id="m37" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math>
</inline-formula> elements for the classifier based on muscle synergies, and <inline-formula id="inf33">
<mml:math id="m38" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math>
</inline-formula> for the classifier based on sEMG signals.</p><p>The classifiers were trained based on pooled data from all participants without downsampling. The synergy activation coefficients or sEMG signals was separated into training and test sets. The training set contained a random permutation of 10 out of the 12 reordered trials per participant (see <xref rid="s2-4" ref-type="sec">Section 2.4</xref>). We used five-fold cross-validation to train the classifiers. The remaining two reordered trials per participant were included in the test set. The training dataset waw in the shape of 100 trials, 3 synergies, 36,000 samples for the synergy-based classifier (100 trials, 7 muscles, 36,000 samples for the sEMG-based classifier), and the testing dataset was in the shape of 20 trials, 3 synergies, 36,000 samples for the synergy-based classifier (20 trials, 7 muscles, 36,000 samples for the sEMG signals-based classifier). Then we calculated the features extracted from sEMG signal and features from synergy activation coefficients to classify facial expressions. We used the <italic>scikit-learn</italic> package to implement the random forest classifiers with the parameters at their default values (setting the number of estimators to 100).</p><p>To evaluate classifier performance, we used the receiver operating characteristic (ROC) curve, F1-score, precision, recall, accuracy, and the confusion matrix. The ROC curve plots the true positive rate (TPR, also known as sensitivity or recall) against the false positive rate (FPR). The F1 score, defined as the harmonic mean of precision and recall, symmetrically incorporates the characteristics of both measures into one comprehensive metric. The confusion matrix visualizes algorithm performance, with rows indicating predicted classes and columns indicating actual classes.</p></sec></sec><sec id="s2-6"><title>2.6 Facial keypoint displacement estimation</title><p>We developed a skin-musculoskeletal model (SMSM) and a linear regression model (LRM), and combined them into a skin-musculoskeletal model with linear regression (SMSM-LRM) to estimate the displacement of facial keypoints using sEMG signals during the execution of facial expressions.</p><sec id="s2-6-1"><title>2.6.1 Skin-musculoskeletal model</title><p>During the execution of a facial expression, facial muscles apply force on the skin, which produces skin deformation. Because skin is viscoelastic in its response to deformation (<xref rid="B95" ref-type="bibr">Zhang et al., 2001</xref>), it opposes the action of muscle force. Therefore, the forces generated by the contraction of facial muscles and those resulting from the deformation of skin are in a state of equilibrium. By integrating models of skin deformation with musculoskeletal models, we can delineate the relationship between muscle activation and skin deformation.</p><p>The relationship between stress and strain in the skin is non-linear, and can be modeled as a mass-spring-damper system with non-linear stiffness (<xref rid="B95" ref-type="bibr">Zhang et al., 2001</xref>). However, for simplicity, here we make three basic assumptions to model forces originating from skin deformation: 1. Skin stiffness is constant (<inline-formula id="inf34">
<mml:math id="m39" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> = 100&#x000a0;N/m), 2. The velocity of skin deformation is low, so that the dampening effect can be ignored, and 3. Skin deformation <inline-formula id="inf35">
<mml:math id="m40" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is equivalent to the displacement of an associated facial keypoint (<inline-formula id="inf36">
<mml:math id="m41" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, where <inline-formula id="inf37">
<mml:math id="m42" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is the current position of the facial keypoint <inline-formula id="inf38">
<mml:math id="m43" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math>
</inline-formula>, and <inline-formula id="inf39">
<mml:math id="m44" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is the neutral position of the facial keypoint <inline-formula id="inf40">
<mml:math id="m45" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math>
</inline-formula>). Following these assumptions, the force <inline-formula id="inf41">
<mml:math id="m46" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> generated by the deformation of skin at point <inline-formula id="inf42">
<mml:math id="m47" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math>
</inline-formula>, is:<disp-formula id="e6">
<mml:math id="m48" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
<label>(6)</label>
</disp-formula>
</p><p>Next, to model forces produced by muscles, we use the Mykin model, which models muscles as springs with muscle activation-dependent stiffness and rest length (<xref rid="B39" ref-type="bibr">He et al., 2022</xref>; <xref rid="B78" ref-type="bibr">Shin et al., 2009</xref>). Thus, the force <inline-formula id="inf43">
<mml:math id="m49" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> generated by muscle <inline-formula id="inf44">
<mml:math id="m50" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:math>
</inline-formula> is:<disp-formula id="e7">
<mml:math id="m51" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>u</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>u</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math>
<label>(7)</label>
</disp-formula>where <inline-formula id="inf45">
<mml:math id="m52" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is the intrinsic stiffness of the muscle, <inline-formula id="inf46">
<mml:math id="m53" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is the muscle stiffness determined by muscle activation <inline-formula id="inf47">
<mml:math id="m54" overflow="scroll"><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:math>
</inline-formula>, <inline-formula id="inf48">
<mml:math id="m55" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is the intrinsic rest length of the muscle, <inline-formula id="inf49">
<mml:math id="m56" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is a factor to determine the muscle rest length as a function of <inline-formula id="inf50">
<mml:math id="m57" overflow="scroll"><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:math>
</inline-formula>, and <inline-formula id="inf51">
<mml:math id="m58" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is the current contraction length of the muscle.</p><p>To integrate the skin and musculoskeletal models, we classified the facial points measured experimentally (<xref rid="s2-2" ref-type="sec">Section 2.2</xref>) as single-muscle systems or double-muscle systems (<xref rid="B36" ref-type="bibr">Hager et al., 2002</xref>; <xref rid="B87" ref-type="bibr">Waller et al., 2008</xref>). Single muscle systems included the outer eyebrow point with the outer frontalis muscle, the point on the superior end of the nasolabial fold with the levator labii superioris alaeque nasi muscle, and the point on the chin with the mentalis muscle. The outer frontalis elevates the outer eyebrow. The levator labii superioris alaeque nasi wrinkles the skin alongside the nose, elevating the position of the marker on the superior end of the nasolabial fold. The mentalis acts to depress and evert the base of the lower lip, while also wrinkling the skin of the chin, elevating the marker on the chin. On the other hand, the double muscle systems included the inner eyebrow point with the inner frontalis and corrugator supercilii muscle group, and the point on the corner of the mouth with the zygomaticus major and depressor anguli oris muscle group (<xref rid="B87" ref-type="bibr">Waller et al., 2008</xref>).</p><p>For the single muscle systems, the structure of the skin-musculoskeletal model is illustrated in <xref rid="F4" ref-type="fig">Figure 4A</xref>. In this case, the stretch length of the muscle, <inline-formula id="inf52">
<mml:math id="m59" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, and the deformation length of the skin, <inline-formula id="inf53">
<mml:math id="m60" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, are the same:<disp-formula id="e8">
<mml:math id="m61" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
<label>(8)</label>
</disp-formula>
</p><fig position="float" id="F4"><label>FIGURE 4</label><caption><p>Skin-musculoskeletal models. <bold>(A)</bold> Single-muscle systems. A facial point is subject to <inline-formula id="inf54">
<mml:math id="m62" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, the force exerted by muscle <inline-formula id="inf55">
<mml:math id="m63" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:math>
</inline-formula>, and <inline-formula id="inf56">
<mml:math id="m64" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, the force exerted by skin deformation at facial point <inline-formula id="inf57">
<mml:math id="m65" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math>
</inline-formula>. Muscle force is modeled as a spring with variable stiffness, as a function of muscle activation <inline-formula id="inf58">
<mml:math id="m66" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>. Single muscle systems include the outer frontalis, levator labii superioris alaeque nasi, and mentalis. <bold>(B)</bold> Double-muscle system: skin-musculoskeletal model of the mouth corner. The mouth corner is subject to forces <inline-formula id="inf59">
<mml:math id="m67" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula id="inf60">
<mml:math id="m68" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, generated by the zygomaticus major and depressor anguli oris muscles, respectively, and <inline-formula id="inf61">
<mml:math id="m69" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, the force generated by the skin deformation <inline-formula id="inf62">
<mml:math id="m70" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> at the mouth corner. The neutral position of the mouth corner is given by <inline-formula id="inf63">
<mml:math id="m71" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, and its current position by <inline-formula id="inf64">
<mml:math id="m72" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>. <bold>(C)</bold> Double muscle system: skin-musculoskeletal model for the inner eyebrow. The inner eyebrow is subject to forces <inline-formula id="inf65">
<mml:math id="m73" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula id="inf66">
<mml:math id="m74" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, generated by the inner frontalis and corrugator supercilii muscles, respectively, and <inline-formula id="inf67">
<mml:math id="m75" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, the force generated by the skin deformation <inline-formula id="inf68">
<mml:math id="m76" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> at the outer eyebrow. The neutral position of the outer eyebrow is given by <inline-formula id="inf69">
<mml:math id="m77" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, and its current position by <inline-formula id="inf70">
<mml:math id="m78" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>. <bold>(D)</bold> Facial keypoints, as shown in <xref rid="F2" ref-type="fig">Figure 2</xref>.</p></caption><graphic xlink:href="fbioe-13-1490919-g004" position="float"/></fig><p>The neutral position for each single muscle system is defined as the position of the facial point when muscles are not activated. By combining <xref rid="e6" ref-type="disp-formula">Equations 6</xref>, <xref rid="e7" ref-type="disp-formula">7</xref>, the force equilibrium equation is:<disp-formula id="e9">
<mml:math id="m79" overflow="scroll"><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>u</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>u</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math>
<label>(9)</label>
</disp-formula>
</p><p>Therefore, according to the skin-musculoskeletal model, by combining the <xref rid="e8" ref-type="disp-formula">Equations 8</xref>, <xref rid="e9" ref-type="disp-formula">9</xref>, the displacement of the facial point defined in each single muscle system (outer eyebrow, superior end of the nasolabial fold, and chin, as shown in <xref rid="F4" ref-type="fig">Figure 4D</xref>) can be expressed as <xref rid="e10" ref-type="disp-formula">Equation 10</xref>:<disp-formula id="e10">
<mml:math id="m80" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>u</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>u</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>u</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:math>
<label>(10)</label>
</disp-formula>
</p><p>In single-muscle systems <inline-formula id="inf71">
<mml:math id="m81" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula id="inf72">
<mml:math id="m82" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula id="inf73">
<mml:math id="m83" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, and <inline-formula id="inf74">
<mml:math id="m84" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> are free parameters that are optimized to fit the experimental data as described in <xref rid="s2-6-4" ref-type="sec">Section 2.6.4</xref>.</p><p>
<xref rid="F4" ref-type="fig">Figures 4B, C</xref> shows the structure of the skin-musculoskeletal model for double-muscle systems, that is, for the mouth corner muscle system, and the inner eyebrow muscle system. Here, we develop the skin-musculoskeletal model for the mouth corner system, but note that the resulting model is directly applicable to the inner eyebrow system. Even though the displacement of facial points in double-muscle systems is two-dimensional, here we assume that in the facial expression tasks, the direction of the displacement is highly biased in a single direction, allowing us to describe the displacement of the point as a one-dimensional quantity. Furthermore, the displacement of the facial point is associated with a change in the length of each of the two muscles in the system. For small displacement magnitudes, this relationship can be assumed to be linear. In the mouth corner system with the zygomaticus major and the depressor anguli oris muscles, this relationship can be expressed as:<disp-formula id="e11">
<mml:math id="m85" overflow="scroll"><mml:mrow><mml:mfenced open="{" close=""><mml:mrow><mml:mtable class="cases"><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mspace width="1em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mspace width="1em"/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math>
<label>(11)</label>
</disp-formula>where <inline-formula id="inf75">
<mml:math id="m86" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula id="inf76">
<mml:math id="m87" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> are the changes in length of the zygomaticus major and depressor anguli oris muscles, respectively, <inline-formula id="inf77">
<mml:math id="m88" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is the displacement of the mouth corner, and<inline-formula id="inf78">
<mml:math id="m89" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula id="inf79">
<mml:math id="m90" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> are linear coefficients that depend on the geometry of the attachment of muscle and skin to the facial point. The muscle forces <inline-formula id="inf80">
<mml:math id="m91" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula id="inf81">
<mml:math id="m92" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> and the skin deformation force <inline-formula id="inf82">
<mml:math id="m93" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> at the mouth corner are in equilibrium, as <xref rid="e12" ref-type="disp-formula">Equation 12</xref>:<disp-formula id="e12">
<mml:math id="m94" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math>
<label>(12)</label>
</disp-formula>
</p><p>The magnitude of the force exerted by the skin can be expressed in terms of the magnitudes of the muscle forces as:<disp-formula id="e13">
<mml:math id="m95" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x02016;</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02016;</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02016;</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02016;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02016;</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02016;</mml:mo></mml:mrow></mml:math>
<label>(13)</label>
</disp-formula>where <inline-formula id="inf83">
<mml:math id="m96" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula id="inf84">
<mml:math id="m97" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> denote projection coefficients of <inline-formula id="inf85">
<mml:math id="m98" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> and <inline-formula id="inf86">
<mml:math id="m99" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> onto the direction of <inline-formula id="inf87">
<mml:math id="m100" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, which also depend on the geometry of the attachment of muscle and skin to the facial point, but for small displacements can be assumed to be constant. Inserting <xref rid="e11" ref-type="disp-formula">Equation 11</xref> into <xref rid="e6" ref-type="disp-formula">Equations 6</xref>, <xref rid="e7" ref-type="disp-formula">7</xref>, and the resulting expressions into <xref rid="e13" ref-type="disp-formula">Equation 13</xref>, the displacement <inline-formula id="inf88">
<mml:math id="m101" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> of the corner of the mouth becomes <xref rid="e14" ref-type="disp-formula">Equation 14</xref>:<disp-formula id="e14">
<mml:math id="m102" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>Z</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>Z</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:math>
<label>(14)</label>
</disp-formula>
</p><p>Following a similar procedure, the displacement of the inner eyebrow <inline-formula id="inf89">
<mml:math id="m103" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> can be expressed as <xref rid="e15" ref-type="disp-formula">Equation 15</xref>:<disp-formula id="e15">
<mml:math id="m104" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:math>
<label>(15)</label>
</disp-formula>
</p><p>In double-muscle systems <inline-formula id="inf90">
<mml:math id="m105" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula id="inf91">
<mml:math id="m106" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula id="inf92">
<mml:math id="m107" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula id="inf93">
<mml:math id="m108" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, <inline-formula id="inf94">
<mml:math id="m109" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>, and <inline-formula id="inf95">
<mml:math id="m110" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> are free parameters that are optimized to fit the experimental data as described in <xref rid="s2-6-4" ref-type="sec">Section 2.6.4</xref>.</p></sec><sec id="s2-6-2"><title>2.6.2 Linear regression model</title><p>We used a multivariate linear regression model (LRM) to relate the sEMG signals (or muscle activations) of all seven muscles to the displacements of all five facial points in the experiment. The LRM can be expressed as <xref rid="e16" ref-type="disp-formula">Equation 16</xref>:<disp-formula id="e16">
<mml:math id="m111" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x00394;</mml:mi><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">LRM</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">LRM</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003f5;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">LRM</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
<label>(16)</label>
</disp-formula>where <inline-formula id="inf96">
<mml:math id="m112" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">LRM</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is a matrix of displacements of five facial points estimated by LRM (shape: <inline-formula id="inf97">
<mml:math id="m113" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math>
</inline-formula>, <inline-formula id="inf98">
<mml:math id="m114" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math>
</inline-formula> represents the number of samples), <inline-formula id="inf99">
<mml:math id="m115" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">LRM</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is the weight matrix computed via linear regression (shape: <inline-formula id="inf100">
<mml:math id="m116" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>5,7</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math>
</inline-formula>), <inline-formula id="inf101">
<mml:math id="m117" overflow="scroll"><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:math>
</inline-formula> is a matrix of the sEMG signal from all seven muscles in the experiment (shape: <inline-formula id="inf102">
<mml:math id="m118" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math>
</inline-formula>), and <inline-formula id="inf103">
<mml:math id="m119" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003f5;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">lrm</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is a matrix containing the residuals unaccounted by the linear regression. The weights in <inline-formula id="inf104">
<mml:math id="m120" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">LRM</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> are free parameters optimized as described in <xref rid="s2-6-4" ref-type="sec">Section 2.6.4</xref>.</p></sec><sec id="s2-6-3"><title>2.6.3 Skin-musculoskeletal model with linear regression model</title><p>The SMSM does not take into account the effects of the displacement of facial points outside the single or double muscle systems to estimate the displacement of a given facial point. However, facial points may be connected to other facial points through skin, and thus may be subject to forces other than those considered in the single and double muscle systems. Here, we addressed this issue by combining the SMSM and the LRM to integrate their estimation capabilities. The skin-musculoskeletal model with linear regression (SMSM-LRM) can be expressed as <xref rid="e17" ref-type="disp-formula">Equation 17</xref>:<disp-formula id="e17">
<mml:math id="m121" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">&#x00394;</mml:mi><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">SMSM</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">&#x003f5;</mml:mi></mml:mrow></mml:math>
<label>(17)</label>
</disp-formula>where <inline-formula id="inf105">
<mml:math id="m122" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:math>
</inline-formula> is the matrix of displacements of facial points estimated by the SMSM-LRM (shape: <inline-formula id="inf106">
<mml:math id="m123" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math>
</inline-formula>), <inline-formula id="inf107">
<mml:math id="m124" overflow="scroll"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math>
</inline-formula> is a weight matrix computed by linear regression (shape: [5,5]), <inline-formula id="inf108">
<mml:math id="m125" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">SMSM</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> is a matrix of facial point displacements produced by the SMSM (shape: <inline-formula id="inf109">
<mml:math id="m126" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math>
</inline-formula>) and <inline-formula id="inf110">
<mml:math id="m127" overflow="scroll"><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math>
</inline-formula> represents residuals unaccounted by the SMSM-LRM (shape: <inline-formula id="inf111">
<mml:math id="m128" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math>
</inline-formula>). The free parameters in the SMSM component of the model and the weights in <inline-formula id="inf112">
<mml:math id="m129" overflow="scroll"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math>
</inline-formula> are optimized to fit the experimental data as described in <xref rid="s2-6-4" ref-type="sec">Section 2.6.4</xref>.</p></sec><sec id="s2-6-4"><title>2.6.4 Training the SMSM, LRM and SMSM-LRM models</title><p>The free parameters in the SMSM, LRM, and SMSM-LRM were determined through iterative optimization within a supervised learning framework. We initialized a set of parameters which were subsequently refined across 18,000 epochs using gradient descent, facilitated by the Adam optimizer. The optimization process aimed to minimize the mean squared error between the estimations of the models and the actual data. The training data and test data sets were the same sets as those defined for the facial expression recognition task with downsampling. The training dataset was in the shape of 10 trials, seven muscles, 900 samples, and the testing dataset was in the shape of two trials, seven muscles, 900 samples per participant. We used 5-fold cross-validation to enhance the model&#x02019;s generalizability and prevent overfitting. The model with the best performance across the five folds was selected for use on the test dataset.</p></sec><sec id="s2-6-5"><title>2.6.5 Evaluation methods</title><p>We evaluated the model&#x02019;s performance using two standard metrics: the coefficient of determination <inline-formula id="inf113">
<mml:math id="m130" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</inline-formula> and the normalized root-mean-square error (NRMSE) with respect to the difference in maximum and minimum values in the data. We assessed differences in the metrics associated with each model using ANOVA tests. We used Tukey&#x02019;s Honestly Significant Difference (HSD) <italic>post hoc</italic> test to identify specific group disparities. We used the <italic>stats</italic> module from the <italic>scipy</italic> package to perform the statistical analysis.</p></sec></sec></sec><sec sec-type="results" id="s3"><title>3 Results</title><sec id="s3-1"><title>3.1 Muscle synergies allow low-dimensional visualization of facial muscle control</title><p>The normalized sEMG signals and the displacement of the five measured facial keypoints in a reordered trial of the facial expression task of a representative participant are illustrated in <xref rid="F5" ref-type="fig">Figure 5</xref>. These reordered signals were employed to extract both muscle synergies and relevant classification features for the classification of facial expressions. To determine the optimal number of muscle synergy modules, we computed the variance accounted for (VAF) with the synergy module number ranging from 1 to 7 per participant. For all participants, 3 synergies were enough to account for 90% of the variability in the muscle activation data (<xref rid="F6" ref-type="fig">Figure 6A</xref>).</p><fig position="float" id="F5"><label>FIGURE 5</label><caption><p>Normalized sEMG signals and displacement of facial keypoints during the facial expression task for a representative participant (participant 2). <bold>(A)</bold> Normalized sEMG signals of seven muscles during one reordered trial containing a single repetition of each facial expression. <bold>(B)</bold> Displacement of five facial key points during one reordered trial containing a single repetition of each facial expression.</p></caption><graphic xlink:href="fbioe-13-1490919-g005" position="float"/></fig><fig position="float" id="F6"><label>FIGURE 6</label><caption><p>Muscle synergies of facial muscles during facial expressions. <bold>(A)</bold> VAF in the measured sEMG signals as a function of the number of synergy components extracted across all participants. Three synergies were enough to account for at least 90% of the variance in all participants. <bold>(B)</bold> Extracted synergy components for a representative participant (participant 2), and synergy activation coefficients for each synergy during a representative trial in the facial expression task (participant 2, trial 2). <bold>(C)</bold> Clusters of synergy activation coefficients in the 3-dimensional synergy space across all trials (participant 2). The shaded regions in the figure show the convex hulls containing the muscle synergy activations produced during each facial expression. The convex hulls are computed in three-dimensional space from the points representing the synergy activations and reflect the range and shape of each expression within the muscle synergy activation space. The convex hulls were calculated using the <italic>ConvexHull</italic> function from the <italic>scipy</italic> library.</p></caption><graphic xlink:href="fbioe-13-1490919-g006" position="float"/></fig><p>
<xref rid="F6" ref-type="fig">Figure 6B</xref> shows the muscle synergies and synergy activation coefficients for a representative participant in one of the reordered trials of the facial expression task. Synergy 1 primarily activated corrugator supercilii, levator labii superioris alaeque nasi, and depressor anguli oris; synergy 2 involved significant activation in inner and outer frontalis, depressor anguli oris, and mentalis; synergy 3 predominantly activated zygomaticus major and depressor anguli oris. We compared the extracted muscle synergies across all participants using the cosine similarity metric (<xref rid="B72" ref-type="bibr">Rimini et al., 2017</xref>; <xref rid="B19" ref-type="bibr">d&#x02019;Avella and Bizzi, 2005</xref>). We found that the three extracted synergies were similar across participants, especially synergy 2 (average cosine similarity; synergy 1: 0.75 (SD 0.16), synergy 2: 0.87 (SD 0.10), and synergy 3: 0.72 (SD 0.20). Synergy 1 was predominantly activated during anger, disgust, and sadness expressions; synergy 2 was predominantly activated during fear, sadness, surprise and disgust expressions; synergy 3 was predominantly activated during disgust and happiness expressions. <xref rid="F6" ref-type="fig">Figure 6C</xref> shows clusters of synergy activation coefficients in the three-dimensional synergy space for a representative participant. Interestingly, the synergy activation coefficients for the expressions of anger, surprise, and happiness are predominantly clustered around a single dimension of the synergy space for all participants. Anger is mainly associated with synergy 1, surprise with synergy 2, and happiness with synergy 3. The remaining expressions are associated mainly with combinations of two or three dimensions in the synergy space. For example, for participant 2, the expressions of disgust, sadness, and fear are clustered in regions of the synergy space spanning a combination of synergies 1, 2, and 3, synergies 1 and 2, and synergies 2 and 3, respectively. Results for the rest of the participants are provided in the <xref rid="s12" ref-type="sec">Supplementary Material</xref> (<xref rid="s12" ref-type="sec">Supplementary Figure S1</xref>).</p></sec><sec id="s3-2"><title>3.2 Performance of synergy-based classification of facial expressions is good enough compared to sEMG-based classification</title><p>
<xref rid="F7" ref-type="fig">Figures 7A&#x02013;D</xref> shows the results of the facial expression classifier based on synergy activation coefficients and the classifier based on sEMG signals. The synergy-based classifier achieved expression-specific accuracies across all participants of 99.5% for the neutral expression (vs. sEMG-based classifier: 99.5%), 98.5% for anger (vs. 99.5%), 97.3% for disgust (vs. 99.2%), 93.3% for fear (vs. 99.0%), 97.4% for happiness (vs. 98.82%), 89.5% for sadness (vs. 97.4%), and 93.2% for surprise (vs. 98.5%), as shown in the confusion matrix (<xref rid="F7" ref-type="fig">Figure 7A</xref>). Furthermore, the synergy-based classifier achieved average accuracies across all participants of 97.4% (vs. sEMG-based classifier: 99.2%) and accuracy for each participant of 96.6%, 99.5%, 98.4%, 97.6%, 97.5%, 98.1%, 97.1%, 95.3%, 96.7% and 97.5% (<xref rid="F7" ref-type="fig">Figure 7B</xref>). Additionally, both classifiers maintain a high level of performance across all expressions, with most precision, recall, and F1 scores exceeding 0.9 (<xref rid="F7" ref-type="fig">Figure 7C</xref>). Finally, the proximity of the ROC curve of each expression to the upper left corner of the graph indicates a high true positive rate (TPR) and a low false positive rate (FPR) (<xref rid="F7" ref-type="fig">Figure 7D</xref>). Notably, all expressions exhibit an area under the curve (AUC) value close to 1 for both classifiers.</p><fig position="float" id="F7"><label>FIGURE 7</label><caption><p>Evaluation metrics for the facial expression recognition systems. <bold>(A)</bold> Confusion matrices across all participants of the synergy- and sEMG-based classifiers. Each row and column represents the true labels and predicted labels, respectively. The intensity of the shade in each box is proportional to the displayed accuracy. <bold>(B)</bold> Average classification accuracy across all facial expressions and participants for the synergy- and sEMG-based classifiers. Error bars indicate the standard deviation in overall accuracy across all participants. Points represent the accuracy for individual participants. <bold>(C)</bold> Recall, precision, and F1 score of each facial expression across all participants for the synergy- and sEMG-based classifiers. <bold>(D)</bold> ROC curves for each expression for the synergy- and sEMG-based classifiers. The line representing the performance of a classifier with an Area Under the Curve (AUC) of 0.5 is not shown due to the scale of the axes.</p></caption><graphic xlink:href="fbioe-13-1490919-g007" position="float"/></fig></sec><sec id="s3-3"><title>3.3 SMSM-LRM has the best performance in estimation of displacement of facial points</title><p>
<xref rid="F8" ref-type="fig">Figure 8</xref> presents representative results on the test set (participant 2) of the SMSM, LRM and SMSM-LRM models in the estimation of the displacements of five facial points defined in the experiment. We evaluated the performance of the three models by computing the coefficient of determination <inline-formula id="inf114">
<mml:math id="m131" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</inline-formula> and Normalized Root Mean Square Error (NRMSE) (<xref rid="F9" ref-type="fig">Figure 9</xref>) across participants. Notably, the SMSM-LRM model demonstrated the highest performance in predicting the displacements of all five facial points across all participants, according to both <inline-formula id="inf115">
<mml:math id="m132" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> (77.02 (SD 2.45)) and NRMSE (0.0668 (SD 0.0051)), compared to the SMSM (<inline-formula id="inf116">
<mml:math id="m133" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>: 57.07 (SD 6.17), NRMSE: 0.0890 (SD 0.0068)) and LRM (<inline-formula id="inf117">
<mml:math id="m134" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>: 62.58 (SD 3.61), NRMSE: 0.0842 (SD 0.0049)) models. A one-way ANOVA revealed that there was a statistically significant difference in <inline-formula id="inf118">
<mml:math id="m135" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> scores between at least two models (F (2, 27) = 5.5699, p = 0.0094). Tukey&#x02019;s HSD Test for multiple comparisons found that the mean value of <inline-formula id="inf119">
<mml:math id="m136" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> scores was significantly different between LRM and SMSM-LRM (p = 0.0242) and between SMSM and SMSM-LRM (p = 0.0182). However, there was no statistically significant difference between LRM and SMSM (p = 0.5715). Similarly, a one-way ANOVA revealed that there was a statistically significant difference in NRMSE between at least two models (F (2, 27) = 4.2775, p = 0.0243). Tukey&#x02019;s HSD Test for multiple comparisons found that the mean value of NRMSE was significantly different between LRM and SMSM-LRM (p = 0.0243) and between SMSM-LRM and SMSM (p = 0.0120). However, there was no statistically significant difference between LRM and SMSM (p = 0.4542).</p><fig position="float" id="F8"><label>FIGURE 8</label><caption><p>Estimation results of facial keypoints by SMSM, LRM, and SMSM-LRM (participant 2): the displacement of the inner eyebrow, the outer eyebrow, the nose, the mouth corner, and the chin, respectively. The blue lines represented the measured displacements of five facial points calculated based on DeepLabCut. The orange dashed, green dotted, and red lines represent the prediction results from the LRM, SMSM, and SMSM-LRM, respectively.</p></caption><graphic xlink:href="fbioe-13-1490919-g008" position="float"/></fig><fig position="float" id="F9"><label>FIGURE 9</label><caption><p>Comparisons of five-fold cross-validation results of all participants: <bold>(A)</bold> the average NRMSE of each model. <bold>(B)</bold> The average <inline-formula id="inf120">
<mml:math id="m137" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> of each model.</p></caption><graphic xlink:href="fbioe-13-1490919-g009" position="float"/></fig></sec></sec><sec sec-type="discussion" id="s4"><title>4 Discussion</title><p>In this study we defined two aims: 1. To establish a framework for recognizing facial expressions based on muscle synergies of facial muscles, and 2. To estimate the displacement of facial keypoints from the sEMG signals of the facial muscles as a step towards generating facial expressions in robotic systems. For the facial expression recognition task, we employed non-negative matrix factorization (NMF) to identify muscle synergies and their synergy activation coefficients from the measured sEMG of seven facial muscles, and used the synergy activation coefficients to train a random forest classifier to recognize six different facial expressions. For the facial expression generation task, we introduced the skin-musculoskeletal model combined with linear regression (SMSM-LRM) as a novel approach to estimate the displacements of five facial keypoints: inner eyebrow, outer eyebrow, superior end of the nasolabial fold, mouth corner, and chin, which we measured using video-based object tracking software. The FER system based on muscle synergies had a high accuracy in classifying facial expressions compared to existing methods, but a slightly lower accuracy than using an sEMG-based classifier. We also found that the proposed SMSM-LRM outperforms the SMSM and LRM in estimating the displacement of the facial keypoints.</p><sec id="s4-1"><title>4.1 Muscle synergy-based facial recognition system has good performance</title><p>Our facial expression recognition system based on muscle synergies yielded enhanced performance for all six facial expressions compared to previous research (<xref rid="B13" ref-type="bibr">Chen et al., 2015</xref>; <xref rid="B11" ref-type="bibr">Cha et al., 2020</xref>; <xref rid="B45" ref-type="bibr">Kehri et al., 2019</xref>; <xref rid="B59" ref-type="bibr">Mithbavkar and Shah, 2021</xref>; <xref rid="B34" ref-type="bibr">Gruebler and Suzuki, 2010</xref>). Notably, the recognition rates for the expressions of fear (93.3% vs. 65.4% (<xref rid="B12" ref-type="bibr">Cha and Im, 2022</xref>)), sadness (89.5% vs. 78.8% (<xref rid="B12" ref-type="bibr">Cha and Im, 2022</xref>)), surprise (93.7% vs. 88.9%) (<xref rid="B13" ref-type="bibr">Chen et al., 2015</xref>) and anger (98.5% vs. 91.7% (<xref rid="B13" ref-type="bibr">Chen et al., 2015</xref>)) observed considerable improvements. The main reason for this superior performance is likely that previous studies primarily focused on sEMG signals collected around the eyes, whereas our method expanded its scope to include sEMG signals collected around the mouth. For instance, the depressor anguli oris influences the motion of the mouth corner, which is useful to discern expressions of fear and sadness, enhancing the recognition accuracy of our system.</p><p>However, we found that a classifier based on sEMG from individual muscles outperforms the classifier based on muscle synergies (average accuracy: sEMG-based - 99.2% vs. muscle synergy-based - 97.4%). This aligns with the finding that the residuals <inline-formula id="inf121">
<mml:math id="m138" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mrow></mml:math>
</inline-formula> in the sEMG signals <inline-formula id="inf122">
<mml:math id="m139" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow></mml:math>
</inline-formula> that the identified muscle synergies <inline-formula id="inf123">
<mml:math id="m140" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula> cannot account for may contain task-relevant information (<xref rid="e1" ref-type="disp-formula">Equation 1</xref>) (<xref rid="B3" ref-type="bibr">Barradas et al., 2020</xref>). However, the difference in performance of the FER systems based on individual muscles and muscle synergies is small, and the performance of the muscle synergy-based system is considerably better than previously developed systems. Therefore, the muscle synergy-based FER system is good enough for this classification task.</p><p>
<xref rid="F6" ref-type="fig">Figure 6C</xref> and <xref rid="s12" ref-type="sec">Supplementary Figure S1</xref> show the contribution of each identified muscle synergy to the execution of each facial expression. Across all participants, the neutral expression is associated to a null activation of all three synergies, as expected. Interestingly, across all participants, synergies 1, 2, and 3 are each predominantly related to only one facial expression: anger, surprise, and happiness, respectively. On the other hand, the facial expressions of disgust, fear, and sadness are associated with combinations of synergies 1, 2, and 3. As shown in <xref rid="F6" ref-type="fig">Figure 6C</xref>, participant 2 showed clearly separate clusters of synergy activation coefficients for each facial expression. However, for other participants, the clusters of expressions of fear, happiness, sadness, and surprise showed some overlap (<xref rid="s12" ref-type="sec">Supplementary Figure S1</xref>). This is especially true for participants 1, 8, 9 and 10, resulting in lower recognition accuracy than for other participants (<xref rid="F7" ref-type="fig">Figure 7B</xref>). We also found instances of global misclassification of facial expressions due to common synergies involved in the execution of different expressions. Particularly, the accuracy for fear, sadness, and surprise expressions did not exceed 95%. This may be because the activation of synergy 2 for these emotions is similar, given that synergy 2 predominantly activates the inner and outer frontalis muscle, which belongs to action unit (AU) 1 in the facial action coding system (FACS), and AU1 is known to be involved in these facial expressions (fear, sadness, surprise) (<xref rid="T1" ref-type="table">Tables 1</xref>, <xref rid="T2" ref-type="table">2</xref> (<xref rid="B36" ref-type="bibr">Hager et al., 2002</xref>).</p><table-wrap position="float" id="T1"><label>TABLE 1</label><caption><p>Facial expressions and corresponding action units (AU) (<xref rid="B36" ref-type="bibr">Hager et al., 2002</xref>).</p></caption><table frame="hsides" rules="groups"><thead valign="top"><tr><th align="center" rowspan="1" colspan="1">Facial expression</th><th align="center" rowspan="1" colspan="1">Action units (AU)</th></tr></thead><tbody valign="top"><tr><td align="center" rowspan="1" colspan="1">Happiness</td><td align="center" rowspan="1" colspan="1">6 + 12</td></tr><tr><td align="center" rowspan="1" colspan="1">Sadness</td><td align="center" rowspan="1" colspan="1">1 + 4 + 15</td></tr><tr><td align="center" rowspan="1" colspan="1">Surprise</td><td align="center" rowspan="1" colspan="1">1 + 2 + 5 + 26</td></tr><tr><td align="center" rowspan="1" colspan="1">Fear</td><td align="center" rowspan="1" colspan="1">1 + 2 + 4 + 5 + 7 + 20 + 26</td></tr><tr><td align="center" rowspan="1" colspan="1">Anger</td><td align="center" rowspan="1" colspan="1">4 + 5 + 7 + 23</td></tr><tr><td align="center" rowspan="1" colspan="1">Disgust</td><td align="center" rowspan="1" colspan="1">9 + 15 + 16</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T2"><label>TABLE 2</label><caption><p>Action units (AU) and corresponding muscles (<xref rid="B36" ref-type="bibr">Hager et al., 2002</xref>).</p></caption><table frame="hsides" rules="groups"><thead valign="top"><tr><th align="center" rowspan="1" colspan="1">AU number</th><th align="center" rowspan="1" colspan="1">Description</th><th align="center" rowspan="1" colspan="1">Muscle</th></tr></thead><tbody valign="top"><tr><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">Inner Brow raiser</td><td align="center" rowspan="1" colspan="1">Frontalis* (pars medialis)</td></tr><tr><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">Outer brow raiser</td><td align="center" rowspan="1" colspan="1">Frontalis* (pars lateralis)</td></tr><tr><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">Brow lowerer</td><td align="center" rowspan="1" colspan="1">Depressor Glabellae, Depressor Supercilli, Corrugator Supercilli*</td></tr><tr><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">Upper lid raiser</td><td align="center" rowspan="1" colspan="1">Levator Palpebrae Superioris</td></tr><tr><td align="center" rowspan="1" colspan="1">6</td><td align="center" rowspan="1" colspan="1">Cheek raiser</td><td align="center" rowspan="1" colspan="1">Orbicularis Oculi (pars orbitalis)</td></tr><tr><td align="center" rowspan="1" colspan="1">7</td><td align="center" rowspan="1" colspan="1">Lid tightener</td><td align="center" rowspan="1" colspan="1">Orbicularis Oculi (pars palpebralis)</td></tr><tr><td align="center" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">Nose wrinkler</td><td align="center" rowspan="1" colspan="1">Levator Labii Superioris Alaeque Nasi*</td></tr><tr><td align="center" rowspan="1" colspan="1">12</td><td align="center" rowspan="1" colspan="1">Lip corner puller</td><td align="center" rowspan="1" colspan="1">Zygomatic Major*</td></tr><tr><td align="center" rowspan="1" colspan="1">15</td><td align="center" rowspan="1" colspan="1">Lip corner depressor</td><td align="center" rowspan="1" colspan="1">Depressor Anguli Oris (Triangularis)</td></tr><tr><td align="center" rowspan="1" colspan="1">16</td><td align="center" rowspan="1" colspan="1">Lower lip depressor</td><td align="center" rowspan="1" colspan="1">Depressor Labii Inferioris</td></tr><tr><td align="center" rowspan="1" colspan="1">17</td><td align="center" rowspan="1" colspan="1">Chin raiser</td><td align="center" rowspan="1" colspan="1">Mentalis*</td></tr><tr><td align="center" rowspan="1" colspan="1">20</td><td align="center" rowspan="1" colspan="1">Lip stretcher</td><td align="center" rowspan="1" colspan="1">Risorius</td></tr><tr><td align="center" rowspan="1" colspan="1">23</td><td align="center" rowspan="1" colspan="1">Lip tightener</td><td align="center" rowspan="1" colspan="1">Orbicularis Oris</td></tr><tr><td align="center" rowspan="1" colspan="1">26</td><td align="center" rowspan="1" colspan="1">Jaw drop</td><td align="center" rowspan="1" colspan="1">Masseter, Temporal and Internal Pterygoid relaxed</td></tr></tbody></table><table-wrap-foot><fn><p>*represents muscle measured in our study.</p></fn></table-wrap-foot></table-wrap><p>Nevertheless, the impact of the misclassified instances described above is not too large, as our facial expression recognition system showed uniform high scores for precision, recall and F1-score (<xref rid="F7" ref-type="fig">Figure 7C</xref>). This indicates a balanced classification performance, with no significant trade-off between precision and recall for any of the expressions. Such consistent results underscore the robustness of the classification model in recognizing and differentiating between the different facial expressions. This conclusion is also supported by the ROC curve and its area (AUC) (<xref rid="F7" ref-type="fig">Figure 7D</xref>), which demonstrate the model&#x02019;s ability to achieve a high true positive rate with a very low false positive rate, and a strong capability to distinguish between facial expressions, with good potential for practical applications.</p></sec><sec id="s4-2"><title>4.2 SMSM and LRM complement each other to achieve higher quality estimations of facial keypoint displacements</title><p>In predicting the displacement of facial points, the SMSM-LRM method showed the most effective performance as measured by <inline-formula id="inf124">
<mml:math id="m141" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> and NRMSE metrics (<xref rid="F9" ref-type="fig">Figure 9</xref>). On the other hand, in isolation, the SMSM and LRM models showed indistinguishable <inline-formula id="inf125">
<mml:math id="m142" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula> and NRMSE scores, indicating no significant differences between SMSM and LRM in their performance to estimate displacements of facial keypoints. However, a closer examination of the estimations produced by each model in isolation for individual participants revealed notable distinctions (<xref rid="F8" ref-type="fig">Figure 8</xref>; <xref rid="s12" ref-type="sec">Supplementary Figure S2</xref>). Specifically, SMSM showed difficulty in predicting the displacement of facial points with substantial skin connectivity to other facial points. Here, we used separate local SMSM models for each considered facial point. However, skin and muscle forces in one point may interact with other nearby points, which the SMSM model does not account for. This is evident in the case of the disgust expression across all participants, where SMSM consistently deviated from the measured results in predicting the movement of the outer eyebrow (<xref rid="F8" ref-type="fig">Figure 8</xref>), as this point is relatively close to the inner eyebrow. In contrast, LRM was more successful in estimating displacements in these cases, as it establishes a multivariate relation between sEMG signals and facial point displacements, albeit without considering muscle characteristics. However, LRM predictions exhibited greater fluctuations than those from SMSM and SMSM-LRM. For instance, the LRM estimations of displacement of the inner and outer eyebrow were highly variable for expressions producing large displacements (<xref rid="F8" ref-type="fig">Figure 8</xref>). This may be because large displacements are usually associated with larger sEMG signals, which contain considerable signal-dependent noise (<xref rid="B38" ref-type="bibr">Harris and Wolpert, 1998</xref>). Therefore, combining the SMSM and LRM creates a connectivity map between facial points (obtained through multivariate linear regression) informed by muscle and skin mechanics (obtained through SMSM), improving over each method applied in isolation. This approach is supported by results in the field of computer graphics, where models of skin elasticity and connectivity are crucial in achieving realistic and natural facial expressions (<xref rid="B46" ref-type="bibr">Kim et al., 2020</xref>; <xref rid="B79" ref-type="bibr">Sifakis et al., 2005</xref>; <xref rid="B95" ref-type="bibr">Zhang et al., 2001</xref>; <xref rid="B50" ref-type="bibr">Lee et al., 1995</xref>; <xref rid="B43" ref-type="bibr">K&#x000e4;hler et al., 2001</xref>).</p></sec><sec id="s4-3"><title>4.3 Identified muscle synergies may provide insights for generation of facial expressions</title><p>In its current form, the SMSM-LRM model is not directly applicable to the facial expression generation task because it is a forward model of the physics of facial motion. That is, it relates muscle activations (sEMG signals) to the displacement of facial keypoints. However, in the facial generation task, an inverse model of the facial motion is needed: a mapping from desired displacements of facial points (or desired facial expressions) to muscle activations. Here, we notice that the results of the muscle synergy analysis could be used in conjunction with the SMSM-LRM model to build an actual facial expression generation system.</p><p>As mentioned above, the different facial expressions are associated with clusters of specific combinations of muscle synergies (<xref rid="F6" ref-type="fig">Figure 6C</xref>). Therefore, it is possible to generate trajectories in the synergy activation space from one expression cluster to another. These trajectories in synergy space can be mapped directly to muscle activations <inline-formula id="inf126">
<mml:math id="m143" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow></mml:math>
</inline-formula> by using <xref rid="e1" ref-type="disp-formula">Equation 1</xref> and ignoring the residual term <inline-formula id="inf127">
<mml:math id="m144" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mrow></mml:math>
</inline-formula>. These muscle activations could in turn be used as an input to the SMSM-LRM model to produce trajectories for the defined facial keypoints. Therefore, the clusters associated to the different facial expressions in the muscle synergy activation space could act as the inverse model needed by the SMSM-LRM to achieve a desired expression. Moreover, these trajectories in synergy activation space could be used to create transitions between different expressions and between different intensities of a given expression, creating a continuum in the space of facial expressions, as opposed to a discrete encoding of predefined expressions.</p><p>Evidently, this approach is not exclusively achievable using the extracted muscle synergies, as transitions between facial expressions could also be defined in a space where the activation of each individual muscle constitutes a different dimension. However, using muscle synergies simplifies the visualization of these transitions, and ensures that the transitions follow realistic muscle coordination patterns, resulting in potentially more natural transitions.</p></sec><sec id="s4-4"><title>4.4 Limitations</title><p>Here, we have described an effective system to recognize facial expressions and estimate displacements of facial points based on sEMG measurements, muscle synergy analysis, and a skin-musculoskeletal model enhanced with linear regression. However, there remain some limitations in implementation, experimental and application aspects that should be addressed in future work. In the implementation aspect, the accuracy of facial point displacement measurements needs to be quantitatively verified. We measured the displacement of facial points with a 2D camera using object tracking software. Tracking of the position of objects using vision systems is prone to measurement errors due to changes in the pose of the objects. Methods to alleviate this problem have been addressed in the tracking of facial points by using linear transformations to align the measured points given the coordinates of fixed landmarks (<xref rid="B89" ref-type="bibr">Wu et al., 2021</xref>). However, in our experimental setup, electrodes for the measurement of sEMG occluded large portions of the face, making it difficult to stably detect facial landmarks using established algorithms. For this reason, we trained a custom algorithm using DeepLabCut to track facial points that we physically marked using stickers on participants&#x02019; faces. We manually labeled a subset of the captured images based on stickers, and used this labeled set to train the DeepLabCut algorithm to track the facial points in unlabeled images. This makes it difficult to evaluate the error in our displacement measurements, as we do not have a ground truth for the predictions obtained by DeepLabCut. However, visual inspection of the tracked facial points in the unlabeled images suggests that the tracking performance is adequate.</p><p>An additional limitation regarding implementation aspects is that we only measured sEMG from muscles without measuring their contralateral counterparts. This prevents analyzing more complex facial expressions that may be asymmetric, and may reveal more complex patterns of coordination across muscles. The main obstacle for this problem is the number of electrodes that can be placed on the face without obstructing the placement of other electrodes. In our case, the upper limit in the number of electrodes was close to 7. Therefore, analysis of other types of expressions would require removing electrodes from muscles that may provide valuable information. This may be alleviated by future miniaturization of hardware, or use of intramuscular EMG, but this has obvious disadvantages for participant comfort during the execution of facial expressions.</p><p>In the experimental aspect, the inter-subject variability in sEMG measurements revealed some limitations in the muscle synergy analysis. The similarity of muscle synergies across all participants suggests a mostly consistent pattern of muscle activation during the six facial expressions (<xref rid="F6" ref-type="fig">Figure 6B</xref>), which correlates with our experimental design based on FACS. However, the structure of synergy 3 was somewhat variable across all participants. These results may be attributable to at least 3 different factors: individual differences in facial structure across participants, differences in the execution of the task across participants, and inconsistency across participants in the measured sEMG caused by cross-talk (<xref rid="B26" ref-type="bibr">Ekman and Rosenberg, 1997</xref>). Furthermore, participants provided feedback that some of the facial expressions were similar and challenging to differentiate, making it easy to confuse them during a single trial. They noted that certain expressions were difficult to perform and were not commonly used in their regular emotional expressions, which could result in different activation of specific muscles. This observation aligns with the findings that the facial expressions used to convey emotions in non-photographic scenarios differ from the classic expressions outlined in the FACS (<xref rid="B75" ref-type="bibr">Sato et al., 2019</xref>). These variations in muscle activation and expression habits could be contributing factors to the observed differences in muscle synergies among participants. However, these differences do not seem to severely affect performance in the FER task.</p><p>Furthermore, we acknowledge a limitation regarding the reliance on a single operator for muscle palpation, electrode placement, and keypoints identification, without external validation. While the operator underwent extensive training, operator-dependent bias cannot be completely eliminated. Future work should incorporate cross-operator validation or automated placement systems to further enhance the reproducibility and consistency of the experimental protocol.</p><p>In the context of muscle fatigue, although participants were given rest periods during each trial and experiment to minimize the risk of fatigue, no quantitative measures were employed to monitor or evaluate the occurrence of muscle fatigue. Despite the potential influence of muscle fatigue, our results demonstrate high performance in both recognition and estimation tasks. Future work should incorporate methods to quantitatively assess muscle fatigue, particularly to explore its impact on outcomes in longer experiments and real-world application scenarios.</p><p>In the application aspect, the proposed SMSM-LRM model focused on five facial keypoints, which is significantly fewer than the number typically used in facial landmarks detection tasks. However, the proposed model is able to handle the estimation of additional facial keypoint displacements by combining the musculoskeletal model estimations with the linear regression estimations, which allows us to bypass the physical modeling of point-to-point skin interactions. For instance, while the inner and outer points of the eyebrow are influenced by common muscles, their direct interactions are limited. By incorporating the LRM, we can effectively model these types of interrelationships, thereby improving the accuracy of predictions for additional keypoints. We recognize the benefits of including more keypoints and are exploring advances that may allow us to expand our model in future studies. Additionally, we plan to augment the dynamics of the proposed model by employing dynamic models (<xref rid="B14" ref-type="bibr">Chen et al., 2024</xref>; <xref rid="B90" ref-type="bibr">Xu et al., 2024</xref>), focusing on the transition duration between different facial expressions in our future research.</p><p>Finally, regarding the application aspect, the system we propose here is not currently a feasible alternative to vision-based FER. As mentioned above, the placement of the sEMG electrodes makes it difficult to integrate our proposed system into practical applications, especially those involving a direct interaction with a robotic agent. Other studies have explored attaching sEMG sensors to virtual reality (VR) headsets, which could be useful for facial expression generation in VR environments to bypass the use of real-time camera capture, but the placement of the electrodes is limited to the area of the face that the headset covers (<xref rid="B89" ref-type="bibr">Wu et al., 2021</xref>). Therefore, new headset designs and further work into the miniaturization of sEMG electrodes could increase the applicability of our system in VR.</p></sec></sec><sec sec-type="conclusion" id="s5"><title>5 Conclusion</title><p>This study presents a framework for facial expression recognition and generation using facial sEMG signals in the realm of Human-Robot Interaction (HRI). We used muscle synergy analysis to accurately recognize facial expressions and developed a skin-musculoskeletal model with linear regression (SMSM-LRM) to predict the displacement of facial keypoints. We achieved significant advancement in performance in a facial expression recognition task based on sEMG signals and muscle synergy activations. The extracted muscle synergies offer a more detailed understanding of the coordination of muscles during the execution of facial expressions. Additionally, our proposed SMSM-LRM shows high fidelity in estimating facial point displacements, showing potential as a useful tool in the field of facial expression generation. Specifically, the relation between muscle activity and facial motions extracted by our model could create the basis to study relationships between muscle synergies and the coordinated motion of facial keypoints. This could be applied to develop a library of controllers for facial actuators in expressive robots that produce more human-like facial motions. By combining muscle synergy analysis and skin-musculoskeletal dynamics, we provide a new perspective in understanding and replicating human facial expressions, paving the way for more expressive humanoid robots, potentially enhancing human-robot interactions.</p></sec></body><back><sec sec-type="data-availability" id="s6"><title>Data availability statement</title><p>The data presented in this study are available upon reasonable request from the corresponding author.</p></sec><sec sec-type="ethics-statement" id="s7"><title>Ethics statement</title><p>The studies involving humans were approved by Institutional Review Board of Institute of Science Tokyo (Approval No. 2024011). The studies were conducted in accordance with the local legislation and institutional requirements. The participants provided their written informed consent to participate in this study. Written informed consent was obtained from the individual(s) for the publication of any potentially identifiable images or data included in this article.</p></sec><sec sec-type="author-contributions" id="s8"><title>Author contributions</title><p>LS: Conceptualization, Data curation, Methodology, Software, Validation, Visualization, Writing&#x02013;original draft, Writing&#x02013;review and editing. VB: Validation, Writing&#x02013;review and editing. ZQ: Software, Writing&#x02013;review and editing. YK: Conceptualization, Funding acquisition, Validation, Writing&#x02013;review and editing.</p></sec><sec sec-type="COI-statement" id="s10"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="disclaimer" id="s11"><title>Publisher&#x02019;s note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><sec id="s12"><title>Supplementary material</title><p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fbioe.2025.1490919/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fbioe.2025.1490919/full#supplementary-material</ext-link>
</p><supplementary-material id="SM1" position="float" content-type="local-data"><media xlink:href="DataSheet1.pdf"/></supplementary-material></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Antuvan</surname><given-names>C. W.</given-names></name><name><surname>Bisio</surname><given-names>F.</given-names></name><name><surname>Marini</surname><given-names>F.</given-names></name><name><surname>Yen</surname><given-names>S.-C.</given-names></name><name><surname>Cambria</surname><given-names>E.</given-names></name><name><surname>Masia</surname><given-names>L.</given-names></name></person-group> (<year>2016</year>). <article-title>Role of muscle synergies in real-time classification of upper limb motions using extreme learning machines</article-title>. <source>J. Neuroengineering Rehabilitation</source>
<volume>13</volume>, <fpage>76</fpage>&#x02013;<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1186/s12984-016-0183-0</pub-id>
</mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Asheber</surname><given-names>W. T.</given-names></name><name><surname>Lin</surname><given-names>C.-Y.</given-names></name><name><surname>Yen</surname><given-names>S. H.</given-names></name></person-group> (<year>2016</year>). <article-title>Humanoid head face mechanism with expandable facial expressions</article-title>. <source>Int. J. Adv. Robotic Syst.</source>
<volume>13</volume>, <fpage>29</fpage>. <pub-id pub-id-type="doi">10.5772/62181</pub-id>
</mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Barradas</surname><given-names>V. R.</given-names></name><name><surname>Kutch</surname><given-names>J. J.</given-names></name><name><surname>Kawase</surname><given-names>T.</given-names></name><name><surname>Koike</surname><given-names>Y.</given-names></name><name><surname>Schweighofer</surname><given-names>N.</given-names></name></person-group> (<year>2020</year>). <article-title>When 90% of the variance is not enough: residual emg from muscle synergy extraction influences task performance</article-title>. <source>J. Neurophysiology</source>
<volume>123</volume>, <fpage>2180</fpage>&#x02013;<lpage>2190</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00472.2019</pub-id>
<pub-id pub-id-type="pmid">32267198</pub-id>
</mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Belpaeme</surname><given-names>T.</given-names></name><name><surname>Kennedy</surname><given-names>J.</given-names></name><name><surname>Ramachandran</surname><given-names>A.</given-names></name><name><surname>Scassellati</surname><given-names>B.</given-names></name><name><surname>Tanaka</surname><given-names>F.</given-names></name></person-group> (<year>2018</year>). <article-title>Social robots for education: a review</article-title>. <source>Sci. Robotics</source>
<volume>3</volume>, <fpage>eaat5954</fpage>. <pub-id pub-id-type="doi">10.1126/scirobotics.aat5954</pub-id>
</mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Bennett</surname><given-names>C. C.</given-names></name><name><surname>&#x00160;abanovi&#x00107;</surname><given-names>S.</given-names></name></person-group> (<year>2014</year>). <article-title>Deriving minimal features for human-like facial expressions in robotic faces</article-title>. <source>Int. J. Soc. Robotics</source>
<volume>6</volume>, <fpage>367</fpage>&#x02013;<lpage>381</lpage>. <pub-id pub-id-type="doi">10.1007/s12369-014-0237-z</pub-id>
</mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Bennett</surname><given-names>K. J.</given-names></name><name><surname>Pizzolato</surname><given-names>C.</given-names></name><name><surname>Martelli</surname><given-names>S.</given-names></name><name><surname>Bahl</surname><given-names>J. S.</given-names></name><name><surname>Sivakumar</surname><given-names>A.</given-names></name><name><surname>Atkins</surname><given-names>G. J.</given-names></name><etal/></person-group> (<year>2022</year>). <article-title>Emg-informed neuromusculoskeletal models accurately predict knee loading measured using instrumented implants</article-title>. <source>IEEE Trans. Biomed. Eng.</source>
<volume>69</volume>, <fpage>2268</fpage>&#x02013;<lpage>2275</lpage>. <pub-id pub-id-type="doi">10.1109/tbme.2022.3141067</pub-id>
<pub-id pub-id-type="pmid">34990350</pub-id>
</mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Berns</surname><given-names>K.</given-names></name><name><surname>Hirth</surname><given-names>J.</given-names></name></person-group> (<year>2006</year>). <article-title>Control of facial expressions of the humanoid robot head roman</article-title>. <source>2006 IEEE/RSJ Int. Conf. Intelligent Robots Syst.</source>, <fpage>3119</fpage>&#x02013;<lpage>3124</lpage>. <pub-id pub-id-type="doi">10.1109/iros.2006.282331</pub-id>
</mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Blair</surname><given-names>R. J. R.</given-names></name></person-group> (<year>2003</year>). <article-title>Facial expressions, their communicatory functions and neuro&#x02013;cognitive substrates</article-title>. <source>Philosophical Trans. R. Soc. Lond. Ser. B Biol. Sci.</source>
<volume>358</volume>, <fpage>561</fpage>&#x02013;<lpage>572</lpage>. <pub-id pub-id-type="doi">10.1098/rstb.2002.1220</pub-id>
</mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Boughida</surname><given-names>A.</given-names></name><name><surname>Kouahla</surname><given-names>M. N.</given-names></name><name><surname>Lafifi</surname><given-names>Y.</given-names></name></person-group> (<year>2022</year>). <article-title>A novel approach for facial expression recognition based on gabor filters and genetic algorithm</article-title>. <source>Evol. Syst.</source>
<volume>13</volume>, <fpage>331</fpage>&#x02013;<lpage>345</lpage>. <pub-id pub-id-type="doi">10.1007/s12530-021-09393-2</pub-id>
</mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Cameron</surname><given-names>D.</given-names></name><name><surname>Millings</surname><given-names>A.</given-names></name><name><surname>Fernando</surname><given-names>S.</given-names></name><name><surname>Collins</surname><given-names>E. C.</given-names></name><name><surname>Moore</surname><given-names>R.</given-names></name><name><surname>Sharkey</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>The effects of robot facial emotional expressions and gender on child&#x02013;robot interaction in a field study</article-title>. <source>Connect. Sci.</source>
<volume>30</volume>, <fpage>343</fpage>&#x02013;<lpage>361</lpage>. <pub-id pub-id-type="doi">10.1080/09540091.2018.1454889</pub-id>
</mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Cha</surname><given-names>H.-S.</given-names></name><name><surname>Choi</surname><given-names>S.-J.</given-names></name><name><surname>Im</surname><given-names>C.-H.</given-names></name></person-group> (<year>2020</year>). <article-title>Real-time recognition of facial expressions using facial electromyograms recorded around the eyes for social virtual reality applications</article-title>. <source>IEEE Access</source>
<volume>8</volume>, <fpage>62065</fpage>&#x02013;<lpage>62075</lpage>. <pub-id pub-id-type="doi">10.1109/access.2020.2983608</pub-id>
</mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Cha</surname><given-names>H.-S.</given-names></name><name><surname>Im</surname><given-names>C.-H.</given-names></name></person-group> (<year>2022</year>). <article-title>Performance enhancement of facial electromyogram-based facial-expression recognition for social virtual reality applications using linear discriminant analysis adaptation</article-title>. <source>Virtual Real.</source>
<volume>26</volume>, <fpage>385</fpage>&#x02013;<lpage>398</lpage>. <pub-id pub-id-type="doi">10.1007/s10055-021-00575-6</pub-id>
<pub-id pub-id-type="pmid">34493922</pub-id>
</mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Eyebrow emotional expression recognition using surface emg signals</article-title>. <source>Neurocomputing</source>
<volume>168</volume>, <fpage>871</fpage>&#x02013;<lpage>879</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2015.05.037</pub-id>
</mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Zhan</surname><given-names>G.</given-names></name><name><surname>Jiang</surname><given-names>Z.</given-names></name><name><surname>Zhang</surname><given-names>W.</given-names></name><name><surname>Rao</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><etal/></person-group> (<year>2024</year>). <article-title>Adaptive impedance control for docking robot via stewart parallel mechanism</article-title>. <source>ISA Trans.</source>
<volume>155</volume>, <fpage>361</fpage>&#x02013;<lpage>372</lpage>. <pub-id pub-id-type="doi">10.1016/j.isatra.2024.09.008</pub-id>
<pub-id pub-id-type="pmid">39368867</pub-id>
</mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chiovetto</surname><given-names>E.</given-names></name><name><surname>Curio</surname><given-names>C.</given-names></name><name><surname>Endres</surname><given-names>D.</given-names></name><name><surname>Giese</surname><given-names>M.</given-names></name></person-group> (<year>2018</year>). <article-title>Perceptual integration of kinematic components in the recognition of emotional facial expressions</article-title>. <source>J. Vis.</source>
<volume>18</volume>, <fpage>13</fpage>. <pub-id pub-id-type="doi">10.1167/18.4.13</pub-id>
</mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Cohen</surname><given-names>L.</given-names></name><name><surname>Khoramshahi</surname><given-names>M.</given-names></name><name><surname>Salesse</surname><given-names>R. N.</given-names></name><name><surname>Bortolon</surname><given-names>C.</given-names></name><name><surname>S&#x00142;owi&#x00144;ski</surname><given-names>P.</given-names></name><name><surname>Zhai</surname><given-names>C.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Influence of facial feedback during a cooperative human-robot task in schizophrenia</article-title>. <source>Sci. Rep.</source>
<volume>7</volume>, <fpage>15023</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-017-14773-3</pub-id>
<pub-id pub-id-type="pmid">29101325</pub-id>
</mixed-citation></ref><ref id="B17"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Cohn</surname><given-names>J. F.</given-names></name><name><surname>Ekman</surname><given-names>P.</given-names></name></person-group> (<year>2005</year>). &#x0201c;<article-title>Measuring facial action</article-title>,&#x0201d; in <source>The New Handbook of Methods in Nonverbal Behavior Research</source>. Editors <person-group person-group-type="editor"><name><surname>Harrigan</surname><given-names>J. A.</given-names></name><name><surname>Rosenthal</surname><given-names>R.</given-names></name><name><surname>Scherer</surname><given-names>K. R.</given-names></name></person-group> (<publisher-name>Oxford Academic</publisher-name>). <pub-id pub-id-type="doi">10.1093/oso/9780198529613.003.0002</pub-id>
</mixed-citation></ref><ref id="B18"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Darwin</surname><given-names>C.</given-names></name></person-group> (<year>1872</year>). <source>The expression of the emotions in man and animals</source>. <publisher-name>J. Murray</publisher-name>. <comment>No. 31 in Darwin&#x02019;s works</comment>.</mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>d&#x02019;Avella</surname><given-names>A.</given-names></name><name><surname>Bizzi</surname><given-names>E.</given-names></name></person-group> (<year>2005</year>). <article-title>Shared and specific muscle synergies in natural motor behaviors</article-title>. <source>Proc. Natl. Acad. Sci.</source>
<volume>102</volume>, <fpage>3076</fpage>&#x02013;<lpage>3081</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0500199102</pub-id>
<pub-id pub-id-type="pmid">15708969</pub-id>
</mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>d&#x02019;Avella</surname><given-names>A.</given-names></name><name><surname>Portone</surname><given-names>A.</given-names></name><name><surname>Fernandez</surname><given-names>L.</given-names></name><name><surname>Lacquaniti</surname><given-names>F.</given-names></name></person-group> (<year>2006</year>). <article-title>Control of fast-reaching movements by muscle synergy combinations</article-title>. <source>J. Neurosci.</source>
<volume>26</volume>, <fpage>7791</fpage>&#x02013;<lpage>7810</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.0830-06.2006</pub-id>
<pub-id pub-id-type="pmid">16870725</pub-id>
</mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>d&#x02019;Avella</surname><given-names>A.</given-names></name><name><surname>Saltiel</surname><given-names>P.</given-names></name><name><surname>Bizzi</surname><given-names>E.</given-names></name></person-group> (<year>2003</year>). <article-title>Combinations of muscle synergies in the construction of a natural motor behavior</article-title>. <source>Nat. Neurosci.</source>
<volume>6</volume>, <fpage>300</fpage>&#x02013;<lpage>308</lpage>. <pub-id pub-id-type="doi">10.1038/nn1010</pub-id>
<pub-id pub-id-type="pmid">12563264</pub-id>
</mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Delis</surname><given-names>I.</given-names></name><name><surname>Chen</surname><given-names>C.</given-names></name><name><surname>Jack</surname><given-names>R. E.</given-names></name><name><surname>Garrod</surname><given-names>O. G. B.</given-names></name><name><surname>Panzeri</surname><given-names>S.</given-names></name><name><surname>Schyns</surname><given-names>P. G.</given-names></name></person-group> (<year>2016</year>). <article-title>Space-by-time manifold representation of dynamic facial expressions for emotion categorization</article-title>. <source>J. Vis.</source>
<volume>16</volume>, <fpage>14</fpage>. <pub-id pub-id-type="doi">10.1167/16.8.14</pub-id>
</mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Egger</surname><given-names>M.</given-names></name><name><surname>Ley</surname><given-names>M.</given-names></name><name><surname>Hanke</surname><given-names>S.</given-names></name></person-group> (<year>2019</year>). <article-title>Emotion recognition from physiological signal analysis: a review</article-title>. <source>Electron. Notes Theor. Comput. Sci.</source>
<volume>343</volume>, <fpage>35</fpage>&#x02013;<lpage>55</lpage>. <pub-id pub-id-type="doi">10.1016/j.entcs.2019.04.009</pub-id>
</mixed-citation></ref><ref id="B24"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P.</given-names></name></person-group> (<year>1984</year>). &#x0201c;<article-title>Expression and the nature of emotion</article-title>,&#x0201d; in <source>Approaches to emotion</source> (<publisher-loc>New York</publisher-loc>: <publisher-name>Routledge</publisher-name>), <fpage>319</fpage>&#x02013;<lpage>343</lpage>.</mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P.</given-names></name><name><surname>Friesen</surname><given-names>W. V.</given-names></name></person-group> (<year>1969</year>). <article-title>The repertoire of nonverbal behavior: categories, origins, usage, and coding</article-title>. <source>semiotica</source>
<volume>1</volume>, <fpage>49</fpage>&#x02013;<lpage>98</lpage>. <pub-id pub-id-type="doi">10.1515/semi.1969.1.1.49</pub-id>
</mixed-citation></ref><ref id="B26"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P.</given-names></name><name><surname>Rosenberg</surname><given-names>E. L.</given-names></name></person-group> (<year>1997</year>). <source>What the face reveals: basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)</source>. <publisher-loc>USA</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Faraj</surname><given-names>Z.</given-names></name><name><surname>Selamet</surname><given-names>M.</given-names></name><name><surname>Morales</surname><given-names>C.</given-names></name><name><surname>Torres</surname><given-names>P.</given-names></name><name><surname>Hossain</surname><given-names>M.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Facially expressive humanoid robotic face</article-title>. <source>HardwareX</source>
<volume>9</volume>, <fpage>e00117</fpage>. <pub-id pub-id-type="doi">10.1016/j.ohx.2020.e00117</pub-id>
<pub-id pub-id-type="pmid">35492039</pub-id>
</mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Fridlund</surname><given-names>A. J.</given-names></name><name><surname>Cacioppo</surname><given-names>J. T.</given-names></name></person-group> (<year>1986</year>). <article-title>Guidelines for human electromyographic research</article-title>. <source>Psychophysiology</source>
<volume>23</volume>, <fpage>567</fpage>&#x02013;<lpage>589</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-8986.1986.tb00676.x</pub-id>
<pub-id pub-id-type="pmid">3809364</pub-id>
</mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Fu</surname><given-names>D.</given-names></name><name><surname>Abawi</surname><given-names>F.</given-names></name><name><surname>Wermter</surname><given-names>S.</given-names></name></person-group> (<year>2023</year>). <article-title>The robot in the room: influence of robot facial expressions and gaze on human-human-robot collaboration</article-title>. <source>2023 32nd IEEE Int. Conf. Robot Hum. Interact. Commun. (RO-MAN)</source>, <fpage>85</fpage>&#x02013;<lpage>91</lpage>. <pub-id pub-id-type="doi">10.1109/RO-MAN57019.2023.10309334</pub-id>
</mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ghorbandaei Pour</surname><given-names>A.</given-names></name><name><surname>Taheri</surname><given-names>A.</given-names></name><name><surname>Alemi</surname><given-names>M.</given-names></name><name><surname>Meghdari</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>Human&#x02013;robot facial expression reciprocal interaction platform: case studies on children with autism</article-title>. <source>Int. J. Soc. Robotics</source>
<volume>10</volume>, <fpage>179</fpage>&#x02013;<lpage>198</lpage>. <pub-id pub-id-type="doi">10.1007/s12369-017-0461-4</pub-id>
</mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Gonsior</surname><given-names>B.</given-names></name><name><surname>Sosnowski</surname><given-names>S.</given-names></name><name><surname>Mayer</surname><given-names>C.</given-names></name><name><surname>Blume</surname><given-names>J.</given-names></name><name><surname>Radig</surname><given-names>B.</given-names></name><name><surname>Wollherr</surname><given-names>D.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>Improving aspects of empathy and subjective performance for HRI through mirroring facial expressions</article-title>. <source>2011 RO-MAN</source>, <fpage>350</fpage>&#x02013;<lpage>356</lpage>. <pub-id pub-id-type="doi">10.1109/ROMAN.2011.6005294</pub-id>
</mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Gonzalez-Aguirre</surname><given-names>J. A.</given-names></name><name><surname>Osorio-Oliveros</surname><given-names>R.</given-names></name><name><surname>Rodr&#x000ed;guez-Hern&#x000e1;ndez</surname><given-names>K. L.</given-names></name><name><surname>Liz&#x000e1;rraga-Iturralde</surname><given-names>J.</given-names></name><name><surname>Morales Menendez</surname><given-names>R.</given-names></name><name><surname>Ram&#x000ed;rez-Mendoza</surname><given-names>R. A.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Service robots: trends and technology</article-title>. <source>Appl. Sci.</source>
<volume>11</volume>, <fpage>10702</fpage>. <pub-id pub-id-type="doi">10.3390/app112210702</pub-id>
</mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Gonzalez-Franco</surname><given-names>M.</given-names></name><name><surname>Steed</surname><given-names>A.</given-names></name><name><surname>Hoogendyk</surname><given-names>S.</given-names></name><name><surname>Ofek</surname><given-names>E.</given-names></name></person-group> (<year>2020</year>). <article-title>Using facial animation to increase the enfacement illusion and avatar self-identification</article-title>. <source>IEEE Trans. Vis. Comput. Graph.</source>
<volume>26</volume>, <fpage>2023</fpage>&#x02013;<lpage>2029</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2020.2973075</pub-id>
<pub-id pub-id-type="pmid">32070973</pub-id>
</mixed-citation></ref><ref id="B34"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Gruebler</surname><given-names>A.</given-names></name><name><surname>Suzuki</surname><given-names>K.</given-names></name></person-group> (<year>2010</year>). &#x0201c;<article-title>Measurement of distal emg signals using a wearable device for reading facial expressions</article-title>,&#x0201d; in <conf-name>2010 Annual International Conference of the IEEE Engineering in Medicine and Biology</conf-name> (<publisher-name>IEEE</publisher-name>), <fpage>4594</fpage>&#x02013;<lpage>4597</lpage>.</mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Guo</surname><given-names>K.</given-names></name><name><surname>Lincoln</surname><given-names>P.</given-names></name><name><surname>Davidson</surname><given-names>P.</given-names></name><name><surname>Busch</surname><given-names>J.</given-names></name><name><surname>Yu</surname><given-names>X.</given-names></name><name><surname>Whalen</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>The relightables: volumetric performance capture of humans with realistic relighting</article-title>. <source>ACM Trans. Graph.</source>
<volume>38</volume>, <fpage>1</fpage>&#x02013;<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1145/3355089.3356571</pub-id>
</mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Hager</surname><given-names>J. C.</given-names></name><name><surname>Ekman</surname><given-names>P.</given-names></name><name><surname>Friesen</surname><given-names>W. V.</given-names></name></person-group> (<year>2002</year>). <article-title>Facial action coding system</article-title>. <source>Salt Lake City, UT: A Hum. Face</source>
<volume>8</volume>.</mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Hamedi</surname><given-names>M.</given-names></name><name><surname>Salleh</surname><given-names>S.-H.</given-names></name><name><surname>Ting</surname><given-names>C.-M.</given-names></name><name><surname>Astaraki</surname><given-names>M.</given-names></name><name><surname>Noor</surname><given-names>A. M.</given-names></name></person-group> (<year>2016</year>). <article-title>Robust facial expression recognition for muci: a comprehensive neuromuscular signal analysis</article-title>. <source>IEEE Trans. Affect. Comput.</source>
<volume>9</volume>, <fpage>102</fpage>&#x02013;<lpage>115</lpage>. <pub-id pub-id-type="doi">10.1109/taffc.2016.2569098</pub-id>
</mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Harris</surname><given-names>C. M.</given-names></name><name><surname>Wolpert</surname><given-names>D. M.</given-names></name></person-group> (<year>1998</year>). <article-title>Signal-dependent noise determines motor planning</article-title>. <source>Nature</source>
<volume>394</volume>, <fpage>780</fpage>&#x02013;<lpage>784</lpage>. <pub-id pub-id-type="doi">10.1038/29528</pub-id>
<pub-id pub-id-type="pmid">9723616</pub-id>
</mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>He</surname><given-names>Z.</given-names></name><name><surname>Qin</surname><given-names>Z.</given-names></name><name><surname>Koike</surname><given-names>Y.</given-names></name></person-group> (<year>2022</year>). <article-title>Continuous estimation of finger and wrist joint angles using a muscle synergy based musculoskeletal model</article-title>. <source>Appl. Sci.</source>
<volume>12</volume>, <fpage>3772</fpage>. <pub-id pub-id-type="doi">10.3390/app12083772</pub-id>
</mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Jack</surname><given-names>R. E.</given-names></name><name><surname>Garrod</surname><given-names>O. G. B.</given-names></name><name><surname>Yu</surname><given-names>H.</given-names></name><name><surname>Caldara</surname><given-names>R.</given-names></name><name><surname>Schyns</surname><given-names>P. G.</given-names></name></person-group> (<year>2012</year>). <article-title>Facial expressions of emotion are not culturally universal</article-title>. <source>Proc. Natl. Acad. Sci.</source>
<volume>109</volume>, <fpage>7241</fpage>&#x02013;<lpage>7244</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1200155109</pub-id>
<pub-id pub-id-type="pmid">22509011</pub-id>
</mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Johanson</surname><given-names>D. L.</given-names></name><name><surname>Ahn</surname><given-names>H. S.</given-names></name><name><surname>Broadbent</surname><given-names>E.</given-names></name></person-group> (<year>2021</year>). <article-title>Improving interactions with healthcare robots: a review of communication behaviours in social and healthcare contexts</article-title>. <source>Int. J. Soc. Robotics</source>
<volume>13</volume>, <fpage>1835</fpage>&#x02013;<lpage>1850</lpage>. <pub-id pub-id-type="doi">10.1007/s12369-020-00719-9</pub-id>
</mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Johanson</surname><given-names>D. L.</given-names></name><name><surname>Ahn</surname><given-names>H. S.</given-names></name><name><surname>Sutherland</surname><given-names>C. J.</given-names></name><name><surname>Brown</surname><given-names>B.</given-names></name><name><surname>MacDonald</surname><given-names>B. A.</given-names></name><name><surname>Lim</surname><given-names>J. Y.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>Smiling and use of first-name by a healthcare receptionist robot: effects on user perceptions, attitudes, and behaviours</article-title>. <source>Paladyn, J. Behav. Robotics</source>
<volume>11</volume>, <fpage>40</fpage>&#x02013;<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1515/pjbr-2020-0008</pub-id>
</mixed-citation></ref><ref id="B43"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>K&#x000e4;hler</surname><given-names>K.</given-names></name><name><surname>Haber</surname><given-names>J.</given-names></name><name><surname>Seidel</surname><given-names>H.-P.</given-names></name></person-group> (<year>2001</year>). &#x0201c;<article-title>Geometry-based muscle modeling for facial animation</article-title>,&#x0201d; in <source>Proceedings of Graphics Interface 2001</source> (<publisher-loc>Ottawa, ON</publisher-loc>: <publisher-name>Canadian Information Processing Society</publisher-name>), <fpage>37</fpage>&#x02013;<lpage>46</lpage>.</mixed-citation></ref><ref id="B44"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Kawai</surname><given-names>Y.</given-names></name><name><surname>Harajima</surname><given-names>H.</given-names></name></person-group> (<year>2005</year>). <source>Nikutan (NIKUTAN): word book of anatomical English terms with etymological memory aids</source>. <edition>second edition edn</edition>. <publisher-loc>Tokyo</publisher-loc>: <publisher-name>STS Publishing Co., Ltd</publisher-name>.</mixed-citation></ref><ref id="B45"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Kehri</surname><given-names>V.</given-names></name><name><surname>Ingle</surname><given-names>R.</given-names></name><name><surname>Patil</surname><given-names>S.</given-names></name><name><surname>Awale</surname><given-names>R. N.</given-names></name></person-group> (<year>2019</year>). &#x0201c;<article-title>Analysis of facial emg signal for emotion recognition using wavelet packet transform and svm</article-title>,&#x0201d; in <source>Machine intelligence and signal analysis</source>. Editors <person-group person-group-type="editor"><name><surname>Tanveer</surname><given-names>M.</given-names></name><name><surname>Pachori</surname><given-names>R. B.</given-names></name></person-group> (<publisher-loc>Singapore</publisher-loc>: <publisher-name>Springer Singapore</publisher-name>), <fpage>247</fpage>&#x02013;<lpage>257</lpage>.</mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kim</surname><given-names>J.</given-names></name><name><surname>Choi</surname><given-names>M. G.</given-names></name><name><surname>Kim</surname><given-names>Y. J.</given-names></name></person-group> (<year>2020</year>). <article-title>Real-time muscle-based facial animation using shell elements and force decomposition</article-title>. <source>Symposium Interact. 3D Graph. Games</source>, <fpage>1</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1145/3384382.3384531</pub-id>
</mixed-citation></ref><ref id="B97"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>K&#x000f6;stinger</surname><given-names>M.</given-names></name><name><surname>Wohlhart</surname><given-names>P.</given-names></name><name><surname>Roth</surname><given-names>P. M.</given-names></name><name><surname>Bischof</surname><given-names>H.</given-names></name></person-group> (<year>2011</year>). &#x0201c;<article-title>Annotated facial landmarks in the wild: a large-scale, real-world database for facial landmark localization</article-title>,&#x0201d; in <source>IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</source>, <fpage>2144</fpage>&#x02013;<lpage>2151</lpage>. <pub-id pub-id-type="doi">10.1109/ICCVW.2011.6130513</pub-id>
</mixed-citation></ref><ref id="B98"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>K&#x000fc;ntzler</surname><given-names>T.</given-names></name><name><surname>H&#x000f6;fling</surname><given-names>T. T. A.</given-names></name><name><surname>Alpers</surname><given-names>G. W.</given-names></name></person-group> (<year>2021</year>). <article-title>Automatic facial expression recognition in standardized and non-standardized emotional expressions</article-title>. <source>Front. Psychol.</source>
<volume>12</volume>, <fpage>627561</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2021.627561</pub-id>
<pub-id pub-id-type="pmid">34025503</pub-id>
</mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kyrarini</surname><given-names>M.</given-names></name><name><surname>Lygerakis</surname><given-names>F.</given-names></name><name><surname>Rajavenkatanarayanan</surname><given-names>A.</given-names></name><name><surname>Sevastopoulos</surname><given-names>C.</given-names></name><name><surname>Nambiappan</surname><given-names>H. R.</given-names></name><name><surname>Chaitanya</surname><given-names>K. K.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>A survey of robots in healthcare</article-title>. <source>Technologies</source>
<volume>9</volume>, <fpage>8</fpage>. <pub-id pub-id-type="doi">10.3390/technologies9010008</pub-id>
</mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lambert-Shirzad</surname><given-names>N.</given-names></name><name><surname>Van der Loos</surname><given-names>H. M.</given-names></name></person-group> (<year>2017</year>). <article-title>On identifying kinematic and muscle synergies: a comparison of matrix factorization methods using experimental data from the healthy population</article-title>. <source>J. neurophysiology</source>
<volume>117</volume>, <fpage>290</fpage>&#x02013;<lpage>302</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00435.2016</pub-id>
<pub-id pub-id-type="pmid">27852733</pub-id>
</mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lee</surname><given-names>D. D.</given-names></name><name><surname>Seung</surname><given-names>H. S.</given-names></name></person-group> (<year>1999</year>). <article-title>Learning the parts of objects by non-negative matrix factorization</article-title>. <source>Nature</source>
<volume>401</volume>, <fpage>788</fpage>&#x02013;<lpage>791</lpage>. <pub-id pub-id-type="doi">10.1038/44565</pub-id>
<pub-id pub-id-type="pmid">10548103</pub-id>
</mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lee</surname><given-names>Y.</given-names></name><name><surname>Terzopoulos</surname><given-names>D.</given-names></name><name><surname>Waters</surname><given-names>K.</given-names></name></person-group> (<year>1995</year>). <article-title>Realistic modeling for facial animation</article-title>. <source>Proc. 22nd Annu. Conf. Comput. Graph. Interact. Tech.</source>, <fpage>55</fpage>&#x02013;<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1145/218380.218407</pub-id>
</mixed-citation></ref><ref id="B51"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Levenson</surname><given-names>R. W.</given-names></name></person-group> (<year>1994</year>). &#x0201c;<article-title>Human emotion: a functional view</article-title>,&#x0201d; in <source>The nature of emotion: fundamental questions</source> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>).</mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Deng</surname><given-names>W.</given-names></name></person-group> (<year>2020</year>). <article-title>Deep facial expression recognition: a survey</article-title>. <source>IEEE Trans. Affect. Comput.</source>
<volume>13</volume>, <fpage>1195</fpage>&#x02013;<lpage>1215</lpage>. <pub-id pub-id-type="doi">10.1109/taffc.2020.2981446</pub-id>
</mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Liu</surname><given-names>P.</given-names></name><name><surname>Han</surname><given-names>S.</given-names></name><name><surname>Meng</surname><given-names>Z.</given-names></name><name><surname>Tong</surname><given-names>Y.</given-names></name></person-group> (<year>2014</year>). <article-title>Facial expression recognition via a boosted deep belief network</article-title>. <source>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</source>, <fpage>1805</fpage>&#x02013;<lpage>1812</lpage>. <pub-id pub-id-type="doi">10.1109/cvpr.2014.233</pub-id>
</mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lloyd</surname><given-names>D. G.</given-names></name><name><surname>Besier</surname><given-names>T. F.</given-names></name></person-group> (<year>2003</year>). <article-title>An emg-driven musculoskeletal model to estimate muscle forces and knee joint moments <italic>in vivo</italic>
</article-title>. <source>J. biomechanics</source>
<volume>36</volume>, <fpage>765</fpage>&#x02013;<lpage>776</lpage>. <pub-id pub-id-type="doi">10.1016/s0021-9290(03)00010-1</pub-id>
</mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lou</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Nduka</surname><given-names>C.</given-names></name><name><surname>Hamedi</surname><given-names>M.</given-names></name><name><surname>Mavridou</surname><given-names>I.</given-names></name><name><surname>Wang</surname><given-names>F.-Y.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>Realistic facial expression reconstruction for vr hmd users</article-title>. <source>IEEE Trans. Multimedia</source>
<volume>22</volume>, <fpage>730</fpage>&#x02013;<lpage>743</lpage>. <pub-id pub-id-type="doi">10.1109/TMM.2019.2933338</pub-id>
</mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A.</given-names></name><name><surname>Mamidanna</surname><given-names>P.</given-names></name><name><surname>Cury</surname><given-names>K. M.</given-names></name><name><surname>Abe</surname><given-names>T.</given-names></name><name><surname>Murthy</surname><given-names>V. N.</given-names></name><name><surname>Mathis</surname><given-names>M. W.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Deeplabcut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nat. Neurosci.</source>
<volume>21</volume>, <fpage>1281</fpage>&#x02013;<lpage>1289</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id>
<pub-id pub-id-type="pmid">30127430</pub-id>
</mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mehrabian</surname><given-names>A.</given-names></name><name><surname>Ferris</surname><given-names>S. R.</given-names></name></person-group> (<year>1967</year>). <article-title>Inference of attitudes from nonverbal communication in two channels</article-title>. <source>J. Consult. Psychol.</source>
<volume>31</volume>, <fpage>248</fpage>&#x02013;<lpage>252</lpage>. <pub-id pub-id-type="doi">10.1037/h0024648</pub-id>
<pub-id pub-id-type="pmid">6046577</pub-id>
</mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mithbavkar</surname><given-names>S. A.</given-names></name><name><surname>Shah</surname><given-names>M. S.</given-names></name></person-group> (<year>2019</year>). <article-title>Recognition of emotion through facial expressions using emg signal</article-title>. <source>2019 Int. Conf. Nascent Technol. Eng. (ICNTE)</source>, <fpage>1</fpage>&#x02013;<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1109/ICNTE44896.2019.8945843</pub-id>
</mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mithbavkar</surname><given-names>S. A.</given-names></name><name><surname>Shah</surname><given-names>M. S.</given-names></name></person-group> (<year>2021</year>). <article-title>Analysis of emg based emotion recognition for multiple people and emotions</article-title>. <source>2021 IEEE 3rd Eurasia Conf. Biomed. Eng. Healthc. Sustain. (ECBIOS)</source>, <fpage>1</fpage>&#x02013;<lpage>4</lpage>. <pub-id pub-id-type="doi">10.1109/ECBIOS51820.2021.9510858</pub-id>
</mixed-citation></ref><ref id="B60"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Moshkina</surname><given-names>L.</given-names></name></person-group> (<year>2021</year>). <article-title>Improving request compliance through robot affect</article-title>. <source>Proc. AAAI Conf. Artif. Intell.</source>
<volume>26</volume>, <fpage>2031</fpage>&#x02013;<lpage>2037</lpage>. <pub-id pub-id-type="doi">10.1609/aaai.v26i1.8384</pub-id>
</mixed-citation></ref><ref id="B61"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mueller</surname><given-names>N.</given-names></name><name><surname>Trentzsch</surname><given-names>V.</given-names></name><name><surname>Grassme</surname><given-names>R.</given-names></name><name><surname>Guntinas-Lichius</surname><given-names>O.</given-names></name><name><surname>Volk</surname><given-names>G. F.</given-names></name><name><surname>Anders</surname><given-names>C.</given-names></name></person-group> (<year>2022</year>). <article-title>High-resolution surface electromyographic activities of facial muscles during mimic movements in healthy adults: a prospective observational study</article-title>. <source>Front. Hum. Neurosci.</source>
<volume>16</volume>, <fpage>1029415</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2022.1029415</pub-id>
<pub-id pub-id-type="pmid">36579128</pub-id>
</mixed-citation></ref><ref id="B62"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Nakayama</surname><given-names>Y.</given-names></name><name><surname>Takano</surname><given-names>Y.</given-names></name><name><surname>Matsubara</surname><given-names>M.</given-names></name><name><surname>Suzuki</surname><given-names>K.</given-names></name><name><surname>Terasawa</surname><given-names>H.</given-names></name></person-group> (<year>2017</year>). <article-title>The sound of smile: auditory biofeedback of facial emg activity</article-title>. <source>Displays</source>
<volume>47</volume>, <fpage>32</fpage>&#x02013;<lpage>39</lpage>. <pub-id pub-id-type="doi">10.1016/j.displa.2016.09.002</pub-id>
</mixed-citation></ref><ref id="B63"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ortony</surname><given-names>A.</given-names></name><name><surname>Turner</surname><given-names>T. J.</given-names></name></person-group> (<year>1990</year>). <article-title>What&#x02019;s basic about basic emotions?</article-title>
<source>Psychol. Rev.</source>
<volume>97</volume>, <fpage>315</fpage>&#x02013;<lpage>331</lpage>. <pub-id pub-id-type="doi">10.1037//0033-295x.97.3.315</pub-id>
<pub-id pub-id-type="pmid">1669960</pub-id>
</mixed-citation></ref><ref id="B64"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Paluch</surname><given-names>S.</given-names></name><name><surname>Wirtz</surname><given-names>J.</given-names></name><name><surname>Kunz</surname><given-names>W. H.</given-names></name></person-group> (<year>2020</year>). &#x0201c;<article-title>Service robots and the future of services</article-title>,&#x0201d; in <source>Marketing Weiterdenken &#x02013; Zukunftspfade f&#x000fc;r eine marktorientierte Unternehmensf&#x000fc;hrung</source> (<publisher-name>Springer Gabler-Verlag</publisher-name>).</mixed-citation></ref><ref id="B65"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Perusquia-Hernandez</surname><given-names>M.</given-names></name><name><surname>Dollack</surname><given-names>F.</given-names></name><name><surname>Tan</surname><given-names>C. K.</given-names></name><name><surname>Namba</surname><given-names>S.</given-names></name><name><surname>Ayabe-Kanamura</surname><given-names>S.</given-names></name><name><surname>Suzuki</surname><given-names>K.</given-names></name></person-group> (<year>2020</year>). <article-title>Facial movement synergies and action unit detection from distal wearable electromyography and computer vision</article-title>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.2008.08791</pub-id>
</mixed-citation></ref><ref id="B66"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Pumarola</surname><given-names>A.</given-names></name><name><surname>Agudo</surname><given-names>A.</given-names></name><name><surname>Martinez</surname><given-names>A. M.</given-names></name><name><surname>Sanfeliu</surname><given-names>A.</given-names></name><name><surname>Moreno-Noguer</surname><given-names>F.</given-names></name></person-group> (<year>2020</year>). <article-title>Ganimation: one-shot anatomically consistent facial animation</article-title>. <source>Int. J. Comput. Vis.</source>
<volume>128</volume>, <fpage>698</fpage>&#x02013;<lpage>713</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-019-01210-3</pub-id>
</mixed-citation></ref><ref id="B67"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Qin</surname><given-names>Z.</given-names></name><name><surname>He</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Saetia</surname><given-names>S.</given-names></name><name><surname>Koike</surname><given-names>Y.</given-names></name></person-group> (<year>2022</year>). <article-title>A cw-cnn regression model-based real-time system for virtual hand control</article-title>. <source>Front. Neurorobotics</source>
<volume>16</volume>, <fpage>1072365</fpage>. <pub-id pub-id-type="doi">10.3389/fnbot.2022.1072365</pub-id>
</mixed-citation></ref><ref id="B68"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Rabbi</surname><given-names>M. F.</given-names></name><name><surname>Pizzolato</surname><given-names>C.</given-names></name><name><surname>Lloyd</surname><given-names>D. G.</given-names></name><name><surname>Carty</surname><given-names>C. P.</given-names></name><name><surname>Devaprakash</surname><given-names>D.</given-names></name><name><surname>Diamond</surname><given-names>L. E.</given-names></name></person-group> (<year>2020</year>). <article-title>Non-negative matrix factorisation is the most appropriate method for extraction of muscle synergies in walking and running</article-title>. <source>Sci. Rep.</source>
<volume>10</volume>, <fpage>8266</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-020-65257-w</pub-id>
<pub-id pub-id-type="pmid">32427881</pub-id>
</mixed-citation></ref><ref id="B69"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Rajagopal</surname><given-names>A.</given-names></name><name><surname>Dembia</surname><given-names>C. L.</given-names></name><name><surname>DeMers</surname><given-names>M. S.</given-names></name><name><surname>Delp</surname><given-names>D. D.</given-names></name><name><surname>Hicks</surname><given-names>J. L.</given-names></name><name><surname>Delp</surname><given-names>S. L.</given-names></name></person-group> (<year>2016</year>). <article-title>Full-body musculoskeletal model for muscle-driven simulation of human gait</article-title>. <source>IEEE Trans. Biomed. Eng.</source>
<volume>63</volume>, <fpage>2068</fpage>&#x02013;<lpage>2079</lpage>. <pub-id pub-id-type="doi">10.1109/tbme.2016.2586891</pub-id>
<pub-id pub-id-type="pmid">27392337</pub-id>
</mixed-citation></ref><ref id="B70"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Rawal</surname><given-names>N.</given-names></name><name><surname>Stock-Homburg</surname><given-names>R. M.</given-names></name></person-group> (<year>2022</year>). <article-title>Facial emotion expressions in human&#x02013;robot interaction: a survey</article-title>. <source>Int. J. Soc. Robotics</source>
<volume>14</volume>, <fpage>1583</fpage>&#x02013;<lpage>1604</lpage>. <pub-id pub-id-type="doi">10.1007/s12369-022-00867-0</pub-id>
</mixed-citation></ref><ref id="B71"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Reyes</surname><given-names>M. E.</given-names></name><name><surname>Meza</surname><given-names>I. V.</given-names></name><name><surname>Pineda</surname><given-names>L. A.</given-names></name></person-group> (<year>2019</year>). <article-title>Robotics facial expression of anger in collaborative human&#x02013;robot interaction</article-title>. <source>Int. J. Adv. Robotic Syst.</source>
<volume>16</volume>, <fpage>1729881418817972</fpage>. <pub-id pub-id-type="doi">10.1177/1729881418817972</pub-id>
</mixed-citation></ref><ref id="B72"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Rimini</surname><given-names>D.</given-names></name><name><surname>Agostini</surname><given-names>V.</given-names></name><name><surname>Knaflitz</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>Intra-subject consistency during locomotion: similarity in shared and subject-specific muscle synergies</article-title>. <source>Front. Hum. Neurosci.</source>
<volume>11</volume>, <fpage>586</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2017.00586</pub-id>
<pub-id pub-id-type="pmid">29255410</pub-id>
</mixed-citation></ref><ref id="B73"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Root</surname><given-names>A. A.</given-names></name><name><surname>Stephens</surname><given-names>J. A.</given-names></name></person-group> (<year>2003</year>). <article-title>Organization of the central control of muscles of facial expression in man</article-title>. <source>J. Physiology</source>
<volume>549</volume>, <fpage>289</fpage>&#x02013;<lpage>298</lpage>. <pub-id pub-id-type="doi">10.1113/jphysiol.2002.035691</pub-id>
</mixed-citation></ref><ref id="B74"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Sagonas</surname><given-names>C.</given-names></name><name><surname>Antonakos</surname><given-names>E.</given-names></name><name><surname>Tzimiropoulos</surname><given-names>G.</given-names></name><name><surname>Zafeiriou</surname><given-names>S.</given-names></name><name><surname>Pantic</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). <article-title>300 faces in-the-wild challenge: database and results</article-title>. <source>Image Vis. Comput.</source>
<volume>47</volume>, <fpage>3</fpage>&#x02013;<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1016/j.imavis.2016.01.002</pub-id>
</mixed-citation></ref><ref id="B75"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Sato</surname><given-names>W.</given-names></name><name><surname>Hyniewska</surname><given-names>S.</given-names></name><name><surname>Minemoto</surname><given-names>K.</given-names></name><name><surname>Yoshikawa</surname><given-names>S.</given-names></name></person-group> (<year>2019</year>). <article-title>Facial expressions of basic emotions in Japanese laypeople</article-title>. <source>Front. Psychol.</source>
<volume>10</volume>, <fpage>259</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2019.00259</pub-id>
<pub-id pub-id-type="pmid">30809180</pub-id>
</mixed-citation></ref><ref id="B76"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Saunderson</surname><given-names>S.</given-names></name><name><surname>Nejat</surname><given-names>G.</given-names></name></person-group> (<year>2019</year>). <article-title>How robots influence humans: a survey of nonverbal communication in social human&#x02013;robot interaction</article-title>. <source>Int. J. Soc. Robotics</source>
<volume>11</volume>, <fpage>575</fpage>&#x02013;<lpage>608</lpage>. <pub-id pub-id-type="doi">10.1007/s12369-019-00523-0</pub-id>
</mixed-citation></ref><ref id="B77"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Scherer</surname><given-names>K. R.</given-names></name><name><surname>Wallbott</surname><given-names>H. G.</given-names></name></person-group> (<year>1994</year>). <article-title>Evidence for universality and cultural variation of differential emotion response patterning</article-title>. <source>J. personality Soc. Psychol.</source>
<volume>66</volume>, <fpage>310</fpage>&#x02013;<lpage>328</lpage>. <pub-id pub-id-type="doi">10.1037//0022-3514.66.2.310</pub-id>
</mixed-citation></ref><ref id="B78"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Shin</surname><given-names>D.</given-names></name><name><surname>Kim</surname><given-names>J.</given-names></name><name><surname>Koike</surname><given-names>Y.</given-names></name></person-group> (<year>2009</year>). <article-title>A myokinetic arm model for estimating joint torque and stiffness from emg signals during maintained posture</article-title>. <source>J. neurophysiology</source>
<volume>101</volume>, <fpage>387</fpage>&#x02013;<lpage>401</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00584.2007</pub-id>
<pub-id pub-id-type="pmid">19005007</pub-id>
</mixed-citation></ref><ref id="B79"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Sifakis</surname><given-names>E.</given-names></name><name><surname>Neverov</surname><given-names>I.</given-names></name><name><surname>Fedkiw</surname><given-names>R.</given-names></name></person-group> (<year>2005</year>). <article-title>Automatic determination of facial muscle activations from sparse motion capture marker data</article-title>. <source>ACM Trans. Graph.</source>
<volume>24</volume>, <fpage>417</fpage>&#x02013;<lpage>425</lpage>. <pub-id pub-id-type="doi">10.1145/1073204.1073208</pub-id>
</mixed-citation></ref><ref id="B80"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Stenner</surname><given-names>T.</given-names></name><name><surname>Boulay</surname><given-names>C.</given-names></name><name><surname>Grivich</surname><given-names>M.</given-names></name><name><surname>Medine</surname><given-names>D.</given-names></name><name><surname>Kothe</surname><given-names>C.</given-names></name><name><surname>tobiasherzke</surname></name><etal/></person-group> (<year>2023</year>). <source>sccn/liblsl: v1.16.2</source>. <publisher-name>Zenodo</publisher-name>. <pub-id pub-id-type="doi">10.5281/zenodo.7978343</pub-id>
</mixed-citation></ref><ref id="B81"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Stock-Homburg</surname><given-names>R.</given-names></name></person-group> (<year>2022</year>). <article-title>Survey of emotions in human&#x02013;robot interactions: perspectives from robotic psychology on 20 years of research</article-title>. <source>Int. J. Soc. Robotics</source>
<volume>14</volume>, <fpage>389</fpage>&#x02013;<lpage>411</lpage>. <pub-id pub-id-type="doi">10.1007/s12369-021-00778-6</pub-id>
</mixed-citation></ref><ref id="B82"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Takayuki Kanda</surname><given-names>D. E.</given-names></name><name><surname>Hirano</surname><given-names>T.</given-names></name><name><surname>Ishiguro</surname><given-names>H.</given-names></name></person-group> (<year>2004</year>). <article-title>Interactive robots as social partners and peer tutors for children: a field trial</article-title>. <source>Human&#x02013;Computer Interact.</source>
<volume>19</volume>, <fpage>61</fpage>&#x02013;<lpage>84</lpage>. <pub-id pub-id-type="doi">10.1207/s15327051hci1901and2_4</pub-id>
</mixed-citation></ref><ref id="B83"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Tang</surname><given-names>B.</given-names></name><name><surname>Cao</surname><given-names>R.</given-names></name><name><surname>Chen</surname><given-names>R.</given-names></name><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Hua</surname><given-names>B.</given-names></name><name><surname>Wu</surname><given-names>F.</given-names></name></person-group> (<year>2023</year>). &#x0201c;<article-title>Automatic generation of robot facial expressions with preferences</article-title>,&#x0201d; in <conf-name>2023 IEEE International Conference on Robotics and Automation (ICRA)</conf-name> (<publisher-name>IEEE</publisher-name>), <fpage>7606</fpage>&#x02013;<lpage>7613</lpage>.</mixed-citation></ref><ref id="B84"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Tapus</surname><given-names>A.</given-names></name><name><surname>Peca</surname><given-names>A.</given-names></name><name><surname>Aly</surname><given-names>A.</given-names></name><name><surname>Pop</surname><given-names>C.</given-names></name><name><surname>Jisa</surname><given-names>L.</given-names></name><name><surname>Pintea</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2012</year>). <article-title>Children with autism social engagement in interaction with Nao, an imitative robot: a series of single case experiments</article-title>. <source>Interact. Stud.</source>
<volume>13</volume>, <fpage>315</fpage>&#x02013;<lpage>347</lpage>. <pub-id pub-id-type="doi">10.1075/is.13.3.01tap</pub-id>
</mixed-citation></ref><ref id="B85"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Toan</surname><given-names>N. K.</given-names></name><name><surname>Le Duc Thuan</surname><given-names>L. B. L.</given-names></name><name><surname>Thinh</surname><given-names>N. T.</given-names></name></person-group> (<year>2022</year>). <article-title>Development of humanoid robot head based on facs</article-title>. <source>Int. J. Mech. Eng. Robotics Res.</source>
<volume>11</volume>, <fpage>365</fpage>&#x02013;<lpage>372</lpage>. <pub-id pub-id-type="doi">10.18178/ijmerr.11.5.365-372</pub-id>
</mixed-citation></ref><ref id="B86"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Turpin</surname><given-names>N. A.</given-names></name><name><surname>Uriac</surname><given-names>S.</given-names></name><name><surname>Dalleau</surname><given-names>G.</given-names></name></person-group> (<year>2021</year>). <article-title>How to improve the muscle synergy analysis methodology?</article-title>
<source>Eur. J. Appl. physiology</source>
<volume>121</volume>, <fpage>1009</fpage>&#x02013;<lpage>1025</lpage>. <pub-id pub-id-type="doi">10.1007/s00421-021-04604-9</pub-id>
</mixed-citation></ref><ref id="B87"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Waller</surname><given-names>B.</given-names></name><name><surname>Parr</surname><given-names>L.</given-names></name><name><surname>Gothard</surname><given-names>K.</given-names></name><name><surname>Burrows</surname><given-names>A.</given-names></name><name><surname>Fuglevand</surname><given-names>A.</given-names></name></person-group> (<year>2008</year>). <article-title>Mapping the contribution of single muscles to facial movements in the rhesus macaque</article-title>. <source>Physiology and Behav.</source>
<volume>95</volume>, <fpage>93</fpage>&#x02013;<lpage>100</lpage>. <pub-id pub-id-type="doi">10.1016/j.physbeh.2008.05.002</pub-id>
</mixed-citation></ref><ref id="B88"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wolf</surname><given-names>K.</given-names></name></person-group> (<year>2015</year>). <article-title>Measuring facial expression of emotion</article-title>. <source>Dialogues Clin. Neurosci.</source>
<volume>17</volume>, <fpage>457</fpage>&#x02013;<lpage>462</lpage>. <pub-id pub-id-type="doi">10.31887/DCNS.2015.17.4/kwolf</pub-id>
<pub-id pub-id-type="pmid">26869846</pub-id>
</mixed-citation></ref><ref id="B89"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y.</given-names></name><name><surname>Kakaraparthi</surname><given-names>V.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Pham</surname><given-names>T.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Nguyen</surname><given-names>P.</given-names></name></person-group> (<year>2021</year>). &#x0201c;<article-title>Bioface-3d: continuous 3d facial reconstruction through lightweight single-ear biosensors</article-title>,&#x0201d; in <conf-name>Proceedings of the 27th Annual International Conference on Mobile Computing and Networking</conf-name> (<publisher-loc>New York, NY, USA</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>), <fpage>350</fpage>&#x02013;<lpage>363</lpage>.</mixed-citation></ref><ref id="B90"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Deng</surname><given-names>C.</given-names></name><name><surname>Wang</surname><given-names>S.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name></person-group> (<year>2024</year>). <article-title>Lcdl: toward dynamic localization for autonomous landing of unmanned aerial vehicle based on lidar&#x02013;camera fusion</article-title>. <source>IEEE Sensors J.</source>
<volume>24</volume>, <fpage>26407</fpage>&#x02013;<lpage>26415</lpage>. <pub-id pub-id-type="doi">10.1109/JSEN.2024.3424218</pub-id>
</mixed-citation></ref><ref id="B91"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>Ge</surname><given-names>S. S.</given-names></name><name><surname>Lee</surname><given-names>T. H.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name></person-group> (<year>2008</year>). <article-title>Facial expression recognition and tracking for intelligent human-robot interaction</article-title>. <source>Intell. Serv. Robot.</source>
<volume>1</volume>, <fpage>143</fpage>&#x02013;<lpage>157</lpage>. <pub-id pub-id-type="doi">10.1007/s11370-007-0014-z</pub-id>
</mixed-citation></ref><ref id="B92"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Yu</surname><given-names>Z.</given-names></name><name><surname>Liu</surname><given-names>Q.</given-names></name><name><surname>Liu</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>). <article-title>Deeper cascaded peak-piloted network for weak expression recognition</article-title>. <source>Vis. Comput.</source>
<volume>34</volume>, <fpage>1691</fpage>&#x02013;<lpage>1699</lpage>. <pub-id pub-id-type="doi">10.1007/s00371-017-1443-0</pub-id>
</mixed-citation></ref><ref id="B93"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>K.</given-names></name><name><surname>Zheng</surname><given-names>J.</given-names></name></person-group> (<year>2022a</year>). <article-title>Facial expression retargeting from human to avatar made easy</article-title>. <source>IEEE Trans. Vis. Comput. Graph.</source>
<volume>28</volume>, <fpage>1274</fpage>&#x02013;<lpage>1287</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2020.3013876</pub-id>
<pub-id pub-id-type="pmid">32746288</pub-id>
</mixed-citation></ref><ref id="B94"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name><name><surname>Shone</surname><given-names>F.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Frangi</surname><given-names>A. F.</given-names></name><name><surname>Xie</surname><given-names>S. Q.</given-names></name><etal/></person-group> (<year>2022b</year>). <article-title>Physics-informed deep learning for musculoskeletal modeling: predicting muscle forces and joint kinematics from surface emg</article-title>. <source>IEEE Trans. Neural Syst. Rehabilitation Eng.</source>
<volume>31</volume>, <fpage>484</fpage>&#x02013;<lpage>493</lpage>. <pub-id pub-id-type="doi">10.1109/tnsre.2022.3226860</pub-id>
</mixed-citation></ref><ref id="B95"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Prakash</surname><given-names>E. C.</given-names></name><name><surname>Sung</surname><given-names>E.</given-names></name></person-group> (<year>2001</year>). &#x0201c;<article-title>Animation of facial expressions by physical modeling</article-title>,&#x0201d; in <source>Eurographics 2001 - short presentations</source> (<publisher-loc>Netherlands</publisher-loc>: <publisher-name>Eurographics Association</publisher-name>).</mixed-citation></ref><ref id="B96"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Luo</surname><given-names>P.</given-names></name><name><surname>Loy</surname><given-names>C. C.</given-names></name><name><surname>Tang</surname><given-names>X.</given-names></name></person-group> (<year>2018</year>). <article-title>From facial expression recognition to interpersonal relation prediction</article-title>. <source>Int. J. Comput. Vis.</source>
<volume>126</volume>, <fpage>550</fpage>&#x02013;<lpage>569</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-017-1055-1</pub-id>
</mixed-citation></ref></ref-list></back></article>