<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408435</article-id><article-id pub-id-type="pmc">PMC12101736</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0322504</article-id><article-id pub-id-type="publisher-id">PONE-D-25-00173</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Nonverbal Communication</subject><subj-group><subject>Facial Expressions</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Nonverbal Communication</subject><subj-group><subject>Facial Expressions</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Cerebral Hemispheres</subject><subj-group><subject>Left Hemisphere</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Cerebral Hemispheres</subject><subj-group><subject>Left Hemisphere</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Cerebral Hemispheres</subject><subj-group><subject>Right Hemisphere</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Cerebral Hemispheres</subject><subj-group><subject>Right Hemisphere</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Fear</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Fear</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Effect of verbal interference and response hand on hemisphere asymmetries in sad facial expression processing</article-title><alt-title alt-title-type="running-head">Effect of verbal interference and response hand on hemisphere asymmetries in sad facial expression processing</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2611-1337</contrib-id><name><surname>Burgund</surname><given-names>E. Darcy</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0004-6699-3188</contrib-id><name><surname>Cushing</surname><given-names>Solana R.</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Saad</surname><given-names>Moura</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Department of Psychology, Macalester College, Saint Paul, MN,</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Neuroscience Program, Macalester College, Saint Paul, MN</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Rezaei</surname><given-names>Mehdi</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>University of Birjand, IRAN, ISLAMIC REPUBLIC OF</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>dburgund@macalester.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0322504</elocation-id><history><date date-type="received"><day>3</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>23</day><month>3</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Burgund et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Burgund et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0322504.pdf">
</self-uri><abstract><p>A growing amount of evidence highlights a role for the left hemisphere in negative facial expression processing. The present study investigated the extent to which language contributes to this left hemisphere involvement by comparing performance during an emotion detection task presented to the left and right hemispheres using divided visual field under conditions of verbal interference (covertly rehearsing a 6-digit string for a subsequent memory) and no interference. Participants were college undergraduates with no known neurological or psychiatric conditions. Half used their right hand to respond and half used their left. In line with the hypothesis that language contributes to left hemisphere involvement in negative expression processing, participants who used their right hand to respond were more accurate with sad facial expressions when they were presented to the left hemisphere than the right during the no interference condition, but this left-hemisphere advantage disappeared during the verbal interference condition. Contrary to the hypothesis, participants who used their left hand to respond were more accurate with sad facial expressions when they were presented to the right hemisphere than when they were presented to the left, and this right-hemisphere advantage did not differ significantly between interference groups. Results highlight the influence of language as well as response hand on hemisphere asymmetries in facial expression processing and point towards areas for future research.</p></abstract><funding-group><funding-statement>The author(s) received no specific funding for this work.</funding-statement></funding-group><counts><fig-count count="1"/><table-count count="3"/><page-count count="12"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>Data from the study are available from <ext-link xlink:href="https://doi.org/10.5061/dryad.r2280gbpw" ext-link-type="uri">https://doi.org/10.5061/dryad.r2280gbpw</ext-link>
</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>Data from the study are available from <ext-link xlink:href="https://doi.org/10.5061/dryad.r2280gbpw" ext-link-type="uri">https://doi.org/10.5061/dryad.r2280gbpw</ext-link>
</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>The ability to recognize others&#x02019; facial expressions is critical for successful interactions in a socially complex world. Research investigating hemisphere asymmetries in facial expression processing has demonstrated an important role for areas within the right hemisphere (e.g., [<xref rid="pone.0322504.ref001" ref-type="bibr">1</xref>&#x02013;<xref rid="pone.0322504.ref009" ref-type="bibr">9</xref>]), especially for negative facial expression processing (e.g., [<xref rid="pone.0322504.ref010" ref-type="bibr">10</xref>&#x02013;<xref rid="pone.0322504.ref019" ref-type="bibr">19</xref>]). This research is in line with general theories of hemisphere asymmetries in emotion processing, which argue that the right hemisphere is superior to the left (e.g., [<xref rid="pone.0322504.ref020" ref-type="bibr">20</xref>]) and that this is especially true for negative emotions (e.g., [<xref rid="pone.0322504.ref021" ref-type="bibr">21</xref>]). In contrast to this dominant perspective, a growing amount of evidence has highlighted situations in which the left hemisphere is dominant for negative facial expression processing [<xref rid="pone.0322504.ref022" ref-type="bibr">22</xref>&#x02013;<xref rid="pone.0322504.ref028" ref-type="bibr">28</xref>]. The reasons for this left hemisphere involvement, however, are unclear.</p><p>One important factor may be the extent to which language is involved in facial expression processing, as left hemisphere regions are dominant for language in most people. The influence of language on facial expression processing has been demonstrated in a number of studies in which facial expression processing is affected by the presentation of emotion labels [<xref rid="pone.0322504.ref029" ref-type="bibr">29</xref>&#x02013;<xref rid="pone.0322504.ref032" ref-type="bibr">32</xref>]. For example, Lindquist et al. [<xref rid="pone.0322504.ref031" ref-type="bibr">31</xref>] observed decreased facial expression categorization performance after labels describing the expressions (e.g., &#x0201c;sad&#x0201d;) had been made less accessible through satiation, while Nook et al. [<xref rid="pone.0322504.ref032" ref-type="bibr">32</xref>] observed increased facial expression recognition when expressions were paired with emotion labels. Moreover, the influence of language on facial expression processing has been tied to the left hemisphere. Burt and Hausmann [<xref rid="pone.0322504.ref024" ref-type="bibr">24</xref>] observed an effect of linguistic labels on facial expression processing for faces presented to the left hemisphere, but not for faces presented to the right hemisphere. Thus, it is possible that left hemisphere advantages in facial expression perception, especially those for negative facial expressions, are due to the influence of language on processing.</p><p>This hypothesis was tested in a recent study by Burgund [<xref rid="pone.0322504.ref023" ref-type="bibr">23</xref>] in which the six basic facial expressions (angry, disgusted, fearful, happy, sad, surprised) were presented to the left and right hemispheres using divided visual field. In Experiment 1, participants identified which facial expression was presented by pushing one of six keys corresponding to the six expressions. This task required participants to select linguistic labels for the expressions and thus required the use of language. In Experiment 2, participants simply detected whether facial expressions were emotional or neutral and were not required to identify which facial expression was presented. This detection task, therefore, did not require the use of language to the same extent that was required by the identification task. Results for fearful facial expressions were in line with the hypothesis that left hemisphere advantages in negative facial expression perception are due to the influence of language on processing&#x02014;a left hemisphere advantage was observed during the identification task but not during the detection task. Notably, however, results for sad facial expressions were not, with equivalent left hemisphere advantages observed during the identification and detection tasks. Thus, language involvement may not be the entire explanation for left hemisphere advantages in negative facial expression processing.</p><p>Nonetheless, it is possible that language influenced processing during the facial emotion detection task even though it was not explicitly required. For example, participants could have mentally labeled the facial expressions (e.g., &#x0201c;sad&#x0201d;) and then decided whether the expression was emotional or not. If this happened, then the emotion detection task used in Burgund [<xref rid="pone.0322504.ref023" ref-type="bibr">23</xref>] may not have been as free of language influence as assumed, and language processing could have contributed to the left hemisphere advantage in this task. Indeed, several other studies observing left hemisphere advantages for negative facial expressions do not use tasks that require language explicitly [<xref rid="pone.0322504.ref025" ref-type="bibr">25</xref>&#x02013;<xref rid="pone.0322504.ref028" ref-type="bibr">28</xref>], and therefore explicit language use cannot be a requirement if language influence is the explanation for left hemisphere advantages for negative facial expressions. As such, a manipulation that interferes with participants&#x02019; ability to use language during a non-linguistic facial expression task is needed in order to more fully examine this issue.</p><p>One way in which the influence of language on non-linguistic processing is interfered with is by having participants perform a verbal task at the same time as the primary task (see, e.g., [<xref rid="pone.0322504.ref033" ref-type="bibr">33</xref>]). This method assumes that performing the verbal task uses the language processing resources so that they cannot influence performance of the primary task. Memory-based tasks are a commonly used type of interference task in which participants are asked to covertly rehearse verbal material (often a string of 6&#x02013;9 digits) while performing the primary task, and memory for the verbal material is tested subsequently (e.g., [<xref rid="pone.0322504.ref034" ref-type="bibr">34</xref>,<xref rid="pone.0322504.ref035" ref-type="bibr">35</xref>]). Thus, the present study used a memory-based verbal interference task to assess the influence of language on facial expression processing. In the primary task, participants detected whether facial expressions presented to the left or right hemisphere using divided visual-field were emotional or neutral, as in Burgund [<xref rid="pone.0322504.ref023" ref-type="bibr">23</xref>] and others [<xref rid="pone.0322504.ref018" ref-type="bibr">18</xref>,<xref rid="pone.0322504.ref028" ref-type="bibr">28</xref>]. Half of the participants performed this primary task while covertly rehearsing a 6-digit string for a subsequent memory test (verbal interference) and half simply performed the task (no interference). If language processing contributes to the left hemisphere advantage for negative facial expressions, then this advantage should be reduced in the verbal interference condition compared to the no interference condition. If language processing does not contribute to the left hemisphere advantage, then the advantage should not be reduced by interference.</p><p>The present study also examined the effect of response hand on hemisphere asymmetries in facial expression processing by having half of the participants in each interference group use their right (dominant) hand to respond and half use their left (non-dominant) hand to respond. Although response hand has an effect on visual hemisphere asymmetries in multiple contexts (e.g., [<xref rid="pone.0322504.ref036" ref-type="bibr">36</xref>&#x02013;<xref rid="pone.0322504.ref040" ref-type="bibr">40</xref>]), previous studies using the divided-visual field emotion-detection task used in the present study have not observed response-hand effects [<xref rid="pone.0322504.ref018" ref-type="bibr">18</xref>,<xref rid="pone.0322504.ref023" ref-type="bibr">23</xref>]. However, these studies manipulated hand within participants, by switching response hand between blocks of trials, which decreased the number of trials per hand per participant, and may have reduced the studies&#x02019; sensitivity to a hand effect. As such, the between-participants manipulation of response hand in the present study was included in order to test this possibility. Critically, we did not expect response hand to have an effect.</p></sec><sec id="sec002"><title>Method</title><sec id="sec003"><title>Ethics statement</title><p>The present experiment was approved by the Institutional Review Board at Macalester College (Approval number: 22010). Participants provided written consent in accordance with the guidelines established by the Code of Ethics of the World Medical Association (Declaration of Helsinki).</p></sec><sec id="sec004"><title>Participants</title><p>Data collection occurred between April 12, 2022 and December 4, 2023. The initial participant sample consisted of 172 right-handed young adults with normal or corrected-to-normal vision recruited from Introduction to Psychology courses at Macalester College and the larger Macalester student body. However, since previous research indicates atypical hemisphere asymmetries in many common psychological and neurodevelopmental disorders (see, e.g., [<xref rid="pone.0322504.ref041" ref-type="bibr">41</xref>&#x02013;<xref rid="pone.0322504.ref043" ref-type="bibr">43</xref>]), people who had been diagnosed with a neurological or psychological condition within the last five years and/or were currently taking medications to treat a diagnosed neurological or psychological condition (<italic toggle="yes">N</italic>&#x02009;=&#x02009;52) were excluded.</p><p>The remaining 120 participants were distributed roughly evenly across the interference and the response hand conditions, as shown in <xref rid="pone.0322504.t001" ref-type="table">Table 1</xref>. All participants were between 18&#x02013;24 years old (<italic toggle="yes">M</italic>&#x02009;=&#x02009;20.20, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;1.29). Approximately half (55%) identified as racially White; and participants of color identified as Asian (16%), mixed race (11%), Black (8%), Hispanic/Latinx (6%), Middle Eastern (2%), and South Asian (2%). The majority of participants (84%) spent most of their childhood in the United States of America. The next largest percentages were from Brazil and China with 2.5% each.</p><table-wrap position="float" id="pone.0322504.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0322504.t001</object-id><label>Table 1</label><caption><title>Number, gender, and handedness for interference &#x000d7; response hand conditions.</title></caption><alternatives><graphic xlink:href="pone.0322504.t001" id="pone.0322504.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="4" rowspan="1">Response hand</th></tr><tr><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="2" rowspan="1">Left hand</th><th align="left" colspan="2" rowspan="1">Right hand</th></tr><tr><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="2" rowspan="1">Interference</th><th align="left" colspan="2" rowspan="1">Interference</th></tr><tr><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1">None</th><th align="left" rowspan="1" colspan="1">Verbal</th><th align="left" rowspan="1" colspan="1">None</th><th align="left" rowspan="1" colspan="1">Verbal</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">N</italic>
</td><td align="left" rowspan="1" colspan="1">27</td><td align="left" rowspan="1" colspan="1">28</td><td align="left" rowspan="1" colspan="1">31</td><td align="left" rowspan="1" colspan="1">34</td></tr><tr><td align="left" rowspan="1" colspan="1">Gender (f/m)</td><td align="left" rowspan="1" colspan="1">14/13</td><td align="left" rowspan="1" colspan="1">16/12</td><td align="left" rowspan="1" colspan="1">15/16</td><td align="left" rowspan="1" colspan="1">23/11</td></tr><tr><td align="left" rowspan="1" colspan="1">Handedness</td><td align="left" rowspan="1" colspan="1">.94 (.11)</td><td align="left" rowspan="1" colspan="1">.96 (.10)</td><td align="left" rowspan="1" colspan="1">.96 (.09)</td><td align="left" rowspan="1" colspan="1">.94 (.12)</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t001fn001"><p>&#x02018;<italic toggle="yes">N</italic>&#x02019; indicates the number of participants in each group. &#x02018;Gender&#x02019; provides the number of female (f) and male (m) participants (there were not any nonbinary participants in the sample). &#x02018;Handedness&#x02019; is the mean laterality quotient as determined by the Edinburgh Inventory [<xref rid="pone.0322504.ref044" ref-type="bibr">44</xref>], which produces scores from &#x02010;1.0 (strongly left-handed) to 1.0 (strongly right-handed); parentheses indicate the standard deviation of the mean.</p></fn></table-wrap-foot></table-wrap></sec><sec id="sec005"><title>Design</title><p>The experiment employed a 6 (expression: angry vs. disgusted vs. fearful vs. happy vs. sad vs. surprised) x 2 (hemisphere: left vs. right) x 2 (interference: none vs. verbal) x 2 (response hand: left hand vs. right hand) mixed-factorial design in which expression and hemisphere were within-participants independent variables, and interference and response hand were between-participants independent variables. A sensitivity analysis using G*Power 3.1 [<xref rid="pone.0322504.ref045" ref-type="bibr">45</xref>] revealed that, with an alpha of.05 and 80% power, 120 participants is sufficient to detect a small effect size of &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;.016 in this design. The dependent variable was sensitivity measured by d&#x02019; [<xref rid="pone.0322504.ref046" ref-type="bibr">46</xref>].</p></sec><sec id="sec006"><title>Materials</title><p>Stimuli were photographs of 16 White individuals (8 female; 8 male) displaying 7 different expressions (angry, disgusted, fearful, happy, sad, surprised, and neutral) taken from the Radboud Faces Database [<xref rid="pone.0322504.ref047" ref-type="bibr">47</xref>]. Faces were depicted from the straight-on view gazing directly at the camera, and were gray-scaled, cropped to an oval shape that excluded the ears and hair, and presented on a white background. Each face subtended a visual angle of approximately 5.7 x 3.6&#x002da; in vertical and horizontal dimensions, respectively, and was presented in the left or right visual field such that the center of each was approximately 5.4&#x002da; from the center of the display and the inner edge was approximately 3.6&#x002da; from the center. Participants placed their chins in a chinrest to keep their eyes approximately 20 inches from the computer screen. Response recording and stimulus presentation were controlled using PsyToolkit [<xref rid="pone.0322504.ref048" ref-type="bibr">48</xref>,<xref rid="pone.0322504.ref049" ref-type="bibr">49</xref>] on a Macintosh computer.</p></sec><sec id="sec007"><title>Procedure</title><p>Participants began the study by completing a consent form that described the nature of the tasks they would perform. They were then randomly assigned to the no interference or the verbal interference condition. Both groups of participants then performed the emotion detection task in which they decided whether faces displayed emotional expressions or not. Each trial began with the presentation of a fixation cross (+) in the center of the screen for 1500&#x02009;ms and was followed by the presentation of a face in the left or right visual field for 150&#x02009;ms. After the face presentation, the center fixation cross remained on the screen until the participant pushed a key indicating their response and ended the trial. Within each interference condition, half of the participants used the index and middle fingers of their left hand to respond, with the &#x02018;E&#x02019; key indicating &#x02018;emotional&#x02019; and the &#x02018;D&#x02019; key indicating &#x02018;not emotional&#x02019;, and half of the participants used the index and middle fingers their right hand to respond, with the &#x02018;P&#x02019; key indicating &#x02018;emotional&#x02019; and the &#x02018;L&#x02019; key indicating &#x02018;not emotional&#x02019;. Participants were instructed to keep their eyes focused on the fixation cross throughout, and to respond as quickly and accurately as possible when each face appeared. Six practice trials were administered before the experimental trials began to ensure that participants understood the task instructions and familiarize them with the brief divided-visual field presentations and response keys. Experimental trials were administered in four blocks of 48 trials each, 24 emotional, with 4 for each of the 6 emotional facial expressions, and 24 not emotional, with half presented in the left visual field and half presented in the right. Stimuli within each block were presented in a pseudorandom order that was constrained such that no more than three of the same type of expression or visual field were presented in a row.</p><p>The difference between the no interference and verbal interference conditions was that participants in the verbal interference condition performed the above task while silently rehearsing a 6-digit number in order to recall it for a subsequent memory test. Participants in the verbal interference condition were instructed as follows:</p><disp-quote><p>&#x0201c;While you perform the facial expression task, you will be asked to remember a 6-digit number. An effective way to remember the number is to recite it over and over to yourself. The number will appear on the screen immediately before you begin the facial expression task, and you should remember it while you perform the task. At the end, you will be asked to enter the number.&#x0201d;</p></disp-quote><p>At the beginning of each block of trials, a 6-digit number appeared in the center of the screen. Participants looked at the number for as long as they wanted and pressed the spacebar when they were ready to begin the emotion detection task. After a block of 48 trials, participants were prompted to type in the 6-digit number. Then, when they were ready, they viewed another 6-digit number and completed the next block of trials. Participants completed 4 blocks of trials and remembered 4 different 6-digit numbers.</p><p>After completing 4 blocks of trials, all participants completed the Edinburgh Inventory to assess their handedness [<xref rid="pone.0322504.ref044" ref-type="bibr">44</xref>] and reported demographic information (age, gender, race, and countries lived in) as well as whether they had ever been diagnosed with a neurological or psychological condition, and if so, what condition and at what age, and whether they currently take medication to treat it. Participants were then debriefed, thanked, and compensated. The entire study took between 20&#x02013;30 minutes.</p></sec></sec><sec sec-type="results" id="sec008"><title>Results</title><p>Performance on the 6-digit number memory test in the verbal interference condition was assessed in terms of a strict and lenient coding of accuracy. In the strict coding of accuracy, a response was only considered correct when it matched the 6-digit number exactly (the correct digits in the correct order). Participants were moderately accurate using this strict coding, with mean accuracy of 85% (<italic toggle="yes">SD</italic>&#x02009;=&#x02009;20%) in the left response-hand group and mean accuracy of 90% (<italic toggle="yes">SD</italic>&#x02009;=&#x02009;15%) in the right response-hand group, which did not differ from each other, <italic toggle="yes">t</italic>(60) = 1.10, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.274, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.142, 95% CI [-.112,.396]. In the lenient coding of accuracy, a response was considered correct if 5 of the 6 digits were correct and in the correct order, or if all 6 digits were correct but two were reversed in order. With this more lenient coding of accuracy, participants in the left response-hand group had a mean accuracy of 93% (<italic toggle="yes">SD</italic>&#x02009;=&#x02009;15%) and participants in the right response-hand group had a mean accuracy of 94% (<italic toggle="yes">SD</italic>&#x02009;=&#x02009;12%). These means did not differ, <italic toggle="yes">t</italic>(60) =.362, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.718, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.047, 95% CI [-.206,.300].</p><p>Performance on the emotion detection task was assessed in terms of d&#x02019;, calculated as the difference between the normalized proportion of hits (correct response that a face was emotional) minus the normalized proportion of false alarms (incorrect response that a face was emotional). Because values of d&#x02019; are undefined when hit or false alarm rates are 1 or 0, proportions of 0 were converted to 1/(2<italic toggle="yes">N</italic>) and proportions of 1 were converted to 1-(1/(2<italic toggle="yes">N</italic>)), with <italic toggle="yes">N</italic> referring to the number of trials contributing to the proportion.</p><p>d&#x02019; scores were analyzed in a 6 (expression: angry vs. disgusted vs. fearful vs. happy vs. sad vs. surprised) x 2 (hemisphere: left vs. right) x 2 (interference: none vs. verbal) x 2 (response hand: left hand vs. right hand) repeated-measures analysis of variance (ANOVA) in which expression and hemisphere were within-participants independent variables, and interference and response hand were between-participants independent variables. Results are shown in <xref rid="pone.0322504.g001" ref-type="fig">Fig 1</xref>.</p><fig position="float" id="pone.0322504.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0322504.g001</object-id><label>Fig 1</label><caption><title>Mean d&#x02019; scores displayed as a function of expression, hemisphere, interference, and response hand.</title><p>Vertical bars indicate standard error of the mean.</p></caption><graphic xlink:href="pone.0322504.g001" position="float"/></fig><p>The main effect of interference was significant, <italic toggle="yes">F</italic>(1, 116) = 4.18, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.043, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;.035, 95% CI [-.032,.101]. d&#x02019; scores were higher in the no interference condition (<italic toggle="yes">M</italic>&#x02009;=&#x02009;2.04, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.452) than the verbal interference condition (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.86, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.466). The main effect of expression was also significant, <italic toggle="yes">F</italic>(5, 580) = 115.64, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;.166, 95% CI [.106,.227]. d&#x02019; scores were highest for happy (<italic toggle="yes">M</italic>&#x02009;=&#x02009;2.22, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.504), disgusted (<italic toggle="yes">M</italic>&#x02009;=&#x02009;2.19, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.499), and surprised (<italic toggle="yes">M</italic>&#x02009;=&#x02009;2.18, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.533) expressions, which did not differ, happy vs. disgusted: <italic toggle="yes">t</italic>(119) = 1.41, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.161, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.129, 95% CI [-.051,.310]; happy vs. surprised: <italic toggle="yes">t</italic>(119) = 1.53, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.130, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.140, 95% CI [-.040,.321]; disgus<italic toggle="yes">t</italic>ed vs. surprised: <italic toggle="yes">t</italic>(119) =.150, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.881, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.014, 95% CI [-.166,.193]. d&#x02019; scores for fearful expressions were lower (<italic toggle="yes">M</italic>&#x02009;=&#x02009;2.04, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.535), fearful vs. happy: <italic toggle="yes">t</italic>(119) = 5.37, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.492, 95% CI [.302,.683]; fearful vs. disgus<italic toggle="yes">t</italic>ed: <italic toggle="yes">t</italic>(119) = 4.62, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.424, 95% CI [.263,.611]; fearful vs. surprised: <italic toggle="yes">t</italic>(119) = 5.15, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.472, 95% CI [.283,.662]. d&#x02019; scores for angry (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.50, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.608) and sad (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.56, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.633) expressions were lower, angry vs. fearful: <italic toggle="yes">t</italic>(119) = 11.13, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic toggle="yes">d</italic>&#x02009;=&#x02009;1.02, 95% CI [.799, 1.24]; sad vs. fearful: <italic toggle="yes">t</italic>(119) = 10.52, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.964, 95% CI [.747, 1.18], and did no<italic toggle="yes">t</italic> differ from each other, <italic toggle="yes">t</italic>(119) = 1.36, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.176, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.125, 95% CI [-.056,.305]. The expression x response hand interac<italic toggle="yes">t</italic>ion, <italic toggle="yes">F</italic>(5, 580) = 2.48, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.031, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;.004, 95% CI [-.006,.015], and the qualifying four-way interaction of expression x hemisphere x in<italic toggle="yes">t</italic>erference x response hand, <italic toggle="yes">F</italic>(5, 580) = 3.14, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.008, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;.005, 95% CI [-.007,.017], were also significant (all other <italic toggle="yes">p</italic>s&#x02009;&#x0003e;&#x02009;.107).</p><p>The four-way interaction was explored further by examining the three-way interaction of hemisphere x interference x response hand separately for each expression (see <xref rid="pone.0322504.t002" ref-type="table">Table 2</xref>). The only expressions that exhibited significant effects in these analyses were angry and sad. For angry expressions (see <xref rid="pone.0322504.g001" ref-type="fig">Fig 1A</xref>), d&#x02019; scores were higher in the no interference condition (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.64, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.588) than the verbal interference condition (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.38, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.606), <italic toggle="yes">F</italic>(1, 116) = 5.01, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.027, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;.041, 95% CI [-.031,.114], for the main effect of interference condition. Similarly, for sad expressions (see <xref rid="pone.0322504.g001" ref-type="fig">Fig 1E</xref>), d&#x02019; scores were higher in the no interference condition (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.70, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.642) than the verbal interference condition (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.43, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.601), <italic toggle="yes">F</italic>(1, 116) = 5.47, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.021, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;.045, 95% CI [-.030,.121], for the main effect of interference condition.</p><table-wrap position="float" id="pone.0322504.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0322504.t002</object-id><label>Table 2</label><caption><title><italic toggle="yes">F</italic> values for effects from analysis of d&#x02019; in hemisphere x interference x response hand ANOVA for each expression.</title></caption><alternatives><graphic xlink:href="pone.0322504.t002" id="pone.0322504.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" colspan="7" rowspan="1">Expression</th></tr><tr><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">Effect</italic>
</th><th align="left" rowspan="1" colspan="1">Angry</th><th align="left" rowspan="1" colspan="1">Disgusted</th><th align="left" rowspan="1" colspan="1">Fearful</th><th align="left" rowspan="1" colspan="1">Happy</th><th align="left" rowspan="1" colspan="1">Sad</th><th align="left" rowspan="1" colspan="1">Surprised</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Hemisphere</td><td align="left" rowspan="1" colspan="1">3.90</td><td align="left" rowspan="1" colspan="1">3.68</td><td align="left" rowspan="1" colspan="1">2.89</td><td align="left" rowspan="1" colspan="1">.488</td><td align="left" rowspan="1" colspan="1">.252</td><td align="left" rowspan="1" colspan="1">.131</td></tr><tr><td align="left" rowspan="1" colspan="1">Interference</td><td align="left" rowspan="1" colspan="1">5.01*</td><td align="left" rowspan="1" colspan="1">1.53</td><td align="left" rowspan="1" colspan="1">1.80</td><td align="left" rowspan="1" colspan="1">1.72</td><td align="left" rowspan="1" colspan="1">5.47*</td><td align="left" rowspan="1" colspan="1">2.74</td></tr><tr><td align="left" rowspan="1" colspan="1">Response hand</td><td align="left" rowspan="1" colspan="1">3.68</td><td align="left" rowspan="1" colspan="1">.160</td><td align="left" rowspan="1" colspan="1">.124</td><td align="left" rowspan="1" colspan="1">.000</td><td align="left" rowspan="1" colspan="1">.217</td><td align="left" rowspan="1" colspan="1">.077</td></tr><tr><td align="left" rowspan="1" colspan="1">Hemisphere x interference</td><td align="left" rowspan="1" colspan="1">.321</td><td align="left" rowspan="1" colspan="1">.349</td><td align="left" rowspan="1" colspan="1">.062</td><td align="left" rowspan="1" colspan="1">.800</td><td align="left" rowspan="1" colspan="1">.047</td><td align="left" rowspan="1" colspan="1">.982</td></tr><tr><td align="left" rowspan="1" colspan="1">Hemisphere x response hand</td><td align="left" rowspan="1" colspan="1">.099</td><td align="left" rowspan="1" colspan="1">.364</td><td align="left" rowspan="1" colspan="1">.056</td><td align="left" rowspan="1" colspan="1">.014</td><td align="left" rowspan="1" colspan="1">2.59</td><td align="left" rowspan="1" colspan="1">.089</td></tr><tr><td align="left" rowspan="1" colspan="1">Hemisphere x interference x response hand</td><td align="left" rowspan="1" colspan="1">2.12</td><td align="left" rowspan="1" colspan="1">.036</td><td align="left" rowspan="1" colspan="1">.023</td><td align="left" rowspan="1" colspan="1">.035</td><td align="left" rowspan="1" colspan="1">8.13**</td><td align="left" rowspan="1" colspan="1">.523</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t002fn001"><p>* = <italic toggle="yes">p</italic> &#x0003c;.050; ** = <italic toggle="yes">p</italic> &#x0003c;.010.</p></fn></table-wrap-foot></table-wrap><p>Critically, the hemisphere x interference x response hand interaction was also significant for sad expressions, <italic toggle="yes">F</italic>(1, 116) = 8.13, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.005, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;.065, 95% CI [-.025,.156]. This three-way interaction was probed by examining the two-way interaction of hemisphere x interference condition separately for each response hand group. This two-way interaction was significant in participants who used their right hand to respond, <italic toggle="yes">F</italic>(1, 63) = 5.82, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.019, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;.085, 95% CI [.004,.247], with higher d&#x02019; scores for faces presented to the left hemisphere (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.80, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.637) than the right (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.53, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.926) in the no interference condition, <italic toggle="yes">t</italic>(30) = 2.13, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.042, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.389, 95% CI [.018,.760], and no difference between the left (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.36, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.654) and right (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.48, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.797) hemispheres in the verbal interference condition, <italic toggle="yes">t</italic>(33) = 1.19, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.241, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.207, 95% CI [-.138,.552] (see right side of <xref rid="pone.0322504.g001" ref-type="fig">Fig 1E</xref>). The two-way interaction of hemisphere x interference condition was not significant in participants who used their left hand to respond, <italic toggle="yes">F</italic>(1, 53) = 2.81, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.100, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;.050, 95% CI [.000,.212], however post-hoc <italic toggle="yes">t</italic> tests revealed higher d&#x02019; scores for faces presented to the right hemisphere (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.89, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.614) than the left (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.59, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.725) in the no interference condition, <italic toggle="yes">t</italic>(26) = 2.17, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.040, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.426, 95% CI [.024,.827], and no difference between the left (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.47, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.579) and right (<italic toggle="yes">M</italic>&#x02009;=&#x02009;1.44, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;.726) hemispheres in the verbal interference condition, <italic toggle="yes">t</italic>(27) =.233, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.818, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.045, 95% CI [-.333,.422] (see left side of <xref rid="pone.0322504.g001" ref-type="fig">Fig 1E</xref>).</p><p>Response times for correct responses were analyzed similarly to d&#x02019; scores in a 6 (expression: angry vs. disgusted vs. fearful vs. happy vs. sad vs. surprised) x 2 (hemisphere: left vs. right) x 2 (interference: none vs. verbal) x 2 (response hand: left hand vs. right hand) repeated-measures ANOVA in which expression and hemisphere were within-participants independent variables, and interference and response hand were between-participants independent variables. Prior to analysis, response times that were longer than 3 standard deviations from the mean (1.8% of correct responses) were removed, and cells for which there were no remaining response times (.35%) were replaced with the grand mean. Results are provided in <xref rid="pone.0322504.t003" ref-type="table">Table 3</xref>.</p><table-wrap position="float" id="pone.0322504.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0322504.t003</object-id><label>Table 3</label><caption><title>Mean response times as a function of expression, hemisphere, interference, and response hand.</title></caption><alternatives><graphic xlink:href="pone.0322504.t003" id="pone.0322504.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="5" colspan="1">
<italic toggle="yes">Expression</italic>
</th><th align="left" colspan="8" rowspan="1">Response hand</th></tr><tr><th align="left" colspan="4" rowspan="1">Left hand</th><th align="left" colspan="4" rowspan="1">Right hand</th></tr><tr><th align="left" colspan="4" rowspan="1">Interference</th><th align="left" colspan="4" rowspan="1">Interference</th></tr><tr><th align="left" rowspan="1" colspan="1">None</th><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1">Verbal</th><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1">None</th><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1">Verbal</th><th align="left" rowspan="1" colspan="1"/></tr><tr><th align="left" rowspan="1" colspan="1">Left</th><th align="left" rowspan="1" colspan="1">Right</th><th align="left" rowspan="1" colspan="1">Left</th><th align="left" rowspan="1" colspan="1">Right</th><th align="left" rowspan="1" colspan="1">Left</th><th align="left" rowspan="1" colspan="1">Right</th><th align="left" rowspan="1" colspan="1">Left</th><th align="left" rowspan="1" colspan="1">Right</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Angry</td><td align="left" rowspan="1" colspan="1">775 (113)</td><td align="left" rowspan="1" colspan="1">804 (201)</td><td align="left" rowspan="1" colspan="1">822 (128)</td><td align="left" rowspan="1" colspan="1">859 (141)</td><td align="left" rowspan="1" colspan="1">842 (201)</td><td align="left" rowspan="1" colspan="1">859 (179)</td><td align="left" rowspan="1" colspan="1">850 (219)</td><td align="left" rowspan="1" colspan="1">850 (227)</td></tr><tr><td align="left" rowspan="1" colspan="1">Disgusted</td><td align="left" rowspan="1" colspan="1">662 (107)</td><td align="left" rowspan="1" colspan="1">683 (117)</td><td align="left" rowspan="1" colspan="1">727 (102)</td><td align="left" rowspan="1" colspan="1">717 (116)</td><td align="left" rowspan="1" colspan="1">689 (105)</td><td align="left" rowspan="1" colspan="1">693 (116)</td><td align="left" rowspan="1" colspan="1">718 (163)</td><td align="left" rowspan="1" colspan="1">709 (155)</td></tr><tr><td align="left" rowspan="1" colspan="1">Fearful</td><td align="left" rowspan="1" colspan="1">703 (137)</td><td align="left" rowspan="1" colspan="1">704 (128)</td><td align="left" rowspan="1" colspan="1">751 (92)</td><td align="left" rowspan="1" colspan="1">760 (125)</td><td align="left" rowspan="1" colspan="1">743 (128)</td><td align="left" rowspan="1" colspan="1">770 (125)</td><td align="left" rowspan="1" colspan="1">740 (145)</td><td align="left" rowspan="1" colspan="1">757 (127)</td></tr><tr><td align="left" rowspan="1" colspan="1">Happy</td><td align="left" rowspan="1" colspan="1">651 (127)</td><td align="left" rowspan="1" colspan="1">626 (105)</td><td align="left" rowspan="1" colspan="1">695 (131)</td><td align="left" rowspan="1" colspan="1">675 (104)</td><td align="left" rowspan="1" colspan="1">634 (79)</td><td align="left" rowspan="1" colspan="1">649 (80)</td><td align="left" rowspan="1" colspan="1">640 (131)</td><td align="left" rowspan="1" colspan="1">682 (143)</td></tr><tr><td align="left" rowspan="1" colspan="1">Sad</td><td align="left" rowspan="1" colspan="1">821 (182)</td><td align="left" rowspan="1" colspan="1">813 (182)</td><td align="left" rowspan="1" colspan="1">826 (103)</td><td align="left" rowspan="1" colspan="1">858 (179)</td><td align="left" rowspan="1" colspan="1">811 (170)</td><td align="left" rowspan="1" colspan="1">832 (161)</td><td align="left" rowspan="1" colspan="1">817 (221)</td><td align="left" rowspan="1" colspan="1">834 (136)</td></tr><tr><td align="left" rowspan="1" colspan="1">Surprised</td><td align="left" rowspan="1" colspan="1">671 (106)</td><td align="left" rowspan="1" colspan="1">673 (127)</td><td align="left" rowspan="1" colspan="1">725 (119)</td><td align="left" rowspan="1" colspan="1">700 (102)</td><td align="left" rowspan="1" colspan="1">678 (109)</td><td align="left" rowspan="1" colspan="1">701 (105)</td><td align="left" rowspan="1" colspan="1">688 (135)</td><td align="left" rowspan="1" colspan="1">716 (163)</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t003fn001"><p>&#x02018;Left&#x02019; indicates left-hemisphere presentations; &#x02018;right&#x02019; indicates right-hemisphere presentations. Times are provided in milliseconds. Parentheses indicate standard deviation of the mean.</p></fn></table-wrap-foot></table-wrap><p>The analysis of response time revealed a main effect of expression, <italic toggle="yes">F</italic>(5, 580) = 126.31, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;.179, 95% CI [.116,.241]. Response times (ms) were fastest for happy expressions (<italic toggle="yes">M</italic>&#x02009;=&#x02009;656, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;108), followed by surprised (<italic toggle="yes">M</italic>&#x02009;=&#x02009;694, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;112), <italic toggle="yes">t</italic>(119) = 6.23, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.571, 95% CI [.377,.765], and disgus<italic toggle="yes">t</italic>ed expressions (<italic toggle="yes">M</italic>&#x02009;=&#x02009;700, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;119), <italic toggle="yes">t</italic>(119) = 6.89, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.632, 95% CI [.435,.828], which did not differ, <italic toggle="yes">t</italic>(119) =.971, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.333, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.089, 95% CI [.091,.269]. Response times for fearful expressions (<italic toggle="yes">M</italic>&#x02009;=&#x02009;742, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;114) were longer than <italic toggle="yes">t</italic>hose for surprised, <italic toggle="yes">t</italic>(119) = 6.90, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.633, 95% CI [.436,.829], and disgusted expressions, <italic toggle="yes">t</italic>(119) = 5.87, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.538, 95% CI [.346,.730]. Finally, response <italic toggle="yes">t</italic>imes for sad (<italic toggle="yes">M</italic>&#x02009;=&#x02009;826, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;147) and angry expressions (<italic toggle="yes">M</italic>&#x02009;=&#x02009;834, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;161) were longer than those for fearful expressions, sad vs. fearful: <italic toggle="yes">t</italic>(119) = 8.00, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.733, 95% CI [.531,.936]; angry vs. fearful: <italic toggle="yes">t</italic>(119) = 8.19, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.751, 95% CI [.547,.954], and did not differ from each other, <italic toggle="yes">t</italic>(119) =.762, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.448, <italic toggle="yes">d</italic>&#x02009;=&#x02009;.070, 95% CI [-.110,.250].</p><p>The main effect of hemisphere was marginally significant, <italic toggle="yes">F</italic>(1, 116) = 3.65, <italic toggle="yes">p</italic>&#x02009;=&#x02009;.059, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;.031, 95% CI [-.032,.093], with a trend towards faster response times in the right hemisphere (<italic toggle="yes">M</italic>&#x02009;=&#x02009;737, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;112) than the left (<italic toggle="yes">M</italic>&#x02009;=&#x02009;748, <italic toggle="yes">SD</italic>&#x02009;=&#x02009;117). No other effects from the analysis of response times approach significance, all <italic toggle="yes">p</italic>s&#x02009;&#x0003e;&#x02009;.107.</p></sec><sec sec-type="conclusions" id="sec009"><title>Discussion</title><p>While much previous research investigating hemisphere asymmetries in facial expression processing observes right hemisphere advantages for processing negative facial expressions (e.g., [<xref rid="pone.0322504.ref010" ref-type="bibr">10</xref>&#x02013;<xref rid="pone.0322504.ref019" ref-type="bibr">19</xref>]), a growing amount of evidence highlights a role for the left hemisphere [<xref rid="pone.0322504.ref022" ref-type="bibr">22</xref>&#x02013;<xref rid="pone.0322504.ref028" ref-type="bibr">28</xref>]. The present study investigated the extent to which language contributes to this left hemisphere involvement by comparing performance during an emotion detection task presented to the left and right hemispheres under conditions of verbal interference (covertly rehearsing a 6-digit string for a subsequent memory) or no interference. We predicted that, if the left hemisphere advantage for negative facial expressions is due to language contributions, it should be reduced in the verbal interference condition compared to the no interference condition. Critically, half of the participants within the no interference and verbal interference groups used their right hand to respond and half used their left hand. For sad facial expressions, results from participants who used their right hand to respond were in line with our prediction but results from those who used their left hand were not. Hemisphere asymmetries were not observed for the other facial expressions. Results are discussed below.</p><p>As stated above, results from participants who used their right hand to respond to sad facial expressions support the idea that left hemisphere involvement in negative facial expression processing is due to contributions from language (see, e.g., [<xref rid="pone.0322504.ref023" ref-type="bibr">23</xref>,<xref rid="pone.0322504.ref024" ref-type="bibr">24</xref>]). During the no interference condition, when language processes were free to contribute to facial expression processing, a left hemisphere advantage was observed for sad expressions. This advantage disappeared, however, during the verbal interference condition when language contributions were disrupted. As such, results provide an explanation for previous findings of left hemisphere advantages for negative facial expression processing during non-linguistic tasks (e.g., [<xref rid="pone.0322504.ref023" ref-type="bibr">23</xref>,<xref rid="pone.0322504.ref025" ref-type="bibr">25</xref>&#x02013;<xref rid="pone.0322504.ref028" ref-type="bibr">28</xref>]) by suggesting that language may have contributed to processing despite not being required.</p><p>Importantly, participants who used their left hand to respond exhibited a different pattern of results. For these participants, a right hemisphere advantage was observed for sad expressions during the no interference condition, and this effect did not differ from that in the verbal interference condition, although the effect of hemisphere was not significant during verbal interference. This right hemisphere advantage for sad expressions replicates the right hemisphere advantage for negative facial expressions observed in many studies (e.g., [<xref rid="pone.0322504.ref010" ref-type="bibr">10</xref>&#x02013;<xref rid="pone.0322504.ref019" ref-type="bibr">19</xref>]), however, the effect of response hand on the direction of hemisphere asymmetry for sad expressions was surprising. As noted in the Introduction, previous studies using the divided-visual field emotion-detection task used in the present study have not observed an effect of response hand [<xref rid="pone.0322504.ref018" ref-type="bibr">18</xref>,<xref rid="pone.0322504.ref023" ref-type="bibr">23</xref>] and the goal of including the response-hand variable in the present study was to rule out the existence of a response-hand effect in a potentially more powerful design. Critically, we did not achieve this goal, and thus, the possibility that response hand influences hemisphere asymmetries in facial expression processing is supported.</p><p>The effect of response hand on visual hemisphere asymmetries is not well understood. Some research suggests that performance is negatively impacted when the visual stimulus and motor response are processed in the same hemisphere (e.g., right visual field and right hand) compared to when they are processed in different hemispheres [<xref rid="pone.0322504.ref037" ref-type="bibr">37</xref>,<xref rid="pone.0322504.ref039" ref-type="bibr">39</xref>], while other research suggests a cooperative relationship in which performance is improved when the visual stimulus and motor response are processed in the same hemisphere [<xref rid="pone.0322504.ref036" ref-type="bibr">36</xref>]. Results from the present study are more in line with the cooperative perspective. Performance for sad facial expressions was better in the left than right hemisphere when motor responses were controlled by the left hemisphere (right hand), and better in the right than left hemisphere when motor responses were controlled by the right hemisphere (left hand). Clearly more research will be needed to understand this relationship, however the present research highlights the relevance of this relationship to facial expression processing.</p><p>Notably, sad expressions were the only expressions of the 6 tested that exhibited a hemisphere asymmetry. One reason for this could be that existing hemisphere asymmetries for the other facial expressions were obscured due to ceiling effects. Indeed, performance on sad expressions was lower than performance on all other expressions (except angry) as indicated by the main effect of expression in d&#x02019; scores and response times. However, ceiling effects cannot be the entire explanation for the lack of hemisphere asymmetries in the other expressions because, as noted, performance on angry expressions was also lower than the others and equivalent to sad expressions. As such, it is possible that sad facial expressions are unique in their engagement of regions within the left hemisphere during non-linguistic tasks.</p><p>A growing amount of evidence supports this possibility. As described in the Introduction, Burgund [<xref rid="pone.0322504.ref023" ref-type="bibr">23</xref>] observed a left hemisphere advantage for both sad and fearful facial expressions during an expression labeling task, but only sad expressions continued to exhibit this advantage when the requirement for linguistic labels was removed during an emotion detection task. In another study, Matt et al. [<xref rid="pone.0322504.ref025" ref-type="bibr">25</xref>] observed a greater EEG signal over occipital-temporal sites for sad facial expressions than fearful or happy expressions, especially in the left hemisphere. Similarly, Jia et al. [<xref rid="pone.0322504.ref050" ref-type="bibr">50</xref>] observed a greater N170 over parietal-occipital sites for infant faces with sad expressions compared to those with happy or neutral expressions, and for female participants, this effect was more pronounced in the left hemisphere than the right. Finally, in a meta-analysis including 141 fMRI studies of facial expression processing, Xu et al. [<xref rid="pone.0322504.ref051" ref-type="bibr">51</xref>] observed a hemisphere asymmetry in which activity for sad facial expressions was greater in the left ventromedial prefrontal cortex (vmPFC) than the right, while activity for fearful, happy, and disgusted facial expressions was greater in the right vmPFC than the left. The present study adds to this expanding body of work distinguishing sad expressions from other facial expressions in terms of left hemisphere engagement.</p><p>Critically, we attribute the left hemisphere involvement in sad facial expression processing to contributions from language, as this advantage was eliminated during the verbal interference condition when language contributions were disrupted. It is clear that the verbal interference task was &#x0201c;interfering&#x0201d; as d&#x02019; scores were lower in the verbal interference condition than the no interference condition overall, as shown by the main effect of interference condition, and for sad (and angry) expressions separately (see <xref rid="pone.0322504.t002" ref-type="table">Table 2</xref>). It is also clear that participants in the verbal interference condition were performing the task of remembering the 6-digit number because their performance on the number memory test was high (&#x0003e; 93% with the lenient scoring criteria). What is unclear is whether the effect of the verbal interference task on hemisphere asymmetries was due to the verbal nature of the task or more simply due to the demands of performing a secondary task. Previous research has observed decreased right hemisphere involvement in facial expression processing when a non-verbal interference task is performed compared to control [<xref rid="pone.0322504.ref052" ref-type="bibr">52</xref>]. This is similar to the decreased right hemisphere performance in the verbal compared to no interference conditions for sad expressions in the present study when participants used their left hand, but contrasts with the lack of the same effect when participants used their right hand highlighting the difference between the verbal interference task used in the present study and a non-verbal interference task. Nonetheless, future studies should compare hemisphere asymmetries for facial expressions with no interference to those observed during verbal and non-verbal interference tasks in order to isolate the effect of language more definitively.</p></sec><sec sec-type="conclusions" id="sec010"><title>Conclusion</title><p>In conclusion, the present study investigated the effect of verbal interference on hemisphere asymmetries in a non-linguistic facial emotion detection task. Results from participants who used their right hand to respond supported the hypothesis that verbal interference reduces the left hemisphere advantage for negative facial expressions, in particular sad facial expressions. Results from participants who used their left hand to respond did not support this hypothesis, highlighting the influence of response hand on hemisphere asymmetries in facial expression processing. Future research should investigate the effect of response hand further, as well as probe the extent to which effects of interference are due to language specifically or more general processing demands.</p></sec></body><back><ack><p>Thanks are owed to Silas Benevento Zahner, Olive Fenlon, Ram Guruprasad, Julia Hanson, Inaya Laubach, Juan Pablo Trujillo, and Yiyang Zhao for assistance with data collection.</p></ack><ref-list><title>References</title><ref id="pone.0322504.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Borgomaneri</surname><given-names>S</given-names></name>, <name><surname>Vitale</surname><given-names>F</given-names></name>, <name><surname>Battaglia</surname><given-names>S</given-names></name>, <name><surname>Avenanti</surname><given-names>A</given-names></name>. <article-title>Early right motor cortex response to happy and fearful facial expressions: a TMS motor-evoked potential study</article-title>. <source>Brain Sci</source>. <year>2021</year>;<volume>11</volume>(<issue>9</issue>):<fpage>1203</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/brainsci11091203</pub-id>
<pub-id pub-id-type="pmid">34573224</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Hausmann</surname><given-names>M</given-names></name>, <name><surname>Innes</surname><given-names>BR</given-names></name>, <name><surname>Birch</surname><given-names>YK</given-names></name>, <name><surname>Kentridge</surname><given-names>RW</given-names></name>. <article-title>Laterality and (in)visibility in emotional face perception: Manipulations in spatial frequency content</article-title>. <source>Emotion</source>. <year>2021</year>;<volume>21</volume>(<issue>1</issue>):<fpage>175</fpage>&#x02013;<lpage>83</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/emo0000648</pub-id>
<pub-id pub-id-type="pmid">31368746</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Hazelton</surname><given-names>JL</given-names></name>, <name><surname>Devenney</surname><given-names>E</given-names></name>, <name><surname>Ahmed</surname><given-names>R</given-names></name>, <name><surname>Burrell</surname><given-names>J</given-names></name>, <name><surname>Hwang</surname><given-names>Y</given-names></name>, <name><surname>Piguet</surname><given-names>O</given-names></name>, <etal>et al</etal>. <article-title>Hemispheric contributions toward interoception and emotion recognition in left-vs right-semantic dementia</article-title>. <source>Neuropsychologia</source>. <year>2023</year>;<volume>188</volume>:<fpage>108628</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2023.108628</pub-id>
<pub-id pub-id-type="pmid">37348648</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Leiva</surname><given-names>S</given-names></name>, <name><surname>Micciulli</surname><given-names>A</given-names></name>, <name><surname>Ferreres</surname><given-names>A</given-names></name>. <article-title>Impaired recognition of dynamic body expressions after right hemisphere damage</article-title>. <source>Psychol Neurosci</source>. <year>2022</year>;<volume>15</volume>(<issue>2</issue>):<fpage>186</fpage>&#x02013;<lpage>97</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/pne0000272</pub-id></mixed-citation></ref><ref id="pone.0322504.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Loi</surname><given-names>N</given-names></name>, <name><surname>Ginatempo</surname><given-names>F</given-names></name>, <name><surname>Manca</surname><given-names>A</given-names></name>, <name><surname>Melis</surname><given-names>F</given-names></name>, <name><surname>Deriu</surname><given-names>F</given-names></name>. <article-title>Faces emotional expressions: from perceptive to motor areas in aged and young subjects</article-title>. <source>J Neurophysiol</source>. <year>2021</year>;<volume>126</volume>(<issue>5</issue>):<fpage>1642</fpage>&#x02013;<lpage>52</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1152/jn.00328.2021</pub-id>
<pub-id pub-id-type="pmid">34614362</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Nesbit</surname><given-names>RJ</given-names></name>, <name><surname>Watling</surname><given-names>D</given-names></name>. <article-title>Comparing two versions of the Chimeric Face Test: a pilot investigation</article-title>. <source>Laterality</source>. <year>2024</year>;<volume>29</volume>(<issue>1</issue>):<fpage>19</fpage>&#x02013;<lpage>36</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/1357650X.2023.2252569</pub-id>
<pub-id pub-id-type="pmid">37676081</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Qiu</surname><given-names>Z</given-names></name>, <name><surname>Lei</surname><given-names>X</given-names></name>, <name><surname>Becker</surname><given-names>SI</given-names></name>, <name><surname>Pegna</surname><given-names>AJ</given-names></name>. <article-title>Neural activities during the processing of unattended and unseen emotional faces: a voxel-wise meta-analysis</article-title>. <source>Brain Imaging Behav</source>. <year>2022</year>;<volume>16</volume>(<issue>5</issue>):<fpage>2426</fpage>&#x02013;<lpage>43</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11682-022-00697-8</pub-id>
<pub-id pub-id-type="pmid">35739373</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Smekal</surname><given-names>V</given-names></name>, <name><surname>Burt</surname><given-names>DM</given-names></name>, <name><surname>Kentridge</surname><given-names>RW</given-names></name>, <name><surname>Hausmann</surname><given-names>M</given-names></name>. <article-title>Emotion lateralization in a graduated emotional chimeric face task: An online study</article-title>. <source>Neuropsychology</source>. <year>2022</year>;<volume>36</volume>(<issue>5</issue>):<fpage>443</fpage>&#x02013;<lpage>55</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/neu0000804</pub-id>
<pub-id pub-id-type="pmid">35389720</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Van der Donck</surname><given-names>S</given-names></name>, <name><surname>Hendriks</surname><given-names>M</given-names></name>, <name><surname>Vos</surname><given-names>S</given-names></name>, <name><surname>Op de Beeck</surname><given-names>H</given-names></name>, <name><surname>Boets</surname><given-names>B</given-names></name>. <article-title>Neural sensitivity to facial identity and facial expression discrimination in adults with autism</article-title>. <source>Autism Res</source>. <year>2023</year>;<volume>16</volume>(<issue>11</issue>):<fpage>2110</fpage>&#x02013;<lpage>24</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/aur.3036</pub-id>
<pub-id pub-id-type="pmid">37823568</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Bourne</surname><given-names>VJ</given-names></name>. <article-title>How are emotions lateralised in the brain? Contrasting existing hypotheses using the Chimeric Faces Test</article-title>. <source>Cognit Emot</source>. <year>2010</year>;<volume>24</volume>(<issue>5</issue>):<fpage>903</fpage>&#x02013;<lpage>11</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/02699930903007714</pub-id></mixed-citation></ref><ref id="pone.0322504.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Cushing</surname><given-names>CA</given-names></name>, <name><surname>Im</surname><given-names>HY</given-names></name>, <name><surname>Adams RB</surname><given-names>Jr</given-names></name>, <name><surname>Ward</surname><given-names>N</given-names></name>, <name><surname>Kveraga</surname><given-names>K</given-names></name>. <article-title>Magnocellular and parvocellular pathway contributions to facial threat cue processing</article-title>. <source>Soc Cogn Affect Neurosci</source>. <year>2019</year>;<volume>14</volume>(<issue>2</issue>):<fpage>151</fpage>&#x02013;<lpage>62</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/scan/nsz003</pub-id>
<pub-id pub-id-type="pmid">30721981</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Damaskinou</surname><given-names>N</given-names></name>, <name><surname>Watling</surname><given-names>D</given-names></name>. <article-title>Neurophysiological evidence (ERPs) for hemispheric processing of facial expressions of emotions: evidence from whole face and chimeric face stimuli</article-title>. <source>Laterality</source>. <year>2018</year>;<volume>23</volume>(<issue>3</issue>):<fpage>318</fpage>&#x02013;<lpage>43</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/1357650X.2017.1361963</pub-id>
<pub-id pub-id-type="pmid">28857672</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Gainotti</surname><given-names>G</given-names></name>. <article-title>Emotions related to threatening events are mainly linked to the right hemisphere</article-title>. <source>J Psychiatry Neurosci</source>. <year>2024</year>;<volume>49</volume>(<issue>3</issue>):E208&#x02013;<lpage>11</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1503/jpn.240002</pub-id>
<pub-id pub-id-type="pmid">38816030</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Innes</surname><given-names>BR</given-names></name>, <name><surname>Burt</surname><given-names>DM</given-names></name>, <name><surname>Birch</surname><given-names>YK</given-names></name>, <name><surname>Hausmann</surname><given-names>M</given-names></name>. <article-title>A leftward bias however you look at it: Revisiting the emotional chimeric face task as a tool for measuring emotion lateralization</article-title>. <source>Laterality</source>. <year>2016</year>;<volume>21</volume>(4&#x02013;6):<fpage>643</fpage>&#x02013;<lpage>61</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/1357650X.2015.1117095</pub-id>
<pub-id pub-id-type="pmid">26710248</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Kajal</surname><given-names>DS</given-names></name>, <name><surname>Fioravanti</surname><given-names>C</given-names></name>, <name><surname>Elshahabi</surname><given-names>A</given-names></name>, <name><surname>Ruiz</surname><given-names>S</given-names></name>, <name><surname>Sitaram</surname><given-names>R</given-names></name>, <name><surname>Braun</surname><given-names>C</given-names></name>. <article-title>Involvement of top-down networks in the perception of facial emotions: a magnetoencephalographic investigation</article-title>. <source>Neuroimage</source>. <year>2020</year>;<volume>222</volume>:<fpage>117075</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117075</pub-id>
<pub-id pub-id-type="pmid">32585348</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Kumar</surname><given-names>D</given-names></name>, <name><surname>Srinivasan</surname><given-names>N</given-names></name>. <article-title>Emotion perception is mediated by spatial frequency content</article-title>. <source>Emotion</source>. <year>2011</year>;<volume>11</volume>(<issue>5</issue>):<fpage>1144</fpage>&#x02013;<lpage>51</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/a0025453</pub-id>
<pub-id pub-id-type="pmid">21942699</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>S</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>H</given-names></name>, <name><surname>Hao</surname><given-names>B</given-names></name>, <name><surname>He</surname><given-names>W</given-names></name>, <name><surname>Luo</surname><given-names>W</given-names></name>. <article-title>Is processing superiority a universal trait for all threats? Divergent impacts of fearful, angry, and disgusted faces on attentional capture</article-title>. <source>Cortex</source>. <year>2024</year>;<volume>177</volume>:<fpage>37</fpage>&#x02013;<lpage>52</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cortex.2024.05.005</pub-id>
<pub-id pub-id-type="pmid">38833819</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Najt</surname><given-names>P</given-names></name>, <name><surname>Bayer</surname><given-names>U</given-names></name>, <name><surname>Hausmann</surname><given-names>M</given-names></name>. <article-title>Models of hemispheric specialization in facial emotion perception--a reevaluation</article-title>. <source>Emotion</source>. <year>2013</year>;<volume>13</volume>(<issue>1</issue>):<fpage>159</fpage>&#x02013;<lpage>67</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/a0029723</pub-id>
<pub-id pub-id-type="pmid">22906088</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Rahman</surname><given-names>Q</given-names></name>, <name><surname>Anchassi</surname><given-names>T</given-names></name>. <article-title>Men appear more lateralized when noticing emotion in male faces</article-title>. <source>Emotion</source>. <year>2012</year>;<volume>12</volume>(<issue>1</issue>):<fpage>174</fpage>&#x02013;<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/a0024416</pub-id>
<pub-id pub-id-type="pmid">21707155</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Borod</surname><given-names>JC</given-names></name>, <name><surname>Cicero</surname><given-names>BA</given-names></name>, <name><surname>Obler</surname><given-names>LK</given-names></name>, <name><surname>Welkowitz</surname><given-names>J</given-names></name>, <name><surname>Erhan</surname><given-names>HM</given-names></name>, <name><surname>Santschi</surname><given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Right hemisphere emotional perception: evidence across multiple channels</article-title>. <source>Neuropsychology</source>. <year>1998</year>;<volume>12</volume>(<issue>3</issue>):<fpage>446</fpage>&#x02013;<lpage>58</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037//0894-4105.12.3.446</pub-id>
<pub-id pub-id-type="pmid">9673999</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Adolphs</surname><given-names>R</given-names></name>, <name><surname>Jansari</surname><given-names>A</given-names></name>, <name><surname>Tranel</surname><given-names>D</given-names></name>. <article-title>Hemispheric perception of emotional valence from facial expressions</article-title>. <source>Neuropsychology</source>. <year>2001</year>;<volume>15</volume>(<issue>4</issue>):<fpage>516</fpage>&#x02013;<lpage>24</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/0894-4105.15.4.516</pub-id><pub-id pub-id-type="pmid">11761041</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Bublatzky</surname><given-names>F</given-names></name>, <name><surname>Kavc&#x00131;o&#x0011f;lu</surname><given-names>F</given-names></name>, <name><surname>Guerra</surname><given-names>P</given-names></name>, <name><surname>Doll</surname><given-names>S</given-names></name>, <name><surname>Jungh&#x000f6;fer</surname><given-names>M</given-names></name>. <article-title>Contextual information resolves uncertainty about ambiguous facial emotions: behavioral and magnetoencephalographic correlates</article-title>. <source>Neuroimage</source>. <year>2020</year>;<volume>215</volume>:<fpage>116814</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116814</pub-id>
<pub-id pub-id-type="pmid">32276073</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Burgund</surname><given-names>ED</given-names></name>. <article-title>Left hemisphere dominance for negative facial expressions: the influence of task</article-title>. <source>Front Hum Neurosci</source>. <year>2021</year>;<volume>15</volume>:<fpage>742018</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fnhum.2021.742018</pub-id>
<pub-id pub-id-type="pmid">34602999</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Burt</surname><given-names>DM</given-names></name>, <name><surname>Hausmann</surname><given-names>M</given-names></name>. <article-title>Hemispheric asymmetries in categorical facial expression perception</article-title>. <source>Emotion</source>. <year>2019</year>;<volume>19</volume>(<issue>4</issue>):<fpage>584</fpage>&#x02013;<lpage>92</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/emo0000460</pub-id>
<pub-id pub-id-type="pmid">29771545</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Matt</surname><given-names>S</given-names></name>, <name><surname>Dzhelyova</surname><given-names>M</given-names></name>, <name><surname>Maillard</surname><given-names>L</given-names></name>, <name><surname>Lighezzolo-Alnot</surname><given-names>J</given-names></name>, <name><surname>Rossion</surname><given-names>B</given-names></name>, <name><surname>Caharel</surname><given-names>S</given-names></name>. <article-title>The rapid and automatic categorization of facial expression changes in highly variable natural images</article-title>. <source>Cortex</source>. <year>2021</year>;<volume>144</volume>:<fpage>168</fpage>&#x02013;<lpage>84</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cortex.2021.08.005</pub-id>
<pub-id pub-id-type="pmid">34666300</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Prete</surname><given-names>G</given-names></name>, <name><surname>D&#x02019;Ascenzo</surname><given-names>S</given-names></name>, <name><surname>Laeng</surname><given-names>B</given-names></name>, <name><surname>Fabri</surname><given-names>M</given-names></name>, <name><surname>Foschi</surname><given-names>N</given-names></name>, <name><surname>Tommasi</surname><given-names>L</given-names></name>. <article-title>Conscious and unconscious processing of facial expressions: evidence from two split-brain patients</article-title>. <source>J Neuropsychol</source>. <year>2015</year>;<volume>9</volume>(<issue>1</issue>):<fpage>45</fpage>&#x02013;<lpage>63</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/jnp.12034</pub-id>
<pub-id pub-id-type="pmid">24325712</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Prete</surname><given-names>G</given-names></name>, <name><surname>Marzoli</surname><given-names>D</given-names></name>, <name><surname>Brancucci</surname><given-names>A</given-names></name>, <name><surname>Fabri</surname><given-names>M</given-names></name>, <name><surname>Foschi</surname><given-names>N</given-names></name>, <name><surname>Tommasi</surname><given-names>L</given-names></name>. <article-title>The processing of chimeric and dichotic emotional stimuli by connected and disconnected cerebral hemispheres</article-title>. <source>Behav Brain Res</source>. <year>2014</year>;<volume>271</volume>:<fpage>354</fpage>&#x02013;<lpage>64</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.bbr.2014.06.034</pub-id>
<pub-id pub-id-type="pmid">24971689</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Stankovi&#x00107;</surname><given-names>M</given-names></name>, <name><surname>Ne&#x00161;i&#x00107;</surname><given-names>M</given-names></name>. <article-title>Functional brain asymmetry for emotions: psychological stress-induced reversed hemispheric asymmetry in emotional face perception</article-title>. <source>Exp Brain Res</source>. <year>2020</year>;<volume>238</volume>(<issue>11</issue>):<fpage>2641</fpage>&#x02013;<lpage>51</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s00221-020-05920-w</pub-id>
<pub-id pub-id-type="pmid">32924076</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Barker</surname><given-names>MS</given-names></name>, <name><surname>Bidstrup</surname><given-names>EM</given-names></name>, <name><surname>Robinson</surname><given-names>GA</given-names></name>, <name><surname>Nelson</surname><given-names>NL</given-names></name>. <article-title>&#x0201c;Grumpy&#x0201d; or &#x0201c;furious&#x0201d;? arousal of emotion labels influences judgments of facial expressions</article-title>. <source>PLoS One</source>. <year>2020</year>;<volume>15</volume>(<issue>7</issue>):e0235390. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0235390</pub-id>
<pub-id pub-id-type="pmid">32609780</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Gendron</surname><given-names>M</given-names></name>, <name><surname>Lindquist</surname><given-names>KA</given-names></name>, <name><surname>Barsalou</surname><given-names>L</given-names></name>, <name><surname>Barrett</surname><given-names>LF</given-names></name>. <article-title>Emotion words shape emotion percepts</article-title>. <source>Emotion</source>. <year>2012</year>;<volume>12</volume>(<issue>2</issue>):<fpage>314</fpage>&#x02013;<lpage>25</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/a0026007</pub-id>
<pub-id pub-id-type="pmid">22309717</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Lindquist</surname><given-names>KA</given-names></name>, <name><surname>Wager</surname><given-names>TD</given-names></name>, <name><surname>Kober</surname><given-names>H</given-names></name>, <name><surname>Bliss-Moreau</surname><given-names>E</given-names></name>, <name><surname>Barrett</surname><given-names>LF</given-names></name>. <article-title>The brain basis of emotion: a meta-analytic review</article-title>. <source>Behav Brain Sci</source>. <year>2012</year>;<volume>35</volume>(<issue>3</issue>):<fpage>121</fpage>&#x02013;<lpage>43</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1017/S0140525X11000446</pub-id>
<pub-id pub-id-type="pmid">22617651</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Nook</surname><given-names>EC</given-names></name>, <name><surname>Lindquist</surname><given-names>KA</given-names></name>, <name><surname>Zaki</surname><given-names>J</given-names></name>. <article-title>A new look at emotion perception: concepts speed and shape facial emotion recognition</article-title>. <source>Emotion</source>. <year>2015</year>;<volume>15</volume>(<issue>5</issue>):<fpage>569</fpage>&#x02013;<lpage>78</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/a0039166</pub-id>
<pub-id pub-id-type="pmid">25938612</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Nedergaard</surname><given-names>JSK</given-names></name>, <name><surname>Wallentin</surname><given-names>M</given-names></name>, <name><surname>Lupyan</surname><given-names>G</given-names></name>. <article-title>Verbal interference paradigms: a systematic review investigating the role of language in cognition</article-title>. <source>Psychon Bull Rev</source>. <year>2023</year>;<volume>30</volume>(<issue>2</issue>):<fpage>464</fpage>&#x02013;<lpage>88</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3758/s13423-022-02144-7</pub-id>
<pub-id pub-id-type="pmid">35996045</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Lupyan</surname><given-names>G</given-names></name>. <article-title>Extracommunicative functions of language: verbal interference causes selective categorization impairments</article-title>. <source>Psychon Bull Rev</source>. <year>2009</year>;<volume>16</volume>(<issue>4</issue>):<fpage>711</fpage>&#x02013;<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3758/PBR.16.4.711</pub-id>
<pub-id pub-id-type="pmid">19648457</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Winawer</surname><given-names>J</given-names></name>, <name><surname>Witthoft</surname><given-names>N</given-names></name>, <name><surname>Frank</surname><given-names>MC</given-names></name>, <name><surname>Wu</surname><given-names>L</given-names></name>, <name><surname>Wade</surname><given-names>AR</given-names></name>, <name><surname>Boroditsky</surname><given-names>L</given-names></name>. <article-title>Russian blues reveal effects of language on color discrimination</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2007</year>;<volume>104</volume>(<issue>19</issue>):<fpage>7780</fpage>&#x02013;<lpage>5</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.0701644104</pub-id>
<pub-id pub-id-type="pmid">17470790</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Diwadkar</surname><given-names>VA</given-names></name>, <name><surname>Bellani</surname><given-names>M</given-names></name>, <name><surname>Chowdury</surname><given-names>A</given-names></name>, <name><surname>Savazzi</surname><given-names>S</given-names></name>, <name><surname>Perlini</surname><given-names>C</given-names></name>, <name><surname>Marinelli</surname><given-names>V</given-names></name>, <etal>et al</etal>. <article-title>Activations in gray and white matter are modulated by uni-manual responses during within and inter-hemispheric transfer: effects of response hand and right-handedness</article-title>. <source>Brain Imaging Behav</source>. <year>2018</year>;<volume>12</volume>(<issue>4</issue>):<fpage>942</fpage>&#x02013;<lpage>61</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11682-017-9750-7</pub-id>
<pub-id pub-id-type="pmid">28808866</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Heck</surname><given-names>N</given-names></name>, <name><surname>Mohr</surname><given-names>B</given-names></name>. <article-title>Response hand differentially affects action word processing</article-title>. <source>Front Psychol</source>. <year>2017</year>;<volume>8</volume>:<fpage>2223</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fpsyg.2017.02223</pub-id>
<pub-id pub-id-type="pmid">29312071</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref038"><label>38</label><mixed-citation publication-type="journal"><name><surname>Markus</surname><given-names>A</given-names></name>, <name><surname>Eviatar</surname><given-names>Z</given-names></name>. <article-title>Handedness in the presence of prior knowledge: effects of interhemispheric configuration on performance</article-title>. <source>Neuropsychologia</source>. <year>2023</year>;<volume>178</volume>:<fpage>108429</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2022.108429</pub-id>
<pub-id pub-id-type="pmid">36427540</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>Wendt</surname><given-names>M</given-names></name>, <name><surname>Vietze</surname><given-names>I</given-names></name>, <name><surname>Kluwe</surname><given-names>RH</given-names></name>. <article-title>Visual field x response hand interactions and level priming in the processing of laterally presented hierarchical stimuli</article-title>. <source>Brain Cogn</source>. <year>2007</year>;<volume>63</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>12</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.bandc.2006.06.007</pub-id>
<pub-id pub-id-type="pmid">16901597</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref040"><label>40</label><mixed-citation publication-type="journal"><name><surname>Wolski</surname><given-names>P</given-names></name>, <name><surname>Asanowicz</surname><given-names>D</given-names></name>. <article-title>Does CUD measure interhemispheric transfer time? The allocation of attention influences the Poffenberger effect</article-title>. <source>Neuropsychologia</source>. <year>2023</year>;<volume>185</volume>:<fpage>108581</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2023.108581</pub-id>
<pub-id pub-id-type="pmid">37156411</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref041"><label>41</label><mixed-citation publication-type="journal"><name><surname>Berretz</surname><given-names>G</given-names></name>, <name><surname>Wolf</surname><given-names>OT</given-names></name>, <name><surname>G&#x000fc;nt&#x000fc;rk&#x000fc;n</surname><given-names>O</given-names></name>, <name><surname>Ocklenburg</surname><given-names>S</given-names></name>. <article-title>Atypical lateralization in neurodevelopmental and psychiatric disorders: What is the role of stress?</article-title>. <source>Cortex</source>. <year>2020</year>;<volume>125</volume>:<fpage>215</fpage>&#x02013;<lpage>32</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cortex.2019.12.019</pub-id>
<pub-id pub-id-type="pmid">32035318</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref042"><label>42</label><mixed-citation publication-type="journal"><name><surname>Lindell</surname><given-names>AK</given-names></name>, <name><surname>Hudry</surname><given-names>K</given-names></name>. <article-title>Atypicalities in cortical structure, handedness, and functional lateralization for language in autism spectrum disorders</article-title>. <source>Neuropsychol Rev</source>. <year>2013</year>;<volume>23</volume>(<issue>3</issue>):<fpage>257</fpage>&#x02013;<lpage>70</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11065-013-9234-5</pub-id>
<pub-id pub-id-type="pmid">23649809</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref043"><label>43</label><mixed-citation publication-type="journal"><name><surname>Thibodeau</surname><given-names>R</given-names></name>, <name><surname>Jorgensen</surname><given-names>RS</given-names></name>, <name><surname>Kim</surname><given-names>S</given-names></name>. <article-title>Depression, anxiety, and resting frontal EEG asymmetry: a meta-analytic review</article-title>. <source>J Abnorm Psychol</source>. <year>2006</year>;<volume>115</volume>(<issue>4</issue>):<fpage>715</fpage>&#x02013;<lpage>29</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/0021-843X.115.4.715</pub-id>
<pub-id pub-id-type="pmid">17100529</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref044"><label>44</label><mixed-citation publication-type="journal"><name><surname>Oldfield</surname><given-names>RC</given-names></name>. <article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title>. <source>Neuropsychologia</source>. <year>1971</year>;<volume>9</volume>(<issue>1</issue>):<fpage>97</fpage>&#x02013;<lpage>113</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id>
<pub-id pub-id-type="pmid">5146491</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref045"><label>45</label><mixed-citation publication-type="journal"><name><surname>Faul</surname><given-names>F</given-names></name>, <name><surname>Erdfelder</surname><given-names>E</given-names></name>, <name><surname>Lang</surname><given-names>A-G</given-names></name>, <name><surname>Buchner</surname><given-names>A</given-names></name>. <article-title>G*Power 3: a flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title>. <source>Behav Res Methods</source>. <year>2007</year>;<volume>39</volume>(<issue>2</issue>):<fpage>175</fpage>&#x02013;<lpage>91</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3758/bf03193146</pub-id>
<pub-id pub-id-type="pmid">17695343</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref046"><label>46</label><mixed-citation publication-type="journal"><name><surname>Stanislaw</surname><given-names>H</given-names></name>, <name><surname>Todorov</surname><given-names>N</given-names></name>. <article-title>Calculation of signal detection theory measures</article-title>. <source>Behav Res Methods Instrum Comput</source>. <year>1999</year>;<volume>31</volume>(<issue>1</issue>):<fpage>137</fpage>&#x02013;<lpage>49</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3758/bf03207704</pub-id>
<pub-id pub-id-type="pmid">10495845</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref047"><label>47</label><mixed-citation publication-type="journal"><name><surname>Langner</surname><given-names>O</given-names></name>, <name><surname>Dotsch</surname><given-names>R</given-names></name>, <name><surname>Bijlstra</surname><given-names>G</given-names></name>, <name><surname>Wigboldus</surname><given-names>DHJ</given-names></name>, <name><surname>Hawk</surname><given-names>ST</given-names></name>, <name><surname>van Knippenberg</surname><given-names>A</given-names></name>. <article-title>Presentation and validation of the Radboud faces database</article-title>. <source>Cognit Emot</source>. <year>2010</year>;<volume>24</volume>(<issue>8</issue>):<fpage>1377</fpage>&#x02013;<lpage>88</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/02699930903485076</pub-id></mixed-citation></ref><ref id="pone.0322504.ref048"><label>48</label><mixed-citation publication-type="journal"><name><surname>Stoet</surname><given-names>G</given-names></name>. <article-title>PsyToolkit: a software package for programming psychological experiments using Linux</article-title>. <source>Behav Res Methods</source>. <year>2010</year>;<volume>42</volume>(<issue>4</issue>):<fpage>1096</fpage>&#x02013;<lpage>104</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3758/BRM.42.4.1096</pub-id>
<pub-id pub-id-type="pmid">21139177</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref049"><label>49</label><mixed-citation publication-type="journal"><name><surname>Stoet</surname><given-names>G</given-names></name>. <article-title>A novel web-based method for running online questionnaires and reaction-time experiments</article-title>. <source>Teach Psychol</source>. <year>2016</year>;<volume>44</volume>(<issue>1</issue>):<fpage>24</fpage>&#x02013;<lpage>31</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1177/0098628316677643</pub-id></mixed-citation></ref><ref id="pone.0322504.ref050"><label>50</label><mixed-citation publication-type="journal"><name><surname>Jia</surname><given-names>YC</given-names></name>, <name><surname>Ding</surname><given-names>FY</given-names></name>, <name><surname>Cheng</surname><given-names>G</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Yu</surname><given-names>W</given-names></name>, <name><surname>Zou</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Infants&#x02019; neutral facial expressions elicit the strongest initial attentional bias in adults: Behavioral and electrophysiological evidence</article-title>. <source>Psychophysiology</source>. <year>2022</year>;<volume>59</volume>(<issue>1</issue>):e13944. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/psyp.13944</pub-id>
<pub-id pub-id-type="pmid">34553377</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref051"><label>51</label><mixed-citation publication-type="journal"><name><surname>Xu</surname><given-names>P</given-names></name>, <name><surname>Peng</surname><given-names>S</given-names></name>, <name><surname>Luo</surname><given-names>Y-J</given-names></name>, <name><surname>Gong</surname><given-names>G</given-names></name>. <article-title>Facial expression recognition: a meta-analytic review of theoretical models and neuroimaging evidence</article-title>. <source>Neurosci Biobehav Rev</source>. <year>2021</year>;<volume>127</volume>:<fpage>820</fpage>&#x02013;<lpage>36</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neubiorev.2021.05.023</pub-id>
<pub-id pub-id-type="pmid">34052280</pub-id>
</mixed-citation></ref><ref id="pone.0322504.ref052"><label>52</label><mixed-citation publication-type="journal"><name><surname>Duncan</surname><given-names>J</given-names></name>, <name><surname>Dugas</surname><given-names>G</given-names></name>, <name><surname>Brisson</surname><given-names>B</given-names></name>, <name><surname>Blais</surname><given-names>C</given-names></name>, <name><surname>Fiset</surname><given-names>D</given-names></name>. <article-title>Dual-task interference on left eye utilization during facial emotion perception</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>2019</year>;<volume>45</volume>(<issue>10</issue>):<fpage>1319</fpage>&#x02013;<lpage>30</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/xhp0000674</pub-id>
<pub-id pub-id-type="pmid">31259582</pub-id>
</mixed-citation></ref></ref-list></back></article>