<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39730794</article-id><article-id pub-id-type="pmc">PMC11681242</article-id><article-id pub-id-type="publisher-id">82501</article-id><article-id pub-id-type="doi">10.1038/s41598-024-82501-9</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Explainable AI improves task performance in human&#x02013;AI collaboration</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Senoner</surname><given-names>Julian</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Schallmoser</surname><given-names>Simon</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Kratzwald</surname><given-names>Bernhard</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Feuerriegel</surname><given-names>Stefan</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Netland</surname><given-names>Torbj&#x000f8;rn</given-names></name><address><email>tnetland@ethz.ch</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05a28rw58</institution-id><institution-id institution-id-type="GRID">grid.5801.c</institution-id><institution-id institution-id-type="ISNI">0000 0001 2156 2780</institution-id><institution>ETH Zurich, </institution></institution-wrap>Zurich, Switzerland </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05591te55</institution-id><institution-id institution-id-type="GRID">grid.5252.0</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 973X</institution-id><institution>LMU Munich, </institution></institution-wrap>Munich, Germany </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02nfy3535</institution-id><institution-id institution-id-type="ISNI">0000 0005 1103 3702</institution-id><institution>Munich Center for Machine Learning (MCML), </institution></institution-wrap>Munich, Germany </aff><aff id="Aff4"><label>4</label>Present Address: EthonAI, Zurich, Switzerland </aff></contrib-group><pub-date pub-type="epub"><day>28</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>28</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>31150</elocation-id><history><date date-type="received"><day>10</day><month>10</month><year>2024</year></date><date date-type="accepted"><day>5</day><month>12</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Artificial intelligence (AI) provides considerable opportunities to assist human work. However, one crucial challenge of human&#x02013;AI collaboration is that many AI algorithms operate in a black-box manner where the way how the AI makes predictions remains opaque. This makes it difficult for humans to validate a prediction made by AI against their own domain knowledge. For this reason, we hypothesize that augmenting humans with explainable AI improves task performance in human&#x02013;AI collaboration. To test this hypothesis, we implement explainable AI in the form of visual heatmaps in inspection tasks conducted by domain experts. Visual heatmaps have the advantage that they are easy to understand and help to localize relevant parts of an image. We then compare participants that were either supported by (a)&#x000a0;black-box AI or (b)&#x000a0;explainable AI, where the latter supports them to follow AI predictions when the AI is accurate or overrule the AI when the AI predictions are wrong. We conducted two preregistered experiments with representative, real-world visual inspection tasks from manufacturing and medicine. The first experiment was conducted with factory workers from an electronics factory, who performed <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=9,600$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq1.gif"/></alternatives></inline-formula> assessments of whether electronic products have defects. The second experiment was conducted with radiologists, who performed <inline-formula id="IEq2"><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=5,650$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq2.gif"/></alternatives></inline-formula> assessments of chest X-ray images to identify lung lesions. The results of our experiments with domain experts performing real-world tasks show that task performance improves when participants are supported by explainable AI with heatmaps instead of black-box AI. We find that explainable AI as a decision aid improved the task performance by 7.7 percentage points (95% confidence interval [CI]: 3.3% to 12.0%, <inline-formula id="IEq3"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.001$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq3.gif"/></alternatives></inline-formula>) in the manufacturing experiment and by 4.7 percentage points (95% CI: 1.1% to 8.3%, <inline-formula id="IEq4"><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.010$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq4.gif"/></alternatives></inline-formula>) in the medical experiment compared to black-box AI. These gains represent a significant improvement in task performance.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Explainable AI</kwd><kwd>Task performance</kwd><kwd>Decision-making</kwd><kwd>Human-centered AI</kwd><kwd>Human&#x02013;AI collaboration</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Mechanical engineering</kwd><kwd>Skin manifestations</kwd><kwd>Computer science</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Schweizerischer Nationalfonds zur F&#x000f6;rderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>186932</award-id><principal-award-recipient><name><surname>Feuerriegel</surname><given-names>Stefan</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Artificial intelligence (AI) provides considerable opportunities to assist human work in various domains<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. For example, in manufacturing, AI is widely used to support humans when inspecting the quality of produced products to identify defects<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Similarly, in medicine, disease diagnosis now makes increasing use of AI systems. For instance, a recent survey found that AI is used by about 15% of radiologists at least weekly<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. More broadly, an analysis showed that about 30% of all jobs in the United States are at high exposure to be assisted by AI<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Hence, the importance of human&#x02013;AI collaboration is expected to grow in the near future.</p><p id="Par3">However, many questions regarding the effective design of human&#x02013;AI collaborations remain open. One particular challenge in the use of AI for human work is that state-of-the-art AI algorithms, which frequently involve millions of trainable parameters<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>, operate as &#x0201c;black-box&#x0201d; algorithms. The term &#x0201c;black-box&#x0201d; refers to the opacity of these systems, meaning that the internal workings and decision-making processes of these algorithms are not transparent or easily understandable by humans<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. This can have crucial implications in practice as the lack of transparency makes it difficult&#x02014;or even impossible&#x02014;for humans to validate the predictions made by an AI against their domain knowledge. Hence, without being able to assess whether a prediction generated by an AI is accurate, humans will not be able to correct predictions of the AI, because of which the unique expertise of workers is essentially lost, which will make the collaboration between domain experts and AI largely ineffective.</p><p id="Par4">Increasing efforts have been made to overcome the black-box nature of AI by developing methods that generate <italic>explanations</italic> for how AI algorithms reach their decisions<sup><xref ref-type="bibr" rid="CR9">9</xref>&#x02013;<xref ref-type="bibr" rid="CR15">15</xref></sup>. Explainable AI refers to a set of methods that support humans in understanding how AI algorithms map certain inputs (e.g., lung X-rays, patient characteristics) to certain outputs (e.g., probability estimates for pneumonia)<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. Explainable AI can be broadly categorized into inherently interpretable models and post-hoc explanation techniques (see Supplement <xref rid="MOESM1" ref-type="media">A</xref> for an extended literature review on explainable AI). For inherently interpretable algorithms, the decision-making of the algorithm can be inspected by humans, e.g., by inspecting the coefficients in linear regression or the splitting rules in decision trees<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Post-hoc explanation techniques are required when the inner workings of an AI algorithm become too complex to be understood by humans such as in deep neural networks. For example, one approach is to approximate the behavior of a black-box AI with a simpler model (e.g., a linear model) that can be interpreted<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. Other methods rely on game theory to estimate the contribution of each model input to the model output while considering possible interaction effects<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. Such explanation techniques are commonly used by AI engineers in the development of AI algorithms. Therefore, this literature stream is orthogonal to the use of explainable AI in our work, where we use post-hoc explanation techniques to improve decisions by domain experts in real-world job tasks.</p><p id="Par5">Common methods for explaining AI algorithms in computer vision include the use of heatmaps. These heatmaps visually highlight the areas that are most relevant to the predictions made by the AI<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. In this research, we focus on heatmaps because they can easily be understood by humans and already have broad adoption in practice. This choice is motivated by our research setting, which involves visual inspection tasks in manufacturing and medicine where spatially localized explanations are highly relevant. Heatmaps are very common in explaining black-box AI models for visual inspection tasks and are considered state-of-the-art with respect to their localization performance across various settings<sup><xref ref-type="bibr" rid="CR23">23</xref>&#x02013;<xref ref-type="bibr" rid="CR26">26</xref></sup>. Another advantage of heatmaps is their ease of use, which is especially relevant in real-world settings where not all domain experts may be familiar with explainable AI methods. These domain experts can then compare the easily interpretable heatmap explanations to their domain knowledge, thereby validating whether the AI is correct or overwriting the AI if it is not correct.</p><p id="Par6">Several works have studied behavioral dimensions of human&#x02013;AI collaboration. For example, it has been examined whether humans are willing to delegate work to AI<sup><xref ref-type="bibr" rid="CR27">27</xref>&#x02013;<xref ref-type="bibr" rid="CR29">29</xref></sup>. Another common dimension is algorithm aversion, where humans are averse to following decisions by algorithms and instead rely on their own (mis)judgment<sup><xref ref-type="bibr" rid="CR30">30</xref>&#x02013;<xref ref-type="bibr" rid="CR34">34</xref></sup>. An antecedent to algorithm aversion is <italic>trust in AI</italic>, critically influencing whether humans adopt or reject AI recommendations<sup><xref ref-type="bibr" rid="CR35">35</xref>&#x02013;<xref ref-type="bibr" rid="CR37">37</xref></sup>. Oppositely to algorithm aversion, overreliance is also a problem negatively impacting the effectiveness of human&#x02013;AI collaboration<sup><xref ref-type="bibr" rid="CR38">38</xref>&#x02013;<xref ref-type="bibr" rid="CR40">40</xref></sup>. That is, humans risk following AI predictions blindly without attentively performing the task. While all these dimensions are interesting from a behavioral perspective, the main outcome of interest for business and healthcare organizations is <italic>task performance</italic>. However, the impact of explainable AI on task performance in human&#x02013;AI collaboration in real-world job tasks remains unclear.</p><p id="Par7">We hypothesize that augmenting domain experts with explainable AI through visual heatmaps, as opposed to black-box AI, improves task performance in human&#x02013;AI collaboration. Specifically, we treat explainable AI as a form of decision aid that supports domain experts in better understanding algorithmic decisions. Here, explainable AI does not provide more information from an AI perspective (i.e., identical predictive performance). However, for domain experts, accessing explainable AI through heatmaps gives rich additional information by making the AI predictions more intelligible. Thus, we expect that domain experts supported by explainable AI through heatmaps will outperform those supported by black-box AI in two ways: (1)&#x000a0;they are more likely to follow AI predictions when they are accurate, and (2)&#x000a0;they are more likely to overrule AI predictions when they were wrong.</p><p id="Par8">Previous research has studied the effect of explainable AI on task performance in human&#x02013;AI collaboration (see Supplement <xref rid="MOESM1" ref-type="media">A</xref> for a detailed overview), yet with key limitations. In particular, existing works are typically restricted by either (i) recruiting laypeople or (ii) overly simplified tasks that are not representative of real job tasks<sup><xref ref-type="bibr" rid="CR41">41</xref>&#x02013;<xref ref-type="bibr" rid="CR44">44</xref></sup>. However, a realistic estimate of the effect of explainable AI on task performance requires a real-world task performed by domain experts. Such works that actually study real-world tasks with domain experts are on the other hand restricted by (i) comparing explainable AI vs humans alone<sup><xref ref-type="bibr" rid="CR45">45</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>, (ii) using no real explainable AI<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, or (iii) research designs that do not isolate the effect of explainable AI on task performance<sup><xref ref-type="bibr" rid="CR48">48</xref>&#x02013;<xref ref-type="bibr" rid="CR50">50</xref></sup>. In contrast, the strength of our work is that we study the effect of explainable AI through visual heatmaps on task performance relative to black-box AI in human&#x02013;AI collaborations with <italic>real-world tasks</italic> and actual <italic>domain experts</italic>.</p><p id="Par9">The objective of this study is to analyze the effect of explainable AI in the form of visual heatmaps (as opposed to black-box AI) on task performance of domain experts performing real-world job tasks. For this, we conducted two preregistered experiments in which domain experts were asked to solve real-world visual inspection tasks in manufacturing (Study&#x000a0;1) and medicine (Study&#x000a0;2). We followed a between-subject design where we randomly assigned participants to two treatments: (a)&#x000a0;black-box AI (i.e., where AI predictions are opaque) and (b)&#x000a0;explainable AI (i.e., where AI predictions are explained with heatmaps). For simplicity, we later refer to our heatmap explanations as &#x0201c;explainable AI&#x0201d;. The latter thus offers not only the prediction from the AI but also shows explanations in the form of a visual heatmap as a decision aid. Study&#x000a0;1 was conducted in a manufacturing setting, where participants had to identify quality defects in electronic products. For this, we specifically recruited factory workers performing <inline-formula id="IEq5"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=9,600$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq5.gif"/></alternatives></inline-formula> assessments of electronic products in a factory setting at <italic>Siemens</italic>. Study&#x000a0;2 was conducted in a medical setting, where participants had to identify lung lesions on chest X-ray images. To that end, medical professionals, i.e., radiologists, were recruited and performed <inline-formula id="IEq6"><alternatives><tex-math id="M6">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=5,650$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq6.gif"/></alternatives></inline-formula> assessments of chest X-ray images. In both studies, participants performed better when being supported by explainable AI as a decision aid.</p><p id="Par10">The tasks of both experiments are representative of many real-world human&#x02013;AI collaborations. The manufacturing task is an identical, one-to-one copy of a real-world job task at <italic>Siemens</italic> and, hence, highly representative of visual inspection tasks in manufacturing<sup><xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup>. Visual inspection tasks are standard in the manufacturing industry. Regardless of how much manufacturers have sought to build quality into products and processes, labor-intensive inspection tasks still abound<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. In healthcare, visual inspection tasks are common across many different subdisciplines, such as dermatology, radiology, pathology, ophthalmology, and dentistry, among many others. As concrete examples, physicians have to inspect, for instance, skin lesions in dermatology, tissues in pathology, and lung lesions in radiology<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR54">54</xref>,<xref ref-type="bibr" rid="CR55">55</xref></sup>. Therefore, establishing whether physicians benefit from explainable AI in visual inspection tasks is highly relevant for disease diagnosis.</p></sec><sec id="Sec2"><title>Methods</title><p id="Par11">We conducted two randomized experiments across two different visual inspection settings: manufacturing (Study&#x000a0;1) and medicine (Study&#x000a0;2). In both experiments, participants had to perform a visual inspection task. We preregistered our hypotheses (i.e., Study&#x000a0;1: <ext-link ext-link-type="uri" xlink:href="https://osf.io/7djxb">https://osf.io/7djxb</ext-link> and Study&#x000a0;2: <ext-link ext-link-type="uri" xlink:href="https://osf.io/69yqt">https://osf.io/69yqt</ext-link>; see also Supplement <xref rid="MOESM1" ref-type="media">K</xref>), which were tested in two randomized experiments. The research design was approved by the Ethics Commission of ETH Zurich (EK 2021-N-34). We confirm that all methods were performed in accordance with the relevant guidelines and regulations. All participants provided informed consent.</p><sec id="Sec3"><title>Experiment design</title><p id="Par12">In the following, details of the experiment designs of the two visual inspection tasks are provided.</p><sec id="Sec4"><title>Study 1: manufacturing experiment</title><p id="Par13">For the manufacturing task, we designed a representative, real-world visual inspection task in collaboration with <italic>Siemens Smart Infrastructure</italic> in Zug, Switzerland. The experimental task is representative of various domains in which workers have to make decisions under a limited time budget. During the experiment, workers were shown images of electronic products and were asked to label them as faultless or defective. Reassuringly, we emphasize that our experiment involved a real work scenario: we conducted it with real workers familiar with quality management practice, a real user interface for state-of-the-art quality management, realistic incentives, and real product images. All steps in the experiments were carried out via a computer interface that was designed analogously to the real-world quality inspection setup at <italic>Siemens</italic> (see Supplement <xref rid="MOESM1" ref-type="media">D</xref> for details). In the experiment, we made sure that all workers have the exact same conditions (exact same product images, same computer setup, same time limits, etc.). Thereby, we can rule out confounding variables that would arise naturally during the usual work routines and thus ensure that the experiment is scientifically sound.</p><p id="Par14">We obtained 200 images of four different types of electronic products (printed circuit boards) from <italic>Siemens</italic>. Example images are provided in Supplement <xref rid="MOESM1" ref-type="media">B</xref>. All four different product types are of equal importance to <italic>Siemens</italic>. Each product type comprised 43 images with faultless products and 7 images with defective products (e.g., missing components, wrong components, and faulty components). The different defects are all considered equally bad by the partner company, i.e., the products are considered to be either functional or non-functional. Hence, we considered defective products as scrap as best practice in quality management<sup><xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup>.</p><p id="Par15">We implemented an AI algorithm that computed an individual quality score for each image. The quality score gives a numerical value between 0 (most certainly defect) and 100 (most certainly faultless). Workers were instructed that a quality score below 90 suggests an increased likelihood of a quality defect and that the AI algorithm can make mistakes. As humans cannot understand how the AI algorithm arrives at the prediction, the quality score is regarded as opaque (&#x0201c;black-box AI&#x0201d;). When evaluating the quality score with a cutoff of 90 for mapping the numerical value onto a binary faultless/defect label, the standalone AI algorithm achieves a balanced accuracy of 95.6% and a defect detection rate of 92.9%. The prediction performance of the standalone AI algorithm was not communicated to the workers. The AI algorithm was trained on an additional set of product images that was not included in the experiment.</p><p id="Par16">We used anomaly heatmaps<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> to explain the opaque quality score from the AI algorithm. The heatmaps were computed with standard computer vision methods and highlighted image regions with suspected quality defects (i.e., deviations from a faultless product). We chose heatmaps as the explanation technique for our AI algorithm as they provide a clear and intuitive way of highlighting areas with quality defects. This is especially important since the recruited domain experts are typically not familiar with explanation techniques for AI algorithms. Further, heatmaps are frequently used for images and are considered state-of-the-art with respect to their localization performance across various settings<sup><xref ref-type="bibr" rid="CR23">23</xref>&#x02013;<xref ref-type="bibr" rid="CR26">26</xref></sup>. Details on the implementation of the AI algorithm and heatmaps are provided in Supplement <xref rid="MOESM1" ref-type="media">C</xref>.</p><p id="Par17">In the experiment, workers were randomly assigned to one of the two treatments: (a)&#x000a0;black-box AI or (b)&#x000a0;explainable AI in the form of visual heatmaps (Figure <xref rid="Fig1" ref-type="fig">1</xref>A). Workers in the black-box AI treatment arm were only supported by the opaque quality score. Workers in the explainable AI treatment arm had access to the same quality score but additionally received the heatmap that explained the otherwise opaque quality score. Of note, the explainable AI had the same accuracy as the black-box AI and did not carry more information from an AI perspective (i.e., the heatmaps were of the same predictive power).</p><p id="Par18">Before starting the experiment, workers had to give written consent to participate and then pass a tutorial on how to use the interface. After that, the workers were randomly assigned to one of the two treatments, i.e., either black-box AI or explainable AI. During the experiment, the 200 product images were consecutively shown in random order. For each image, the workers had to assess the quality; that is, to &#x0201c;approve&#x0201d; or &#x0201c;reject&#x0201d; the shown product. We tracked the decision speed and the quality assessment (i.e., labeled as faultless or defective) made by the worker. To match real-world conditions, the workers were given a maximum of 35 minutes to finish the inspection task of 200 product images (around 10 seconds per image). In total, <inline-formula id="IEq7"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=9,600$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq7.gif"/></alternatives></inline-formula> assessments of product images were performed by the workers. Finally, workers completed a post-experimental questionnaire (Supplement <xref rid="MOESM1" ref-type="media">L</xref>).</p></sec><sec id="Sec5"><title>Study 2: medical experiment</title><p id="Par19">For the medical task, radiologists had to identify lung lesions in real chest X-ray images. Lung lesions are common findings in chest X-ray images<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> and can be easily overlooked due to their frequently small size<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. The radiologists were asked whether at least one lung lesion was visible in the X-ray image. The experiment was conducted via Qualtrics. To ensure a realistic experimental setup that resembles the same task in daily medical practice, we implemented a zoom function, which allowed the radiologists to investigate an enlarged view of the image by moving their computer mouse over the image. Analogous to Study&#x000a0;1, we emphasize that our medical experiment involved a realistic work scenario: we conducted it with actual medical professionals, who were asked to investigate real chest X-ray images.</p><p id="Par20">We used 50 chest X-ray images from the CheXpert dataset<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. The dataset comprised 7 images with at least one lung lesion and 43 images without lung lesions. Example images are provided in Supplement <xref rid="MOESM1" ref-type="media">B</xref>.</p><p id="Par21">We implemented an AI algorithm that outputs the probability of whether a lung lesion is visible in the chest X-ray image. We transformed these probability outputs for lung lesions such that the AI score gives a numerical value between 0 (most certainly contains a lung lesion) and 100 (most certainly does not contain a lung lesion) to mirror the quality score from the manufacturing setting. As a result, the AI output can be interpreted as a risk score, which is widely used in medical practice. Radiologists were instructed that an AI score below 90 indicates that the AI algorithm suspects at least one lung lesion is visible and that the AI algorithm can make mistakes. When evaluating the AI score with a cutoff of 90 for mapping the numerical value onto a binary label (lung lesion visible yes/no), the standalone AI algorithm achieves a balanced accuracy of 82.2% and a disease detection rate of 71.4%. Analogously to the manufacturing task, the prediction performance of the standalone AI algorithm was not communicated to the participants.</p><p id="Par22">As in the manufacturing task, the black-box AI algorithm was converted into an explainable AI by explaining the AI score via a heatmap, which is a state-of-the-art explanation technique for chest X-ray images in medicine<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Further details about the implementation of the AI algorithm and the heatmap are provided in Supplement <xref rid="MOESM1" ref-type="media">C</xref>.</p><p id="Par23">The procedure was analogous to the manufacturing experiment. Before starting the experiment, radiologists had to confirm their area of specialization and give written consent to participate. Subsequently, the task was explained and the radiologists had to pass a tutorial on how to use the interface. After that, radiologists were randomly assigned to one of the two treatments, i.e., either black-box AI or explainable AI (Figure <xref rid="Fig1" ref-type="fig">1</xref>B). During the experiment, 50 chest X-ray images were randomly shown either in forward or reverse order. For each chest X-ray image, the radiologists had to answer whether at least one lung lesion is visible. The corresponding answers as well as the decision speed were tracked. Radiologists were given a maximum of 35 minutes to finish the inspection task of 50 chest X-ray images. Radiologists were given more time per image compared to factory workers in the manufacturing experiment to reflect the differences in manufacturing and clinical practice. In total, <inline-formula id="IEq8"><alternatives><tex-math id="M8">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=5,650$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq8.gif"/></alternatives></inline-formula> assessments of chest X-ray images were performed by the radiologists. Finally, all radiologists completed a post-experimental questionnaire (Supplement <xref rid="MOESM1" ref-type="media">L</xref>).<fig id="Fig1"><label>Figure 1</label><caption><p>Overview of the experiments for assessing the effect of explainable AI on task performance. ( A )&#x000a0;Experimental design of the manufacturing experiment where factory workers were asked to &#x0201c;approve&#x0201d; images of faultless products and to &#x0201c;reject&#x0201d; images of defective products through a computer interface. ( B )&#x000a0;Experimental design of the medical experiment where radiologists were asked to decide whether lung lesions are visible in the chest X-ray image. In both experiments, participants were randomly assigned to one of the two treatments: (<bold>a</bold>)&#x000a0;black-box AI or (<bold>b</bold>)&#x000a0;explainable AI.</p></caption><graphic xlink:href="41598_2024_82501_Fig1_HTML" id="MO1"/></fig></p></sec></sec><sec id="Sec6"><title>Study populations</title><p id="Par24">The inclusion was as follows. Participants had to be at least 18 years old. For the manufacturing task, participants additionally had to have no self-reported visual impairment. The exclusion criteria were preregistered and were as follows. In both studies, we excluded participants who failed the tutorial or did not finish the inspection task on time. Participants with obvious misbehavior were also excluded from our analyses. In the manufacturing task, this was the case for workers who approved all products (i.e., labeled no images as defective). In the medical task, this was the case for radiologists who assigned the same label for all 50 chest X-ray images. In both studies, participants whose performance with respect to balanced accuracy was more than three standard deviations worse than the mean of their respective treatment arm were excluded. We performed randomization checks to confirm that all treatment arms were demographically unbiased (Supplement <xref rid="MOESM1" ref-type="media">E</xref>).</p><sec id="Sec7"><title>Study 1: manufacturing experiment</title><p id="Par25">The manufacturing experiment was carried out from June 29 to July 8, 2021, on-site at a <italic>Siemens</italic> factory in Zug, Switzerland. The objective of the field experiment was to get a real-world estimate of the treatment effect based on a representative sample of actual factory workers. Therefore, we only considered factory workers who were experienced in quality control practices. The factory workers were well familiar with the shown products and the visual inspection task. Overall, 56 factory workers (consisting of manufacturing employees, quality engineers, and team leaders) participated in our study. Out of them, all workers passed the tutorial; 6 did not finish on time and 2 were excluded due to obvious misbehavior. The final sample consisted of 48 factory workers with an average working experience of 13.8 years (<inline-formula id="IEq9"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SD=9.8$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq9.gif"/></alternatives></inline-formula>). A larger sample size in our manufacturing experiment was not possible because the entire available workforce in one shift did not exceed 56 workers. Still, the experiment is well-powered as the treatment effect in the field experiment is considerably large. No additional financial incentive was given beyond the base salary to be representative of many real-world tasks from domain experts (e.g., as in manufacturing at <italic>Siemens</italic>).</p></sec><sec id="Sec8"><title>Study 2: medical experiment</title><p id="Par26">The medical experiment was carried out via an online interface from February 27 to March 31, 2024. Actual radiologists based in the United States were recruited via MSI-ACI (<ext-link ext-link-type="uri" xlink:href="https://site.msi-aci.com/">https://site.msi-aci.com/</ext-link>). MSI-ACI paid a financial compensation to radiologists regardless of performance and adheres to the federal minimum wage in the United States. Overall, 122 radiologists started the study. Out of them, all passed the tutorial; 4 did not complete the study and 2 did not finish on time; and 3 were excluded due to obvious misbehavior. Thus, the final sample consisted of 113 radiologists with an average tenure as radiologist of 13.5 years (<inline-formula id="IEq10"><alternatives><tex-math id="M10">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SD=10.4$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq10.gif"/></alternatives></inline-formula>).</p></sec></sec><sec id="Sec9"><title>Statistical analysis</title><p id="Par27">In manufacturing, it is common that all defects (e.g., missing components, wrong components, and faulty components) are considered equally bad<sup><xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup>, so that the product to inspect could be either functional or non-functional. Analogously, in the medical setting, either lung lesions were present or not. Hence, in both settings, the outcomes were binary. Therefore, task performance in the visual inspection tasks between the participants&#x02019; assessments and the ground-truth labels was computed via (1)&#x000a0;balanced accuracy (i.e., average sensitivity across faultless and defective products) and (2)&#x000a0;defect/disease detection rate. For (1), the balanced accuracy is calculated via <inline-formula id="IEq11"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.5 \times [ TP /P + TN /N]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq11.gif"/></alternatives></inline-formula> with true positives <inline-formula id="IEq12"><alternatives><tex-math id="M12">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$TP$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq12.gif"/></alternatives></inline-formula>, positives <italic>P</italic>, true negatives <inline-formula id="IEq13"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$TN$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq13.gif"/></alternatives></inline-formula>, and negatives <italic>N</italic>. Here, we used balanced accuracy since it accounts for imbalanced distributions of labels by equally weighing the performance on each label, thus following best practice<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. In contrast, the standard accuracy score would not account for the imbalanced distribution of positive and negative labels encountered in both settings (i.e., 172 faultless products and 28 defective products in the manufacturing setting; 7 chest X-ray images with and 43 without lung lesions in the medical setting). For (2), the defect/disease detection rate is defined as <inline-formula id="IEq14"><alternatives><tex-math id="M14">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$TN /N$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq14.gif"/></alternatives></inline-formula>, where defective products and chest X-ray images with lung lesions were defined as negatives. In our manufacturing setting, missing a defective product has more severe implications than labeling a faultless product as defective. In medicine, missing a lung lesion on a chest X-ray image has more severe implications than additionally performing a CT scan for a healthy patient. For that reason, it is crucial to find the negative samples.</p><p id="Par28">All statistical tests in the results are based on one-sided Welch&#x02019;s <italic>t</italic>-tests. We further used ordinary least square (OLS) regression models to estimate the treatment effect of explainable AI on task performance. The OLS models are estimated via<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Y_{i} = \beta _{0} + \beta _{1}\, Treatment _{i} + \varepsilon _{i}, \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82501_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq15"><alternatives><tex-math id="M16">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y_{i}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq15.gif"/></alternatives></inline-formula> is the observed task performance (i.e., balanced accuracy or defect/disease detection rate), <inline-formula id="IEq16"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Treatment_{i}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq16.gif"/></alternatives></inline-formula> is a binary variable which equals 0 if participant <italic>i</italic> received the black-box AI treatment and 1 if participant <italic>i</italic> received the explainable AI treatment. The <italic>P</italic>-values for <inline-formula id="IEq17"><alternatives><tex-math id="M18">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta _{1}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq17.gif"/></alternatives></inline-formula> were derived using Student&#x02019;s t-test. A significance level of <inline-formula id="IEq18"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha = 0.05$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq18.gif"/></alternatives></inline-formula> was preregistered.</p><p id="Par29">All analyses were conducted using Python (3.11) with <italic>numpy</italic> (1.24.3)<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>, <italic>pandas</italic> (1.5.3)<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>, <italic>scipy</italic> (1.11.1)<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>, and <italic>statsmodels</italic> (0.14.0)<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. The data visualizations were created with <italic>seaborn</italic> (0.12.2)<sup><xref ref-type="bibr" rid="CR64">64</xref></sup> and <italic>matplotlib</italic> (3.7.1)<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>.</p></sec><sec id="Sec10"><title>Robustness checks</title><p id="Par30">To assess whether our findings are also generalizable to non-experts, we repeated the manufacturing task with participants recruited from Amazon MTurk (Results and Supplement <xref rid="MOESM1" ref-type="media">J</xref>).</p><p id="Par31">Additionally, we conducted the following robustness checks. First, we repeated our analyses using precision as an additional task performance metric (Supplement <xref rid="MOESM1" ref-type="media">G</xref>). Second, we repeated the OLS regression models with additional participant-specific controls to estimate the treatment effect of explainable AI (Supplement <xref rid="MOESM1" ref-type="media">H</xref>). Third, we estimated the treatment effect with quasi-binomial regression (Supplement <xref rid="MOESM1" ref-type="media">H</xref>). Fourth, we estimated the regression models including participants that were previously excluded due to obvious misbehavior or because they did not finish the inspection task in time (Supplement <xref rid="MOESM1" ref-type="media">I</xref>). All robustness checks yielded conclusive findings.</p><p id="Par32">To demonstrate that the heatmaps in our medical setting are robust with respect to the choice of algorithm, we used two additional, different algorithms to generate heatmaps. We find that different algorithms lead to similar heatmaps (Supplement <xref rid="MOESM1" ref-type="media">F</xref>).</p></sec></sec><sec id="Sec11"><title>Results</title><p id="Par33">To analyze the effect of explainable AI on task performance in human&#x02013;AI collaboration, we conducted two randomized experiments across two different settings, i.e., in manufacturing (Study&#x000a0;1) and medicine (Study&#x000a0;2). In both experiments, participants had to perform a visual inspection task. In the manufacturing experiment, factory workers were asked to inspect electronic products and to identify defective products. In the medical experiment, radiologists were asked to decide whether lung lesions are visible in chest X-ray images. Participants were randomly assigned to one of two different treatments aiding them in the task: (a)&#x000a0;black-box AI or (b)&#x000a0;explainable AI (Figure <xref rid="Fig1" ref-type="fig">1</xref>). Participants with black-box AI received an opaque AI score as a decision aid. Participants with explainable AI received the same score and an additional decision aid: the explanation of the score in the form of a heatmap. The heatmap does not provide more information from an AI perspective (the score is identical) but allows users to verify the prediction made by the AI. However, heatmaps provide a clear and intuitive way of highlighting quality defects/lung lesions<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>. We hypothesized that explainable AI as a decision aid improves task performance of domain experts in human&#x02013;AI collaboration.</p><sec id="Sec12"><title>Study 1: manufacturing experiment</title><p id="Par34">In the manufacturing experiment, factory workers were asked to identify quality defects in electronic products (e.g., missing components, wrong components, and faulty components) with high accuracy. We found that workers supported by explainable AI achieved better task performance than workers supported by black-box AI. Workers with black-box AI achieved a balanced accuracy with a mean of only 88.6%, whereas workers with explainable AI treatment achieved a balanced accuracy with a mean of 96.3% (Fig. <xref rid="Fig2" ref-type="fig">2</xref>A). We then estimated the treatment effect of explainable AI by regressing the balanced accuracy on the treatment (black-box AI <inline-formula id="IEq19"><alternatives><tex-math id="M20">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$= 0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq19.gif"/></alternatives></inline-formula>, explainable AI <inline-formula id="IEq20"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$= 1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq20.gif"/></alternatives></inline-formula>). The regression results show that the treatment effect of explainable AI is statistically significant and large (<inline-formula id="IEq21"><alternatives><tex-math id="M22">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta =7.653$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq21.gif"/></alternatives></inline-formula>, <inline-formula id="IEq22"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SE =2.178$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq22.gif"/></alternatives></inline-formula>, <inline-formula id="IEq23"><alternatives><tex-math id="M24">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t = 3.513$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq23.gif"/></alternatives></inline-formula>, <inline-formula id="IEq24"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.001$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq24.gif"/></alternatives></inline-formula>, 95&#x000a0;%&#x000a0;<inline-formula id="IEq25"><alternatives><tex-math id="M26">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {confidence interval [CI]} = [3.268, 12.038]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq25.gif"/></alternatives></inline-formula>); that is, an improvement of 7.7 percentage points. Compared to the black-box AI, the explainable AI leads to a five-fold decrease in the median error rate.</p><p id="Par35">Workers with explainable AI outperformed workers with black-box AI also with respect to the defect detection rate with a mean of 93.0% versus a mean of 82.0% (Fig. <xref rid="Fig2" ref-type="fig">2</xref>B). The regression results again confirm that the treatment effect of explainable AI is statistically significant and large (<inline-formula id="IEq26"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta =11.014$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq26.gif"/></alternatives></inline-formula>, <inline-formula id="IEq27"><alternatives><tex-math id="M28">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SE =3.680$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq27.gif"/></alternatives></inline-formula>, <inline-formula id="IEq28"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t = 2.993$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq28.gif"/></alternatives></inline-formula>, <inline-formula id="IEq29"><alternatives><tex-math id="M30">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.004$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq29.gif"/></alternatives></inline-formula>, 95&#x000a0;%&#x000a0;<inline-formula id="IEq30"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {CI} = [3.607, 18.421]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq30.gif"/></alternatives></inline-formula>). All regression results remain statistically significant when including relevant control variables (demographics, tenure, self-reported IT skills, and decision speed) in the regression model (see Supplement <xref rid="MOESM1" ref-type="media">H</xref>).</p><p id="Par36">A detailed analysis of the workers&#x02019; assessments revealed that workers with explainable AI followed accurate predictions more often than workers with black-box AI (<inline-formula id="IEq31"><alternatives><tex-math id="M32">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {mean} = 93.5\%$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq31.gif"/></alternatives></inline-formula> for black-box AI, <inline-formula id="IEq32"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {mean} = 98.6\%$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq32.gif"/></alternatives></inline-formula> for explainable AI). In particular, workers supported by black-box AI were 3.6 times more likely to erroneously overrule an AI prediction, despite the prediction being accurate (<inline-formula id="IEq33"><alternatives><tex-math id="M34">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t=2.437$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq33.gif"/></alternatives></inline-formula>, <inline-formula id="IEq34"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.011$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq34.gif"/></alternatives></inline-formula>). Interestingly, 73.1% of the workers with explainable AI performed even better than the standalone AI algorithm. This suggests that the explanations (i.e., the heatmaps) not only improve adherence to accurate AI predictions, but also help humans make correct assessments when the AI predictions are wrong. We found that workers with explainable AI were, on average, able to identify and overrule 96.9% of the wrong AI predictions. For comparison, workers supported by black-box AI only overruled 86.4% of the wrong AI predictions. These results are highly relevant since &#x02013; regardless of an AI&#x02019;s performance&#x02014;wrong AI predictions can always occur due to external factors such as dust or different light conditions. The difference between both treatments is again statistically significant (<inline-formula id="IEq35"><alternatives><tex-math id="M36">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t=2.631$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq35.gif"/></alternatives></inline-formula>, <inline-formula id="IEq36"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.007$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq36.gif"/></alternatives></inline-formula>). These findings underscore the effectiveness of augmenting humans with explainable AI.<fig id="Fig2"><label>Figure 2</label><caption><p>Results of manufacturing experiment. The boxplots compare the task performance between the two treatments: black-box AI and explainable AI. The task performance is measured by the balanced accuracy (<bold>A</bold>) and the defect detection rate (<bold>B</bold>) based on the quality assessment of workers and the ground-truth labels of the product images. A balanced accuracy of 50% provides a na&#x000ef;ve baseline corresponding to a random guess (black dotted line). The standalone AI algorithm attains a balanced accuracy of 95.6% and a defect detection rate of 92.9% (orange dashed lines). Statistical significance is based on a one-sided Welch&#x02019;s <italic>t</italic>-test (<sup>***</sup><inline-formula id="IEq37"><alternatives><tex-math id="M38">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P&#x0003c;0.001$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq37.gif"/></alternatives></inline-formula>, <sup>**</sup><inline-formula id="IEq38"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P&#x0003c;0.01$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq38.gif"/></alternatives></inline-formula>, <sup>*</sup><inline-formula id="IEq39"><alternatives><tex-math id="M40">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P&#x0003c;0.05$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq39.gif"/></alternatives></inline-formula>). In the boxplots, the center line denotes the median; box limits are upper and lower quartiles; whiskers are defined as the 1.5x interquartile range.</p></caption><graphic xlink:href="41598_2024_82501_Fig2_HTML" id="MO2"/></fig></p><p id="Par37">Finally, we assessed whether workers with explainable AI spent more time on making their quality assessments. For this, we analyzed whether the workers&#x02019; median decision speeds across the 200 product images differed. No statistically significant difference (<inline-formula id="IEq40"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t=0.308$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq40.gif"/></alternatives></inline-formula>, <inline-formula id="IEq41"><alternatives><tex-math id="M42">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.380$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq41.gif"/></alternatives></inline-formula>) was observed between both treatments (<inline-formula id="IEq42"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {mean} = {5.01}\,{\textrm{s}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq42.gif"/></alternatives></inline-formula> for black-box AI, <inline-formula id="IEq43"><alternatives><tex-math id="M44">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {mean} = {4.88}\,{\textrm{s}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq43.gif"/></alternatives></inline-formula> for explainable AI). Therefore, explainable AI improved task performance without affecting the productivity of the workers.</p></sec><sec id="Sec13"><title>Study 2: medical experiment</title><p id="Par38">In the medical experiment, radiologists were asked to visually inspect 50 chest X-ray images and decide whether at least one lung lesion was visible (Fig. <xref rid="Fig1" ref-type="fig">1</xref>B). Visual inspection tasks like ours are common in medicine across various subdisciplines<sup><xref ref-type="bibr" rid="CR54">54</xref>,<xref ref-type="bibr" rid="CR55">55</xref></sup>.</p><p id="Par39">Radiologists augmented with explainable AI outperformed peers with black-box AI. Radiologists with black-box AI achieved a balanced accuracy with a mean of only 79.1%, whereas radiologists with explainable AI achieved a balanced accuracy with a mean of 83.8% (Fig. <xref rid="Fig3" ref-type="fig">3</xref>A). We again estimated the treatment effect of explainable AI by regressing the balanced accuracy on the treatment (black-box AI <inline-formula id="IEq44"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$= 0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq44.gif"/></alternatives></inline-formula>, explainable AI <inline-formula id="IEq45"><alternatives><tex-math id="M46">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$= 1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq45.gif"/></alternatives></inline-formula>). The regression results show that the treatment effect of explainable AI is statistically significant and large (<inline-formula id="IEq46"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta =4.693$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq46.gif"/></alternatives></inline-formula>, <inline-formula id="IEq47"><alternatives><tex-math id="M48">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SE =1.800$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq47.gif"/></alternatives></inline-formula>, <inline-formula id="IEq48"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t = 2.608$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq48.gif"/></alternatives></inline-formula>, <inline-formula id="IEq49"><alternatives><tex-math id="M50">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.010$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq49.gif"/></alternatives></inline-formula>, 95&#x000a0;%&#x000a0;<inline-formula id="IEq50"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {CI} = [1.127, 8.259]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq50.gif"/></alternatives></inline-formula>); that is, an improvement of 4.7 percentage points. All results remain statistically significant when including relevant control variables (tenure, self-reported IT skills, and decision speed) in the regression model (see Supplement <xref rid="MOESM1" ref-type="media">H</xref>). In contrast to the manufacturing experiment, no difference in task performance with respect to the disease detection rate was observed; radiologists in both treatment arms achieved a disease detection rate with a mean of 90.4% (Fig. <xref rid="Fig3" ref-type="fig">3</xref>B). This was also observed when regressing the disease detection rate on the treatment (<inline-formula id="IEq51"><alternatives><tex-math id="M52">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta =-0.014$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq51.gif"/></alternatives></inline-formula>, <inline-formula id="IEq52"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SE =2.244$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq52.gif"/></alternatives></inline-formula>, <inline-formula id="IEq53"><alternatives><tex-math id="M54">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t = -0.006$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq53.gif"/></alternatives></inline-formula>, <inline-formula id="IEq54"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.995$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq54.gif"/></alternatives></inline-formula>, 95&#x000a0;%&#x000a0;<inline-formula id="IEq55"><alternatives><tex-math id="M56">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {CI} = [-4.460, 4.433]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq55.gif"/></alternatives></inline-formula>). This can be expected since missing a lung lesion has more serious consequences than erroneously believing a lung lesion is visible; thus, leading to conservative decision-making of radiologists. Therefore, we additionally inspected precision as a task performance metric. We find that radiologists augmented with explainable AI were significantly more precise (improvement of 6.4 percentage points, <inline-formula id="IEq56"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.014$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq56.gif"/></alternatives></inline-formula>) in identifying lung lesions compared to radiologists with black-box AI (see Supplement <xref rid="MOESM1" ref-type="media">G</xref>).</p><p id="Par40">As in Study&#x000a0;1, we found that radiologists with explainable AI followed accurate AI predictions more often than radiologists with black-box AI treatment (<inline-formula id="IEq57"><alternatives><tex-math id="M58">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {mean} = 72.4\%$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq57.gif"/></alternatives></inline-formula> for black-box AI, <inline-formula id="IEq58"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {mean} = 82.1\%$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq58.gif"/></alternatives></inline-formula> for explainable AI). In particular, radiologists supported by black-box AI were 54.2% times more likely to erroneously overrule an AI prediction, although it was correct (<inline-formula id="IEq59"><alternatives><tex-math id="M60">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t=3.084$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq59.gif"/></alternatives></inline-formula>, <inline-formula id="IEq60"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.001$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq60.gif"/></alternatives></inline-formula>). We observed that radiologists with explainable AI only overruled 50.8% of the wrong AI predictions compared to 57.7% for radiologists with black-box AI treatment.<fig id="Fig3"><label>Figure 3</label><caption><p>Results of medical experiment. The boxplots compare the task performance between the two treatments: black-box AI and explainable AI. The task performance is measured by the balanced accuracy (<bold>A</bold>) and the disease detection rate (<bold>B</bold>) based on the quality assessment of radiologists and the ground-truth labels of the chest X-ray images. A balanced accuracy of 50% provides a na&#x000ef;ve baseline corresponding to a random guess (black dotted line). The standalone AI algorithm attains a balanced accuracy of 82.2% and a disease detection rate of 71.4% (orange dashed lines). Statistical significance is based on a one-sided Welch&#x02019;s <italic>t</italic>-test (<sup>***</sup><inline-formula id="IEq61"><alternatives><tex-math id="M62">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P&#x0003c;0.001$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq61.gif"/></alternatives></inline-formula>, <sup>**</sup><inline-formula id="IEq62"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P&#x0003c;0.01$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq62.gif"/></alternatives></inline-formula>, <sup>*</sup><inline-formula id="IEq63"><alternatives><tex-math id="M64">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P&#x0003c;0.05$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq63.gif"/></alternatives></inline-formula>). In the boxplots, the center line denotes the median; box limits are upper and lower quartiles; whiskers are defined as the 1.5x interquartile range.</p></caption><graphic xlink:href="41598_2024_82501_Fig3_HTML" id="MO3"/></fig></p><p id="Par41">Again, we assessed the decision speed of radiologists in both treatment arms. We found no significant difference (<inline-formula id="IEq64"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t=0.392$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq64.gif"/></alternatives></inline-formula>, <inline-formula id="IEq65"><alternatives><tex-math id="M66">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.348$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq65.gif"/></alternatives></inline-formula>) between both treatments (<inline-formula id="IEq66"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {mean} = {10.71}\,{\textrm{s}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq66.gif"/></alternatives></inline-formula> for black-box AI, <inline-formula id="IEq67"><alternatives><tex-math id="M68">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {mean} = {10.29}\,{\textrm{s}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq67.gif"/></alternatives></inline-formula> for explainable AI). Thus, task performance was improved by explainable AI without reducing the productivity of the radiologists.</p></sec><sec id="Sec14"><title>Comparison to non-experts</title><p id="Par42">Additionally, we repeated the manufacturing task with non-experts recruited from Amazon MTurk as a robustness check (Supplement <xref rid="MOESM1" ref-type="media">J</xref>). Typically, non-experts can not leverage explanations in the same way as domain experts due to missing domain knowledge. Hence, we were interested whether the large treatment effect of explainable AI on task performance we observed with domain experts transfers also to non-experts.</p><p id="Par43">We found that non-experts supported by explainable AI also achieved a higher task performance than non-experts supported by black-box AI. Task performance of non-experts augmented with explainable AI was improved by 6.3 percentage points with respect to balanced accuracy. However, the treatment effect of explainable AI is slightly smaller as compared to the experiment with domain experts (where the balanced accuracy increased by 7.7 percentage points). Furthermore, non-experts with explainable AI achieved a higher defect detection rate with an improvement of 11.3 percentage points. This is of a similar effect size as in the real-world experiment (11.0 percentage points). The treatment effects in both metrics were again statistically significant (balanced accuracy: <inline-formula id="IEq68"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta =6.252$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq68.gif"/></alternatives></inline-formula>, <inline-formula id="IEq69"><alternatives><tex-math id="M70">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SE =1.733$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq69.gif"/></alternatives></inline-formula>, <inline-formula id="IEq70"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t = 3.608$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq70.gif"/></alternatives></inline-formula>, <inline-formula id="IEq71"><alternatives><tex-math id="M72">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P&#x0003c;0.001$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq71.gif"/></alternatives></inline-formula>, 95&#x000a0;%&#x000a0;<inline-formula id="IEq72"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {CI} = [2.841, 9.664]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq72.gif"/></alternatives></inline-formula>; defect detection rate: <inline-formula id="IEq73"><alternatives><tex-math id="M74">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta =11.271$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq73.gif"/></alternatives></inline-formula>, <inline-formula id="IEq74"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SE =3.276$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq74.gif"/></alternatives></inline-formula>, <inline-formula id="IEq75"><alternatives><tex-math id="M76">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t = 3.440$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq75.gif"/></alternatives></inline-formula>, <inline-formula id="IEq76"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.001$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq76.gif"/></alternatives></inline-formula>, 95&#x000a0;%&#x000a0;<inline-formula id="IEq77"><alternatives><tex-math id="M78">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {CI} = [4.822, 17.720]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq77.gif"/></alternatives></inline-formula>). For more details, see Supplement <xref rid="MOESM1" ref-type="media">J</xref>.</p></sec></sec><sec id="Sec15"><title>Discussion</title><p id="Par44">The &#x0201c;age of AI&#x0201d; redefines the way humans and machines collaborate, thus raising questions about how human&#x02013;AI collaborations can be effectively designed. As we show, the effectiveness of human&#x02013;AI collaboration largely depends on the extent to which humans incorporate correct AI predictions and overrule wrong ones. However, many state-of-the-art AI algorithms operate as black-box, thus making it difficult for humans to compare the reasoning of the AI to their own domain knowledge. In this paper, we contribute a unique perspective by studying the impact of AI explainability through heatmaps on task performance of domain experts in human&#x02013;AI collaboration, presenting empirical evidence from different domains with robust and generalizable results.</p><sec id="Sec16"><title>Interpretation</title><p id="Par45">We conducted two preregistered experiments to estimate the effect of explainable AI through heatmaps in human&#x02013;AI collaboration in real-world visual inspection tasks. Our results demonstrate that domain experts make subpar decisions when they are supported by a black-box AI algorithm with opaque predictions. In contrast, we find that explanations from an explainable AI are a powerful decision aid. Explanations were provided in the form of visual heatmaps, which provide a clear and intuitive way of highlighting areas that are determinants of AI predictions and, additionally, have been shown to perform well in localizing relevant parts of an image<sup><xref ref-type="bibr" rid="CR23">23</xref>&#x02013;<xref ref-type="bibr" rid="CR26">26</xref></sup>. The explanations do not provide more information from an AI perspective (i.e., the prediction performance is identical), but rather make the information more accessible to domain experts. Specifically, compared to black-box AI, augmenting domain experts with visual heatmaps improved the task performance by 7.7 percentage points (95% CI: 3.3% to 12.0%, <inline-formula id="IEq78"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.001$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq78.gif"/></alternatives></inline-formula>) in a manufacturing experiment and by 4.7 percentage points (95% CI: 1.1% to 8.3%, <inline-formula id="IEq79"><alternatives><tex-math id="M80">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.010$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82501_Article_IEq79.gif"/></alternatives></inline-formula>) in a medical experiment. In the manufacturing experiment, 73.1% of the domain experts even outperformed the standalone AI algorithm when they were augmented with explainable AI through heatmaps. The prime reason was that domain experts supported by explainable AI were more likely to follow AI predictions when they were accurate and more likely to overrule them when they were wrong.</p><p id="Par46">Our work contributes experimental evidence to the literature on human&#x02013;AI collaboration<sup><xref ref-type="bibr" rid="CR30">30</xref>&#x02013;<xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR67">67</xref>&#x02013;<xref ref-type="bibr" rid="CR75">75</xref></sup>. Cognitive biases of humans in the use of AI may impede decision-making processes and further provide a barrier to the wider adoption of human&#x02013;AI collaboration. For example, overreliance and algorithm aversion are two common cognitive biases that refer to the decision-making processes and the adoption of AI systems, respectively. To counteract these, prior literature has presented several remedies, such as describing the functional logic of an algorithm<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>, giving users permission to modify an algorithm<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, letting users integrate their own forecasts into an algorithm<sup><xref ref-type="bibr" rid="CR72">72</xref></sup>, or cognitive forcing functions that force humans to analytically engage with the AI recommendations<sup><xref ref-type="bibr" rid="CR76">76</xref></sup>. Our paper presents evidence of an effective alternative; that is, explaining individual predictions from an otherwise opaque AI algorithm. Such explanations allow domain experts to validate how an AI arrives at a certain prediction. Naturally, it is possible that cognitive biases may also extend toward explanations of an AI, which has been reported previously in experiments with non-experts<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. Yet, a strength of our experiment is that we account for cognitive biases in human decision-making (e.g., overreliance) due to the fact that we measure the overall performance of the actual decisions based on the AI system in the field. Yet, we acknowledge that we can only control for biases in the decision-making processes of domain experts and not biases related to whether managers may make decisions against the adoption of AI systems in the first place (e.g., due to algorithm aversion).</p><p id="Par47">A strength of our study is that we gather empirical evidence of improved task performance by explainable AI through heatmaps compared to black-box AI. In particular, by performing experiments in two different settings, we demonstrate that these results are generalizable. Unlike previous works on studying task performance in human&#x02013;AI collaboration<sup><xref ref-type="bibr" rid="CR41">41</xref>&#x02013;<xref ref-type="bibr" rid="CR44">44</xref></sup>, we (i) conducted experiments of two real-world job tasks in manufacturing and medicine and (ii) recruited domain experts for those tasks, i.e., factory workers and radiologists. When experiments use simplified decision tasks (e.g., object recognition) that are not representative of actual human work in the field, real-world validity is reduced. In contrast, our study has high external validity.</p><p id="Par48">Our work is orthogonal to the literature on explainable AI in computer science, where the main goal is to develop and evaluate new methods for explaining black-box AI algorithms. Contrary, we are interested in a behavioral outcome, namely task performance in human&#x02013;AI collaboration. A previous study on the effect of explainable AI on task performance found an improvement of 1.5 percentage points in accuracy relative to black-box AI<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. However, their experiment was designed such that participants were not only shown the real explainable AI but also systemically biased explainable AI. This could have decreased the trust of the participants in the AI and, thus, explain the smaller treatment effect in comparison to our experiments. Prior work has also made use of expert annotations as a proxy for explainable AI<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. However, this prevents any conclusion on whether real explainable AI improves task performance.</p><p id="Par49">Policy initiatives in many countries aim to promote transparency in AI algorithms (e.g., the United States<sup><xref ref-type="bibr" rid="CR77">77</xref></sup> and the European Union<sup><xref ref-type="bibr" rid="CR78">78</xref></sup>). These efforts are usually motivated from the perspective of ethics, regulation, and safety<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR79">79</xref>&#x02013;<xref ref-type="bibr" rid="CR81">81</xref></sup>. Our research suggests that the benefits of algorithmic transparency are more profound: augmenting domain experts with explainable AI can enable better decisions with benefits for individuals, organizations, and society.</p></sec><sec id="Sec17"><title>Implications</title><p id="Par50">Improving the task performance of domain experts has wide-ranging practical implications across fields&#x02014;not only in manufacturing and medicine as documented in this work. For example, factory workers at <italic>Siemens</italic> augmented with explainable AI through heatmaps were able to identify 13% more defects than peers augmented with black-box AI. Thus, explainable AI can help reduce costs for manufacturing companies by filtering out defective products at the earliest possible stage. It also assists workers in succeeding at their work tasks, potentially contributing to higher work satisfaction. Similarly, in medicine, task performance of physicians is crucial, especially for important tasks such as identifying possibly cancerogenous lung lesions. By showing that the results are consistent across different settings, we demonstrate that our insights are likely to be generalizable to other visual inspection tasks.</p></sec><sec id="Sec18"><title>Limitations</title><p id="Par51">One limitation of our research is that we, in both experiments, studied one specific human&#x02013;AI work setting (a visual inspection task) with one specific form of explainability (a heatmap indicating the location of potential quality defects or lung lesions). However, this experimental task is representative of many real-world human&#x02013;AI work settings and heatmaps are standard in explaining AI predictions of images. We also show that other heatmap algorithms lead to similar results (see Supplement <xref rid="MOESM1" ref-type="media">F</xref>). Nevertheless, it is possible that other explainable AI methods may have a different effect on task performance.</p><p id="Par52">We also acknowledge reservations against using explainable AI in general. Explainable AI can be fooled by adversarial attacks<sup><xref ref-type="bibr" rid="CR82">82</xref></sup> or may itself generate explanations that are unreliable and thus lead to misleading conclusions<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. Nevertheless, it is likely that performance improvements from explainable AI can be achieved in other settings, where explanations serve as a decision aid. Further, it is important to note that, in both experiments, 14% of the images contained quality defects/lung lesions. Thus, quality defects/lung lesions were more prevalent than what domain experts would typically encounter in their respective job. This discrepancy might have influenced their prior expectations while performing the task. However, as participants in both treatment arms were shown exactly the same images, this factor likely had minimal impact on our findings.</p></sec><sec id="Sec19"><title>Future work</title><p id="Par53">In this study, we focused on two distinct settings, where workers perform visual inspection tasks, namely manufacturing and medicine. Future research may seek to generalize our findings to entirely different settings, such as finance (e.g., fraud detection) or human resources (e.g., candidate screening).</p><p id="Par54">Our study relied on heatmaps as the explainable AI method for images. However, a plethora of different methods exist. For example, counterfactual explanations are another common approach to explain a black-box AI model<sup><xref ref-type="bibr" rid="CR83">83</xref></sup>. There, the goal is to identify minimal changes in the image to alter the prediction of the AI model, allowing a counterfactual image to be compared to the original. This comparison can highlight relevant features, such as lung lesions in X-ray images. Future research could investigate whether different explainable AI methods, such as counterfactual explanations, impact the task performance in human&#x02013;AI collaboration differently.</p><p id="Par55">Further, the tasks in this work, which had to be performed by domain experts, relied only on unstructured data (i.e., images). However, structured data require different explainable AI methods than unstructured data, because of which heatmaps may not be applicable. We invite future work to further analyze the effect of explainable AI on task performance using appropriate methods for structured data such as Shapley additive explanations (SHAP)<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR75">75</xref></sup> or local interpretable model-agnostic explanations (LIME)<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, which try to estimate the impact of each feature on the predicted outcome.</p></sec><sec id="Sec20"><title>Conclusion</title><p id="Par56">This study demonstrates that explainable AI in the form of heatmaps significantly enhances task performance in human&#x02013;AI collaboration compared to black-box AI. By providing visual heatmaps, explainable AI supports domain experts in making more accurate visual inspection decisions, allowing them to validate AI predictions against their own domain knowledge. This approach led to notable performance improvements in both manufacturing and medical inspection tasks. These findings highlight the potential value of explainable AI in increasing the reliability and utility of human&#x02013;AI collaboration across fields.</p></sec></sec><sec id="Sec21" sec-type="supplementary-material"><title>Supplementary Information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2024_82501_MOESM1_ESM.pdf"><caption><p>Supplementary Information.</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Julian Senoner and Simon Schallmoser.</p></fn></fn-group><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-024-82501-9.</p></sec><ack><title>Acknowledgements</title><p>The authors thank all participants, as well as Davide Vecchione, Burak Seyid, and Alexander Dierolf for enabling the manufacturing experiment at <italic>Siemens</italic>. SF acknowledges funding via Swiss National Science Foundation Grant 186932. TN acknowledges funding from <italic>Siemens</italic>; however, without competing interest.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>J.S., S.S., and B.K. conceptualized the study and designed the experiments. S.F. and T.N. reviewed and provided feedback on the research design. J.S., S.S., and B.K. conducted the experiments and collected the data. J.S. and S.S. analyzed the data and prepared the figures. All authors contributed to interpreting the results and writing the manuscript. J.S. and S.S. contributed equally to the study.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The code to reproduce the results from all studies is publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/simonschallmoser/ExplainableAIimprovesTaskPerformance">https://github.com/simonschallmoser/ExplainableAIimprovesTaskPerformance</ext-link>. Data will be made available by the corresponding author upon reasonable request.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par60">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Brynjolfsson</surname><given-names>E</given-names></name><name><surname>Mitchell</surname><given-names>T</given-names></name></person-group><article-title>What can machine learning do? Workforce implications</article-title><source>Science</source><year>2017</year><volume>358</volume><fpage>1530</fpage><lpage>1534</lpage><pub-id pub-id-type="doi">10.1126/science.aap8062</pub-id><pub-id pub-id-type="pmid">29269459</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Brynjolfsson, E. &#x00026; Mitchell, T. What can machine learning do? Workforce implications. <italic>Science</italic><bold>358</bold>, 1530&#x02013;1534 (2017).<pub-id pub-id-type="pmid">29269459</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Perrault, R. &#x00026; Clark, J. Artificial intelligence index report 2024. Human-centered artificial intelligence. United States of America. <ext-link ext-link-type="uri" xlink:href="https://policycommons.net/artifacts/12089781/hai_ai-index-report-2024/12983534/">https://policycommons.net/artifacts/12089781/hai_ai-index-report-2024/12983534/</ext-link>. Accessed 26 Apr 2024. CID: 20.500.12592/h70s46h (2024).</mixed-citation></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Bertolini</surname><given-names>M</given-names></name><name><surname>Mezzogori</surname><given-names>D</given-names></name><name><surname>Neroni</surname><given-names>M</given-names></name><name><surname>Zammori</surname><given-names>F</given-names></name></person-group><article-title>Machine learning for industrial applications: A comprehensive literature review</article-title><source>Expert Syst. Appl.</source><year>2021</year><volume>175</volume><fpage>114820</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2021.114820</pub-id></element-citation><mixed-citation id="mc-CR3" publication-type="journal">Bertolini, M., Mezzogori, D., Neroni, M. &#x00026; Zammori, F. Machine learning for industrial applications: A comprehensive literature review. <italic>Expert Syst. Appl.</italic><bold>175</bold>, 114820 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Scheetz</surname><given-names>J</given-names></name><name><surname>Rothschild</surname><given-names>P</given-names></name><name><surname>McGuinness</surname><given-names>M</given-names></name><name><surname>Hadoux</surname><given-names>X</given-names></name><name><surname>Soyer</surname><given-names>HP</given-names></name><name><surname>Janda</surname><given-names>M</given-names></name><name><surname>Condon</surname><given-names>JJJ</given-names></name><name><surname>Oakden-Rayner</surname><given-names>L</given-names></name><name><surname>Palmer</surname><given-names>LJ</given-names></name><name><surname>Keel</surname><given-names>S</given-names></name><name><surname>van Wijngaarden</surname><given-names>P</given-names></name></person-group><article-title>A survey of clinicians on the use of artificial intelligence in ophthalmology, dermatology, radiology and radiation oncology</article-title><source>Sci. Rep.</source><year>2021</year><volume>11</volume><fpage>5193</fpage><pub-id pub-id-type="doi">10.1038/s41598-021-84698-5</pub-id><pub-id pub-id-type="pmid">33664367</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Scheetz, J. et al. A survey of clinicians on the use of artificial intelligence in ophthalmology, dermatology, radiology and radiation oncology. <italic>Sci. Rep.</italic><bold>11</bold>, 5193 (2021).<pub-id pub-id-type="pmid">33664367</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Cazzaniga, M., Jaumotte, F., Li, L., Melina, G., Panton, A.&#x000a0;J., Pizzinelli, C., Rockall, E.&#x000a0;J. &#x00026; Tavares, M.&#x000a0;M. Gen-AI: Artificial intelligence and the future of work. International Monetary Fund. Staff Discussion Notes 2024/001. <ext-link ext-link-type="uri" xlink:href="https://www.imf.org/en/Publications/Staff-Discussion-Notes/Issues/2024/01/14/Gen-AI-Artificial-Intelligence-and-the-Future-of-Work-542379">https://www.imf.org/en/Publications/Staff-Discussion-Notes/Issues/2024/01/14/Gen-AI-Artificial-Intelligence-and-the-Future-of-Work-542379</ext-link> (2024).</mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Simonyan, K. &#x00026; Zisserman, A. Very deep convolutional networks for large-scale image recognition. In <italic>International Conference on Learning Representations</italic> (2015).</mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &#x00026; Sun, J. Deep residual learning for image recognition. In <italic>IEEE Conference on Computer Vision and Pattern Recognition</italic>. 770&#x02013;778 (2016).</mixed-citation></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Rudin</surname><given-names>C</given-names></name></person-group><article-title>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</article-title><source>Nat. Mach. Intell.</source><year>2019</year><volume>1</volume><fpage>206</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1038/s42256-019-0048-x</pub-id><pub-id pub-id-type="pmid">35603010</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. <italic>Nat. Mach. Intell.</italic><bold>1</bold>, 206&#x02013;215 (2019).<pub-id pub-id-type="pmid">35603010</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Guidotti</surname><given-names>R</given-names></name><name><surname>Monreale</surname><given-names>A</given-names></name><name><surname>Ruggieri</surname><given-names>S</given-names></name><name><surname>Turini</surname><given-names>F</given-names></name><name><surname>Giannotti</surname><given-names>F</given-names></name><name><surname>Pedreschi</surname><given-names>D</given-names></name></person-group><article-title>A survey of methods for explaining black box models</article-title><source>ACM Comput. Surv.</source><year>2018</year><volume>51</volume><fpage>1</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1145/3236009</pub-id></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Guidotti, R. et al. A survey of methods for explaining black box models. <italic>ACM Comput. Surv.</italic><bold>51</bold>, 1&#x02013;42 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Murdoch</surname><given-names>WJ</given-names></name><name><surname>Singh</surname><given-names>C</given-names></name><name><surname>Kumbier</surname><given-names>K</given-names></name><name><surname>Abbasi-Asl</surname><given-names>R</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name></person-group><article-title>Definitions, methods, and applications in interpretable machine learning</article-title><source>Proc. Natl. Acad. Sci.</source><year>2019</year><volume>116</volume><fpage>22071</fpage><lpage>22080</lpage><pub-id pub-id-type="doi">10.1073/pnas.1900654116</pub-id><pub-id pub-id-type="pmid">31619572</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Murdoch, W. J., Singh, C., Kumbier, K., Abbasi-Asl, R. &#x00026; Yu, B. Definitions, methods, and applications in interpretable machine learning. <italic>Proc. Natl. Acad. Sci.</italic><bold>116</bold>, 22071&#x02013;22080 (2019).<pub-id pub-id-type="pmid">31619572</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Gunning</surname><given-names>D</given-names></name><name><surname>Stefik</surname><given-names>M</given-names></name><name><surname>Choi</surname><given-names>J</given-names></name><name><surname>Miller</surname><given-names>T</given-names></name><name><surname>Stumpf</surname><given-names>S</given-names></name><name><surname>Yang</surname><given-names>G-Z</given-names></name></person-group><article-title>XAI&#x02014;Explainable artificial intelligence</article-title><source>Sci. Robot.</source><year>2019</year><volume>4</volume><fpage>eaay7120</fpage><pub-id pub-id-type="doi">10.1126/scirobotics.aay7120</pub-id><pub-id pub-id-type="pmid">33137719</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Gunning, D. et al. XAI&#x02014;Explainable artificial intelligence. <italic>Sci. Robot.</italic><bold>4</bold>, eaay7120 (2019).<pub-id pub-id-type="pmid">33137719</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Cheng, H.-F., Wang, R., Zhang, Z., O&#x02019;Connell, F., Gray, T., Harper, F.&#x000a0;M. &#x00026; Zhu, H. Explaining decision-making algorithms through UI. In <italic>CHI Conference on Human Factors in Computing Systems</italic>. 1&#x02013;12 (2019).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Liao, Q.&#x000a0;V., Gruen, D. &#x00026; Miller, S. Questioning the AI: Informing design practices for explainable AI user experiences. In <italic>CHI Conference on Human Factors in Computing Systems</italic> (2020).</mixed-citation></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Bodria</surname><given-names>F</given-names></name><name><surname>Giannotti</surname><given-names>F</given-names></name><name><surname>Guidotti</surname><given-names>R</given-names></name><name><surname>Naretto</surname><given-names>F</given-names></name><name><surname>Pedreschi</surname><given-names>D</given-names></name><name><surname>Rinzivillo</surname><given-names>S</given-names></name></person-group><article-title>Benchmarking and survey of explanation methods for black box models</article-title><source>Data Min. Knowl. Discov.</source><year>2023</year><volume>37</volume><fpage>1719</fpage><lpage>1778</lpage><pub-id pub-id-type="doi">10.1007/s10618-023-00933-9</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Bodria, F. et al. Benchmarking and survey of explanation methods for black box models. <italic>Data Min. Knowl. Discov.</italic><bold>37</bold>, 1719&#x02013;1778 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Dwivedi</surname><given-names>R</given-names></name><name><surname>Dave</surname><given-names>D</given-names></name><name><surname>Naik</surname><given-names>H</given-names></name><name><surname>Singhal</surname><given-names>S</given-names></name><name><surname>Omer</surname><given-names>R</given-names></name><name><surname>Patel</surname><given-names>P</given-names></name><name><surname>Qian</surname><given-names>B</given-names></name><name><surname>Wen</surname><given-names>Z</given-names></name><name><surname>Shah</surname><given-names>T</given-names></name><name><surname>Morgan</surname><given-names>G</given-names></name><name><surname>Ranjan</surname><given-names>R</given-names></name></person-group><article-title>Explainable AI (XAI): Core ideas, techniques, and solutions</article-title><source>ACM Comput. Surv.</source><year>2023</year><volume>55</volume><fpage>1</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1145/3561048</pub-id></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Dwivedi, R. et al. Explainable AI (XAI): Core ideas, techniques, and solutions. <italic>ACM Comput. Surv.</italic><bold>55</bold>, 1&#x02013;33 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Samek, W., Montavon, G., Vedaldi, A., Hansen, L.&#x000a0;K. &#x00026; M&#x000fc;ller, K.-R. <italic>Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</italic> (Springer Nature, 2019).</mixed-citation></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Samek</surname><given-names>W</given-names></name><name><surname>Montavon</surname><given-names>G</given-names></name><name><surname>Lapuschkin</surname><given-names>S</given-names></name><name><surname>Anders</surname><given-names>CJ</given-names></name><name><surname>Muller</surname><given-names>K-R</given-names></name></person-group><article-title>Explaining deep neural networks and beyond: A review of methods and applications</article-title><source>Proc. IEEE</source><year>2021</year><volume>109</volume><fpage>247</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2021.3060483</pub-id></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Samek, W., Montavon, G., Lapuschkin, S., Anders, C. J. &#x00026; Muller, K.-R. Explaining deep neural networks and beyond: A review of methods and applications. <italic>Proc. IEEE</italic><bold>109</bold>, 247&#x02013;278 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Molnar, C. <italic>Interpretable Machine Learning: A Guide for Making Black Box Models Interpretable</italic> (Lulu. com, 2019).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Ribeiro, M.&#x000a0;T., Singh, S. &#x00026; Guestrin, C. Why should I trust you? Explaining the predictions of any classifier. In <italic>ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic> (2016).</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Lundberg, S. &#x00026; Lee, S.-I. A unified approach to interpreting model predictions. In <italic>Advances in Neural Information Processing Systems</italic> (2017).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Simonyan, K., Vedaldi, A. &#x00026; Zisserman, A. Deep inside convolutional networks: Visualising image classification models and saliency maps. In <italic>Workshop at International Conference on Learning Representations</italic> (2014).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Selvaraju, R.&#x000a0;R., Cogswell, M., Das, A., Vedantam, R., Parikh, D. &#x00026; Batra, D. Grad-CAM: Visual explanations from deep networks via gradient-based localization. In <italic>IEEE International Conference on Computer Vision</italic>. 618&#x02013;626 (2017).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Bergmann, P., L&#x000f6;we, S., Fauser, M., Sattlegger, D. &#x00026; Steger, C. Improving unsupervised defect segmentation by applying structural similarity to autoencoders. Preprint <italic>arXiv</italic>10.48550/arXiv.1807.02011 (2019).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Bergmann, P., Fauser, M., Sattlegger, D. &#x00026; Steger, C. MVTec AD &#x02014; A comprehensive real-world dataset for unsupervised anomaly detection. In <italic>IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> (2019).</mixed-citation></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>A</given-names></name><name><surname>Bockmayr</surname><given-names>M</given-names></name><name><surname>H&#x000e4;gele</surname><given-names>M</given-names></name><name><surname>Wienert</surname><given-names>S</given-names></name><name><surname>Heim</surname><given-names>D</given-names></name><name><surname>Hellweg</surname><given-names>K</given-names></name><name><surname>Ishii</surname><given-names>M</given-names></name><name><surname>Stenzinger</surname><given-names>A</given-names></name><name><surname>Hocke</surname><given-names>A</given-names></name><name><surname>Denkert</surname><given-names>C</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>K-R</given-names></name><name><surname>Klauschen</surname><given-names>F</given-names></name></person-group><article-title>Morphological and molecular breast cancer profiling through explainable machine learning</article-title><source>Nat. Mach. Intell.</source><year>2021</year><volume>3</volume><fpage>355</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/s42256-021-00303-4</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Binder, A. et al. Morphological and molecular breast cancer profiling through explainable machine learning. <italic>Nat. Mach. Intell.</italic><bold>3</bold>, 355&#x02013;366 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Saporta</surname><given-names>A</given-names></name><name><surname>Gui</surname><given-names>X</given-names></name><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Pareek</surname><given-names>A</given-names></name><name><surname>Truong</surname><given-names>SQH</given-names></name><name><surname>Nguyen</surname><given-names>CDT</given-names></name><name><surname>Ngo</surname><given-names>V-D</given-names></name><name><surname>Seekins</surname><given-names>J</given-names></name><name><surname>Blankenberg</surname><given-names>FG</given-names></name><name><surname>Ng</surname><given-names>AY</given-names></name><name><surname>Lungren</surname><given-names>MP</given-names></name><name><surname>Rajpurkar</surname><given-names>P</given-names></name></person-group><article-title>Benchmarking saliency methods for chest X-ray interpretation</article-title><source>Nat. Mach. Intell.</source><year>2022</year><volume>4</volume><fpage>867</fpage><lpage>878</lpage><pub-id pub-id-type="doi">10.1038/s42256-022-00536-x</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Saporta, A. et al. Benchmarking saliency methods for chest X-ray interpretation. <italic>Nat. Mach. Intell.</italic><bold>4</bold>, 867&#x02013;878 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>F&#x000fc;gener</surname><given-names>A</given-names></name><name><surname>Grahl</surname><given-names>J</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Ketter</surname><given-names>W</given-names></name></person-group><article-title>Cognitive challenges in human-artificial intelligence collaboration: Investigating the path toward productive delegation</article-title><source>Inf. Syst. Res.</source><year>2022</year><volume>33</volume><fpage>678</fpage><lpage>696</lpage><pub-id pub-id-type="doi">10.1287/isre.2021.1079</pub-id></element-citation><mixed-citation id="mc-CR27" publication-type="journal">F&#x000fc;gener, A., Grahl, J., Gupta, A. &#x00026; Ketter, W. Cognitive challenges in human-artificial intelligence collaboration: Investigating the path toward productive delegation. <italic>Inf. Syst. Res.</italic><bold>33</bold>, 678&#x02013;696 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">F&#x000fc;gener, A., Gupta, A., Grahl, J., Ketter, W. &#x00026; Taudien, A. Exploring user heterogeneity in human delegation behavior towards AI. In <italic>International Conference on Information Systems</italic> (2021).</mixed-citation></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Bauer</surname><given-names>K</given-names></name><name><surname>von Zahn</surname><given-names>M</given-names></name><name><surname>Hinz</surname><given-names>O</given-names></name></person-group><article-title>Please take over: XAI, delegation of authority, and domain knowledge</article-title><source>Preprint SSRN</source><year>2023</year><pub-id pub-id-type="doi">10.2139/ssrn.4512594</pub-id></element-citation><mixed-citation id="mc-CR29" publication-type="journal">Bauer, K., von Zahn, M. &#x00026; Hinz, O. Please take over: XAI, delegation of authority, and domain knowledge. <italic>Preprint SSRN</italic>10.2139/ssrn.4512594 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Dietvorst</surname><given-names>BJ</given-names></name><name><surname>Simmons</surname><given-names>JP</given-names></name><name><surname>Massey</surname><given-names>C</given-names></name></person-group><article-title>Algorithm aversion: People erroneously avoid algorithms after seeing them err</article-title><source>J. Exp. Psychol. Gen.</source><year>2015</year><volume>144</volume><fpage>114</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1037/xge0000033</pub-id><pub-id pub-id-type="pmid">25401381</pub-id>
</element-citation><mixed-citation id="mc-CR30" publication-type="journal">Dietvorst, B. J., Simmons, J. P. &#x00026; Massey, C. Algorithm aversion: People erroneously avoid algorithms after seeing them err. <italic>J. Exp. Psychol. Gen.</italic><bold>144</bold>, 114&#x02013;126 (2015).<pub-id pub-id-type="pmid">25401381</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Dietvorst</surname><given-names>BJ</given-names></name><name><surname>Simmons</surname><given-names>JP</given-names></name><name><surname>Massey</surname><given-names>C</given-names></name></person-group><article-title>Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them</article-title><source>Manag. Sci.</source><year>2018</year><volume>64</volume><fpage>1155</fpage><lpage>1170</lpage><pub-id pub-id-type="doi">10.1287/mnsc.2016.2643</pub-id></element-citation><mixed-citation id="mc-CR31" publication-type="journal">Dietvorst, B. J., Simmons, J. P. &#x00026; Massey, C. Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them. <italic>Manag. Sci.</italic><bold>64</bold>, 1155&#x02013;1170 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Dietvorst</surname><given-names>BJ</given-names></name><name><surname>Bharti</surname><given-names>S</given-names></name></person-group><article-title>People reject algorithms in uncertain decision domains because they have diminishing sensitivity to forecasting error</article-title><source>Psychol. Sci.</source><year>2020</year><volume>31</volume><fpage>1302</fpage><lpage>1314</lpage><pub-id pub-id-type="doi">10.1177/0956797620948841</pub-id><pub-id pub-id-type="pmid">32916083</pub-id>
</element-citation><mixed-citation id="mc-CR32" publication-type="journal">Dietvorst, B. J. &#x00026; Bharti, S. People reject algorithms in uncertain decision domains because they have diminishing sensitivity to forecasting error. <italic>Psychol. Sci.</italic><bold>31</bold>, 1302&#x02013;1314 (2020).<pub-id pub-id-type="pmid">32916083</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>JW</given-names></name><name><surname>Stein</surname><given-names>M-K</given-names></name><name><surname>Jensen</surname><given-names>TB</given-names></name></person-group><article-title>A systematic review of algorithm aversion in augmented decision making</article-title><source>J. Behav. Decis. Mak.</source><year>2020</year><volume>33</volume><fpage>220</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1002/bdm.2155</pub-id></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Burton, J. W., Stein, M.-K. &#x00026; Jensen, T. B. A systematic review of algorithm aversion in augmented decision making. <italic>J. Behav. Decis. Mak.</italic><bold>33</bold>, 220&#x02013;239 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Ben David, D., Resheff, Y.&#x000a0;S. &#x00026; Tron, T. Explainable AI and adoption of financial algorithmic advisors. In <italic>AAAI/ACM Conference on AI, Ethics, and Society</italic>. 390&#x02013;400 (2021).</mixed-citation></ref><ref id="CR35"><label>35.</label><citation-alternatives><element-citation id="ec-CR35" publication-type="journal"><person-group person-group-type="author"><name><surname>Choung</surname><given-names>H</given-names></name><name><surname>David</surname><given-names>P</given-names></name><name><surname>Ross</surname><given-names>A</given-names></name></person-group><article-title>Trust in AI and its role in the acceptance of AI technologies</article-title><source>Int. J. Hum.-Comput. Interact.</source><year>2023</year><volume>39</volume><fpage>1727</fpage><lpage>1739</lpage><pub-id pub-id-type="doi">10.1080/10447318.2022.2050543</pub-id></element-citation><mixed-citation id="mc-CR35" publication-type="journal">Choung, H., David, P. &#x00026; Ross, A. Trust in AI and its role in the acceptance of AI technologies. <italic>Int. J. Hum.-Comput. Interact.</italic><bold>39</bold>, 1727&#x02013;1739 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Nourani, M., Kabir, S., Mohseni, S. &#x00026; Ragan, E.&#x000a0;D. The effects of meaningful and meaningless explanations on trust and perceived system accuracy in intelligent systems. In <italic>AAAI Conference on Human Computation and Crowdsourcing</italic>. Vol. 7 (2019).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Panigutti, C., Beretta, A., Giannotti, F. &#x00026; Pedreschi, D. Understanding the impact of explanations on advice-taking: a user study for AI-based clinical decision support systems. In <italic>CHI Conference on Human Factors in Computing Systems</italic> (2022).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Bansal, G., Wu, T., Zhou, J., Fok, R., Nushi, B., Kamar, E., Ribeiro, M.&#x000a0;T. &#x00026; Weld, D. Does the whole exceed its parts? The effect of AI explanations on complementary team performance. In <italic>CHI Conference on Human Factors in Computing Systems</italic> (2021).</mixed-citation></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name><surname>Vasconcelos</surname><given-names>H</given-names></name><name><surname>J&#x000f6;rke</surname><given-names>M</given-names></name><name><surname>Grunde-McLaughlin</surname><given-names>M</given-names></name><name><surname>Gerstenberg</surname><given-names>T</given-names></name><name><surname>Bernstein</surname><given-names>MS</given-names></name><name><surname>Krishna</surname><given-names>R</given-names></name></person-group><article-title>Explanations can reduce overreliance on AI systems during decision-making</article-title><source>ACM Hum.-Comput. Interact.</source><year>2023</year><volume>7</volume><fpage>129</fpage></element-citation><mixed-citation id="mc-CR39" publication-type="journal">Vasconcelos, H. et al. Explanations can reduce overreliance on AI systems during decision-making. <italic>ACM Hum.-Comput. Interact.</italic><bold>7</bold>, 129 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>V</given-names></name><name><surname>Liao</surname><given-names>QV</given-names></name><name><surname>Wortman Vaughan</surname><given-names>J</given-names></name><name><surname>Bansal</surname><given-names>G</given-names></name></person-group><article-title>Understanding the role of human intuition on reliance in human&#x02013;AI decision-making with explanations</article-title><source>ACM Hum.-Comput. Interact.</source><year>2023</year><volume>7</volume><fpage>370</fpage></element-citation><mixed-citation id="mc-CR40" publication-type="journal">Chen, V., Liao, Q. V., Wortman Vaughan, J. &#x00026; Bansal, G. Understanding the role of human intuition on reliance in human&#x02013;AI decision-making with explanations. <italic>ACM Hum.-Comput. Interact.</italic><bold>7</bold>, 370 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Bu&#x000e7;inca, Z., Lin, P., Gajos, K.&#x000a0;Z. &#x00026; Glassman, E.&#x000a0;L. Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems. In <italic>International Conference on Intelligent User Interfaces</italic> (2020).</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Chu, E., Roy, D. &#x00026; Andreas, J. Are visual explanations useful? A case study in model-in-the-loop prediction. Preprint 10.48550/arXiv.2007.12248 (2020).</mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Alufaisan, Y., Marusich, L.&#x000a0;R., Bakdash, J.&#x000a0;Z., Zhou, Y. &#x00026; Kantarcioglu, M. Does explainable artificial intelligence improve human decision-making? In <italic>AAAI Conference on Artificial Intelligence</italic>. Vol. 35 (2021).</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Schemmer, M., Kuehl, N., Benz, C., Bartos, A. &#x00026; Satzger, G. Appropriate reliance on AI advice: Conceptualization and the effect of explanations. In <italic>International Conference on Intelligent User Interfaces</italic> (2023).</mixed-citation></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Lundberg</surname><given-names>SM</given-names></name><name><surname>Erion</surname><given-names>G</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>DeGrave</surname><given-names>A</given-names></name><name><surname>Prutkin</surname><given-names>JM</given-names></name><name><surname>Nair</surname><given-names>B</given-names></name><name><surname>Katz</surname><given-names>R</given-names></name><name><surname>Himmelfarb</surname><given-names>J</given-names></name><name><surname>Bansal</surname><given-names>N</given-names></name><name><surname>Lee</surname><given-names>S-I</given-names></name></person-group><article-title>From local explanations to global understanding with explainable AI for trees</article-title><source>Nat. Mach. Intell.</source><year>2020</year><volume>2</volume><fpage>56</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1038/s42256-019-0138-9</pub-id><pub-id pub-id-type="pmid">32607472</pub-id>
</element-citation><mixed-citation id="mc-CR45" publication-type="journal">Lundberg, S. M. et al. From local explanations to global understanding with explainable AI for trees. <italic>Nat. Mach. Intell.</italic><bold>2</bold>, 56&#x02013;67 (2020).<pub-id pub-id-type="pmid">32607472</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Das</surname><given-names>N</given-names></name><etal/></person-group><article-title>Collaboration between explainable artificial intelligence and pulmonologists improves the accuracy of pulmonary function test interpretation</article-title><source>Eur. Respir. J.</source><year>2023</year><volume>61</volume><fpage>2201720</fpage><pub-id pub-id-type="doi">10.1183/13993003.01720-2022</pub-id><pub-id pub-id-type="pmid">37080566</pub-id>
</element-citation><mixed-citation id="mc-CR46" publication-type="journal">Das, N. et al. Collaboration between explainable artificial intelligence and pulmonologists improves the accuracy of pulmonary function test interpretation. <italic>Eur. Respir. J.</italic><bold>61</bold>, 2201720 (2023).<pub-id pub-id-type="pmid">37080566</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Gaube</surname><given-names>S</given-names></name><name><surname>Suresh</surname><given-names>H</given-names></name><name><surname>Raue</surname><given-names>M</given-names></name><name><surname>Lermer</surname><given-names>E</given-names></name><name><surname>Koch</surname><given-names>TK</given-names></name><name><surname>Hudecek</surname><given-names>MFC</given-names></name><name><surname>Ackery</surname><given-names>AD</given-names></name><name><surname>Grover</surname><given-names>SC</given-names></name><name><surname>Coughlin</surname><given-names>JF</given-names></name><name><surname>Frey</surname><given-names>D</given-names></name><name><surname>Kitamura</surname><given-names>FC</given-names></name><name><surname>Ghassemi</surname><given-names>M</given-names></name><name><surname>Colak</surname><given-names>E</given-names></name></person-group><article-title>Non-task expert physicians benefit from correct explainable AI advice when reviewing X-rays</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><fpage>1383</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-28633-w</pub-id><pub-id pub-id-type="pmid">36697450</pub-id>
</element-citation><mixed-citation id="mc-CR47" publication-type="journal">Gaube, S. et al. Non-task expert physicians benefit from correct explainable AI advice when reviewing X-rays. <italic>Sci. Rep.</italic><bold>13</bold>, 1383 (2023).<pub-id pub-id-type="pmid">36697450</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Jesus, S., Bel&#x000e9;m, C., Balayan, V., Bento, J., Saleiro, P., Bizarro, P. &#x00026; Gama, J. How can I choose an explainer? In <italic>ACM Conference on Fairness, Accountability, and Transparency</italic> (2021).</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Sivaraman, V., Bukowski, L.&#x000a0;A., Levin, J., Kahn, J.&#x000a0;M. &#x00026; Perer, A. Ignore, trust, or negotiate: Understanding Clinician Acceptance of AI-Based Treatment Recommendations in Health Care. In <italic>CHI Conference on Human Factors in Computing Systems</italic> (2023).</mixed-citation></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Jabbour</surname><given-names>S</given-names></name><name><surname>Fouhey</surname><given-names>D</given-names></name><name><surname>Shepard</surname><given-names>S</given-names></name><name><surname>Valley</surname><given-names>TS</given-names></name><name><surname>Kazerooni</surname><given-names>EA</given-names></name><name><surname>Banovic</surname><given-names>N</given-names></name><name><surname>Wiens</surname><given-names>J</given-names></name><name><surname>Sjoding</surname><given-names>MW</given-names></name></person-group><article-title>Measuring the impact of AI in the diagnosis of hospitalized patients: A randomized clinical vignette survey study</article-title><source>JAMA</source><year>2023</year><volume>330</volume><fpage>2275</fpage><lpage>2284</lpage><pub-id pub-id-type="doi">10.1001/jama.2023.22295</pub-id><pub-id pub-id-type="pmid">38112814</pub-id>
</element-citation><mixed-citation id="mc-CR50" publication-type="journal">Jabbour, S. et al. Measuring the impact of AI in the diagnosis of hospitalized patients: A randomized clinical vignette survey study. <italic>JAMA</italic><bold>330</bold>, 2275&#x02013;2284 (2023).<pub-id pub-id-type="pmid">38112814</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="book"><person-group person-group-type="author"><name><surname>Juran</surname><given-names>J</given-names></name><name><surname>Gryna</surname><given-names>F</given-names></name><name><surname>Bingham</surname><given-names>R</given-names></name></person-group><source>Quality Control Handbook</source><year>1979</year><publisher-name>McGraw-Hill</publisher-name></element-citation><mixed-citation id="mc-CR51" publication-type="book">Juran, J., Gryna, F. &#x00026; Bingham, R. <italic>Quality Control Handbook</italic> (McGraw-Hill, 1979).</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Hoyle, D. <italic>Quality Management Essentials</italic> (Butterworth-Heinemann, 2007).</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Baudin, M. &#x00026; Netland, T.&#x000a0;H. <italic>Introduction to Manufacturing: An Industrial Engineering and Management Perspective</italic> (Taylor &#x00026; Francis, 2022).</mixed-citation></ref><ref id="CR54"><label>54.</label><citation-alternatives><element-citation id="ec-CR54" publication-type="journal"><person-group person-group-type="author"><name><surname>Tschandl</surname><given-names>P</given-names></name><etal/></person-group><article-title>Human&#x02013;computer collaboration for skin cancer recognition</article-title><source>Nat. Med.</source><year>2020</year><volume>26</volume><fpage>1229</fpage><lpage>1234</lpage><pub-id pub-id-type="doi">10.1038/s41591-020-0942-0</pub-id><pub-id pub-id-type="pmid">32572267</pub-id>
</element-citation><mixed-citation id="mc-CR54" publication-type="journal">Tschandl, P. et al. Human&#x02013;computer collaboration for skin cancer recognition. <italic>Nat. Med.</italic><bold>26</bold>, 1229&#x02013;1234 (2020).<pub-id pub-id-type="pmid">32572267</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR55"><label>55.</label><citation-alternatives><element-citation id="ec-CR55" publication-type="journal"><person-group person-group-type="author"><name><surname>Pantanowitz</surname><given-names>L</given-names></name><name><surname>Valenstein</surname><given-names>PN</given-names></name><name><surname>Evans</surname><given-names>AJ</given-names></name><name><surname>Kaplan</surname><given-names>KJ</given-names></name><name><surname>Pfeifer</surname><given-names>JD</given-names></name><name><surname>Wilbur</surname><given-names>DC</given-names></name><name><surname>Collins</surname><given-names>LC</given-names></name><name><surname>Colgan</surname><given-names>TJ</given-names></name></person-group><article-title>Review of the current state of whole slide imaging in pathology</article-title><source>J. Pathol. Inform.</source><year>2011</year><volume>2</volume><fpage>36</fpage><pub-id pub-id-type="doi">10.4103/2153-3539.83746</pub-id><pub-id pub-id-type="pmid">21886892</pub-id>
</element-citation><mixed-citation id="mc-CR55" publication-type="journal">Pantanowitz, L. et al. Review of the current state of whole slide imaging in pathology. <italic>J. Pathol. Inform.</italic><bold>2</bold>, 36 (2011).<pub-id pub-id-type="pmid">21886892</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">UTSouthwestern Medical Center. Pulmonary Nodules and Lung Lesions. <ext-link ext-link-type="uri" xlink:href="https://utswmed.org/conditions-treatments/pulmonary-nodules-and-lung-lesions/">https://utswmed.org/conditions-treatments/pulmonary-nodules-and-lung-lesions/</ext-link>. Accessed 04 Sep 2024.</mixed-citation></ref><ref id="CR57"><label>57.</label><citation-alternatives><element-citation id="ec-CR57" publication-type="journal"><person-group person-group-type="author"><name><surname>Oestmann</surname><given-names>JW</given-names></name><name><surname>Greene</surname><given-names>R</given-names></name><name><surname>Kushner</surname><given-names>DC</given-names></name><name><surname>Bourgouin</surname><given-names>PM</given-names></name><name><surname>Linetsky</surname><given-names>L</given-names></name><name><surname>Llewellyn</surname><given-names>HJ</given-names></name></person-group><article-title>Lung lesions: Correlation between viewing time and detection</article-title><source>Radiology</source><year>1988</year><volume>166</volume><fpage>451</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1148/radiology.166.2.3336720</pub-id><pub-id pub-id-type="pmid">3336720</pub-id>
</element-citation><mixed-citation id="mc-CR57" publication-type="journal">Oestmann, J. W. et al. Lung lesions: Correlation between viewing time and detection. <italic>Radiology</italic><bold>166</bold>, 451&#x02013;453 (1988).<pub-id pub-id-type="pmid">3336720</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Irvin, J. et al. CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In <italic>AAAI Conference on Artificial Intelligence</italic>. Vol. 33 (2019).</mixed-citation></ref><ref id="CR59"><label>59.</label><citation-alternatives><element-citation id="ec-CR59" publication-type="journal"><person-group person-group-type="author"><name><surname>Hollon</surname><given-names>T</given-names></name><etal/></person-group><article-title>Artificial-intelligence-based molecular classification of diffuse gliomas using rapid, label-free optical imaging</article-title><source>Nat. Med.</source><year>2023</year><volume>29</volume><fpage>828</fpage><lpage>832</lpage><pub-id pub-id-type="doi">10.1038/s41591-023-02252-4</pub-id><pub-id pub-id-type="pmid">36959422</pub-id>
</element-citation><mixed-citation id="mc-CR59" publication-type="journal">Hollon, T. et al. Artificial-intelligence-based molecular classification of diffuse gliomas using rapid, label-free optical imaging. <italic>Nat. Med.</italic><bold>29</bold>, 828&#x02013;832 (2023).<pub-id pub-id-type="pmid">36959422</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR60"><label>60.</label><citation-alternatives><element-citation id="ec-CR60" publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><etal/></person-group><article-title>Array programming with NumPy</article-title><source>Nature</source><year>2020</year><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id>
</element-citation><mixed-citation id="mc-CR60" publication-type="journal">Harris, C. R. et al. Array programming with NumPy. <italic>Nature</italic><bold>585</bold>, 357&#x02013;362 (2020).<pub-id pub-id-type="pmid">32939066</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="other">McKinney, W. Data structures for statistical computing in Python. In <italic>Proceedings of the 9th Python in Science Conference</italic>. 56 &#x02013; 61 (2010).</mixed-citation></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="other">Virtanen, P. et al. SciPy 1.0: Fundamental algorithms for scientific computing in Python. <italic>Nat. Methods</italic><bold>17</bold>, 261&#x02013;272 (2020).</mixed-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Seabold, S. &#x00026; Perktold, J. statsmodels: Econometric and statistical modeling with python. In <italic>9th Python in Science Conference</italic> (2010).</mixed-citation></ref><ref id="CR64"><label>64.</label><citation-alternatives><element-citation id="ec-CR64" publication-type="journal"><person-group person-group-type="author"><name><surname>Waskom</surname><given-names>ML</given-names></name></person-group><article-title>Seaborn: Statistical data visualization</article-title><source>J. Open Source Softw.</source><year>2021</year><volume>6</volume><fpage>3021</fpage><pub-id pub-id-type="doi">10.21105/joss.03021</pub-id></element-citation><mixed-citation id="mc-CR64" publication-type="journal">Waskom, M. L. Seaborn: Statistical data visualization. <italic>J. Open Source Softw.</italic><bold>6</bold>, 3021 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR65"><label>65.</label><citation-alternatives><element-citation id="ec-CR65" publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group><article-title>Matplotlib: A 2D graphics environment</article-title><source>Comput. Sci. Eng.</source><year>2007</year><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></element-citation><mixed-citation id="mc-CR65" publication-type="journal">Hunter, J. D. Matplotlib: A 2D graphics environment. <italic>Comput. Sci. Eng.</italic><bold>9</bold>, 90&#x02013;95 (2007).</mixed-citation></citation-alternatives></ref><ref id="CR66"><label>66.</label><citation-alternatives><element-citation id="ec-CR66" publication-type="journal"><person-group person-group-type="author"><name><surname>Ibrahim</surname><given-names>R</given-names></name><name><surname>Shafiq</surname><given-names>MO</given-names></name></person-group><article-title>Explainable convolutional neural networks: A taxonomy, review, and future directions</article-title><source>ACM Comput. Surv.</source><year>2023</year><volume>55</volume><fpage>1</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1145/3563691</pub-id></element-citation><mixed-citation id="mc-CR66" publication-type="journal">Ibrahim, R. &#x00026; Shafiq, M. O. Explainable convolutional neural networks: A taxonomy, review, and future directions. <italic>ACM Comput. Surv.</italic><bold>55</bold>, 1&#x02013;37 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="other">Amershi, S., Weld, D., Vorvoreanu, M., Fourney, A., Nushi, B., Collisson, P., Suh, J., Iqbal, S., Bennett, P.&#x000a0;N., Inkpen, K., Teevan, J., Kikin-Gil, R. &#x00026; Horvitz, E. Guidelines for human&#x02013;AI interaction. In <italic>CHI Conference on Human Factors in Computing Systems</italic> (2019).</mixed-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="other">De-Arteaga, M., Fogliato, R. &#x00026; Chouldechova, A. A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic scores. In <italic>CHI Conference on Human Factors in Computing Systems</italic> (2020).</mixed-citation></ref><ref id="CR69"><label>69.</label><citation-alternatives><element-citation id="ec-CR69" publication-type="journal"><person-group person-group-type="author"><name><surname>F&#x000fc;gener</surname><given-names>A</given-names></name><name><surname>Grahl</surname><given-names>J</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Ketter</surname><given-names>W</given-names></name></person-group><article-title>Will humans-in-the-loop become borgs? Merits and pitfalls of working with AI</article-title><source>MIS Q.</source><year>2021</year><volume>45</volume><fpage>1527</fpage><lpage>1556</lpage><pub-id pub-id-type="doi">10.25300/MISQ/2021/16553</pub-id></element-citation><mixed-citation id="mc-CR69" publication-type="journal">F&#x000fc;gener, A., Grahl, J., Gupta, A. &#x00026; Ketter, W. Will humans-in-the-loop become borgs? Merits and pitfalls of working with AI. <italic>MIS Q.</italic><bold>45</bold>, 1527&#x02013;1556 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR70"><label>70.</label><citation-alternatives><element-citation id="ec-CR70" publication-type="journal"><person-group person-group-type="author"><name><surname>Cadario</surname><given-names>R</given-names></name><name><surname>Longoni</surname><given-names>C</given-names></name><name><surname>Morewedge</surname><given-names>CK</given-names></name></person-group><article-title>Understanding, explaining, and utilizing medical artificial intelligence</article-title><source>Nat. Hum. Behav.</source><year>2021</year><volume>5</volume><fpage>1636</fpage><lpage>1642</lpage><pub-id pub-id-type="doi">10.1038/s41562-021-01146-0</pub-id><pub-id pub-id-type="pmid">34183800</pub-id>
</element-citation><mixed-citation id="mc-CR70" publication-type="journal">Cadario, R., Longoni, C. &#x00026; Morewedge, C. K. Understanding, explaining, and utilizing medical artificial intelligence. <italic>Nat. Hum. Behav.</italic><bold>5</bold>, 1636&#x02013;1642 (2021).<pub-id pub-id-type="pmid">34183800</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR71"><label>71.</label><citation-alternatives><element-citation id="ec-CR71" publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>DJ</given-names></name><name><surname>Hu</surname><given-names>H</given-names></name><name><surname>van Mieghem</surname><given-names>JA</given-names></name></person-group><article-title>Predicting human discretion to adjust algorithmic prescription: A large-scale field experiment in warehouse operations</article-title><source>Manag. Sci.</source><year>2022</year><volume>68</volume><fpage>846</fpage><lpage>865</lpage><pub-id pub-id-type="doi">10.1287/mnsc.2021.3990</pub-id></element-citation><mixed-citation id="mc-CR71" publication-type="journal">Sun, J., Zhang, D. J., Hu, H. &#x00026; van Mieghem, J. A. Predicting human discretion to adjust algorithmic prescription: A large-scale field experiment in warehouse operations. <italic>Manag. Sci.</italic><bold>68</bold>, 846&#x02013;865 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR72"><label>72.</label><citation-alternatives><element-citation id="ec-CR72" publication-type="journal"><person-group person-group-type="author"><name><surname>Kawaguchi</surname><given-names>K</given-names></name></person-group><article-title>When will workers follow an algorithm? A field experiment with a retail business</article-title><source>Manag. Sci.</source><year>2021</year><volume>67</volume><fpage>1670</fpage><lpage>1695</lpage><pub-id pub-id-type="doi">10.1287/mnsc.2020.3599</pub-id></element-citation><mixed-citation id="mc-CR72" publication-type="journal">Kawaguchi, K. When will workers follow an algorithm? A field experiment with a retail business. <italic>Manag. Sci.</italic><bold>67</bold>, 1670&#x02013;1695 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR73"><label>73.</label><citation-alternatives><element-citation id="ec-CR73" publication-type="journal"><person-group person-group-type="author"><name><surname>Castelo</surname><given-names>N</given-names></name><name><surname>Bos</surname><given-names>MW</given-names></name><name><surname>Lehmann</surname><given-names>DR</given-names></name></person-group><article-title>Task-dependent algorithm aversion</article-title><source>J. Market. Res.</source><year>2019</year><volume>56</volume><fpage>809</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1177/0022243719851788</pub-id></element-citation><mixed-citation id="mc-CR73" publication-type="journal">Castelo, N., Bos, M. W. &#x00026; Lehmann, D. R. Task-dependent algorithm aversion. <italic>J. Market. Res.</italic><bold>56</bold>, 809&#x02013;825 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR74"><label>74.</label><citation-alternatives><element-citation id="ec-CR74" publication-type="journal"><person-group person-group-type="author"><name><surname>Yeomans</surname><given-names>M</given-names></name><name><surname>Shah</surname><given-names>A</given-names></name><name><surname>Mullainathan</surname><given-names>S</given-names></name><name><surname>Kleinberg</surname><given-names>J</given-names></name></person-group><article-title>Making sense of recommendations</article-title><source>J. Behav. Decis. Mak.</source><year>2019</year><volume>32</volume><fpage>403</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1002/bdm.2118</pub-id></element-citation><mixed-citation id="mc-CR74" publication-type="journal">Yeomans, M., Shah, A., Mullainathan, S. &#x00026; Kleinberg, J. Making sense of recommendations. <italic>J. Behav. Decis. Mak.</italic><bold>32</bold>, 403&#x02013;414 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR75"><label>75.</label><citation-alternatives><element-citation id="ec-CR75" publication-type="journal"><person-group person-group-type="author"><name><surname>Senoner</surname><given-names>J</given-names></name><name><surname>Netland</surname><given-names>T</given-names></name><name><surname>Feuerriegel</surname><given-names>S</given-names></name></person-group><article-title>Using explainable artificial intelligence to improve process quality: Evidence from semiconductor manufacturing</article-title><source>Manag. Sci.</source><year>2022</year><volume>68</volume><fpage>5704</fpage><lpage>5723</lpage><pub-id pub-id-type="doi">10.1287/mnsc.2021.4190</pub-id></element-citation><mixed-citation id="mc-CR75" publication-type="journal">Senoner, J., Netland, T. &#x00026; Feuerriegel, S. Using explainable artificial intelligence to improve process quality: Evidence from semiconductor manufacturing. <italic>Manag. Sci.</italic><bold>68</bold>, 5704&#x02013;5723 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR76"><label>76.</label><citation-alternatives><element-citation id="ec-CR76" publication-type="journal"><person-group person-group-type="author"><name><surname>Bu&#x000e7;inca</surname><given-names>Z</given-names></name><name><surname>Malaya</surname><given-names>MB</given-names></name><name><surname>Gajos</surname><given-names>KZ</given-names></name></person-group><article-title>To trust or to think: Cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making</article-title><source>ACM on Hum.-Comput. Interact.</source><year>2021</year><volume>5</volume><fpage>188</fpage></element-citation><mixed-citation id="mc-CR76" publication-type="journal">Bu&#x000e7;inca, Z., Malaya, M. B. &#x00026; Gajos, K. Z. To trust or to think: Cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making. <italic>ACM on Hum.-Comput. Interact.</italic><bold>5</bold>, 188 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR77"><label>77.</label><mixed-citation publication-type="other">Algorithmic Accountability Act. H.R.2231&#x02014;Algorithmic Accountability Act of 2019 (116th Congress). <ext-link ext-link-type="uri" xlink:href="https://www.congress.gov/bill/116th-congress/house-bill/2231">https://www.congress.gov/bill/116th-congress/house-bill/2231</ext-link> (2019).</mixed-citation></ref><ref id="CR78"><label>78.</label><mixed-citation publication-type="other">European Commission. Ethics Guidelines for Trustworthy AI. <ext-link ext-link-type="uri" xlink:href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</ext-link> (2019).</mixed-citation></ref><ref id="CR79"><label>79.</label><mixed-citation publication-type="other">Wachter, S., Mittelstadt, B. &#x00026; Floridi, L. Transparent, explainable, and accountable AI for robotics. <italic>Sci. Robot.</italic><bold>2</bold>, eaan6080 (2017).</mixed-citation></ref><ref id="CR80"><label>80.</label><citation-alternatives><element-citation id="ec-CR80" publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>H</given-names></name><name><surname>Mayrhofer</surname><given-names>MT</given-names></name><name><surname>van Veen</surname><given-names>E-B</given-names></name><name><surname>Holzinger</surname><given-names>A</given-names></name></person-group><article-title>The ten commandments of ethical medical AI</article-title><source>Computer</source><year>2021</year><volume>54</volume><fpage>119</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1109/MC.2021.3074263</pub-id></element-citation><mixed-citation id="mc-CR80" publication-type="journal">Muller, H., Mayrhofer, M. T., van Veen, E.-B. &#x00026; Holzinger, A. The ten commandments of ethical medical AI. <italic>Computer</italic><bold>54</bold>, 119&#x02013;123 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR81"><label>81.</label><citation-alternatives><element-citation id="ec-CR81" publication-type="journal"><person-group person-group-type="author"><name><surname>Jobin</surname><given-names>A</given-names></name><name><surname>Ienca</surname><given-names>M</given-names></name><name><surname>Vayena</surname><given-names>E</given-names></name></person-group><article-title>The global landscape of AI ethics guidelines</article-title><source>Nat. Mach. Intell.</source><year>2019</year><volume>1</volume><fpage>389</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1038/s42256-019-0088-2</pub-id></element-citation><mixed-citation id="mc-CR81" publication-type="journal">Jobin, A., Ienca, M. &#x00026; Vayena, E. The global landscape of AI ethics guidelines. <italic>Nat. Mach. Intell.</italic><bold>1</bold>, 389&#x02013;399 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR82"><label>82.</label><mixed-citation publication-type="other">Slack, D., Hilgard, S., Jia, E., Singh, S. &#x00026; Lakkaraju, H. Fooling LIME and SHAP: Adversarial attacks on post hoc explanation methods. In <italic>AAAI/ACM Conference on AI, Ethics, and Society</italic> (2020).</mixed-citation></ref><ref id="CR83"><label>83.</label><mixed-citation publication-type="other">Verma, S., Dickerson, J. P. &#x00026; Hines, K. E. Counterfactual explanations for machine learning: A review. Preprint <italic>arXiv</italic>10.48550/arXiv.2010.10596 (2020).</mixed-citation></ref></ref-list></back></article>