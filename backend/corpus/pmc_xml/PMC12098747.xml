<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Nat Commun</journal-id><journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id><journal-title-group><journal-title>Nature Communications</journal-title></journal-title-group><issn pub-type="epub">2041-1723</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40404616</article-id><article-id pub-id-type="pmc">PMC12098747</article-id>
<article-id pub-id-type="publisher-id">59820</article-id><article-id pub-id-type="doi">10.1038/s41467-025-59820-0</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Revealing 3D microanatomical structures of unlabeled thick cancer tissues using holotomography and virtual H&#x00026;E staining</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Park</surname><given-names>Juyeon</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9114-8438</contrib-id><name><surname>Shin</surname><given-names>Su-Jin</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Kim</surname><given-names>Geon</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Cho</surname><given-names>Hyungjoo</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3600-281X</contrib-id><name><surname>Ryu</surname><given-names>Dongmin</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Ahn</surname><given-names>Daewoong</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Heo</surname><given-names>Ji Eun</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6222-6457</contrib-id><name><surname>Clemenceau</surname><given-names>Jean R.</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Barnfather</surname><given-names>Isabel</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0009-0009-6787-0354</contrib-id><name><surname>Kim</surname><given-names>Minji</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Jang</surname><given-names>Inyeop</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6607-2117</contrib-id><name><surname>Sung</surname><given-names>Ji-Youn</given-names></name><xref ref-type="aff" rid="Aff6">6</xref><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4522-9928</contrib-id><name><surname>Park</surname><given-names>Jeong Hwan</given-names></name><xref ref-type="aff" rid="Aff6">6</xref><xref ref-type="aff" rid="Aff8">8</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7435-7884</contrib-id><name><surname>Min</surname><given-names>Hyun-seok</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Lee</surname><given-names>Kwang Suk</given-names></name><xref ref-type="aff" rid="Aff9">9</xref></contrib><contrib contrib-type="author"><name><surname>Cho</surname><given-names>Nam Hoon</given-names></name><xref ref-type="aff" rid="Aff10">10</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6668-7269</contrib-id><name><surname>Hwang</surname><given-names>Tae Hyun</given-names></name><address><email>taehyun.hwang@vumc.org</email></address><xref ref-type="aff" rid="Aff6">6</xref><xref ref-type="aff" rid="Aff11">11</xref><xref ref-type="aff" rid="Aff12">12</xref><xref ref-type="aff" rid="Aff13">13</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0528-6661</contrib-id><name><surname>Park</surname><given-names>YongKeun</given-names></name><address><email>yk.park@kaist.ac.kr</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05apxxy63</institution-id><institution-id institution-id-type="GRID">grid.37172.30</institution-id><institution-id institution-id-type="ISNI">0000 0001 2292 0500</institution-id><institution>Department of Physics, </institution><institution>Korea Advanced Institute of Science and Technology (KAIST), </institution></institution-wrap>Daejeon, Republic of Korea </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05apxxy63</institution-id><institution-id institution-id-type="GRID">grid.37172.30</institution-id><institution-id institution-id-type="ISNI">0000 0001 2292 0500</institution-id><institution>KAIST Institute for Health Science and Technology, </institution><institution>KAIST, </institution></institution-wrap>Daejeon, Republic of Korea </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01wjejq96</institution-id><institution-id institution-id-type="GRID">grid.15444.30</institution-id><institution-id institution-id-type="ISNI">0000 0004 0470 5454</institution-id><institution>Department of Pathology, Gangnam Severance Hospital, </institution><institution>Yonsei University College of Medicine, </institution></institution-wrap>Seoul, Republic of Korea </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.518951.1</institution-id><institution>Tomocube Inc., </institution></institution-wrap>Daejeon, Republic of Korea </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01wjejq96</institution-id><institution-id institution-id-type="GRID">grid.15444.30</institution-id><institution-id institution-id-type="ISNI">0000 0004 0470 5454</institution-id><institution>Department of Urology, </institution><institution>Yonsei University College of Medicine, </institution></institution-wrap>Seoul, Republic of Korea </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02qp3tb03</institution-id><institution-id institution-id-type="GRID">grid.66875.3a</institution-id><institution-id institution-id-type="ISNI">0000 0004 0459 167X</institution-id><institution>Department of Artificial Intelligence and Informatics, </institution><institution>Mayo Clinic, </institution></institution-wrap>Jacksonville, FL USA </aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01zqcg218</institution-id><institution-id institution-id-type="GRID">grid.289247.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 2171 7818</institution-id><institution>Department of Pathology, Kyung Hee University Hospital, </institution><institution>Kyung Hee University College of Medicine, </institution></institution-wrap>Seoul, Republic of Korea </aff><aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/014xqzt56</institution-id><institution-id institution-id-type="GRID">grid.412479.d</institution-id><institution>Department of Pathology, </institution><institution>Seoul National University Boramae Medical Center, </institution></institution-wrap>Seoul, Republic of Korea </aff><aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01wjejq96</institution-id><institution-id institution-id-type="GRID">grid.15444.30</institution-id><institution-id institution-id-type="ISNI">0000 0004 0470 5454</institution-id><institution>Department of Urology, Gangnam Severance Hospital, </institution><institution>Yonsei University College of Medicine, </institution></institution-wrap>Seoul, Republic of Korea </aff><aff id="Aff10"><label>10</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01wjejq96</institution-id><institution-id institution-id-type="GRID">grid.15444.30</institution-id><institution-id institution-id-type="ISNI">0000 0004 0470 5454</institution-id><institution>Department of Pathology, </institution><institution>Yonsei University College of Medicine, </institution></institution-wrap>Seoul, Republic of Korea </aff><aff id="Aff11"><label>11</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02qp3tb03</institution-id><institution-id institution-id-type="GRID">grid.66875.3a</institution-id><institution-id institution-id-type="ISNI">0000 0004 0459 167X</institution-id><institution>Florida Department of Health Cancer Chair, </institution><institution>Mayo Clinic, </institution></institution-wrap>Jacksonville, FL USA </aff><aff id="Aff12"><label>12</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02qp3tb03</institution-id><institution-id institution-id-type="GRID">grid.66875.3a</institution-id><institution-id institution-id-type="ISNI">0000 0004 0459 167X</institution-id><institution>Department of Immunology | Department of Cancer Biology, </institution><institution>Mayo Clinic, </institution></institution-wrap>Jacksonville, FL USA </aff><aff id="Aff13"><label>13</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05dq2gs74</institution-id><institution-id institution-id-type="GRID">grid.412807.8</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9916</institution-id><institution>Present Address: Section of Surgical Sciences, </institution><institution>Vanderbilt University Medical Center, </institution></institution-wrap>Nashville, TN USA </aff></contrib-group><pub-date pub-type="epub"><day>22</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>22</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>16</volume><elocation-id>4781</elocation-id><history><date date-type="received"><day>8</day><month>8</month><year>2024</year></date><date date-type="accepted"><day>6</day><month>5</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">In histopathology, acquiring subcellular-level three-dimensional (3D) tissue structures efficiently and without damaging the tissues during serial sectioning and staining remains a formidable challenge. We address this by integrating holotomography with deep learning and creating 3D virtual hematoxylin and eosin (H&#x00026;E) images from label-free thick cancer tissues. This method involves measuring the tissues&#x02019; 3D refractive index (RI) distribution using holotomography, followed by processing with a deep learning-based image translation framework to produce virtual H&#x00026;E staining in 3D. Applied to colon cancer tissues up to 50&#x02009;&#x000b5;m thick&#x02014;far surpassing conventional slide thickness&#x02014;this technique provides direct methodological validation through chemical H&#x00026;E staining. It reveals quantitative 3D microanatomical structures of colon cancer with subcellular resolution. Further validation of our method&#x02019;s repeatability and scalability is demonstrated on gastric cancer samples across different institutional settings. This innovative 3D virtual H&#x00026;E staining method enhances histopathological efficiency and reliability, marking a significant advancement in extending histopathology to the 3D realm and offering substantial potential for cancer research and diagnostics.</p></abstract><abstract id="Abs2" abstract-type="web-summary"><p id="Par2">Acquiring subcellular-level three-dimensional (3D) tissue structures efficiently without damaging the tissue remains challenging in histopathology. Here, the authors integrate holotomography with deep learning to generate 3D virtual H&#x00026;E staining images from label-free thick cancer tissues, and apply this approach to colon and gastric cancer samples.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Cancer imaging</kwd><kwd>Machine learning</kwd><kwd>Colon cancer</kwd><kwd>Gastric cancer</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100003661</institution-id><institution>Korea Institute for Advancement of Technology (KIAT)</institution></institution-wrap></funding-source><award-id>P0028463</award-id><principal-award-recipient><name><surname>Park</surname><given-names>YongKeun</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>This work was supported by the National Research Foundation of Korea (RS-2024-00442348, 2022M3H4A1A02074314, RS-2023-00241278, RS-2024-00351903), an Institute of Information &#x00026; Communications Technology Planning &#x00026; Evaluation (IITP; 2021-0-00745) grant funded by the Korea government (MSIT), Korea Institute for Advancement of Technology (KIAT) through the International Cooperative R&#x00026;D program (P0028463), and the Korean Fund for Regenerative Medicine (KFRM) grant funded by the Korea government (the Ministry of Science and ICT and the Ministry of Health &#x00026; Welfare) (21A0101L1-12), and the Korea Health Technology R&#x00026;D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health &#x00026; Welfare, Korea (HI21C0977, HR22C1605).</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par3">Histopathology, the meticulous microscopic examination of tissue samples, is fundamental in identifying structural and cellular alterations within diseased tissues. For centuries, the application of hematoxylin and eosin (H&#x00026;E) staining has been the cornerstone of this process, enhancing the visibility of cellular and tissue architectures under bright-field microscopy. Such procedures have been instrumental in diagnosing, grading, and classifying diseases by providing valuable insights into microscopic structural alterations. Nonetheless, this conventional method is primarily confined to two-dimensional (2D) analysis, which significantly limits its capacity to reveal the intricate three-dimensional (3D) architecture inherent in biological tissues<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. The constraints posed by bright-field microscopy and H&#x00026;E staining become particularly evident when attempting to analyze thick 3D tissue samples, thereby restricting our understanding of 3D histopathological structures. Recent advancements underscore the indispensable role of 3D analysis in histopathology, offering insights into critical features like vascular and neural structures<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, tissue heterogeneity<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>, and nuanced aspects of cancer grading<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. Current methods for achieving 3D histopathological analysis, however, entail labor-intensive sample preparation processes, including extensive sectioning, staining, or optical clearing. Such procedures are not only demanding in terms of time and resources but also prone to introducing artifacts during the serial sectioning and staining steps&#x02014;artifacts that are irreversible once they occur.</p><p id="Par4">In response to the considerable demands of 3D histopathology, significant strides have been made towards eliminating traditional staining processes. Notably, quantitative phase imaging (QPI) has emerged as a pioneering label-free technique that quantitatively captures the optical phase delay in specimens, offering a non-invasive alternative for examining histopathological samples<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. With the advantages of label-free imaging capabilities and quantitative measurements, QPI has become an invaluable tool for investigating histopathological specimens, including structures of diverse cancers<sup><xref ref-type="bibr" rid="CR9">9</xref>&#x02013;<xref ref-type="bibr" rid="CR18">18</xref></sup>, Alzheimer&#x02019;s disease<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, and Crohn&#x02019;s disease<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. While the studies predominantly focused on 2D specimens, recent advancements in 3D QPI techniques are now being applied to various histopathological specimens<sup><xref ref-type="bibr" rid="CR21">21</xref>&#x02013;<xref ref-type="bibr" rid="CR24">24</xref></sup>.</p><p id="Par5">Recently, deep learning has been integrated with QPI, enhancing its ability to discern subcellular structures through the innovative concept of virtual staining<sup><xref ref-type="bibr" rid="CR25">25</xref>&#x02013;<xref ref-type="bibr" rid="CR27">27</xref></sup>. Virtual staining involves an image-to-image translation framework, using deep learning to convert label-free images to the desired stained images. This approach has been applied to diverse histopathological slides and target stains. For instance, unsupervised deep learning has successfully generated immunohistochemistry-stained images from phase images of kidney tissue slides<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Additionally, supervised deep learning enabled virtual staining of nuclei and F-actin using the label-free phase images of kidney tissue slides<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Furthermore, Phasestain has generated H&#x00026;E, John&#x02019;s, and Masson&#x02019;s trichrome stained images from label-free phase images of skin, kidney, and liver tissue, respectively<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. These studies have demonstrated the potential of virtual staining to make standard histopathology more efficient by eliminating the staining procedures and replacing them with computational staining. However, it is important to note that these research efforts have primarily focused on 2D applications. A recent study has demonstrated that the integration of the 3D QPI technique with virtual staining has enabled 3D virtual H&#x00026;E staining of tissue<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, revealing the 3D structure of tumor margins through virtual H&#x00026;E images. However, due to the absence of ground truth images for comparison, the validation of the proposed framework heavily relies on indirect approaches.</p><p id="Par6">In this study, we confront two predominant challenges in histopathology: the limitation to two-dimensional analysis and the intensive resources required by traditional H&#x00026;E staining procedures. Our innovative strategy leverages holotomography combined with deep learning to bypass conventional staining, facilitating the generation of three-dimensional H&#x00026;E-stained images without direct staining (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>a, <xref rid="Fig1" ref-type="fig">b</xref>). Holotomography, a 3D QPI technique, reconstructs the label-free 3D refractive index (RI) distribution of the sample with high spatial resolution and optical sectioning capabilities. To realize the holotomography, we utilize the deconvolution phase microscopy which involves the deconvolution of intensity images using optical transfer functions to capture the 3D RI distribution of specimen without external staining<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1c</xref>). A deep learning technique for image-to-image translation is integrated with the holotomography to generate virtual H&#x00026;E stained images from label-free RI images in 3D (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1d</xref>). The framework includes training and testing phase. During the training phase, the neural network learns to map between the RI images and H&#x00026;E stained images obtained from a conventional 4 &#x003bc;m-thick H&#x00026;E-stained tissue slide. Once the training is completed, the optimized neural network is directly applied to the 3D RI images acquired from thick, label-free cancer tissue slides, resulting in the 3D virtual H&#x00026;E stained images. Specifically, we employ a supervised learning approach, utilizing direct ground truth images to validate the accuracy of virtual H&#x00026;E images, ensuring the reliability and credibility of our framework (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1e</xref>).<fig id="Fig1"><label>Fig. 1</label><caption><title>Overview of the proposed framework.</title><p><bold>a</bold> Procedures for conventional 3D histopathology including multiple sectioning, H&#x00026;E staining, and imaging under BF microscopy. <bold>b</bold> The proposed framework of 3D virtual H&#x00026;E staining. Integration of holotomography and deep learning enabled label-free 3D virtual H&#x00026;E staining of thick cancer tissue slides. <bold>c</bold> Optical setup for holotomography and reconstruction procedures. Raw intensity images are deconvolved with the optical transfer functions, reconstructing 3D RI images. <bold>d</bold> Training and testing phases for 3D virtual staining. The neural network is trained to map between RI and BF images using the conventional 4 &#x003bc;m-thick H&#x00026;E-stained tissue slides during the training phase. The trained neural network is directly applied to the 3D RI images obtained from label-free, thick tissue slides. <bold>e</bold> A supervised learning workflow employed during the training phase. Figures&#x000a0;1a, b, d were created with BioRender.com.</p></caption><graphic xlink:href="41467_2025_59820_Fig1_HTML" id="d33e529"/></fig></p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Network training</title><p id="Par7">To train the network, we prepared the paired training dataset from a 4 &#x003bc;m-thick, H&#x00026;E-stained colon cancer tissue slide (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2a</xref>). Initially, we utilized holotomography to capture both the 3D RI images and 2D single-channel BF (scBF) images. To integrate cellular details from various axial positions into a single focal plane, we applied an all-in-focus technique to the 3D RI images<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> (See &#x0201c;Methods&#x0201d;). Subsequently, we imaged the same slide using a whole slide scanner (WSS) to acquire H&#x00026;E-stained BF images. Then, image registration was conducted to align the BF images obtained from WSS and scBF images acquired from holotomography. This alignment resulted in a paired dataset of BF and RI images since scBF and RI images are inherently aligned due to being captured at identical positions. The registration procedures were based on the spatial transform network<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> (See &#x0201c;Methods&#x0201d;). Further refinement involved cropping these images into patches of 1024&#x02009;&#x000d7;&#x02009;1024 pixels and applying the same registration techniques to each patch to enhance accuracy. Patches with a Pearson&#x02019;s correlation coefficient below 0.65 between the scBF and BF images were excluded to maintain data quality. The final curated dataset comprised 2538 paired patches, which were then divided into two subsets: 1996 patches for training and 542 patches for validation<fig id="Fig2"><label>Fig. 2</label><caption><title>Validations of the trained network with a 4 m-thick, H&#x00026;E-stained colon cancer tissue slide.</title><p><bold>a</bold> Dataset preparation steps before network training. <bold>b</bold> A wide-field RI image obtained from a 4 &#x003bc;m-thick, H&#x00026;E-stained colon cancer slide and its detailed images. Results are from a single experiment. <bold>c</bold> A wide-field virtual H&#x00026;E image generated by the trained neural network and its detailed images. Results are from a single experiment. <bold>d</bold> A ground truth chemical H&#x00026;E image obtained using a WSS and its detailed images. Results are from a single experiment. <bold>e</bold> Comparison of virtual and chemical H&#x00026;E images across various pathological features. Results are from a single experiment. <bold>f</bold> Distribution of SSIM values computed from 100 cropped patches. In the box plot, the white circle at the center of the box represents the median, the ends of the box represent the first and third quartiles, and the whicker spans a 1.5 interquartile range from the ends. <bold>g</bold> Distribution of Jaccard index values computed from 100 cropped patches of nucleus segmentation results. In each box plot, the white circle at the center of the box represents the median, the ends of the box represent the first and third quartiles, and the whicker spans a 1.5 interquartile range from the ends. <bold>h</bold> An overlapped nuclei segmentation map obtained from virtual (blue) and chemical (purple) H&#x00026;E images. Results are from a single experiment. <bold>i</bold>, <bold>j</bold> Distribution of average nucleus area (<bold>i</bold>) and average number of nuclei (<bold>j</bold>) at each cropped patch obtained from virtual (blue) and chemical (purple) H&#x00026;E images. The distributions were derived from a total of 100 patches. Paired, two-sided Student&#x02019;s t-test was used to calculate <italic>P</italic> values. In each box plot, the white line represents the median, the ends of the box represent the first and third quartiles, and the whicker spans a 1.5 interquartile range from the ends. Figure&#x000a0;2a was created with BioRender.com.</p></caption><graphic xlink:href="41467_2025_59820_Fig2_HTML" id="d33e595"/></fig></p><p id="Par8">For the architecture of the neural network, we employed a conditional generative adversarial network to train the model (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1e</xref>). The network consists of a generator and discriminator, which are adversarially learning to map RI images to BF images. Scalable neural architecture search (SCNAS) has been used as the generator, an optimized architecture for 3D medical image datasets<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">2a</xref>). The discriminator consists of five convolutional layers (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">2b</xref>). While the generator creates the virtual H&#x00026;E stained images from input RI images, the discriminator&#x02019;s role is to distinguish the created image from the ground truth images when provided with input images.</p><p id="Par9">To assess the performance of the trained networks, we compared virtual H&#x00026;E images and corresponding ground truth images. First, we obtained a wide-field, all-in-focused RI image from a different region of the same slide used for training (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b</xref>), which revealed various glandular structures (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b</xref>i&#x02013;iv). Subsequently, the wide-field RI image was cropped into 1024&#x02009;&#x000d7;&#x02009;1024-pixel patches with a 50% overlap and we fed the patches into the trained network. The resultant virtual H&#x00026;E images were stitched back to reconstruct the original wide-field image using the ImageJ plugin<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2c</xref>). For a direct quantitative comparison between the virtual H&#x00026;E images and the ground truth images, we obtained the WSS image of the corresponding region (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2d</xref>). The results demonstrate that the trained network successfully predicted most of the histological and anatomical features. To further validate its performance, we compared various histological features, including subnuclear structures and signet ring cells, between the virtual and chemical H&#x00026;E images (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2e</xref>). The virtual H&#x00026;E images accurately captured key features, such as subnuclear details, the skewed nuclear position characteristic of signet ring cells, and mucinous regions, which were clearly distinguishable from the vacant background or surrounding cytoplasm. Moreover, circular structures of normal mucosa, with nuclei aligned along the perimeter, and the pinkish aggregated structures of necrosis were successfully reproduced by our trained network.</p><p id="Par10">In our quantitative analysis, the structural similarity index measure (SSIM) was employed to evaluate the fidelity of virtual H&#x00026;E images against their chemically stained counterparts across selected regions of interest. To account for the resolution discrepancies between holotomography-derived images and those obtained from the WSS, we first normalized the spatial frequency ranges of both datasets, ensuring a fair comparison of intensity levels before proceeding with SSIM calculations. This adjustment allowed for a meaningful comparison that accurately reflects the structural preservation in the virtual staining process. We then extended this evaluation to the entire field by calculating SSIM values for individual 1024&#x02009;&#x000d7;&#x02009;1024-pixel patches across the slides. The SSIM values ranged from 0.75 to 0.83, with an average of 0.78, indicating a consistent level of similarity between the virtual and chemical H&#x00026;E images across the dataset (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2f</xref>).</p><p id="Par11">While SSIM values are a prevalent metric for evaluating the performance of virtual staining techniques, it&#x02019;s critical to recognize their limitations. Sole reliance on SSIM can sometimes lead to counterintuitive findings, as this measure may not fully capture the nuances of staining fidelity or the biological relevance of the imaged structures<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup>. Consequently, we directed our evaluation efforts towards analyzing the virtual staining quality through the lens of H&#x00026;E staining&#x02019;s essential attributes. Given that the primary purpose of H&#x00026;E staining is to facilitate the visualization of nuclei and cytoplasm, we conducted a comparative analysis of nuclei detection in both virtual and chemical H&#x00026;E images (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2g&#x02013;j</xref>). Utilizing HoVer-Net<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, we segmented nuclei from virtual and chemical H&#x00026;E images. Overlaying the segmentation maps revealed a predominance of overlapped pixels (yellow) across the wide-field images (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2h</xref>). To quantitatively assess the degree of overlap, Jaccard indices were computed for each 1024&#x02009;&#x000d7;&#x02009;1024-pixel patch, yielding a mean value of 0.5 (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2g</xref>).</p><p id="Par12">In addition to pixel-level comparative analysis, we compared essential histological parameters such as nucleus area and quantity. We calculated the average number and size of nuclei in each patch from both segmentation maps. Statistical comparison between the results showed no significant difference between the values computed from virtual and chemical H&#x00026;E images (Figs.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>i, <xref rid="Fig2" ref-type="fig">j</xref>). These results suggest that quantitative nuclei identification, a primary objective of H&#x00026;E staining, yields consistent results regardless of whether they are derived from virtual or chemical H&#x00026;E images.</p><p id="Par13">From a model architecture perspective, we evaluated several image translation networks for the virtual staining task, including U-Net<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, conditional diffusion<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, and pix2pix<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref> and Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>). Quantitatively, both U-Net and our algorithm achieved the highest SSIM and PSNR values, with U-Net exhibiting a slight advantage in these metrics. However, qualitative assessments are equally critical in histopathology, where expert interpretations of morphological features are paramount. Accordingly, detailed comparisons of U-Net- and our algorithm-generated images (Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">1c&#x02013;e</xref>) showed that U-Net tends to produce more blurred outputs, while our method more accurately preserves intricate tissue structures. Considering both quantitative metrics and qualitative evaluations, we conclude that our algorithm is better suited for the goals of virtual staining.</p></sec><sec id="Sec4"><title>3D virtual H&#x00026;E staining of thick colon cancer tissue slides</title><p id="Par14">To create 3D virtual H&#x00026;E images of label-free thick cancer tissue, we sourced an additional colon cancer slide from a different patient, capturing its 3D RI images. This slide, prepared at a thickness of 10&#x02009;&#x000b5;m&#x02014;2.5 times thicker than standard histopathology slides&#x02014;was imaged without staining. Employing holotomography, we acquired wide-field 3D RI images covering ~1.2&#x02009;&#x000d7;&#x02009;1.2&#x02009;mm regions of the slide. These images vividly rendered anatomical structures characteristic of colon cancer, such as glands and lumens (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3b</xref>). A deeper exploration of these structures was achieved by zooming into the 3D RI representations, revealing the lumens&#x02019; vacant regions and the intricate glandular formations around them with clear axial detail (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3c</xref>).<fig id="Fig3"><label>Fig. 3</label><caption><title>Virtual 3D H&#x00026;E staining of 10 &#x003bc;m-thick colon cancer tissue slide.</title><p><bold>a</bold> Workflow of generating 3D virtual H&#x00026;E of 10 &#x003bc;m-thick colon cancer tissue slides (<bold>b</bold>). A wide-field RI image obtained from the label-free 10 &#x003bc;m-thick colon cancer tissue slides using holotomography. A single focal plane of the 3D RI image is presented. <bold>c</bold> Detailed 3D RI images of glands and lumens obtained from 10 &#x003bc;m-thick colon cancer tissue slides. <bold>d</bold> A wide-field virtual H&#x00026;E image predicted from (<bold>b</bold>). A single focal plane of the 3D virtual H&#x00026;E image is presented. <bold>e</bold> Detailed images of 3D virtual H&#x00026;E images. Cyan arrows indicate the necrotic structures. Figure&#x000a0;3a was created with BioRender.com.</p></caption><graphic xlink:href="41467_2025_59820_Fig3_HTML" id="d33e727"/></fig></p><p id="Par15">Subsequently, we input each axial slice of the 3D RI images into our trained network (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3a</xref>). We cropped each wide-field RI image from every axial position into 1024&#x02009;&#x000d7;&#x02009;1024-pixel patches with 50% overlap. These patches were then fed into the trained neural network, which generated corresponding H&#x00026;E images for each patch. The predicted virtual H&#x00026;E patches were then stitched back together to reconstruct the original wide-field image (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3d</xref>). By repeating this prediction process for all axial positions, we successfully generated 3D virtual H&#x00026;E images from the label-free, 10 &#x003bc;m-thick colon cancer tissue slide. The predicted H&#x00026;E images effectively represented anatomical structures such as glands and lumens (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3e</xref>). Furthermore, the necrotic regions within glands were also depicted, as indicated by cyan arrows.</p><p id="Par16">To delve further into the three-dimensional intricacies of colon cancer, we prepared an additional tissue slide from a separate patient, this time with a thickness of 20&#x02009;&#x000b5;m&#x02014;fivefold that of traditional slides. Utilizing the identical approach applied to the previously mentioned 10 &#x000b5;m-thick slide, we successfully captured 3D RI images and subsequently generated detailed 3D virtual H&#x00026;E images (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3a</xref>). Similarly, the wide-field RI image effectively depicted the anatomical structures of glands and lumens (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4a</xref>). Detailed examination of selected lumens and glands in 3D clearly identified vacant regions of lumens, along with their surrounding glandular structures (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4b</xref>). Notably, as the sample thickness increased to 20&#x02009;&#x003bc;m, the warping of lumens and glands along the axial axis became more evident, as indicated with yellow arrows. This dynamic presentation provides compelling evidence of the 3D reconstruction of anatomical structures of label-free 20 &#x003bc;m-thick colon cancer tissue slides.<fig id="Fig4"><label>Fig. 4</label><caption><title>Virtual 3D H&#x00026;E staining of 20 &#x003bc;m-thick colon cancer tissue slide.</title><p><bold>a</bold> A wide-field RI image obtained from the label-free 20 &#x003bc;m-thick colon cancer tissue slide using holotomography. A single focal plane of the 3D RI image is presented. <bold>b</bold> Detailed 3D RI images of glands and lumens obtained from the 20 &#x003bc;m-thick colon cancer tissue slide. <bold>c</bold> A wide-field virtual H&#x00026;E image predicted from (<bold>a</bold>). A single focal plane of the 3D virtual H&#x00026;E image is presented. <bold>d</bold> Detailed images of 3D virtual H&#x00026;E images. Cyan arrows indicate lumens.</p></caption><graphic xlink:href="41467_2025_59820_Fig4_HTML" id="d33e772"/></fig></p><p id="Par17">Following the methodology used for 10 &#x003bc;m-thick tissue slide, label-free RI images of 20 &#x003bc;m-thick colon cancer tissue were input to the trained neural network to generate 3D virtual H&#x00026;E images (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4c</xref>). The resulting virtual H&#x00026;E images clearly depicted histological structures, with the warping of lumens along the axial axis as highlighted by yellow arrows (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4d</xref>). The outcomes underscore the capability of holotomography to accurately reconstruct the three-dimensional architecture of colon cancer tissues, handling tissue slides up to five times thicker than the standard. Furthermore, these findings emphasize the proficiency of our trained neural network in producing 3D virtual H&#x00026;E images, showcasing its predictive strength in visualizing complex tissue structures without traditional staining methods.</p></sec><sec id="Sec5"><title>Subcellular examination of 3D virtual H&#x00026;E images</title><p id="Par18">Delving into the intricate three-dimensional outcomes produced by our trained neural network, we scrutinized the histological features in 3D. Cross-sectional analyzes unveiled the lumen and gland structures, with the lumens&#x02019; void spaces consistently formed along the axial axis (Figs.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>a, <xref rid="Fig5" ref-type="fig">b</xref>). These structures demonstrated noticeable structural variations&#x02014;such as widening, shrinking, or warping&#x02014;along the <italic>z-</italic>axis. Moreover, the glandular formations surrounding these spaces were observed to change in width across different cross-sectional views. This variance underscores the 3D reconstruction of histological features within the virtual H&#x00026;E images, affirming the network&#x02019;s capability to accurately replicate complex tissue architectures.<fig id="Fig5"><label>Fig. 5</label><caption><title>3D subcellular examination of colon cancer tissue slide a, b.</title><p>The <italic>x</italic>&#x02013;<italic>y</italic>, <italic>y</italic>&#x02013;<italic>z</italic>, and <italic>x</italic>&#x02013;<italic>z</italic> cross sections of 3D virtual H&#x00026;E images of the 10 (<bold>a</bold>) and 20 &#x003bc;m-thick colon cancer tissue slide (<bold>b</bold>). <bold>c</bold>, <bold>d</bold> Detailed images to visualize individual nuclei in input RI images and corresponding virtual H&#x00026;E images from the 10 (<bold>c</bold>) and 20 &#x003bc;m-thick tissue slide (<bold>d</bold>) and their corresponding chemical H&#x00026;E images. Yellow and cyan arrows indicate the individual nuclei at different axial positions.</p></caption><graphic xlink:href="41467_2025_59820_Fig5_HTML" id="d33e840"/></fig></p><p id="Par19">From the cross-sectional views, we observed distinct regions where the nuclei are either sparsely distributed or densely packed in 3D. For instance, in the uppermost cross-section of the 20 &#x003bc;m-thick sample result (Line 4 in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5b</xref>), nuclei are densely distributed across all axial sections, while in the middle cross-section of the same result (Line 5 in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5b</xref>), nuclei exhibit a relatively sparse distribution across most sections, except for a few initial axial sections.</p><p id="Par20">Zooming in to the level of individual nuclei, we closely examined the 3D virtual H&#x00026;E images alongside their corresponding input RI images. This comparison aimed to assess the fidelity with which the virtual staining process replicated the detailed structures and features of the nuclei within the tissue context (Figs.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>c, <xref rid="Fig5" ref-type="fig">d</xref>). In our direct comparison, we observed circular structures of the nucleus in the input RI images, which is consistent with the predicted H&#x00026;E images of blue circular morphologies. As we move along the axial axis, nuclei dynamically alter size and shape&#x02013;contracting or expanding&#x02013;indicating 3D construction of the nucleus. By comparing the virtual H&#x00026;E images with the corresponding regions in chemical H&#x00026;E images, we observed that while our approach effectively resolved both the shapes and axial positions of individual nuclei, the chemical H&#x00026;E images were unable to achieve this, failing to distinguish single nuclei in the 20 &#x003bc;m thick tissue slide. Additionally, we examined the relative 3D distribution among nuclei by tracking the emergence and disappearance of some nuclei along the <italic>z</italic>-axis. For example, a nucleus indicated by a yellow arrow is located ~5.4&#x02009;&#x003bc;m below a neighboring nucleus pointed by a cyan arrow (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5c</xref>). These results highlight the potential of 3D virtual H&#x00026;E images to provide detailed insights into the 3D morphology and spatial distribution of cellular and subcellular structures, which can be easily overlooked in 2D analysis.</p></sec><sec id="Sec6"><title>Validation of the virtual H&#x00026;E images with the chemically stained images</title><p id="Par21">We further validated the capabilities of our method in the 3D histopathology of thick colon cancer tissue slides. To achieve this, we compared the 3D virtual H&#x00026;E images to the chemically stained images of thick tissue slides (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>). After measuring label-free RI images of the two thick colon cancer slides, we chemically stained the same slide and obtained images using WSS. However, since the WSS images were only accessible in 2D, we needed to devise a strategy to generate representative 2D images from 3D virtual H&#x00026;E images for a fair comparison. For this purpose, we utilized a single focal plane as one representative (Figs.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>a, <xref rid="Fig6" ref-type="fig">d</xref>) and applied minimum intensity projection along the axial axis for another representative 2D image (Figs.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>b, <xref rid="Fig6" ref-type="fig">e</xref>).<fig id="Fig6"><label>Fig. 6</label><caption><title>Validations using the standard histopathology procedures.</title><p><bold>a</bold>, <bold>d</bold> A single focal plane of 3D virtual H&#x00026;E images predicted from label-free 10 (<bold>a</bold>) and 20 (<bold>d</bold>) &#x003bc;m-thick colon cancer tissue slide. <bold>b</bold>, <bold>e</bold>, Minimum intensity projection of 3D virtual H&#x00026;E images predicted from label-free 10 (<bold>b</bold>) and 20 &#x003bc;m-thick colon cancer tissue slide (<bold>e</bold>). <bold>c</bold>, <bold>f</bold> Same 10 (<bold>c</bold>) and 20 &#x003bc;m-thick tissue slides (<bold>f)</bold> were stained with H&#x00026;E and imaged using WSS. Detailed images of selected glandular structures are presented (i, ii). Data shown are from a single experiment.</p></caption><graphic xlink:href="41467_2025_59820_Fig6_HTML" id="d33e927"/></fig></p><p id="Par22">The comparison between virtual and chemical H&#x00026;E images for the 10 &#x003bc;m-thick tissue slide highlighted the reliability and fidelity of the proposed virtual staining framework (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6a&#x02013;c</xref>). The analysis of the wide-field images exhibited the accurate reproduction of anatomical features by the trained network. The anatomical structures including glands and stroma are visible in the virtual H&#x00026;E stained images. To provide a detailed examination, we zoomed the glandular structures, revealing comparable nucleus distributions to the chemically stained slides (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6a&#x02013;c</xref> (i, ii)).</p><p id="Par23">Comparing virtual and chemical H&#x00026;E for the 20 &#x003bc;m-thick tissue slides demonstrated the robustness of the virtual staining framework, even for sample thickness where the chemical staining becomes abnormal (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6d&#x02013;f</xref>). The colorimetric characteristics of virtual H&#x00026;E remain consistent with minimal deviation, while chemical staining resulted in a distinctly different color distribution. Nevertheless, we could compare the anatomical features between virtual and chemical H&#x00026;E images. The structures of glands and stroma were successfully reproduced by the trained network. For a detailed investigation of glandular structures, we zoomed into the selected glands, clearly illustrating the consistency of lumens and surrounding glandular structures (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6d&#x02013;f</xref> (i, ii)). All zoomed-in regions are confirmed by the experienced pathologist (SJS).</p></sec><sec id="Sec7"><title>3D subcellular and microanatomical structures across the whole slide in colon cancer</title><p id="Par24">To further investigate the volumetric structures of thick tissue, our framework was applied to a 50 &#x000b5;m-thick tissue slide. This label-free colon cancer tissue slide, obtained from a different patient, was prepared to a thickness of 50&#x02009;&#x000b5;m (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7a</xref>). Through holotomography, we captured a whole slide 3D RI image, which were subsequently cropped and input into the same trained neural network to produce 3D virtual H&#x00026;E images, with non-sample regions masked out. The results, encompassing a total volume of 15&#x02009;mm&#x02009;&#x000d7;&#x02009;15&#x02009;mm&#x02009;&#x000d7;&#x02009;0.05&#x02009;mm, were compared with a WSS image of the same sample (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7b&#x02013;d</xref>). Notably, for this experiment, we utilized a different holotomography system, situated at another institute compared to the equipment used in previous sections, to ensure the repeatability and robustness of our RI measurements.<fig id="Fig7"><label>Fig. 7</label><caption><title>3D microanatomical rendering and quantitative analysis of a whole 50 &#x003bc;m-thick colon cancer tissue slide.</title><p><bold>a</bold> A label-free colon cancer slide of 50&#x02009;&#x003bc;m thickness used for imaging. <bold>b</bold> A maximum projected whole slide 3D RI image of (<bold>a</bold>). <bold>c</bold> A minimum projected whole slide 3D virtual H&#x00026;E image of (<bold>a</bold>). <bold>d</bold> A WSS image of (<bold>a</bold>) after the staining. <bold>e</bold> Zoomed-in RI images of (<bold>b</bold>). <bold>f</bold> Zoomed-in virtual H&#x00026;E images of (<bold>c</bold>). <bold>g</bold> Zoomed in WSS images of (<bold>d</bold>). <bold>h</bold> Detailed 3D RI images of the selected region. <bold>i</bold>, Detailed 3D virtual H&#x00026;E images of the selected region. <bold>j</bold> Detailed WSS image of the selected region. <bold>k</bold> 3D rendering of the microanatomical structure of the selected region. <bold>l</bold> 3D binary masks of the single nucleus overlaid to the virtual H&#x00026;E images. <bold>m</bold> 3D rendering of the nucleus using the binary masks in (<bold>l</bold>). <bold>n</bold> Tracking of average numbers, areas, and eccentricities of the nuclei along the axial axis. <bold>o</bold> Tracking of perimeter, area, and major axis length of the lumen along the axial axis.</p></caption><graphic xlink:href="41467_2025_59820_Fig7_HTML" id="d33e1030"/></fig></p><p id="Par25">Chemical H&#x00026;E staining of thick tissue sections (50&#x02009;&#x000b5;m) often exhibits color artifacts&#x02014;such as overly bluish or reddish regions and inconsistent tones&#x02014;due to non-uniform staining (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7d, g</xref>). These artifacts highlight the limitations of conventional histology for thicker samples. In contrast, our virtual H&#x00026;E approach produces more consistent color distributions (Figs.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>c, <xref rid="Fig7" ref-type="fig">7f</xref>) and offers enhanced analytical capabilities, including the ability to visualize the axial distribution of overlapping nuclei&#x02014;features not readily discernible in standard chemical H&#x00026;E images.</p><p id="Par26">To closely examine the 3D lumen structure, we focused on detailed images of a lumen and surrounding gland within a region of interest, and compared it with the chemical H&#x00026;E images (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7h&#x02013;j</xref>). Through volumetric representation and multiple axial images, we elucidated the structural alterations of lumens as axial position changed. These alterations became more pronounced as the sample thickness increased compared to previous results. Furthermore, within the same region of interest, we identified the 3D distribution of nuclei and microanatomical structures by segmenting nuclei and the lumen (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7k</xref>). Leveraging the capabilities of Hover-Net<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, we automated the segmentation of nuclei while labeling the lumens manually. With the resulting binary masks, we rendered the 3D microanatomical structures of lumens and the surrounding nuclei distribution (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7k</xref>).</p><p id="Par27">To investigate deeper into subcellular structures, we extracted a cube encompassing a single nucleus, guided by the corresponding binary masks (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7l, m</xref>). This extraction not only unveiled the 3D structure of the nucleus but also provided quantitative morphological parameters such as volume and surface area. Additionally, we computed the average numbers, areas, and eccentricities of nuclei and traced their trends along the axial axis. Intriguingly, our analysis revealed a subtle increase in nuclear eccentricity, while their area and the number remained relatively consistent along the axial axis, with some observable fluctuations (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7n</xref>). Similarly, we quantified the perimeter, area, and major axis length of the lumen at each image slice. By tracking their variations along the axial axis, we unveiled a trend of contracting lumens as axial position increased, supported by a consistent decrease in these parameters. Specifically, we observed approximately a 40% reduction in the lumen area between the uppermost and lowermost axial sections (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7o</xref>). Unlike the quantitative analyzes of volumetric lumen structures enabled by 3D virtual H&#x00026;E images, chemical H&#x00026;E staining could not provide axial information or volumetric parameters of the lumens. Furthermore, due to distorted staining results, chemical H&#x00026;E images were unable to resolve the nuclei distribution around the lumen (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7j</xref>).</p><p id="Par28">These results not only provide our understanding of 3D colon cancer structure but also effectively highlight the scalability of 3D virtual staining, enabling a multitude of downstream quantitative analyzes on the 3D distribution of nuclei and microanatomical structures.</p></sec><sec id="Sec8"><title>Validation of virtual 3D H&#x00026;E staining across sample origins and institutional settings</title><p id="Par29">To ensure the scalability and repeatability of our virtual staining framework, validating its application across varied institutional settings and sample origins is essential. Our approach was tested in different countries, using gastric cancer slides imaged with holotomography at an institute distinct from those mentioned previously. The methodology for creating the training dataset mirrored that of the colon cancer studies, with variations only in the slide numbers and the types of WSS utilized (refer to Methods for details). This process yielded a dataset comprising 9231 patches, divided into 7002 for training and 2229 for validation. To quantitatively assess the network&#x02019;s performance, SSIM values were computed for each selected window (see Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">3</xref>). Furthermore, to ensure methodological consistency, we maintained the same neural network architecture and hyperparameters as those used in the colon cancer study, reinforcing the consistency and robustness of our approach.</p><p id="Par30">Following training, we prepared a 20 &#x003bc;m-thick gastric cancer tissue slide without any staining. Then, we acquired 3D RI images spanning ~1.92&#x02009;&#x000d7;&#x02009;1.22&#x02009;mm regions using holotomography (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8a</xref>). These wide-field RI images clearly depicted anatomical structures such as smooth muscle and blood vessels, with vascular structures displaying individual cells within circular surrounding tissue structures pointed by a yellow arrow, alongside wavy textures in the smooth muscle as indicated with a cyan arrow (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8b</xref>).<fig id="Fig8"><label>Fig. 8</label><caption><title>Validations of the virtual staining across different organ types and institutional settings a.</title><p>A wide-field RI image obtained from the label-free 20 &#x003bc;m-thick gastric cancer tissue slide. A single section of the 3D RI image is presented. <bold>b</bold> Detailed 3D RI images of muscular (cyan arrows) and vascular (yellow arrows) structures obtained from the 20 &#x003bc;m-thick gastric cancer tissue slide. <bold>c</bold> A wide-field H&#x00026;E image predicted from (<bold>a</bold>). A single section of the 3D H&#x00026;E image is presented. <bold>d</bold> Detailed images of 3D H&#x00026;E images predicted from (<bold>a</bold>). <bold>e</bold>, <bold>f</bold> The <italic>x</italic>-<italic>y</italic>, <italic>y</italic>-<italic>z</italic>, and <italic>x</italic>-<italic>z</italic> cross sections of predicted 3D H&#x00026;E images.</p></caption><graphic xlink:href="41467_2025_59820_Fig8_HTML" id="d33e1137"/></fig></p><p id="Par31">Applying the same cropping and stitching procedures used for colon cancer data, we inputted the 3D RI images into our trained network to generate virtually stained 3D H&#x00026;E images of the gastric cancer slides (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8c</xref>). Notably, the network effectively predicted most anatomical structures, including smooth muscle and vascular structures, as evidenced in wide-field images. Detailed examination revealed a successful depiction of vascular structures as red blood cells with surrounding circular connective tissue structures (a yellow arrow in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8d</xref>) and wavy textures of smooth muscle (cyan arrows in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8d</xref>). Leveraging the 3D nature of the data, we observed changes in the cross-sectional morphology of vascular structures along the axial axis, as well as alterations in the distribution of red blood cells within it. Moreover, variations in the wavy texture of smooth muscle along the axial axis were visualized as pointed by cyan arrows, highlighting the ability of virtual 3D H&#x00026;E staining to reveal intricate 3D tissue textures.</p><p id="Par32">To further elucidate structural features in 3D, we visualized cross-sections of detailed images depicting vascular and muscular structures (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>e, <xref rid="Fig8" ref-type="fig">f</xref>). These cross-sectional views delineated continuous structural alterations along the axial axis in vessels and muscles, exhibiting 3D reconstruction. Additionally, we observed densely packed red blood cells throughout the whole axial sections (Lines 2 and 5 in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8e</xref>), as well as a widening tendency of smooth muscle as axial position increases (Lines 2 and 3 in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8f</xref>).</p></sec></sec><sec id="Sec9" sec-type="discussion"><title>Discussion</title><p id="Par33">The developed virtual volumetric H&#x00026;E staining framework adeptly overcomes two significant challenges: the labor-intensive nature of traditional sample staining and the limitations imposed by two-dimensional analysis. By training a neural network with RI images from a single, routinely prepared histopathological slide, we achieved the translation of these images into their H&#x00026;E stained equivalents. This neural network, once applied to 3D RI images of label-free thick tissue, effectively produced 3D virtual H&#x00026;E images. This method not only bypasses the need for physical staining, thereby reducing the requisite number of tissue slices, but also unveils detailed 3D representations of glandular, muscular, and vascular structures at the subcellular level within thick cancer tissues. Additionally, this approach facilitates downstream analyzes, allowing for quantitative assessments and 3D visualizations of microanatomical structures and also for various multi-omics measurements including spatial transcriptomics, thus offering valuable insights into the three-dimensional architectural complexities of cancerous tissues.</p><p id="Par34">The versatility of our virtual H&#x00026;E staining method is particularly significant, as it is not confined to specific organ types or reliant on data training from any particular institution. This flexibility originates from a sample preparation, imaging, and network training process that is universally applicable, allowing for the method&#x02019;s scalability to tissue slides from a wide array of organs. Additionally, our approach is equally adaptable to cytology slides, which, despite their inherent 3D structures, have traditionally been examined in a 2D framework using bright-field microscopy. Our framework also proves its repeatability and reliability across various institutional settings, underscoring the robustness of both the holotomography technique and the neural network architecture employed. Such consistency further highlights the method&#x02019;s potential for broad application, promising valuable contributions to both research and clinical practices globally.</p><p id="Par35">Building on this foundation, the proposed framework has significant potential to address a range of pathological challenges, such as detecting tumors in lymph nodes, identifying tumor-infiltrating lymphocytes, and assessing tumor margins. By offering comprehensive 3D views, this approach surmounts the limitations of conventional histology, which depends on 2D sections and may overlook critical features. Moreover, with further technical adaptations, the framework could be extended to immunohistochemistry or immunofluorescence, enabling more precise and specific assessments of pathological status in a 3D context.</p><p id="Par36">Moreover, holotomography quantifies RI values, determining the protein concentration in specific regions. This empowers the proposed framework to conduct further quantitative analysis of subcellular structures. Specifically, segmentation or classification of cell types within the generated 3D H&#x00026;E stained images can be performed using open-source, pre-trained resources<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, with results directly applicable to the RI images for calculating 3D morphological and biophysical parameters.</p><p id="Par37">While our proposed virtual staining method has been successfully applied to tissues with a thickness of up to 50&#x02009;&#x003bc;m, the applicable thickness can be pushed further through technical advances of holotomography. Currently, holotomography faces challenges related to image degradation as the sample becomes thicker. The degradation in image quality can be attributed to the multiple scattering of the light within the thick sample, which hinders the accurate reconstruction of RI values through holotomography. Nonetheless, several strategies are being employed to address this challenge, including enhancements to optical conditions, the implementation of reconstruction algorithms tailored for thicker samples<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, and the application of aberration correction algorithms<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. Additionally, variations in standard sample preparation protocols&#x02014;such as incomplete deparaffinization, selection of mounting media, and choice of fixatives&#x02014;can influence the refractive index distribution and overall image quality. In particular, residual paraffin can introduce optical mismatches, while different fixatives may induce tissue shrinkage or dehydration, thereby affecting holotomography measurements. By adhering to standardized deparaffinization procedures, verifying the absence of residual paraffin, and carefully selecting or validating fixatives, we can mitigate these confounding factors. Optimizing these aspects of sample preparation and establishing robust quality-control measures will further improve the consistency and reliability of holotomography when integrated with virtual staining. With these refinements, we envision a synergistic advancement of both virtual staining and holotomography.</p><p id="Par38">Beyond these challenges in image quality, scaling holotomography to whole-slide imaging introduces additional hurdles related to imaging speed and data storage requirements. This arises from its higher resolution, which is ~two to ten times greater than that of conventional approaches<sup><xref ref-type="bibr" rid="CR2">2</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref></sup>. Recent advancements in imaging schemes<sup><xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup>, instrument design, and data processing methods<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> promise to enhance imaging speed and computational efficiency. By leveraging these developments, high-throughput holotomography can advance precise 3D tissue analyzes, thereby expanding its applications in both biomedical research and clinical practice.</p><p id="Par39">With these capabilities and potential technical advancements, we believe that the proposed framework can effectively overcome current limitations in histopathology and immunology in unexplored dimentions<sup><xref ref-type="bibr" rid="CR47">47</xref>,<xref ref-type="bibr" rid="CR48">48</xref></sup>, and broaden its utility across various histopathological studies<sup><xref ref-type="bibr" rid="CR49">49</xref>,<xref ref-type="bibr" rid="CR50">50</xref></sup>, aiding in diagnosis and precision medicine.</p></sec><sec id="Sec10"><title>Methods</title><sec id="Sec11"><title>Colon cancer slides preparation from Gangnam Severance Hospital</title><p id="Par40">This study complied with all relevant ethical regulations and was approved by the Institutional Review Board (IRB) of Gangnam Severance Hospital (IRB No. 3-2022-0083) and Seoul National University Boramae Medical Center (IRB No. 30-2018-32). Written informed consent was waived by the IRB due to the retrospective nature of the study and the use of residual samples without any clinical data collection. To create the training datasets, we utilized a 4 &#x003bc;m-thick colon cancer tissue slide prepared as tumor microarrays. The slide was newly diagnosed and reviewed by a pathologist (SJS) at Gangnam Severance Hospital in January 2023. Among 26 cases, a colon cancer tissue slide with grade two was randomly selected. From the chosen case, a formalin-fixed paraffin-embedded tissue block was obtained. The tissue block was sliced to a 4&#x02009;&#x003bc;m thickness using a microtome. The slice was carefully placed on a slide glass and deparaffinized using xylene. Subsequently, the slide was then stained with H&#x00026;E, mounted with a mounting medium (Consul-Mount, Epredia), which has an RI value of 1.495 measured by a refractometer (R-5000, Atago), and finally covered with coverslips. Annotations of glands and stroma in the H&#x00026;E stained images were carried out by an experienced pathologist (SJS).</p><p id="Par41">For the label-free thick tissue slides used in the testing phase, we randomly selected two cases of colon cancer. Formalin-fixed paraffin-embedded blocks from the selected cases were sliced into 10, 20, and 50 &#x003bc;m-thick sections using a microtome, respectively. The subsequent procedures were consistent with those of training datasets, except that we did not stain the slides and mounted the slides immediately after the deparaffinization.</p></sec><sec id="Sec12"><title>Gastric cancer slides preparation from mayo clinic</title><p id="Par42">For the training datasets, we obtained four gastric cancer tissue slides, each with a 5&#x02009;&#x003bc;m thickness and H&#x00026;E stain. The slide was newly diagnosed and reviewed by pathologists at the Seoul National University Boramae Medical Center in January 2006. The slide preparation procedures were consistent with those conducted at Gangnam Severance Hospital.</p><p id="Par43">For testing purposes, a label-free, 20 &#x003bc;m-thick tissue slide was randomly selected from the gastric cancer cases. The slide was prepared using the same procedures as those performed at Gangnam Severance Hospital.</p></sec><sec id="Sec13"><title>Image acquisition</title><p id="Par44">To measure the RI distributions of tissue slides, we employed a low-coherence holotomography setup (HT-X1 and X1-Plus, Tomocube). This system measures transmitted intensity images of a sample using four optimized K&#x000f6;hler illumination patterns<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, from which the 3D RI distribution was reconstructed by deconvolution of the intensity images with theoretically estimated point spread functions. In the experimental setup, a light-emitting diode (LED) with a center wavelength of 449&#x02009;nm within a digital micromirror device (DMD) module (DLP4500, Texas Instrument) was used as the illumination source. The blue wavelength was chosen to avoid overlap with the peak absorption spectrum of H&#x00026;E staining. The illumination intensity was controlled using a DMD located in the Fourier plane, which was then projected onto the sample using a condenser lens (<italic>f</italic>&#x02009;=&#x02009;180&#x02009;mm, numerical aperture (NA)&#x02009;=&#x02009;0.75). The diffracted light from the specimen was collected using an objective lens (numerical aperture (NA)&#x02009;&#x02009;=&#x02009;&#x02009;0.95) and measured using a CMOS camera (FS-U3-28S5, FLIR) located in the image plane. The RI images offer a lateral and axial resolution of 156&#x02009;nm and 1.07&#x02009;&#x003bc;m, respectively. In addition to RI images, this system offers single-channel BF (scBF) images, which are single-intensity images taken under uniform circular pattern illumination. Wide-field RI images were obtained by stitching the tiled RI images of each field of view with the size of 227&#x02009;&#x000d7;&#x02009;227&#x02009;&#x003bc;m. For the stitching, the raw intensity images were captured with an overlap of 23&#x02009;&#x003bc;m for both RI and scBF images. To obtain the ground truth images, the H&#x00026;E stained slides were scanned using the whole slide scanner manufactured by 3D HISTECH for colon cancer slides and by Leica for gastric cancer slides.</p></sec><sec id="Sec14"><title>All-in-focus</title><p id="Par45">We applied an all-in-focus algorithm only to the RI images obtained from a slide used for network training<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. This algorithm ensures that the single focal plane includes most of the cellular structures. Specifically, we used a vignette size of 100 pixels with a step size of 10 pixels. Normalized variance was calculated in each window for all axial sections, and the window from the section with the maximum normalized variance value was selected as the best focal plane. Repeating the procedures for the whole 3D RI images resulted in a single all-in-focused RI image.</p></sec><sec id="Sec15"><title>Registration</title><p id="Par46">Image registration between the RI and WSS images involved three steps. First, we manually cropped the regions from the WSS images where they were imaged using holotomography. Subsequently, we applied a spatial transform network<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> to register the manually cropped WSS images with the scBF images acquired using holotomography. This step resulted in paired images of scBF and WSS at a wide-field scale. Then, we cropped the paired datasets into 1024&#x02009;&#x000d7;&#x02009;1024-pixel patches and repeated the registration using a spatial transform network between these patches to ensure precise alignment at each patch scale. Note that aligning the WSS images with scBF images directly leads to the alignment with RI images, as scBF and RI images are captured at the same position.</p><p id="Par47">All the registration steps, except for the manual cropping, were conducted using spatial transform networks<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Specifically, EfficientNet<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> was trained to discover the optimal affine transform matrix for the input WSS images, which will be used to transform the WSS images to be aligned with the target scBF images. The training was achieved by minimizing the loss defined using Pearson&#x02019;s correlation coefficient (PCC):<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e1274">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\mathscr{L}}}}_{{pcc}}\left(x,y\right)=1-\frac{{\mathbb{E}}\left[\left(x-{\mu }_{x}\right)\left(y-{\mu }_{y}\right)\right]}{{\sigma }_{x}{\sigma }_{y}}$$\end{document}</tex-math><mml:math id="d33e1280"><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mfenced close=")" open="("><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41467_2025_59820_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>x</italic> and <italic>y</italic> refer to the input WSS and target scBF images, respectively. <inline-formula id="IEq1"><alternatives><tex-math id="d33e1341">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}</tex-math><mml:math id="d33e1346"><mml:mi>&#x003bc;</mml:mi></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq2"><alternatives><tex-math id="d33e1350">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="d33e1355"><mml:mi>&#x003c3;</mml:mi></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq2.gif"/></alternatives></inline-formula> denote the mean and the standard deviation of each image, respectively. Once the training was completed, we employed the trained network to predict the optimized affine transform matrix, which was then applied to register the WSS images.</p></sec><sec id="Sec16"><title>Training details</title><p id="Par48">A conditional GAN was used to train the network for generating 3D H&#x00026;E images, primarily based on a previous study<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. In this framework, the generator&#x02019;s objective is to transfer the RI images into the feasible H&#x00026;E stained BF images by minimizing the <inline-formula id="IEq3"><alternatives><tex-math id="d33e1367">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\mathscr{L}}}}_{{pcc}}$$\end{document}</tex-math><mml:math id="d33e1372"><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq3.gif"/></alternatives></inline-formula> between the images generated by the network and their corresponding ground truth images. Simultaneously, the discriminator aims to distinguish the images from the network and genuine images when provided with the corresponding input images. The final objective of this framework becomes<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e1383">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}^{*}={{\rm{arg}}} \, {\min }_{G}\,{\max }_{D}{{{\mathscr{L}}}}_{{cGAN}}\left(G,D\right)+\lambda {{{\mathscr{L}}}}_{{pcc}}\left(G\left(x\right),y\right)$$\end{document}</tex-math><mml:math id="d33e1389"><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">arg</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:mi>G</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2025_59820_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq4"><alternatives><tex-math id="d33e1445">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\mathscr{L}}}}_{{cGAN}}$$\end{document}</tex-math><mml:math id="d33e1450"><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq4.gif"/></alternatives></inline-formula> is defined as:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e1462">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\mathscr{L}}}}_{{cGAN}}(G,D){\mathbb{=}}{\mathbb{E}}[og D(x,y)]+{\mathbb{E}}[\log \left(\right.1-D(x,G(x))]\,$$\end{document}</tex-math><mml:math id="d33e1468"><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="double-struck">=</mml:mi><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mfenced open="("><mml:mrow/></mml:mfenced><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math><graphic xlink:href="41467_2025_59820_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <italic>G</italic>(<inline-formula id="IEq5"><alternatives><tex-math id="d33e1534">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdot$$\end{document}</tex-math><mml:math id="d33e1539"><mml:mo>&#x022c5;</mml:mo></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq5.gif"/></alternatives></inline-formula>) and <italic>D</italic>(<inline-formula id="IEq6"><alternatives><tex-math id="d33e1546">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdot$$\end{document}</tex-math><mml:math id="d33e1551"><mml:mo>&#x022c5;</mml:mo></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq5.gif"/></alternatives></inline-formula>) refer to the generator and discriminator network operators, respectively. <italic>x</italic> denotes an input image and <italic>y</italic> indicates a ground truth image.</p><p id="Par49">For the architecture of the generator, we used SCNAS<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, an optimized network specifically tailored for 3D medical image segmentation (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">2a</xref>). SCNAS employs a stochastic sampling algorithm within a gradient-based bi-level optimization framework to simultaneously search for the optimal network parameters at multiple levels using generic 3D medical imaging datasets. This search resulted in the discovery of a U-Net-like encoder-decoder structure with skip connections. In each micro-level architecture, we added a squeeze excitation block<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. The key network parameters were as follows: activation function, leaky ReLU; normalization function, instance normalization; the size of the initial feature map, 12; the number of layers, 8; feature map multiplier, 2.</p><p id="Par50">The discriminator consists of five convolutional layers with a kernel size of 4 and a stride of 2 (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">2b</xref>). The first four layers use leaky ReLU with a slope of 0.2, while the last layer employs ReLU as an activation function. Following the last layer, the result is activated by a Sigmoid function. The first and last layers use batch normalization, while the rest of the layers use instance normalization.</p><p id="Par51">Both the generator and discriminator were trained using an adaptive moment estimation optimizer (ADAM) to update the learnable parameters<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. Image augmentation techniques, such as adding blur, adjusting brightness, and introducing Gaussian noise, were randomly added to the training patches. A learning rate of 1&#x02009;&#x000d7;&#x02009;10<sup>-4</sup> was maintained throughout the training process. Training was conducted for 40 epochs with a batch size of 1, and early stopping was employed to prevent overfitting. The network was implemented using Python version 3.8.0 and PyTorch<sup><xref ref-type="bibr" rid="CR54">54</xref></sup> version 1.13.1, and the procedures were executed on NVIDIA GeForce 4090 GPU.</p><p id="Par52">The training results are compared with the ground truth images using SSIM, which is defined as below:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="d33e1593">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\rm{SSIM}}}\left(x,y\right)=\frac{1}{3}{\sum}_{i=1,2,3}\frac{(2{\mu }_{x,i}{\mu }_{y,i}+{C}_{1})(2{\sigma }_{{xy},i}+{C}_{2})}{\left({{\mu }_{x,i}}^{2}+{{\mu }_{y,i}}^{2}+{C}_{1}\right)\left({{\sigma }_{x,i}}^{2}+{{\sigma }_{y,i}}^{2}+{C}_{2}\right)}$$\end{document}</tex-math><mml:math id="d33e1599"><mml:mi mathvariant="normal">SSIM</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo mathsize="big">&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41467_2025_59820_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <italic>x</italic> and <italic>y</italic> are the network-generated output and corresponding ground truth image, respectively. <inline-formula id="IEq7"><alternatives><tex-math id="d33e1742">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}</tex-math><mml:math id="d33e1747"><mml:mi>&#x003bc;</mml:mi></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq8"><alternatives><tex-math id="d33e1751">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="d33e1756"><mml:mi>&#x003c3;</mml:mi></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq2.gif"/></alternatives></inline-formula> refer to the mean and the standard deviation of each image, respectively, where an index <inline-formula id="IEq9"><alternatives><tex-math id="d33e1761">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="d33e1766"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq9.gif"/></alternatives></inline-formula> refers to the RGB channels of each image. <inline-formula id="IEq10"><alternatives><tex-math id="d33e1770">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sigma }_{{xy},i}$$\end{document}</tex-math><mml:math id="d33e1775"><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq10.gif"/></alternatives></inline-formula> represents the covariance of both images calculated from <inline-formula id="IEq11"><alternatives><tex-math id="d33e1786">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="d33e1791"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq9.gif"/></alternatives></inline-formula>-th image channels. <inline-formula id="IEq12"><alternatives><tex-math id="d33e1795">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${C}_{1}$$\end{document}</tex-math><mml:math id="d33e1800"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq12.gif"/></alternatives></inline-formula> and <inline-formula id="IEq13"><alternatives><tex-math id="d33e1808">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${C}_{2}$$\end{document}</tex-math><mml:math id="d33e1813"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2025_59820_Article_IEq13.gif"/></alternatives></inline-formula> are regularization constants, set as 6.5025 and 58.5225, respectively, by default. The SSIM values were calculated for the selected windows.</p></sec><sec id="Sec17"><title>Reporting summary</title><p id="Par53">Further information on research design is available in the&#x000a0;<xref rid="MOESM3" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p></sec></sec><sec id="Sec18" sec-type="supplementary-material"><title>Supplementary information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41467_2025_59820_MOESM1_ESM.pdf"><caption><p>Supplementary Information</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="41467_2025_59820_MOESM2_ESM.pdf"><caption><p>Peer Review File</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="41467_2025_59820_MOESM3_ESM.pdf"><caption><p>Reporting Summary</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Juyeon Park, Su-Jin Shin.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s41467-025-59820-0.</p></sec><ack><title>Acknowledgements</title><p>The authors gratefully acknowledge the insightful comments provided by Seung-Mo Hong of Asan Medical Center. This work was supported by the National Research Foundation of Korea (RS-2024-00442348, 2022M3H4A1A02074314, RS-2023-00241278, RS-2024-00351903), an Institute of Information &#x00026; Communications Technology Planning &#x00026; Evaluation (IITP; 2021-0-00745) grant funded by the Korea government (MSIT), Korea Institute for Advancement of Technology (KIAT) through the International Cooperative R&#x00026;D program (P0028463), and the Korean Fund for Regenerative Medicine (KFRM) grant funded by the Korea government (the Ministry of Science and ICT and the Ministry of Health &#x00026; Welfare) (21A0101L1-12), Korea Health Technology R&#x00026;D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health &#x00026; Welfare, Korea (HI21C0977, HR22C1605),&#x000a0;National Cancer Institute (NCI) through grant 1R01CA276690 awarded to Tae Hyun Hwang (THH), grant 1R37CA265967 to THH, and grant U01CA294518 to THH.&#x000a0;Additional support was provided by the Department of Defense (DOD) under grant CA190578 to THH. THH is also supported by the Eric and Wendy Schmidt Foundation&#x02019;s AI Innovation Award through the Mayo Clinic Foundation and the AACR Innovation and Discovery Grant.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>J.P. and S.-J.S. contributed equally to this work. J.P., G.K., H.C., D.R., D.A. and H.-S.M. developed the deep-learning pipeline, and J.P. implemented and optimized the pipeline. J.P., S.-J.S., J.C., I.B., M.K. and I.J., collected the data. S.-J.S., J.E.H., J.-Y.S., J.H.P., K.S.L and N.H.C. reviewed the pathology images. J.P., G.K. and Y.P. wrote the paper with critical inputs from all authors. T.H.H. and Y.P. supervised all aspects of the work.</p></notes><notes notes-type="peer-review"><title>Peer review</title><sec id="FPar1"><title>Peer review information</title><p id="Par54"><italic>Nature Communications</italic> thanks the anonymous, reviewers for their contribution to the peer review of this work. A peer review file is available.</p></sec></notes><notes notes-type="data-availability"><title>Data availability</title><p>The training, testing, and exemplary data used in this study are available in the science data&#x000a0;bank 10.57760/sciencedb.24217.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>All neural network training, testing, and data analysis were performed using Python 3.8.0 and MATLAB R2021b. The code for developing the neural network is available in a GitHub repository <ext-link ext-link-type="uri" xlink:href="https://github.com/BMOLKAIST/3D_virtual_HE_staining">https://github.com/BMOLKAIST/3D_virtual_HE_staining</ext-link> or 10.5281/zenodo.15030226<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> under the MIT License.</p></notes><notes id="FPar2" notes-type="COI-statement"><title>Competing interests</title><p id="Par55">D.R., D.A., H.C., H.-S.M. and Y.K.P. have financial interests in Tomocube, a company that commercializes holotomography instruments.&#x000a0;T.H.H. is a scientific co-founder of Kure.ai Therapeutics and its subsidiary, Kure.S. He holds no official roles in the companies and receives no salary or consulting fees. The companies did not influence the design, execution, or interpretation of this study. All other authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Hwang</surname><given-names>TH</given-names></name></person-group><article-title>The 3D revolution in cancer discovery</article-title><source>Cancer Discov.</source><year>2024</year><volume>14</volume><fpage>625</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1158/2159-8290.CD-23-1499</pub-id><pub-id pub-id-type="pmid">38571426</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Wang, L., Li, M. &#x00026; Hwang, T. H. The 3D revolution in cancer discovery. <italic>Cancer Discov.</italic><bold>14</bold>, 625&#x02013;629 (2024).<pub-id pub-id-type="pmid">38571426</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Kiemen</surname><given-names>AL</given-names></name><etal/></person-group><article-title>CODA: quantitative 3D reconstruction of large tissues at cellular resolution</article-title><source>Nat. Methods</source><year>2022</year><volume>19</volume><fpage>1490</fpage><lpage>1499</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01650-9</pub-id><pub-id pub-id-type="pmid">36280719</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Kiemen, A. L. et al. CODA: quantitative 3D reconstruction of large tissues at cellular resolution. <italic>Nat. Methods</italic><bold>19</bold>, 1490&#x02013;1499 (2022).<pub-id pub-id-type="pmid">36280719</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Olson</surname><given-names>E</given-names></name><name><surname>Levene</surname><given-names>MJ</given-names></name><name><surname>Torres</surname><given-names>R</given-names></name></person-group><article-title>Multiphoton microscopy with clearing for three dimensional histology of kidney biopsies</article-title><source>Biomed. Opt. Express</source><year>2016</year><volume>7</volume><fpage>3089</fpage><lpage>3096</lpage><pub-id pub-id-type="doi">10.1364/BOE.7.003089</pub-id><pub-id pub-id-type="pmid">27570700</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Olson, E., Levene, M. J. &#x00026; Torres, R. Multiphoton microscopy with clearing for three dimensional histology of kidney biopsies. <italic>Biomed. Opt. Express</italic><bold>7</bold>, 3089&#x02013;3096 (2016).<pub-id pub-id-type="pmid">27570700</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Paul</surname><given-names>D</given-names></name><name><surname>Cowan</surname><given-names>AE</given-names></name><name><surname>Ge</surname><given-names>S</given-names></name><name><surname>Pachter</surname><given-names>JS</given-names></name></person-group><article-title>Novel 3D analysis of Claudin-5 reveals significant endothelial heterogeneity among CNS microvessels</article-title><source>Microvascular Res.</source><year>2013</year><volume>86</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1016/j.mvr.2012.12.001</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Paul, D., Cowan, A. E., Ge, S. &#x00026; Pachter, J. S. Novel 3D analysis of Claudin-5 reveals significant endothelial heterogeneity among CNS microvessels. <italic>Microvascular Res.</italic><bold>86</bold>, 1&#x02013;10 (2013).</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Glaser</surname><given-names>AK</given-names></name><etal/></person-group><article-title>Light-sheet microscopy for slide-free non-destructive pathology of large clinical specimens</article-title><source>Nat. Biomed. Eng.</source><year>2017</year><volume>1</volume><fpage>0084</fpage><pub-id pub-id-type="doi">10.1038/s41551-017-0084</pub-id><pub-id pub-id-type="pmid">29750130</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Glaser, A. K. et al. Light-sheet microscopy for slide-free non-destructive pathology of large clinical specimens. <italic>Nat. Biomed. Eng.</italic><bold>1</bold>, 0084 (2017).<pub-id pub-id-type="pmid">29750130</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>JTC</given-names></name><etal/></person-group><article-title>Harnessing non-destructive 3D pathology</article-title><source>Nat. Biomed. Eng.</source><year>2021</year><volume>5</volume><fpage>203</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1038/s41551-020-00681-x</pub-id><pub-id pub-id-type="pmid">33589781</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Liu, J. T. C. et al. Harnessing non-destructive 3D pathology. <italic>Nat. Biomed. Eng.</italic><bold>5</bold>, 203&#x02013;218 (2021).<pub-id pub-id-type="pmid">33589781</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>Y</given-names></name><name><surname>Depeursinge</surname><given-names>C</given-names></name><name><surname>Popescu</surname><given-names>G</given-names></name></person-group><article-title>Quantitative phase imaging in biomedicine</article-title><source>Nat. Photonics</source><year>2018</year><volume>12</volume><fpage>578</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1038/s41566-018-0253-x</pub-id></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Park, Y., Depeursinge, C. &#x00026; Popescu, G. Quantitative phase imaging in biomedicine. <italic>Nat. Photonics</italic><bold>12</bold>, 578&#x02013;589 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Uttam</surname><given-names>S</given-names></name></person-group><article-title>Perspective on quantitative phase imaging to improve precision cancer medicine</article-title><source>J. Biomed. Opt.</source><year>2024</year><volume>29</volume><fpage>S22705</fpage><pub-id pub-id-type="doi">10.1117/1.JBO.29.S2.S22705</pub-id><pub-id pub-id-type="pmid">38584967</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Liu, Y. &#x00026; Uttam, S. Perspective on quantitative phase imaging to improve precision cancer medicine. <italic>J. Biomed. Opt.</italic><bold>29</bold>, S22705 (2024).<pub-id pub-id-type="pmid">38584967</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>J</given-names></name><etal/></person-group><article-title>Quantification of structural heterogeneity in H&#x00026;E stained clear cell renal cell carcinoma using refractive index tomography</article-title><source>Biomed. Opt. Express</source><year>2023</year><volume>14</volume><fpage>1071</fpage><lpage>1081</lpage><pub-id pub-id-type="doi">10.1364/BOE.484092</pub-id><pub-id pub-id-type="pmid">36950245</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Park, J. et al. Quantification of structural heterogeneity in H&#x00026;E stained clear cell renal cell carcinoma using refractive index tomography. <italic>Biomed. Opt. Express</italic><bold>14</bold>, 1071&#x02013;1081 (2023).<pub-id pub-id-type="pmid">36950245</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Zhang, J. K., He, Y. R., Sobh, N. &#x00026; Popescu, G. Label-free colorectal cancer screening using deep learning and spatial light interference microscopy (SLIM). <italic>APL Photonics</italic><bold>5</bold>, 10.1063/5.0004723 (2020).</mixed-citation></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Majeed</surname><given-names>H</given-names></name><name><surname>Nguyen</surname><given-names>TH</given-names></name><name><surname>Kandel</surname><given-names>ME</given-names></name><name><surname>Kajdacsy-Balla</surname><given-names>A</given-names></name><name><surname>Popescu</surname><given-names>G</given-names></name></person-group><article-title>Label-free quantitative evaluation of breast tissue using Spatial Light Interference Microscopy (SLIM)</article-title><source>Sci. Rep.</source><year>2018</year><volume>8</volume><fpage>6875</fpage><pub-id pub-id-type="doi">10.1038/s41598-018-25261-7</pub-id><pub-id pub-id-type="pmid">29720678</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Majeed, H., Nguyen, T. H., Kandel, M. E., Kajdacsy-Balla, A. &#x00026; Popescu, G. Label-free quantitative evaluation of breast tissue using Spatial Light Interference Microscopy (SLIM). <italic>Sci. Rep.</italic><bold>8</bold>, 6875 (2018).<pub-id pub-id-type="pmid">29720678</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Gladstein</surname><given-names>S</given-names></name><etal/></person-group><article-title>Correlating colorectal cancer risk with field carcinogenesis progression using partial wave spectroscopic microscopy</article-title><source>Cancer Med.</source><year>2018</year><volume>7</volume><fpage>2109</fpage><lpage>2120</lpage><pub-id pub-id-type="doi">10.1002/cam4.1357</pub-id><pub-id pub-id-type="pmid">29573208</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Gladstein, S. et al. Correlating colorectal cancer risk with field carcinogenesis progression using partial wave spectroscopic microscopy. <italic>Cancer Med.</italic><bold>7</bold>, 2109&#x02013;2120 (2018).<pub-id pub-id-type="pmid">29573208</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Takabayashi</surname><given-names>M</given-names></name><name><surname>Majeed</surname><given-names>H</given-names></name><name><surname>Kajdacsy-Balla</surname><given-names>A</given-names></name><name><surname>Popescu</surname><given-names>G</given-names></name></person-group><article-title>Disorder strength measured by quantitative phase imaging as intrinsic cancer marker in fixed tissue biopsies</article-title><source>PLOS ONE</source><year>2018</year><volume>13</volume><fpage>e0194320</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0194320</pub-id><pub-id pub-id-type="pmid">29561905</pub-id>
</element-citation><mixed-citation id="mc-CR13" publication-type="journal">Takabayashi, M., Majeed, H., Kajdacsy-Balla, A. &#x00026; Popescu, G. Disorder strength measured by quantitative phase imaging as intrinsic cancer marker in fixed tissue biopsies. <italic>PLOS ONE</italic><bold>13</bold>, e0194320 (2018).<pub-id pub-id-type="pmid">29561905</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Sridharan</surname><given-names>S</given-names></name><name><surname>Macias</surname><given-names>V</given-names></name><name><surname>Tangella</surname><given-names>K</given-names></name><name><surname>Kajdacsy-Balla</surname><given-names>A</given-names></name><name><surname>Popescu</surname><given-names>G</given-names></name></person-group><article-title>Prediction of prostate cancer recurrence using quantitative phase imaging</article-title><source>Sci. Rep.</source><year>2015</year><volume>5</volume><fpage>9976</fpage><pub-id pub-id-type="doi">10.1038/srep09976</pub-id><pub-id pub-id-type="pmid">25975368</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Sridharan, S., Macias, V., Tangella, K., Kajdacsy-Balla, A. &#x00026; Popescu, G. Prediction of prostate cancer recurrence using quantitative phase imaging. <italic>Sci. Rep.</italic><bold>5</bold>, 9976 (2015).<pub-id pub-id-type="pmid">25975368</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Uttam</surname><given-names>S</given-names></name><etal/></person-group><article-title>Early prediction of cancer progression by depth-resolved nanoscale mapping of nuclear architecture from unstained tissue specimens</article-title><source>Cancer Res.</source><year>2015</year><volume>75</volume><fpage>4718</fpage><lpage>4727</lpage><pub-id pub-id-type="doi">10.1158/0008-5472.CAN-15-1274</pub-id><pub-id pub-id-type="pmid">26383164</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Uttam, S. et al. Early prediction of cancer progression by depth-resolved nanoscale mapping of nuclear architecture from unstained tissue specimens. <italic>Cancer Res.</italic><bold>75</bold>, 4718&#x02013;4727 (2015).<pub-id pub-id-type="pmid">26383164</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>P</given-names></name><etal/></person-group><article-title>Nanoscale nuclear architecture for cancer diagnosis beyond pathology via spatial-domain low-coherence quantitative phase microscopy</article-title><source>J. Biomed. Opt.</source><year>2010</year><volume>15</volume><fpage>066028</fpage><pub-id pub-id-type="doi">10.1117/1.3523618</pub-id><pub-id pub-id-type="pmid">21198202</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Wang, P. et al. Nanoscale nuclear architecture for cancer diagnosis beyond pathology via spatial-domain low-coherence quantitative phase microscopy. <italic>J. Biomed. Opt.</italic><bold>15</bold>, 066028 (2010).<pub-id pub-id-type="pmid">21198202</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>P</given-names></name><name><surname>Bista</surname><given-names>R</given-names></name><name><surname>Bhargava</surname><given-names>R</given-names></name><name><surname>Brand</surname><given-names>RE</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name></person-group><article-title>Spatial-domain low-coherence quantitative phase microscopy for cancer diagnosis</article-title><source>Opt. Lett.</source><year>2010</year><volume>35</volume><fpage>2840</fpage><lpage>2842</lpage><pub-id pub-id-type="doi">10.1364/OL.35.002840</pub-id><pub-id pub-id-type="pmid">20808342</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Wang, P., Bista, R., Bhargava, R., Brand, R. E. &#x00026; Liu, Y. Spatial-domain low-coherence quantitative phase microscopy for cancer diagnosis. <italic>Opt. Lett.</italic><bold>35</bold>, 2840&#x02013;2842 (2010).<pub-id pub-id-type="pmid">20808342</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Subramanian</surname><given-names>H</given-names></name><etal/></person-group><article-title>Nanoscale cellular changes in field carcinogenesis detected by partial wave spectroscopy</article-title><source>Cancer Res.</source><year>2009</year><volume>69</volume><fpage>5357</fpage><lpage>5363</lpage><pub-id pub-id-type="doi">10.1158/0008-5472.CAN-08-3895</pub-id><pub-id pub-id-type="pmid">19549915</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">Subramanian, H. et al. Nanoscale cellular changes in field carcinogenesis detected by partial wave spectroscopy. <italic>Cancer Res.</italic><bold>69</bold>, 5357&#x02013;5363 (2009).<pub-id pub-id-type="pmid">19549915</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>M</given-names></name><etal/></person-group><article-title>Label-free optical quantification of structural alterations in Alzheimer&#x02019;s disease</article-title><source>Sci. Rep.</source><year>2016</year><volume>6</volume><fpage>31034</fpage><pub-id pub-id-type="doi">10.1038/srep31034</pub-id><pub-id pub-id-type="pmid">27485313</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Lee, M. et al. Label-free optical quantification of structural alterations in Alzheimer&#x02019;s disease. <italic>Sci. Rep.</italic><bold>6</bold>, 31034 (2016).<pub-id pub-id-type="pmid">27485313</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Bokemeyer</surname><given-names>A</given-names></name><etal/></person-group><article-title>Quantitative phase imaging using digital holographic microscopy reliably assesses morphology and reflects elastic properties of fibrotic intestinal tissue</article-title><source>Sci. Rep.</source><year>2019</year><volume>9</volume><fpage>19388</fpage><pub-id pub-id-type="doi">10.1038/s41598-019-56045-2</pub-id><pub-id pub-id-type="pmid">31852983</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">Bokemeyer, A. et al. Quantitative phase imaging using digital holographic microscopy reliably assesses morphology and reflects elastic properties of fibrotic intestinal tissue. <italic>Sci. Rep.</italic><bold>9</bold>, 19388 (2019).<pub-id pub-id-type="pmid">31852983</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>AJ</given-names></name><etal/></person-group><article-title>Volumetric refractive index measurement and quantitative density analysis of mouse brain tissue with sub-micrometer spatial resolution</article-title><source>Adv. Photonics Res.</source><year>2023</year><volume>4</volume><fpage>2300112</fpage><pub-id pub-id-type="doi">10.1002/adpr.202300112</pub-id></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Lee, A. J. et al. Volumetric refractive index measurement and quantitative density analysis of mouse brain tissue with sub-micrometer spatial resolution. <italic>Adv. Photonics Res.</italic><bold>4</bold>, 2300112 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Hugonnet</surname><given-names>H</given-names></name><etal/></person-group><article-title>Multiscale label-free volumetric holographic histopathology of thick-tissue slides with subcellular resolution</article-title><source>Adv. Photonics</source><year>2021</year><volume>3</volume><fpage>026004</fpage><pub-id pub-id-type="doi">10.1117/1.AP.3.2.026004</pub-id></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Hugonnet, H. et al. Multiscale label-free volumetric holographic histopathology of thick-tissue slides with subcellular resolution. <italic>Adv. Photonics</italic><bold>3</bold>, 026004 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Filan</surname><given-names>C</given-names></name><name><surname>Song</surname><given-names>H</given-names></name><name><surname>Platt</surname><given-names>M</given-names></name><name><surname>Robles</surname><given-names>F</given-names></name></person-group><article-title>Analysis of structural effects of sickle cell disease on brain vasculature of mice using three-dimensional quantitative phase imaging</article-title><source>J. Biomed. Opt.</source><year>2023</year><volume>28</volume><fpage>096501</fpage><pub-id pub-id-type="pmid">37692563</pub-id>
</element-citation><mixed-citation id="mc-CR23" publication-type="journal">Filan, C., Song, H., Platt, M. &#x00026; Robles, F. Analysis of structural effects of sickle cell disease on brain vasculature of mice using three-dimensional quantitative phase imaging. <italic>J. Biomed. Opt.</italic><bold>28</bold>, 096501 (2023).<pub-id pub-id-type="pmid">37692563</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>TM</given-names></name><etal/></person-group><article-title>Label- and slide-free tissue histology using 3D epi-mode quantitative phase imaging and virtual hematoxylin and eosin staining</article-title><source>Optica</source><year>2023</year><volume>10</volume><fpage>1605</fpage><lpage>1618</lpage><pub-id pub-id-type="doi">10.1364/OPTICA.502859</pub-id><pub-id pub-id-type="pmid">39640229</pub-id>
</element-citation><mixed-citation id="mc-CR24" publication-type="journal">Abraham, T. M. et al. Label- and slide-free tissue histology using 3D epi-mode quantitative phase imaging and virtual hematoxylin and eosin staining. <italic>Optica</italic><bold>10</bold>, 1605&#x02013;1618 (2023).<pub-id pub-id-type="pmid">39640229</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Park, J. et al. Artificial intelligence-enabled quantitative phase imaging methods for life sciences. <italic>Nat. Methods</italic>, 10.1038/s41592-023-02041-4 (2023).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Goda, K., Popescu, G., Tsia, K. K. &#x00026; Psaltis, D. Computational optical imaging goes viral. <italic>APL Photonics</italic><bold>5</bold>, 030401 (2020).</mixed-citation></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>B</given-names></name><etal/></person-group><article-title>Deep learning-enabled virtual histological staining of biological samples</article-title><source>Light.: Sci. Appl.</source><year>2023</year><volume>12</volume><fpage>57</fpage><pub-id pub-id-type="doi">10.1038/s41377-023-01104-7</pub-id><pub-id pub-id-type="pmid">36864032</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Bai, B. et al. Deep learning-enabled virtual histological staining of biological samples. <italic>Light.: Sci. Appl.</italic><bold>12</bold>, 57 (2023).<pub-id pub-id-type="pmid">36864032</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>R</given-names></name><etal/></person-group><article-title>Virtual brightfield and fluorescence staining for Fourier ptychography via unsupervised deep learning</article-title><source>Opt. Lett.</source><year>2020</year><volume>45</volume><fpage>5405</fpage><lpage>5408</lpage><pub-id pub-id-type="doi">10.1364/OL.400244</pub-id><pub-id pub-id-type="pmid">33001905</pub-id>
</element-citation><mixed-citation id="mc-CR28" publication-type="journal">Wang, R. et al. Virtual brightfield and fluorescence staining for Fourier ptychography via unsupervised deep learning. <italic>Opt. Lett.</italic><bold>45</bold>, 5405&#x02013;5408 (2020).<pub-id pub-id-type="pmid">33001905</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>S-M</given-names></name><etal/></person-group><article-title>Revealing architectural order with quantitative label-free imaging and deep learning</article-title><source>elife</source><year>2020</year><volume>9</volume><fpage>e55502</fpage><pub-id pub-id-type="doi">10.7554/eLife.55502</pub-id><pub-id pub-id-type="pmid">32716843</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Guo, S.-M. et al. Revealing architectural order with quantitative label-free imaging and deep learning. <italic>elife</italic><bold>9</bold>, e55502 (2020).<pub-id pub-id-type="pmid">32716843</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Rivenson</surname><given-names>Y</given-names></name><etal/></person-group><article-title>PhaseStain: the digital staining of label-free quantitative phase microscopy images using deep learning</article-title><source>Light.: Sci. Appl.</source><year>2019</year><volume>8</volume><fpage>23</fpage><pub-id pub-id-type="doi">10.1038/s41377-019-0129-y</pub-id><pub-id pub-id-type="pmid">30728961</pub-id>
</element-citation><mixed-citation id="mc-CR30" publication-type="journal">Rivenson, Y. et al. PhaseStain: the digital staining of label-free quantitative phase microscopy images using deep learning. <italic>Light.: Sci. Appl.</italic><bold>8</bold>, 23 (2019).<pub-id pub-id-type="pmid">30728961</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Hugonnet</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>M</given-names></name><name><surname>Park</surname><given-names>Y</given-names></name></person-group><article-title>Optimizing illumination in three-dimensional deconvolution microscopy for accurate refractive index tomography</article-title><source>Opt. Express</source><year>2021</year><volume>29</volume><fpage>6293</fpage><lpage>6301</lpage><pub-id pub-id-type="doi">10.1364/OE.412510</pub-id><pub-id pub-id-type="pmid">33726154</pub-id>
</element-citation><mixed-citation id="mc-CR31" publication-type="journal">Hugonnet, H., Lee, M. &#x00026; Park, Y. Optimizing illumination in three-dimensional deconvolution microscopy for accurate refractive index tomography. <italic>Opt. Express</italic><bold>29</bold>, 6293&#x02013;6301 (2021).<pub-id pub-id-type="pmid">33726154</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>M</given-names></name><etal/></person-group><article-title>All-in-focus fine needle aspiration biopsy imaging based on Fourier ptychographic microscopy</article-title><source>J. Pathol. Inform.</source><year>2022</year><volume>13</volume><fpage>100119</fpage><pub-id pub-id-type="doi">10.1016/j.jpi.2022.100119</pub-id><pub-id pub-id-type="pmid">36268073</pub-id>
</element-citation><mixed-citation id="mc-CR32" publication-type="journal">Liang, M. et al. All-in-focus fine needle aspiration biopsy imaging based on Fourier ptychographic microscopy. <italic>J. Pathol. Inform.</italic><bold>13</bold>, 100119 (2022).<pub-id pub-id-type="pmid">36268073</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Jaderberg, M., Simonyan, K. &#x00026; Zisserman, A. Spatial Transformer Networks. <italic>Advances In Neural Information Processing Systems</italic><bold>28</bold>, Preprint at 10.48550/arXiv.1506.02025 (2015).</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Kim, S. et al. in <italic>Medical Image Computing and Computer Assisted Intervention&#x02013;MICCAI 2019: 22nd International Conference</italic>, <italic>Shenzhen, China, Proceedings, Part III</italic><bold>22</bold>, 220&#x02013;228 (Springer, 2019).</mixed-citation></ref><ref id="CR35"><label>35.</label><citation-alternatives><element-citation id="ec-CR35" publication-type="journal"><person-group person-group-type="author"><name><surname>Preibisch</surname><given-names>S</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Tomancak</surname><given-names>P</given-names></name></person-group><article-title>Globally optimal stitching of tiled 3D microscopic image acquisitions</article-title><source>Bioinformatics</source><year>2009</year><volume>25</volume><fpage>1463</fpage><lpage>1465</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btp184</pub-id><pub-id pub-id-type="pmid">19346324</pub-id>
</element-citation><mixed-citation id="mc-CR35" publication-type="journal">Preibisch, S., Saalfeld, S. &#x00026; Tomancak, P. Globally optimal stitching of tiled 3D microscopic image acquisitions. <italic>Bioinformatics</italic><bold>25</bold>, 1463&#x02013;1465 (2009).<pub-id pub-id-type="pmid">19346324</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name><surname>Reinke</surname><given-names>A</given-names></name><etal/></person-group><article-title>Understanding metric-related pitfalls in image analysis validation</article-title><source>Nat. Methods</source><year>2024</year><volume>21</volume><fpage>182</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1038/s41592-023-02150-0</pub-id><pub-id pub-id-type="pmid">38347140</pub-id>
</element-citation><mixed-citation id="mc-CR36" publication-type="journal">Reinke, A. et al. Understanding metric-related pitfalls in image analysis validation. <italic>Nat. Methods</italic><bold>21</bold>, 182&#x02013;194 (2024).<pub-id pub-id-type="pmid">38347140</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Nilsson, J. &#x00026; Akenine-M&#x000f6;ller, T. Understanding ssim. <italic>arXiv</italic> Preprint 10.48550/arXiv.2006.13846 (2020).</mixed-citation></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>S</given-names></name><etal/></person-group><article-title>Hover-net: simultaneous segmentation and classification of nuclei in multi-tissue histology images</article-title><source>Med. image Anal.</source><year>2019</year><volume>58</volume><fpage>101563</fpage><pub-id pub-id-type="doi">10.1016/j.media.2019.101563</pub-id><pub-id pub-id-type="pmid">31561183</pub-id>
</element-citation><mixed-citation id="mc-CR38" publication-type="journal">Graham, S. et al. Hover-net: simultaneous segmentation and classification of nuclei in multi-tissue histology images. <italic>Med. image Anal.</italic><bold>58</bold>, 101563 (2019).<pub-id pub-id-type="pmid">31561183</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &#x00026; Brox, T. in <italic>Medical Image Computing And Computer-assisted Intervention&#x02013;miccai 2015: 18th International Conference, Munich, Germany, Proceedings, Part III</italic><bold>18</bold>, 234&#x02013;241 (Springer, 2015).</mixed-citation></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Saharia</surname><given-names>C</given-names></name><etal/></person-group><article-title>Image Super-Resolution via Iterative Refinement</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>45</volume><fpage>4713</fpage><lpage>4726</lpage><pub-id pub-id-type="pmid">36094974</pub-id>
</element-citation><mixed-citation id="mc-CR40" publication-type="journal">Saharia, C. et al. Image Super-Resolution via Iterative Refinement. <italic>IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>45</bold>, 4713&#x02013;4726 (2023).<pub-id pub-id-type="pmid">36094974</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Isola, P., Zhu, J.-Y., Zhou, T. &#x00026; Efros, A. A. Image-to-image translation with conditional adversarial networks. <italic>Proceedings of the IEEE Conference On Computer Vision And Pattern Recognition</italic>. 1125&#x02013;1134, 10.1109/CVPR.2017.632.</mixed-citation></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>M</given-names></name><name><surname>Hugonnet</surname><given-names>H</given-names></name><name><surname>Park</surname><given-names>Y</given-names></name></person-group><article-title>Inverse problem solver for multiple light scattering using modified Born series</article-title><source>Optica</source><year>2022</year><volume>9</volume><fpage>177</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1364/OPTICA.446511</pub-id></element-citation><mixed-citation id="mc-CR42" publication-type="journal">Lee, M., Hugonnet, H. &#x00026; Park, Y. Inverse problem solver for multiple light scattering using modified Born series. <italic>Optica</italic><bold>9</bold>, 177&#x02013;182 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Phillips</surname><given-names>ZF</given-names></name><name><surname>Waller</surname><given-names>L</given-names></name></person-group><article-title>Quantitative differential phase contrast (DPC) microscopy with computational aberration correction</article-title><source>Opt. Express</source><year>2018</year><volume>26</volume><fpage>32888</fpage><lpage>32899</lpage><pub-id pub-id-type="doi">10.1364/OE.26.032888</pub-id><pub-id pub-id-type="pmid">30645449</pub-id>
</element-citation><mixed-citation id="mc-CR43" publication-type="journal">Chen, M., Phillips, Z. F. &#x00026; Waller, L. Quantitative differential phase contrast (DPC) microscopy with computational aberration correction. <italic>Opt. Express</italic><bold>26</bold>, 32888&#x02013;32899 (2018).<pub-id pub-id-type="pmid">30645449</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>C</given-names></name><etal/></person-group><article-title>Single-shot refractive index slice imaging using spectrally multiplexed optical transfer function reshaping</article-title><source>Opt. Express</source><year>2023</year><volume>31</volume><fpage>13806</fpage><lpage>13816</lpage><pub-id pub-id-type="doi">10.1364/OE.485559</pub-id><pub-id pub-id-type="pmid">37157259</pub-id>
</element-citation><mixed-citation id="mc-CR44" publication-type="journal">Lee, C. et al. Single-shot refractive index slice imaging using spectrally multiplexed optical transfer function reshaping. <italic>Opt. Express</italic><bold>31</bold>, 13806&#x02013;13816 (2023).<pub-id pub-id-type="pmid">37157259</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Hugonnet</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>MJ</given-names></name><name><surname>Park</surname><given-names>YK</given-names></name></person-group><article-title>Quantitative phase and refractive index imaging of 3D objects via optical transfer function reshaping</article-title><source>Opt. Express</source><year>2022</year><volume>30</volume><fpage>13802</fpage><lpage>13809</lpage><pub-id pub-id-type="doi">10.1364/OE.454533</pub-id><pub-id pub-id-type="pmid">35472985</pub-id>
</element-citation><mixed-citation id="mc-CR45" publication-type="journal">Hugonnet, H., Lee, M. J. &#x00026; Park, Y. K. Quantitative phase and refractive index imaging of 3D objects via optical transfer function reshaping. <italic>Opt. Express</italic><bold>30</bold>, 13802&#x02013;13809 (2022).<pub-id pub-id-type="pmid">35472985</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Ruan</surname><given-names>X</given-names></name><etal/></person-group><article-title>Image processing tools for petabyte-scale light sheet microscopy data</article-title><source>Nat. Methods</source><year>2024</year><volume>21</volume><fpage>2342</fpage><lpage>2352</lpage><pub-id pub-id-type="doi">10.1038/s41592-024-02475-4</pub-id><pub-id pub-id-type="pmid">39420143</pub-id>
</element-citation><mixed-citation id="mc-CR46" publication-type="journal">Ruan, X. et al. Image processing tools for petabyte-scale light sheet microscopy data. <italic>Nat. Methods</italic><bold>21</bold>, 2342&#x02013;2352 (2024).<pub-id pub-id-type="pmid">39420143</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Thorsson</surname><given-names>V</given-names></name><etal/></person-group><article-title>The immune landscape of cancer</article-title><source>Immunity</source><year>2018</year><volume>48</volume><fpage>812</fpage><lpage>830</lpage><pub-id pub-id-type="doi">10.1016/j.immuni.2018.03.023</pub-id><pub-id pub-id-type="pmid">29628290</pub-id>
</element-citation><mixed-citation id="mc-CR47" publication-type="journal">Thorsson, V. et al. The immune landscape of cancer. <italic>Immunity</italic><bold>48</bold>, 812&#x02013;830 (2018).<pub-id pub-id-type="pmid">29628290</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="journal"><person-group person-group-type="author"><name><surname>Bagaev</surname><given-names>A</given-names></name><etal/></person-group><article-title>Conserved pan-cancer microenvironment subtypes predict response to immunotherapy</article-title><source>Cancer cell</source><year>2021</year><volume>39</volume><fpage>845</fpage><lpage>865</lpage><pub-id pub-id-type="doi">10.1016/j.ccell.2021.04.014</pub-id><pub-id pub-id-type="pmid">34019806</pub-id>
</element-citation><mixed-citation id="mc-CR48" publication-type="journal">Bagaev, A. et al. Conserved pan-cancer microenvironment subtypes predict response to immunotherapy. <italic>Cancer cell</italic><bold>39</bold>, 845&#x02013;865 (2021).<pub-id pub-id-type="pmid">34019806</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR49"><label>49.</label><citation-alternatives><element-citation id="ec-CR49" publication-type="journal"><person-group person-group-type="author"><name><surname>Ghahremani</surname><given-names>P</given-names></name><etal/></person-group><article-title>Deep learning-inferred multiplex immunofluorescence for immunohistochemical image quantification</article-title><source>Nat. Mach. Intell.</source><year>2022</year><volume>4</volume><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1038/s42256-022-00471-x</pub-id><pub-id pub-id-type="pmid">36118303</pub-id>
</element-citation><mixed-citation id="mc-CR49" publication-type="journal">Ghahremani, P. et al. Deep learning-inferred multiplex immunofluorescence for immunohistochemical image quantification. <italic>Nat. Mach. Intell.</italic><bold>4</bold>, 401&#x02013;412 (2022).<pub-id pub-id-type="pmid">36118303</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>J-R</given-names></name><name><surname>Fallahi-Sichani</surname><given-names>M</given-names></name><name><surname>Sorger</surname><given-names>PK</given-names></name></person-group><article-title>Highly multiplexed imaging of single cells using a high-throughput cyclic immunofluorescence method</article-title><source>Nat. Commun.</source><year>2015</year><volume>6</volume><fpage>8390</fpage><pub-id pub-id-type="doi">10.1038/ncomms9390</pub-id><pub-id pub-id-type="pmid">26399630</pub-id>
</element-citation><mixed-citation id="mc-CR50" publication-type="journal">Lin, J.-R., Fallahi-Sichani, M. &#x00026; Sorger, P. K. Highly multiplexed imaging of single cells using a high-throughput cyclic immunofluorescence method. <italic>Nat. Commun.</italic><bold>6</bold>, 8390 (2015).<pub-id pub-id-type="pmid">26399630</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Tan, M. &#x00026; Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. <italic>International conference on machine learning</italic>. 6105&#x02013;6114, 10.48550/arXiv.1905.11946 (PMLR).</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Hu, J., Shen, L. &#x00026; Sun, G. Squeeze-and-excitation networks. <italic>Proceedings of the IEEE Conference On Computer Vision And Pattern Recognition</italic>. 7132&#x02013;7141, 10.1109/CVPR.2018.00745.</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Kingma, D. P. &#x00026; Ba, J. Adam: a method for stochastic optimization. <italic>arXiv</italic> Preprint 10.48550/arXiv.1412.6980 (2014).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Paszke, A. et al. Pytorch: an imperative style, high-performance deep learning library. <italic>Advances in neural information processing systems</italic><bold>32</bold>, Preprint at 10.48550/arXiv.1912.01703 (2019).</mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Juyeon P. et al. Revealing 3D microanatomical structures of unlabeled thick cancer tissues using holotomography and virtual H&#x00026;E staining. <italic>Zenodo</italic>, 10.5281/zenodo.15030226 (2025).</mixed-citation></ref></ref-list></back></article>