<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39890838</article-id><article-id pub-id-type="pmc">PMC11785737</article-id><article-id pub-id-type="publisher-id">87688</article-id><article-id pub-id-type="doi">10.1038/s41598-025-87688-z</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Feasibility study of emotion mimicry analysis in human&#x02013;machine interaction</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Arabian</surname><given-names>Herag</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Alshirbaji</surname><given-names>Tamer Abdulbaki</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Bhave</surname><given-names>Ashish</given-names></name><address><email>ab1533@students.uni-freiburg.de</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Wagner-Hartl</surname><given-names>Verena</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Igel</surname><given-names>Marcel</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><name><surname>Chase</surname><given-names>J. Geoffrey</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Moeller</surname><given-names>Knut</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02m11x738</institution-id><institution-id institution-id-type="GRID">grid.21051.37</institution-id><institution-id institution-id-type="ISNI">0000 0001 0601 6589</institution-id><institution>Institute of Technical Medicine (ITeM), </institution><institution>Furtwangen University, </institution></institution-wrap>78054 Villingen-Schwenningen, Germany </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03s7gtk40</institution-id><institution-id institution-id-type="GRID">grid.9647.c</institution-id><institution-id institution-id-type="ISNI">0000 0004 7669 9786</institution-id><institution>Innovation Center Computer Assisted Surgery (ICCAS), </institution><institution>University of Leipzig, </institution></institution-wrap>04103 Leipzig, Germany </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0245cg223</institution-id><institution-id institution-id-type="GRID">grid.5963.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 0491 7203</institution-id><institution>Department of Microsystems Engineering (IMTEK), Faculty of Engineering, </institution><institution>University of Freiburg, </institution></institution-wrap>Georges-Koehler-Allee 101, 79110 Freiburg, Germany </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02m11x738</institution-id><institution-id institution-id-type="GRID">grid.21051.37</institution-id><institution-id institution-id-type="ISNI">0000 0001 0601 6589</institution-id><institution>Department of Industrial Technologies, </institution><institution>Campus Tuttlingen Furtwangen University, </institution></institution-wrap>78532 Tuttlingen, Germany </aff><aff id="Aff5"><label>5</label>Imsimity GmbH, 78112 St. Georgen im Schwarzwald, Germany </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03y7q9t39</institution-id><institution-id institution-id-type="GRID">grid.21006.35</institution-id><institution-id institution-id-type="ISNI">0000 0001 2179 4063</institution-id><institution>Department of Mechanical Engineering, </institution><institution>University of Canterbury, </institution></institution-wrap>Christchurch, 8041 New Zealand </aff></contrib-group><pub-date pub-type="epub"><day>31</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>31</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><elocation-id>3859</elocation-id><history><date date-type="received"><day>29</day><month>7</month><year>2024</year></date><date date-type="accepted"><day>21</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Health apps have increased in popularity as people increasingly follow the advice these apps provide to enhance physical and mental well-being. One key aspect of improving neurosensory health is identifying and expressing emotions. Emotional intelligence is crucial for maintaining and enhancing social interactions. In this context, a preliminary closed-loop feedback system has been developed to help people project specific emotions by altering their facial expressions. This system is part of a research intervention aimed at therapeutic applications for individuals with autism spectrum disorder. The proposed system functions as a digital mirror, initially displaying an animated avatar&#x02019;s face expressing a predefined emotion. Users are then asked to mimic the avatar&#x02019;s expression. During this process, a custom emotion recognition model analyzes the user&#x02019;s facial expressions and provides feedback on the accuracy of their projection. A small experimental study involving 8 participants tested the system for feasibility, with avatars projecting the six basic emotions and a neutral expression. The study results indicated a positive correlation between the projected facial expressions and the emotions identified by participants. Participants effectively recognized the emotions, with 85.40% accuracy demonstrating the system&#x02019;s potential in enhancing the well-being of individuals. The participants were also able to mimic the given expression effectively with an accuracy of 46.67%. However, a deficiency in the performance of one of the expressions, surprise, was noticed. In the post processing, this issue was addressed and model enhancements were tailored to boost the performance by ~&#x02009;30%. This approach shows promise for therapeutic use and emotional skill development. A further wider experimental study is still required to validate the findings of this study and analyze the impact of modifications made.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Human behaviour</kwd><kwd>Patient education</kwd><kwd>Quality of life</kwd><kwd>Therapeutics</kwd><kwd>Software</kwd></kwd-group><funding-group><award-group><funding-source><institution>Albert-Ludwigs-Universit&#x000e4;t Freiburg im Breisgau (1016)</institution></funding-source></award-group><open-access><p>Open Access funding enabled and organized by Projekt DEAL.</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">In recent years, digital health applications have rapidly expanded, offering significant contributions to physical and mental wellness while encouraging a healthier lifestyle<sup><xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR4">4</xref></sup>. As healthcare systems face increasing strain<sup><xref ref-type="bibr" rid="CR5">5</xref>&#x02013;<xref ref-type="bibr" rid="CR8">8</xref></sup>, these applications, along with digital therapies, present a promising solution for both patients and healthcare providers by facilitating digital patient-led care<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>. These apps enable a more personalized, patient-centric approach by collecting vital information over extended periods, which can lead to more accurate diagnoses and treatments than the brief interactions typically experienced during medical appointments. Intelligent learning systems also offer the potential to further improve the quality, efficiency, and accessibility of patient care<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>.</p><p id="Par3">To support therapeutic interventions for individuals with autism spectrum disorder (ASD) novel digital emotion recognition tools are required. ASD is a neurodevelopmental disorder that affects an estimated 1&#x02013;2% of the population, impacting social skills, communication, behavior, and interests<sup><xref ref-type="bibr" rid="CR12">12</xref>&#x02013;<xref ref-type="bibr" rid="CR15">15</xref></sup>. It can lead to significant health issues, such as depression and anxiety, due to social isolation and underemployment<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. Individuals with ASD often face significant challenges in recognizing and interpreting emotional expressions, both in themselves and others. People with ASD may struggle to identify facial expressions, tone of voice, and body language, which are crucial for understanding emotions in social contexts. As a result, they might have trouble responding appropriately to emotional signals, affecting their ability to engage in empathetic or reciprocal social exchanges. These challenges can contribute to social isolation, anxiety, and difficulties in forming relationships<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. To address these challenges, tailored therapies for various ASD levels are designed, such as single or group sessions.</p><p id="Par4">This study aims to develop and validate a real-time facial emotion recognition model using a geometric-based approach, focusing on facial landmarks for effective emotion classification in therapeutic applications, particularly for individuals with ASD. The new tool being developed involves a closed-loop feedback system, which will immerse users in gamified scenarios that are designed to elicit emotional responses and help them practice social interactions in a virtual environment. By leveraging real-time feedback, the system not only fosters emotional recognition and mimicry but also provides clinicians with essential data to monitor progress and tailor therapy to individual needs. Such digital interventions hold promise for helping individuals with ASD better navigate and master social situations<sup><xref ref-type="bibr" rid="CR17">17</xref>&#x02013;<xref ref-type="bibr" rid="CR19">19</xref></sup>.</p><p id="Par5">The definition of emotion is often widely contested<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. For this research, emotion is viewed as the component process model, where emotions are sequences of events triggered by internal or external stimuli<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. The understanding of emotional stimuli, particularly their role and influence in clinical contexts, has evolved over time. Emotional stimuli identified as threats significantly impact attention, processing priorities, and Pavlovian responses<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. Recent studies suggest reactions to these stimuli are context-dependent and aligned with personal goals, illustrating the complexity and difficulty in understanding emotional responses<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>.</p><p id="Par6">Emotional intelligence (EI) has gained recognition as both a theoretical construct and a critical factor in human functioning. It refers to the ability to recognize, understand, and manage emotions in oneself and others, as well as using emotions to guide thinking, behavior, and decision-making. The concept of EI has been explored in various frameworks, with some models focusing on mental abilities and others emphasizing personality traits. Research has shown that EI is a significant predictor of success in various domains, including personal well-being, mental health, and social functioning. Studies have demonstrated that individuals with higher EI tend to exhibit better emotion regulation, higher social engagement, and enhanced coping mechanisms in the face of stress<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. While the emotional intelligence literature is extensive, there remains a need for deeper understanding of how specific emotional regulation strategies contribute to overall emotional and social well-being, particularly in real-world settings<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. In the study of emotion and attention, tools like the International Affective Picture System (IAPS) provide standardized emotional stimuli, allowing researchers to explore how emotional cues influence cognitive processes, thus contributing to our understanding of emotional responses in various contexts<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>.</p><p id="Par7">Identifying emotions is challenging due to the diverse ways individuals express them<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. However, emotions are often communicated through facial expressions (55%), speech and voice patterns (35%), and physiological signals (10%)<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. Previous research<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup> has also shed light on the importance of whole-body expressions in understanding emotions, especially when facial expressions are not visible. The growing interest in non-verbal emotional cues underscores the importance of emotion recognition in enhancing human-machine interactions<sup><xref ref-type="bibr" rid="CR33">33</xref>&#x02013;<xref ref-type="bibr" rid="CR38">38</xref></sup>. Facial expressions are often analyzed using two main methods: image-based and geometric-based approaches. This study employs a geometric-based approach, extracting facial landmarks and implementing a simple pattern recognition model to classify emotional states. Using facial landmarks offers a comprehensive approach to emotion recognition. Prior research demonstrated the effectiveness of incorporating facial landmark locations into classification processes, achieving promising results using a unique loss function with a diverse range of emotion datasets<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. In another study, facial landmarks from a Kinect 3D device were used to identify action units (AUs), achieving 96% accuracy on the Karolinska directed emotional faces (KDEF) dataset<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. Combining facial landmarks and physiological signals yielded a high 86.94% accuracy in classifying emotions<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. Selecting key facial landmarks for geometric analysis of facial gestures also yielded strong performance on multiple datasets, with machine learning models achieving up to 97% accuracy on the extended Cohn-Kanade (CK+) dataset using a k-nearest neighbor (KNN) classifier with real-time processing<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>.</p><p id="Par8">A facial expression mimicry experiment is valuable as it enhances the understanding of emotional cues and fosters empathy<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Emotional mimicry is not just about replicating muscle movements, but rather interpreting and responding to emotions, which facilitates social bonding and improves interactions. Research in<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> emphasizes that mimicry is more likely when there is an intention to affiliate, highlighting its importance in establishing social connections. Additionally, Majumder et al.<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> explores a novel approach for generating empathetic responses in dialogue systems by mimicking user emotions. The study uses a transformer encoder to classify emotions and generate responses that balance empathy and appropriateness, focusing on positive and negative emotional clusters. An experiment was carried out in<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, where the aim was to recognize the six basic emotional states of joy, sadness, surprise, anger, fear, and disgust, and an additional neutral expression based on facial expressions using a three-dimensional face model. The study used a Microsoft Kinect to create 3D models of participants&#x02019; faces and calculated features based on six AUs derived from the facial action coding system (FACS)<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. Participants in this study were instructed to mimic emotional expressions displayed on a screen while the Kinect recorded their facial movements. The features extracted were classified using a k-nearest neighbors (k-NN) and multilayer perceptron (MLP) neural network classifiers. The experiment involved six male subjects, and each participated in two sessions, resulting in a total of 252 facial expression recordings for analysis<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. These studies demonstrate the importance of emotional mimicry in both social and technological contexts. While they highlight the benefits of emotional recognition and mimicry, they do not fully address the challenges faced by individuals with conditions like ASD, where emotion recognition and expression can be particularly difficult.</p><p id="Par9">To develop an efficient emotional feedback system, a robust model for expression recognition is essential. This study builds on these insights by developing a system that uses facial expression mimicry to improve emotional recognition, aiming to create more effective therapeutic interventions and emotion-aware AI systems. The proposed model is based on a pattern recognition framework and employs a geometric-based approach, utilizing facial landmarks extracted in real time for emotion classification. Unlike previous approaches that primarily focus on image-based or action unit-based methods, this approach is computationally simpler, allowing for faster processing and making it suitable for real-time applications. Additionally, the use of facial landmarks differentiates this model from other systems that rely on more complex 3D modeling or multimodal data.</p><p id="Par10">To achieve these objectives, a facial emotion recognition model was developed, trained, and validated on a combination of the OULU-CASIA<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> and FACES<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> emotion databases. The integration of these two distinct databases introduces variation in external influences such as lighting conditions, camera angles, and subject demographics (e.g., age, gender, ethnicity), which provides a more robust feature space analysis for emotion classification.</p><p id="Par11">In this study, an experiment was designed to evaluate facial expression recognition in real time. The developed model was used in a preliminary, proof-of-concept feasibility trial involving neurotypical participants. During the trial, participants engaged in emotion recognition and mimicry tasks, attempting to replicate avatar-displayed emotions under controlled conditions. The system&#x02019;s performance was assessed based on classification accuracy and participant feedback, with a focus on identifying challenges and areas for refinement. Full experimental details, including data collection and analysis, are outlined in the &#x0201c;<xref rid="Sec9" ref-type="sec">Methods</xref>&#x0201d; section.</p><p id="Par12">The experimental results demonstrate that the model is computationally efficient and aligns well with existing theories on emotion recognition. This supports the feasibility of the model for practical, real-world use and provides a simpler alternative to more complex 3D and multimodal methods. The proof-of-concept trial helped identify key issues in complete system development and highlighted areas for improvement. It also collected data for further refinement of the recognition model. Overall, this work demonstrates the feasibility of such a system in real-time settings, laying the groundwork for further development and optimization in future studies.</p><p id="Par13">The paper is divided into the following sections, the &#x0201c;<xref rid="Sec2" ref-type="sec">Results</xref>&#x0201d; section provides the results from the recognition model, the experiment and post analysis of the recognition model. The &#x0201c;<xref rid="Sec6" ref-type="sec">Discussions</xref>&#x0201d; section follows with the findings from the respective results. The &#x0201c;<xref rid="Sec9" ref-type="sec">Methods</xref>&#x0201d; section then follows with the description of the methods used for the model building, feature extraction, experimental setup and analysis criteria.</p></sec><sec id="Sec2"><title>Results</title><sec id="Sec3"><title>Emotion recognition model</title><p id="Par15">The emotion recognition model was trained to provide a two-class output of positive and negative as described in the &#x0201c;<xref rid="Sec9" ref-type="sec">Methods</xref>&#x0201d; section. The validation set consisted of 219 images with the training set having 1971 images. The model yielded a true positive (TP) accuracy of 93.61%, precision of 94.96%, recall of 93.61% and an F1-score of 93.84% on the validation set. When compared to the entire dataset, i.e., training and validation set combined, the model achieved a TP accuracy of 94.98%, precision of 95.60%, recall of 94.98% and F1-score of 95.11%. In Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> the confusion matrices are depicted for both the validation set (left a) and the entire dataset (right b).</p><p id="Par16">
<fig id="Fig1"><label>Fig. 1</label><caption><p>Confusion matrices of the validation set (left <bold>a</bold>), and the entire dataset (right <bold>b</bold>).</p></caption><graphic xlink:href="41598_2025_87688_Fig1_HTML" id="d33e466"/></fig>
</p></sec><sec id="Sec4"><title>Emotion assessment experiment</title><p id="Par19">The results from the emotional assessment showed a positive identification of the facial expressions with a TP accuracy of 85.40%. In Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> the confusion matrix between the presented avatar expressions and the participants responses can be seen. The number of observations varies as the participants were shown random sequences of the expressions minus one (i.e., six out of the seven expressions).</p><p id="Par17">
<fig id="Fig2"><label>Fig. 2</label><caption><p>Confusion matrix of the pre-defined facial expression and the participants response. Blue indicates a positive identification; pinkish orange represents a misclassification with intensity of the hue varying with respect to the number of identifications.</p></caption><graphic xlink:href="41598_2025_87688_Fig2_HTML" id="d33e483"/></fig>
</p></sec><sec id="Sec5"><title>Emotion mimicry experiment</title><p id="Par22">In the emotion mimicry experiment, a first look was performed into the number of times the participant was asked to try to mimic again due to failure to reach the threshold. The ratio of class repetitions with respect to the max number of attempts showed a median of 37.50%, with an inter quartile range (IQR) of 20.83% from 26.04% (25th percentile) to 46.87% (75th percentile). Although the recognition model is set to classify positive and negative, the evaluation was performed on the six&#x02009;+&#x02009;neutral emotion class basis. The surprise class revealed the highest number of repeated attempts at 66.67% with the sad class having the least at 0%. Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> shows the distribution and number of times an expression was given the feedback &#x0201c;Try Again!&#x0201d;.</p><p id="Par23">
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Results of &#x0201c;try again&#x0201d; feedback given to the participants.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Class name</th><th align="left">&#x0201c;Try again&#x0201d; count</th><th align="left">% Max attempts</th></tr></thead><tbody><tr><td align="left">Anger</td><td char="." align="char">9</td><td char="." align="char">37.50</td></tr><tr><td align="left">Disgust</td><td char="." align="char">9</td><td char="." align="char">37.50</td></tr><tr><td align="left">Fear</td><td char="." align="char">6</td><td char="." align="char">25.00</td></tr><tr><td align="left">Happiness</td><td char="." align="char">7</td><td char="." align="char">29.17</td></tr><tr><td align="left">Neutral</td><td char="." align="char">12</td><td char="." align="char">50.00</td></tr><tr><td align="left">Sadness</td><td char="." align="char">0</td><td char="." align="char">0.00</td></tr><tr><td align="left">Surprise</td><td char="." align="char">16</td><td char="." align="char">66.67</td></tr></tbody></table></table-wrap>
</p><p id="Par24">Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> shows the classification results of the two-class system for the different avatar facial expressions. As seen in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, the surprise class had the highest number in absolute frequency, due to the number of repeat attempts after false classification. The sad class the lowest. Thus, the sad class showed the best ratio of correct classification versus false classification, while the surprise class showed the worst. The large number of instances is related to the frequency in prediction over the period of 5&#x000a0;s, where, in this experiment, the recognition model was operating at an average rate of around 3.2&#x000a0;Hz. The results also show the model achieved a TP accuracy of 46.67% during the experiment.</p><p id="Par20">
<fig id="Fig3"><label>Fig. 3</label><caption><p>Prediction results from the two-class emotion recognition model visualized for each avatar facial expression. The bars in red indicate a false classification while that in blue represent correct classifications.</p></caption><graphic xlink:href="41598_2025_87688_Fig3_HTML" id="d33e578"/></fig>
</p></sec><sec id="Sec27"><title>Feature post experiment result</title><p id="Par29">After analyzing the results, a post analysis on the feature space was conducted. An additional distance feature (between landmark points 18 and 106) was added to improve the performance of the recognition model, which was re-trained. In Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> the results show the features, after performing a principal component analysis (PCA), of two of the principal components for the six classes (left a), while on the right (b) are the same principal components for the two-class system of positive and negative using the new 16 feature space.</p><p id="Par21">
<fig id="Fig4"><label>Fig. 4</label><caption><p>Principal component analysis results from post experiment feature alterations, showing the first two principal components for both the six-class (left <bold>a</bold>), and two-class system (right <bold>b</bold>).</p></caption><graphic xlink:href="41598_2025_87688_Fig4_HTML" id="d33e602"/></fig>
</p></sec></sec><sec id="Sec6"><title>Discussion</title><p id="Par31">The primary goal of this study was to develop and validate a real-time facial emotion recognition model using a geometric-based approach, focusing on facial landmarks for effective emotion classification in therapeutic applications. By leveraging this model, the study aims to provide a simpler, more computationally efficient alternative to complex 3D and multimodal methods, facilitating real-time emotional feedback for individuals with ASD and supporting tailored therapeutic interventions.</p><p id="Par32">The feature reduction process adopted for the emotion recognition model proved to be effective in retaining the essential geometric features necessary for accurate emotion classification. By focusing on the rate of change in distances and angles derived from facial landmarks, a simplified model was obtained without having to compromise accuracy. As illustrated in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, the model successfully classified emotion data from the combined OULU-CASIA and FACES datasets. The integration of these two diverse datasets ensured a robust representation and enhanced the model&#x02019;s capability to capture a wide range of emotional patterns. This approach not only validates the model&#x02019;s performance across different datasets, but also demonstrates its potential applicability in varied real-world scenarios. Successful implementation of feature reduction underscores the importance of selecting pertinent features which contribute significantly to model accuracy, while maintaining computational efficiency.</p><p id="Par33">Results from the emotion assessment experiment illustrated in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> underscore participants&#x02019; ability to relate to the virtual characters (avatars) and accurately distinguish between different virtual emotional expressions. However, an interesting pattern emerged in which participants exhibited confusion when navigating various negative emotions (anger, disgust, fear, and sadness). This difficulty is evident in the confusion matrix in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, where most errors occurred within the negative classes, such as mistaking anger for disgust or fear for disgust, and even occasionally identifying sadness as neutral. Similarly, the recognition model struggled to differentiate some of these classes from the datasets.</p><p id="Par34">The observed accuracy of 46.67% in mimicking expressions, achieved by the participants, provides an initial reference point for evaluating performance, but its significance must be interpreted within the broader context of existing research on emotional expression and recognition. Prior studies<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> have highlighted that individuals with ASD often exhibit atypical emotional expressions, which are challenging for both neurotypical (NT) and ASD individuals to recognize. This raises questions about how NT participants&#x02019; performance in such tasks compares to that of individuals with ASD, particularly given the differences in emotional representation and expression across these groups. These findings emphasize the need for further studies to compare NT and ASD performance in similar tasks, establishing clearer benchmarks and evaluating how well systems, such as the one proposed, can address these specific challenges.</p><p id="Par35">Examining the feature space map of the principal components in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>a reveals a clear boundary between the negative emotions of anger, fear, disgust, and sadness is notably challenging. This difficulty is mirrored in the participants&#x02019; responses during the emotion assessment, highlighting a broader issue in accurately identifying these negative expressions. Consequently, these findings suggest both human perception and machine learning models face significant hurdles in distinguishing between similar negative emotions, thus requiring more input such as voice or whole-body position<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>.</p><p id="Par36">Understanding the social context and intention behind mimicry helps in deciphering the subtle ways in which people connect emotionally. The significance of this work lies in its potential to enhance human-computer interactions by making them more emotionally intelligent and contextually aware. In the six-class avatar expression identification assessment trial, where the emotion to be identified is randomized for each user, collecting data for only five out of the six classes (i.e., number of classes minus one) is a strategic approach to ensure balanced, unbiased, and high-quality data collection. By rotating and ensuring each user identifies only five out of the six expressions, potential biases can be controlled to ensure more balance in the collected data. This approach can also mitigate order effects, such as fatigue or learning biases, which may distort the data, as participant performance varies with respect to time. Thus, by limiting the number of classes each participant interacts with, the risk of these order effects influencing the results is minimized. This approach thus also reduces the cognitive load on participants and can lead to higher quality data as users are less likely to experience fatigue.</p><p id="Par37">The emotion mimicry experiment exposed significant weaknesses in the model&#x02019;s ability to classify new data from participants. As depicted in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, there was a higher rate of misclassifications compared to correct classifications across the various facial expressions. This deficiency is largely attributed to the high rate of emotion expression repetitions, indicating the model&#x02019;s struggle with robust recognition. While certain expressions, particularly negative ones, are inherently more challenging for participants to replicate accurately, the primary cause for repetition was the model&#x02019;s limited ability to accurately identify these different classes.</p><p id="Par38">Furthermore, the classification results in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> highlight distinct performance patterns of the model. The surprise class exhibited the highest frequency of occurrences and subsequently the worst ratio of correct to false classifications, underscoring the complexity of accurately recognizing this emotion from the given features. In contrast, the sadness class showed the best performance, with the highest ratio of correct classifications. Despite this set of results, the overall moderate accuracy of the model indicates significant room for improvement, especially in recognizing more complex or subtle expressions. These findings emphasize the need to refine the emotion recognition algorithms and adjust threshold settings to enhance overall performance, reducing the necessity for repeated mimicry attempts. Such improvements are important for achieving a more seamless and intuitive user experience.</p><p id="Par39">The addition of the distance feature between landmark points 18 and 106 allowed for more distinct classification of the surprise emotion, as evidenced by the improved separation observed in the principal component analysis (PCA) plot (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>), which is often subtle and can be easily confused with other emotions. The inclusion of this feature resulted in a clearer differentiation of surprise from other emotional states, which was not as evident in the initial model. Additionally, the significant improvement in accuracy on the collected experimental data (~&#x02009;30%) demonstrates the model&#x02019;s increased sensitivity to the real-time facial expressions, especially when the input feature space is enhanced. The model&#x02019;s performance increase highlights the importance of refining feature selection to improve real-time emotion recognition, confirming that the adjustments made have positively impacted the system&#x02019;s robustness and reliability.</p><sec id="Sec7"><title>Limitations</title><p id="Par40">With any experiment and study there are limitations. A notable limitation of this study is the relatively low frequency of recognition, which may have impacted the real-time performance of the system. This constraint potentially hindered the ability to accurately capture and classify rapid changes in participants&#x02019; facial expressions. Communication difficulties arose from the use of different programming languages for various components of the system, leading to integration challenges and increased latency. These technical issues not only affected seamless operation of the experiment, but also may have introduced errors and inconsistencies in the data collection and analysis processes. Another limitation of this study is the relatively small sample size, which may have an impact on the generalizability of the findings but was effective in demonstrating the potential of the approach without significant burden on subjects and volunteer numbers. However, the limited sample size can signify that the variability and diversity in facial expressions across different demographics and individual differences may not have been fully captured. This study was conducted with neurotypical participants to establish a baseline performance for the model. This approach provided a controlled setting to validate the system&#x02019;s stability and effectiveness before introducing additional variables. However, this limits the immediate applicability of the findings to individuals with autism, whose unique emotional expressions and recognition patterns may require specific adjustments.</p><p id="Par41">As the primary aim of this study is to establish the fundamental viability of the system for real-time emotion recognition, particularly focusing on the accuracy of classifications under controlled conditions, only the strongest representations were used. This approach provides a first assessment of model performance before adding added complexity or subtlety. However, identifying weaker, more ambiguous emotional expressions is crucial for real-world applications, particularly to benefit individuals with ASD who may face challenges with subtle emotional cues. Thus, incorporating weaker representations is a necessary next step given the results from this pilot study.</p><p id="Par42">Addressing these limitations in future studies will be crucial for enhancing the model&#x02019;s performance and ensuring more reliable and efficient emotion recognition.</p></sec><sec id="Sec8"><title>Future work</title><p id="Par43">The developed system has significant practical implications for various applications, including human-computer interaction, gaming, and mental health monitoring. Its real-time emotion recognition capability can greatly enhance user experience by providing valuable insights into user behavior. Future work will focus on expanding the dataset to include a broader range of emotional states and improving the interface to boost user engagement. Exploring advanced machine learning models and incorporating more features could further enhance the system&#x02019;s accuracy and robustness.</p><p id="Par44">Future research will also aim to improve the discriminatory power of recognition systems and refine methods to help participants accurately identify subtle differences in negative emotional expressions. This could involve refining training protocols, incorporating additional contextual cues, and developing more sophisticated algorithms for emotion recognition. The model will be further tested and refined with participants on the autism spectrum to ensure it effectively meets their unique needs and addresses specific challenges in emotion recognition. Such advancements highlight the potential for applying these techniques to other emotion recognition tasks, thereby promoting scalability and adaptability in future research.</p><p id="Par45">While this study focuses on the development and evaluation of a real-time facial emotion recognition system for therapeutic interventions, the concept of facial emotion mimicry has broader applications, such as shared experiences during media consumption or in advertising contexts. These applications could provide valuable insights into the role of mimicry in social interactions and human-computer interfaces, representing an intriguing direction for future research.</p></sec></sec><sec id="Sec9"><title>Methods</title><p id="Par46">A feasibility experiment was designed to evaluate the proposed facial expression recognition as a therapeutic tool system in real-time. In this experiment, a facial point landmark algorithm estimated point landmark locations in 3D space from a USB web camera. This data was combined with geometric relations used as features for the recognition model. The aim was to evaluate facial expression recognition in real time, as well as collect data to improve on the model&#x02019;s performance in post analysis. Participants were recruited and shown facial expressions performed by virtual avatars developed by one of the project partners (Imsimity GmbH, St. Georgen im Schwarzwald Germany). Participants were instructed at the start to mimic, similar to a mirror, the expression they saw the avatar express. The recognition model then ran the evaluation in the background and provided feedback to the user on how well they did and if they should repeat the task.</p><sec id="Sec10"><title>Emotion recognition model</title><sec id="Sec11"><title>Feature extraction</title><p id="Par47">To develop an efficient emotion recognition model resilient to variations in lighting and subject demographics, the geometrical approach to emotion classification was undertaken. Here, a point landmark detection algorithm was employed for estimating facial landmarks from which the different geometric features could be extracted. The selected algorithm was the Mediapipe<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> framework for facial landmark detection, due to its robustness and capability to provide 3D landmarks. This algorithm adapted to facial orientation and distance relative to the camera, delivering 478 facial point landmarks encompassing the entire face and key features, such as the eyes, nose, mouth, and eyebrows. It also accounted for dynamic obstructions, such as a hand placed in front of the face<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>.</p><p id="Par48">The choice to use Mediapipe<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> was further motivated by its flexibility and lightweight design, making it well-suited for real-time emotion recognition tasks. Its robustness in facial landmark detection and adaptability to various facial orientations ensured high detection accuracy, while its open-source nature allowed seamless integration and customization within the experimental framework. These attributes made Mediapipe<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> an ideal choice for tailoring the system to the specific objectives of this study.</p><p id="Par49">Once the facial landmarks were extracted, a reduced number of points was selected based on<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. Using the reduced points, geometric relations were set and the features of input for the recognition model generated. Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> represents most of the points from the facial point landmark cloud that were used for the feature extraction. Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> shows the full range of point landmarks used for the study.</p><p id="Par25">
<fig id="Fig5"><label>Fig. 5</label><caption><p>Reduced facial landmark points used. The point locations in point cloud are depicted in black bold numbers, feature angles are depicted in red, two out of the ten distance metrics in green. The blue lines represent the relation between the points that was undertaken.</p></caption><graphic xlink:href="41598_2025_87688_Fig5_HTML" id="d33e721"/></fig>
</p><p id="Par52">For the feature space, 15 features were used as input to the classification model, with a set of ten distance ratio metrics and five angles utilized. Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> lists the points used to calculate the Euclidean distance in 3D space as well as the points for calculating the feature angles for each individual frame.</p><p id="Par53">
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Features extracted to be used for the recognition model during the study. The term n.a. represents &#x02018;not applicable&#x02019;.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature name</th><th align="left">Landmark point 1</th><th align="left">Landmark point 2</th><th align="left">Landmark point 3</th></tr></thead><tbody><tr><td align="left">d<sub>1</sub></td><td char="." align="char">1</td><td char="." align="char">18</td><td align="left">n.a</td></tr><tr><td align="left">d<sub>2</sub></td><td char="." align="char">14</td><td char="." align="char">15</td><td align="left">n.a</td></tr><tr><td align="left">d<sub>3</sub></td><td char="." align="char">62</td><td char="." align="char">292</td><td align="left">n.a</td></tr><tr><td align="left">d<sub>4</sub></td><td char="." align="char">79</td><td char="." align="char">309</td><td align="left">n.a</td></tr><tr><td align="left">d<sub>5</sub></td><td char="." align="char">5</td><td char="." align="char">292</td><td align="left">n.a</td></tr><tr><td align="left">d<sub>6</sub></td><td char="." align="char">62</td><td char="." align="char">5</td><td align="left">n.a</td></tr><tr><td align="left">d<sub>7</sub></td><td char="." align="char">5</td><td char="." align="char">106</td><td align="left">n.a</td></tr><tr><td align="left">d<sub>8</sub></td><td char="." align="char">335</td><td char="." align="char">5</td><td align="left">n.a</td></tr><tr><td align="left">d<sub>9</sub></td><td char="." align="char">5</td><td char="." align="char">51</td><td align="left">n.a</td></tr><tr><td align="left">d<sub>10</sub></td><td char="." align="char">281</td><td char="." align="char">5</td><td align="left">n.a</td></tr><tr><td align="left">&#x003b8;<sub>1</sub></td><td char="." align="char">1</td><td char="." align="char">62</td><td align="left">18</td></tr><tr><td align="left">&#x003b8;<sub>2</sub></td><td char="." align="char">292</td><td char="." align="char">1</td><td align="left">62</td></tr><tr><td align="left">&#x003b8;<sub>3</sub></td><td char="." align="char">427</td><td char="." align="char">5</td><td align="left">207</td></tr><tr><td align="left">&#x003b8;<sub>4</sub></td><td char="." align="char">160</td><td char="." align="char">34</td><td align="left">146</td></tr><tr><td align="left">&#x003b8;<sub>5</sub></td><td char="." align="char">108</td><td char="." align="char">106</td><td align="left">71</td></tr></tbody></table></table-wrap>
</p><p id="Par54">The distance between the points was calculated using the Euclidean distance in Eq.&#x000a0;(<xref rid="Equ1" ref-type="disp-formula">1</xref>). The angles were calculated using Eq.&#x000a0;(<xref rid="Equ2" ref-type="disp-formula">2</xref>). <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87688_Article_IEq1.gif"/></alternatives></inline-formula> represents the feature number for distance <inline-formula id="IEq2"><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87688_Article_IEq2.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq3"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87688_Article_IEq3.gif"/></alternatives></inline-formula> represents the feature number for the angles <inline-formula id="IEq4"><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87688_Article_IEq4.gif"/></alternatives></inline-formula>. <inline-formula id="IEq5"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$px$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87688_Article_IEq5.gif"/></alternatives></inline-formula>, <inline-formula id="IEq6"><alternatives><tex-math id="M6">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$py$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87688_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$pz$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87688_Article_IEq7.gif"/></alternatives></inline-formula> represent the x, y, and z coordinate, respectively, of the particular point.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M8">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${d}_{n}=\sqrt{{\left({px}_{1}-{px}_{2}\right)}^{2}-{\left({py}_{1}-{py}_{2}\right)}^{2}-{\left({pz}_{1}-{pz}_{2}\right)}^{2}}$$\end{document}</tex-math><graphic xlink:href="41598_2025_87688_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\theta }_{m}= {\text{tan}}^{-1}\frac{\left({py}_{3}-{py}_{2}\right)}{\left({px}_{3}-{px}_{2}\right)}- {\text{tan}}^{-1}\frac{\left({py}_{1}-{py}_{2}\right)}{\left({px}_{1}-{px}_{2}\right)}$$\end{document}</tex-math><graphic xlink:href="41598_2025_87688_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par55">For the recognition model to be more robust to differences in people&#x02019;s facial structure, the ratio of the distance features between a reference frame captured at the start of the experiment, which represents a neutral expression, and the current frame captured at the time of emotional mimicry study was computed and used as input for the recognition model alongside the angles from the current frame. This was calculated by Eq.&#x000a0;(<xref rid="Equ3" ref-type="disp-formula">3</xref>) where <inline-formula id="IEq8"><alternatives><tex-math id="M10">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{n}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87688_Article_IEq8.gif"/></alternatives></inline-formula> is the distance ratio between the current and reference frame, <inline-formula id="IEq9"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${d}_{nr}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87688_Article_IEq9.gif"/></alternatives></inline-formula> is the distance feature <inline-formula id="IEq10"><alternatives><tex-math id="M12">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87688_Article_IEq10.gif"/></alternatives></inline-formula> calculated at the reference frame, and <inline-formula id="IEq11"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${d}_{nc}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87688_Article_IEq11.gif"/></alternatives></inline-formula> is the distance feature <inline-formula id="IEq12"><alternatives><tex-math id="M14">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87688_Article_IEq12.gif"/></alternatives></inline-formula> calculated at the current frame.<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{n}=\frac{{d}_{nr}}{{d}_{nc}}$$\end{document}</tex-math><graphic xlink:href="41598_2025_87688_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec12"><title>Classification model</title><p id="Par56">For the classification of the emotions, a simple pattern recognition model was implemented. The classification model included an input layer, one dense layer with output size 10, a sigmoid activation function and a classification layer i.e., a dense layer with output size same as number of classes and a SoftMax activation. The model was trained on 200 epochs with an adaptive moment estimation (adam) optimizer, a batch size of 128 and an initial learning rate of 0.01.</p><p id="Par57">
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Conversion of the six-class emotion and neutral expression representation to a two-class representation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Original Class Name</th><th align="left">New Class Name</th></tr></thead><tbody><tr><td align="left">Anger</td><td align="left" rowspan="4">Negative</td></tr><tr><td align="left">Disgust</td></tr><tr><td align="left">Fear</td></tr><tr><td align="left">Sadness</td></tr><tr><td align="left">Happiness</td><td align="left" rowspan="3">Positive</td></tr><tr><td align="left">Surprise</td></tr><tr><td align="left">Neutral</td></tr></tbody></table></table-wrap>
</p><p id="Par58">The dataset was also split with a ratio of 90/10 for training and validation set respectively. To evaluate the performance, the validation sets the true positive accuracy was used and the confusion matrices visualized.</p><p id="Par59">The six-class emotion representation along with the neutral expression was transformed into a two-class system of positive and negative classes based on the conditions depicted in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>.</p></sec></sec><sec id="Sec13"><title>Dataset and data collection</title><p id="Par60">The widely recognized facial expression recognition databases of OULU-CASIA<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> and FACES<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> were selected for the training and initial validation of the network models. The OULU-CASIA database was created with two image acquisition systems (infrared and visible light) under three different lighting conditions (strong, weak, and natural light). A subset from the database was chosen for the analysis. Specifically, the non-cropped RGB images taken under strong illumination. The data comprised a set of image sequences from 80 subjects portraying the six basic emotions: anger, disgust, fear, happiness, sadness, and surprise. The sequence begins with a weak representation and ends with a strong representation of the particular emotion. The strongest representation of each subject for each emotion was used for the analysis, while the first frame was used as the reference frame for the feature extraction for the model training. This subset had a final count of 480 emotion images and 480 neutral weak/neutral expression with an image resolution of 320&#x02009;&#x000d7;&#x02009;240 pixels.</p><p id="Par61">The FACES database, was then combined with the data from the OULU-CASIA to encompass different subject demographics for an improved model robustness. The FACES database consists of facial portraits from subjects of various ages, with two images per emotional expression, covering the five basic emotions: anger, disgust, fear, happiness, and sadness as well as an additional neutral expression. The neutral expression from the FACES dataset for each subject was used as the reference frame to extract the features for the model training. Therefore, the selected database subset included a total of 1,710 emotion images and 342 neutral expressions, with a resolution of 2835&#x02009;&#x000d7;&#x02009;3543 pixels.</p><p id="Par62">Finally, data were collected during a preliminary beta phase of an experimental study. Participants (8 in total, 7 male and 1 female) volunteered for the experimental trial of the proposed mimicry system developed for emotion training. The data collected was later used to test the robustness of the system and modify model approaches. The visual data from a camera was collected at a resolution of 480&#x02009;&#x000d7;&#x02009;640. The data size varied based on the number of attempts the subject performed to get a correct reading from the mimicry experiment. The details of the experimental setup can be seen in the &#x0201c;<xref rid="Sec14" ref-type="sec">Experimental setup</xref>&#x0201d; section.</p></sec><sec id="Sec14"><title>Experimental setup</title><p id="Par64">The conducted experimental trial involved having the participants sit in front of a monitor. In the first, &#x02018;emotion assessment&#x02019;, stage, participants attempted to identify the emotion that is portrayed by an avatar on the screen. In the second stage, to be referred to as emotion mimicry, the participants were asked to mimic the avatar&#x02019;s facial expression, and were given feedback on their ability to recreate the expression. The system setup was separated into two sections, one for the participant and the other for the administrator.</p><p id="Par65">The avatars depicted the six basic emotions of anger, disgust, fear, happiness, sadness, and surprise as well as a neutral expression. Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> represents the different avatars&#x02019; emotional expressions (anger, disgust, fear, happy, neutral, sad, and surprise) used during the study. The avatars were designed and developed by Imsimity GmbH (St. Georgen im Schwarzwald Germany). The design of the expressions was based on the facial action coding system (FACS)<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, which is a widely adopted popular method for modeling and identifying facial expressions. The FACS provides a comprehensive and standardized framework for identifying and describing facial movements. By breaking down expressions into individual action units (AUs), which correspond to specific muscle movements, FACS allows for replication of human emotions. This level of detail enhanced the realism and accuracy of avatar facial expressions, making them more relatable and capable of conveying a wide range of emotions effectively.</p><p id="Par26">
<fig id="Fig6"><label>Fig. 6</label><caption><p>Different Avatar facial expressions displayed to the subject.</p></caption><graphic xlink:href="41598_2025_87688_Fig6_HTML" id="d33e1117"/></fig>
</p><sec id="Sec28"><title>System workflow emotion assessment</title><p id="Par69">In the participants&#x02019; section the user only sees the interface of the proposed system. The user is able to select from the stages of the experiment at any order they choose. In the assessment stage the user was prompted with four selectable options in which each option is set to an emotional category. An avatar then appears with a randomly selected pre-defined facial expression representing a particular emotion from the 7 pre-set expressions defined. The user can then select the emotion they most relate to the avatar&#x02019;s expression. Among the four options given, only one was correct in which it would be highlighted green when the subject assumed it correct or red when the subject guesses wrong. This step was repeated for the different facial expressions defined. The order was randomized for each user. Figure&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref> shows the participants view perspective at the launch of the system and during the assessment task.</p><p id="Par28">
<fig id="Fig7"><label>Fig. 7</label><caption><p>Participant view of the start interface (left <bold>a</bold>), the first screen after clicking on the assessment experiment (<bold>b</bold>), choosing the correct response (<bold>c</bold>), and in case of incorrect response (<bold>d</bold>).</p></caption><graphic xlink:href="41598_2025_87688_Fig7_HTML" id="d33e1147"/></fig>
</p></sec><sec id="Sec15"><title>System workflow emotion mimicry</title><p id="Par72">Figure&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref> represents the system workflow for the emotional mimicry developed for the emotion recognition interface. The workflow outlines the sequence of steps from data acquisition to emotion recognition and feedback processing. A detailed description covers each step, including the conditions and actions taken at each phase, to provide a comprehensive understanding of the system&#x02019;s operation.<list list-type="bullet"><list-item><p id="Par79">Start Button pressed:</p><p id="Par75">The workflow began when the user pressed the &#x0201c;Start Button&#x0201d; This action initiates the entire process, setting the system to start acquiring data from the user. Figure&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>a represents the screen at the start of the interface after already having completed the assessment experiment.</p></list-item><list-item><p id="Par790">Initialization:</p><p id="Par77">Upon pressing the start button, the system performed initialization procedures. This involves loading the pre-selected trained recognition network model, creating a data recording file, loading the necessary packages for the software to run, ensuring all devices (camera) and applications (emotion recognition model, data recording) are ready for operation.</p></list-item><list-item><p id="Par791">Reference frame capture button pressed:</p><p id="Par80">After the initialization step was complete, the button to capture the reference frame (i.e., neutral expression) of the subject was visible to the user with a display showing the view from the camera. This display allowed the user to adjust his position as one does typically when standing in front of a mirror. This is visualized in Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>b which represents the screen at the start of the reference capture process.</p><p id="Par81">After the button was pressed the system takes a snapshot of the image from the camera and extracts the relevant facial features required for the emotional recognition model to process. After this processing is complete the user was then migrated to the next scene of the interface where a &#x0201c;Start&#x0201d; button was displayed, that would take the user to the avatar facial expressions and begin the emotion recognition process. This is seen in Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>c which represents the screen at the phase post reference frame capture and prior to emotion recognition start.</p></list-item><list-item><p id="Par792">Avatar mimicry start button pressed:</p><p id="Par82">The system then proceeds to display the avatar and a corresponding facial expression. The user is asked to mimic the expression for a period of 5&#x000a0;s. This can be observed in Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>d where the avatar is shown with a specific facial expression. The system then internally proceeds to detect the user&#x02019;s face using the visual information from the camera. This step is crucial as the subsequent emotion recognition process relies on accurately identifying and capturing the user&#x02019;s facial features. If the face was detected successfully, the workflow advances to the next step. If not, the system generates a &#x0201c;Face Not Recognized&#x0201d; notification, prompting the user to adjust their position to ensure proper detection.</p></list-item><list-item><p id="Par793">Face recognition and data capture:</p><p id="Par85">Once the face was detected, the system captured the current frame. This frame was then used to extract the relevant features needed for the emotion recognition model. This process was performed on a continuous basis throughout the duration (i.e., 5&#x000a0;s) of the current emotional depiction by the avatar. An idle period of 3&#x000a0;s was set as a transition phase between the different facial expressions, as seen in Fig.&#x000a0;<xref rid="Fig11" ref-type="fig">11</xref>a, where after receiving feedback a cool down period of 3&#x000a0;s shown as a countdown in red is displayed before the subject can move on to the next phase.</p><p id="Par87">If at any point the system fails to recognize the face again, it triggered a notification and attempts to re-establish proper detection and data capture. This is represented in Fig.&#x000a0;<xref rid="Fig10" ref-type="fig">10</xref>a where the message is displayed to the participant with a timer countdown in red lettering at the top of the display.</p></list-item><list-item><p id="Par794">Emotion recognition model:</p><p id="Par91">The generated feature data were then processed by the emotion recognition model, which analyzed the facial expressions to classify the user&#x02019;s emotional state. The model was pre-trained to identify specific emotions based on the geometric features of the face. The recognition model ran in near real-time, continuously updating its predictions as new data was captured.</p></list-item><list-item><p id="Par750">Data processing and validation</p><p id="Par93">The emotion recognition results are processed and recorded, and the system validated these predictions against the ground truth data that were the pre-defined avatar expression, to provide an average accuracy reading that would prompt for the appropriate feedback response. If the predictions are above a certain threshold accuracy, a feedback message was displayed to the user. The feedback of &#x0201c;Great Job!&#x0201d;, &#x0201c;Good Job!&#x0201d; and &#x0201c;Try Again!&#x0201d; is sent to the subject based on the mean accuracy being equal to or above 80%, between 50% inclusive and 80%, and less than 50% respectively. The workflow then proceeds to the next stage at the users&#x02019; pace, i.e., via a &#x0201c;Next&#x0201d; button press. If predictions are not above the threshold of 50%, the system would reinitiate the emotion recognition process, using the facial expression of the failed attempt, at the end of the current lineup of emotional expressions. The subject has at most 3 attempts to mimic the avatar expression if they fail to reach a minimum desired threshold. In Fig.&#x000a0;<xref rid="Fig11" ref-type="fig">11</xref>b&#x02013;d the different feedbacks are displayed.</p></list-item><list-item><p>Result storage and display</p><p id="Par95">The experiment results were recorded and stored for future model enhancements as well as for post processing. This data included detailed logs of the recognized emotions, timestamps, and other relevant metadata.</p></list-item><list-item><p id="Par761">Stop button initialized</p><p id="Par97">The workflow concludes when the entire sequence of facial expressions was completed in which the &#x0201c;Stop&#x0201d; button is initialized internally. This action terminates the data acquisition and emotion recognition processes. This process was run in parallel to the user viewing the end of the study screen, as shown in Fig.&#x000a0;<xref rid="Fig10" ref-type="fig">10</xref>b.</p><p id="Par98">After saving the final results, the system disconnected from the camera and software components used during the process. This step ensured no data were lost, and the system was ready for the next use.</p></list-item></list></p><p id="Par30">
<fig id="Fig8"><label>Fig. 8</label><caption><p>System workflow.</p></caption><graphic xlink:href="41598_2025_87688_Fig8_HTML" id="d33e1239"/></fig>
</p><p id="Par50">
<fig id="Fig9"><label>Fig. 9</label><caption><p>Participant view of start interface after completion of assessment experiment (<bold>a</bold>), screen for the capture of the reference frame with a live camera feedback depicted as black rectangle (<bold>b</bold>), transition phase where user feedback is requested to begin the emotion recognition phase (<bold>c</bold>), and the avatar facial expression depicted that the participant is asked to mimic (<bold>d</bold>).</p></caption><graphic xlink:href="41598_2025_87688_Fig9_HTML" id="d33e1262"/></fig>
</p><p id="Par51">
<fig id="Fig10"><label>Fig. 10</label><caption><p>Participant view when the face is not recognized (left <bold>a</bold>), view at the end of the experiment (<bold>b</bold>).</p></caption><graphic xlink:href="41598_2025_87688_Fig10_HTML" id="d33e1279"/></fig>
</p><p id="Par63">
<fig id="Fig11"><label>Fig. 11</label><caption><p>Participant view of the feedback from their mimic accuracy (between 80 and 50%) and countdown in red lettering to the next facial expression (<bold>a</bold>), screen after the cool down period is elapsed and the next button displayed (<bold>b</bold>), the feedback from their mimic accuracy (below 50%) (<bold>c</bold>), and feedback from their mimic accuracy (above 80%) (<bold>d</bold>).</p></caption><graphic xlink:href="41598_2025_87688_Fig11_HTML" id="d33e1302"/></fig>
</p></sec><sec id="Sec24"><title>Inter-program communication</title><p id="Par100">The system was designed with two programming languages, python and c#. The user interface was developed using the Unity (Unity Technologies) program which runs in conjunction with c#, while the code and the recognition models were developed and analyzed using python. A suitable communication between the two platforms was required and therefore a communication protocol was established via 2 &#x0201c;.txt&#x0201d; files. In Fig.&#x000a0;<xref rid="Fig12" ref-type="fig">12</xref> the communication commands and phases of the system workflow are depicted.</p><p id="Par66">
<fig id="Fig12"><label>Fig. 12</label><caption><p>Communication commands sent and received between both platforms, C-sharp (c#) and Unity on the left, and python on the right.</p></caption><graphic xlink:href="41598_2025_87688_Fig12_HTML" id="d33e1319"/></fig>
</p><p id="Par102">During the mimicry experiment, when the user first selects the experiment by pressing the button the command of &#x0201c;Initialize&#x0201d; as well as the user ID set by the admin is sent to the recognition system to begin the subsequent steps necessary. After the initialization is complete, time for completion 30 milli-seconds (ms), the command &#x0201c;initialization complete&#x0201d; is sent followed by a &#x0201c;Ready01&#x0201d; command after 2 ms and sets the system in idle mode waiting for the user to do the next step.</p><p id="Par103">When the user then presses the &#x0201c;Reference Frame Capture&#x0201d; button a command &#x0201c;Capture&#x0201d; was sent to the system to take a snapshot of the visual data and process the information to retrieve the relevant features from the reference frame and store it for use in the model recognition task. After the capture is completed and the features extracted and stored, duration of 1&#x000a0;s (s), a follow-up command of &#x0201c;Ready02&#x0201d; is sent to the interface within 2 ms and the system goes back into idle mode.</p><p id="Par104">Once the &#x0201c;Start&#x0201d; button is pressed to begin the mimicry sequence, a command of &#x0201c;Start ER&#x0201d; along with the avatars pre-set emotion class was sent to the system. This emotion was used as the ground truth to assess the subject&#x02019;s ability to mimic the expression. If the face was not recognized, a command is sent back to the interface to notify the subject and a delay of one second was set in order to provide ample time for the subject to adjust his position or head orientation. The emotion recognition would then run continuously for a period of 5&#x000a0;s. After which feedback consisting of &#x0201c;Great Job!&#x0201d;, &#x0201c;Good Job!&#x0201d;, and &#x0201c;Try Again!&#x0201d; was provided to the subject based on the mean accuracy of the predictions against the ground truth over the 5&#x000a0;s period.</p><p id="Par105">After the time period has elapsed and the feedback is displayed to the user on the interface the system and interface perform a 3&#x000a0;s idle period in which the commands of &#x0201c;Wait&#x0201d; and &#x0201c;Ready&#x0201d; are exchanged after second. Then depending on the users pace the next emotion avatar was depicted and the process was repeated for the entire sequence of emotions, totaling 7 times in this case. If the subject failed to reach the threshold and has a &#x0201c;Try Again!&#x0201d; feedback, the failed emotion mimicry would be repeated again after the completion of the sequence. This repeat will happen up to a maximum of two times, therefore having each emotion repeated a maximum of 3 times in total.</p><p id="Par106">Once the sequence was finished, the user was then notified on the interface and the command of &#x0201c;Stop&#x0201d; is sent to the system to shut down and terminate operations. The system then proceeds to send the command of &#x0201c;Terminating code&#x0201d;, after which it performs the necessary data recordings and closures of operations.</p></sec><sec id="Sec200"><title>Hardware components</title><p id="Par108">The participant section included a monitor with a resolution of 1080&#x02009;&#x000d7;&#x02009;1920, a USB camera with a resolution of up to 1920&#x02009;&#x000d7;&#x02009;1080, and a computer-mouse for giving feedback to stage 1 of the experiment. The administrator section was composed of two monitors of resolution 1920&#x02009;&#x000d7;&#x02009;1080, and a computer-keyboard and mouse. Figure&#x000a0;<xref rid="Fig13" ref-type="fig">13</xref> displays the system setup of both participant and administrator view.</p><p id="Par68">
<fig id="Fig13"><label>Fig. 13</label><caption><p>Participant view right and administrator view left of hardware components used for the experimental study. The camera is depicted by the red oval in the participant view.</p></caption><graphic xlink:href="41598_2025_87688_Fig13_HTML" id="d33e1346"/></fig>
</p><p id="Par110">The experiment was conducted on a desktop with an AMD Ryzen 7 3700&#x02009;&#x000d7;&#x02009;8 core processor, 64.00 GB memory (RAM) and an NVIDIA graphics card RTX 3070 (NVIDIA Corporation, Santa Clara, CA, USA).</p></sec><sec id="Sec70"><title>Software components</title><p id="Par112">The software used in this experimental feasibility study included Unity (Unity Technology) for avatar display and interaction recording, Python for emotion recognition and data recording, and OBS Studio (Open Broadcaster Software) for virtual camera setup and video recording. The use of virtual cameras allowed for the reduction in the number of hardware cameras required.</p></sec></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This research was partially funded by the German Federal Ministry of Research and Education (BMBF) under grant LESSON FKZ: 3FH5E10IA, a grant from KOMPASS funded by the Ministerium f&#x000fc;r Wissenschaft, Forschung und Kunst (MWK) of Baden-Wuerttemberg Germany, a grant from the ERAPERMED2022-276&#x02014;ETAP BMG FKZ 2523FSB110, a grant from the New Zealand Ministry of Business, Innovation and Employment (MBIE) under the Catalyst Leaders Grant (UOCX2201), and a grant from the Education New Zealand Programme for Project Related Personal Exchange (PPP): New Zealand&#x02013;German Academic Exchange (DAAD) Programme under grant AIDE-ASD FKZ 57656657.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Conceptualization, H.A. and K.M.; methodology, H.A. and T.A.A.; software, H.A. and T.A.A.; validation, H.A.; formal analysis, H.A., A.B., T.A.A., J.G.C. and K.M.; investigation, H.A.; resources, J.G.C. and K.M.; data curation, H.A.; writing&#x02014;original draft preparation, H.A.; writing&#x02014;review and editing, T.A.A., A.B., V.W.-H., M.I., J.G.C. and K.M.; visualization, H.A.; supervision, J.G.C. and K.M.; project administration, K.M.; funding acquisition, J.G.C. and K.M. All authors have read and reviewed this version of the manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>Open Access funding enabled and organized by Projekt DEAL.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets generated during and/or analyzed during the current study are not publicly available due to privacy, legal and ethical restrictions but are available from the corresponding author on reasonable request pending review.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par114">The authors declare no competing interests.</p></notes><notes id="FPar3"><title>Ethical approval</title><p id="Par116">The study was conducted in accordance with the Declaration of Helsinki and approved by the Institutional Review Board (or Ethics Committee) of Hochschule Furtwangen (application number 22&#x02013;028 and date of approval 16 June 2022). Informed consent was obtained from all participants involved in the study. Data sharing is not applicable to this article. The authors extend their gratitude and appreciation to the voluntary participants, whose time and commitment significantly enriched our study.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>M</given-names></name></person-group><article-title>How digital health technologies promote healthy life in the post-COVID-19 era: Evidences from national survey on Chinese adolescents and youngsters</article-title><source>Front. Public Health</source><year>2023</year><volume>11</volume><fpage>1135313</fpage><pub-id pub-id-type="doi">10.3389/fpubh.2023.1135313</pub-id><pub-id pub-id-type="pmid">37228730</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Li, X. &#x00026; Zhang, M. How digital health technologies promote healthy life in the post-COVID-19 era: Evidences from national survey on Chinese adolescents and youngsters. <italic>Front. Public Health</italic><bold>11</bold>, 1135313. 10.3389/fpubh.2023.1135313 (2023).<pub-id pub-id-type="pmid">37228730</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Boosting Digital Health Can Help Prevent Millions of Deaths from Noncommunicable Diseases Available online: <ext-link ext-link-type="uri" xlink:href="https://www.who.int/news/item/23-09-2024-boosting-digital-health-can-help-prevent-millions-of-deaths-from-noncommunicable-diseases">https://www.who.int/news/item/23-09-2024-boosting-digital-health-can-help-prevent-millions-of-deaths-from-noncommunicable-diseases</ext-link>. Accessed 15 Nov 2024.</mixed-citation></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Yeung</surname><given-names>AWK</given-names></name><name><surname>Torkamani</surname><given-names>A</given-names></name><name><surname>Butte</surname><given-names>AJ</given-names></name><name><surname>Glicksberg</surname><given-names>BS</given-names></name><name><surname>Schuller</surname><given-names>B</given-names></name><name><surname>Rodriguez</surname><given-names>B</given-names></name><name><surname>Ting</surname><given-names>DSW</given-names></name><name><surname>Bates</surname><given-names>D</given-names></name><name><surname>Schaden</surname><given-names>E</given-names></name><name><surname>Peng</surname><given-names>H</given-names></name><etal/></person-group><article-title>The promise of digital healthcare technologies</article-title><source>Front. Public Health</source><year>2023</year><volume>11</volume><fpage>1196596</fpage><pub-id pub-id-type="doi">10.3389/fpubh.2023.1196596</pub-id><pub-id pub-id-type="pmid">37822534</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Yeung, A. W. K. et al. The promise of digital healthcare technologies. <italic>Front. Public Health</italic><bold>11</bold>, 1196596. 10.3389/fpubh.2023.1196596 (2023).<pub-id pub-id-type="pmid">37822534</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Huhn</surname><given-names>S</given-names></name><name><surname>Axt</surname><given-names>M</given-names></name><name><surname>Gunga</surname><given-names>H-C</given-names></name><name><surname>Maggioni</surname><given-names>MA</given-names></name><name><surname>Munga</surname><given-names>S</given-names></name><name><surname>Obor</surname><given-names>D</given-names></name><name><surname>Si&#x000e9;</surname><given-names>A</given-names></name><name><surname>Boudo</surname><given-names>V</given-names></name><name><surname>Bunker</surname><given-names>A</given-names></name><name><surname>Sauerborn</surname><given-names>R</given-names></name><etal/></person-group><article-title>The impact of wearable technologies in health research: Scoping review</article-title><source>JMIR mHealth uHealth</source><year>2022</year><volume>10</volume><fpage>e34384</fpage><pub-id pub-id-type="doi">10.2196/34384</pub-id><pub-id pub-id-type="pmid">35076409</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Huhn, S. et al. The impact of wearable technologies in health research: Scoping review. <italic>JMIR mHealth uHealth</italic><bold>10</bold>, e34384. 10.2196/34384 (2022).<pub-id pub-id-type="pmid">35076409</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Filip</surname><given-names>R</given-names></name><name><surname>Gheorghita Puscaselu</surname><given-names>R</given-names></name><name><surname>Anchidin-Norocel</surname><given-names>L</given-names></name><name><surname>Dimian</surname><given-names>M</given-names></name><name><surname>Savage</surname><given-names>WK</given-names></name></person-group><article-title>Global challenges to public health care systems during the COVID-19 pandemic: A review of pandemic measures and problems</article-title><source>J. Personal. Med.</source><year>2022</year><volume>12</volume><fpage>1295</fpage><pub-id pub-id-type="doi">10.3390/jpm12081295</pub-id></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Filip, R., Gheorghita Puscaselu, R., Anchidin-Norocel, L., Dimian, M. &#x00026; Savage, W. K. Global challenges to public health care systems during the COVID-19 pandemic: A review of pandemic measures and problems. <italic>J. Personal. Med.</italic><bold>12</bold>, 1295. 10.3390/jpm12081295 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><collab>Fragility and Challenges of Health Systems in Pandemic</collab></person-group><article-title>Lessons from India&#x02019;s second wave of coronavirus disease 2019 (COVID-19)</article-title><source>Glob. Health J.</source><year>2022</year><volume>6</volume><fpage>44</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.glohj.2022.01.006</pub-id><pub-id pub-id-type="pmid">35070474</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Fragility and Challenges of Health Systems in Pandemic. Lessons from India&#x02019;s second wave of coronavirus disease 2019 (COVID-19). <italic>Glob. Health J.</italic><bold>6</bold>, 44&#x02013;49. 10.1016/j.glohj.2022.01.006 (2022).<pub-id pub-id-type="pmid">35070474</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Dombovy</surname><given-names>ML</given-names></name></person-group><article-title>U.S. health care in conflict-part I. The challenges of balancing cost, quality and access</article-title><source>Physician Exec.</source><year>2002</year><volume>28</volume><fpage>43</fpage><lpage>47</lpage><pub-id pub-id-type="pmid">12219531</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Dombovy, M. L. U.S. health care in conflict-part I. The challenges of balancing cost, quality and access. <italic>Physician Exec.</italic><bold>28</bold>, 43&#x02013;47 (2002).<pub-id pub-id-type="pmid">12219531</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Baumol, W. J. <italic>The Cost Disease: Why Computers Get Cheaper and Health Care Doesn&#x02019;t</italic> (Yale University Press, 2012). ISBN 0-300-17928-6.</mixed-citation></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Dorsey</surname><given-names>ER</given-names></name><name><surname>Venkataraman</surname><given-names>V</given-names></name><name><surname>Grana</surname><given-names>MJ</given-names></name><name><surname>Bull</surname><given-names>MT</given-names></name><name><surname>George</surname><given-names>BP</given-names></name><name><surname>Boyd</surname><given-names>CM</given-names></name><name><surname>Beck</surname><given-names>CA</given-names></name><name><surname>Rajan</surname><given-names>B</given-names></name><name><surname>Seidmann</surname><given-names>A</given-names></name><name><surname>Biglan</surname><given-names>KM</given-names></name></person-group><article-title>Randomized controlled clinical trial of &#x0201c;virtual house calls&#x0201d; for Parkinson disease</article-title><source>JAMA Neurol.</source><year>2013</year><volume>70</volume><fpage>565</fpage><pub-id pub-id-type="doi">10.1001/jamaneurol.2013.123</pub-id><pub-id pub-id-type="pmid">23479138</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Dorsey, E. R. et al. Randomized controlled clinical trial of &#x0201c;virtual house calls&#x0201d; for Parkinson disease. <italic>JAMA Neurol.</italic><bold>70</bold>, 565. 10.1001/jamaneurol.2013.123 (2013).<pub-id pub-id-type="pmid">23479138</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>S</given-names></name></person-group><article-title>From face-to-face to facetime</article-title><source>IEEE Pulse</source><year>2020</year><volume>11</volume><fpage>7</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1109/MPULS.2020.3022140</pub-id><pub-id pub-id-type="pmid">33064637</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Campbell, S. From face-to-face to facetime. <italic>IEEE Pulse</italic><bold>11</bold>, 7&#x02013;11. 10.1109/MPULS.2020.3022140 (2020).<pub-id pub-id-type="pmid">33064637</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Holder-Pearson</surname><given-names>L</given-names></name><name><surname>Chase</surname><given-names>JG</given-names></name></person-group><article-title>Socio-economic inequity: Diabetes in New Zealand</article-title><source>Front. Med.</source><year>2022</year><volume>9</volume><fpage>756223</fpage><pub-id pub-id-type="doi">10.3389/fmed.2022.756223</pub-id></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Holder-Pearson, L. &#x00026; Chase, J. G. Socio-economic inequity: Diabetes in New Zealand. <italic>Front. Med.</italic><bold>9</bold>, 756223. 10.3389/fmed.2022.756223 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Tebartz Van Elst</surname><given-names>L</given-names></name><name><surname>Fangmeier</surname><given-names>T</given-names></name><name><surname>Schaller</surname><given-names>UM</given-names></name><name><surname>Hennig</surname><given-names>O</given-names></name><name><surname>Kieser</surname><given-names>M</given-names></name><name><surname>Koelkebeck</surname><given-names>K</given-names></name><name><surname>Kuepper</surname><given-names>C</given-names></name><name><surname>Roessner</surname><given-names>V</given-names></name><name><surname>Wildgruber</surname><given-names>D</given-names></name><name><surname>Dziobek</surname><given-names>I</given-names></name></person-group><article-title>FASTER and SCOTT&#x00026;EVA trainings for adults with high-functioning autism spectrum disorder (ASD): Study protocol for a randomized controlled trial</article-title><source>Trials</source><year>2021</year><volume>22</volume><fpage>261</fpage><pub-id pub-id-type="doi">10.1186/s13063-021-05205-9</pub-id><pub-id pub-id-type="pmid">33832537</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Tebartz Van Elst, L. et al. FASTER and SCOTT&#x00026;EVA trainings for adults with high-functioning autism spectrum disorder (ASD): Study protocol for a randomized controlled trial. <italic>Trials</italic><bold>22</bold>, 261. 10.1186/s13063-021-05205-9 (2021).<pub-id pub-id-type="pmid">33832537</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Rylaarsdam</surname><given-names>L</given-names></name><name><surname>Guemez-Gamboa</surname><given-names>A</given-names></name></person-group><article-title>Genetic causes and modifiers of autism spectrum disorder</article-title><source>Front. Cell. Neurosci.</source><year>2019</year><volume>13</volume><fpage>385</fpage><pub-id pub-id-type="doi">10.3389/fncel.2019.00385</pub-id><pub-id pub-id-type="pmid">31481879</pub-id>
</element-citation><mixed-citation id="mc-CR13" publication-type="journal">Rylaarsdam, L. &#x00026; Guemez-Gamboa, A. Genetic causes and modifiers of autism spectrum disorder. <italic>Front. Cell. Neurosci.</italic><bold>13</bold>, 385. 10.3389/fncel.2019.00385 (2019).<pub-id pub-id-type="pmid">31481879</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Grifantini</surname><given-names>K</given-names></name></person-group><article-title>Detecting faces, saving lives</article-title><source>IEEE Pulse</source><year>2020</year><volume>11</volume><fpage>2</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1109/MPULS.2020.2984288</pub-id><pub-id pub-id-type="pmid">32386130</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Grifantini, K. Detecting faces, saving lives. <italic>IEEE Pulse</italic><bold>11</bold>, 2&#x02013;7. 10.1109/MPULS.2020.2984288 (2020).<pub-id pub-id-type="pmid">32386130</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><collab>Committee on Children With Disabilities</collab></person-group><article-title>The pediatrician&#x02019;s role in the diagnosis and management of autistic spectrum disorder in children</article-title><source>Pediatrics</source><year>2001</year><volume>107</volume><fpage>1221</fpage><lpage>1226</lpage><pub-id pub-id-type="doi">10.1542/peds.107.5.1221</pub-id><pub-id pub-id-type="pmid">11331713</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Committee on Children With Disabilities. The pediatrician&#x02019;s role in the diagnosis and management of autistic spectrum disorder in children. <italic>Pediatrics</italic><bold>107</bold>, 1221&#x02013;1226. 10.1542/peds.107.5.1221 (2001).<pub-id pub-id-type="pmid">11331713</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Molnar-Szakacs</surname><given-names>I</given-names></name><name><surname>Wang</surname><given-names>MJ</given-names></name><name><surname>Laugeson</surname><given-names>EA</given-names></name><name><surname>Overy</surname><given-names>K</given-names></name><name><surname>Wu</surname><given-names>W-L</given-names></name><name><surname>Piggot</surname><given-names>J</given-names></name></person-group><article-title>Autism, emotion recognition and the mirror neuron system: The case of music</article-title><source>McGill J. Med. MJM</source><year>2009</year><volume>12</volume><fpage>87</fpage><pub-id pub-id-type="pmid">21264050</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Molnar-Szakacs, I. et al. Autism, emotion recognition and the mirror neuron system: The case of music. <italic>McGill J. Med. MJM</italic><bold>12</bold>, 87 (2009).<pub-id pub-id-type="pmid">21264050</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Golan</surname><given-names>O</given-names></name><name><surname>Ashwin</surname><given-names>E</given-names></name><name><surname>Granader</surname><given-names>Y</given-names></name><name><surname>McClintock</surname><given-names>S</given-names></name><name><surname>Day</surname><given-names>K</given-names></name><name><surname>Leggett</surname><given-names>V</given-names></name><name><surname>Baron-Cohen</surname><given-names>S</given-names></name></person-group><article-title>Enhancing emotion recognition in children with autism spectrum conditions: An intervention using animated vehicles with real emotional faces</article-title><source>J. Autism Dev. Disord.</source><year>2010</year><volume>40</volume><fpage>269</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1007/s10803-009-0862-9</pub-id><pub-id pub-id-type="pmid">19763807</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Golan, O. et al. Enhancing emotion recognition in children with autism spectrum conditions: An intervention using animated vehicles with real emotional faces. <italic>J. Autism Dev. Disord.</italic><bold>40</bold>, 269&#x02013;279. 10.1007/s10803-009-0862-9 (2010).<pub-id pub-id-type="pmid">19763807</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>SNV</given-names></name><name><surname>Ip</surname><given-names>HHS</given-names></name></person-group><article-title>Using virtual reality to train emotional and social skills in children with autism spectrum disorder</article-title><source>Lond. J. Primary Care</source><year>2018</year><volume>10</volume><fpage>110</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1080/17571472.2018.1483000</pub-id></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Yuan, S. N. V. &#x00026; Ip, H. H. S. Using virtual reality to train emotional and social skills in children with autism spectrum disorder. <italic>Lond. J. Primary Care</italic><bold>10</bold>, 110&#x02013;112. 10.1080/17571472.2018.1483000 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Ravindran</surname><given-names>V</given-names></name><name><surname>Osgood</surname><given-names>M</given-names></name><name><surname>Sazawal</surname><given-names>V</given-names></name><name><surname>Solorzano</surname><given-names>R</given-names></name><name><surname>Turnacioglu</surname><given-names>S</given-names></name></person-group><article-title>Virtual reality support for joint attention using the floreo joint attention module: Usability and feasibility pilot study</article-title><source>JMIR Pediatr. Parent.</source><year>2019</year><volume>2</volume><fpage>e14429</fpage><pub-id pub-id-type="doi">10.2196/14429</pub-id><pub-id pub-id-type="pmid">31573921</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Ravindran, V., Osgood, M., Sazawal, V., Solorzano, R. &#x00026; Turnacioglu, S. Virtual reality support for joint attention using the floreo joint attention module: Usability and feasibility pilot study. <italic>JMIR Pediatr. Parent.</italic><bold>2</bold>, e14429. 10.2196/14429 (2019).<pub-id pub-id-type="pmid">31573921</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Scherer</surname><given-names>KR</given-names></name></person-group><article-title>What are emotions? And how can they be measured?</article-title><source>Soc. Sci. Inf.</source><year>2005</year><volume>44</volume><fpage>695</fpage><lpage>729</lpage><pub-id pub-id-type="doi">10.1177/0539018405058216</pub-id></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Scherer, K. R. What are emotions? And how can they be measured?. <italic>Soc. Sci. Inf.</italic><bold>44</bold>, 695&#x02013;729. 10.1177/0539018405058216 (2005).</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Lang</surname><given-names>PJ</given-names></name><name><surname>Bradley</surname><given-names>MM</given-names></name></person-group><article-title>Emotion and the motivational brain</article-title><source>Biol. Psychol.</source><year>2010</year><volume>84</volume><fpage>437</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2009.10.007</pub-id><pub-id pub-id-type="pmid">19879918</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Lang, P. J. &#x00026; Bradley, M. M. Emotion and the motivational brain. <italic>Biol. Psychol.</italic><bold>84</bold>, 437&#x02013;450. 10.1016/j.biopsycho.2009.10.007 (2010).<pub-id pub-id-type="pmid">19879918</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group><article-title>How brains beware: Neural mechanisms of emotional attention</article-title><source>Trends Cogn. Sci.</source><year>2005</year><volume>9</volume><fpage>585</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.10.011</pub-id><pub-id pub-id-type="pmid">16289871</pub-id>
</element-citation><mixed-citation id="mc-CR22" publication-type="journal">Vuilleumier, P. How brains beware: Neural mechanisms of emotional attention. <italic>Trends Cogn. Sci.</italic><bold>9</bold>, 585&#x02013;594. 10.1016/j.tics.2005.10.011 (2005).<pub-id pub-id-type="pmid">16289871</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Mancini</surname><given-names>C</given-names></name><name><surname>Falciati</surname><given-names>L</given-names></name><name><surname>Maioli</surname><given-names>C</given-names></name><name><surname>Mirabella</surname><given-names>G</given-names></name></person-group><article-title>Happy facial expressions impair inhibitory control with respect to fearful facial expressions but only when task-relevant</article-title><source>Emotion</source><year>2022</year><volume>22</volume><fpage>142</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1037/emo0001058</pub-id><pub-id pub-id-type="pmid">34968143</pub-id>
</element-citation><mixed-citation id="mc-CR23" publication-type="journal">Mancini, C., Falciati, L., Maioli, C. &#x00026; Mirabella, G. Happy facial expressions impair inhibitory control with respect to fearful facial expressions but only when task-relevant. <italic>Emotion</italic><bold>22</bold>, 142&#x02013;152. 10.1037/emo0001058 (2022).<pub-id pub-id-type="pmid">34968143</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Mirabella</surname><given-names>G</given-names></name><name><surname>Grassi</surname><given-names>M</given-names></name><name><surname>Mezzarobba</surname><given-names>S</given-names></name><name><surname>Bernardis</surname><given-names>P</given-names></name></person-group><article-title>Angry and happy expressions affect forward gait initiation only when task relevant</article-title><source>Emotion</source><year>2023</year><volume>23</volume><fpage>387</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1037/emo0001112</pub-id><pub-id pub-id-type="pmid">35588387</pub-id>
</element-citation><mixed-citation id="mc-CR24" publication-type="journal">Mirabella, G., Grassi, M., Mezzarobba, S. &#x00026; Bernardis, P. Angry and happy expressions affect forward gait initiation only when task relevant. <italic>Emotion</italic><bold>23</bold>, 387&#x02013;399. 10.1037/emo0001112 (2023).<pub-id pub-id-type="pmid">35588387</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>FCM</given-names></name><name><surname>Kubiak</surname><given-names>T</given-names></name><name><surname>Siewert</surname><given-names>K</given-names></name><name><surname>Weber</surname><given-names>H</given-names></name></person-group><article-title>Cardiac vagal tone is associated with social engagement and self-regulation</article-title><source>Biol. Psychol.</source><year>2013</year><volume>93</volume><fpage>279</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2013.02.013</pub-id><pub-id pub-id-type="pmid">23466587</pub-id>
</element-citation><mixed-citation id="mc-CR25" publication-type="journal">Geisler, F. C. M., Kubiak, T., Siewert, K. &#x00026; Weber, H. Cardiac vagal tone is associated with social engagement and self-regulation. <italic>Biol. Psychol.</italic><bold>93</bold>, 279&#x02013;286. 10.1016/j.biopsycho.2013.02.013 (2013).<pub-id pub-id-type="pmid">23466587</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Emotional Intelligence&#x02014;Peter Salovey, John D. Mayer, 1990. Available online: 10.2190/DUGG-P24E-52WK-6CDG. Accessed 15 Nov 2024.</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Mayer, J. D., Salovey, P., Caruso, D. R. &#x00026; Cherkasskiy, L. Emotional intelligence. In <italic>The Cambridge Handbook of Intelligence</italic>. Cambridge Handbooks in Psychology, 528&#x02013;549 (Cambridge University Press, 2011). ISBN 978-0-521-73911-5.</mixed-citation></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Lang</surname><given-names>P</given-names></name><name><surname>Bradley</surname><given-names>MM</given-names></name></person-group><article-title>The international affective picture system (IAPS) in the study of emotion and attention</article-title><source>Hand. Emot. Elicitation Assess.</source><year>2007</year><volume>29</volume><fpage>70</fpage><lpage>73</lpage></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Lang, P. &#x00026; Bradley, M. M. The international affective picture system (IAPS) in the study of emotion and attention. <italic>Hand. Emot. Elicitation Assess.</italic><bold>29</bold>, 70&#x02013;73 (2007).</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Tautkute, I., Trzcinski, T. &#x00026; Bielski, A. <italic>I Know How You Feel: Emotion Recognition with Facial Landmarks</italic>, 1878&#x02013;1880 (2018).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Mehrabian, A. <italic>Communication Without Words | 15 | v2 | Communication Theory | Albert</italic>.</mixed-citation></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>De Gelder</surname><given-names>B</given-names></name></person-group><article-title>Why bodies? Twelve reasons for including bodily expressions in affective neuroscience</article-title><source>Philos. Trans. R. Soc. B</source><year>2009</year><volume>364</volume><fpage>3475</fpage><lpage>3484</lpage><pub-id pub-id-type="doi">10.1098/rstb.2009.0190</pub-id></element-citation><mixed-citation id="mc-CR31" publication-type="journal">De Gelder, B. Why bodies? Twelve reasons for including bodily expressions in affective neuroscience. <italic>Philos. Trans. R. Soc. B</italic><bold>364</bold>, 3475&#x02013;3484. 10.1098/rstb.2009.0190 (2009).</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>De Gelder</surname><given-names>B</given-names></name><name><surname>Van Den Stock</surname><given-names>J</given-names></name><name><surname>Meeren</surname><given-names>HKM</given-names></name><name><surname>Sinke</surname><given-names>CBA</given-names></name><name><surname>Kret</surname><given-names>ME</given-names></name><name><surname>Tamietto</surname><given-names>M</given-names></name></person-group><article-title>Standing up for the body. Recent progress in uncovering the networks involved in the perception of bodies and bodily expressions</article-title><source>Neurosci. Biobehav. Rev.</source><year>2010</year><volume>34</volume><fpage>513</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2009.10.008</pub-id><pub-id pub-id-type="pmid">19857515</pub-id>
</element-citation><mixed-citation id="mc-CR32" publication-type="journal">De Gelder, B. et al. Standing up for the body. Recent progress in uncovering the networks involved in the perception of bodies and bodily expressions. <italic>Neurosci. Biobehav. Rev.</italic><bold>34</bold>, 513&#x02013;527. 10.1016/j.neubiorev.2009.10.008 (2010).<pub-id pub-id-type="pmid">19857515</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Arabian</surname><given-names>H</given-names></name><name><surname>Battistel</surname><given-names>A</given-names></name><name><surname>Chase</surname><given-names>JG</given-names></name><name><surname>Moeller</surname><given-names>K</given-names></name></person-group><article-title>Attention-guided network model for image-based emotion recognition</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><fpage>10179</fpage><pub-id pub-id-type="doi">10.3390/app131810179</pub-id></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Arabian, H., Battistel, A., Chase, J. G. &#x00026; Moeller, K. Attention-guided network model for image-based emotion recognition. <italic>Appl. Sci.</italic><bold>13</bold>, 10179. 10.3390/app131810179 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Sepas-Moghaddam, A., Etemad, A., Pereira, F. &#x00026; Correia, P. L. Facial emotion recognition using light field images with deep attention-based bidirectional LSTM. In <italic>Proceedings of the ICASSP 2020&#x02014;2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic>, 3367&#x02013;3371 (2020).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Khaireddin, Y. &#x00026; Chen, Z. <italic>Facial Emotion Recognition: State of the Art Performance on FER2013</italic> (2021).</mixed-citation></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name><surname>Mehendale</surname><given-names>N</given-names></name></person-group><article-title>Facial emotion recognition using convolutional neural networks (FERC)</article-title><source>SN Appl. Sci.</source><year>2020</year><volume>2</volume><fpage>446</fpage><pub-id pub-id-type="doi">10.1007/s42452-020-2234-1</pub-id></element-citation><mixed-citation id="mc-CR36" publication-type="journal">Mehendale, N. Facial emotion recognition using convolutional neural networks (FERC). <italic>SN Appl. Sci.</italic><bold>2</bold>, 446. 10.1007/s42452-020-2234-1 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Zhao, X., Liang, X., Liu, L., Li, T., Han, Y., Vasconcelos, N. &#x00026; Yan, S. <italic>Peak-Piloted Deep Network for Facial Expression Recognition</italic> (2017).</mixed-citation></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Lu</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>D</given-names></name></person-group><article-title>Facial expression recognition in the wild using multi-level features and attention mechanisms</article-title><source>IEEE Trans. Affect. Comput.</source><year>2023</year><volume>14</volume><fpage>451</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2020.3031602</pub-id></element-citation><mixed-citation id="mc-CR38" publication-type="journal">Li, Y., Lu, G., Li, J., Zhang, Z. &#x00026; Zhang, D. Facial expression recognition in the wild using multi-level features and attention mechanisms. <italic>IEEE Trans. Affect. Comput.</italic><bold>14</bold>, 451&#x02013;462. 10.1109/TAFFC.2020.3031602 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name><surname>Tarnowski</surname><given-names>P</given-names></name><name><surname>Ko&#x00142;odziej</surname><given-names>M</given-names></name><name><surname>Majkowski</surname><given-names>A</given-names></name><name><surname>Rak</surname><given-names>RJ</given-names></name></person-group><article-title>Emotion recognition using facial expressions</article-title><source>Procedia Comput. Sci.</source><year>2017</year><volume>108</volume><fpage>1175</fpage><lpage>1184</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2017.05.025</pub-id></element-citation><mixed-citation id="mc-CR39" publication-type="journal">Tarnowski, P., Ko&#x00142;odziej, M., Majkowski, A. &#x00026; Rak, R. J. Emotion recognition using facial expressions. <italic>Procedia Comput. Sci.</italic><bold>108</bold>, 1175&#x02013;1184. 10.1016/j.procs.2017.05.025 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Gao</surname><given-names>Q</given-names></name><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Yu</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name></person-group><article-title>Facial expression recognition based on electroencephalogram and facial landmark localization</article-title><source>THC</source><year>2019</year><volume>27</volume><fpage>373</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.3233/THC-181538</pub-id></element-citation><mixed-citation id="mc-CR40" publication-type="journal">Li, D. et al. Facial expression recognition based on electroencephalogram and facial landmark localization. <italic>THC</italic><bold>27</bold>, 373&#x02013;387. 10.3233/THC-181538 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name><surname>Siam</surname><given-names>AI</given-names></name><name><surname>Soliman</surname><given-names>NF</given-names></name><name><surname>Algarni</surname><given-names>AD</given-names></name><name><surname>Abd El-Samie</surname><given-names>FE</given-names></name><name><surname>Sedik</surname><given-names>A</given-names></name></person-group><article-title>Deploying machine learning techniques for human emotion detection</article-title><source>Comput. Intell. Neurosci.</source><year>2022</year><volume>2022</volume><fpage>8032673</fpage><pub-id pub-id-type="doi">10.1155/2022/8032673</pub-id><pub-id pub-id-type="pmid">35154306</pub-id>
</element-citation><mixed-citation id="mc-CR41" publication-type="journal">Siam, A. I., Soliman, N. F., Algarni, A. D., Abd El-Samie, F. E. &#x00026; Sedik, A. Deploying machine learning techniques for human emotion detection. <italic>Comput. Intell. Neurosci.</italic><bold>2022</bold>, 8032673. 10.1155/2022/8032673 (2022).<pub-id pub-id-type="pmid">35154306</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="book"><person-group person-group-type="author"><name><surname>Besel</surname><given-names>LDS</given-names></name></person-group><source>Empathy: The Role of Facial Expression Recognition</source><year>2006</year><publisher-name>University of British Columbia</publisher-name></element-citation><mixed-citation id="mc-CR42" publication-type="book">Besel, L. D. S. <italic>Empathy: The Role of Facial Expression Recognition</italic> (University of British Columbia, 2006).</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Hess</surname><given-names>U</given-names></name></person-group><article-title>Who to whom and why: The social nature of emotional mimicry</article-title><source>Psychophysiology</source><year>2021</year><volume>58</volume><fpage>e13675</fpage><pub-id pub-id-type="doi">10.1111/psyp.13675</pub-id><pub-id pub-id-type="pmid">32915999</pub-id>
</element-citation><mixed-citation id="mc-CR43" publication-type="journal">Hess, U. Who to whom and why: The social nature of emotional mimicry. <italic>Psychophysiology</italic><bold>58</bold>, e13675. 10.1111/psyp.13675 (2021).<pub-id pub-id-type="pmid">32915999</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Majumder, N., Hong, P., Peng, S., Lu, J., Ghosal, D., Gelbukh, A., Mihalcea, R. &#x00026; Poria, S. <italic>MIME: MIMicking Emotions for Empathetic Response Generation</italic> (2020).</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Ekman, P. &#x00026; Friesen, W. V. Facial action coding system. In <italic>Environmental Psychology and Nonverbal Behavior</italic> (1978).</mixed-citation></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>G</given-names></name><name><surname>Huang</surname><given-names>X</given-names></name><name><surname>Taini</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>SZ</given-names></name><name><surname>Pietik&#x000e4;Inen</surname><given-names>M</given-names></name></person-group><article-title>Facial expression recognition from near-infrared videos</article-title><source>Image Vis. Comput.</source><year>2011</year><volume>29</volume><fpage>607</fpage><lpage>619</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2011.07.002</pub-id></element-citation><mixed-citation id="mc-CR46" publication-type="journal">Zhao, G., Huang, X., Taini, M., Li, S. Z. &#x00026; Pietik&#x000e4;Inen, M. Facial expression recognition from near-infrared videos. <italic>Image Vis. Comput.</italic><bold>29</bold>, 607&#x02013;619 (2011).</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Ebner</surname><given-names>NC</given-names></name><name><surname>Riediger</surname><given-names>M</given-names></name><name><surname>Lindenberger</surname><given-names>U</given-names></name></person-group><article-title>FACES&#x02014;A database of facial expressions in young, middle-aged, and older women and men: development and validation</article-title><source>Behav. Res. Methods</source><year>2010</year><volume>42</volume><fpage>351</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.3758/BRM.42.1.351</pub-id><pub-id pub-id-type="pmid">20160315</pub-id>
</element-citation><mixed-citation id="mc-CR47" publication-type="journal">Ebner, N. C., Riediger, M. &#x00026; Lindenberger, U. FACES&#x02014;A database of facial expressions in young, middle-aged, and older women and men: development and validation. <italic>Behav. Res. Methods</italic><bold>42</bold>, 351&#x02013;362. 10.3758/BRM.42.1.351 (2010).<pub-id pub-id-type="pmid">20160315</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="journal"><person-group person-group-type="author"><name><surname>Brewer</surname><given-names>R</given-names></name><name><surname>Biotti</surname><given-names>F</given-names></name><name><surname>Catmur</surname><given-names>C</given-names></name><name><surname>Press</surname><given-names>C</given-names></name><name><surname>Happ&#x000e9;</surname><given-names>F</given-names></name><name><surname>Cook</surname><given-names>R</given-names></name><name><surname>Bird</surname><given-names>G</given-names></name></person-group><article-title>Can neurotypical individuals read autistic facial expressions? Atypical production of emotional facial expressions in autism spectrum disorders</article-title><source>Autism Res.</source><year>2016</year><volume>9</volume><fpage>262</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1002/aur.1508</pub-id><pub-id pub-id-type="pmid">26053037</pub-id>
</element-citation><mixed-citation id="mc-CR48" publication-type="journal">Brewer, R. et al. Can neurotypical individuals read autistic facial expressions? Atypical production of emotional facial expressions in autism spectrum disorders. <italic>Autism Res.</italic><bold>9</bold>, 262&#x02013;271. 10.1002/aur.1508 (2016).<pub-id pub-id-type="pmid">26053037</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Lugaresi, C., Tang, J., Nash, H., McClanahan, C., Uboweja, E., Hays, M., Zhang, F., Chang, C.-L., Yong, M. &#x00026; Lee, J. Mediapipe: A framework for perceiving and processing reality. In <italic>Proceedings of the Third workshop on computer vision for AR/VR at IEEE computer vision and pattern recognition (CVPR)</italic>, vol. 2019 (2019).</mixed-citation></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Arabian</surname><given-names>H</given-names></name><name><surname>Abdulbaki Alshirbaji</surname><given-names>T</given-names></name><name><surname>Chase</surname><given-names>JG</given-names></name><name><surname>Moeller</surname><given-names>K</given-names></name></person-group><article-title>Emotion recognition beyond pixels: Leveraging facial point landmark meshes</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><fpage>3358</fpage><pub-id pub-id-type="doi">10.3390/app14083358</pub-id></element-citation><mixed-citation id="mc-CR50" publication-type="journal">Arabian, H., Abdulbaki Alshirbaji, T., Chase, J. G. &#x00026; Moeller, K. Emotion recognition beyond pixels: Leveraging facial point landmark meshes. <italic>Appl. Sci.</italic><bold>14</bold>, 3358. 10.3390/app14083358 (2024).</mixed-citation></citation-alternatives></ref></ref-list></back></article>