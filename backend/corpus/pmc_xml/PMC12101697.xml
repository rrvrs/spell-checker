<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408320</article-id><article-id pub-id-type="pmc">PMC12101697</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0321209</article-id><article-id pub-id-type="publisher-id">PONE-D-24-45779</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Cardiovascular Anatomy</subject><subj-group><subject>Heart</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Cardiovascular Anatomy</subject><subj-group><subject>Heart</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Thermodynamics</subject><subj-group><subject>Entropy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Bioassays and Physiological Analysis</subject><subj-group><subject>Electrophysiological Techniques</subject><subj-group><subject>Cardiac Electrophysiology</subject><subj-group><subject>Electrocardiography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Medical Conditions</subject><subj-group><subject>Cardiovascular Diseases</subject><subj-group><subject>Congenital Heart Defects</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Cardiology</subject><subj-group><subject>Cardiovascular Medicine</subject><subj-group><subject>Cardiovascular Diseases</subject><subj-group><subject>Congenital Heart Defects</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Medical Conditions</subject><subj-group><subject>Congenital Disorders</subject><subj-group><subject>Birth Defects</subject><subj-group><subject>Congenital Heart Defects</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Developmental Biology</subject><subj-group><subject>Morphogenesis</subject><subj-group><subject>Birth Defects</subject><subj-group><subject>Congenital Heart Defects</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Medical Conditions</subject><subj-group><subject>Cardiovascular Diseases</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Cardiology</subject><subj-group><subject>Cardiovascular Medicine</subject><subj-group><subject>Cardiovascular Diseases</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject><subj-group><subject>Support Vector Machines</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Diagnostic Medicine</subject><subj-group><subject>Auscultation</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Advanced heart disease classification based on multi-channel heart sound coupling features</article-title><alt-title alt-title-type="running-head">Advanced heart sound classification</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Fang</surname><given-names>Yu</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6743-0959</contrib-id><name><surname>Liu</surname><given-names>Dongbo</given-names></name><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Guo</surname><given-names>Zijian</given-names></name><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Leng</surname><given-names>Hongxia</given-names></name><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Xing</given-names></name><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Xiaochen</given-names></name><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>School of Electrical Engineering and Electronic Information, Xihua University, Chengdu, China</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Department of Cardiovascular, General Hospital of Western Command Theater, Chengdu, China</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Alqudah</surname><given-names>Ali Mohammad</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>University of Manitoba, CANADA</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>liudb@mail.xhu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0321209</elocation-id><history><date date-type="received"><day>23</day><month>10</month><year>2024</year></date><date date-type="accepted"><day>3</day><month>3</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Fang et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Fang et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0321209.pdf"/><abstract><p>Conventional heart sound classification methods often rely on single-channel, one-dimensional feature extraction, which inadequately captures pathological relationships across different auscultation zones, thereby limiting the accuracy of heart disease detection. To address this issue, a novel classification framework based on multi-channel heart sound coupling feature extraction is proposed to enhance heart disease identification. This approach begins with denoising preprocessing applied to four-channel heart sound signals and a single-channel electrocardiogram. These five-channel signals are systematically paired to extract five types of coupling features, resulting in 130 distinct features per multi-channel sample. The ReliefF algorithm is then used to evaluate feature importance, retaining the top 20% of features to construct a coupling feature set. A convolutional neural network is employed to classify normal and abnormal heart sounds. When applied to clinical congenital heart disease datasets, the proposed method achieved a classification accuracy of 95.6%, while on the PhysioNet heart sound challenge dataset, it reached an accuracy of 98.3%. Experimental results demonstrate that compared to single-channel, one-dimensional features, multi-channel coupling features more effectively capture pathological characteristics in heart sound signals, significantly improving the accuracy of heart disease classification and addressing challenges in the refined categorization of cardiac conditions.</p></abstract><funding-group><funding-statement>The author(s) received no specific funding for this work.</funding-statement></funding-group><counts><fig-count count="13"/><table-count count="4"/><page-count count="21"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>Data is available from <ext-link xlink:href="https://www.physionet.org/content/circor-heart-sound/1.0.3/" ext-link-type="uri">https://www.physionet.org/content/circor-heart-sound/1.0.3/</ext-link>. For data requests or inquiries, we provide a non-author point of contact, please contact Hanbing Wang, email address: Hbin_1@163.com.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>Data is available from <ext-link xlink:href="https://www.physionet.org/content/circor-heart-sound/1.0.3/" ext-link-type="uri">https://www.physionet.org/content/circor-heart-sound/1.0.3/</ext-link>. For data requests or inquiries, we provide a non-author point of contact, please contact Hanbing Wang, email address: Hbin_1@163.com.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Cardiovascular diseases continue to be among the leading causes of mortality worldwide [<xref rid="pone.0321209.ref001" ref-type="bibr">1</xref>]. Although many cardiac conditions can be managed through pharmacological treatments or surgical interventions, the early detection and monitoring of congenital heart disease (CHD) remain critical. Without timely intervention, CHD can result in significant long-term health complications, such as heart failure, pulmonary hypertension, and life-threatening arrhythmias. Therefore, regular medical follow-ups and continuous monitoring are essential for CHD patients.</p><p>Heart sounds, produced by the movement of blood during the cardiac cycle, mainly comprise two key components: the first heart sound (S1) and the second heart sound (S2). These sounds correspond to the transitions between diastole and systole, and the beginning of diastole following systole, respectively [<xref rid="pone.0321209.ref002" ref-type="bibr">2</xref>]. In addition to these core sounds, murmurs may arise from abnormalities, including valvular dysfunction, valve malformation, myocardial hypertrophy, or ischemia, signaling potential cardiac issues [<xref rid="pone.0321209.ref003" ref-type="bibr">3</xref>]. By analyzing the characteristics of heart sounds&#x02014;such as the presence of murmurs, their timing, intensity, and quality&#x02014;clinicians can assess heart health and identify possible diseases. Variations in sound intensity or frequency often reflect changes in myocardial contractility or valvular function, enabling initial evaluations through auscultation [<xref rid="pone.0321209.ref004" ref-type="bibr">4</xref>].</p><p>During clinical evaluations, several auscultation sites are examined, including the aortic, pulmonary, tricuspid, and mitral valve areas [<xref rid="pone.0321209.ref005" ref-type="bibr">5</xref>]. Given the anatomical and hemodynamic properties of the heart, interactions between different auscultation sites can yield valuable diagnostic information. Analyzing multi-channel signals provides a more comprehensive evaluation, improving diagnostic accuracy.</p><p>Feature extraction and classifier selection are critical elements of heart sound classification. Recent studies have introduced various techniques: Nizam NB et al. extracted Hilbert envelope features and cardiac cycle features from preprocessed heart sound signals and employed a random forest classifier to analyze the Yaseen database, achieving an average classification accuracy of 94.78% [<xref rid="pone.0321209.ref006" ref-type="bibr">6</xref>]. Deng et al. proposed a classification method based on improved Mel-frequency cepstral coefficients (MFCC) and convolutional recurrent neural networks, achieving a classification accuracy of 98% for distinguishing pathological from non-pathological heart sounds [<xref rid="pone.0321209.ref007" ref-type="bibr">7</xref>]. Deperlioglu et al. employed signal energy features and stacked autoencoders, achieving a 99.61% classification accuracy on the PASCAL-B dataset [<xref rid="pone.0321209.ref008" ref-type="bibr">8</xref>]. Singh et al. used wavelet decomposition, homomorphic filtering, and power spectral density to extract coupled features from Electrocardiogram (ECG) and Phonocardiogram (PCG), achieving 93.13% accuracy in the PhysioNet/CinC 2016 challenge [<xref rid="pone.0321209.ref009" ref-type="bibr">9</xref>]. Choudhary et al. performed a five-level signal decomposition using discrete wavelet decomposition and conducted binary classification on the PhysioNet/CinC 2016 dataset by utilizing convolutional neural networks and GRU network layers [<xref rid="pone.0321209.ref010" ref-type="bibr">10</xref>]. Liu et al. introduced a heart sound classification method combining bispectrum features and the Vision Transformer model, using data from the PhysioNet Challenge 2016 and 2022 databases to enhance early cardiovascular disease diagnosis [<xref rid="pone.0321209.ref011" ref-type="bibr">11</xref>]. While many studies examine single-channel signals using one-dimensional feature analysis, this approach often overlooks crucial information present in multi-channel data. Specifically, the focus on one-dimensional, chain-like features in single channels neglects the richness of multi-channel signals, potentially leading to a loss of vital internal information.</p><p>Several studies have investigated various features of heart sound signals for cardiac disease diagnosis, including bispectral, homomorphic, Hilbert, power spectral density, wavelet envelope, and traditional MFCC features. Zeye Liu et al. proposed a classification algorithm using bispectral features and Vision Transformer models, achieving 91% accuracy on the PhysioNet Challenge 2022 dataset [<xref rid="pone.0321209.ref012" ref-type="bibr">12</xref>]. Sofia Monteiro et al. combined homomorphic, Hilbert, and wavelet envelope features with BiLSTM networks, achieving 75.1% accuracy [<xref rid="pone.0321209.ref013" ref-type="bibr">13</xref>]. Yunendah Nur Fuadah et al. applied MFCC features with various classifiers, achieving 76.31% accuracy on the 2022 heart sound dataset [<xref rid="pone.0321209.ref014" ref-type="bibr">14</xref>]. However, a common limitation is that these studies, despite utilizing multi-channel signals, neglect the inter-channel relationships and lack analysis of the coupling between them. Consequently, their classification performance remains suboptimal. Ali et al. proposed a novel methodology leveraging a hybrid approach that combines energy and entropy features to identify distinct characteristics from one-dimensional heart sound signals [<xref rid="pone.0321209.ref015" ref-type="bibr">15</xref>]. Liu et al. collected 5-minute multi-channel heart sound signals from 21 CAD patients and 15 non-CAD subjects, extracting time-domain, frequency-domain, entropy, and cross-entropy features, with a support vector machine achieving 90.92% classification accuracy [<xref rid="pone.0321209.ref016" ref-type="bibr">16</xref>]. Samanta et al. collected heart sound signals from four chest auscultation sites in CAD patients and normal subjects, extracting features such as approximate entropy, maximum Lyapunov exponent, mutual information, and path length entropy, resulting in a classification accuracy of 82.57% using an artificial neural network [<xref rid="pone.0321209.ref017" ref-type="bibr">17</xref>].While these studies analyze multi-channel heart sound datasets, they often fail to consider the coupling relationships between channels, leading to suboptimal classification performance.</p><p>Research indicates that multi-channel heart sound signals provide richer pathological information, enhancing classification performance. However, the majority of existing studies fail to fully exploit this potential by neglecting the crucial inter-channel relationships within heart sounds. Recognizing that understanding these inter-channel connections is vital for improving diagnostic accuracy and care for cardiovascular patients, this paper introduces a novel heart sound classification method. In this proposed approach, multi-channel coupling features are leveraged, and machine learning models are employed for classification. The experimental results demonstrate that a significant enhancement in classification accuracy is achieved by this method, offering clinicians a more precise and efficient diagnostic tool for cardiac diseases.</p></sec><sec id="sec002"><title>Multi-channel heart sound coupling feature extraction</title><p>The multi-channel heart sound dataset utilized in this study comprises 293 heart sound samples, including 126 normal heart sounds and 167 abnormal heart sounds. The dataset is primarily sourced from two parts. First, a cardiovascular surgery dataset from the department of cardiovascular surgery general hospital of the western theater command people&#x02019;s liberation army of china comprising 126 heart sound samples was utilized, including 18 normal heart sounds and 108 abnormal heart sounds. Second, 167 heart sound samples were selected from the PhysioNet 2022 heart sound challenge dataset, consisting of 108 normal heart sounds and 59 abnormal heart sounds.</p><table-wrap position="float" id="pone.0321209.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.t001</object-id><label>Table 1</label><caption><title>Summary of the PhysioNet 2022 heart sound challenge dataset and clinical dataset.</title></caption><alternatives><graphic xlink:href="pone.0321209.t001" id="pone.0321209.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Dataset</th><th align="left" rowspan="1" colspan="1">Sample Category</th><th align="left" rowspan="1" colspan="1">Number of Samples</th></tr></thead><tbody><tr><td align="left" rowspan="3" colspan="1">2022 Heart Sound Challenge Dataset</td><td align="left" rowspan="1" colspan="1">Normal</td><td align="left" rowspan="1" colspan="1">108</td></tr><tr><td align="left" rowspan="1" colspan="1">Abnormal</td><td align="left" rowspan="1" colspan="1">59</td></tr><tr><td align="left" rowspan="1" colspan="1">Total</td><td align="left" rowspan="1" colspan="1">167</td></tr><tr><td align="left" rowspan="6" colspan="1">Clinical Dataset</td><td align="left" rowspan="1" colspan="1">Normal Heart Sounds</td><td align="left" rowspan="1" colspan="1">18</td></tr><tr><td align="left" rowspan="1" colspan="1">Atrial Septal Defect</td><td align="left" rowspan="1" colspan="1">33</td></tr><tr><td align="left" rowspan="1" colspan="1">Ventricular Septal Defect</td><td align="left" rowspan="1" colspan="1">51</td></tr><tr><td align="left" rowspan="1" colspan="1">Patent Ductus Arteriosus</td><td align="left" rowspan="1" colspan="1">6</td></tr><tr><td align="left" rowspan="1" colspan="1">Tetralogy of Fallot</td><td align="left" rowspan="1" colspan="1">16</td></tr><tr><td align="left" rowspan="1" colspan="1">Total</td><td align="left" rowspan="1" colspan="1">126</td></tr></tbody></table></alternatives></table-wrap><p>This study was performed in line with the principles of the Declaration of Helsinki. Approval was granted by the ethics committee of General Hospital of Western Command Theater (No. 2015 research 01).</p><p>Regarding data acquisition, the signals in the cardiovascular surgery dataset are measured in volts (Volts), with a sampling rate of 20&#x02009;kHz. This dataset encompasses four-channel heart sound signals from the aortic valve (A), pulmonary valve (P), tricuspid valve (T), and mitral valve (M), along with one ECG signal. The Challenge 2022 dataset also contains the same four-channel signals, but with a sampling frequency of 4&#x02009;kHz.</p><p>The proposed algorithm consists of four steps: preprocessing, coupling feature extraction, dimensionality reduction using the ReliefF method, and classification. The overall workflow of the algorithm is illustrated in <xref rid="pone.0321209.g001" ref-type="fig">Fig 1</xref>.</p><fig position="float" id="pone.0321209.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g001</object-id><label>Fig 1</label><caption><title>Overall flowchart of the algorithm.</title></caption><graphic xlink:href="pone.0321209.g001" position="float"/></fig><p>Initially, a Butterworth filter is employed to denoise the multi-channel heart sound signals. The filter cutoff frequency is set between 4.85 Hz and 1250 Hz, allowing the retention of effective heart sound information while removing noise or interference within specific frequency ranges [<xref rid="pone.0321209.ref018" ref-type="bibr">18</xref>,<xref rid="pone.0321209.ref019" ref-type="bibr">19</xref>].</p><p>Subsequently, a discrete wavelet transform algorithm [<xref rid="pone.0321209.ref020" ref-type="bibr">20</xref>] is applied to detect and remove the trend components from the ECG signal. The Daubechies wavelet is selected, and the original ECG signal undergoes a ten-level decomposition. To eliminate long-term trends, the approximate coefficients of the tenth level are discarded. The ECG signal is denoted as <inline-formula id="pone.0321209.e001"><alternatives><graphic xlink:href="pone.0321209.e001.jpg" id="pone.0321209.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pone.0321209.e002"><alternatives><graphic xlink:href="pone.0321209.e002.jpg" id="pone.0321209.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, with <italic toggle="yes">N</italic> representing the signal duration. The formula for the discrete wavelet transform is expressed as follows:</p><disp-formula id="pone.0321209.e003"><alternatives><graphic xlink:href="pone.0321209.e003.jpg" id="pone.0321209.e003g" position="anchor"/><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mstyle></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>&#x000b7;</mml:mi><mml:msub><mml:mi>&#x003c8;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>(1)</label></disp-formula><p>where <italic toggle="yes">W</italic><sub><italic toggle="yes">j</italic>,<italic toggle="yes">k</italic></sub> represents the wavelet coefficient at level <italic toggle="yes">j</italic> and position <italic toggle="yes">k</italic>, and <inline-formula id="pone.0321209.e004"><alternatives><graphic xlink:href="pone.0321209.e004.jpg" id="pone.0321209.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c8;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the translated discrete wavelet basis function. The filter coefficient sequence associated with the wavelet decomposition is denoted as <inline-formula id="pone.0321209.e005"><alternatives><graphic xlink:href="pone.0321209.e005.jpg" id="pone.0321209.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and <italic toggle="yes">n</italic> indicates the sample points of the signal. <xref rid="pone.0321209.g002" ref-type="fig">Fig 2</xref> illustrates the spatial partitioning diagram of wavelet decomposition, demonstrating the three-level decomposition process, which can be similarly extended for multiple levels. The original ECG signal is decomposed into high-frequency (D) and low-frequency (A) components, with the numerical values indicating the level of decomposition.</p><fig position="float" id="pone.0321209.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g002</object-id><label>Fig 2</label><caption><title>Diagram of wavelet decomposition.</title></caption><graphic xlink:href="pone.0321209.g002" position="float"/></fig><p>To eliminate noise interference from the ECG signal, a Butterworth bandpass filter with cutoff frequencies of 5 Hz and 15 Hz is utilized. The de-trended electrocardiogram and the filtered ECG signals are depicted in <xref rid="pone.0321209.g003" ref-type="fig">Fig 3</xref>. Compared to the original ECG signal, the processed signal exhibits a smoother waveform, with the R-peaks and other key features becoming more pronounced and clearly visible.</p><fig position="float" id="pone.0321209.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g003</object-id><label>Fig 3</label><caption><title>Electrocardiogram signal preprocessing diagram.</title></caption><graphic xlink:href="pone.0321209.g003" position="float"/></fig><p>The clinical dataset used in this study comprises four channels of heart sound signals corresponding to the auscultation areas of the aortic valve (A), pulmonary valve (P), tricuspid valve (T), and mitral valve (M), along with one channel of ECG signal (denoted as E). By extracting coupling features from different pairs of channels, a total of 10 combination forms are obtained: AP, AT, AM, AE, PT, PM, PE, TM, TE, and ME. Since coupling features are extracted from every pair of signals among the five channels, the number of combinations is calculated using the combination formula <inline-formula id="pone.0321209.e006"><alternatives><graphic xlink:href="pone.0321209.e006.jpg" id="pone.0321209.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">(</mml:mo><mml:mfrac linethickness="0"><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo maxsize="2.047em" minsize="2.047em">)</mml:mo><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. The combination forms are illustrated in <xref rid="pone.0321209.g004" ref-type="fig">Fig 4</xref>. For each combination, 13 coupling features are extracted. Each coupling method is applied separately to the multi-channel dataset, resulting in a total of 130 coupling features.</p><fig position="float" id="pone.0321209.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g004</object-id><label>Fig 4</label><caption><title>Multichannel heart sound coupling combinations diagram.</title></caption><graphic xlink:href="pone.0321209.g004" position="float"/></fig><sec id="sec003"><title>Mutual entropy analysis</title><p>Mutual Sample Entropy (SamEn) is a technique used to analyze the asynchrony between two correlated time series [<xref rid="pone.0321209.ref021" ref-type="bibr">21</xref>]. The more regular the coupling pattern between the two sequences, the lower the value of mutual sample entropy, and conversely, the more complex the pattern, the higher the entropy value. It extends the design concept of sample entropy but focuses on comparing sequence segments from one time series with corresponding segments in another, thereby measuring the level of synchrony or consistency between the two time series. Mutual sample entropy evaluates the temporal dynamic characteristics of two signals by calculating the probability that paired segments from the two normalized heart sound channels, denoted as <inline-formula id="pone.0321209.e007"><alternatives><graphic xlink:href="pone.0321209.e007.jpg" id="pone.0321209.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321209.e008"><alternatives><graphic xlink:href="pone.0321209.e008.jpg" id="pone.0321209.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, remain similar under a given distance threshold.</p><p>Let <inline-formula id="pone.0321209.e009"><alternatives><graphic xlink:href="pone.0321209.e009.jpg" id="pone.0321209.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321209.e010"><alternatives><graphic xlink:href="pone.0321209.e010.jpg" id="pone.0321209.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> represent the reconstructed state space of two signals:</p><disp-formula id="pone.0321209.e011">
<alternatives><graphic xlink:href="pone.0321209.e011.jpg" id="pone.0321209.e011g" position="anchor"/><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><disp-formula id="pone.0321209.e012">
<alternatives><graphic xlink:href="pone.0321209.e012.jpg" id="pone.0321209.e012g" position="anchor"/><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>where <italic toggle="yes">m</italic> is the embedding dimension, and <italic toggle="yes">r</italic> is the threshold parameter. The calculation steps for mutual sample entropy are as follows:</p><p>First, the reconstructed state space is used to compute:</p><disp-formula id="pone.0321209.e013">
<alternatives><graphic xlink:href="pone.0321209.e013.jpg" id="pone.0321209.e013g" position="anchor"/><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover></mml:mstyle><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>where <inline-formula id="pone.0321209.e014"><alternatives><graphic xlink:href="pone.0321209.e014.jpg" id="pone.0321209.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the Heaviside function, and <italic toggle="yes">d</italic><sup><italic toggle="yes">m</italic></sup> represents the distance between two segments. The average value of <inline-formula id="pone.0321209.e015"><alternatives><graphic xlink:href="pone.0321209.e015.jpg" id="pone.0321209.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is computed as:</p><disp-formula id="pone.0321209.e016">
<alternatives><graphic xlink:href="pone.0321209.e016.jpg" id="pone.0321209.e016g" position="anchor"/><mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover></mml:mstyle><mml:msubsup><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>The mutual sample entropy is then given by:</p><disp-formula id="pone.0321209.e017">
<alternatives><graphic xlink:href="pone.0321209.e017.jpg" id="pone.0321209.e017g" position="anchor"/><mml:math id="M17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p><xref rid="pone.0321209.g005" ref-type="fig">Fig 5</xref> shows the box plots of mutual sample entropy for all channel combinations of normal and abnormal heart sound signals in the clinical dataset. The distribution of mutual sample entropy for normal heart sound signals is more concentrated compared to that of abnormal heart sound signals. As illustrated in <xref rid="pone.0321209.g005" ref-type="fig">Fig 5</xref>, there are significant differences in the overall coupling trends between normal and abnormal features, particularly in the first three and last three values along the x-axis.</p><fig position="float" id="pone.0321209.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g005</object-id><label>Fig 5</label><caption><title>Box plots of mutual sample entropy for heart sound signals.</title></caption><graphic xlink:href="pone.0321209.g005" position="float"/></fig><p>Mutual Fuzzy Entropy (FuzEn) is often used to assess the complexity of time series [<xref rid="pone.0321209.ref022" ref-type="bibr">22</xref>]. It measures the synchrony or similarity between different signals and, compared to sample entropy, is more effective for handling nonlinear, non-Gaussian, and short data records. By applying fuzzy set theory to similarity functions, mutual fuzzy entropy is less sensitive to noise, thereby enhancing robustness. The primary difference from mutual sample entropy lies in the use of a Gaussian function instead of the Heaviside function, reducing the strict requirement on the threshold <italic toggle="yes">r</italic>. The mutual fuzzy entropy is calculated as:</p><disp-formula id="pone.0321209.e018">
<alternatives><graphic xlink:href="pone.0321209.e018.jpg" id="pone.0321209.e018g" position="anchor"/><mml:math id="M18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>j</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mi>exp</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfrac><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>where <italic toggle="yes">m</italic> is the embedding dimension and <italic toggle="yes">r</italic> is the threshold parameter.</p><p>Joint Distribution Entropy (JDistEn) is a measure used to quantify the uncertainty in the joint distribution of multidimensional random variables, revealing the complex dependencies between multichannel data [<xref rid="pone.0321209.ref023" ref-type="bibr">23</xref>]. This fundamentally addresses the entropy dependence on <italic toggle="yes">r</italic>, providing a comprehensive measure of coupling between channels. JDistEn is defined based on the empirical probability density function approximated by a histogram with <italic toggle="yes">B</italic> bins:</p><disp-formula id="pone.0321209.e019">
<alternatives><graphic xlink:href="pone.0321209.e019.jpg" id="pone.0321209.e019g" position="anchor"/><mml:math id="M19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>J</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>log</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>where <italic toggle="yes">p</italic><sub><italic toggle="yes">t</italic></sub> represents the probability of each bin <italic toggle="yes">t</italic>.</p></sec><sec id="sec004"><title>Phase locking value</title><p>Phase Locking Value (PLV) is a commonly used metric for quantifying the phase synchrony between two signals at a specific frequency. It reflects whether the instantaneous phases of two signals remain consistent over a specific time period or within a series of time windows. PLV is widely applied in fields such as neuroscience, ECG analysis, and other domains to study the dynamic interactions between physiological signals. The phase of a signal typically represents a specific part of its periodic variation, such as the rising edge, peak, or other significant points of a sine wave. If two periodic signals are aligned at the same point in each cycle, they are considered phase-locked.</p><p>The calculation of PLV between signals <inline-formula id="pone.0321209.e020"><alternatives><graphic xlink:href="pone.0321209.e020.jpg" id="pone.0321209.e020g" position="anchor"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321209.e021"><alternatives><graphic xlink:href="pone.0321209.e021.jpg" id="pone.0321209.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> involves several steps:</p><p>First, the analytic signals of <inline-formula id="pone.0321209.e022"><alternatives><graphic xlink:href="pone.0321209.e022.jpg" id="pone.0321209.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321209.e023"><alternatives><graphic xlink:href="pone.0321209.e023.jpg" id="pone.0321209.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> are obtained via the Hilbert transform:</p><disp-formula id="pone.0321209.e024">
<alternatives><graphic xlink:href="pone.0321209.e024.jpg" id="pone.0321209.e024g" position="anchor"/><mml:math id="M24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mi>H</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mi>H</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>where <italic toggle="yes">H</italic> represents the Hilbert transform operation that generates a signal orthogonal to the original signal.</p><p>The instantaneous phases are then extracted:</p><disp-formula id="pone.0321209.e025">
<alternatives><graphic xlink:href="pone.0321209.e025.jpg" id="pone.0321209.e025g" position="anchor"/><mml:math id="M25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">arg</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">arg</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>The phase difference is computed for each time point <inline-formula id="pone.0321209.e026"><alternatives><graphic xlink:href="pone.0321209.e026.jpg" id="pone.0321209.e026g" position="anchor"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>:</p><disp-formula id="pone.0321209.e027">
<alternatives><graphic xlink:href="pone.0321209.e027.jpg" id="pone.0321209.e027g" position="anchor"/><mml:math id="M27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>Finally, the PLV is calculated by averaging the phase differences over all time points:</p><disp-formula id="pone.0321209.e028">
<alternatives><graphic xlink:href="pone.0321209.e028.jpg" id="pone.0321209.e028g" position="anchor"/><mml:math id="M28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">|</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo fence="true" form="postfix" stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>The PLV ranges from 0 to 1, where a value close to 1 indicates strong phase consistency between the two signals, while a value close to 0 suggests little or no phase synchrony. <xref rid="pone.0321209.g006" ref-type="fig">Fig 6</xref> illustrates the PLV values for different channel combinations of normal and abnormal heart sound signals from the clinical dataset. As shown in <xref rid="pone.0321209.g006" ref-type="fig">Fig 6</xref>, although the trends of the PLV values for normal and abnormal heart sound types are quite similar, the PLV values for normal heart sounds are more spread out across the first three coordinates on the x-axis, while the values for abnormal heart sounds are concentrated within the 0.1-0.25 range.</p><fig position="float" id="pone.0321209.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g006</object-id><label>Fig 6</label><caption><title>Box plots of PLV values for heart sound signals.</title></caption><graphic xlink:href="pone.0321209.g006" position="float"/></fig></sec><sec id="sec005"><title>Magnitude squared coherence</title><p>Magnitude Squared Coherence (MSC) is a frequency-domain statistic used to evaluate the linear correlation between two signals in terms of their frequency components. It quantifies the strength or consistency of synchronized variations across frequencies between two heart sound channels. The MSC is calculated using the following formula:</p><disp-formula id="pone.0321209.e029">
<alternatives><graphic xlink:href="pone.0321209.e029.jpg" id="pone.0321209.e029g" position="anchor"/><mml:math id="M29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>where <italic toggle="yes">P</italic><sub><italic toggle="yes">xx</italic></sub>(<italic toggle="yes">f</italic>) and <italic toggle="yes">P</italic><sub><italic toggle="yes">yy</italic></sub>(<italic toggle="yes">f</italic>) are the power spectral densities of <inline-formula id="pone.0321209.e030"><alternatives><graphic xlink:href="pone.0321209.e030.jpg" id="pone.0321209.e030g" position="anchor"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321209.e031"><alternatives><graphic xlink:href="pone.0321209.e031.jpg" id="pone.0321209.e031g" position="anchor"/><mml:math id="M31" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, respectively, and <italic toggle="yes">P</italic><sub><italic toggle="yes">xy</italic></sub>(<italic toggle="yes">f</italic>) is the cross-power spectral density of <inline-formula id="pone.0321209.e032"><alternatives><graphic xlink:href="pone.0321209.e032.jpg" id="pone.0321209.e032g" position="anchor"/><mml:math id="M32" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321209.e033"><alternatives><graphic xlink:href="pone.0321209.e033.jpg" id="pone.0321209.e033g" position="anchor"/><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. In this study, the mean and standard deviation of MSC are extracted as coupling features for further analysis.</p></sec><sec id="sec006"><title>Waveform similarity measures</title><p>Waveform similarity measures are metrics used to assess the consistency or similarity between the shapes and patterns of multichannel signals. Heart sound signals contain fundamental information about cardiac activity, and similar patterns are produced in different channels due to the closing and opening of heart valves. Therefore, assessing the waveform similarity quality of multichannel heart sound signals provides valuable insights into the interactions between different regions of the heart. Several metrics are used to quantify waveform similarity, which can reflect coupling strength, temporal relationships, and spatial consistency, providing potential value for heart disease monitoring.</p><p>The correlation coefficient [<xref rid="pone.0321209.ref024" ref-type="bibr">24</xref>] is used to quantify the linear dependence between two heart sound signals. When two heart sound signals from different locations are correlated, they may exhibit similar patterns and fluctuations over a complete cardiac cycle. A correlation coefficient close to +1 indicates a positive correlation, meaning that the peaks and troughs of the two signals are synchronized; a value close to -1 indicates a negative correlation, where the peaks and troughs vary in opposite directions; and a value of 0 indicates no linear relationship. The formula for calculating the correlation coefficient is as follows:</p><disp-formula id="pone.0321209.e034">
<alternatives><graphic xlink:href="pone.0321209.e034.jpg" id="pone.0321209.e034g" position="anchor"/><mml:math id="M34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mover><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mover><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>where <inline-formula id="pone.0321209.e035"><alternatives><graphic xlink:href="pone.0321209.e035.jpg" id="pone.0321209.e035g" position="anchor"/><mml:math id="M35" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321209.e036"><alternatives><graphic xlink:href="pone.0321209.e036.jpg" id="pone.0321209.e036g" position="anchor"/><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> represent the mean values of <inline-formula id="pone.0321209.e037"><alternatives><graphic xlink:href="pone.0321209.e037.jpg" id="pone.0321209.e037g" position="anchor"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321209.e038"><alternatives><graphic xlink:href="pone.0321209.e038.jpg" id="pone.0321209.e038g" position="anchor"/><mml:math id="M38" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, respectively, and <italic toggle="yes">N</italic> is the number of sampling points.</p><p>Cosine similarity (CosSim) [<xref rid="pone.0321209.ref025" ref-type="bibr">25</xref>] measures the angle between two vectors representing multichannel heart sound signals, assessing the degree of difference between them. When two heart sound signals are completely identical, their cosine similarity value is maximized (equal to 1). This measure is less sensitive to the length and amplitude of the signals, primarily focusing on whether the waveforms&#x02019; patterns and structures match. The formula for cosine similarity is given by:</p><disp-formula id="pone.0321209.e039">
<alternatives><graphic xlink:href="pone.0321209.e039.jpg" id="pone.0321209.e039g" position="anchor"/><mml:math id="M39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>CosSim</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>Euclidean distance provides an intuitive measure of point-to-point differences between two heart sound signals. Simply put, it measures the root of the sum of squared differences between the amplitudes of the heart sound signals at corresponding time points. A smaller Euclidean distance indicates greater similarity between the waveforms of the two signals. The formula for Euclidean distance is:</p><disp-formula id="pone.0321209.e040">
<alternatives><graphic xlink:href="pone.0321209.e040.jpg" id="pone.0321209.e040g" position="anchor"/><mml:math id="M40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula></sec><sec id="sec007"><title>Cross power spectral density</title><p>Cross Power Spectral Density (CPSD) is an important tool for analyzing and characterizing the frequency-domain dependencies between two signals. It describes how one signal varies as a function of another signal at a given frequency. CPSD is the Fourier transform of the cross-correlation function between the two signals and is defined by the following formula:</p><disp-formula id="pone.0321209.e041">
<alternatives><graphic xlink:href="pone.0321209.e041.jpg" id="pone.0321209.e041g" position="anchor"/><mml:math id="M41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>X</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>m</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>where <inline-formula id="pone.0321209.e042"><alternatives><graphic xlink:href="pone.0321209.e042.jpg" id="pone.0321209.e042g" position="anchor"/><mml:math id="M42" display="inline" overflow="scroll"><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> denotes the complex conjugate, and <italic toggle="yes">X</italic><sub><italic toggle="yes">m</italic></sub>(<italic toggle="yes">f</italic>) and <italic toggle="yes">Y</italic><sub><italic toggle="yes">m</italic></sub>(<italic toggle="yes">f</italic>) are the discrete Fourier transforms of the <italic toggle="yes">m</italic>-th windowed segments of signals <inline-formula id="pone.0321209.e043"><alternatives><graphic xlink:href="pone.0321209.e043.jpg" id="pone.0321209.e043g" position="anchor"/><mml:math id="M43" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321209.e044"><alternatives><graphic xlink:href="pone.0321209.e044.jpg" id="pone.0321209.e044g" position="anchor"/><mml:math id="M44" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, respectively. <italic toggle="yes">M</italic> represents the number of windows. In this study, the mean and standard deviation of the real and imaginary parts of CPSD are extracted as coupling features for further analysis.</p></sec></sec><sec id="sec008"><title>Feature analysis</title><p>To evaluate the effectiveness of the extracted coupling features in distinguishing between normal and abnormal heart sound signals, a comprehensive analysis was conducted. <xref rid="pone.0321209.g007" ref-type="fig">Figs 7</xref> and <xref rid="pone.0321209.g008" ref-type="fig">8</xref> present box plots of selected coupling features for the channel pairs A-T and T-M, respectively. These features include mutual sample entropy, phase locking value, correlation coefficient, and magnitude-squared coherence, representing different aspects of the inter-channel relationships.</p><fig position="float" id="pone.0321209.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g007</object-id><label>Fig 7</label><caption><title>Box plots of coupling features between normal and abnormal signals on channels A and T.</title></caption><graphic xlink:href="pone.0321209.g007" position="float"/></fig><fig position="float" id="pone.0321209.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g008</object-id><label>Fig 8</label><caption><title>Box plots of coupling features between normal and abnormal signals on channels T and M.</title></caption><graphic xlink:href="pone.0321209.g008" position="float"/></fig><p>In <xref rid="pone.0321209.g007" ref-type="fig">Fig 7</xref>, it can be observed that the mutual sample entropy between channels A and T shows a noticeable difference between normal and abnormal signals. Specifically, abnormal heart sounds tend to exhibit lower entropy values compared to normal heart sounds. This suggests that pathological conditions may lead to a reduction in the complexity of the coupling patterns between these channels. The decreased entropy indicates more regular or predictable interactions, which could be associated with the presence of cardiac anomalies affecting the synchronization of heart sounds.</p><p>The phase locking value (PLV) between channels A and T is generally higher in abnormal signals. A higher PLV indicates stronger phase synchronization, implying that the timing of heart sound events in these channels is more consistently aligned in pathological cases. This could be due to structural or functional abnormalities in the heart that cause simultaneous alterations in the signals recorded from these auscultation sites.</p><p>Similarly, the correlation coefficient shows an increased value for abnormal heart sounds between channels A and T. This heightened linear relationship suggests that abnormal conditions may cause the signals in these channels to vary in a more similar fashion, potentially due to shared pathological influences affecting both areas of the heart.</p><p>The magnitude-squared coherence displays higher coherence levels in abnormal heart sounds across certain frequency bands. This implies that there is greater consistency in the frequency components of the signals between channels A and T when a heart disease is present. Such findings may be indicative of abnormal mechanical vibrations or turbulent blood flow patterns resulting from cardiac defects.</p><p><xref rid="pone.0321209.g008" ref-type="fig">Fig 8</xref> illustrates the coupling features between channels T and M. The mutual fuzzy entropy is observed to be lower in abnormal heart sounds. This further supports the notion that pathological heart conditions reduce the complexity and increase the regularity of inter-channel interactions. The cosine similarity between channels T and M is higher for abnormal signals, indicating greater waveform similarity. This suggests that abnormal heart conditions may cause similar alterations in the signal shapes at these two locations.</p><p>The Euclidean distance between channels T and M is generally smaller for abnormal heart sounds. A reduced Euclidean distance indicates that the amplitude differences between the signals are less pronounced, which could be due to the homogenizing effect of certain heart diseases on the mechanical activities recorded at these sites.</p><p>These observations collectively suggest that abnormal heart sounds tend to exhibit more synchronized, less complex, and more similar inter-channel relationships compared to normal heart sounds. The pathological changes in the heart may lead to alterations in the mechanical and electrical activities that are reflected in the coupling features extracted from different auscultation sites.</p><p>To further understand the discriminative power of these features, statistical tests such as the Mann-Whitney U test can be applied to compare the distributions of each feature between normal and abnormal groups. Significant differences in these features would confirm their relevance for classification purposes.</p><p>Incorporating these coupling features into machine learning models is expected to enhance classification performance. The multi-dimensional nature of the features allows the models to learn complex patterns associated with pathological conditions, potentially leading to more accurate and reliable diagnostic tools.</p></sec><sec sec-type="results" id="sec009"><title>Results</title><p>Traditional one-dimensional convolutional layers can only perform convolution operations using fixed-size filters, which limits their ability to capture feature information at different scales. Convolutional Neural Network (CNN) enhances traditional multilayer neural networks by introducing locally connected convolutional layers and pooling layers [<xref rid="pone.0321209.ref026" ref-type="bibr">26</xref>]. In this study, multidimensional convolutional layers are employed to extract features at various scales. By utilizing convolution kernels of different sizes, features at multiple scales can be simultaneously perceived and utilized. <xref rid="pone.0321209.g009" ref-type="fig">Fig 9</xref> illustrates the structure of the CNN used in this work.</p><fig position="float" id="pone.0321209.g009"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g009</object-id><label>Fig 9</label><caption><title>Structure of the convolutional neural network.</title></caption><graphic xlink:href="pone.0321209.g009" position="float"/></fig><p>The multiscale convolutional layers use convolution kernels of different sizes, including 1<inline-formula id="pone.0321209.e045"><alternatives><graphic xlink:href="pone.0321209.e045.jpg" id="pone.0321209.e045g" position="anchor"/><mml:math id="M45" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>1 convolution layers. Each convolutional layer of each size includes two convolutional layers, which more effectively extract the coupling features between different pathways of heart sound signals. Through parallel concatenation operations, their outputs are connected to obtain three separate feature maps, each representing features extracted by convolution kernels of specific sizes.</p><p>Normalization, the ReLU activation function, and max pooling are subsequently applied to further process the feature maps. These operations help to regularize the data, introduce nonlinearity, and reduce dimensionality. After the sequence concatenation operation, a fully connected layer maps the high-level feature representations to the final class labels. The final output layer employs the softmax function to obtain the predicted probabilities for each category, determining the class to which the heart sound sample belongs.</p><p>Compared to one-dimensional convolutional layers, the proposed model can better capture the temporal relationships between continuous diastolic murmurs across channels, as these murmurs may exhibit certain pattern changes in adjacent time steps. The experimental setup for the algorithm in this study is configured as follows: an Intel(R) Core(TM) i5-8300H CPU @ 2.3 GHz processor, 16 GB of RAM, a 64-bit operating system, and an x64-based architecture.</p><p>Multi-dimensional coupling features were extracted from the multi-channel heart sound signals in the PhysioNet 2022 Heart Sound Challenge dataset and the clinical dataset. The ReliefF algorithm [<xref rid="pone.0321209.ref026" ref-type="bibr">26</xref>] was applied for dimensionality reduction, reducing the 78 extracted coupling features. Finally, binary classification of normal and abnormal signals was performed using various machine learning classifiers, including logistic regression, support vector machines, decision trees, k-nearest neighbors, Bayesian classifiers, and convolutional neural networks.</p><p>To explore the advantages of 3D time-frequency domain matrix features, machine learning techniques were utilized to perform binary classification of normal and abnormal heart sounds, and multiple comparative experiments were conducted. To control experimental variables, the dataset was proportionally divided into training and testing sets, with the training set comprising 70% of the samples and the testing set comprising 30%.</p><p>To ensure the reliability of the experimental results, both the experiments and comparative experiments used the same dataset. The evaluation metrics are defined as shown in equations (17)&#x02013;(20), and the final metric values are the averages over 20 experiments.</p><disp-formula id="pone.0321209.e046"><alternatives><graphic xlink:href="pone.0321209.e046.jpg" id="pone.0321209.e046g" position="anchor"/><mml:math id="M46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>Acc</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(2)</label></disp-formula><disp-formula id="pone.0321209.e047"><alternatives><graphic xlink:href="pone.0321209.e047.jpg" id="pone.0321209.e047g" position="anchor"/><mml:math id="M47" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(3)</label></disp-formula><disp-formula id="pone.0321209.e048"><alternatives><graphic xlink:href="pone.0321209.e048.jpg" id="pone.0321209.e048g" position="anchor"/><mml:math id="M48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>Se</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(4)</label></disp-formula><disp-formula id="pone.0321209.e049"><alternatives><graphic xlink:href="pone.0321209.e049.jpg" id="pone.0321209.e049g" position="anchor"/><mml:math id="M49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>Sp</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(5)</label></disp-formula><disp-formula id="pone.0321209.e050"><alternatives><graphic xlink:href="pone.0321209.e050.jpg" id="pone.0321209.e050g" position="anchor"/><mml:math id="M50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>F1&#x000a0;Score</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mfrac><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mi>&#x000d7;</mml:mi><mml:mtext>Se</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mo>+</mml:mo><mml:mtext>Se</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(6)</label></disp-formula><p>In these equations, the definitions include four result types: True Positive (TP), False Positive (FP), False Negative (FN), and True Negative (TN), which are used to describe the accuracy of classification results.</p><sec id="sec010"><title>Classification of normal and abnormal heart sounds in the open-source dataset</title><p>The classification results of normal and abnormal heart sounds in the open-source dataset are presented in <xref rid="pone.0321209.t002" ref-type="table">Table 2</xref>. It can be observed that using the coupling features reduced by the ReliefF method and the CNN classifier, the precision, sensitivity, specificity, and accuracy for the binary classification of normal and abnormal heart sounds reached 96.6%, 97.6%, 97.7%, and 98.3%, respectively. This classification performance is superior to other classifiers. Specifically, <xref rid="pone.0321209.g010" ref-type="fig">Fig 10</xref> illustrates the feature ranking and importance scores after applying the ReliefF method, which reduced the feature dimension from 78 to 48. As shown in <xref rid="pone.0321209.t002" ref-type="table">Table 2</xref>, the ReliefF-reduced coupling features, when used with the CNN classifier, achieved a precision of 96.6%, a sensitivity of 97.6%, a specificity of 97.7%, and an accuracy of 98.3%, demonstrating superior performance compared to other classification methods. Furthermore, under the premise of using the CNN classifier, the coupling features reduced by the ReliefF method improved the classification performance compared to the unreduced coupling features using the support vector machine (SVM). Specifically, the precision increased by 1.6%, sensitivity by 1.3%, specificity by 0.5%, and accuracy by 0.9%, and also lead to a 1.41% improvement in F1-score. Finally, the confusion matrix for the open-source dataset&#x02019;s binary classification is depicted in <xref rid="pone.0321209.g011" ref-type="fig">Fig 11</xref>, which reveals only one misclassified normal and two misclassified abnormal signals. This indicates that the proposed coupling features have excellent discriminatory capability for the different signals within the dataset.</p><table-wrap position="float" id="pone.0321209.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.t002</object-id><label>Table 2</label><caption><title>Comparison of classification results before and after coupling feature dimensionality reduction under different classifiers.</title></caption><alternatives><graphic xlink:href="pone.0321209.t002" id="pone.0321209.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Features</th><th align="left" rowspan="1" colspan="1">Classifier</th><th align="left" rowspan="1" colspan="1">Pr (%)</th><th align="left" rowspan="1" colspan="1">Se (%)</th><th align="left" rowspan="1" colspan="1">Sp (%)</th><th align="left" rowspan="1" colspan="1">Acc (%)</th><th align="left" rowspan="1" colspan="1">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="6" colspan="1">Unreduced Coupling Features (78 features)</td><td align="left" rowspan="1" colspan="1">Logistic Regression</td><td align="left" rowspan="1" colspan="1">90.61</td><td align="left" rowspan="1" colspan="1">91.21</td><td align="left" rowspan="1" colspan="1">91.21</td><td align="left" rowspan="1" colspan="1">91.62</td><td align="left" rowspan="1" colspan="1">90.90</td></tr><tr><td align="left" rowspan="1" colspan="1">Support Vector Machine</td><td align="left" rowspan="1" colspan="1">95.00</td><td align="left" rowspan="1" colspan="1">96.40</td><td align="left" rowspan="1" colspan="1">97.20</td><td align="left" rowspan="1" colspan="1">97.40</td><td align="left" rowspan="1" colspan="1">95.69</td></tr><tr><td align="left" rowspan="1" colspan="1">Decision Tree</td><td align="left" rowspan="1" colspan="1">93.40</td><td align="left" rowspan="1" colspan="1">95.80</td><td align="left" rowspan="1" colspan="1">96.30</td><td align="left" rowspan="1" colspan="1">96.30</td><td align="left" rowspan="1" colspan="1">94.58</td></tr><tr><td align="left" rowspan="1" colspan="1">K-Nearest Neighbors</td><td align="left" rowspan="1" colspan="1">91.80</td><td align="left" rowspan="1" colspan="1">95.10</td><td align="left" rowspan="1" colspan="1">95.80</td><td align="left" rowspan="1" colspan="1">97.20</td><td align="left" rowspan="1" colspan="1">93.42</td></tr><tr><td align="left" rowspan="1" colspan="1">Bayesian</td><td align="left" rowspan="1" colspan="1">91.90</td><td align="left" rowspan="1" colspan="1">97.20</td><td align="left" rowspan="1" colspan="1">95.40</td><td align="left" rowspan="1" colspan="1">93.50</td><td align="left" rowspan="1" colspan="1">94.48</td></tr><tr><td align="left" rowspan="1" colspan="1">CNN</td><td align="left" rowspan="1" colspan="1">90.20</td><td align="left" rowspan="1" colspan="1">93.50</td><td align="left" rowspan="1" colspan="1">94.70</td><td align="left" rowspan="1" colspan="1">96.80</td><td align="left" rowspan="1" colspan="1">91.82</td></tr><tr><td align="left" rowspan="6" colspan="1">Coupling Features Reduced by ReliefF (48 features)</td><td align="left" rowspan="1" colspan="1">Logistic Regression</td><td align="left" rowspan="1" colspan="1">89.89</td><td align="left" rowspan="1" colspan="1">90.75</td><td align="left" rowspan="1" colspan="1">90.75</td><td align="left" rowspan="1" colspan="1">91.02</td><td align="left" rowspan="1" colspan="1">90.28</td></tr><tr><td align="left" rowspan="1" colspan="1">Support Vector Machine</td><td align="left" rowspan="1" colspan="1">91.80</td><td align="left" rowspan="1" colspan="1">95.50</td><td align="left" rowspan="1" colspan="1">95.70</td><td align="left" rowspan="1" colspan="1">97.80</td><td align="left" rowspan="1" colspan="1">93.61</td></tr><tr><td align="left" rowspan="1" colspan="1">Decision Tree</td><td align="left" rowspan="1" colspan="1">94.90</td><td align="left" rowspan="1" colspan="1">94.30</td><td align="left" rowspan="1" colspan="1">96.90</td><td align="left" rowspan="1" colspan="1">96.00</td><td align="left" rowspan="1" colspan="1">94.60</td></tr><tr><td align="left" rowspan="1" colspan="1">K-Nearest Neighbors</td><td align="left" rowspan="1" colspan="1">91.80</td><td align="left" rowspan="1" colspan="1">95.30</td><td align="left" rowspan="1" colspan="1">95.10</td><td align="left" rowspan="1" colspan="1">96.80</td><td align="left" rowspan="1" colspan="1">93.52</td></tr><tr><td align="left" rowspan="1" colspan="1">Bayesian</td><td align="left" rowspan="1" colspan="1">94.90</td><td align="left" rowspan="1" colspan="1">95.60</td><td align="left" rowspan="1" colspan="1">98.40</td><td align="left" rowspan="1" colspan="1">97.20</td><td align="left" rowspan="1" colspan="1">95.25</td></tr><tr><td align="left" rowspan="1" colspan="1">CNN</td><td align="left" rowspan="1" colspan="1">
<bold>96.60</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>97.60</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>97.70</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>98.30</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>97.10</bold>
</td></tr></tbody></table></alternatives><table-wrap-foot><fn><p>Table notes: Comparison of different classifiers before and after feature dimensionality reduction using ReliefF. Metrics include Precision (Pr), Sensitivity (Se), Specificity (Sp), Accuracy (Acc), and F1-score (F1).</p></fn></table-wrap-foot></table-wrap><fig position="float" id="pone.0321209.g010"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g010</object-id><label>Fig 10</label><caption><title>Feature importance scores for the open-source dataset.</title></caption><graphic xlink:href="pone.0321209.g010" position="float"/></fig><fig position="float" id="pone.0321209.g011"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g011</object-id><label>Fig 11</label><caption><title>Confusion matrix for the open-source dataset.</title></caption><graphic xlink:href="pone.0321209.g011" position="float"/></fig></sec><sec id="sec011"><title>Classification of congenital heart disease in the clinical dataset</title><p>The classification results of congenital heart disease using the reduced coupling features in the clinical dataset are shown in <xref rid="pone.0321209.t003" ref-type="table">Table 3</xref>. It can be seen that using the coupling features reduced by the ReliefF method and the CNN classifier, the precision, sensitivity, specificity, and accuracy for classifying four types of congenital heart disease heart sounds and normal heart sounds reached 97.4%, 94.7%, 94.4%, and 95.6%, respectively. This classification performance is superior to other classifiers.</p><table-wrap position="float" id="pone.0321209.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.t003</object-id><label>Table 3</label><caption><title>Comparison of classification results before and after coupling feature dimensionality reduction under different classifiers.</title></caption><alternatives><graphic xlink:href="pone.0321209.t003" id="pone.0321209.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Features</th><th align="left" rowspan="1" colspan="1">Classifier</th><th align="left" rowspan="1" colspan="1">Pr (%)</th><th align="left" rowspan="1" colspan="1">Se (%)</th><th align="left" rowspan="1" colspan="1">Sp (%)</th><th align="left" rowspan="1" colspan="1">Acc (%)</th><th align="left" rowspan="1" colspan="1">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="6" colspan="1">Unreduced Coupling Features (130 features)</td><td align="left" rowspan="1" colspan="1">Logistic Regression</td><td align="left" rowspan="1" colspan="1">87.15</td><td align="left" rowspan="1" colspan="1">81.96</td><td align="left" rowspan="1" colspan="1">81.96</td><td align="left" rowspan="1" colspan="1">91.27</td><td align="left" rowspan="1" colspan="1">83.98</td></tr><tr><td align="left" rowspan="1" colspan="1">Support Vector Machine</td><td align="left" rowspan="1" colspan="1">96.94</td><td align="left" rowspan="1" colspan="1">93.20</td><td align="left" rowspan="1" colspan="1">93.50</td><td align="left" rowspan="1" colspan="1">94.40</td><td align="left" rowspan="1" colspan="1">95.03</td></tr><tr><td align="left" rowspan="1" colspan="1">Decision Tree</td><td align="left" rowspan="1" colspan="1">95.82</td><td align="left" rowspan="1" colspan="1">91.20</td><td align="left" rowspan="1" colspan="1">91.20</td><td align="left" rowspan="1" colspan="1">92.40</td><td align="left" rowspan="1" colspan="1">93.45</td></tr><tr><td align="left" rowspan="1" colspan="1">K-Nearest Neighbors</td><td align="left" rowspan="1" colspan="1">95.87</td><td align="left" rowspan="1" colspan="1">88.30</td><td align="left" rowspan="1" colspan="1">91.60</td><td align="left" rowspan="1" colspan="1">93.40</td><td align="left" rowspan="1" colspan="1">91.93</td></tr><tr><td align="left" rowspan="1" colspan="1">Bayesian</td><td align="left" rowspan="1" colspan="1">95.90</td><td align="left" rowspan="1" colspan="1">93.10</td><td align="left" rowspan="1" colspan="1">91.20</td><td align="left" rowspan="1" colspan="1">92.70</td><td align="left" rowspan="1" colspan="1">94.48</td></tr><tr><td align="left" rowspan="1" colspan="1">CNN</td><td align="left" rowspan="1" colspan="1">97.40</td><td align="left" rowspan="1" colspan="1">94.70</td><td align="left" rowspan="1" colspan="1">94.40</td><td align="left" rowspan="1" colspan="1">94.30</td><td align="left" rowspan="1" colspan="1">96.03</td></tr><tr><td align="left" rowspan="6" colspan="1">Reduced Coupling Features (74 features)</td><td align="left" rowspan="1" colspan="1">Logistic Regression</td><td align="left" rowspan="1" colspan="1">87.30</td><td align="left" rowspan="1" colspan="1">81.94</td><td align="left" rowspan="1" colspan="1">81.94</td><td align="left" rowspan="1" colspan="1">92.06</td><td align="left" rowspan="1" colspan="1">84.31</td></tr><tr><td align="left" rowspan="1" colspan="1">Support Vector Machine</td><td align="left" rowspan="1" colspan="1">97.40</td><td align="left" rowspan="1" colspan="1">93.60</td><td align="left" rowspan="1" colspan="1">94.60</td><td align="left" rowspan="1" colspan="1">95.20</td><td align="left" rowspan="1" colspan="1">95.46</td></tr><tr><td align="left" rowspan="1" colspan="1">Decision Tree</td><td align="left" rowspan="1" colspan="1">96.05</td><td align="left" rowspan="1" colspan="1">90.20</td><td align="left" rowspan="1" colspan="1">91.80</td><td align="left" rowspan="1" colspan="1">93.00</td><td align="left" rowspan="1" colspan="1">94.81</td></tr><tr><td align="left" rowspan="1" colspan="1">K-Nearest Neighbors</td><td align="left" rowspan="1" colspan="1">96.37</td><td align="left" rowspan="1" colspan="1">88.90</td><td align="left" rowspan="1" colspan="1">92.60</td><td align="left" rowspan="1" colspan="1">93.50</td><td align="left" rowspan="1" colspan="1">92.48</td></tr><tr><td align="left" rowspan="1" colspan="1">Bayesian</td><td align="left" rowspan="1" colspan="1">95.92</td><td align="left" rowspan="1" colspan="1">93.50</td><td align="left" rowspan="1" colspan="1">91.20</td><td align="left" rowspan="1" colspan="1">93.80</td><td align="left" rowspan="1" colspan="1">94.69</td></tr><tr><td align="left" rowspan="1" colspan="1">CNN</td><td align="left" rowspan="1" colspan="1">97.40</td><td align="left" rowspan="1" colspan="1">94.70</td><td align="left" rowspan="1" colspan="1">94.40</td><td align="left" rowspan="1" colspan="1">95.60</td><td align="left" rowspan="1" colspan="1">96.03</td></tr></tbody></table></alternatives></table-wrap><p><xref rid="pone.0321209.g012" ref-type="fig">Fig 12</xref> displays the feature ranking and importance scores after applying the ReliefF method to the clinical dataset, reducing the feature dimension from 130 to 74. As shown in <xref rid="pone.0321209.t003" ref-type="table">Table 3</xref>, the ReliefF-reduced coupling features, when used with a CNN classifier, achieved a precision of 97.4%, a sensitivity of 94.7%, a specificity of 94.4%, an accuracy of 95.6%, and a F1-score of 96.03%, outperforming other classification methods. Furthermore, under the premise of using the CNN classifier, the coupling features reduced by the ReliefF method improved the classification performance compared to the unreduced coupling features using the SVM classifier. Specifically, the precision increased by 0.46%, sensitivity by 0.4%, specificity by 0.9%, and accuracy by 1.3%, and also lead to a 1.0% increase in F1-score. The confusion matrix for the clinical dataset&#x02019;s binary classification of normal and congenital heart disease signals is shown in <xref rid="pone.0321209.g013" ref-type="fig">Fig 13</xref>, which further validates the importance of the proposed multi-channel coupling features for CHD classification, as well as the ability of the proposed algorithm to classify different signal types within the dataset.</p><fig position="float" id="pone.0321209.g012"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g012</object-id><label>Fig 12</label><caption><title>Feature importance scores for the clinical dataset.</title></caption><graphic xlink:href="pone.0321209.g012" position="float"/></fig><fig position="float" id="pone.0321209.g013"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.g013</object-id><label>Fig 13</label><caption><title>Confusion matrix for the clinical dataset.</title></caption><graphic xlink:href="pone.0321209.g013" position="float"/></fig></sec></sec><sec sec-type="conclusions" id="sec012"><title>Discussion</title><p>The multi-channel heart sound coupling features capture time-domain, frequency-domain, and inter-channel variations in heart sound signals, providing richer representations for classification algorithms, thereby improving classification accuracy. In this section, comparative analyses are presented in <xref rid="pone.0321209.t004" ref-type="table">Table 4</xref>.</p><table-wrap position="float" id="pone.0321209.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0321209.t004</object-id><label>Table 4</label><caption><title>Comparison of classification results between the proposed method and methods from the literature.</title></caption><alternatives><graphic xlink:href="pone.0321209.t004" id="pone.0321209.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Dataset</th><th align="left" rowspan="1" colspan="1">Research Method</th><th align="left" rowspan="1" colspan="1">Pr (%)</th><th align="left" rowspan="1" colspan="1">Se (%)</th><th align="left" rowspan="1" colspan="1">Sp (%)</th><th align="left" rowspan="1" colspan="1">Acc (%)</th><th align="left" rowspan="1" colspan="1">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="6" colspan="1">Open-source Dataset</td><td align="left" rowspan="1" colspan="1">MFCC-KNN[<xref rid="pone.0321209.ref014" ref-type="bibr">14</xref>]</td><td align="left" rowspan="1" colspan="1">76.86</td><td align="left" rowspan="1" colspan="1">78.22</td><td align="left" rowspan="1" colspan="1">74.22</td><td align="left" rowspan="1" colspan="1">76.31</td><td align="left" rowspan="1" colspan="1">75.00</td></tr><tr><td align="left" rowspan="1" colspan="1">Homomorphic, Hilbert, PSD, Wavelet Envelope-BiLSTM[<xref rid="pone.0321209.ref013" ref-type="bibr">13</xref>]</td><td align="left" rowspan="1" colspan="1">&#x02014;</td><td align="left" rowspan="1" colspan="1">82.7</td><td align="left" rowspan="1" colspan="1">80.1</td><td align="left" rowspan="1" colspan="1">75.1</td><td align="left" rowspan="1" colspan="1">&#x02014;</td></tr><tr><td align="left" rowspan="1" colspan="1">Bispectrogram-ViT[<xref rid="pone.0321209.ref012" ref-type="bibr">12</xref>]</td><td align="left" rowspan="1" colspan="1">85.71</td><td align="left" rowspan="1" colspan="1">92.31</td><td align="left" rowspan="1" colspan="1">90.36</td><td align="left" rowspan="1" colspan="1">91.0</td><td align="left" rowspan="1" colspan="1">88.89</td></tr><tr><td align="left" rowspan="1" colspan="1">MFCC-CRNN[<xref rid="pone.0321209.ref007" ref-type="bibr">7</xref>]</td><td align="left" rowspan="1" colspan="1">96.4</td><td align="left" rowspan="1" colspan="1">94.8</td><td align="left" rowspan="1" colspan="1">92.3</td><td align="left" rowspan="1" colspan="1">97.2</td><td align="left" rowspan="1" colspan="1">95.59</td></tr><tr><td align="left" rowspan="1" colspan="1">Hilbert Envelope, Acoustic Features-RF[<xref rid="pone.0321209.ref006" ref-type="bibr">6</xref>]</td><td align="left" rowspan="1" colspan="1">96.0</td><td align="left" rowspan="1" colspan="1">95.7</td><td align="left" rowspan="1" colspan="1">93.5</td><td align="left" rowspan="1" colspan="1">94.6</td><td align="left" rowspan="1" colspan="1">95.85</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>Coupling Features-CNN</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>96.6</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>97.6</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>97.7</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>98.3</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>97.10</bold>
</td></tr><tr><td align="left" rowspan="3" colspan="1">Clinical Dataset</td><td align="left" rowspan="1" colspan="1">MFCC-CRNN[<xref rid="pone.0321209.ref007" ref-type="bibr">7</xref>]</td><td align="left" rowspan="1" colspan="1">96.4</td><td align="left" rowspan="1" colspan="1">93.6</td><td align="left" rowspan="1" colspan="1">94.4</td><td align="left" rowspan="1" colspan="1">95.2</td><td align="left" rowspan="1" colspan="1">94.98</td></tr><tr><td align="left" rowspan="1" colspan="1">Hilbert Envelope, Acoustic Features-RF[<xref rid="pone.0321209.ref006" ref-type="bibr">6</xref>]</td><td align="left" rowspan="1" colspan="1">96.1</td><td align="left" rowspan="1" colspan="1">90.2</td><td align="left" rowspan="1" colspan="1">91.8</td><td align="left" rowspan="1" colspan="1">93.0</td><td align="left" rowspan="1" colspan="1">93.06</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>Coupling Features-CNN</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>97.4</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>94.7</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>94.4</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>95.6</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>96.03</bold>
</td></tr></tbody></table></alternatives></table-wrap><p>On the open-source dataset, the MFCC features combined with KNN, as explored in [<xref rid="pone.0321209.ref014" ref-type="bibr">14</xref>], resulted in precision, sensitivity, specificity, and accuracy all below 80%, indicating poor distinction between normal and abnormal heart sounds. Although the method by [<xref rid="pone.0321209.ref013" ref-type="bibr">13</xref>], which extracts homomorphic, Hilbert, and other features combined with a BiLSTM network, improved sensitivity and specificity by 4.48% and 5.88%, respectively, compared to [<xref rid="pone.0321209.ref014" ref-type="bibr">14</xref>], the overall accuracy of only 75.1% still fell short of effective signal distinction. While [<xref rid="pone.0321209.ref012" ref-type="bibr">12</xref>] achieved significantly improved results with precision, sensitivity, specificity, accuracy, and F1-score at 85.71%, 92.31%, 90.36%, 91.0%, and 88.89%, respectively, using a bispectrogram-ViT architecture, the computational complexity of ViT remains a concern, and its performance is contingent upon a large training dataset. Following this, the methods from [<xref rid="pone.0321209.ref007" ref-type="bibr">7</xref>] and [<xref rid="pone.0321209.ref006" ref-type="bibr">6</xref>] were applied to the open-source dataset for binary classification of normal and abnormal heart sounds, achieving accuracies of 97.2% and 94.6%, respectively, showing improved performance.</p><p>The proposed coupling features, combined with a CNN algorithm, demonstrated superior performance, reaching 96.6% precision, 97.6% sensitivity, 97.7% specificity, 98.3% accuracy, and a 97.10% F1-score. Compared to the referenced algorithms, this represents a notable improvement, with an increase of 0.2%, 1.9%, 4.2%, 1.1%, and 1.51% for precision, sensitivity, specificity, accuracy, and F1-score, respectively. Additionally, the proposed algorithm was validated on a clinical dataset. Using the coupling features with CNN, the precision, sensitivity, specificity, and accuracy were 97.4%, 94.7%, 94.4%, and 95.6%, respectively. Compared to [<xref rid="pone.0321209.ref007" ref-type="bibr">7</xref>] and [<xref rid="pone.0321209.ref006" ref-type="bibr">6</xref>], the proposed method improved precision, sensitivity, and accuracy by 1.0%, 1.1%, and 0.4%, respectively, demonstrating its effectiveness and generalizability in distinguishing between normal and congenital heart disease signals.</p></sec><sec sec-type="conclusions" id="sec013"><title>Conclusion</title><p>This paper proposes a method that combines multi-channel coupling features with machine learning techniques. Using a CNN, the classification of normal and abnormal heart sounds from the PhysioNet 2022 Heart Sound Challenge dataset achieved precision, sensitivity, specificity, and accuracy of 96.6%, 97.6%, 97.7%, and 98.3%, respectively. For the classification of congenital heart disease in the clinical dataset, the precision, sensitivity, specificity, and accuracy were 97.4%, 94.7%, 94.4%, and 95.6%, respectively. The algorithm presented in this paper provides a novel and effective method for heart sound signal classification and offers significant clinical value for the identification of heart diseases. Future work will build upon the current research, which considers both time-domain and frequency-domain characteristics of heart sounds, by expanding the collection of case data for different heart disease categories. This will facilitate the development of more accurate diagnostic models for various cardiac conditions and enable a deeper understanding of the complex relationship between heart sound signals and heart health. Furthermore, exploration of additional neural network techniques to enhance model performance is planned, with the ultimate goal of achieving a real-time clinical heart sound analysis system.</p></sec></body><back><ref-list><title>References</title><ref id="pone.0321209.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Roth</surname><given-names>G</given-names></name>, <name><surname>Mensah</surname><given-names>G</given-names></name>, <name><surname>Johnson</surname><given-names>C</given-names></name>. <article-title>Global burden of cardiovascular diseases and risk factors</article-title>, <source>2020 and progress toward sustainable development goal 3.4. J Am Coll Cardiol.</source>
<year>2021</year>;<volume>78</volume>(<issue>20</issue>):<fpage>1159</fpage>&#x02013;<lpage>91</lpage>.</mixed-citation></ref><ref id="pone.0321209.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>El Badlaoui</surname><given-names>O</given-names></name>, <name><surname>Benba</surname><given-names>A</given-names></name>, <name><surname>Hammouch</surname><given-names>A</given-names></name>. <article-title>Novel PCG analysis method for discriminating between abnormal and normal heart sounds</article-title>. <source>IRBM</source>
<year>2020</year>;<volume>41</volume>(<issue>4</issue>):<fpage>223</fpage>&#x02013;<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.irbm.2019.12.003</pub-id></mixed-citation></ref><ref id="pone.0321209.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Kumar</surname><given-names>S</given-names></name>, <name><surname>Singh</surname><given-names>M</given-names></name>, <name><surname>Sharma</surname><given-names>A</given-names></name>. <article-title>Heart murmur detection using time-frequency features and ensemble classifiers</article-title>. <source>Biomed Eng Lett.</source>
<year>2021</year>;<volume>11</volume>(<issue>2</issue>):<fpage>139</fpage>&#x02013;<lpage>49</lpage>.</mixed-citation></ref><ref id="pone.0321209.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>T</given-names></name>, <name><surname>Cheng</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>. <article-title>Heart sound classification based on improved MFCC features and SVM</article-title>. <source>IEEE Access.</source>
<year>2021</year>;<volume>9</volume>:<fpage>10334</fpage>&#x02013;<lpage>44</lpage>.</mixed-citation></ref><ref id="pone.0321209.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>Q</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Dong</surname><given-names>X</given-names></name>. <article-title>Influence of auscultation sites on heart sound biometric recognition</article-title>. <source>Biomed Signal Process Control.</source>
<year>2021</year>;<volume>68</volume>:<fpage>102700</fpage>.</mixed-citation></ref><ref id="pone.0321209.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Nizam</surname><given-names>NB</given-names></name>, <name><surname>Nuhash</surname><given-names>SISK</given-names></name>, <name><surname>Hasan</surname><given-names>T</given-names></name>. <article-title>Hilbert-envelope features for cardiac disease classification from noisy phonocardiograms</article-title>. <source>Biomed Signal Process Control</source>. <year>2022</year>;<volume>78</volume>:<fpage>103864</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.bspc.2022.103864</pub-id></mixed-citation></ref><ref id="pone.0321209.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Deng</surname><given-names>M</given-names></name>, <name><surname>Meng</surname><given-names>T</given-names></name>, <name><surname>Cao</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Zhang</surname><given-names>J</given-names></name>, <name><surname>Fan</surname><given-names>H</given-names></name>. <article-title>Heart sound classification based on improved MFCC features and convolutional recurrent neural networks</article-title>. <source>Neural Netw</source>. <year>2020</year>;<volume>130</volume>:<fpage>22</fpage>&#x02013;<lpage>32</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neunet.2020.06.015</pub-id>
<pub-id pub-id-type="pmid">32589588</pub-id>
</mixed-citation></ref><ref id="pone.0321209.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Deperlioglu</surname><given-names>O</given-names></name>. <article-title>Heart sound classification with signal instant energy and stacked autoencoder network</article-title>. <source>Biomed Signal Process Control</source>. <year>2021</year>;<volume>64</volume>:<fpage>102211</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.bspc.2020.102211</pub-id></mixed-citation></ref><ref id="pone.0321209.ref009"><label>9</label><mixed-citation publication-type="journal">Singh S, Devi N, Singh S, et al. Heart abnormality classification using PCG and ECG recordings. In: 2021 International conference on recent advances in computing; 2021. p. 1&#x02013;6.</mixed-citation></ref><ref id="pone.0321209.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Choudhary</surname><given-names>RR</given-names></name>, <name><surname>Singh</surname><given-names>MR</given-names></name>, <name><surname>Jain</surname><given-names>PK</given-names></name>. <article-title>Heart sound classification using a hybrid of CNN and GRU deep learning models</article-title>. <source>Procedia Comput Sci</source>. <year>2024</year>;<volume>235</volume>:<fpage>3085</fpage>&#x02013;<lpage>93</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.procs.2024.04.292</pub-id></mixed-citation></ref><ref id="pone.0321209.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Z</given-names></name>, <name><surname>Jiang</surname><given-names>H</given-names></name>, <name><surname>Zhang</surname><given-names>F</given-names></name>, <name><surname>Ouyang</surname><given-names>W</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>, <name><surname>Pan</surname><given-names>X</given-names></name>. <article-title>Heart sound classification based on bispectrum features and Vision Transformer mode</article-title>. <source>Alexandria Eng J</source>. <year>2023</year>;<volume>85</volume>:<fpage>49</fpage>&#x02013;<lpage>59</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.aej.2023.11.035</pub-id></mixed-citation></ref><ref id="pone.0321209.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Z</given-names></name>, <name><surname>Jiang</surname><given-names>H</given-names></name>, <name><surname>Zhang</surname><given-names>F</given-names></name>, <name><surname>Ouyang</surname><given-names>W</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>, <name><surname>Pan</surname><given-names>X</given-names></name>. <article-title>Heart sound classification based on bispectrum features and Vision Transformer mode</article-title>. <source>Alexandria Eng J</source>. <year>2023</year>;<volume>85</volume>:<fpage>49</fpage>&#x02013;<lpage>59</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.aej.2023.11.035</pub-id></mixed-citation></ref><ref id="pone.0321209.ref013"><label>13</label><mixed-citation publication-type="journal">Monteiro S, Fred A, da Silva H. Detection of heart sound murmurs and clinical outcome with bidirectional long short-term memory networks. In: 2022 computing in cardiology (CinC); 2022. p. 498.</mixed-citation></ref><ref id="pone.0321209.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Fuadah</surname><given-names>YN</given-names></name>, <name><surname>Pramudito</surname><given-names>MA</given-names></name>, <name><surname>Lim</surname><given-names>KM</given-names></name>. <article-title>An optimal approach for heart sound classification using grid search in hyperparameter optimization of machine learning</article-title>. <source>Bioengineering (Basel)</source>
<year>2022</year>;<volume>10</volume>(<issue>1</issue>):<fpage>45</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/bioengineering10010045</pub-id>
<pub-id pub-id-type="pmid">36671616</pub-id>
</mixed-citation></ref><ref id="pone.0321209.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Ali</surname><given-names>A</given-names></name>, <name><surname>Smith</surname><given-names>J</given-names></name>, <name><surname>Kumar</surname><given-names>S</given-names></name>. <article-title>A novel hybrid approach for systolic murmur detection using energy and entropy features</article-title>. <source>Expert Syst Applic.</source>
<year>2021</year>;<volume>45</volume>:<fpage>10235</fpage>&#x02013;<lpage>47</lpage>.</mixed-citation></ref><ref id="pone.0321209.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>T</given-names></name>, <name><surname>Li</surname><given-names>P</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Jiao</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Detection of coronary artery disease using multi-domain feature fusion of multi-channel heart sound signals</article-title>. <source>Entropy (Basel)</source>
<year>2021</year>;<volume>23</volume>(<issue>6</issue>):<fpage>642</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e23060642</pub-id>
<pub-id pub-id-type="pmid">34064025</pub-id>
</mixed-citation></ref><ref id="pone.0321209.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Samanta</surname><given-names>P</given-names></name>, <name><surname>Pathak</surname><given-names>A</given-names></name>, <name><surname>Mandana</surname><given-names>K</given-names></name>. <article-title>Classification of coronary artery diseased and normal subjects using multi-channel phonocardiogram signal</article-title>. <source>Biocybernet Biomed Eng.</source>
<year>2019</year>;<volume>39</volume>(<issue>2</issue>):<fpage>426</fpage>&#x02013;<lpage>43</lpage>.</mixed-citation></ref><ref id="pone.0321209.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Hazeri</surname><given-names>H</given-names></name>, <name><surname>Zarjam</surname><given-names>P</given-names></name>, <name><surname>Azemi</surname><given-names>G</given-names></name>. <article-title>Classification of normal/abnormal PCG recordings using a time-frequency approach</article-title>. <source>Analog Integr Circ Signal Process.</source>
<year>2019</year>;<volume>98</volume>(<issue>3</issue>):<fpage>383</fpage>&#x02013;<lpage>93</lpage>.</mixed-citation></ref><ref id="pone.0321209.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Kilicarslan</surname><given-names>S</given-names></name>, <name><surname>K</surname><given-names>A</given-names></name>, <name><surname>Celik</surname><given-names>M</given-names></name>. <article-title>Diagnosis of heart sounds using Butterworth filter and machine learning techniques</article-title>. <source>J Med Eng Technol.</source>
<year>2017</year>;<volume>41</volume>(<issue>7</issue>):<fpage>553</fpage>&#x02013;<lpage>63</lpage>.<pub-id pub-id-type="pmid">28990839</pub-id>
</mixed-citation></ref><ref id="pone.0321209.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Hao</surname><given-names>Z</given-names></name>, <name><surname>Lei</surname><given-names>H</given-names></name>. <article-title>A survey of convolutional neural networks: Analysis, applications</article-title>, <source>and prospects. IEEE Trans Neural Netw Learn Syst.</source>
<year>2021</year>;<volume>32</volume>(<issue>12</issue>):<fpage>1</fpage>&#x02013;<lpage>21</lpage>.</mixed-citation></ref><ref id="pone.0321209.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Hossen</surname><given-names>M</given-names></name>, <name><surname>Begum</surname><given-names>S</given-names></name>. <article-title>An efficient method for heart disease classification using machine learning algorithms</article-title>. <source>J Healthc Eng</source>
<year>2021</year>;<volume>2021</volume>(<issue>9982021</issue>):<fpage>1</fpage>&#x02013;<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1155/2021/9982021</pub-id></mixed-citation></ref><ref id="pone.0321209.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Xie</surname><given-names>H</given-names></name>, <name><surname>Zheng</surname><given-names>Y</given-names></name>, <name><surname>Guo</surname><given-names>J</given-names></name>. <article-title>Cross-fuzzy entropy: A new method to test pattern synchrony of bivariate time series</article-title>. <source>Inform Sci.</source>
<year>2010</year>;<volume>180</volume>(<issue>9</issue>):<fpage>1715</fpage>&#x02013;<lpage>24</lpage>.</mixed-citation></ref><ref id="pone.0321209.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Liu</surname><given-names>C</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Jiao</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Discrimination of patients with varying degrees of coronary artery stenosis by ECG and PCG signals based on entropy</article-title>. <source>Entropy (Basel)</source>
<year>2021</year>;<volume>23</volume>(<issue>7</issue>):<fpage>823</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e23070823</pub-id>
<pub-id pub-id-type="pmid">34203339</pub-id>
</mixed-citation></ref><ref id="pone.0321209.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Dimitriadis</surname><given-names>SI</given-names></name>, <name><surname>Sun</surname><given-names>Y</given-names></name>, <name><surname>Kwok</surname><given-names>K</given-names></name>, <name><surname>Laskaris</surname><given-names>NA</given-names></name>, <name><surname>Bezerianos</surname><given-names>A</given-names></name>. <article-title>A tensorial approach to access cognitive workload related to mental arithmetic from EEG functional connectivity estimates</article-title>. <source>Annu Int Conf IEEE Eng Med Biol Soc</source>. <year>2013</year>;<volume>2013</volume>:<fpage>2940</fpage>&#x02013;<lpage>3</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/EMBC.2013.6610156</pub-id>
<pub-id pub-id-type="pmid">24110343</pub-id>
</mixed-citation></ref><ref id="pone.0321209.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Kilicarslan</surname><given-names>S</given-names></name>, <name><surname>Adem</surname><given-names>K</given-names></name>, <name><surname>Celik</surname><given-names>M</given-names></name>. <article-title>Diagnosis and classification of cancer using hybrid model based on ReliefF and convolutional neural network</article-title>. <source>Med Hypotheses</source>. <year>2020</year>;<volume>137</volume>:<fpage>109577</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.mehy.2020.109577</pub-id>
<pub-id pub-id-type="pmid">31991364</pub-id>
</mixed-citation></ref><ref id="pone.0321209.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Megalmani</surname><given-names>DR</given-names></name>, <name><surname>G</surname><given-names>SB</given-names></name>, <name><surname>Rao M V</surname><given-names>A</given-names></name>, <name><surname>Jeevannavar</surname><given-names>SS</given-names></name>, <name><surname>Ghosh</surname><given-names>PK</given-names></name>. <article-title>Unsegmented heart sound classification using hybrid CNN-LSTM neural networks</article-title>. <source>Annu Int Conf IEEE Eng Med Biol Soc</source>. <year>2021</year>;<volume>2021</volume>:<fpage>713</fpage>&#x02013;<lpage>7</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/EMBC46164.2021.9629596</pub-id>
<pub-id pub-id-type="pmid">34891391</pub-id>
</mixed-citation></ref></ref-list></back></article>