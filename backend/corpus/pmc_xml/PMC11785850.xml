<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Asian Bioeth Rev</journal-id><journal-id journal-id-type="iso-abbrev">Asian Bioeth Rev</journal-id><journal-title-group><journal-title>Asian Bioethics Review</journal-title></journal-title-group><issn pub-type="ppub">1793-8759</issn><issn pub-type="epub">1793-9453</issn><publisher><publisher-name>Springer Nature Singapore</publisher-name><publisher-loc>Singapore</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11785850</article-id><article-id pub-id-type="publisher-id">290</article-id><article-id pub-id-type="doi">10.1007/s41649-024-00290-9</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Paper</subject></subj-group></article-categories><title-group><article-title>When can we Kick (Some) Humans &#x0201c;Out of the Loop&#x0201d;? An Examination of the use of AI in Medical Imaging for Lumbar Spinal Stenosis</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8506-0659</contrib-id><name><surname>Muyskens</surname><given-names>Kathryn</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Ma</surname><given-names>Yonghui</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Menikoff</surname><given-names>Jerry</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hallinan</surname><given-names>James</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Savulescu</surname><given-names>Julian</given-names></name><address><email>julian.savulescu@philosophy.ox.ac.uk</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01tgyzw49</institution-id><institution-id institution-id-type="GRID">grid.4280.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2180 6431</institution-id><institution>Centre for Biomedical Ethics, Yong Loo Lin School of Medicine, </institution></institution-wrap>National University of Singapore, Singapore </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00mcjh785</institution-id><institution-id institution-id-type="GRID">grid.12955.3a</institution-id><institution-id institution-id-type="ISNI">0000 0001 2264 7233</institution-id><institution>Centre for Bioethics, </institution><institution>Xiamen University, </institution></institution-wrap>Xiamen, China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01tgyzw49</institution-id><institution-id institution-id-type="GRID">grid.4280.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2180 6431</institution-id><institution>Yong Loo Lin School of Medicine, </institution></institution-wrap>National University of Singapore, Singapore </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/052gg0110</institution-id><institution-id institution-id-type="GRID">grid.4991.5</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8948</institution-id><institution>Uehiro Centre for Practical Ethics, Faculty of Philosophy, </institution><institution>University of Oxford, </institution></institution-wrap>Oxford, UK </aff></contrib-group><pub-date pub-type="epub"><day>15</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>15</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="collection"><month>1</month><year>2025</year></pub-date><volume>17</volume><issue>1</issue><fpage>207</fpage><lpage>223</lpage><history><date date-type="received"><day>16</day><month>10</month><year>2023</year></date><date date-type="rev-recd"><day>23</day><month>2</month><year>2024</year></date><date date-type="accepted"><day>4</day><month>3</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Artificial intelligence (AI) has attracted an increasing amount of attention, both positive and negative. Its potential applications in healthcare are indeed manifold and revolutionary, and within the realm of medical imaging and radiology (which will be the focus of this paper), significant increases in accuracy and speed, as well as significant savings in cost, stand to be gained through the adoption of this technology. Because of its novelty, a norm of keeping humans &#x0201c;in the loop&#x0201d; wherever AI mechanisms are deployed has become synonymous with good ethical practice in some circles. It has been argued that keeping humans &#x0201c;in the loop&#x0201d; is important for reasons of safety, accountability, and the maintenance of institutional trust. However, as the application of machine learning for the detection of lumbar spinal stenosis (LSS) in this paper&#x02019;s case study reveals, there are some scenarios where an insistence on keeping humans in the loop (or in other words, the resistance to automation) seems unwarranted and could possibly lead us to miss out on very real and important opportunities in healthcare&#x02014;particularly in low-resource settings. It is important to acknowledge these opportunity costs of resisting automation in such contexts, where better options may be unavailable. Using an AI model based on convolutional neural networks developed by a team of researchers at NUH/NUS medical school in Singapore for automated detection and classification of the lumbar spinal canal, lateral recess, and neural foraminal narrowing in an MRI scan of the spine to diagnose LSS, we will aim to demonstrate that where certain criteria hold (e.g., the AI is as accurate or better than human experts, risks are low in the event of an error, the gain in wellbeing is significant, and the task being automated is not essentially or importantly human), it is both morally permissible and even desirable to kick the humans out of the loop.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>AI</kwd><kwd>Radiology</kwd><kwd>Medical imaging</kwd><kwd>AI ethics</kwd><kwd>Automation</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; National University of Singapore and Springer Nature Singapore Pte Ltd. 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">New innovations in the frontiers of artificial intelligence (AI) have led to increasing areas of application for this class of technologies, as well as increasing concerns about the bounds of its ethical use. AI&#x02019;s potential applications in healthcare are indeed manifold and revolutionary, and within the realm of medical imaging and radiology (which will be the focus of this paper), significant increases in accuracy and speed, as well as significant savings in cost, stand to be gained through the adoption and implementation of this technology. Because of its novelty, a norm of keeping humans &#x0201c;in the loop&#x0201d; wherever AI mechanisms are deployed has become synonymous with good ethical practice in some circles. It has been argued that keeping humans in the loop is important for reasons of safety, accountability, and the maintenance of institutional trust. However, this paper will argue that there are some scenarios where an insistence on keeping humans in the loop is unwarranted and could come with significant opportunity costs. We adopt a &#x0201c;bottom up&#x0201d; approach to make our arguments. We begin with presenting a case study using an AI model based on convolutional neural networks developed by a team of researchers (including author James Hallinan of this paper) at the National University Hospital and the medical school at the National University of Singapore (NUH/NUS) for automated detection and classification of the lumbar spinal canal, lateral recess, and neural foraminal narrowing in an MRI scan of the spine to diagnose lumbar spinal stenosis (LSS). Using this case as a starting point, we will explore the possibilities generated by such technologies, one possibility being increased automation of certain medical procedures. Some radiologists have looked upon the prospect of automating a large portion of their work with fear, worrying it may make the specialty obsolete (Hayashi <xref ref-type="bibr" rid="CR12">2021</xref>; Chockley and Emanuel <xref ref-type="bibr" rid="CR5">2016</xref>). Even so, keeping radiologists busy or employed is not sufficient reason to insist on keeping them &#x0201c;in the loop&#x0201d; if the benefits of automation are to be taken seriously. Overall, the many practical benefits to the automation of these aspects of medical imaging make it worthwhile to consider both pragmatically and morally. Increasing the efficiency and accuracy of diagnosis can mean improving the quality of care, while also lowering healthcare costs, and improving human wellbeing. These are all important and worthy gains. We will aim to demonstrate that, contrary to the present conventional thinking, where certain criteria hold, it is both morally permissible and even desirable to kick the humans out of the loop. As we will discuss, this is especially the case in low-resource settings where medical professionals may be in short supply.</p></sec><sec id="Sec2"><title>Background</title><p id="Par3">The prospect of automation is increasingly a realistic one in the field of radiology, as many scholars have noted (Mazurowski <xref ref-type="bibr" rid="CR26">2019</xref>; Chockley and Emanuel <xref ref-type="bibr" rid="CR5">2016</xref>; and others). As Mazurowski (<xref ref-type="bibr" rid="CR26">2019</xref>) has discussed, the possibility of AI replacing large numbers of radiologists has often been dismissed by the radiology community. He cites several reasons for this dismissal, including doubt that the technology will ever match human skills, the belief that legal liability issues would be insurmountable and thus regulatory bodies like the FDA would never permit machines to replace radiologists, and that patients would never place their trust in an automated system (Mazurowski <xref ref-type="bibr" rid="CR26">2019</xref>). Recent years have already demonstrated otherwise. While the regulation of AI in healthcare is still being developed, and patient attitudes continue to evolve, the capabilities of AI for medical imaging have proven themselves to be more than capable. Among the more prescient in the field, Chockley and Emanuel wrote in their 2016 article, that &#x0201c;[m]achine learning will become a powerful force in radiology in the next 5 to 10 years and could end radiology as a thriving specialty.&#x0201d;</p><p id="Par4">More recently, Hayashi (<xref ref-type="bibr" rid="CR12">2021</xref>) has expressed concern about automation leading to a devaluation of radiologists themselves. Writing in his review of the Spine AI case study that is the focus of this paper itself, Hayashi observes that this kind of program&#x02019;s many benefits (like reduction in reading time, reduction in intrareader and interreader variabilities, and lowering costs) could lead non-radiologists to question the need for the specialty in the first place (Hayashi <xref ref-type="bibr" rid="CR12">2021</xref>). Indeed, an earlier study by Liu et al. (<xref ref-type="bibr" rid="CR24">2019</xref>) found that the diagnostic performance of deep learning (DL) models in medical imaging was equivalent to that of healthcare professionals. Thus, it would seem that the day that some radiologists thought would not come is fast approaching if not dawning already. In light of this, there is a need to articulate what criteria, if any, would ever make it morally permissible to kick humans out of the loop.</p><p id="Par5">We will adopt a working definition of AI from Richardson et al. (<xref ref-type="bibr" rid="CR31">2020</xref>), and &#x0201c;deem a computer to exhibit artificial intelligence (AI) when it performs a task that would normally require intelligent action by a human&#x0201d;. The specific kind of AI employed in the present case study performs one of the tasks otherwise done by radiologists&#x02014;namely, reading and measuring MRI scans of the spine.</p><p id="Par6">As Marcin Rzadeczka (<xref ref-type="bibr" rid="CR32">2020</xref>) writes, AI is being applied to nearly every aspect of medicine, in the data analysis of scientific literature, the diagnostic process, and now through the use of large language models (LLMs), AI is even capable of mediating the communication between physicians and patients. Medical imaging has been one of the areas of medical application that has garnered the most excitement in recent years (see Richardson et al. <xref ref-type="bibr" rid="CR31">2020</xref>; Siefert et al. <xref ref-type="bibr" rid="CR33">2021</xref>, among others). AI can and has been used to enhance many aspects of medical imaging, including making the process faster and more accurate. Given the repetitiveness and time-consuming nature of reading the MRI scans, this is an attractive area for the application of AI to transform radiology practice. Indeed, while most in the field have argued that the role of the radiologist will only be augmented, not replaced by AI models, others have speculated that as the capabilities of the technology improve this may change (see Jha and Topol <xref ref-type="bibr" rid="CR16">2016</xref>; Chockley and Emanuel <xref ref-type="bibr" rid="CR5">2016</xref>, and Mazurowski <xref ref-type="bibr" rid="CR26">2019</xref>). It is this latter possibility that we seek to explore in the following sections of this paper.</p><p id="Par7">The details of the case study and its findings are as follows:</p><p id="Par8">Spine AI: Medical Imaging for Lumbar Spinal Stenosis</p><p id="Par9">
<italic>Developed from:</italic> Hallinan et al. (<xref ref-type="bibr" rid="CR9">2021</xref>)</p><p id="Par10">A team of researchers at NUH/NUS medical school in Singapore (including author James Hallinan of this piece) has developed an AI model based on convolutional neural networks for automated detection and classification of the lumbar spinal canal, lateral recess, and neural foraminal narrowing in an MRI scan of the spine to diagnose lumbar spinal stenosis (LSS). LSS is a potentially debilitating condition affecting many adults globally, with a considerable impact on livelihood. Most patients with LSS present with lower back pain, which is also the main reason for seeking care. A large proportion of patients eventually undergo lumbar spine MRI for diagnosis and treatment planning. Lumbar spine MRI is an essential tool in the assessment of LSS for the accurate evaluation of the central canal, lateral recesses, and neural foramina. The degree of stenosis at each region plays a role in determining the appropriate treatment, but detailing such information in a report can be repetitive and time-consuming. In addition, there are multiple grading systems for LSS, with a lack of standardization.</p><p id="Par11">The research team was able to show that the Spine AI model for <italic>semi</italic>-automated (e.g., human-in-the-loop) reporting of lumbar spine MRI scans could produce the following benefits:</p><p id="Par12">More <italic>consistent</italic> and <italic>accurate</italic> grading of spinal stenosis: In a trial run, Spine AI performed comparably to expert human radiologists specializing in LSS (Hallinan et al. <xref ref-type="bibr" rid="CR9">2021</xref>, 7). Additionally, it was found to be able to assist inexperienced readers and improve their accuracy over time (Lim et al. <xref ref-type="bibr" rid="CR23">2022</xref>). This can improve clinical decision-making and patient outcomes (Hallinan et al. <xref ref-type="bibr" rid="CR9">2021</xref>), and where institutions lack radiologist expertise, the model offers a tool to improve the accuracy of inexperienced readers, e.g., kappas<xref ref-type="fn" rid="Fn1">1</xref> for trainee and general radiologists increased from 0.6 to 0.9 with the model, matching the performance of a specialist spine radiologist (kappa = 0.9) (Hallinan et al. <xref ref-type="bibr" rid="CR9">2021</xref>; Lim et al. <xref ref-type="bibr" rid="CR23">2022</xref>)</p><p id="Par13">Improved scan <italic>turnaround time</italic> and radiologist <italic>productivity</italic> (which in turn <italic>reduced cost</italic>): The deep learning (DL) solution will also reduce the time taken for report generation. Based on the recent Radiology manuscript (Lim et al. <xref ref-type="bibr" rid="CR23">2022</xref>) reporting time could be reduced by ~70% with, compared to without DL model assistance (e.g., 10 minutes to 3 minutes with the DL model, 7-minute time saved) (Lim et al. <xref ref-type="bibr" rid="CR23">2022</xref>). With ~67,000 MRI lumbar spines a year performed in Singapore (~10 hospitals), a saving of 7 minutes per MRI results in ~469,000 minutes (7817 hours) saved per year in Singapore alone. The per-hour rate for radiologists is $100 SGD, meaning there is a potential cost savings of up to $780,000 SGD each year.</p><p id="Par14">This version of assisted AI solution for MRI spine reporting has the radiologist at the center of the process (e.g., it was not fully automated). The AI model outputs are provided as boxes overlaid on the MRI images. These can be changed as necessary by the reporting radiologist, and once they have reviewed all the outputs a text report can be automatically generated.</p><p id="Par15">The researchers at NUH/NUS recommend that the AI-assisted solution for MRI spine reporting have the radiologist at the center of the process, saying &#x0201c;a fully automated system is unlikely to be acceptable to either radiologists or clinicians&#x0201d; (Hallinan et al. <xref ref-type="bibr" rid="CR9">2021</xref>). AI models of the type deployed in this instance can assist with reading images and identifying patterns in the data <italic>at a faster rate than humans alone</italic>. However, the limitations of current AI models mean that the human radiologist cannot be removed from the process without raising some serious ethical concerns, primarily, concerning issues of <italic>safety, reliability, and accountability</italic>.</p><p id="Par16">The authors of the study concluded, &#x0201c;&#x02026;our deep learning (DL) model is reliable and may be used to quickly assess lumbar spinal stenosis (LSS) at MRI. In clinical practice, the diagnosis of LSS <italic>still relies on the subjective opinion of the reporting radiologist.</italic> Our DL model could provide semi-automated reporting <italic>under the supervision of a radiologist</italic> to provide more consistent and objective reporting. Further development of the DL model could involve a consensus panel of international experts to reduce any labeling errors and biases. The DL model could also be assessed for the longitudinal follow-up of LSS at MRI&#x0201d; (Hallinan et al. <xref ref-type="bibr" rid="CR9">2021</xref>, 137).</p></sec><sec id="Sec3"><title>Discussion</title><p id="Par17">As can be seen above, Spine AI was implemented as a semi-automated system&#x02014;meaning the human radiologist was still firmly within the loop during the case study trials. However, in what follows, we will be arguing that it would be permissible to kick the human out of the loop going forward, and by extension in other similar cases, that meet the criteria we will describe in the &#x0201c;<xref rid="Sec6" ref-type="sec">Criteria for Kicking (Some) Humans Out of the Loop</xref>&#x0201d; section. Before we get to that point, however, it is useful to explore how many of the common concerns about AI might or might not apply in this case.</p><p id="Par18">At first glance, this case is a simple example of the successful implementation of AI in radiology. The technology was safe, effective, and successfully implemented. In fact, this case is interesting precisely because of its &#x0201c;tameness&#x0201d; when compared with other, more controversial applications of AI technology. To explain why that is, we should first give some brief summaries of the common controversies.</p><p id="Par19">Common areas of concern in the wider literature on AI in healthcare center are the following: (1) transparency/opacity, (2) risk/safety, (3) bias, (4) accountability or responsibility, and (5) trustworthiness. We will briefly discuss each in turn as it pertains to the present case. Though these concerns are not without grounds in many instances, there is a worrying tendency to treat them fetishistically, leading some to uncritically or dogmatically apply them to any and all instances where AI technologies might be employed. There is a need, therefore, to continually question where these issues are genuinely concerning.</p><p id="Par20">Let us begin with the question of transparency. Many AIs are &#x0201c;black boxes.&#x0201d; In other words, the systems are &#x0201c;opaque,&#x0201d; meaning that for the users, it is unclear how the system arrived at its conclusions. Ferrario (<xref ref-type="bibr" rid="CR7">2022</xref>) has argued that the epistemic opacity involved in AI technologies is a barrier to their trustworthiness, and that therefore, their use in medical practice cannot be normatively justified. Other scholars have echoed this need for transparency (see Liefgreen et al. <xref ref-type="bibr" rid="CR22">2023</xref>; Chan <xref ref-type="bibr" rid="CR4">2023</xref>; Pierce et al. <xref ref-type="bibr" rid="CR29">2021</xref>). Interestingly, Hatherley et al. (<xref ref-type="bibr" rid="CR11">2022</xref>) observed that the demand for more &#x0201c;explainable&#x0201d; or &#x0201c;interpretable&#x0201d; AI systems may come at the cost of the very accuracy that makes these technologies attractive. Even so, they argue, a preference for interpretable systems is defensible on pragmatic grounds, as they assert that clinicians are more comfortable and therefore more likely to implement and adopt technology that they understand (Hatherley et al. <xref ref-type="bibr" rid="CR11">2022</xref>). In this instance, happily, the Spine AI system is a species of &#x0201c;interpretable&#x0201d; AI. As the Spine AI program reads the image, it will display what region of the scan it has used to determine the grading. Compared to some other forms of image reading AIs, which only label a scan as either showing the presence or absence of a given condition, Spine AI adds an additional valuable step, in that it visually displays highlighted regions which it uses to make the grading&#x02014;allowing for easy interpretation and double checking. Thus, the system is less opaque than other AI image reading tools, which helps the clinicians involved in the study understand the system and reduces potential resistance to implementing it.</p><p id="Par21">As far as questions of safety or risks to patients are involved, Spine AI also appears uncontroversial. The system merely reads the MRI scans and measures the various parts of the spinal canal. The system does not store the data, so there are no risks to the patients regarding its storage or use. Spine AI does not directly interact with the patient at all. Thus, the only direct impact that the system can have on a patient is in the event of misdiagnosis. Because of the patient pathway, which involves a consultation with a clinician both before and after the scan (regardless of whether the scan is read by a human radiologist or the Spine AI program alone), and because the condition itself is not a matter of life and death, the risks associated with overdiagnosis are low. In the case of a false positive, the patient is unlikely to be referred to unnecessary surgery because the clinician will still be present to holistically assess the patient&#x02019;s symptoms and scans before proceeding. Even in cases where surgery for LSS is considered, it is not always the treatment of choice (see Katz et al. <xref ref-type="bibr" rid="CR18">2022</xref> for a discussion of the range of treatments available for LSS and their success rates), and there are many additional steps that a patient would need to go through before surgery would be done&#x02014;meaning there is a high chance of any initial error being caught. Thus, the risk to patients was low. In fact, the highest risk from overdiagnosis would likely be unnecessary spending on the additional scans. In the case of overdiagnosis, the benefits from the reduction in cost are less than there otherwise would be, but even so, the cost to the healthcare system is still likely to be lessened overall.</p><p id="Par22">As to the question of bias, disparities in access, outcomes, and quality of healthcare across various social groups have been an area of concern for some time. Lumbar spinal conditions are no exception. Some studies have found racial disparities in the realm of spinal surgeries as well (though this data was within the USA) (see Mo et al. <xref ref-type="bibr" rid="CR28">2022</xref>). Additionally, it is known that AI systems sometimes have the ability to detect racial (and other kinds) of differences, even when such data is not provided, since differences do exist in the size and condition of spinal muscles across race, gender, and age (see Hida et al. <xref ref-type="bibr" rid="CR14">2021</xref>). In principle, this could lead to an increase in unjust disparities in care. While it is not necessarily morally concerning that racial differences are present, it could become a problem if this results in under-diagnosis of LSS in certain groups. This is a realistic possibility, given that disparities in AI performance across race has been detected in other settings. Crawford and Paglen (<xref ref-type="bibr" rid="CR6">2021</xref>) have argued that automated interpretation of images is an inherently social and political act because it involves teaching a machine to classify images of human beings. This process of classification has historically been intertwined with various kinds of social biases (Crawford and Paglen <xref ref-type="bibr" rid="CR6">2021</xref>). Indeed, AI learns to classify images based on the input that the human designers give it, meaning that it is likely to replicate human biases (even those that are not conscious or intended). AI systems have been observed to be able to detect differences of race, gender, and age, even without those details included in the datasets. Even in otherwise innocuous seeming instances, these biases have the potential to translate into problematic outcomes if they replicate various unjust social disparities. However, in the present case, since the data used to train the system was racially heterogeneous (including Chinese, Malay, Indian, and Caucasian subjects) there is at least some good reason to trust the validity of its readings across at least these groups. More research would need to be done to test the real-world validity of Spine AI in other contexts and to observe whether any worrisome disparities emerge.</p><p id="Par23">All that remains to discuss is accountability and trustworthiness. Both of these are strongly linked with the logic underpinning the norm of keeping humans in the loop, so we will discuss them together in the following section.</p></sec><sec id="Sec4"><title>To Automate or Not to Automate?</title><p id="Par24">As mentioned in the outset of this paper, the phrase &#x0201c;humans in the loop&#x0201d; has become the shorthand within the literature to indicate the importance of reserving the ultimate decision-making power for any application of AI technology for humans, and resisting the temptation to outsource to machines (Rahwan <xref ref-type="bibr" rid="CR30">2018</xref>). There are many reasons for this, both moral and practical. We will begin with the practical. Many scholars have pointed out that fully automated use of AI models is not suitable <italic>at this time</italic>, because the technology has not been proven to be sufficiently adept. There are also cost-related reasons to keep the radiologist involved, given the current limitations of the technology. In prior reporting (in Lim et al. <xref ref-type="bibr" rid="CR23">2022</xref>), the authors (including author James Hallinan of this paper) pointed out, "...if the radiologist constantly supervises the job of an AI system, there is no reason to construct superstructures which, inexorably, will slow down the adoption of AI in MI [medical imaging],&#x0201d; meaning that the combination of the radiologist with the AI model is also the most cost-efficient at present. That said, it is reasonable to anticipate the day when this technology would be realistically able to be implemented in an automated fashion, and it is prudent to explore the ethical implications ahead of time, thus our arguments that follow are directed at that likely future scenario.</p><p id="Par25">That said, some have argued that there are reasons to worry that fully automated medical AI could have corrosive effects on trust in the clinical context (Hatherley <xref ref-type="bibr" rid="CR10">2020</xref>). As Hatherley (<xref ref-type="bibr" rid="CR10">2020</xref>) has argued, in order to protect trust in the institution of medicine, it is important that the ultimate responsibility for diagnosis lies with the radiologist/clinician. Though a system like Spine AI may be adept at pattern recognition, this does not mean that it can assess the holistic picture of the health of the patient in question. In our case, however, even if the radiologist were to be kicked out of the loop, the surgeon would remain at the center of the process&#x02014;they must decide whether or not to operate, informed by the AI (with a radiologist or without) and other tests (e.g., neurological tests).</p><p id="Par26">However, these practical concerns about trust and user uptake are likely to be highly contextual and also subject to change as the technology continues to improve. Attitudes about the trustworthiness of AI technologies are highly variable across culture and generation and also dependent upon the ways these technologies are presented and discussed in the wider media (Araujo et al. <xref ref-type="bibr" rid="CR2">2020</xref>). As Bunz and Braghieri (<xref ref-type="bibr" rid="CR3">2022</xref>) discuss in their article, &#x0201c;The AI doctor will see you now: assessing the framing of AI in news coverage,&#x0201d; there is a tendency to discuss AI as &#x0201c;outperforming&#x0201d; human experts and this has the effect of placing it above critique and concern in the minds of the public (including clinicians), leading to a pressure to defer to it even when it might conflict with the Hippocratic oath. Indeed, there may be a tendency to &#x0201c;overtrust&#x0201d; AI rather than mistrust it (Krugel et al. <xref ref-type="bibr" rid="CR20">2022</xref>).</p><p id="Par27">There are also some well-established moral reasons to insist on keeping humans in the loop. For instance, as Jotterand and Bosco (<xref ref-type="bibr" rid="CR17">2020</xref>) have argued, AI technologies allow healthcare professionals to spend less time on many tasks, from administration to technology-related procedures, and this presents the risk of an ever more de-humanized medical system. They go on to argue that artificial intelligence should only be implemented <italic>at all</italic> in the medical context where it meets three criteria: (1) it serves human ends, (2) respects personal identity, and (3) promotes human interaction (Jotterand and Bosco <xref ref-type="bibr" rid="CR17">2020</xref>). The third criterion seems to be the most crucial, in that it is the deciding factor regarding whether healthcare AI will make the practice of medicine more human or further de-humanize it.</p><p id="Par28">The prospect of de-humanized medicine should concern us for several reasons. First and foremost is the issue of accountability in the case of something going wrong (see Kempt et al. <xref ref-type="bibr" rid="CR19">2023</xref>). Tobia et al. (<xref ref-type="bibr" rid="CR34">2021</xref>) have examined the ways in which reliance on medical AI can increase liability in medical malpractice. They state,<disp-quote><p id="Par29">Our results indicate that physicians who receive advice from an AI system to provide standard care can reduce the risk of liability by accepting, rather than rejecting, that advice, all else equal. However, when an AI system recommends nonstandard care, there is no similar shielding effect of rejecting that advice and so providing standard care. The tort law system is unlikely to undermine the use of AI precision medicine tools and may even encourage the use of these tools (Tobia et al. <xref ref-type="bibr" rid="CR34">2021</xref>).<xref ref-type="fn" rid="Fn2">2</xref></p></disp-quote></p><p id="Par30">Grote (<xref ref-type="bibr" rid="CR8">2021</xref>) has found that various studies examining the interaction of AI systems and physicians have shown that without being able to evaluate their trustworthiness, physicians (especially novices) become over-reliant on this support&#x02014;and ultimately are thus more likely to be led astray by incorrect decisions. Yet, other scholars have come to different conclusions. For instance, Lang (<xref ref-type="bibr" rid="CR21">2022</xref>) has explored the relationship between the physician and automated &#x0201c;decision support systems&#x0201d; (AI-DSS) observing, that when the AI&#x02019;s diagnostic accuracy supercedes or at least matches the performance of a human expert, healthcare administration can improve diagnostic performance by introducing AI-DSS without the unintended byproduct of a responsibility gap.</p><p id="Par31">If we try to apply these critiques within the Spine AI case, we quickly encounter a mismatch. These concerns do not seem applicable in our use case, since Spine AI is not responsible for recommending any specific treatment for the patient, it is simply a measuring tool. Like any such tool, the information it provides may inform or support the decision of a clinician, but its role stops well short of advising any particular treatment plan. This highlights the importance of taking a bottom-up approach, as we intend to do in this paper. The particulars of each kind of health AI can vary so widely that broad generalizations are unlikely to accurately describe or adequately cover the true ethical landscape. Turning again to the Spine AI case, we can see that this scenario was one where the technology was found to be transparent, highly reliable, and low in risk to patients.</p><p id="Par32">With regard to the question of accountability, the use of an AI like Spine AI does not meaningfully alter the existing ethical and legal requirements for care providers to follow the relevant standard of care. In this case, the surgeon or orthopedist responsible for the patient is still ultimately responsible for decisions regarding their patient&#x02019;s care. Kicking the human out of the radiological loop does not affect where the final responsibility lies, even if it ends up that the AI might make some mistake (see Mello and Guha <xref ref-type="bibr" rid="CR27">2024</xref> for more about liability risks in healthcare AI).</p><p id="Par33">Spine AI also appears to force us to critically examine the real moral significance of &#x0201c;de-humanized&#x0201d; medicine. While automating a system like this could &#x0201c;de-humanize&#x0201d; this area of medicine in one sense&#x02014;by removing or reducing the need for human radiologists&#x02014;we should not automatically assume that this is ethically meaningful. If de-humanization is morally concerning because of the criteria Jotterand and Bosco (<xref ref-type="bibr" rid="CR17">2020</xref>) laid out, then removing the role of radiologists could possibly be said to fall under the third criterion. Kicking any human out of any loop plausibly reduces human interaction. But who&#x02019;s perspective matters here? If it is the patient&#x02019;s perspective that matters, in this case, human interaction would not meaningfully be reduced, since radiologists often do not see or interact directly with patients at all, so a patient is unlikely to notice this form of automation. Plausibly, it may weaken any existing bond between clinicians and the radiologists with whom they regularly work, but it is far from obvious why this would be a serious or meaningful instance of &#x0201c;de-humanized&#x0201d; medicine. After all, the surgeon would still be responsible for outlining the case for surgery, based on all results of tests, and obtaining informed consent for surgery. Further, it is not obvious that patients do in fact care that their medical care is humanistic, especially when weighed with other values, like efficacy and efficiency. Thus, Jotterand and Bosco&#x02019;s criteria should not be treated as reflecting obvious, essential, or enduring patient preferences. It therefore seems that employing an automated version of the Spine AI program is unlikely to <italic>meaningfully</italic> reduce the human element in the medical relationship either. In light of this, we believe this case presents an interesting and important opportunity to re-examine the future possibility of kicking humans out of the loop, especially if the opportunity costs of resisting automation are given their proper due.</p></sec><sec id="Sec5"><title>Opportunity Costs and Tradeoffs</title><p id="Par34">There is a realistic possibility of automation replacing a significant proportion of a radiologist&#x02019;s current ordinary workload. Ho et al. (<xref ref-type="bibr" rid="CR15">2019</xref>) identified numerous activities that stand to be automated, including (1) automated image segmentation, lesion detection, measurement, labeling, and comparisons with historical images; (2) generating radiology reports, particularly with the application of natural language processing and natural language generation; (3) semantic error detection reports; (4) data mining research; and (5) improved business intelligence systems that allow real-time dash-boarding and alert systems, workflow analysis and improvement, outcomes measures and performance assessment (330). While some of the hazards of implementing AI in healthcare have been well established in the literature, discussion of the opportunity costs of <italic>not</italic> capitalizing on the benefits of automation for these novel technologies has received comparatively less attention. Without acknowledgement of the tradeoffs involved, no true assessment of the moral picture can be made. The present case of medical imaging for LSS provides an interesting opportunity to remedy that gap.</p><p id="Par35">As was noted in the Spine AI case, the potential savings in terms of both time and money that stand to be gained from this technology are significant (e.g., ~469,000 minutes (7817 hours) and up to $780,000 SGD each year in Singapore alone). Along with the savings in time and money, there are also meaningful gains to be made in the manpower needed. Again, as noted in the case study, the Spine AI model is able to improve the accuracy of inexperienced trainees, meaning the use of such technologies has the potential to also make each individual radiologist more efficient and effective. The benefits of decreased turnaround time for the reading of scans for LSS should not be understated. In some developing or underdeveloped countries, where medical resources are extremely scarce, the benefits brought about by automated technology are significant. The cost of delaying or foregoing automation is not only economic, but also includes significant health costs and potentially much higher mortality rates. In some situations, patients can wait months for a diagnosis, and this can have a significant impact on their well-being and prognosis, as well as their income and ability to support their families. Automating the time consuming process of image reading would also mean that more people could be effectively served by a smaller cohort of radiologists. Indeed, in some contexts, the backlog of medical images simply means no one ever reads them. In effect, then, the real tradeoff we are presented with when choosing to use or not use an automated image reading system like Spine AI in low-resource settings can be more accurately described as the difference between a patient&#x02019;s scan being read by a machine or not read <italic>at all.</italic> In fact, in practice, other countries have already chosen to implement automated systems for medical conditions that are more risky than LSS for similar reasons. For instance, in Hong Kong, an automated scan reading system (with no human in the loop and no parallel checks) has been trialed for the detection of intracranial hemorrhage (Abrigo et al. <xref ref-type="bibr" rid="CR1">2023</xref>). Because the tradeoffs involved are steeper, the promotion and adoption of automated technology can hold a greater urgency and necessity.</p></sec><sec id="Sec6"><title>Criteria for Kicking (Some) Humans Out of the Loop</title><p id="Par36">The norm of keeping humans in the loop is aimed at keeping humans in control over the new technology and making sure that human interests are centered in the process. However, there is no standard sense of what it means to be &#x0201c;in the loop&#x0201d; or indeed of what constitutes a &#x0201c;loop&#x0201d; in the first place. Additionally, it is important to identify which loops are in fact relevant and what kind of role the human should occupy in each (Herman and Pfeiffer, <xref ref-type="bibr" rid="CR13">2023</xref>).<xref ref-type="fn" rid="Fn3">3</xref> In the case of LSS we can break down the tasks involved into the following:<list list-type="order"><list-item><p id="Par37">Initial patient visit</p></list-item><list-item><p id="Par38">Scanning the patient</p></list-item><list-item><p id="Par39">Segmenting the scan to identify morphology</p></list-item><list-item><p id="Par40">Classifying/grading the degree of stenosis in each location</p></list-item><list-item><p id="Par41">Generating a diagnosis of LSS</p></list-item><list-item><p id="Par42">Making judgements about prognosis</p></list-item><list-item><p id="Par43">Making treatment recommendations.</p></list-item></list></p><p id="Par44">Though the full &#x0201c;loop&#x0201d; from the patient&#x02019;s perspective involves 1&#x02013;7, there is an identifiable and separate radiological &#x0201c;loop&#x0201d; from 2 through 5, and a clinician or surgeon would be in charge of overseeing the other steps. The level of automation we are suggesting in this case would apply to the radiological sub-loop only (e.g., doing away with the radiologist&#x02019;s role in supervising or performing parallel checks). What criteria, if any, would make that move ethically permissible? To that question, we propose the following:<list list-type="order"><list-item><p id="Par45">The technology is as effective (or better) than a human at the given task (e.g., error rates are equal to or lower than human experts).</p></list-item><list-item><p id="Par46">The risk to patients (or any humans involved) is low in the event of an error.</p></list-item><list-item><p id="Par47">The wellbeing that is gained by the speed, accuracy, and cost-efficiency of automation is high.</p></list-item></list></p><p id="Par48">We will explain each criterion in more detail. The Spine AI case presents a good opportunity to examine how these criteria can guide our sense of when it is, or is not, important to keep humans in the loop (and perhaps even more narrowly, when it is important to keep <italic>which</italic> humans in the loop). As mentioned in the case study, the algorithm was able to perform as well or better than a human expert at the given task (measuring the spine to diagnose LSS). The risk to patients in the event of an error on the part of the algorithm is also low (see <xref rid="Sec3" ref-type="sec">Discussion</xref> section). The Spine AI program is responsible only for making measurements on the MRI scans of the spine, not for any direct treatment of the patient. The riskiness of the condition itself also influences the moral calculus.</p><p id="Par49">While LSS is a potentially debilitating condition, affecting many adults globally and with a considerable impact on livelihood, it is not life-threatening, meaning the stakes in the event of misdiagnosis are lower than if the target condition was a condition like heart failure. Because of the specific features of this kind of case, even if humans are kicked out of the radiological loop, a surgeon will still be available to look at the scan before further treatment decisions are made or implemented, further minimizing risk to patients.</p><p id="Par50">Additionally, other kinds of worries that have been raised regarding the use of medical imaging&#x02014;like the risk of perpetuating harmful bias&#x02014;seem minimal or absent given the features of the present case. While some scholars have called attention to various kinds of bias in healthcare (which AI can amplify or enshrine), not all conditions present the same levels of risk when it comes to unjust disparities in care. LSS is not a historically stigmatized condition, given its prevalence across groups, there is no notable association between LSS and negative social stereotypes (like assumptions about laziness). This is good news, since it should mean that there is less risk of the Spine AI program worsening these kinds of worrying biases. That said, racial stereotypes or stigma are far from the only origins for unjust disparities in care (e.g., group differences in consumption of preventive care, and different rates in the uptake of screenings), it nevertheless seems safe to assume that the Spine AI is unlikely to worsen any of these disparities through its use. Given the heterogeneity of the population the AI was trained on, the likelihood of racial bias is low (Hallinan et al. <xref ref-type="bibr" rid="CR9">2021</xref>).</p><p id="Par51">A case like that of LSS also presents a strong incentive in terms of the well-being generated through automation&#x02019;s ability to increase the speed and accuracy of diagnosis while significantly reducing costs. LSS affects people of all races and genders, approximately 103 million people worldwide, equivalent to 11% of the world&#x02019;s population (Katz et al. <xref ref-type="bibr" rid="CR18">2022</xref>). Hence, there is a significant opportunity cost to insisting on keeping humans in the loop for parallel checks on the algorithm&#x02019;s measurements (increasing cost, less efficiency, more manpower, etc.). As mentioned, the tradeoff is most significant in low and middle income countries (LMICs) where radiologists are often few and far between, and turnaround time for reading the scans can take months, if it happens at all. Automation in this kind of application, therefore, serves the interests of both beneficence and justice. That said, automation of image reading cannot solve all the problems faced by LMICs, which may still struggle with structural limitations like the unavailability of MRI machines to do the scanning in the first place, or surgeons to treat LSS once diagnosed.</p><p id="Par52">Lastly, with a program like Spine AI, the task being automated is one that is mundane and repetitive. This places Spine AI in contrast to more contentious forms of AI (as in the creative realms, like art, writing, etc.), where AI can seem to threaten rather than enable a person&#x02019;s ability to engage in meaningful human experiences. If such mundane and repetitive tasks are automated, the radiologists themselves do not meaningfully miss out on an opportunity to develop important human qualities or connections. In contrast, their time and attention are liberated and thereby more available for the other dimensions of their profession. Given all these features of the Spine AI case (and by extension any case where these criteria hold), we would argue that it would be morally permissible to kick humans out of the loop.</p></sec><sec id="Sec7"><title>Objections</title><p id="Par53">One possible critique of these arguments is that what we are suggesting does not in fact amount to &#x0201c;kicking humans out of the loop.&#x0201d; After all, depending on how one defines a &#x0201c;loop&#x0201d; the label may still be said to apply if a human has the ultimate deciding power. In the case of Spine AI, a human clinician would still inevitably be involved in a later stage of the process (since the taking and the reading of the scan are sandwiched between meetings between the patient and clinician) and would be responsible for other aspects of the diagnostic and treatment process. One may rightly wonder whether this truly is an instance of automation. Essentially, the question comes down to what actually counts as a &#x0201c;loop&#x0201d;&#x02014;and to which loops are in fact morally important for humans to occupy?</p><p id="Par54">On this note, we are happy to admit that while we may be able to remove some humans from one &#x0201c;loop,&#x0201d; there are still other loops where human involvement remains essential. It was never our intention to argue that humans should be kicked out of all the possible &#x0201c;loops,&#x0201d; nevertheless, we think this is a significant enough step to be worthy of mention. Allowing Spine AI to interpret patient scans without the oversight and monitoring of a radiologist removes the need for parallel checks, which at minimum reduces the number of radiologists needed to serve a given population. Indeed, because of the specifics of this kind of case and the patient pathway involved, even if radiologists are taken out of the loop, a surgeon is present to look at the scan as well. We are not removing all humans from all loops, but removing some humans from one loop. Even if the &#x0201c;loop&#x0201d; that makes up the subject matter of this paper is admittedly a small one, the criteria we have put forth are likely to hold in other instances beyond the present discussion. Such cases are likely to increase in number as the suite of AI technology develops. In line with this, we would argue that it would be permissible to remove the human overseers from those loops as well. Kicking one human radiologist out of one procedural loop may not sound like much, but it is significant since it challenges the assumption that it is something that should in principle never be done.</p><p id="Par55">A second objection that can be foreseen comes from a more pragmatic angle. Knowing that the population is unused to this kind of technology, and that AI has acquired a kind of mystique that leads many to fear it by default&#x02014;there are practical barriers to implementing even such a minor and innocuous degree of automation. The reasoning may be practical rather than moral in this instance. However, once the reality of the low risks and high rewards are fully communicated, this resistance should wane, and then there would no longer be a reason, moral or practical, to avoid autonomous AIs in these cases which meet the criteria we have laid out. It is important to remember that an insistence on keeping more humans than necessary in the loop can sometimes involve harm in an indirect sense, and this presents us with a trade-off that is, at minimum, important to acknowledge.</p></sec><sec id="Sec8"><title>Conclusion</title><p id="Par56">Applications for AI in healthcare continue to develop at a breakneck speed. As they do so, new challenges will emerge, as well as new opportunities. With the discussion of our case, we have highlighted some of the often neglected tradeoffs involved in the development of new ethical norms around novel technologies like this. It is important, as many have highlighted, to ensure to the best of our ability that these new technologies do not expose patients to undue risks, worsen unjust social disparities, or undermine trust in medicine as a whole. But along with those concerns, we must also take care not to lose sight of the very significant benefits that can be provided by these advances and some humans should be kicked out of the loop. It is prudent to use training wheels when learning to ride a bike, but also prudent to anticipate the day that the training wheels can safely come off. To that end, we hope that the criteria laid out here will give some sense of that suitable threshold. When these criteria are met, and there are significant and meaningful gains to be made, we argue that some humans can permissibly be &#x0201c;kicked out of the loop&#x0201d;.</p></sec></body><back><fn-group><fn id="Fn1"><label>1</label><p id="Par61">A kappa value is used to measure the agreement between two people (or entities) when categorizing something (in this case LSS). The kappa score expresses how much agreement there is between the two beyond what could be expected by chance alone, with a high kappa score meaning more agreement or overlap.</p></fn><fn id="Fn2"><label>2</label><p id="Par62">Their findings echo what has been dubbed &#x0201c;the asymmetry of credit and blame&#x0201d; (see Mann et al. (<xref ref-type="bibr" rid="CR25">2023</xref>) for more on the credit-blame asymmetry in large language models).</p></fn><fn id="Fn3"><label>3</label><p id="Par63">Hermann and Pfeiffer (<xref ref-type="bibr" rid="CR13">2023</xref>) identify four different loops in their article: (1) using and assessing AI-output, (2) customizing the AI system, (3) original tasks supported with AI, (4) contextual changes to the system. Writing as they are in the context of organization and AI development, this list is not easily transportable to the Spine AI case, nor is there reason to think this list is exhaustive.</p></fn><fn><p><bold>Publisher&#x02019;s Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author Contributions</title><p>All authors contributed to the ideation and argumentation of the paper. The lead author, Kathryn Muyskens, was responsible for the initial drafting of the paper. Co-authors, Yonghui Ma, Jerry Menikoff, and Julian Savulescu, provided editorial comments. Dr James Hallinan provided the initial case study on which this is based and helped to guide and inform the empirical content of this paper.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>&#x02022; This work was supported by the Singapore Ministry of Health&#x02019;s National Medical Research Council under its Science, Health, and Policy Relevant Ethics, Singapore (SHAPES) Programme (grant number MOH-000951).</p><p>&#x02022; This research was funded in whole, or in part, by the Wellcome Trust (grant number WT203132/Z/16/Z) and (226801/Z/22/Z). For the purpose of open access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.</p><p>&#x02022; This work was supported by the Chinese Social Science Foundation (grant number 19ZDA039); the Social Science Foundation of Fujian Province (grant number FJ2022B146).</p><p>&#x02022; This work was supported by the National University of Singapore,&#x000a0;NUS Start-up Grant [grant number NUHSRO/2022/035/Startup/05.</p></notes><notes><title>Declarations</title><notes id="FPar1"><title>Ethics Approval</title><p id="Par57">N/A.</p></notes><notes id="FPar2"><title>Consent to Participate</title><p id="Par58">N/A.</p></notes><notes id="FPar3"><title>Consent for Publication</title><p id="Par59">N/A.</p></notes><notes id="FPar4" notes-type="COI-statement"><title>Competing Interests</title><p id="Par60">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title> References</title><ref id="CR1"><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Abrigo</surname><given-names>JM</given-names></name><name><surname>Ko</surname><given-names>K-l</given-names></name><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Lai</surname><given-names>BMH</given-names></name><name><surname>Cheung</surname><given-names>TCY</given-names></name><name><surname>Chu</surname><given-names>WCW</given-names></name><name><surname>Yu</surname><given-names>SCH</given-names></name></person-group><article-title>Artificial intelligence for detection of intracranial haemorrhage on head computed tomography scans: diagnostic accuracy in Hong Kong</article-title><source>Hong Kong Medical Journal</source><year>2023</year><volume>29</volume><issue>2</issue><fpage>112</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.12809/hkmj209053</pub-id><pub-id pub-id-type="pmid">37088699</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Abrigo, Jill M., Ka-long Ko, Qianyun Chen, Billy M.H. Lai, Tom C.Y. Cheung, Winnie C.W. Chu, and Simon C.H. Yu. 2023. Artificial intelligence for detection of intracranial haemorrhage on head computed tomography scans: diagnostic accuracy in Hong Kong. <italic>Hong Kong Medical Journal</italic> 29 (2): 112&#x02013;120. 10.12809/hkmj209053.<pub-id pub-id-type="pmid">37088699</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Araujo</surname><given-names>Theo</given-names></name><name><surname>Helberger</surname><given-names>Natali</given-names></name><name><surname>Kruikemeier</surname><given-names>Sanne</given-names></name><name><surname>de Vreese</surname><given-names>Claes H</given-names></name></person-group><article-title>In AI we trust? Perceptions about automated decision-making by artificial intelligence</article-title><source>AI &#x00026; Society</source><year>2020</year><volume>35</volume><issue>3</issue><fpage>611</fpage><lpage>623</lpage><pub-id pub-id-type="doi">10.1007/s00146-019-00931-w</pub-id></element-citation><mixed-citation id="mc-CR2" publication-type="journal">Araujo, Theo, Natali Helberger, Sanne Kruikemeier, and Claes H. de Vreese. 2020. In AI we trust? Perceptions about automated decision-making by artificial intelligence. <italic>AI &#x00026; Society</italic> 35 (3): 611&#x02013;623. 10.1007/s00146-019-00931-w.</mixed-citation></citation-alternatives></ref><ref id="CR3"><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Bunz</surname><given-names>Mercedes</given-names></name><name><surname>Braghieri</surname><given-names>Marco</given-names></name></person-group><article-title>The AI doctor will see you now: assessing the framing of AI in news coverage</article-title><source>AI &#x00026; Society</source><year>2022</year><volume>37</volume><issue>1</issue><fpage>9</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1007/s00146-021-01145-9</pub-id><pub-id pub-id-type="pmid">35291266</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Bunz, Mercedes, and Marco Braghieri. 2022. The AI doctor will see you now: assessing the framing of AI in news coverage. <italic>AI &#x00026; Society</italic> 37 (1): 9&#x02013;22. 10.1007/s00146-021-01145-9.<pub-id pub-id-type="pmid">35291266</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>Berman</given-names></name></person-group><article-title>Black-box assisted medical decisions: AI power vs. ethical physician care</article-title><source>Medicine, Health Care and Philosophy</source><year>2023</year><volume>26</volume><issue>3</issue><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1007/s11019-023-10153-z</pub-id><pub-id pub-id-type="pmid">36656495</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Chan, Berman. 2023. Black-box assisted medical decisions: AI power vs. ethical physician care. <italic>Medicine, Health Care and Philosophy</italic> 26 (3): 1&#x02013;8. 10.1007/s11019-023-10153-z.<pub-id pub-id-type="pmid">36656495</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Chockley</surname><given-names>K</given-names></name><name><surname>Emanuel</surname><given-names>E</given-names></name></person-group><article-title>The end of radiology? Three threats to the future practice of radiology</article-title><source>Journal of the American College of Radiology</source><year>2016</year><volume>13</volume><issue>12</issue><fpage>1415</fpage><lpage>1142</lpage><pub-id pub-id-type="doi">10.1016/j.jacr.2016.07.010</pub-id><pub-id pub-id-type="pmid">27652572</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Chockley, K., and E. Emanuel. 2016. The end of radiology? Three threats to the future practice of radiology. <italic>Journal of the American College of Radiology</italic> 13 (12): 1415&#x02013;1142. 10.1016/j.jacr.2016.07.010.<pub-id pub-id-type="pmid">27652572</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Crawford</surname><given-names>Kate</given-names></name><name><surname>Paglen</surname><given-names>Trevor</given-names></name></person-group><article-title>Excavating AI: the politics of images in machine learning training sets</article-title><source>AI &#x00026; Society</source><year>2021</year><volume>36</volume><issue>4</issue><fpage>1105</fpage><lpage>1116</lpage><pub-id pub-id-type="doi">10.1007/s00146-021-01301-1</pub-id></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Crawford, Kate, and Trevor Paglen. 2021. Excavating AI: the politics of images in machine learning training sets. <italic>AI &#x00026; Society</italic> 36 (4): 1105&#x02013;1116.&#x000a0;<ext-link ext-link-type="uri" xlink:href="https://link.springer.com/article/10.1007/s00146-021-01162-8">https://link.springer.com/article/10.1007/s00146-021-01162-8</ext-link>.</mixed-citation></citation-alternatives></ref><ref id="CR7"><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrario</surname><given-names>Andrea</given-names></name></person-group><article-title>Design of black box algorithms: a support to the epistemic and ethical justifications of medical AI systems</article-title><source>Journal of Medical Ethics</source><year>2022</year><volume>48</volume><issue>7</issue><fpage>492</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1136/medethics-2021-107482</pub-id><pub-id pub-id-type="pmid">33980658</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Ferrario, Andrea. 2022. Design of black box algorithms: a support to the epistemic and ethical justifications of medical AI systems. <italic>Journal of Medical Ethics</italic> 48 (7): 492&#x02013;494. 10.1136/medethics-2021-107482.<pub-id pub-id-type="pmid">33980658</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Grote</surname><given-names>Thomas</given-names></name></person-group><article-title>Trustworthy medical AI systems need to know what they don&#x02019;t know</article-title><source>Journal of Medical Ethics</source><year>2021</year><volume>47</volume><issue>1</issue><fpage>337</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1136/medethics-2021-107463</pub-id></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Grote, Thomas. 2021. Trustworthy medical AI systems need to know what they don&#x02019;t know. <italic>Journal of Medical Ethics</italic> 47 (1): 337&#x02013;338. 10.1136/medethics-2021-107463.</mixed-citation></citation-alternatives></ref><ref id="CR9"><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Hallinan</surname><given-names>JTPD</given-names></name><name><surname>Zhu</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Makmur</surname><given-names>A</given-names></name><name><surname>Algazwi</surname><given-names>DAR</given-names></name><name><surname>Thian</surname><given-names>YL</given-names></name><name><surname>Lau</surname><given-names>S</given-names></name><name><surname>Choo</surname><given-names>YS</given-names></name><name><surname>Eide</surname><given-names>SE</given-names></name><name><surname>Yap</surname><given-names>QV</given-names></name><name><surname>Chan</surname><given-names>YH</given-names></name><name><surname>Tan</surname><given-names>JH</given-names></name><name><surname>Kumar</surname><given-names>N</given-names></name><name><surname>Ooi</surname><given-names>BC</given-names></name><name><surname>Yoshioka</surname><given-names>H</given-names></name><name><surname>Quek</surname><given-names>ST</given-names></name></person-group><article-title>Deep learning model for automated detection and classification of central canal, lateral recess, and neural foraminal stenosis at lumbar spine MRI</article-title><source>Radiology</source><year>2021</year><volume>300</volume><issue>1</issue><fpage>130</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1148/radiol.2021204289</pub-id><pub-id pub-id-type="pmid">33973835</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Hallinan, J.T.P.D., L. Zhu, K. Yang, A. Makmur, D.A.R. Algazwi, Y.L. Thian, S. Lau, Y.S. Choo, S.E. Eide, Q.V. Yap, Y.H. Chan, J.H. Tan, N. Kumar, B.C. Ooi, H. Yoshioka, and S.T. Quek. 2021. Deep learning model for automated detection and classification of central canal, lateral recess, and neural foraminal stenosis at lumbar spine MRI. <italic>Radiology</italic> 300 (1): 130&#x02013;138. 10.1148/radiol.2021204289.<pub-id pub-id-type="pmid">33973835</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Hatherley</surname><given-names>Joshua James</given-names></name></person-group><article-title>Limits of trust in medical AI</article-title><source>Journal of Medical Ethics</source><year>2020</year><volume>46</volume><issue>7</issue><fpage>478</fpage><lpage>481</lpage><pub-id pub-id-type="doi">10.1136/medethics-2019-105935</pub-id><pub-id pub-id-type="pmid">32220870</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Hatherley, Joshua James. 2020. Limits of trust in medical AI. <italic>Journal of Medical Ethics</italic> 46 (7): 478&#x02013;481. 10.1136/medethics-2019-105935.<pub-id pub-id-type="pmid">32220870</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Hatherley</surname><given-names>Joshua</given-names></name><name><surname>Sparrow</surname><given-names>Robert</given-names></name><name><surname>Howard</surname><given-names>Mark</given-names></name></person-group><article-title>The virtues of interpretable medical AI</article-title><source>Cambridge Quarterly of Healthcare Ethics</source><year>2022</year><pub-id pub-id-type="doi">10.1017/S0963180122000305</pub-id></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Hatherley, Joshua, Robert Sparrow, and Mark Howard. 2022. The virtues of interpretable medical AI. <italic>Cambridge Quarterly of Healthcare Ethics</italic>. 10.1017/S0963180122000305.</mixed-citation></citation-alternatives></ref><ref id="CR12"><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Hayashi</surname><given-names>David</given-names></name></person-group><article-title>Deep learning for lumbar spine MRI reporting: a welcome tool for radiologists</article-title><source>Radiology</source><year>2021</year><volume>300</volume><issue>1</issue><fpage>139</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1148/radiol.2021210730</pub-id><pub-id pub-id-type="pmid">33973842</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Hayashi, David. 2021. Deep learning for lumbar spine MRI reporting: a welcome tool for radiologists. <italic>Radiology</italic> 300 (1): 139&#x02013;140. 10.1148/radiol.2021210730.<pub-id pub-id-type="pmid">33973842</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Hermann</surname><given-names>Thomas</given-names></name><name><surname>Pfeiffer</surname><given-names>Sabine</given-names></name></person-group><article-title>Keeping the organization in the loop: a socio-technical extension of human centered artificial intelligence</article-title><source>AI &#x00026; Society</source><year>2023</year><volume>38</volume><fpage>1523</fpage><lpage>1542</lpage><pub-id pub-id-type="doi">10.1007/s00146-022-01391-5</pub-id></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Hermann, Thomas, and Sabine Pfeiffer. 2023. Keeping the organization in the loop: a socio-technical extension of human centered artificial intelligence. <italic>AI &#x00026; Society</italic> 38: 1523&#x02013;1542. 10.1007/s00146-022-01391-5.</mixed-citation></citation-alternatives></ref><ref id="CR14"><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Hida</surname><given-names>Tetsuro</given-names></name><name><surname>Eastlack</surname><given-names>Robert K</given-names></name><name><surname>Kanemura</surname><given-names>Tokumi</given-names></name><name><surname>Mundis</surname><given-names>Gregory M</given-names><suffix>Jr</suffix></name><name><surname>Imagama</surname><given-names>Shiro</given-names></name><name><surname>Akbarnia</surname><given-names>Behrooz A</given-names></name></person-group><article-title>Effect of race, age, and gender on lumbar muscle volume and fat infiltration in the degenerative spine</article-title><source>Journal of Orthopedic Science</source><year>2021</year><volume>26</volume><issue>1</issue><fpage>69</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1016/j.jos.2019.09.006</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Hida, Tetsuro, Robert K. Eastlack, Tokumi Kanemura, Gregory M. Mundis Jr., Shiro Imagama, and Behrooz A. Akbarnia. 2021. Effect of race, age, and gender on lumbar muscle volume and fat infiltration in the degenerative spine. <italic>Journal of Orthopedic Science</italic> 26 (1): 69&#x02013;74. 10.1016/j.jos.2019.09.006.</mixed-citation></citation-alternatives></ref><ref id="CR15"><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>C</given-names></name><name><surname>Soon</surname><given-names>D</given-names></name><name><surname>Caals</surname><given-names>K</given-names></name><name><surname>Kapur</surname><given-names>J</given-names></name></person-group><article-title>Governance of automated image analysis and artificial intelligent analytics in healthcare</article-title><source>Clinical Radiology</source><year>2019</year><volume>74</volume><fpage>329</fpage><lpage>337</lpage><pub-id pub-id-type="doi">10.1016/j.crad.2019.02.005</pub-id><pub-id pub-id-type="pmid">30898383</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Ho, Calvin, Derek Soon, Karel Caals, and Jeevesh Kapur. 2019. Governance of automated image analysis and artificial intelligent analytics in healthcare. <italic>Clinical Radiology</italic> 74: 329&#x02013;337. 10.1016/j.crad.2019.02.005.<pub-id pub-id-type="pmid">30898383</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Jha</surname><given-names>Saurabh</given-names></name><name><surname>Topol</surname><given-names>Eric J</given-names></name></person-group><article-title>Adapting to artificial intelligence: radiologists and pathologists as information specialists</article-title><source>Journal of the American Medical Association</source><year>2016</year><volume>316</volume><issue>22</issue><fpage>2353</fpage><lpage>2354</lpage><pub-id pub-id-type="doi">10.1001/jama.2016.17438</pub-id><pub-id pub-id-type="pmid">27898975</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Jha, Saurabh, and Eric J. Topol. 2016. Adapting to artificial intelligence: radiologists and pathologists as information specialists. <italic>Journal of the American Medical Association</italic> 316 (22): 2353&#x02013;2354. 10.1001/jama.2016.17438.<pub-id pub-id-type="pmid">27898975</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Jotterand</surname><given-names>Fabrice</given-names></name><name><surname>Bosco</surname><given-names>Clara</given-names></name></person-group><article-title>Keeping the &#x02018;human in the loop&#x02019; in the age of artificial intelligence: accompanying commentary for &#x02018;correcting the brain?&#x02019; by Rainey and Erden</article-title><source>Science and Engineering Ethics</source><year>2020</year><volume>26</volume><issue>5</issue><fpage>2455</fpage><lpage>2460</lpage><pub-id pub-id-type="doi">10.1007/s11948-020-00241-1</pub-id><pub-id pub-id-type="pmid">32643058</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Jotterand, Fabrice, and Clara Bosco. 2020. Keeping the &#x02018;human in the loop&#x02019; in the age of artificial intelligence: accompanying commentary for &#x02018;correcting the brain?&#x02019; by Rainey and Erden. <italic>Science and Engineering Ethics</italic> 26 (5): 2455&#x02013;2460. 10.1007/s11948-020-00241-1.<pub-id pub-id-type="pmid">32643058</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Katz</surname><given-names>JN</given-names></name><name><surname>Zimmerman</surname><given-names>ZE</given-names></name><name><surname>Mass</surname><given-names>H</given-names></name><name><surname>Makhni</surname><given-names>MC</given-names></name></person-group><article-title>Diagnosis and management of lumbar spinal stenosis</article-title><source>JAMA Network</source><year>2022</year><volume>327</volume><issue>17</issue><fpage>1688</fpage><lpage>1699</lpage><pub-id pub-id-type="doi">10.1001/jama.2022.5921</pub-id></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Katz, Jeffrey N., Zoe E. Zimmerman, Hanna Mass, and Melvin C. Makhni. 2022. Diagnosis and management of lumbar spinal stenosis. <italic>JAMA Network</italic> 327 (17): 1688&#x02013;1699. 10.1001/jama.2022.5921.</mixed-citation></citation-alternatives></ref><ref id="CR19"><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Kempt</surname><given-names>Hendrik</given-names></name><name><surname>Heilinger</surname><given-names>Jan-Christoph</given-names></name><name><surname>Nagel</surname><given-names>Saskia K</given-names></name></person-group><article-title>&#x02018;I&#x02019;m afraid I can&#x02019;t let you do that, Doctor.&#x02019;: meaningful disagreements with AI in medical contexts</article-title><source>AI &#x00026; Society</source><year>2023</year><volume>1</volume><issue>8</issue><fpage>1407</fpage><lpage>1414</lpage><pub-id pub-id-type="doi">10.1007/s00146-022-01418-x</pub-id></element-citation><mixed-citation id="mc-CR19" publication-type="journal">Kempt, Hendrik, Jan-Christoph. Heilinger, and Saskia K. Nagel. 2023. &#x02018;I&#x02019;m afraid I can&#x02019;t let you do that, Doctor.&#x02019;: meaningful disagreements with AI in medical contexts. <italic>AI &#x00026; Society</italic> 1 (8): 1407&#x02013;1414. 10.1007/s00146-022-01418-x.</mixed-citation></citation-alternatives></ref><ref id="CR20"><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Krugel</surname><given-names>Sebastian</given-names></name><name><surname>Ostermaier</surname><given-names>Andreas</given-names></name><name><surname>Uhl</surname><given-names>Matthias</given-names></name></person-group><article-title>Zombies in the loop? Humans trust untrustworthy AI-advisors for ethical decisions</article-title><source>Philosophy and Technology</source><year>2022</year><volume>35</volume><issue>1</issue><fpage>17</fpage><pub-id pub-id-type="doi">10.1007/s13347-022-00511-9</pub-id></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Krugel, Sebastian, Andreas Ostermaier, and Matthias Uhl. 2022. Zombies in the loop? Humans trust untrustworthy AI-advisors for ethical decisions. <italic>Philosophy and Technology</italic> 35 (1): 17. 10.1007/s13347-022-00511-9.</mixed-citation></citation-alternatives></ref><ref id="CR21"><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Lang</surname><given-names>Benjamin H</given-names></name></person-group><article-title>Are physicians requesting a second opinion really engaging in a reason-giving dialectic? Normative questions on the standards for second opinions and AI</article-title><source>Journal of Medical Ethics</source><year>2022</year><volume>48</volume><issue>4</issue><fpage>234</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1136/medethics-2022-108246</pub-id><pub-id pub-id-type="pmid">35321906</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Lang, Benjamin H. 2022. Are physicians requesting a second opinion really engaging in a reason-giving dialectic? Normative questions on the standards for second opinions and AI. <italic>Journal of Medical Ethics</italic> 48 (4): 234&#x02013;235. 10.1136/medethics-2022-108246.<pub-id pub-id-type="pmid">35321906</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Liefgreen</surname><given-names>Alice</given-names></name><name><surname>Weinstein</surname><given-names>Netta</given-names></name><name><surname>Wachter</surname><given-names>Sandra</given-names></name><name><surname>Mittelstadt</surname><given-names>Brent</given-names></name></person-group><article-title>Beyond ideals: why the (medical) AI industry needs to motivate behavioural change in line with fairness and transparency values, and how it can do it</article-title><source>AI &#x00026; Society</source><year>2023</year><pub-id pub-id-type="doi">10.1007/s00146-023-01684-3</pub-id></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Liefgreen, Alice, Netta Weinstein, Sandra Wachter, and Brent Mittelstadt. 2023. Beyond ideals: why the (medical) AI industry needs to motivate behavioural change in line with fairness and transparency values, and how it can do it. <italic>AI &#x00026; Society</italic>. 10.1007/s00146-023-01684-3.</mixed-citation></citation-alternatives></ref><ref id="CR23"><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>DSW</given-names></name><name><surname>Makmur</surname><given-names>A</given-names></name><name><surname>Zhu</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Cheng</surname><given-names>AJL</given-names></name><name><surname>Sia</surname><given-names>DSY</given-names></name><name><surname>Eide</surname><given-names>SE</given-names></name><name><surname>Ong</surname><given-names>HY</given-names></name><name><surname>Jagmohan</surname><given-names>P</given-names></name><name><surname>Tan</surname><given-names>WC</given-names></name><name><surname>Khoo</surname><given-names>VM</given-names></name><name><surname>Wong</surname><given-names>YM</given-names></name><name><surname>Thian</surname><given-names>YL</given-names></name><name><surname>Baskar</surname><given-names>S</given-names></name><name><surname>Teo</surname><given-names>EC</given-names></name><name><surname>Algazwi</surname><given-names>DAR</given-names></name><name><surname>Yap</surname><given-names>QV</given-names></name><name><surname>Chan</surname><given-names>YH</given-names></name><name><surname>Tan</surname><given-names>JH</given-names></name><etal/></person-group><article-title>Improved productivity using deep learning-assisted reporting for lumbar spine MRI</article-title><source>Radiology</source><year>2022</year><volume>305</volume><issue>1</issue><fpage>160</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1148/radiol.220076</pub-id><pub-id pub-id-type="pmid">35699577</pub-id>
</element-citation><mixed-citation id="mc-CR23" publication-type="journal">Lim, D.S.W., A. Makmur, L. Zhu, W. Zhang, A.J.L. Cheng, D.S.Y. Sia, S.E. Eide, H.Y. Ong, P. Jagmohan, W.C. Tan, V.M. Khoo, Y.M. Wong, Y.L. Thian, S. Baskar, E.C. Teo, D.A.R. Algazwi, Q.V. Yap, Y.H. Chan, J.H. Tan, et al. 2022. Improved productivity using deep learning-assisted reporting for lumbar spine MRI. <italic>Radiology</italic> 305 (1): 160&#x02013;166. 10.1148/radiol.220076.<pub-id pub-id-type="pmid">35699577</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR24"><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Xiaoxuan</given-names></name><name><surname>Faes</surname><given-names>Livia</given-names></name><name><surname>Kale</surname><given-names>Aditya U</given-names></name><name><surname>Wagner</surname><given-names>Siegfried K</given-names></name><name><surname>Jack</surname><given-names>Fu Dun</given-names></name><name><surname>Bruynseels</surname><given-names>Alice</given-names></name><name><surname>Mahendiran</surname><given-names>Thushika</given-names></name><name><surname>Moraes</surname><given-names>Gabriella</given-names></name><name><surname>Shamdas</surname><given-names>Mohith</given-names></name><name><surname>Kern</surname><given-names>Christoph</given-names></name><name><surname>Ledsam</surname><given-names>Joseph R</given-names></name><name><surname>Schmidt</surname><given-names>Martin K</given-names></name><name><surname>Balaskas</surname><given-names>Konstantinos</given-names></name><name><surname>Topol</surname><given-names>Eric J</given-names></name><name><surname>Bachmann</surname><given-names>Lucas M</given-names></name><name><surname>Keane</surname><given-names>Pearse A</given-names></name><name><surname>Deniston</surname><given-names>Alastair K</given-names></name></person-group><article-title>A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis</article-title><source>Lancet: Digital Health</source><year>2019</year><volume>1</volume><issue>6</issue><fpage>e271</fpage><lpage>e297</lpage><pub-id pub-id-type="doi">10.1016/S2589-7500(19)30123-2</pub-id><pub-id pub-id-type="pmid">33323251</pub-id>
</element-citation><mixed-citation id="mc-CR24" publication-type="journal">Liu, Xiaoxuan, Livia Faes, Aditya U. Kale, Siegfried K. Wagner, Fu Dun Jack, Alice Bruynseels, Thushika Mahendiran, Gabriella Moraes, Mohith Shamdas, Christoph Kern, Joseph R. Ledsam, Martin K. Schmidt, Konstantinos Balaskas, Eric J. Topol, Lucas M. Bachmann, Pearse A. Keane, and Alastair K. Deniston. 2019. A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis. <italic>Lancet: Digital Health</italic> 1 (6): e271&#x02013;e297. 10.1016/S2589-7500(19)30123-2.<pub-id pub-id-type="pmid">33323251</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR25"><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Mann</surname><given-names>Sebastian Porsdam</given-names></name><name><surname>Earp</surname><given-names>Brian D</given-names></name><name><surname>Nyholm</surname><given-names>Sven</given-names></name><name><surname>Danaher</surname><given-names>John</given-names></name><name><surname>Moller</surname><given-names>Nikolaj</given-names></name><name><surname>Bowman-Smart</surname><given-names>Hilary</given-names></name><name><surname>Hatherley</surname><given-names>Joshua</given-names></name><name><surname>Koplin</surname><given-names>Julian</given-names></name><name><surname>Plozza</surname><given-names>Monika</given-names></name><name><surname>Rodger</surname><given-names>Daniel</given-names></name><name><surname>Treit</surname><given-names>Peter V</given-names></name><name><surname>Renard</surname><given-names>Gregory</given-names></name><name><surname>McMillan</surname><given-names>John</given-names></name><name><surname>Savulescu</surname><given-names>Julian</given-names></name></person-group><article-title>Generative AI entails a credit-blame asymmetry</article-title><source>Nature Machine Intelligence</source><year>2023</year><volume>5</volume><issue>5</issue><fpage>472</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1038/s42256-023-00653-1</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Mann, Sebastian Porsdam, Brian D. Earp, Sven Nyholm, John Danaher, Nikolaj Moller, Hilary Bowman-Smart, Joshua Hatherley, Julian Koplin, Monika Plozza, Daniel Rodger, Peter V. Treit, Gregory Renard, John McMillan, and Julian Savulescu. 2023. Generative AI entails a credit-blame asymmetry. <italic>Nature Machine Intelligence</italic> 5 (5): 472&#x02013;475. 10.1038/s42256-023-00653-1.</mixed-citation></citation-alternatives></ref><ref id="CR26"><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Mazurowski</surname><given-names>MA</given-names></name></person-group><article-title>Artificial intelligence may cause a significant disruption to the radiology workforce</article-title><source>Journal of the American College of Radiology</source><year>2019</year><volume>16</volume><issue>8</issue><fpage>1077</fpage><lpage>1082</lpage><pub-id pub-id-type="doi">10.1016/j.jacr.2019.01.026</pub-id><pub-id pub-id-type="pmid">30975611</pub-id>
</element-citation><mixed-citation id="mc-CR26" publication-type="journal">Mazurowski, M.A. 2019. Artificial intelligence may cause a significant disruption to the radiology workforce. <italic>Journal of the American College of Radiology</italic> 16 (8): 1077&#x02013;1082. 10.1016/j.jacr.2019.01.026.<pub-id pub-id-type="pmid">30975611</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR27"><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Mello</surname><given-names>Michelle M</given-names></name><name><surname>Guha</surname><given-names>Neel</given-names></name></person-group><article-title>Understanding liability risk from using health care artificial intelligence tools</article-title><source>New England Journal of Medicine</source><year>2024</year><volume>390</volume><issue>3</issue><fpage>271</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1056/NEJMhle2308901</pub-id><pub-id pub-id-type="pmid">38231630</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Mello, Michelle M., and Neel Guha. 2024. Understanding liability risk from using health care artificial intelligence tools. <italic>New England Journal of Medicine</italic> 390 (3): 271&#x02013;278. 10.1056/NEJMhle2308901.<pub-id pub-id-type="pmid">38231630</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR28"><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Mo</surname><given-names>Kevin</given-names></name><name><surname>Ikwuezunma</surname><given-names>Ijezia</given-names></name><name><surname>Mun</surname><given-names>Frederick</given-names></name><name><surname>Ortiz-Babilonia</surname><given-names>Carlos</given-names></name><name><surname>Wang</surname><given-names>Kevin</given-names></name><name><surname>Suresh</surname><given-names>Krishna Vangipuram</given-names></name><name><surname>Mesfin</surname><given-names>Addisu</given-names></name><name><surname>Jain</surname><given-names>Amit</given-names></name></person-group><article-title>Racial disparities in spine surgery: A systematic review</article-title><source>Spine Journal</source><year>2022</year><volume>22</volume><issue>9</issue><fpage>S54</fpage><pub-id pub-id-type="doi">10.1016/j.spinee.2022.06.117</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Mo, Kevin, Ijezia Ikwuezunma, Frederick Mun, Carlos Ortiz-Babilonia, Kevin Wang, Krishna Vangipuram Suresh, Addisu Mesfin, and Amit Jain. 2022. Racial disparities in spine surgery: A systematic review. <italic>Spine Journal</italic> 22 (9): S54. 10.1016/j.spinee.2022.06.117.</mixed-citation></citation-alternatives></ref><ref id="CR29"><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Pierce</surname><given-names>Robin</given-names></name><name><surname>Sterckx</surname><given-names>Sigrid</given-names></name><name><surname>Van Biesen</surname><given-names>Wim</given-names></name></person-group><article-title>A riddle, wrapped in a mystery, inside an enigma: Ho black boxes and opaque artificial intelligence confuse medical decision-making</article-title><source>Bioethics</source><year>2021</year><volume>36</volume><issue>2</issue><fpage>113</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1111/bioe.12924</pub-id><pub-id pub-id-type="pmid">34374441</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Pierce, Robin, Sigrid Sterckx, and Wim Van Biesen. 2021. A riddle, wrapped in a mystery, inside an enigma: Ho black boxes and opaque artificial intelligence confuse medical decision-making. <italic>Bioethics</italic> 36 (2): 113&#x02013;120. 10.1111/bioe.12924.<pub-id pub-id-type="pmid">34374441</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Rahwan</surname><given-names>Iyad</given-names></name></person-group><article-title>Society-in-the-loop: programming the algorithmic social contract</article-title><source>Ethics and Information Technology</source><year>2018</year><volume>20</volume><issue>1</issue><fpage>5</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1007/s10676-017-9430-8</pub-id></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Rahwan, Iyad. 2018. Society-in-the-loop: programming the algorithmic social contract. <italic>Ethics and Information Technology</italic> 20 (1): 5&#x02013;14. 10.1007/s10676-017-9430-8.</mixed-citation></citation-alternatives></ref><ref id="CR31"><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Richardson</surname><given-names>Michael L</given-names></name><name><surname>Garwood</surname><given-names>Elisabeth R</given-names></name><name><surname>Lee</surname><given-names>Yueh</given-names></name><name><surname>Li,</surname><given-names>Matthew D</given-names></name><name><surname>Lo</surname><given-names>Hao S</given-names></name><name><surname>Nagaraju</surname><given-names>Arun</given-names></name><name><surname>Nguyen</surname><given-names>Xuan V</given-names></name><name><surname>Probyn</surname><given-names>Linda</given-names></name><name><surname>Rajiah</surname><given-names>Prabkhar</given-names></name><name><surname>Sin</surname><given-names>Jessica</given-names></name><name><surname>Wasnik</surname><given-names>Ashih P</given-names></name><name><surname>Kali</surname><given-names>Xu</given-names></name></person-group><article-title>Noninterpretive uses of artificial intelligence in radiology</article-title><source>Academic Radiology</source><year>2020</year><volume>28</volume><issue>9</issue><fpage>1225</fpage><lpage>1235</lpage><pub-id pub-id-type="doi">10.1016/j.acra.2020.01.012</pub-id><pub-id pub-id-type="pmid">32059956</pub-id>
</element-citation><mixed-citation id="mc-CR31" publication-type="journal">Richardson, Michael L., Elisabeth R. Garwood, Yueh Lee, Matthew D. Li, Hao S. Lo, Arun Nagaraju, Xuan V. Nguyen, Linda Probyn, Prabkhar Rajiah, Jessica Sin, Ashih P. Wasnik, and Xu. Kali. 2020. Noninterpretive uses of artificial intelligence in radiology. <italic>Academic Radiology</italic> 28 (9): 1225&#x02013;1235. 10.1016/j.acra.2020.01.012.<pub-id pub-id-type="pmid">32059956</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR32"><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Rzadeczka</surname><given-names>Marcin</given-names></name></person-group><article-title>Our understanding of expertise and expert knowledge?</article-title><source>Studies in Logic, Grammar and Rhetoric</source><year>2020</year><volume>63</volume><issue>1</issue><fpage>209</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.2478/slgr-2020-0035</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Rzadeczka, Marcin. 2020. Our understanding of expertise and expert knowledge? <italic>Studies in Logic, Grammar and Rhetoric</italic> 63 (1): 209&#x02013;225. 10.2478/slgr-2020-0035.</mixed-citation></citation-alternatives></ref><ref id="CR33"><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Siefert</surname><given-names>Robert</given-names></name><name><surname>Weber</surname><given-names>Manuel</given-names></name><name><surname>Kocakavuk</surname><given-names>Emre</given-names></name><name><surname>Rischpler</surname><given-names>Christoph</given-names></name><name><surname>Kersting</surname><given-names>David</given-names></name></person-group><article-title>Artificial intelligence and machine learning in nuclear medicine: future perspectives</article-title><source>Seminars in Nuclear Medicine</source><year>2021</year><volume>51</volume><issue>2</issue><fpage>170</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1053/j.semnuclmed.2020.08.003</pub-id><pub-id pub-id-type="pmid">33509373</pub-id>
</element-citation><mixed-citation id="mc-CR33" publication-type="journal">Siefert, Robert, Manuel Weber, Emre Kocakavuk, Christoph Rischpler, and David Kersting. 2021. Artificial intelligence and machine learning in nuclear medicine: future perspectives. <italic>Seminars in Nuclear Medicine</italic> 51 (2): 170&#x02013;177. 10.1053/j.semnuclmed.2020.08.003.<pub-id pub-id-type="pmid">33509373</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR34"><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Tobia</surname><given-names>Kevin</given-names></name><name><surname>Nielsen</surname><given-names>Aileen</given-names></name><name><surname>Stremitzer</surname><given-names>Alexander</given-names></name></person-group><article-title>When does physician use of AI increase liability?</article-title><source>Journal of Nuclear Medicine</source><year>2021</year><volume>62</volume><issue>1</issue><fpage>17</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.2967/jnumed.120.256032</pub-id><pub-id pub-id-type="pmid">32978285</pub-id>
</element-citation><mixed-citation id="mc-CR34" publication-type="journal">Tobia, Kevin, Aileen Nielsen, and Alexander Stremitzer. 2021. When does physician use of AI increase liability? <italic>Journal of Nuclear Medicine</italic> 62 (1): 17&#x02013;21. 10.2967/jnumed.120.256032.<pub-id pub-id-type="pmid">32978285</pub-id>
</mixed-citation></citation-alternatives></ref></ref-list></back></article>