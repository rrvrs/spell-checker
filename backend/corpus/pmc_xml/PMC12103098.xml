<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><?properties manuscript?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-journal-id">101097023</journal-id><journal-id journal-id-type="pubmed-jr-id">22433</journal-id><journal-id journal-id-type="nlm-ta">IEEE Trans Neural Syst Rehabil Eng</journal-id><journal-id journal-id-type="iso-abbrev">IEEE Trans Neural Syst Rehabil Eng</journal-id><journal-title-group><journal-title>IEEE transactions on neural systems and rehabilitation engineering : a publication of the IEEE Engineering in Medicine and Biology Society</journal-title></journal-title-group><issn pub-type="ppub">1534-4320</issn><issn pub-type="epub">1558-0210</issn></journal-meta><article-meta><article-id pub-id-type="pmid">38722725</article-id><article-id pub-id-type="pmc">PMC12103098</article-id><article-id pub-id-type="doi">10.1109/TNSRE.2024.3398610</article-id><article-id pub-id-type="manuscript">HHSPA2069451</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Accuracy of Video-Based Hand Tracking for People With Upper-Body Disabilities</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3658-5293</contrib-id><name><surname>Portnova-Fahreeva</surname><given-names>Alexandra A.</given-names></name><role>Member, IEEE</role><aff id="A1">Department of Computer Science and Engineering, University of Washington, Seattle, WA 98105 USA</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3934-9325</contrib-id><name><surname>Yamagami</surname><given-names>Momona</given-names></name><role>Member, IEEE</role><aff id="A2">Department of Computer Science and Engineering, University of Washington, Seattle, WA 98105 USA.</aff><aff id="A3">Department of Electrical and Computer Engineering, Rice University, Houston, TX 77005 USA</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3082-7194</contrib-id><name><surname>Robert-Gonzalez</surname><given-names>Adri&#x000e0;</given-names></name><aff id="A4">Department of Rehabilitation Medicine, University of Washington, Seattle, WA 98105 USA</aff></contrib><contrib contrib-type="author"><name><surname>Mankoff</surname><given-names>Jennifer</given-names></name><aff id="A5">Department of Computer Science and Engineering, University of Washington, Seattle, WA 98105 USA</aff></contrib><contrib contrib-type="author"><name><surname>Feldner</surname><given-names>Heather</given-names></name><aff id="A6">Department of Rehabilitation Medicine, University of Washington, Seattle, WA 98105 USA</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4128-9387</contrib-id><name><surname>Steele</surname><given-names>Katherine M.</given-names></name><aff id="A7">Department of Mechanical Engineering, University of Washington, Seattle, WA 98105 USA</aff></contrib></contrib-group><author-notes><corresp id="CR1"><italic toggle="yes">(Corresponding author: Alexandra A. Portnova-Fahreeva.)</italic>
<email>aport6@uw.edu</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>25</day><month>4</month><year>2025</year></pub-date><pub-date pub-type="ppub"><year>2024</year></pub-date><pub-date pub-type="epub"><day>15</day><month>5</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>24</day><month>5</month><year>2025</year></pub-date><volume>32</volume><fpage>1863</fpage><lpage>1872</lpage><permissions><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions><abstract id="ABS1"><p id="P1">Utilization of hand-tracking cameras, such as Leap, for hand rehabilitation and functional assessments is an innovative approach to providing affordable alternatives for people with disabilities. However, prior to deploying these commercially-available tools, a thorough evaluation of their performance for disabled populations is necessary. In this study, we provide an in-depth analysis of the accuracy of Leap&#x02019;s hand-tracking feature for both individuals with and without upper-body disabilities for common dynamic tasks used in rehabilitation. Leap is compared against motion capture with conventional techniques such as signal correlations, mean absolute errors, and digit segment length estimation. We also propose the use of dimensionality reduction techniques, such as Principal Component Analysis (PCA), to capture the complex, high-dimensional signal spaces of the hand. We found that Leap&#x02019;s hand-tracking performance did not differ between individuals with and without disabilities, yielding average signal correlations between 0.7&#x02013;0.9. Both low and high mean absolute errors (between 10&#x02013;80mm) were observed across participants. Overall, Leap did well with general hand posture tracking, with the largest errors associated with the tracking of the index finger. Leap&#x02019;s hand model was found to be most inaccurate in the proximal digit segment, underestimating digit lengths with errors as high as 18mm. Using PCA to quantify differences between the high-dimensional spaces of Leap and motion capture showed that high correlations between latent space projections were associated with high accuracy in the original signal space. These results point to the potential of low-dimensional representations of complex hand movements to support hand rehabilitation and assessment.</p></abstract><kwd-group><kwd>Dimensionality reduction</kwd><kwd>hand tracking</kwd><kwd>principal component analysis</kwd><kwd>synergies</kwd><kwd>upper-body disabilities</kwd><kwd>hand therapy</kwd><kwd>rehabilitation</kwd></kwd-group></article-meta></front><body><sec id="S1"><label>I.</label><title>I<sc>ntroduction</sc></title><p id="P2">There is a large number of conditions that may require an individual to participate in upper-extremity rehabilitation that includes assessments of hand function: from cervical spinal cord injuries (166,000 in US) and stroke (795,000/year in US) to Parkinson&#x02019;s disease (one million individuals in US), various forms of arthritis (58.5 million individuals in US), and carpal tunnel syndrome (about 10mil in US) [<xref rid="R1" ref-type="bibr">1</xref>], [<xref rid="R2" ref-type="bibr">2</xref>], [<xref rid="R3" ref-type="bibr">3</xref>], [<xref rid="R4" ref-type="bibr">4</xref>], [<xref rid="R5" ref-type="bibr">5</xref>]. As with any form of medical treatment, constant visits to the office, as well as laborious procedures, can lead to patient/provider burnout. In addition, assessing function in a biological structure as complex as the human hand is a difficult task for providers who must consider all 27 degrees-of-freedom (DOFs) in the hand alone. Recent advances in hand-tracking technologies offer the potential to make more cost-effective and engaging options for routine hand function assessments and interventions as well as provide clinicians with tools for more objective data on outcomes following rehabilitation.</p><p id="P3">There are numerous hand-tracking technologies available: from expensive sensorized gloves (<italic toggle="yes">e.g</italic>., CyberGlove (<italic toggle="yes">Cyber Glove Systems, San Jose, CA, USA</italic>)) to cheaper camera-based solutions. In this paper, we will be evaluating Leap (<italic toggle="yes">Ultraleap, Bristol, England</italic>), a small device (145mm x 18.6mm x 11.1mm) with two infrared cameras that performs image detection to track hands. In the recent decade, many research groups have proposed the utilization of the device&#x02019;s hand-tracking feature for the purpose of hand rehabilitation, especially for stroke and Parkinson&#x02019;s disease [<xref rid="R6" ref-type="bibr">6</xref>], [<xref rid="R7" ref-type="bibr">7</xref>], [<xref rid="R8" ref-type="bibr">8</xref>], [<xref rid="R9" ref-type="bibr">9</xref>], [<xref rid="R10" ref-type="bibr">10</xref>], [<xref rid="R11" ref-type="bibr">11</xref>], [<xref rid="R12" ref-type="bibr">12</xref>], [<xref rid="R13" ref-type="bibr">13</xref>], [<xref rid="R14" ref-type="bibr">14</xref>], [<xref rid="R15" ref-type="bibr">15</xref>], [<xref rid="R16" ref-type="bibr">16</xref>], [<xref rid="R17" ref-type="bibr">17</xref>]. These groups have used Leap to explore incorporating virtual gaming into hand rehabilitation to practice grabbing, reaching, pointing, lifting, and other tasks. The main advantages of Leap are its cost-effectiveness ($100&#x02013;250/piece) and ease of integration with available programming systems that make it an appealing piece of technology for the use in research environments. In addition, its pre-packaged hand-tracking module and the use of built-in infrared cameras and infrared LEDs to illuminate the hand for improved tracking make Leap a more suitable choice over integrating RGB-D cameras with one of many existing object-tracking technologies.</p><p id="P4">However, the accuracy of assessment and rehabilitation intervention activities with Leap has not been formally established with individuals with disabilities, a critical next step if the device is to be recommended to supplement or enhance traditional hand rehabilitation. Past work has been performed either with a static plastic hand [<xref rid="R18" ref-type="bibr">18</xref>], [<xref rid="R19" ref-type="bibr">19</xref>] or nondisabled individuals [<xref rid="R20" ref-type="bibr">20</xref>], [<xref rid="R21" ref-type="bibr">21</xref>], [<xref rid="R22" ref-type="bibr">22</xref>], [<xref rid="R23" ref-type="bibr">23</xref>], [<xref rid="R24" ref-type="bibr">24</xref>], [<xref rid="R25" ref-type="bibr">25</xref>]. Further, the early assessment studies report accuracy only across a small subsection of the high-dimensional data available for the human hand, or display full signal data, but only for one participant. There, accuracy is reported in terms of static poses, possible ranges of motions of each finger, or hand segment estimation at a single time instance [<xref rid="R20" ref-type="bibr">20</xref>], [<xref rid="R22" ref-type="bibr">22</xref>], [<xref rid="R24" ref-type="bibr">24</xref>]. We believe that such approaches, while easy to implement and conventional in their nature, are inefficient for proper accuracy evaluation and insufficient for what they are being used for &#x02013; insight and evaluation of one&#x02019;s hand function. This is because they require individual evaluation of every DOF instead of providing an overall picture of hand-tracking performance that is indicative of the higher-dimensional signals. We suggest that other techniques must be developed and evaluated to provide more insightful evaluations of high-dimensional signal comparison to complement and enhance upper-extremity rehabilitation practices.</p><p id="P5">As a result, in this paper, our objectives were to 1) assess the accuracy of Leap&#x02019;s hand-tracking feature for individuals with and without upper-body disabilities and 2) propose and evaluate a new technique to assess high-dimensional hand-tracking data via dimensionality reduction.</p></sec><sec id="S2"><label>II.</label><title>M<sc>ethods</sc></title><sec id="S3"><label>A.</label><title>Participants</title><p id="P6">Participant recruitment and participation were approved by the University of Washington&#x02019;s Institutional Review Board (STUDY00016152), approved in November 2022, and informed written consent was obtained from each study participant. We recruited individuals with and without upper-body disabilities. The inclusion criteria included sufficient hand dexterity to perform fingertip touches and finger spreading. Disabled participants reported conditions such as motor neuron disease, muscular dystrophy, essential tremor, spinal cord injury, arthritis, stroke, multiple sclerosis, and Charcot-Marie-Tooth disease.</p><p id="P7">A total of 10 disabled (6 females, 3 males, 1 non-binary, 52.5&#x000b1;20.4 y/o) and 7 nondisabled (5 females, 2 males, 30.4&#x000b1;11.6 y/o) participants were recruited for the study. The disabled group was asked to fill out a Quick Disabilities of Arm, Shoulder &#x00026; Hand (QuickDASH) assessment, which is designed to measure physical function and symptoms of the upper-limb module [<xref rid="R26" ref-type="bibr">26</xref>]. The questions were slightly modified to alleviate ableism assumptions that the person&#x02019;s disability was a <italic toggle="yes">problem</italic>. For example, a question asking &#x0201c;&#x02026;to what extent has your arm, shoulder, or hand <italic toggle="yes">problem</italic> interfered with your normal social activities&#x02026;?&#x0201d; was replaced by &#x0201c;&#x02026;to what extend your arm, shoulder, or hand <italic toggle="yes">affected by your disability and/or chronic condition</italic> interfered with your normal social activities&#x02026;?&#x0201d; The average QuickDASH score was 43.6 &#x000b1; 17.6. Participants in both groups were also asked to identify their skin tone according to the Fitzpatrick Classification scale (<xref rid="F1" ref-type="fig">Fig. 1a</xref>). This information was recorded in order to assess the dependence of the accuracy of camera-based hand-tracking solutions, such as Leap, on the user&#x02019;s skin tone. Participants self-identified between levels I and IV, with the following breakdown for each level: I &#x02013; four participants, II &#x02013; three participants, III &#x02013; five participants, IV &#x02013; five participants. In the past, it has been reported that the accuracy of worn sensors is lower for individuals with darker skin tones [<xref rid="R27" ref-type="bibr">27</xref>], [<xref rid="R28" ref-type="bibr">28</xref>], [<xref rid="R29" ref-type="bibr">29</xref>].</p></sec><sec id="S4"><label>B.</label><title>Experimental Setup &#x00026; Protocol</title><p id="P8">For accuracy assessment, we evaluated Ultraleap&#x02019;s high-performance Stereo IR 170 Camera Module (formerly known as Rigel). When compared to the more-general-use Leap Motion Controller, the Stereo IR 170 incorporates a wider field of view (170&#x000b0;&#x000d7;170&#x000b0;) and a longer tracking range (between 10cm to 75cm preferred, up to 1m maximum). We will continue to refer to this module as Leap throughout the paper. Leap performed hand tracking using the Gemini software (<italic toggle="yes">Ultraleap, Bristol, England</italic>), version 3.1.0. When evaluated with an industrial robot, Leap yielded average accuracy of 1.2mm when tracking a single point [<xref rid="R30" ref-type="bibr">30</xref>].</p><p id="P9">Accuracy of Leap was compared against an Optitrack motion capture system (<italic toggle="yes">NaturalPoint, Inc., Corvallis, OR, USA</italic>), with measurement errors of under 0.1mm. The setup consisted of 14 Prime<sup>x</sup> 13W cameras, mounted on a 6 &#x000d7; 6ft canopy and directed towards a small 3&#x000d7;3ft area by the table, where the participant was seated in front of a computer display (<xref rid="SD1" ref-type="supplementary-material">Supplementary Fig. 1</xref>). Leap was secured to the table in the face-up configuration. All data collection and synchronization were implemented in Unity (<italic toggle="yes">Unity Technologies, San Francisco, CA, USA</italic>).</p><p id="P10">A total of 21 4mm reflective markers were placed on each participant&#x02019;s hands on the joint centers and the fingertips (<xref rid="F1" ref-type="fig">Fig. 1b</xref>). For index, middle, ring, and pinky fingers, markers were placed on the metacarpophalangeal (MCP), proximal interphalangeal (PIP), and distal interphalangeal (DIP) joints as well as the most distal point of each digit&#x02019;s dorsal side. For the thumb, markers were placed on the most distal point on the dorsal side as well as the MCP, carpometacarpal (CMC), and interphalangeal (IP) joints. For the wrist joint, a marker was placed on the dorsal side of ulnocarpal joint. With both Optitrack and Leap, we recorded 3D positions of joints and fingertips. Unfortunately, due to an error in the code, the position of the thumb&#x02019;s MCP was not recorded by the Leap, so we removed it from our analysis. Consequently, each system (Leap and Optitrack) yielded 60 signals (3D locations of 20 markers).</p><p id="P11">At the beginning of the experiment, we assessed the hand function of each participant using the Jebsen Taylor Hand Function (JTHF) Test [<xref rid="R31" ref-type="bibr">31</xref>] &#x02013; a common assessment tool among physical and occupational therapists. We performed six out of seven subtests (card turning, picking up small objects, stacking checkers, simulated feeding, moving light objects, and moving heavy objects) for both hands. The writing subtest was avoided due to the time constraints on the experiment.</p><p id="P12">The participants then performed two tasks: a variation of Kapandji (touching the fingertip of each finger with their thumb once) (<xref rid="F1" ref-type="fig">Fig. 1c</xref>) and finger abduction/adduction (<xref rid="F1" ref-type="fig">Fig. 1d</xref>). These tasks are reflective of traditional tasks to assess range of motion of one&#x02019;s hand and representative of common device interactions that are a combination of finger and thumb flexion/extensions and abduction/adduction. Each task was performed 5 times (trials) with each hand. The task was first demonstrated by the experiment coordinator and a corresponding picture appeared on the computer display in front of the participant. Participants were instructed to perform the tasks about 10&#x02013;20cm above the Leap within a 40 &#x000d7; 40cm box, defined by blue tape, with the device in the middle. They were asked to keep their palm as parallel to the table as possible. P105 had sufficient hand dexterity to perform the desired tasks, but not enough muscle strength to keep their hands above the table, so one of the experiment coordinators provided support by holding their arms about 15cm above the desk such that participant&#x02019;s hands were directly above the Leap and the Optitrack cameras were not occluded.</p><p id="P13">Data recording went as follows:</p><list list-type="order" id="L1"><list-item><p id="P14">The participant held their hand flat on the table.</p></list-item><list-item><p id="P15">Following a &#x0201c;START&#x0201d; sound cue, the participant performed the task at a self-selected speed.</p></list-item><list-item><p id="P16">Upon completing the task, the participant placed their hand back on the table and data collection was stopped.</p></list-item><list-item><p id="P17">In case of faulty trials, steps 1&#x02013;3 were repeated.</p></list-item></list><p id="P18">While for majority of the trials, we had 60 signals to compare, the CMC marker was misplaced for a few participants. During the Kapandji task, P123 had a misplaced CMC marker for both hands. During finger abduction/adduction, P105 had a misplaced CMC on the nondominant hand and P123 had a misplaced CMC on the dominant hand. As a result, during these instances, analyses were performed for the remaining 57 signals.</p><p id="P19">Static and dynamic parameters were measured for further detailed assessment. The static parameters were hand size (determined by measuring the distance between the wrist and the middle MCP markers when the palm of the hand was positioned flat on the table), time taken to perform the picking up of small objects subtest of the JTHF test, QuickDASH, and self-identified skin tone. The dynamic parameters were trial-dependent and included the average height of the wrist joint above the table throughout the task as well as the speed of task performance. For the Kapandji task, speed was determined by calculating the speed of the tip of the thumb as it was the main finger involved throughout the task. For finger abduction/adduction, speed was calculated from the index fingertip, as we assumed a uniform finger movement during the task.</p></sec><sec id="S5"><label>C.</label><title>Data Processing</title><sec id="S6"><title>Optitrack:</title><p id="P20">The motion capture data were cleaned using Motive software (<italic toggle="yes">NaturalPoint, Inc., Corvallis, OR, USA</italic>) to eliminate dropped markers. Because each tracking system had unique detection parameters (<italic toggle="yes">i.e</italic>., Optitrack detected hands flat on the table prior to trial and Leap only detected them when they were in the device&#x02019;s range), data were clipped so all comparisons were made with data tracked by both systems. Due to space limitation and the need to work around our participants&#x02019; accommodations such as various mobility aids, Optitrack yielded poor marker tracking during a few trials, which were consequently dropped from analysis.</p></sec><sec id="S7"><title>Leap:</title><p id="P21">Although data were sampled at 50Hz from Leap and Optitrack, due to the Leap&#x02019;s variability in sampling frequency, the final number of samples recorded at the end of each trial for Leap and Optitrack often did not match. The nonuniformly sampled Leap data were resampled to match that of the uniformly sampled Optitrack data. To align the Leap and Optitrack systems, the Leap data were offset in the x, y, and z directions so that the position of the MCP joint of the index finger determined by Leap during the first analysis instance matched that of the index&#x02019;s MCP joint determined by Optitrack. This was done to align the two coordinate systems and account for any potential offsets that might have happened during system calibration. Relative orientations were aligned during the motion capture calibration step by placing the Optitrack calibration square so that the Optitrack coordinate axes aligned with those of Leap.</p></sec><sec id="S8"><title>Hand Dominance:</title><p id="P22">Hand dominance reported by unimpaired individuals was used during analysis. For individuals without a disability, the dominant hand tends to yield faster performance on the JTHF test [<xref rid="R31" ref-type="bibr">31</xref>]. However, for individuals with a disability, there was significant variability in how participants reported hand dominance. For example, some reported their dominant hand as the hand that was least affected by their current condition, regardless of handedness prior to disability onset, while some reported their hand dominance prior to the disability. Thus, we decided to determine hand dominance based on the results of the &#x0201c;picking up small objects&#x0201d; subtest of the JTHF test &#x02013; the hand that performed the task faster was assigned as the dominant hand for analyses. We picked this subtest for its most appropriate representation of hand dexterity.</p></sec></sec><sec id="S9"><label>D.</label><title>Accuracy Assessment</title><sec id="S10"><title>Conventional Techniques:</title><p id="P23">Two of the most common techniques to compare two signals are correlation coefficients and mean absolute error (MAE). We started the accuracy assessment of hand tracking by determining the correlation, <inline-formula><mml:math id="M1" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, of each <inline-formula><mml:math id="M2" display="inline"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> signal between Leap and Optitrack, where each signal represents a joint/tip position either along the x, y, or z axis. For each participant and task, we calculated the average of all correlations, <inline-formula><mml:math id="M3" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M4" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> is the total number of signals in the original data <xref rid="FD1" ref-type="disp-formula">(1)</xref>.</p><disp-formula id="FD1">
<label>(1)</label>
<mml:math id="M5" display="block"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:math>
</disp-formula><p id="P24">We then calculated MAE for each signal pair, <inline-formula><mml:math id="M6" display="inline"><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M7" display="inline"><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> at every time instance <inline-formula><mml:math id="M8" display="inline"><mml:mi>t</mml:mi></mml:math></inline-formula>
<xref rid="FD2" ref-type="disp-formula">(2)</xref>.</p><disp-formula id="FD2">
<label>(2)</label>
<mml:math id="M9" display="block"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</disp-formula><p id="P25">As with correlation, we calculated the average MAE across all signals, <inline-formula><mml:math id="M10" display="inline"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>
<xref rid="FD3" ref-type="disp-formula">(3)</xref>.</p><disp-formula id="FD3">
<label>(3)</label>
<mml:math id="M11" display="block"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:math>
</disp-formula><p id="P26">These conventional accuracy assessment methods are often not optimal for signal comparison. Ideally, when comparing Optitrack and Leap for accuracy, we want to see two signals that exhibit high correlation and low MAE. However, there are several examples when these two methods may lead to contradicting results. For example, two signals that match in temporal pattern, but have a large offset in the amplitude will yield high correlation but also high MAE (<xref rid="SD1" ref-type="supplementary-material">Supplementary Fig. 2a</xref>). In other instances, two signals might yield low MAE (desirable), while resulting in low correlation (<xref rid="SD1" ref-type="supplementary-material">Supplementary Fig. 2b</xref>). Or there might be a temporal offset, as one signal lags behind the other &#x02013;yielding low correlation and, potentially, low MAE, yet there would be no indication of the temporal lag of the signal in these outcomes (<xref rid="SD1" ref-type="supplementary-material">Supplementary Fig. 2c</xref>).</p><p id="P27">Moreover, these conventional techniques are not efficient for comparison of high-dimensional data, requiring multiple steps of average calculations. Such mathematical manipulations result in outcomes that may not be indicative of finer details that are important for assessment or monitoring of hand function.</p><p id="P28">In addition to evaluating correlation coefficients for each hand landmark&#x02019;s 3D position, we also evaluated the correlation of the Euclidian distances between digits for each task, <inline-formula><mml:math id="M12" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Identifying functionally important measures from these tasks is one method to reduce the dimensionality of the high-dimensional data from the hand. For Kapandji, we calculated Euclidian distances between the tip of the thumb and each fingertip. For finger abduction/adduction, we evaluated the Euclidian distances between neighboring digit tips. In both cases, evaluating these distances reduced the available 60D signal space to 4D (<xref rid="SD1" ref-type="supplementary-material">Supplementary Fig. 3</xref>).</p><p id="P29">Lastly, we calculated the length of each finger segment (proximal, middle, distal)&#x02013; a total of 12 segments per hand. The thumb was excluded from this analysis due to the missing data. The segment lengths were evaluated during the finger abduction/adduction exercise in the middle of each trial with a single frame selected for analysis. This assessment technique allowed us to analyze accuracy of the Leap-generated hand model and the percent match of the recorded segments compared to the hand segments measured with Optitrack.</p></sec><sec id="S11"><title>Dimensionality Reduction:</title><p id="P30">While calculating measures like Euclidian distances between fingertips is one method to reduce the dimensionality of available data for analysis, this choice draws from only a limited set of the original data and may not capture important features for a given individual and task. An alternative to manual selection of features is using dimensionality reduction (DR) techniques, which come in a variety of types &#x02013; from complex neural networks to simple mathematical equations. In this study, we used one of the most common linear techniques &#x02013; Principal Component Analysis (PCA) [<xref rid="R32" ref-type="bibr">32</xref>]. We chose PCA for its ease in implementation, low computational cost, and stability in output in comparison to other methods, such as autoencoders or t-Distributed Stochastic Neighbor Embedding (tSNE). Such latent space stability makes it possible to perform comparisons in the low-dimensional spaces. Due to its linear and orthogonal constrains, PCA yields latent spaces that are easy to interpret unlike its nonlinear counterparts. It is also one of the most common DR techniques applied in the biosignal context. PCA calculates the linear combinations of input signals that explain the most variance in the data. This method has been widely applied in the past to reduce the dimensionality of complex hand kinematics during object grasping, American Sign Language gesturing, and dynamic tasks [<xref rid="R33" ref-type="bibr">33</xref>], [<xref rid="R34" ref-type="bibr">34</xref>], [<xref rid="R35" ref-type="bibr">35</xref>], [<xref rid="R36" ref-type="bibr">36</xref>]. In most cases, PCA was able to explain over 80% of variance in the input data with just two latent dimensions, or principal components (PCs).</p><p id="P31">To evaluate the potential of PCA for visualization and analysis of these tasks, we first assessed the impact of common issues with video-based methods &#x02013; noise, offsets, and delays. We used a dataset from our experimental data (P105 performing Kapandji with the dominant hand) and synthetically modified data to mimic these issues. To evaluate the impact of noise, we added Gaussian noise to all 60 signals ranging from small (signal-to-noise ratio of 50dB) to large (30dB). The encoding of the data injected with small noise appeared very similar to the original data encoding (<xref rid="SD1" ref-type="supplementary-material">Supplementary Fig. 4</xref>). Increasing the noise yielded a more distorted encoding. To evaluate an offset, we offset the amplitude of all 60 signals by 10mm. The offset in the dataset resulted in a linear transformation of the projection in the latent space (<xref rid="SD1" ref-type="supplementary-material">Supplementary Fig. 4</xref>). Finally, we included a novel dataset &#x02013; the same participant performing the finger abduction/adduction task &#x02013; to understand how data from different origins are organized on the latent space. In this case, the encoding of the novel data did not appear similar in shape to the original data encoding. This suggests that PCA can be used to determine differences between two datasets (e.g., Leap and Optitrack) since if data are the same in the high-dimensional space, their encodings will resemble each other in the lower-dimensional space.</p><p id="P32">To use PCA to evaluate the accuracy of Leap relative to Optitrack, each trial pair, <inline-formula><mml:math id="M13" display="inline"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M14" display="inline"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (both of which are <inline-formula><mml:math id="M15" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> matrices, where <inline-formula><mml:math id="M16" display="inline"><mml:mi>m</mml:mi></mml:math></inline-formula> is the number of samples and <inline-formula><mml:math id="M17" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of signals) were combined into a single high-dimensional data input, <inline-formula><mml:math id="M18" display="inline"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, resulting in an <inline-formula><mml:math id="M19" display="inline"><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> matrix. The data were then normalized according to <xref rid="FD4" ref-type="disp-formula">(4)</xref>, where <inline-formula><mml:math id="M20" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula> is each individual signal.</p><disp-formula id="FD4">
<label>(4)</label>
<mml:math id="M21" display="block"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>min</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>min</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</disp-formula><p id="P33">The normalized data, <inline-formula><mml:math id="M22" display="inline"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, were then passed through PCA and the two PCs that explained the greatest variance were extracted and plotted in 2D space. To visually differentiate between Leap and Optitrack latent projections, encodings associated with Leap were plotted in red while those generated from Optitrack samples were plotted in blue Once in 2D, the Leap projection was offset by <inline-formula><mml:math id="M23" display="inline"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M24" display="inline"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, such that the mean of the data along the first latent dimension (PC1) and the mean of the data along the second latent dimension (PC2) matched that of Optitrack&#x02019;s encoding <xref rid="FD5" ref-type="disp-formula">(5)</xref>&#x02013;<xref rid="FD6" ref-type="disp-formula">(6)</xref>. This was done to visually assess the difference in latent space projections between Leap and Optitrack.</p><disp-formula id="FD5">
<label>(5)</label>
<mml:math id="M25" display="block"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</disp-formula><disp-formula id="FD6">
<label>(6)</label>
<mml:math id="M26" display="block"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math>
</disp-formula></sec></sec><sec id="S12"><label>E.</label><title>Statistical Analysis</title><p id="P34">For statistical analyses, we used custom scripts using MATLAB&#x02019;s (<italic toggle="yes">MathWorks, Natick, MA, USA</italic>) Statistics Toolbox. A Shapiro-Wilk test [<xref rid="R37" ref-type="bibr">37</xref>], [<xref rid="R38" ref-type="bibr">38</xref>] was used to evaluate normality of mean correlation, MAE, Euclidian distances, and error between finger segments. This analysis indicated the distributions were normal for the mean correlation and MAE values and not normal for the Euclidian distances and errors between finger segments. Subsequently, for mean correlation and MAE, we used Holm-Sidak test [<xref rid="R39" ref-type="bibr">39</xref>], [<xref rid="R40" ref-type="bibr">40</xref>] to account for multiple comparisons. For not-normally distributed data, such as Euclidian distances and absolute errors between estimated finger segments, we utilized Dunn&#x02019;s test [<xref rid="R41" ref-type="bibr">41</xref>], [<xref rid="R42" ref-type="bibr">42</xref>], a procedure used for multiple non-parametric comparisons. We used stepwise regression [<xref rid="R43" ref-type="bibr">43</xref>] to evaluate the effects of static and dynamic parameters on signal correlations.</p></sec></sec><sec id="S13"><label>III.</label><title>R<sc>esults</sc></title><sec id="S14"><label>A.</label><title>Accuracy Assessment Indicates no Significant Difference Between Disabled and Nondisabled Hand Tracking With Leap</title><p id="P35">Using the high-dimensional set of signals from hand landmarks, we found high correlations, <inline-formula><mml:math id="M27" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, (between 0.7&#x02013;0.9) comparing Leap and Optitrack during both the Kapandji and finger abduction/adduction tasks (<xref rid="F2" ref-type="fig">Fig. 2a</xref>). Leap&#x02019;s performance did not differ between dominant and nondominant hands (p &#x0003e; 0.05). The disabled group yielded significantly higher <inline-formula><mml:math id="M28" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> between Leap and Optitrack for nondominant hands during the Kapandji tasks (<italic toggle="yes">p</italic> = 0.003). In other cases, there were no significant differences in mean <inline-formula><mml:math id="M29" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> across both groups. MAEs were similar between dominant and nondominant hands and for both groups (p &#x0003e; 0.05, <xref rid="F2" ref-type="fig">Fig. 2b</xref>). MAE results showed a greater variability with a range of 10&#x02013;80mm error between the Leap and the Optitrack signals. There were no significant differences between the hand dominance or disability status variables (p&#x0003e;0.05). Following these results, both participants from disabled and nondisabled groups were combined in the following analysis as no statistical differences were observed between them.</p><p id="P36">We found limited to no dependency of the <inline-formula><mml:math id="M30" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> between Leap and Optitrack on static (<italic toggle="yes">i.e</italic>., skin tone, hand measurements, and the picking up small objects subtests of JTHF Test) or dynamic (<italic toggle="yes">i.e</italic>., hand height above the Leap and the speed of task performance) parameters. During the Kapandji task, task speed was negatively associated with <inline-formula><mml:math id="M31" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> between Leap and Optitrack for the disabled group, such that accuracy of the Leap decreased with the increase in the speed with which the participant performed the task (<italic toggle="yes">p</italic> &#x0003c; 0.01, <inline-formula><mml:math id="M32" display="inline"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.62</mml:mn></mml:mrow></mml:math></inline-formula>). During the same task, <inline-formula><mml:math id="M33" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for the dominant hand for the nondisabled group depended on the hand position above Leap (<italic toggle="yes">p</italic> = 0.01, <inline-formula><mml:math id="M34" display="inline"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.74</mml:mn></mml:mrow></mml:math></inline-formula>), such that performing the task further away from Leap resulted in more accurate tracking of the hand. For the finger abduction/adduction task, the performance of the Leap&#x02019;s hand-tracking feature was not associated with any of the static or dynamic parameters. QuickDASH scores were not associated with average correlation values for either of the tasks.</p><p id="P37">Evaluating Euclidian distances between digits during the Kapandji demonstrated overall acceptable performance of Leap&#x02019;s hand-tracking of the general fingertip positions in relationship to each other (<xref rid="F3" ref-type="fig">Fig. 3a</xref>). Leap exhibited the lowest performance tracking the index finger (average correlation of 0.63 for both hands). A better performance could be seen for the middle finger (mean <inline-formula><mml:math id="M35" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 0.76, <italic toggle="yes">p</italic> &#x0003e; 0.05, for dominant and 0.73, <italic toggle="yes">p</italic> &#x0003e; 0.05, for nondominant hands). The ring and pinky fingers had the highest correlations, with a mean <inline-formula><mml:math id="M36" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 0.81 for dominant and 0.84 for nondominant hands. The differences between both index and ring and index and pinky pairs were statistically significant for dominant and nondominant hands alike (<italic toggle="yes">p</italic> &#x0003c; 0.01).</p><p id="P38">During the abduction/adduction task, the Euclidian distances between the neighboring fingertips had high <inline-formula><mml:math id="M37" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (&#x0003e;0.76) between Leap and Optitrack for all digit pairs except for between the index-and-middle (IM) which had an average <inline-formula><mml:math id="M38" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 0.65 for dominant and 0.60 for nondominant hands (<xref rid="F3" ref-type="fig">Fig. 3b</xref>). The differences between the thumb-to-index (TI) and IM pairs were statistically significant for both hands (<italic toggle="yes">p</italic> = 0.008 for dominant and <italic toggle="yes">p</italic> = 0.004 for nondominant hands). Similarly, the differences between the IM and ring-pinky (RP) pairs were statistically significant (<italic toggle="yes">p</italic> = 0.001 for dominant and <italic toggle="yes">p</italic> = 0.003 for nondominant hands).</p><p id="P39">The difference between finger segment lengths calculated from Leap and Optitrack models demonstrated that the proximal segments had larger errors than distal segments, with average errors ranging between 12.5&#x02013;18.5mm (<xref rid="F4" ref-type="fig">Fig. 4</xref>). Differences in segment lengths were significantly lower for the middle and distal segments, with errors as small as 2.5mm for the pinky finger of both dominant and nondominant hands.</p></sec><sec id="S15"><label>B.</label><title>Dimensionality Reduction With PCA Demonstrates Correlation Between Low-Dimensional and High-Dimensional Spaces</title><p id="P40">Using PCA to visualize the similarity between Optitrack and Leap trajectories in a 2D space, we found that the first two components from PCA explained a high variance in the 60D for both systems, with an average variance accounted for (VAF) of 71&#x02013;98% from Optitrack and Leap during the Kapandji task, and 68&#x02013;98% for the finger abduction/adduction task. The correlation between the 2D latent spaces from PCA, <inline-formula><mml:math id="M39" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, was generally high with an average of 0.89 and range of 0.19&#x02013;0.98 between Leap and Optitrack for both tasks. When <inline-formula><mml:math id="M40" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> values were high, latent spaces between Leap and Optitrack were more similar in shape (<xref rid="F5" ref-type="fig">Fig 5</xref>). Lower <inline-formula><mml:math id="M41" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> values resulted in two encoded shapes that were very different from each other and often offset.</p><p id="P41">To better understand how differences between the 2D latent spaces of Leap and Optitrack signified differences in the high-dimensional projections, <inline-formula><mml:math id="M42" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> values were plotted against <inline-formula><mml:math id="M43" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (<xref rid="F6" ref-type="fig">Fig. 6</xref>). Fitting a linear model yielded <inline-formula><mml:math id="M44" display="inline"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.67</mml:mn></mml:mrow></mml:math></inline-formula> for Kapandji and <inline-formula><mml:math id="M45" display="inline"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.72</mml:mn></mml:mrow></mml:math></inline-formula> for finger abduction/adduction, highlighting the high dependence of the shape similarity of the latent projections to the Leap-Optitrack correlation in 60D.</p><p id="P42">Additionally, when we removed the trials that had smaller VAF values, we observed an increase in the <inline-formula><mml:math id="M46" display="inline"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> value of the linear regression model between <inline-formula><mml:math id="M47" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M48" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. For example, while the baseline <inline-formula><mml:math id="M49" display="inline"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> values were 0.67 and 0.72 for Kapandji and finger abduction/adduction, removing trials with less than 90% of VAF resulted in <inline-formula><mml:math id="M50" display="inline"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> of 0.84 and 0.83, respectively.</p></sec></sec><sec id="S16"><label>IV.</label><title>D<sc>iscussion</sc></title><p id="P43">In the study, we performed a detailed analysis of Leap&#x02019;s hand-tracking feature for participants with and without upper-body disabilities during tasks that were reflective of those commonly performed in clinical rehabilitation. Our results indicated high correlations that were similar regardless of the participant&#x02019;s disability status. We evaluated the high-dimensional space of hand landmarks and assessed Leap&#x02019;s accuracy in tracking hands, using correlation and mean absolute errors. We also evaluated the device&#x02019;s ability to track the overall hand poses during the Kapandji and finger abduction/adduction tasks by evaluating the fingertip distances, reducing the original 60 dimensions to 4 functionally relevant signals for analysis. To more holistically evaluate the 60 dimensions, we also evaluated PCA for dimensionality reduction and attempted to understand how the differences in the latent projections could be impacted by original data.</p><sec id="S17"><label>A.</label><title>Accuracy of Leap&#x02019;s Hand Tracking</title><p id="P44">The conventional analysis of correlation and MAE between Leap and Optitrack hand joint locations suggested that Leap&#x02019;s hand-tracking features were generally acceptable during these simple dynamic tasks, although with high variability, which points to the need for new methods to monitor and evaluate signal fidelity in real-time. Mean <inline-formula><mml:math id="M51" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> values were between 0.71 and 0.94 while mean MAE was in the 10&#x02013;80mm range. In the latent space identified by PCA, <inline-formula><mml:math id="M52" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> were also high (0.83&#x02013;0.97) for both tasks, similarly to <inline-formula><mml:math id="M53" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> results. Most importantly, no significant differences in performance were observed between nondisabled and disabled participants. One interesting deficit in system performance was Leap&#x02019;s consistent underestimation of the proximal phalanges (<xref rid="F4" ref-type="fig">Fig. 4</xref>).</p><p id="P45">Our results demonstrate similar levels of accuracy for participants with disabilities as prior studies that have focused largely on nondisabled populations. The most recent study evaluated Leap&#x02019;s performance with a marker-based motion capture system, Qualisys, with nondisabled participants performing thumb abduction/adduction and finger flexion/extension [<xref rid="R20" ref-type="bibr">20</xref>]. The reported maximum difference in the proximal segment of the index finger during static trials was lower (5.7) than average differences in segment lengths we found in our work (18mm). The trend of the proximal finger segment yielding greatest errors was similar to what we found in our study. Another study by Mizera et al. also compared Leap&#x02019;s accuracy in tracking both joint angles and fingertip positions to Qualisys during random movement of the fingers and the Kapandji test in nondisabled individuals [<xref rid="R22" ref-type="bibr">22</xref>]. Their reported differences between computed finger segments were closer to the ranges in our study. However, the relationship of proximal segment yielding the largest errors was not observed in the study, with highest errors appearing in the distal finger segment. The study has also reported thumb-to-fingertip distance error during the Kapandji task &#x02013; 8mm in comparison to the 22mm error observed in our study. However, it is important to note that distance errors were calculated during the thumb-to-fingertip contact and not over the entirety of the task as it was done in our work. The same group also evaluated Leap&#x02019;s accuracy for fingertip positions and found an average positioning error of 27.4 mm during Kapandji [<xref rid="R22" ref-type="bibr">22</xref>], which is lower than the average MAE we calculated for all joint positions during the same task (44.4mm). These differences likely stem from the way the position errors were calculated: Mizera&#x02019;s group calculated error considering only fingertip positions,we examined all joint positions and highlighted reduced accuracy for proximal segments. Other papers that assessed the accuracy of Leap have performed analysis mainly in the joint angle space [<xref rid="R20" ref-type="bibr">20</xref>], [<xref rid="R21" ref-type="bibr">21</xref>], [<xref rid="R25" ref-type="bibr">25</xref>] or did not report individual finger tracking [<xref rid="R23" ref-type="bibr">23</xref>], [<xref rid="R24" ref-type="bibr">24</xref>].</p></sec><sec id="S18"><label>B.</label><title>Towards Dimensionality Reduction Analysis for Biosignal Data</title><p id="P46">Our study&#x02019;s biggest motivation was to examine the applicability of dimensionality-reduction techniques for comparing complex high-dimensional biosignal data for the hand. Conventional methods fail to capture temporal delays, amplitude offsets in coordinate frames, or noise &#x02013; all of which can occur during data collection in clinical environments. In conventional analyses, each signal is evaluated separately, which can be tedious to perform quickly or holistically for assessment or monitoring. In addition, different measurements are appropriate for different task conditions: while MAE is ideal for static data analysis (<italic toggle="yes">i.e</italic>., single time instance), correlation is great for dynamic tasks (<italic toggle="yes">i.e</italic>., data over time).</p><p id="P47">Dimensionality-reduction algorithms can prove to be useful by efficiently compressing high-dimensional data into more manageable, lower-dimensional spaces. Such methods can improve the interpretability of results. In our study, we saw that compressing 60D joint position data to 2D via PCA produced meaningful results, where large differences in the original high-dimensional space were reflected in the differences between Leap and Optitrack projections in 2D (<xref rid="F6" ref-type="fig">Fig. 6</xref>). The results produced by evaluating <inline-formula><mml:math id="M54" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the correlations of latent projections, yielded results similar to those obtained when evaluating all the signals in high dimensions, <inline-formula><mml:math id="M55" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. There, the <inline-formula><mml:math id="M56" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> range was 0.71&#x02013;0.94 while mean while <inline-formula><mml:math id="M57" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> range was (0.83&#x02013;0.97). Small discrepancies in the results could be attributed to PCA&#x02019;s inability to effectively compress all the important information in every trial. As noted, average VAF was as low as 71%. Lastly, in our exploration, we found that PCA can account for similar data injected with noise, amplitude offset, and temporal delay, making it a useful tool in performing comparison of high-dimensional biosignal data.</p><p id="P48">Dimensionality reduction has a rich history in evaluating the hand and other physiological signals [<xref rid="R33" ref-type="bibr">33</xref>], [<xref rid="R34" ref-type="bibr">34</xref>], [<xref rid="R35" ref-type="bibr">35</xref>], [<xref rid="R36" ref-type="bibr">36</xref>], [<xref rid="R44" ref-type="bibr">44</xref>], [<xref rid="R45" ref-type="bibr">45</xref>], [<xref rid="R46" ref-type="bibr">46</xref>], [<xref rid="R47" ref-type="bibr">47</xref>]. In the past, it has been utilized to capture hand kinematics during object grasping and gesturing, leg kinematics of human gait, as well as muscle activations during various tasks, with the main objective of reducing the complexity of high-dimensional biological signals. Oftentimes, these applications focused on the derivation of &#x0201c;synergies&#x0201d;, or control units. In this study, however, we leveraged the DR method for a novel application of comparing high-dimensional physiological signals in a lower-dimensional space, with the potential to monitor hand function.</p><p id="P49">As with any tool, there are limitations to the use of DR techniques for the purpose of accuracy assessment. First, for certain trials, VAF by first two components was moderate, with values as low as 68%. Generally, this level of variance is considered acceptable, although it is important to understand that the other 32% of variance are not captured in the latent space. Such performance is tied to PCA&#x02019;s objective to force orthogonality of latent dimensions (principal components) and the assumption of linearity. Given the anatomy of the hand and the tasks performed, there are expected non-orthogonal and dependent components in these actions. Other DR techniques that do not require these constraints, such as nonnegative matrix factorization, may be more appropriate, although they often lack the interpretability of PCA. When trials with lower VAF scores were removed from latent space analysis, we found a stronger linear relationship between the correlation of high-dimensional signals and the correlation of latent representations in 2D. Noting the challenges of compression that PCA sometimes exhibits with complex data, it is essential to remember that it is only one of many DR techniques, some of which (<italic toggle="yes">e.g</italic>., nonlinear autoencoders) have been shown to be significantly more efficient at compressing complex hand kinematics data than their linear counterparts [<xref rid="R36" ref-type="bibr">36</xref>], [<xref rid="R47" ref-type="bibr">47</xref>]. In such cases, VAF values tend to be in the 90% range. Analysis of the latent spaces produced with such techniques could potentially be more reliable and indicative of the original high-dimensional space. While our initial exploration with PCA is promising, future work should consider more complex DR techniques. Additionally, PCA&#x02019;s main objective does not lie in preserving relative distance between data points but with the overall variance along axes. However, preserving local geometries in the case of signal accuracy assessment might be an important aspect and other DR techniques whose goal is to preserve local structures should be considered and evaluated in the future (e.g., Isomap [<xref rid="R44" ref-type="bibr">44</xref>], Locally Linear Embedding [<xref rid="R48" ref-type="bibr">48</xref>]). Lastly, PCA does not account for the temporal aspect of the original data, so that each data point on the latent space signifies a single sample in the high-dimensional space. In the future, temporal DR techniques, such as the ones that rely on long short-term memory [<xref rid="R49" ref-type="bibr">49</xref>], could be explored to evaluate data from dynamic tasks in a more efficient way and complex time-dependent data could be reduced to a single point.</p></sec><sec id="S19"><label>C.</label><title>Study Limitations</title><p id="P50">In this study, we wanted to capture a variety of disabilities that affected hand function. As a result, we had broad inclusion criteria, the participant pool was not homogeneous, and certain disability types appeared more often than others. Another limitation comes from the space allocated for the experiment &#x02013; motion capture was performed in a small room, making it difficult to place cameras to prevent occlusion. Mobility aids also resulted in greater marker occlusions, often losing markers for a few frames. In such cases, the lost marker position was estimated from other markers using regression or pattern recognition techniques. We operated the Leap device in a desktop recording mode, where it is placed flat on the table and the hands are only recognized when in the pronate position (palm down towards the device). On the contrary, for Optitrack, markers were placed on top of the hand, since placing motion capture cameras under the table was impossible due to space limitations and accommodating wheelchairs. As a result, Optitrack joint position tracking happened from the top of the hand while Leap performed image recognition from the bottom of the hand. Gemini, the hand-tracking software, can also operate in &#x0201c;head-mounted&#x0201d; and &#x0201c;screentop&#x0201d; modes, which may impact accuracy and warrant further investigation for hand tracking and assessment. Lastly, the hand tasks selected for data analysis in our study, such as the modified Kapandji and finger abduction/adduction, while are representative of common device interactions and similar to hand assessments for range of motion, cannot be generalized to tasks that involve object manipulation used in hand rehabilitation.</p></sec><sec id="S20"><label>D.</label><title>Looking Ahead</title><p id="P51">Our study has shown that existing off-the-shelf camera-based solutions for hand tracking have a promising potential for novel ways to perform hand rehabilitation. Leap was determined to be an inclusive technology for individuals with different upper-body disabilities. In addition, our work with DR techniques to compare high-dimensional data for accuracy assessment can potentially be used to provide real-time monitoring or biofeedback during and after therapy. In the past, availability of biosignals during rehabilitation exercises has been shown to improve outcomes [<xref rid="R50" ref-type="bibr">50</xref>], [<xref rid="R51" ref-type="bibr">51</xref>], [<xref rid="R52" ref-type="bibr">52</xref>].</p><p id="P52">Analyses using the latent space analysis may also have the potential to provide insight into issues or changes &#x02013; such as to track progress across sessions or monitor for disease progression. Similarly, these representations can prove to be useful not only in the upper-limb but in other physiological domains. For example, in the past, PCA has been used to evaluate and detect abnormalities in breathing [<xref rid="R53" ref-type="bibr">53</xref>] and human gait [<xref rid="R54" ref-type="bibr">54</xref>], [<xref rid="R55" ref-type="bibr">55</xref>] and help identify emergency situations in patients from mobile health application data [<xref rid="R56" ref-type="bibr">56</xref>], [<xref rid="R57" ref-type="bibr">57</xref>]. The overall good and reliable performance of the Leap relative to optical motion capture in participants with and without disabilities in this study suggests that these hand-tracking technologies provide promising and inclusive strategies to deploy in rehabilitation and assistive technology.</p></sec></sec><sec id="S21"><label>V.</label><title>C<sc>onclusion</sc></title><p id="P53">This paper presents an accuracy assessment of the Leap&#x02019;s hand tracking feature for individuals with upper-body disabilities. The feature performed similarly regardless of the disability status of the user, with variable results for participants with and without disabilities. In addition, the paper proposes the use of a linear dimensionality-reduction technique, PCA, to be used as a tool to compare high-dimensional data on a latent space. Following the analysis, we determined that high correlations between lower-dimensional projections were associated with high correlations of the data in the original high-dimensional space, suggesting the potential of PCA to be used for comparative purposes of high-dimensional data.</p></sec><sec sec-type="supplementary-material" id="SM1"><title>Supplementary Material</title><supplementary-material id="SD1" position="float" content-type="local-data"><label>Supplemental1-3398610</label><media xlink:href="NIHMS2069451-supplement-Supplemental1-3398610.docx" id="d67e2274" position="anchor"/></supplementary-material></sec></body><back><ack id="S22"><title>A<sc>cknowledgment</sc></title><p id="P54">The authors would like to thank our participants for their incredible determination and interest in participating in the study, Judy Kong for her help running the experiments, Selest Nasef for providing the motion capture system and setup assistance, and the Center for Research and Education on Accessible Technology and Experiences (CREATE) at the University of Washington for their ongoing support for this project.</p><p id="P55">This work was supported by the National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR) Advanced Rehabilitation Research and Training (ARRT) under Grant 90ARCP0005-01-00.</p><p id="P56">This work involved human subjects or animals in its research. Approval of all ethical and experimental procedures and protocols was granted by the University of Washington&#x02019;s Institutional Review Board under Application No. STUDY00016152.</p></ack><fn-group><fn id="FN2"><p id="P57">This article has supplementary downloadable material available at <ext-link xlink:href="10.1109/TNSRE.2024.3398610" ext-link-type="doi">https://doi.org/10.1109/TNSRE.2024.3398610</ext-link>, provided by the authors.</p></fn></fn-group><ref-list><title>R<sc>eferences</sc></title><ref id="R1"><label>[1]</label><mixed-citation publication-type="journal"><name><surname>Allan</surname><given-names>GM</given-names></name>, <name><surname>Amadio</surname><given-names>PC</given-names></name>, and <name><surname>Kaufman</surname><given-names>KR</given-names></name>, &#x0201c;<article-title>Carpal tunnel syndrome: Diagnosis and management</article-title>,&#x0201d; <source>Amer. Family Phys</source>, vol. <volume>94</volume>, no. <issue>12</issue>, pp. <fpage>993</fpage>&#x02013;<lpage>999</lpage>, <month>Dec</month>. <year>2016</year>. [Online]. Available: <ext-link xlink:href="https://www.aafp.org/pubs/afp/issues/2016/1215/p993.html#afp20161215p993-b1" ext-link-type="uri">https://www.aafp.org/pubs/afp/issues/2016/1215/p993.html#afp20161215p993-b1</ext-link></mixed-citation></ref><ref id="R2"><label>[2]</label><mixed-citation publication-type="journal"><name><surname>Marras</surname><given-names>C</given-names></name>. <etal/>, &#x0201c;<article-title>Prevalence of Parkinson&#x02019;s disease across North America</article-title>,&#x0201d; <source>NPJ Parkinson&#x02019;s Disease</source>, vol. <volume>4</volume>, no. <issue>1</issue>, p. <fpage>21</fpage>, <month>Jul</month>. <year>2018</year>.</mixed-citation></ref><ref id="R3"><label>[3]</label><mixed-citation publication-type="journal"><name><surname>Park</surname><given-names>J</given-names></name>, <name><surname>Mendy</surname><given-names>A</given-names></name>, and <name><surname>Vieira</surname><given-names>ER</given-names></name>, &#x0201c;<article-title>Various types of arthritis in the United States: Prevalence and age-related trends from 1999 to 2014</article-title>,&#x0201d; <source>Amer. J. Public Health</source>, vol. <volume>108</volume>, no. <issue>2</issue>, pp. <fpage>256</fpage>&#x02013;<lpage>258</lpage>, <month>Feb</month>. <year>2018</year>.<pub-id pub-id-type="pmid">29267054</pub-id>
</mixed-citation></ref><ref id="R4"><label>[4]</label><mixed-citation publication-type="journal"><name><surname>Tsao</surname><given-names>CW</given-names></name>
<etal/>, &#x0201c;<article-title>Heart disease and stroke statistics-2022 update: A report from the American Heart Association</article-title>,&#x0201d; <source>Circulation</source>, vol. <volume>145</volume>, no. <issue>8</issue>, pp. <fpage>e153</fpage>&#x02013;<lpage>e639</lpage>, <year>2022</year>.<pub-id pub-id-type="pmid">35078371</pub-id>
</mixed-citation></ref><ref id="R5"><label>[5]</label><mixed-citation publication-type="book"><name><surname>Center</surname><given-names>NSCIS</given-names></name>, <source>Facts and Figures at a Glance</source>. <publisher-loc>Birmingham, AL, USA</publisher-loc>: <publisher-name>Univ. Alabama at Birmingham</publisher-name>, <year>2016</year>.</mixed-citation></ref><ref id="R6"><label>[6]</label><mixed-citation publication-type="journal"><name><surname>Tarakci</surname><given-names>E</given-names></name>, <name><surname>Arman</surname><given-names>N</given-names></name>, <name><surname>Tarakci</surname><given-names>D</given-names></name>, and <name><surname>Kasapcopur</surname><given-names>O</given-names></name>, &#x0201c;<article-title>Leap motion controller&#x02013;based training for upper extremity rehabilitation in children and adolescents with physical disabilities: A randomized controlled trial</article-title>,&#x0201d; <source>J. Hand Therapy</source>, vol. <volume>33</volume>, no. <issue>2</issue>, pp. <fpage>220</fpage>&#x02013;<lpage>228</lpage>, <month>Apr</month>. <year>2020</year>.</mixed-citation></ref><ref id="R7"><label>[7]</label><mixed-citation publication-type="journal"><name><surname>Vanbellingen</surname><given-names>T</given-names></name>, <name><surname>Filius</surname><given-names>SJ</given-names></name>, <name><surname>Nyffeler</surname><given-names>T</given-names></name>, and <name><surname>van Wegen</surname><given-names>EEH</given-names></name>, &#x0201c;<article-title>Usability of videogame-based dexterity training in the early rehabilitation phase of stroke patients: A pilot study</article-title>,&#x0201d; <source>Frontiers Neurol</source>., vol. <volume>8</volume>, p. <fpage>654</fpage>, <month>Dec</month>. <year>2017</year>.</mixed-citation></ref><ref id="R8"><label>[8]</label><mixed-citation publication-type="journal"><name><surname>&#x000d6;g&#x000fc;n</surname><given-names>MN</given-names></name>, <name><surname>Kurul</surname><given-names>R</given-names></name>, <name><surname>Ya&#x0015f;ar</surname><given-names>MF</given-names></name>, <name><surname>Turkoglu</surname><given-names>SA</given-names></name>, <name><surname>Avci</surname><given-names>&#x0015e;</given-names></name>, and <name><surname>Yildiz</surname><given-names>N</given-names></name>, &#x0201c;<article-title>Effect of leap motion-based 3D immersive virtual reality usage on upper extremity function in ischemic stroke patients</article-title>,&#x0201d; <source>Arquivos de Neuro-Psiquiatria</source>, vol. <volume>77</volume>, no. <issue>10</issue>, pp. <fpage>681</fpage>&#x02013;<lpage>688</lpage>, <month>Oct</month>. <year>2019</year>.<pub-id pub-id-type="pmid">31664343</pub-id>
</mixed-citation></ref><ref id="R9"><label>[9]</label><mixed-citation publication-type="journal"><name><surname>Rodr&#x000ed;guez</surname><given-names>ML</given-names></name>, <name><surname>Garc&#x000ed;a</surname><given-names>&#x000c1;G</given-names></name>, <name><surname>Loureiro</surname><given-names>JP</given-names></name>, and <name><surname>Garc&#x000ed;a</surname><given-names>TP</given-names></name>, &#x0201c;<article-title>Personalized virtual reality environments for intervention with people with disability</article-title>,&#x0201d; <source>Electronics</source>, vol. <volume>11</volume>, no. <issue>10</issue>, p. <fpage>1586</fpage>, <month>May</month>
<year>2022</year>.</mixed-citation></ref><ref id="R10"><label>[10]</label><mixed-citation publication-type="journal"><name><surname>Iosa</surname><given-names>M</given-names></name>. <etal/>, &#x0201c;<article-title>Leap motion controlled videogame-based therapy for rehabilitation of elderly patients with subacute stroke: A feasibility pilot study</article-title>,&#x0201d; <source>Topics Stroke Rehabil</source>., vol. <volume>22</volume>, no. <issue>4</issue>, pp. <fpage>306</fpage>&#x02013;<lpage>316</lpage>, <month>Aug</month>. <year>2015</year>.</mixed-citation></ref><ref id="R11"><label>[11]</label><mixed-citation publication-type="confproc"><name><surname>Li</surname><given-names>W-J</given-names></name>, <name><surname>Hsieh</surname><given-names>C-Y</given-names></name>, <name><surname>Lin</surname><given-names>L-F</given-names></name>, and <name><surname>Chu</surname><given-names>W-C</given-names></name>, &#x0201c;<source>Hand gesture recognition for post-stroke rehabilitation using leap motion</source>,&#x0201d; in <conf-name>Proc. Int. Conf. Appl. Syst. Innov. (ICASI)</conf-name>, <conf-date>May 2017</conf-date>, pp. <fpage>386</fpage>&#x02013;<lpage>388</lpage>.</mixed-citation></ref><ref id="R12"><label>[12]</label><mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>S</given-names></name>, <name><surname>Park</surname><given-names>S</given-names></name>, and <name><surname>Lee</surname><given-names>O</given-names></name>, &#x0201c;<article-title>Development of a diagnosis and evaluation system for hemiplegic patients post-stroke based on motion recognition tracking and analysis of wrist joint kinematics</article-title>,&#x0201d; <source>Sensors</source>, vol. <volume>20</volume>, no. <issue>16</issue>, p. <fpage>4548</fpage>, <month>Aug</month>. <year>2020</year>.<pub-id pub-id-type="pmid">32823784</pub-id>
</mixed-citation></ref><ref id="R13"><label>[13]</label><mixed-citation publication-type="confproc"><name><surname>Cohen</surname><given-names>MW</given-names></name>, <name><surname>Voldman</surname><given-names>I</given-names></name>, <name><surname>Regazzoni</surname><given-names>D</given-names></name>, and <name><surname>Vitali</surname><given-names>A</given-names></name>, &#x0201c;<source>Hand rehabilitation via gesture recognition using leap motion controller</source>,&#x0201d; in <conf-name>Proc. 11th Int. Conf. Hum. Syst. Interact. (HSI)</conf-name>, <conf-date>Jul. 2018</conf-date>, pp. <fpage>404</fpage>&#x02013;<lpage>410</lpage>.</mixed-citation></ref><ref id="R14"><label>[14]</label><mixed-citation publication-type="journal"><name><surname>Bank</surname><given-names>PJM</given-names></name>, <name><surname>Cidota</surname><given-names>MA</given-names></name>, <name><surname>Ouwehand</surname><given-names>PW</given-names></name>, and <name><surname>Lukosch</surname><given-names>SG</given-names></name>, &#x0201c;<article-title>Patient-tailored augmented reality games for assessing upper extremity motor impairments in Parkinson&#x02019;s disease and stroke</article-title>,&#x0201d; <source>J. Med. Syst</source>, vol. <volume>42</volume>, no. <issue>12</issue>, pp. <fpage>1</fpage>&#x02013;<lpage>11</lpage>, <month>Dec</month>. <year>2018</year>.</mixed-citation></ref><ref id="R15"><label>[15]</label><mixed-citation publication-type="confproc"><name><surname>Alimanova</surname><given-names>M</given-names></name>. <etal/>, &#x0201c;<source>Gamification of hand rehabilitation process using virtual reality tools: Using leap motion for hand rehabilitation</source>,&#x0201d; in <conf-name>Proc. 1st IEEE Int. Conf. Robotic Comput. (IRC)</conf-name>, <conf-date>Apr. 2017</conf-date>, pp. <fpage>336</fpage>&#x02013;<lpage>339</lpage>.</mixed-citation></ref><ref id="R16"><label>[16]</label><mixed-citation publication-type="journal"><name><surname>Cha</surname><given-names>K</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Shen</surname><given-names>L</given-names></name>, <name><surname>Chen</surname><given-names>Z</given-names></name>, and <name><surname>Long</surname><given-names>J</given-names></name>, &#x0201c;<article-title>A novel upper-limb tracking system in a virtual environment for stroke rehabilitation</article-title>,&#x0201d; <source>J. NeuroEng. Rehabil</source>, vol. <volume>18</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>&#x02013;<lpage>13</lpage>, <month>Dec</month>. <year>2021</year>.<pub-id pub-id-type="pmid">33397401</pub-id>
</mixed-citation></ref><ref id="R17"><label>[17]</label><mixed-citation publication-type="journal"><name><surname>Aguilera-Rubio</surname><given-names>&#x000c1;</given-names></name>, <name><surname>Alguacil-Diego</surname><given-names>IM</given-names></name>, <name><surname>Mallo-L&#x000f3;pez</surname><given-names>A</given-names></name>, and <name><surname>Cuesta-G&#x000f3;mez</surname><given-names>A</given-names></name>, &#x0201c;<article-title>Use of the leap motion controller system in the rehabilitation of the upper limb in Stroke. A systematic review</article-title>,&#x0201d; <source>J. Stroke Cerebrovascular Diseases</source>, vol. <volume>31</volume>, no. <issue>1</issue>, <month>Jan</month>. <year>2022</year>, Art. no. 106174.</mixed-citation></ref><ref id="R18"><label>[18]</label><mixed-citation publication-type="journal"><name><surname>Guna</surname><given-names>J</given-names></name>, <name><surname>Jakus</surname><given-names>G</given-names></name>, <name><surname>Pogacnik</surname><given-names>M</given-names></name>, <name><surname>Toma&#x0017e;i&#x002c7; c</surname><given-names>S</given-names></name>, and <name><surname>Sodnik</surname><given-names>J</given-names></name>, &#x0201c;<article-title>A&#x00148; analysis of the precision and reliability of the leap motion sensor and its suitability for static and dynamic tracking</article-title>,&#x0201d; <source>Sensors</source>, vol. <volume>14</volume>, no. <issue>2</issue>, pp. <fpage>3702</fpage>&#x02013;<lpage>3720</lpage>, <month>Feb</month>. <year>2014</year>.<pub-id pub-id-type="pmid">24566635</pub-id>
</mixed-citation></ref><ref id="R19"><label>[19]</label><mixed-citation publication-type="book"><name><surname>Jakus</surname><given-names>G</given-names></name>, <name><surname>Guna</surname><given-names>J</given-names></name>, <name><surname>Toma&#x0017e;ic</surname><given-names>S</given-names></name>, and <name><surname>Sodnik</surname><given-names>J</given-names></name>, &#x0201c;<part-title>Evaluation of leap&#x002c7; motion controller with a high precision optical tracking system</part-title>,&#x0201d; in <source>Human-Computer Interaction. Advanced Interaction Modalities and Techniques, Heraklion, Greece</source>
<publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <month>Jun</month>. <year>2014</year>, pp. <fpage>254</fpage>&#x02013;<lpage>263</lpage>.</mixed-citation></ref><ref id="R20"><label>[20]</label><mixed-citation publication-type="journal"><name><surname>Ganguly</surname><given-names>A</given-names></name>, <name><surname>Rashidi</surname><given-names>G</given-names></name>, and <name><surname>Mombaur</surname><given-names>K</given-names></name>, &#x0201c;<article-title>Comparison of the performance of the leap motion ControllerTM with a standard marker-based motion capture system</article-title>,&#x0201d; <source>Sensors</source>, vol. <volume>21</volume>, no. <issue>5</issue>, p. <fpage>1750</fpage>, <month>Mar</month>. <year>2021</year>.<pub-id pub-id-type="pmid">33802495</pub-id>
</mixed-citation></ref><ref id="R21"><label>[21]</label><mixed-citation publication-type="journal"><name><surname>Houston</surname><given-names>A</given-names></name>, <name><surname>Walters</surname><given-names>V</given-names></name>, <name><surname>Corbett</surname><given-names>T</given-names></name>, and <name><surname>Coppack</surname><given-names>R</given-names></name>, &#x0201c;<article-title>Evaluation of a multi-sensor leap motion setup for biomechanical motion capture of the hand</article-title>,&#x0201d; <source>J. Biomechanics</source>, vol. <volume>127</volume>, <month>Oct</month>. <year>2021</year>, Art. no. 110713.</mixed-citation></ref><ref id="R22"><label>[22]</label><mixed-citation publication-type="journal"><name><surname>Mizera</surname><given-names>C</given-names></name>, <name><surname>Delrieu</surname><given-names>T</given-names></name>, <name><surname>Weistroffer</surname><given-names>V</given-names></name>, <name><surname>Andriot</surname><given-names>C</given-names></name>, <name><surname>Decatoire</surname><given-names>A</given-names></name>, and <name><surname>Gazeau</surname><given-names>J-P</given-names></name>, &#x0201c;<article-title>Evaluation of hand-tracking systems in teleoperation and virtual dexterous manipulation</article-title>,&#x0201d; <source>IEEE Sensors J</source>., vol. <volume>20</volume>, no. <issue>3</issue>, pp. <fpage>1642</fpage>&#x02013;<lpage>1655</lpage>, <month>Feb</month>. <year>2020</year>.</mixed-citation></ref><ref id="R23"><label>[23]</label><mixed-citation publication-type="journal"><name><surname>Niechwiej-Szwedo</surname><given-names>E</given-names></name>, <name><surname>Gonzalez</surname><given-names>D</given-names></name>, <name><surname>Nouredanesh</surname><given-names>M</given-names></name>, and <name><surname>Tung</surname><given-names>J</given-names></name>, &#x0201c;<article-title>Evaluation of the leap motion controller during the performance of visually-guided upper limb movements</article-title>,&#x0201d; <source>PLoS ONE</source>, vol. <volume>13</volume>, no. <issue>3</issue>, <month>Mar</month>. <year>2018</year>, Art. no. e0193639.</mixed-citation></ref><ref id="R24"><label>[24]</label><mixed-citation publication-type="journal"><name><surname>Tung</surname><given-names>JY</given-names></name>, <name><surname>Lulic</surname><given-names>T</given-names></name>, <name><surname>Gonzalez</surname><given-names>DA</given-names></name>, <name><surname>Tran</surname><given-names>J</given-names></name>, <name><surname>Dickerson</surname><given-names>CR</given-names></name>, and <name><surname>Roy</surname><given-names>EA</given-names></name>, &#x0201c;<article-title>Evaluation of a portable markerless finger position capture device: Accuracy of the leap motion controller in healthy adults</article-title>,&#x0201d; <source>Physiolog. Meas</source>, vol. <volume>36</volume>, no. <issue>5</issue>, pp. <fpage>1025</fpage>&#x02013;<lpage>1035</lpage>, <month>May</month>
<year>2015</year>.</mixed-citation></ref><ref id="R25"><label>[25]</label><mixed-citation publication-type="journal"><name><surname>Smeragliuolo</surname><given-names>AH</given-names></name>, <name><surname>Hill</surname><given-names>NJ</given-names></name>, <name><surname>Disla</surname><given-names>L</given-names></name>, and <name><surname>Putrino</surname><given-names>D</given-names></name>, &#x0201c;<article-title>Validation of the leap motion controller using markered motion capture technology</article-title>,&#x0201d; <source>J. Biomechanics</source>, vol. <volume>49</volume>, no. <issue>9</issue>, pp. <fpage>1742</fpage>&#x02013;<lpage>1750</lpage>, <month>Jun</month>. <year>2016</year>.</mixed-citation></ref><ref id="R26"><label>[26]</label><mixed-citation publication-type="journal"><name><surname>Gummesson</surname><given-names>C</given-names></name>, <name><surname>Ward</surname><given-names>MM</given-names></name>, and <name><surname>Atroshi</surname><given-names>I</given-names></name>, &#x0201c;<article-title>The shortened disabilities of the arm, shoulder and hand questionnaire (Quick DASH): Validity and reliability based on responses within the full-length DASH</article-title>,&#x0201d; <source>BMC Musculoskeletal Disorders</source>, vol. <volume>7</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>&#x02013;<lpage>7</lpage>, <month>Dec</month>. <year>2006</year>.<pub-id pub-id-type="pmid">16393336</pub-id>
</mixed-citation></ref><ref id="R27"><label>[27]</label><mixed-citation publication-type="journal"><name><surname>Colvonen</surname><given-names>PJ</given-names></name>, <name><surname>DeYoung</surname><given-names>PN</given-names></name>, <name><surname>Bosompra</surname><given-names>N-O-A</given-names></name>, and <name><surname>Owens</surname><given-names>RL</given-names></name>, &#x0201c;<article-title>Limiting racial disparities and bias for wearable devices in health science research</article-title>,&#x0201d; <source>Sleep</source>, vol. <volume>43</volume>, no. <issue>10</issue>, <month>Oct</month>. <year>2020</year>, Art. no. zsaa159.</mixed-citation></ref><ref id="R28"><label>[28]</label><mixed-citation publication-type="journal"><name><surname>Bent</surname><given-names>B</given-names></name>, <name><surname>Goldstein</surname><given-names>BA</given-names></name>, <name><surname>Kibbe</surname><given-names>WA</given-names></name>, and <name><surname>Dunn</surname><given-names>JP</given-names></name>, &#x0201c;<article-title>Investigating sources of inaccuracy in wearable optical heart rate sensors</article-title>,&#x0201d; <source>NPJ Digit. Med</source>, vol. <volume>3</volume>, no. <issue>1</issue>, p. <fpage>18</fpage>, <month>Feb</month>. <year>2020</year>.<pub-id pub-id-type="pmid">32047863</pub-id>
</mixed-citation></ref><ref id="R29"><label>[29]</label><mixed-citation publication-type="journal"><name><surname>Shcherbina</surname><given-names>A</given-names></name>. <etal/>, &#x0201c;<article-title>Accuracy in wrist-worn, sensor-based measurements of heart rate and energy expenditure in a diverse cohort</article-title>,&#x0201d; <source>J. Personalized Med</source>, vol. <volume>7</volume>, no. <issue>2</issue>, p. <fpage>3</fpage>, <month>May</month>
<year>2017</year>.</mixed-citation></ref><ref id="R30"><label>[30]</label><mixed-citation publication-type="journal"><name><surname>Weichert</surname><given-names>F</given-names></name>, <name><surname>Bachmann</surname><given-names>D</given-names></name>, <name><surname>Rudak</surname><given-names>B</given-names></name>, and <name><surname>Fisseler</surname><given-names>D</given-names></name>, &#x0201c;<article-title>Analysis of the accuracy and robustness of the leap motion controller</article-title>,&#x0201d; <source>Sensors</source>, vol. <volume>13</volume>, no. <issue>5</issue>, pp. <fpage>6380</fpage>&#x02013;<lpage>6393</lpage>, <month>May</month>
<year>2013</year>.<pub-id pub-id-type="pmid">23673678</pub-id>
</mixed-citation></ref><ref id="R31"><label>[31]</label><mixed-citation publication-type="journal"><name><surname>RH</surname><given-names>J</given-names></name>, &#x0201c;<article-title>An objective and standardized test of hand function</article-title>,&#x0201d; <source>Arch. Phys. Med. Rehabil</source>, vol. <volume>50</volume>, pp. <fpage>311</fpage>&#x02013;<lpage>319</lpage>, <month>Jan</month>. <year>1969</year>.<pub-id pub-id-type="pmid">5788487</pub-id>
</mixed-citation></ref><ref id="R32"><label>[32]</label><mixed-citation publication-type="book"><name><surname>Hotelling</surname><given-names>H</given-names></name>, <source>Analysis of a Complex of Statistical Variables Into Principal Components</source>. <publisher-loc>Baltimore, MD, USA</publisher-loc>: <publisher-name>Warwick &#x00026; York</publisher-name>, <year>1933</year>.</mixed-citation></ref><ref id="R33"><label>[33]</label><mixed-citation publication-type="journal"><name><surname>Santello</surname><given-names>M</given-names></name>, <name><surname>Flanders</surname><given-names>M</given-names></name>, and <name><surname>Soechting</surname><given-names>JF</given-names></name>, &#x0201c;<article-title>Postural hand synergies for tool use</article-title>,&#x0201d; <source>J. Neurosci</source>, vol. <volume>18</volume>, no. <issue>23</issue>, pp. <fpage>10105</fpage>&#x02013;<lpage>10115</lpage>, <month>Dec</month>. <year>1998</year>.<pub-id pub-id-type="pmid">9822764</pub-id>
</mixed-citation></ref><ref id="R34"><label>[34]</label><mixed-citation publication-type="journal"><name><surname>Ingram</surname><given-names>JN</given-names></name>, <name><surname>K&#x000f6;rding</surname><given-names>KP</given-names></name>, <name><surname>Howard</surname><given-names>IS</given-names></name>, and <name><surname>Wolpert</surname><given-names>DM</given-names></name>, &#x0201c;<article-title>The statistics of natural hand movements</article-title>,&#x0201d; <source>Exp. Brain Res</source>, vol. <volume>188</volume>, no. <issue>2</issue>, pp. <fpage>223</fpage>&#x02013;<lpage>236</lpage>, <month>Jun</month>. <year>2008</year>.<pub-id pub-id-type="pmid">18369608</pub-id>
</mixed-citation></ref><ref id="R35"><label>[35]</label><mixed-citation publication-type="confproc"><name><surname>Todorov</surname><given-names>E</given-names></name>. and <name><surname>Ghahramani</surname><given-names>Z</given-names></name>, &#x0201c;<source>Analysis of the synergies underlying complex hand manipulation</source>,&#x0201d; in <conf-name>Proc. 26th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc</conf-name>., <conf-date>Aug. 2004</conf-date>, pp. <fpage>4637</fpage>&#x02013;<lpage>4640</lpage>.</mixed-citation></ref><ref id="R36"><label>[36]</label><mixed-citation publication-type="journal"><name><surname>Portnova-Fahreeva</surname><given-names>AA</given-names></name>, <name><surname>Rizzoglio</surname><given-names>F</given-names></name>, <name><surname>Nisky</surname><given-names>I</given-names></name>, <name><surname>Casadio</surname><given-names>M</given-names></name>, <name><surname>Mussa-Ivaldi</surname><given-names>FA</given-names></name>, and <name><surname>Rombokas</surname><given-names>E</given-names></name>, &#x0201c;<article-title>Linear and non-linear dimensionality-reduction techniques on full hand kinematics</article-title>,&#x0201d; <source>Frontiers Bioeng. Biotechnol</source>, vol. <volume>8</volume>, p. <fpage>429</fpage>, <month>May</month>
<year>2020</year>.</mixed-citation></ref><ref id="R37"><label>[37]</label><mixed-citation publication-type="journal"><name><surname>Shapiro</surname><given-names>SS</given-names></name> and <name><surname>Wilk</surname><given-names>MB</given-names></name>, &#x0201c;<article-title>An analysis of variance test for normality (complete samples)</article-title>,&#x0201d; <source>Biometrika</source>, vol. 52, nos. 3&#x02013;<volume>4</volume>, pp. <fpage>591</fpage>&#x02013;<lpage>611</lpage>, <month>Dec</month>. <year>1965</year>, doi: <pub-id pub-id-type="doi">10.1093/biomet/52.3-4.591</pub-id>.</mixed-citation></ref><ref id="R38"><label>[38]</label><mixed-citation publication-type="other"><article-title>Shapiro-Wilk and Shapiro-Francia Normality Tests</article-title>. (<year>2009</year>). <source>MATLAB Central File Exchange</source>. [Online]. Available: <ext-link xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/13964-shapirowilk-and-shapiro-francia-normality-tests" ext-link-type="uri">https://www.mathworks.com/matlabcentral/fileexchange/13964-shapirowilk-and-shapiro-francia-normality-tests</ext-link></mixed-citation></ref><ref id="R39"><label>[39]</label><mixed-citation publication-type="journal"><name><surname>Holm</surname><given-names>S</given-names></name>, &#x0201c;<article-title>A simple sequentially rejective multiple test procedure</article-title>,&#x0201d; <source>Scandin. J. Statist</source>, vol. <volume>6</volume>, no. <issue>2</issue>, pp. <fpage>65</fpage>&#x02013;<lpage>70</lpage>, <year>1979</year>.</mixed-citation></ref><ref id="R40"><label>[40]</label><mixed-citation publication-type="webpage"><source>Holm-Sidak t-Test: A Routine for Multiple t-Test Comparisons</source>. (<year>2006</year>). [Online]. Available: <ext-link xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/12786-holm-sidak-t-test" ext-link-type="uri">https://www.mathworks.com/matlabcentral/fileexchange/12786-holm-sidak-t-test</ext-link></mixed-citation></ref><ref id="R41"><label>[41]</label><mixed-citation publication-type="journal"><name><surname>Dunn</surname><given-names>OJ</given-names></name>, &#x0201c;<article-title>Multiple comparisons among means</article-title>,&#x0201d; <source>J. Amer. Stat. Assoc</source>, vol. <volume>56</volume>, no. <issue>293</issue>, pp. <fpage>52</fpage>&#x02013;<lpage>64</lpage>, <month>Mar</month>. <year>1961</year>.</mixed-citation></ref><ref id="R42"><label>[42]</label><mixed-citation publication-type="other"><article-title>Dunn&#x02019;s test</article-title>. (<year>2006</year>). <source>GitHub</source>. [Online]. Available: <ext-link xlink:href="https://github.com/dnafinder/dunn" ext-link-type="uri">https://github.com/dnafinder/dunn</ext-link></mixed-citation></ref><ref id="R43"><label>[43]</label><mixed-citation publication-type="journal"><name><surname>Johnsson</surname><given-names>T</given-names></name>, &#x0201c;<article-title>A procedure for stepwise regression analysis</article-title>,&#x0201d; <source>Stat. Papers</source>, vol. <volume>33</volume>, no. <issue>1</issue>, pp. <fpage>21</fpage>&#x02013;<lpage>29</lpage>, <month>Dec</month>. <year>1992</year>.</mixed-citation></ref><ref id="R44"><label>[44]</label><mixed-citation publication-type="journal"><name><surname>Tenenbaum</surname><given-names>JB</given-names></name>, <name><surname>De Silva</surname><given-names>V</given-names></name>, and <name><surname>Langford</surname><given-names>JC</given-names></name>, &#x0201c;<article-title>A global geometric framework for nonlinear dimensionality reduction</article-title>,&#x0201d; <source>Science</source>, vol. <volume>290</volume>, no. <issue>5500</issue>, pp. <fpage>2319</fpage>&#x02013;<lpage>2323</lpage>, <year>2000</year>.<pub-id pub-id-type="pmid">11125149</pub-id>
</mixed-citation></ref><ref id="R45"><label>[45]</label><mixed-citation publication-type="journal"><name><surname>Vinjamuri</surname><given-names>R</given-names></name>, <name><surname>Sun</surname><given-names>M</given-names></name>, <name><surname>Chang</surname><given-names>C-C</given-names></name>, <name><surname>Lee</surname><given-names>H-N</given-names></name>, <name><surname>Sclabassi</surname><given-names>RJ</given-names></name>, and <name><surname>Mao</surname><given-names>Z-H</given-names></name>, &#x0201c;<article-title>Dimensionality reduction in control and coordination of the human hand</article-title>,&#x0201d; <source>IEEE Trans. Biomed. Eng</source>, vol. <volume>57</volume>, no. <issue>2</issue>, pp. <fpage>284</fpage>&#x02013;<lpage>295</lpage>, <month>Feb</month>. <year>2010</year>.<pub-id pub-id-type="pmid">19789098</pub-id>
</mixed-citation></ref><ref id="R46"><label>[46]</label><mixed-citation publication-type="journal"><name><surname>Torres-Oviedo</surname><given-names>G</given-names></name>. and <name><surname>Ting</surname><given-names>LH</given-names></name>, &#x0201c;<article-title>Muscle synergies characterizing human postural responses</article-title>,&#x0201d; <source>J. Neurophysiol</source>, vol. <volume>98</volume>, no. <issue>4</issue>, pp. <fpage>2144</fpage>&#x02013;<lpage>2156</lpage>, <month>Oct</month>. <year>2007</year>.<pub-id pub-id-type="pmid">17652413</pub-id>
</mixed-citation></ref><ref id="R47"><label>[47]</label><mixed-citation publication-type="journal"><name><surname>Boe</surname><given-names>D</given-names></name>. <etal/>, &#x0201c;<article-title>Dimensionality reduction of human gait for prosthetic control</article-title>,&#x0201d; <source>Frontiers Bioeng. Biotechnol</source>, vol. <volume>9</volume>, <month>Oct</month>. <year>2021</year>, Art. no. 724626.</mixed-citation></ref><ref id="R48"><label>[48]</label><mixed-citation publication-type="webpage"><name><surname>Saul</surname><given-names>LK</given-names></name> and <name><surname>Roweis</surname><given-names>ST</given-names></name> (<year>2000</year>). <source>An Introduction to Locally Linear Embedding</source>. [Online]. Available: <ext-link xlink:href="http://www.cs.toronto.edu/~roweis/lle/publications.html" ext-link-type="uri">http://www.cs.toronto.edu/~roweis/lle/publications.html</ext-link></mixed-citation></ref><ref id="R49"><label>[49]</label><mixed-citation publication-type="journal"><name><surname>Hochreiter</surname><given-names>S</given-names></name>. and <name><surname>Schmidhuber</surname><given-names>J</given-names></name>, &#x0201c;<article-title>Long short-term memory</article-title>,&#x0201d; <source>Neural Comput</source>., vol. <volume>9</volume>, no. <issue>8</issue>, pp. <fpage>1735</fpage>&#x02013;<lpage>1780</lpage>, <year>1997</year>.<pub-id pub-id-type="pmid">9377276</pub-id>
</mixed-citation></ref><ref id="R50"><label>[50]</label><mixed-citation publication-type="journal"><name><surname>Sturma</surname><given-names>A</given-names></name>, <name><surname>Hruby</surname><given-names>LA</given-names></name>, <name><surname>Prahm</surname><given-names>C</given-names></name>, <name><surname>Mayer</surname><given-names>JA</given-names></name>, and <name><surname>Aszmann</surname><given-names>OC</given-names></name>, &#x0201c;<article-title>Rehabilitation of upper extremity nerve injuries using surface EMG biofeedback: Protocols for clinical application</article-title>,&#x0201d; <source>Frontiers Neurosci</source>., vol. <volume>12</volume>, p. <fpage>906</fpage>, <month>Dec</month>. <year>2018</year>.</mixed-citation></ref><ref id="R51"><label>[51]</label><mixed-citation publication-type="journal"><name><surname>Giggins</surname><given-names>OM</given-names></name>, <name><surname>Persson</surname><given-names>UM</given-names></name>, and <name><surname>Caulfield</surname><given-names>B</given-names></name>, &#x0201c;<article-title>Biofeedback in rehabilitation</article-title>,&#x0201d; <source>J. Neuroeng. Rehabil</source>, vol. <volume>10</volume>, pp. <fpage>1</fpage>&#x02013;<lpage>11</lpage>, <year>2013</year>.<pub-id pub-id-type="pmid">23336711</pub-id>
</mixed-citation></ref><ref id="R52"><label>[52]</label><mixed-citation publication-type="journal"><name><surname>Cisnal</surname><given-names>A</given-names></name>, <name><surname>Gordaliza</surname><given-names>P</given-names></name>, <name><surname>Turiel</surname><given-names>JP</given-names></name>, and <name><surname>Fraile</surname><given-names>JC</given-names></name>, &#x0201c;<article-title>Interaction with a hand rehabilitation exoskeleton in EMG-driven bilateral therapy: Influence of visual biofeedback on the users&#x02019; performance</article-title>,&#x0201d; <source>Sensors</source>, vol. <volume>23</volume>, no. <issue>4</issue>, p. <fpage>2048</fpage>, <month>Feb</month>. <year>2023</year>.<pub-id pub-id-type="pmid">36850650</pub-id>
</mixed-citation></ref><ref id="R53"><label>[53]</label><mixed-citation publication-type="journal"><name><surname>Ali</surname><given-names>M</given-names></name>, <name><surname>Jones</surname><given-names>MW</given-names></name>, <name><surname>Xie</surname><given-names>X</given-names></name>, and <name><surname>Williams</surname><given-names>M</given-names></name>, &#x0201c;<article-title>TimeCluster: Dimension reduction applied to temporal data for visual analytics</article-title>,&#x0201d; <source>Vis. Comput</source>, vol. <volume>35</volume>, nos. <issue>6</issue>&#x02013;8, pp. <fpage>1013</fpage>&#x02013;<lpage>1026</lpage>, <month>Jun</month>. <year>2019</year>.</mixed-citation></ref><ref id="R54"><label>[54]</label><mixed-citation publication-type="confproc"><name><surname>Hobbs</surname><given-names>B</given-names></name>. and <name><surname>Artemiadis</surname><given-names>P</given-names></name>, &#x0201c;<source>A systematic method for outlier detection in human gait data</source>,&#x0201d; in <conf-name>Proc. Int. Conf. Rehabil. Robot. (ICORR)</conf-name>, <month>Jul</month>. <year>2022</year>, pp. <fpage>1</fpage>&#x02013;<lpage>6</lpage>.</mixed-citation></ref><ref id="R55"><label>[55]</label><mixed-citation publication-type="journal"><name><surname>Paragliola</surname><given-names>G</given-names></name>. and <name><surname>Coronato</surname><given-names>A</given-names></name>, &#x0201c;<article-title>Gait anomaly detection of subjects with Parkinson&#x02019;s disease using a deep time series-based approach</article-title>,&#x0201d; <source>IEEE Access</source>, vol. <volume>6</volume>, pp. <fpage>73280</fpage>&#x02013;<lpage>73292</lpage>, <year>2018</year>.</mixed-citation></ref><ref id="R56"><label>[56]</label><mixed-citation publication-type="confproc"><name><surname>Amor</surname><given-names>LB</given-names></name>, <name><surname>Lahyani</surname><given-names>I</given-names></name>, and <name><surname>Jmaiel</surname><given-names>M</given-names></name>, &#x0201c;<source>PCA-based multivariate anomaly detection in mobile healthcare applications</source>,&#x0201d; in <conf-name>Proc. IEEE/ACM 21st Int. Symp. Distrib. Simulation Real Time Appl. (DS-RT)</conf-name>, <conf-date>Nov. 2017</conf-date>, pp. <fpage>1</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="R57"><label>[57]</label><mixed-citation publication-type="confproc"><name><surname>Ben Amor</surname><given-names>L</given-names></name>, <name><surname>Lahyani</surname><given-names>I</given-names></name>, <name><surname>Jmaiel</surname><given-names>M</given-names></name>, and <name><surname>Drira</surname><given-names>K</given-names></name>, &#x0201c;<source>Anomaly detection and diagnosis scheme for mobile health applications</source>,&#x0201d; in <conf-name>Proc. IEEE 32nd Int. Conf. Adv. Inf. Netw. Appl. (AINA)</conf-name>, <conf-date>May 2018</conf-date>, pp. <fpage>777</fpage>&#x02013;<lpage>784</lpage>.</mixed-citation></ref></ref-list></back><floats-group><fig position="float" id="F1"><label>Fig. 1.</label><caption><p id="P58"><bold>(a)</bold> The Fitzpatrick scale for skin tone self-assessment. <bold>(b)</bold> Placement of the 21 markers on the hand. <bold>(c)</bold> Kapandji task. <bold>(d)</bold> Finger abduction/adduction task.</p></caption><graphic xlink:href="nihms-2069451-f0001" position="float"/></fig><fig position="float" id="F2"><label>Fig. 2.</label><caption><p id="P59"><bold>(a)</bold> Correlation, <inline-formula><mml:math id="M58" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <bold>(b)</bold> mean absolute error (MAE) of digit landmark trajectories between Leap and Optitrack averaged across all signals for each participant in the disabled (N=10) and nondisabled (N=7) groups during Kapandji and finger abduction/adduction tasks. Averages and standard errors across participants are plotted. * indicates significant difference in the mean correlation for nondominant hand between disabled and nondisabled participants (<italic toggle="yes">p</italic> = 0.003, Holm-Sidak test).</p></caption><graphic xlink:href="nihms-2069451-f0002" position="float"/></fig><fig position="float" id="F3"><label>Fig. 3.</label><caption><p id="P60">Correlation <inline-formula><mml:math id="M59" display="inline"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> between Euclidian distances calculated between (a) thumb tip and other fingertips for the Kapandji task and (b) neighboring fingertips during the finger abduction/adduction task. Average correlation was calculated for each participant across all trials, and then averaged across participants. The graph presents average and standard errors. * indicate statistical significance with p &#x0003c; 0.01, Dunn&#x02019;s test for multiple comparisons.</p></caption><graphic xlink:href="nihms-2069451-f0003" position="float"/></fig><fig position="float" id="F4"><label>Fig. 4.</label><caption><p id="P61">Mean absolute errors between finger segments calculated from the Leap and Optitrack hand models for the dominant (top) and nondominant (bottom) hands. * indicates statistical significance with p &#x0003c; 0.01, Dunn&#x02019;s test for multiple comparisons.</p></caption><graphic xlink:href="nihms-2069451-f0004" position="float"/></fig><fig position="float" id="F5"><label>Fig. 5.</label><caption><p id="P62">Examples of Optitrack (blue) and Leap (red) datasets encoded in a 2D latent space using PCA for the Optitrack and Leap data recorded during a single repetition of the Kapandji task by one of the participants.</p></caption><graphic xlink:href="nihms-2069451-f0005" position="float"/></fig><fig position="float" id="F6"><label>Fig. 6.</label><caption><p id="P63">Correlation between Leap and Optitrack across all signals in the high-dimensional space <inline-formula><mml:math id="M60" display="inline"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> versus the 2D latent spaces <inline-formula><mml:math id="M61" display="inline"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> during Kapandji (top) and finger abduction/adduction (bottom) tasks.</p></caption><graphic xlink:href="nihms-2069451-f0006" position="float"/></fig></floats-group></article>