<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006341</article-id><article-id pub-id-type="pmc">PMC11859350</article-id><article-id pub-id-type="doi">10.3390/s25041113</article-id><article-id pub-id-type="publisher-id">sensors-25-01113</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Laser Stripe Centerline Extraction Method for Deep-Hole Inner Surfaces Based on Line-Structured Light Vision Sensing</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4039-0264</contrib-id><name><surname>Du</surname><given-names>Huifu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01113" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6620-1831</contrib-id><name><surname>Yu</surname><given-names>Daguo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-01113" ref-type="aff">1</xref><xref rid="af2-sensors-25-01113" ref-type="aff">2</xref><xref rid="c1-sensors-25-01113" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-3199-5511</contrib-id><name><surname>Zhao</surname><given-names>Xiaowei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01113" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Ziyang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01113" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Perri</surname><given-names>Stefania</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01113"><label>1</label>School of Mechanical Engineering, North University of China, Taiyuan 030051, China; <email>b20230211@st.nuc.edu.cn</email> (H.D.); <email>b20220206@st.nuc.edu.cn</email> (X.Z.); <email>2202041109@st.nuc.edu.cn</email> (Z.Z.)</aff><aff id="af2-sensors-25-01113"><label>2</label>School of Mechanical Engineering, Nantong Institute of Technology, Nantong 226001, China</aff><author-notes><corresp id="c1-sensors-25-01113"><label>*</label>Correspondence: <email>yudaguo@nuc.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1113</elocation-id><history><date date-type="received"><day>10</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>10</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>11</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>This paper proposes a point cloud post-processing method based on the minimum spanning tree (MST) and depth-first search (DFS) to extract laser stripe centerlines from the complex inner surfaces of deep holes. Addressing the limitations of traditional image processing methods, which are affected by burrs and low-frequency random noise, this method utilizes 360&#x000b0; structured light to illuminate the inner wall of the deep hole. A sensor captures laser stripe images, and the Steger algorithm is employed to extract sub-pixel point clouds. Subsequently, an MST is used to construct the point cloud connectivity structure, while DFS is applied for path search and noise removal to enhance extraction accuracy. Experimental results demonstrate that this method significantly improves extraction accuracy, with a dice similarity coefficient (DSC) approaching 1 and a maximum Hausdorff distance (HD) of 3.3821 pixels, outperforming previous methods. This study provides an efficient and reliable solution for the precise extraction of complex laser stripes and lays a solid data foundation for subsequent feature parameter calculations and 3D reconstruction.</p></abstract><kwd-group><kwd>laser stripe centerline extraction</kwd><kwd>image processing</kwd><kwd>minimum spanning tree</kwd><kwd>depth-first search</kwd><kwd>noise removal</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>51875532</award-id></award-group><award-group><funding-source>Central Guidance for Local Scientific and Technological Development Funding Projects</funding-source><award-id>YDZJSX2022C006</award-id></award-group><funding-statement>This research was funded by the National Natural Science Foundation of China (grant number 51875532) and the Central Guidance for Local Scientific and Technological Development Funding Projects (grant number YDZJSX2022C006).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01113"><title>1. Introduction</title><p>Deep holes are defined as holes with a depth-to-diameter ratio of greater than 5. Deep hole inspection technology must overcome challenges such as hole depth, insufficient illumination, complex geometries, and high-precision measurement. Therefore, the difficulty of deep hole inspection is significantly higher than that of external surface inspection [<xref rid="B1-sensors-25-01113" ref-type="bibr">1</xref>]. Deep hole inspection based on the annular light-sectioning method is one of the most important applications of line-structured light sensors. This technology is widely used in the aerospace, automotive manufacturing, precision machining, petroleum drill pipe, and defense industries. It offers advantages such as high precision, fast measurement, and non-contact inspection [<xref rid="B2-sensors-25-01113" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01113" ref-type="bibr">3</xref>]. The core technology involves projecting 360&#x000b0; line-structured light onto the inner wall of the deep hole to form continuous laser stripes. High-precision vision sensors capture these stripe images, and efficient image processing algorithms are then used to accurately extract the centerline coordinates of the stripes. Based on these coordinate data, further measurements of geometric parameters such as inner diameter, roundness, straightness, and cylindricity can be performed, as well as 3D reconstruction. Throughout the measurement process, extracting the laser stripe centerline is critical, as its accuracy directly affects the reliability and precision of the measurement results [<xref rid="B4-sensors-25-01113" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01113" ref-type="bibr">5</xref>].</p><p>The measurement accuracy of deep hole components is crucial for ensuring the assembly precision and functional reliability of the parts, especially in the measurement of complex deep hole components such as internal gears, internal splines, and gun barrels. The complexity of geometric shapes, surface roughness, machining tool marks, and surface reflection characteristics often cause discontinuous and uneven laser stripes, significantly increasing the difficulty of extracting the laser stripe centerline [<xref rid="B6-sensors-25-01113" ref-type="bibr">6</xref>]. These factors pose severe challenges to traditional image processing methods, such as the Steger, Canny, Sobel, Prewitt, and Roberts algorithms [<xref rid="B7-sensors-25-01113" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01113" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01113" ref-type="bibr">9</xref>]. When dealing with complex geometric stripe images, contour point cloud data extracted by these algorithms often contain burrs and low-frequency random noise, making it difficult for them to meet the high-precision measurement requirements in terms of extraction accuracy and reliability [<xref rid="B10-sensors-25-01113" ref-type="bibr">10</xref>].</p><p>In the study of laser stripe centerline extraction, many researchers have improved and optimized existing algorithms to enhance their adaptability and accuracy. Common improvement methods include Hessian matrix-based algorithms, grayscale gravity methods, DBSCAN clustering, adaptive bidirectional grayscale methods, and adaptive convolution techniques. These methods each have their own advantages and limitations in different application scenarios. Hessian matrix-based algorithms [<xref rid="B11-sensors-25-01113" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01113" ref-type="bibr">12</xref>] identify the stripe center position by calculating the second-order derivatives of the image. They are suitable for flat or relatively simple surfaces but are highly sensitive to noise and perform poorly when dealing with complex stripes or blurry edges. The grayscale gravity method [<xref rid="B13-sensors-25-01113" ref-type="bibr">13</xref>] optimizes stripe extraction using image grayscale information and a gravity model, effectively handling flat stripes, but its accuracy decreases on complex surfaces and in noisy environments. DBSCAN clustering [<xref rid="B14-sensors-25-01113" ref-type="bibr">14</xref>], as an unsupervised algorithm, effectively handles noise and non-uniform stripe data, offering strong adaptability. However, its computational efficiency is low in dense stripe regions or large-scale datasets, limiting its accuracy. The adaptive bidirectional grayscale method [<xref rid="B15-sensors-25-01113" ref-type="bibr">15</xref>] incorporates stripe contextual information, making it suitable for irregular stripe shapes, but it has high computational complexity and poor real-time performance when processing large-scale stripes. Adaptive convolution techniques [<xref rid="B16-sensors-25-01113" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01113" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01113" ref-type="bibr">18</xref>], particularly convolutional neural network (CNN)-based methods, dynamically adjust extraction strategies to improve accuracy. They perform well for complex backgrounds and discontinuous stripe processing but require a large amount of labeled data and consume substantial computational resources.</p><p>In recent years, emerging technologies have made significant progress in improving the accuracy and adaptability of laser stripe extraction. For example, deep learning-based image segmentation methods such as U-Net [<xref rid="B19-sensors-25-01113" ref-type="bibr">19</xref>] can effectively handle complex backgrounds and irregular stripe patterns, enhancing extraction accuracy by learning image structural information. Adaptive filtering methods [<xref rid="B20-sensors-25-01113" ref-type="bibr">20</xref>] dynamically adjust filter parameters, performing well in noisy environments and on complex surfaces. Multi-scale analysis methods [<xref rid="B21-sensors-25-01113" ref-type="bibr">21</xref>] combine stripe features at different resolutions, showing strong adaptability, especially when processing multi-scale stripes. Deep generative adversarial networks (GANs) [<xref rid="B22-sensors-25-01113" ref-type="bibr">22</xref>] have been widely applied to generate high-quality stripe images, demonstrating remarkable advantages in noise removal and stripe restoration. Self-supervised learning [<xref rid="B23-sensors-25-01113" ref-type="bibr">23</xref>] enables effective training without requiring large amounts of labeled data, improving algorithm scalability and flexibility. Sensor fusion techniques [<xref rid="B24-sensors-25-01113" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-01113" ref-type="bibr">25</xref>], which integrate multiple sensor sources such as laser stripes, visual imaging, and structured light, effectively address challenges posed by complex surfaces and diverse materials, providing more precise 3D reconstruction and stripe extraction results. In particular, the combination of deep learning and sensor fusion [<xref rid="B26-sensors-25-01113" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01113" ref-type="bibr">27</xref>] offers significant advantages over traditional methods in noise suppression, background complexity handling, and real-time performance. However, despite these advancements, these methods still face challenges in applications involving complex components due to their high computational demands and relatively low real-time performance, necessitating further research and optimization.</p><p>The research team previously used the annular light-sectioning method to measure the diameter of deep-hole steel pipe components. For components with relatively smooth inner surfaces, the Steger algorithm can quickly and accurately extract the contour of the annular laser beam. However, when applying this algorithm to extract the laser stripe centerline of components with complex internal cavity shapes (e.g., petal-shaped structures, internal gears, splines, and internal octagons), issues such as burrs, low-frequency noise, and redundant branches often arise. These problems severely affect the accuracy and completeness of the extraction results, making it difficult to meet the high-precision measurement and complex component analysis requirements [<xref rid="B28-sensors-25-01113" ref-type="bibr">28</xref>]. Nevertheless, the Steger algorithm can still retain contour details during the overall extraction process. However, automatically identifying the precise centerline of laser stripes from noisy and sub-pixel point cloud data remains a major challenge in the field of complex laser stripe extraction [<xref rid="B29-sensors-25-01113" ref-type="bibr">29</xref>]. Currently, noise points are mainly removed manually to obtain a complete contour before performing feature parameter calculations and 3D reconstruction. This process is time-consuming and susceptible to human error.</p><p>Against this background, this paper proposes a method for extracting the laser stripe centerline in complex deep-hole internal cavities based on the minimum spanning tree (MST) and depth-first search (DFS). This study first employed the Steger algorithm to generate a sub-pixel point cloud model that contains noise. Then, MST and DFS are applied to search and track the main contour, effectively eliminating interfering point cloud data to accurately obtain the point cloud coordinates of the laser stripe&#x02019;s geometric center. This method fully utilizes MST&#x02019;s advantage in constructing an effective connectivity structure for point cloud data, ensuring reasonable connections between point clouds. By combining DFS&#x02019;s traversal and path-searching capabilities, it precisely locates and tracks the main contour. Experimental results show that this method can quickly and effectively remove burrs and extraneous branches, significantly improving the extraction accuracy of laser stripe centerlines in complex components. This provides a reliable data foundation for subsequent feature parameter calculations, 3D reconstruction, and further analysis [<xref rid="B30-sensors-25-01113" ref-type="bibr">30</xref>].</p><sec><title>Technical Terms and Abbreviations</title><p>To facilitate understanding, the following abbreviations and technical terms are used in this paper:</p><p>MST (minimum spanning tree): An algorithm used to construct a connected, acyclic graph that connects all vertices while minimizing the total edge weight.</p><p>DFS (depth-first search): An algorithm used to traverse or search tree or graph structures, starting from one node and exploring as deep as possible before backtracking.</p><p>DSC (dice similarity coefficient): A statistical measure used to quantify the similarity between two sample sets, with values ranging from 0 to 1, where 1 represents perfect overlap and 0 indicates no intersection.</p><p>HD (Hausdorff distance): A measure used to assess the maximum distance between two sets, often used to evaluate the maximum deviation between two boundaries.</p></sec></sec><sec id="sec2-sensors-25-01113"><title>2. Analysis of Laser Stripe Characteristics in Complex Deep-Hole Geometries</title><p>In the field of image processing, HALCON is a commercially available computer vision software library (<uri xlink:href="https://www.mvtec.com/products/halcon">https://www.mvtec.com/products/halcon</uri>, accessed on 10 February 2025) that has been widely validated through industrial applications, demonstrating excellent reliability and efficiency. The linesgauss operator in HALCON, based on the core of the Steger algorithm, is capable of effectively detecting various geometric shapes, such as straight lines, curves, arcs, elliptical arcs, and waveforms. This algorithm is an efficient line feature extraction method, particularly outstanding in applications that require high precision and subpixel-level edge detection. In the previous research, the Steger algorithm was successfully applied to the point cloud extraction of the laser stripe centerline on the inner surface of a deep hole part, and significant results were achieved. Therefore, this study first applies the Steger algorithm to extract the laser stripe centerlines from contours of varying complexities, aiming to investigate the performance of the algorithm in such applications.</p><p>The research team previously conducted a systematic analysis of the deep hole inspection principles based on the annular light-sectioning method. They successfully developed deep hole inspection equipment and completed the 3D reconstruction of steel pipe components, as well as temperature error compensation [<xref rid="B31-sensors-25-01113" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-01113" ref-type="bibr">32</xref>]. The laser stripe images presented below were all acquired using the deep hole inspection equipment independently developed by the research team.</p><p><xref rid="sensors-25-01113-f001" ref-type="fig">Figure 1</xref>a shows the laser stripe image of a smooth deep hole, while <xref rid="sensors-25-01113-f001" ref-type="fig">Figure 1</xref>b presents a 3D intensity distribution of the grayscale values from the image. It is evident from the figure that the laser stripe is uniformly distributed and has a simple contour shape. <xref rid="sensors-25-01113-f001" ref-type="fig">Figure 1</xref>c shows the result of the laser stripe centerline extraction using the Steger algorithm, and <xref rid="sensors-25-01113-f001" ref-type="fig">Figure 1</xref>d is a local zoomed-in view of the extracted result. From these results, it is clear that the Steger algorithm performs well in extracting the centerline of laser stripes with simple contours. The algorithm accurately extracts a subpixel contour curve, facilitating subsequent contour feature parameter calculations and 3D reconstruction.</p><p><xref rid="sensors-25-01113-f002" ref-type="fig">Figure 2</xref>a shows an image of the contour of the flap laser stripe, which exhibits an annular distribution consisting of a combination of straight lines and circular arcs. <xref rid="sensors-25-01113-f002" ref-type="fig">Figure 2</xref>b, on the other hand, shows the three-dimensional distribution of the gray intensity of this image. It can be observed that there is an obvious abrupt change in the gray intensity at the junction of straight lines and circular arcs, which leads to an uneven energy distribution of the laser stripes. <xref rid="sensors-25-01113-f002" ref-type="fig">Figure 2</xref>c demonstrates the results of the laser stripe centerline extracted using Steger&#x02019;s algorithm (after optimizing the parameters), and <xref rid="sensors-25-01113-f002" ref-type="fig">Figure 2</xref>d is a local zoomed-in view of the laser stripe center extraction results. It can be clearly seen that the algorithm identifies multiple line segments, and the figure distinguishes the extracted line segments with different colors, which is again very different from the extraction result of the laser stripe centerline of the smooth hole in <xref rid="sensors-25-01113-f002" ref-type="fig">Figure 2</xref>c above, and there are many burr noises in the junction of the straight line and the circular arc. This kind of gray scale mutation easily causes Steger&#x02019;s algorithm to misidentify the pseudo-edges, which poses a challenge to the extraction of the centerline.</p><p>The inner surface of an internal gear has a complex shape and can be considered one of the representatives for the extraction of complex laser stripe centerlines. <xref rid="sensors-25-01113-f003" ref-type="fig">Figure 3</xref>a shows the grayscale image of the laser stripes on the inner surface of the internal gear. It can be observed that the laser stripe intensity distribution is uneven, with alternating light and dark areas, which significantly differs from the laser stripe grayscale image of the smooth deep hole surface. <xref rid="sensors-25-01113-f003" ref-type="fig">Figure 3</xref>b presents the 3D intensity distribution of the laser stripes on the internal gear. From the figure, it is clear that there is a significant grayscale intensity difference between the tooth crest, tooth root, and the involute surface. The laser stripe intensity is higher on the tooth crest and tooth root, while the intensity on the involute surface is relatively lower. The main reason for this phenomenon lies in the fact that the optical flux generated by the annular laser beam is uniform within a unit angle. However, within the same angular range, the length of the involute curve is greater than that of the tooth crest and tooth root curves, resulting in lower optical intensity per unit length on the involute curve, and higher intensity on the tooth crest and tooth root curves. Additionally, the variation in laser stripe grayscale intensity is influenced by factors such as surface roughness, machining tool marks, and surface reflectivity.</p><p><xref rid="sensors-25-01113-f003" ref-type="fig">Figure 3</xref>c shows the result of the laser stripe centerline extraction on the internal gear using the Steger algorithm, and <xref rid="sensors-25-01113-f003" ref-type="fig">Figure 3</xref>d provides a local zoom-in of the extracted result. It can be clearly seen that the algorithm identifies multiple line segments, distinguishing them by different colors. However, in the zoomed-in subpixel point cloud, numerous noise points can be observed, appearing as burrs or chaotic branch points. Despite this, the overall extraction effectively retains the detailed information of the internal gear contour. By manually removing the noise points, a complete contour of the internal gear can be obtained.</p><p><xref rid="sensors-25-01113-f004" ref-type="fig">Figure 4</xref>a shows the grayscale image of the laser stripes on the surface of a rectangular spline. From the image, it can be observed that the laser stripe intensity distribution is similar to that of the internal gear surface. <xref rid="sensors-25-01113-f004" ref-type="fig">Figure 4</xref>b displays the 3D intensity distribution of the laser stripes on the rectangular spline. It is clearly evident that the laser stripe intensity is higher in the arc regions, while the intensity is relatively lower in the rectangular keyway areas. This phenomenon is similar to the behavior observed in the internal gear laser stripes and is mainly influenced by a combination of factors, including the inner surface profile shape, surface roughness, machining tool marks, and surface reflectivity. <xref rid="sensors-25-01113-f004" ref-type="fig">Figure 4</xref>c presents the result of laser stripe centerline extraction on the rectangular spline using the Steger algorithm, and <xref rid="sensors-25-01113-f004" ref-type="fig">Figure 4</xref>d shows a local zoom-in of the extracted result. From the images, it is clear that the algorithm successfully identifies multiple line segments, with different colors representing different segments. However, in the zoomed-in subpixel point cloud, a large number of noise points can be observed, which primarily appear as burrs or chaotic branch points. Despite this, the overall extraction process retains the detailed information of the rectangular spline contour quite effectively.</p><p><xref rid="sensors-25-01113-f005" ref-type="fig">Figure 5</xref> presents the feature analysis and geometric centerline extraction results of the internal octagonal laser stripe image. <xref rid="sensors-25-01113-f005" ref-type="fig">Figure 5</xref>a shows the grayscale image of the internal octagonal laser stripes, where the stripe distribution exhibits a regular octagonal pattern, with the stripe intensity more evenly distributed along the edges. <xref rid="sensors-25-01113-f005" ref-type="fig">Figure 5</xref>b presents the 3D grayscale distribution of the image. From this, it is clear that the grayscale intensity along one edge of the octagon is higher, while the intensity along the other edges remains relatively uniform, with a few points showing sudden intensity changes. This distribution characteristic is closely related to surface shape, roughness, machining tool marks, and reflective properties. <xref rid="sensors-25-01113-f005" ref-type="fig">Figure 5</xref>c displays the result of the geometric centerline extraction of the internal octagonal laser stripes based on the Steger algorithm, with different colors used to distinguish each line segment. The overall extraction result is relatively clear. <xref rid="sensors-25-01113-f005" ref-type="fig">Figure 5</xref>d provides a local zoom-in of the extracted result, where the details of the local line segments can be clearly observed. However, at the junctions of these segments, multiple line segments are identified, with numerous noise points and chaotic branches appearing.</p><p>In summary, for images with simple shape contours and uniform energy intensity distribution, the mature Steger algorithm can accurately extract the centerline of the laser stripes. However, when dealing with complex laser stripe images, the limitations of this algorithm become apparent. These parts, due to their complex inner surface shapes, varying roughness, random machining tool marks, diverse reflective properties, and the combined effects of laser and sensor system errors, result in low-frequency random noise signals in the subpixel contour point cloud data extracted by the Steger algorithm. This error is difficult to fully eliminate. The main issues with the Steger algorithm in complex laser stripe centerline extraction are as follows.</p><p>Shape Complexity: Complex contours, such as petals, internal gears, rectangular splines, etc., include multiple concave and convex structures, and the internal structural differences may lead to uneven illumination, thereby reducing image contrast and significantly affecting the extraction results.</p><p>Noise Interference: Reflections from the metal surface or machining marks are easily misidentified as edges, increasing the difficulty and complexity of stripe recognition.</p><p>Curvature Variation: At the junctions of straight lines with straight lines, straight lines with arcs, or arcs with arcs, large curvature variations are common, which can cause discontinuities in the extraction results or generate spurious edges.</p><p>Parameter Adjustment Limitations: Although the Steger operator can improve extraction results through parameter adjustments, for complex shapes such as petals, internal gears, rectangular splines, and internal octagons, relying solely on parameter adjustments cannot achieve ideal detection outcomes.</p><p>These limitations significantly affect the direct application of the Steger algorithm in complex laser stripe centerline extraction. However, from an overall extraction perspective, the Steger algorithm still retains the contour&#x02019;s detailed information relatively well. Currently, complete contours are obtained by manually removing noisy points from point cloud data containing noise, which is a time-consuming and human-influenced process. Therefore, this study proposes an efficient method to automatically extract a complete contour curve from point cloud data containing noise points, providing an effective solution for the precise extraction of complex contours.</p></sec><sec sec-type="methods" id="sec3-sensors-25-01113"><title>3. Methodology</title><p>Based on the above laser stripe extraction results, it can be seen that the Steger algorithm has some limitations in dealing with complex laser stripe centerlines. Nevertheless, it can be observed from the extraction results of the algorithm that the details of the laser stripes are still well preserved. However, the extracted sub-pixel point cloud contains a large number of noise points and cluttered branching lines. Therefore, we can post-process the sub-pixel point cloud of laser stripes extracted by Steger&#x02019;s algorithm, and use mathematical methods to quickly eliminate the noise points in the point cloud, so as to obtain accurate geometric center point cloud data. To this end, this study proposes a method combining minimum spanning tree and depth-first search for effective data processing of point clouds containing noise points.</p><sec id="sec3dot1-sensors-25-01113"><title>3.1. Minimum Spanning Tree (MST)</title><p>A minimum spanning tree is a subgraph of an undirected weighted graph that connects all vertices and has no rings, and whose total edge weights are minimized. In this study, the coordinates of a sub-pixel point cloud containing noise points can be viewed as an undirected weighted subgraph and the minimum spanning tree can be realized by the greedy Prim algorithm. The algorithm starts from a starting node and expands gradually, connecting the unvisited nodes by selecting edges with the minimum weight, where an edge is a connecting line between two points.</p><p>Mathematically, let <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be a connected undirected graph, where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the set of vertices, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the set of edges, and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the weight function for the edges. The goal is to find the minimum spanning tree <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where the edge set of <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02286;</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, satisfies the following:<list list-type="simple"><list-item><label>&#x027a2;</label><p><inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a connected and acyclic subgraph.</p></list-item><list-item><label>&#x027a2;</label><p><inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mi>V</mml:mi><mml:mo>|</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>&#x027a2;</label><p>The total weight <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) is minimized.</p></list-item></list>
</p><p>The steps for generating the MST are as follows.</p><list list-type="simple"><list-item><p>Input variables:</p></list-item></list><list list-type="bullet"><list-item><p>Select an initial vertex <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Define the set <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to represent the visited vertices.</p></list-item><list-item><p>Define the set <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02205;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent the edges of the spanning tree.</p></list-item></list><list list-type="simple"><list-item><p>Algorithm body:</p></list-item></list><list list-type="bullet"><list-item><p>While <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, repeat the following steps:</p></list-item><list-item><p>Choose an edge <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in E such that <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>V</mml:mi><mml:mo>&#x02216;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is minimal.</p></list-item><list-item><p>Add edge <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, i.e., <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x0222a;</mml:mo><mml:mo>{</mml:mo><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Add vertex <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to the set <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, i.e., <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x0222a;</mml:mo><mml:mo>{</mml:mo><mml:mi>v</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list><list list-type="simple"><list-item><p>Output variables:</p></list-item></list><list list-type="bullet"><list-item><p>When <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the minimum spanning tree.</p></list-item><list-item><p>The total weight is <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list><p>As shown in <xref rid="sensors-25-01113-f006" ref-type="fig">Figure 6</xref>, four sets of random points are generated in a 2D plane, each consisting of 9 points labeled: A, B, C, D, E, F, G, H, and I. The Euclidean distance is used to calculate the distance between each pair of points, and the numbers on one side of each line represent the edge weights. This forms a weighted connected graph. The Prim algorithm [<xref rid="B33-sensors-25-01113" ref-type="bibr">33</xref>] is then used to construct the MST from the weighted connected graph. The red line segments in the figure represent the MST, where all vertices are connected, and the total edge length between all points is minimized. The process involves gradually selecting the edge with the smallest weight at each step, ensuring that each connected vertex is part of the MST.</p></sec><sec id="sec3dot2-sensors-25-01113"><title>3.2. Depth-First Search (DFS)</title><p>Depth-first search (DFS) is an algorithm for traversing a search tree or graph [<xref rid="B34-sensors-25-01113" ref-type="bibr">34</xref>]. The algorithm starts from a starting node and visits nodes as deeply as possible along a path until it reaches a node where it is not possible to proceed further, and then backtracks to the previous node to continue exploring other paths. This process is carried out recursively by visiting deeper and deeper sub-nodes until the entire sub-graph has been traversed. Since MSTs are acyclic connected trees where the path from one node to another is unique, DFS can be used to find the path from the starting node to the terminating node in a minimum spanning tree. As shown in <xref rid="sensors-25-01113-f007" ref-type="fig">Figure 7</xref>, the maximum and minimum value points in the <italic toggle="yes">Y</italic>-axis direction are selected as the start and termination points in the MST, and the unique path from the start point to the termination point found using DFS is shown in <xref rid="sensors-25-01113-f007" ref-type="fig">Figure 7</xref>, where the green line segment indicates the path searched by the DFS&#x02019;s in the MTS, which demonstrates its unique advantage in exploring complex structures.</p><p>In this study, we employ a MST to construct the basic connectivity structure of the point cloud data. The acyclic nature of the MST ensures the existence of unique paths between points. Based on this, we apply a DFS algorithm to traverse the entire tree structure from a specified start node until reaching a predetermined termination node to identify and eliminate points that do not match the expected path. Through four sets of random cases, we thoroughly explore a path search method that combines the principles of minimum spanning tree (MST) and depth-first search (DFS). Simulation results indicate that this method can effectively remove spurious and cluttered branch interference points from point cloud data containing noise points and successfully identify the unique path between the start and end points. Therefore, this method is suitable for extracting the centerline of complex laser stripes.</p></sec></sec><sec id="sec4-sensors-25-01113"><title>4. Experiment and Analysis</title><p>In <xref rid="sec3-sensors-25-01113" ref-type="sec">Section 3</xref> of this paper, we detailed a complex laser stripe centerline extraction method based on the minimum spanning tree (MST) and depth-first search (DFS). This method was initially verified for feasibility using four sets of random data. In this section, we apply the proposed algorithm to extract laser stripe centerlines for complex shapes, including petal-shaped, internal gear, rectangular spline, and internal octagonal patterns. To better present the results, this study uses a segmented approach to display the extraction outcomes. The specific steps of the overall algorithm are as follows:<list list-type="simple"><list-item><label>&#x027a2;</label><p>Convert the sub-pixel point cloud coordinates to relative positions with respect to a specific origin and calculate the polar coordinates (angle) for each point.</p></list-item><list-item><label>&#x027a2;</label><p>Divide the angle range into multiple segments (for example, each 45&#x000b0; as one segment) for segmented processing.</p></list-item><list-item><label>&#x027a2;</label><p>For each segment, extract the points within the specified angle range and compute the Euclidean distance between each pair of points to construct the distance matrix.</p></list-item><list-item><label>&#x027a2;</label><p>Use the Prim algorithm to construct the MST from the distance matrix, ensuring that within each angular segment, all points are connected with the minimum total edge length.</p></list-item><list-item><label>&#x027a2;</label><p>For each angular segment, select the starting and ending points: the point closest to the starting angle of the segment is chosen as the starting point. This is performed by calculating the absolute difference between each point&#x02019;s angle and the starting angle and selecting the point with the smallest difference as the starting point. Similarly, the point closest to the ending angle is selected as the ending point.</p></list-item><list-item><label>&#x027a2;</label><p>Find the path from the start point to the end point on a minimal spanning tree using DFS and visualize it.</p></list-item></list></p><p>By following these steps, the extracted complex laser stripe centerlines for different shapes are obtained, as shown in <xref rid="sensors-25-01113-f008" ref-type="fig">Figure 8</xref>, <xref rid="sensors-25-01113-f009" ref-type="fig">Figure 9</xref>, <xref rid="sensors-25-01113-f010" ref-type="fig">Figure 10</xref> and <xref rid="sensors-25-01113-f011" ref-type="fig">Figure 11</xref>. To better demonstrate the advantages of the MST and DFS-based method in extracting complex laser stripe centerlines, we compare the extraction results with the original point cloud coordinates. The blue area in the figure shows the subpixel point cloud data extracted using the Steger algorithm, which contains a significant amount of burrs and scattered branch noise points. The red area, on the other hand, represents the laser stripe center coordinates automatically extracted using the MTS and DFS algorithms. This indicates that the proposed algorithm can effectively remove noise and form an accurate and complete contour. After performing multiple experiments on 32 different shape samples, the results indicate that the proposed MST and DFS-based method excels in extracting centerlines from complex laser stripes. This method can effectively remove spurious and erratic branch points from point clouds containing noise, ensuring that the extracted centerlines are coherent and smooth. These results validate the stability and practicality of the algorithm.</p><list list-type="simple"><list-item><p>Dice Similarity Coefficient (DSC):</p></list-item></list><p>The dice similarity coefficient (DSC) is a statistical measure used to assess the similarity between two sample sets [<xref rid="B35-sensors-25-01113" ref-type="bibr">35</xref>]. Its value ranges from 0 to 1, where 1 indicates perfect overlap and 0 indicates no intersection. In this study, the DSC is used to compare the overlap between the centerline extraction results based on MST and DFS with the manually extracted results. The formula is as follows:<disp-formula id="FD1-sensors-25-01113"><label>(1)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x02229;</mml:mo><mml:mi mathvariant="normal">Y</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="normal">X</mml:mi><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="normal">Y</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In Formula (1), X and Y represent the pixel sets of the automated segmentation and the manual segmentation, respectively.</p><list list-type="simple"><list-item><p>Hausdorff Distance (HD):</p></list-item></list><p>The Hausdorff distance (HD) is a metric used to measure the maximum distance between two sets. It quantifies the greatest deviation between the boundaries of the two sets. In this study, the HD is used to evaluate the maximum distance between the boundary of the laser stripe centerline extracted using the MST and DFS methods and the boundary of the manually extracted centerline. It is defined as<disp-formula id="FD2-sensors-25-01113"><label>(2)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:munder><mml:mo>&#x000a0;</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="normal">Y</mml:mi></mml:mrow></mml:munder><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="normal">Y</mml:mi></mml:mrow></mml:munder><mml:mo>&#x000a0;</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:munder><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In Formula (2), d(x, y) is the Euclidean distance between points x and y, and sup and inf represent the supremum and infimum, respectively.</p><p>This study evaluated the performance of laser stripe centerline extraction for four different test objects: petal shape, internal gear, spline, and internal octagon. <xref rid="sensors-25-01113-f012" ref-type="fig">Figure 12</xref> shows the laser stripe centerline extraction results based on MTS and DFS methods, alongside the manually extracted results. The first row presents the automatic extraction results using MTS and DFS, while the second row shows the manually extracted results. As can be seen from the figure, the extraction results based on the MTS and DFS methods are highly similar to the manually extracted results. This similarity is further demonstrated in the evaluation using the DSC and HD, with the specific values shown in <xref rid="sensors-25-01113-t001" ref-type="table">Table 1</xref>.</p><p>The evaluation metrics indicate that the DSC values for all test objects are very close to 1, with specific values of 0.9986 for petal shape, 0.9987 for internal gear, 0.9953 for spline, and 0.9992 for internal octagon. These results demonstrate that the overlap between the laser stripe centerline extraction results based on MTS and DFS methods and the manually extracted results is extremely high, almost identical. This high consistency reflects the universality and accuracy of the algorithm across different shapes. The maximum HD value is 3.3821 pixels for the petal shape, with other objects showing HD values of 1.6414 for internal gear, 2.0000 for spline, and 0.9653 for internal octagon. These results suggest that the maximum boundary deviation between the MTS and DFS extraction results and the manually extracted results is minimal, confirming the effectiveness of the algorithm in precise localization and boundary preservation.</p><p>Combining the results of DSC and HD, it can be concluded that the MTS and DFS-based laser stripe centerline method demonstrates extremely high accuracy and consistency across test objects with various shapes and sizes. The DSC values close to 1 indicate that the extraction results are almost identical to the manual reference, while the low HD values further validate the algorithm&#x02019;s excellent performance in boundary accuracy. Together, these metrics demonstrate the good adaptability and robustness of the proposed method for center extraction of complex laser fringes.</p></sec><sec sec-type="discussion" id="sec5-sensors-25-01113"><title>5. Discussion and Conclusions</title><p>In this study, we approach the problem of laser stripe centerline extraction from the perspective of path planning and propose a mathematical method based on minimum spanning tree (MTS) and depth-first search (DFS). The advantage of this method lies in its ability to search and track the main contour along the target area in a point cloud model containing noise, effectively eliminating spurious, disordered branches of interference data, and accurately identifying the geometric coordinates of the laser stripe center.</p><p>To validate this method, we employed a deep hole detection device developed in our previous work to collect laser stripe images from the inner walls of several different complex deep hole parts (such as petal-shaped, internal gears, rectangular splines, and internal octagons), and performed centerline extraction of the complex laser stripes. The similarity and differences between the laser stripe extraction results based on MTS and DFS and those from manual extraction were evaluated using dice similarity coefficient (DSC) and Hausdorff distance (HD). The experimental results show that the point cloud post-processing method based on MTS and DFS achieves a DSC value close to 1 and a maximum Hausdorff distance of 3.3821 pixels, which is highly similar to the manually selected point cloud data. Compared with other methods, the crack propagation method [<xref rid="B36-sensors-25-01113" ref-type="bibr">36</xref>] shows an average DSC and HD of 0.9387 and 1.6290 mm, respectively; &#x0201c;Real-Time Ultrasound Segmentation&#x0201d; [<xref rid="B37-sensors-25-01113" ref-type="bibr">37</xref>] has an average DSC and HD of 0.9191 and 6.4700 mm; and &#x0201c;Knowledge-based Grouping Adaptation and New Step-wise Registration with Discrete Cosines&#x0201d; [<xref rid="B38-sensors-25-01113" ref-type="bibr">38</xref>] has an average DSC of 0.8500. These results indicate that the laser stripe extraction method based on MTS and DFS outperforms others in terms of DSC and HD, exhibiting higher precision and better performance, thus fully demonstrating its robustness and adaptability in complex laser stripe centerline extraction.</p><p>This paper addresses the challenges of extracting the laser stripe centerline in complex deep hole inner cavities and proposes an innovative method based on the minimum spanning tree (MST) and depth first search (DFS). By combining the point cloud connectivity construction capability of MST and the path search advantage of DFS, the method effectively eliminates noise points and scattered branches, achieving the high-precision extraction of laser stripe centerlines in complex shapes. This study not only provides new technological means for high-precision measurement of complex deep hole inner cavities but also offers valuable reference for image processing and 3D reconstruction tasks in related fields. Future work should further explore the potential application of this method using larger-scale data and in more complex environments, as well as its integration with other advanced technologies to further enhance its performance and applicability.</p></sec></body><back><ack><title>Acknowledgments</title><p>All authors have read and agreed to the published version of the manuscript. The authors would like to sincerely thank all other members of the research team for their contributions to this research.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, D.Y. and H.D.; methodology, H.D.; software, X.Z. and H.D.; validation, D.Y., H.D. and Z.Z.; formal analysis, X.Z. and D.Y.; investigation, H.D., Z.Z. and D.Y.; resources, D.Y.; data curation, Z.Z. and H.D.; writing&#x02014;original draft preparation, H.D.; writing&#x02014;review and editing, H.D., D.Y., X.Z. and Z.Z.; visualization, H.D. and X.Z.; project administration, D.Y.; funding acquisition, D.Y. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>The authors declare that they consent to the publications of this paper.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01113"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>X.</given-names></name>
<name><surname>Wu</surname><given-names>B.</given-names></name>
</person-group><article-title>Algorithm for real-time defect detection of micro pipe inner surface</article-title><source>Appl. Opt.</source><year>2021</year><volume>60</volume><fpage>9167</fpage><lpage>9179</lpage><pub-id pub-id-type="doi">10.1364/AO.438287</pub-id><pub-id pub-id-type="pmid">34623999</pub-id>
</element-citation></ref><ref id="B2-sensors-25-01113"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yoshizawa</surname><given-names>T.</given-names></name>
<name><surname>Wakayama</surname><given-names>T.</given-names></name>
<name><surname>Kamakura</surname><given-names>Y.</given-names></name>
</person-group><article-title>Development of a probe for inner profile measurement and flaw detection</article-title><source>Proc. SPIE</source><year>2011</year><volume>8133</volume><fpage>81330D-1</fpage><lpage>81330D-6</lpage><pub-id pub-id-type="doi">10.1117/12.893463</pub-id></element-citation></ref><ref id="B3-sensors-25-01113"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wakayama</surname><given-names>T.</given-names></name>
<name><surname>Yoshizawa</surname><given-names>T.</given-names></name>
</person-group><article-title>Optical center alignment technique based on inner profile measurement method</article-title><source>Proc. SPIE</source><year>2014</year><volume>9110</volume><fpage>91100I-1</fpage><lpage>91100I-6</lpage><pub-id pub-id-type="doi">10.1117/12.2050892</pub-id></element-citation></ref><ref id="B4-sensors-25-01113"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wakayama</surname><given-names>T.</given-names></name>
<name><surname>Machi</surname><given-names>K.</given-names></name>
<name><surname>Yoshizawa</surname><given-names>T.</given-names></name>
</person-group><article-title>Small size probe for inner profile measurement of pipes using optical fiber ring beam device</article-title><source>Proc. SPIE</source><year>2012</year><volume>8563</volume><fpage>85630L-1</fpage><lpage>85630L-7</lpage><pub-id pub-id-type="doi">10.1117/12.999878</pub-id></element-citation></ref><ref id="B5-sensors-25-01113"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yoshizawa</surname><given-names>T.</given-names></name>
<name><surname>Wakayama</surname><given-names>T.</given-names></name>
</person-group><article-title>State of the art of compact optical 3D profile measurement apparatuses: From outer surface to inner surface measurement</article-title><source>Proc. SPIE</source><year>2013</year><volume>8769</volume><fpage>87691G-1</fpage><lpage>87691G-8</lpage><pub-id pub-id-type="doi">10.1117/12.2021094</pub-id></element-citation></ref><ref id="B6-sensors-25-01113"><label>6.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Beng</surname><given-names>W.J.W.</given-names></name>
</person-group><source>Process and Machine Improvements and Process Condition Monitoring for a Deep-Hole Internal Milling Machine</source><publisher-name>The University of Manchester</publisher-name><publisher-loc>Manchester, UK</publisher-loc><year>2017</year></element-citation></ref><ref id="B7-sensors-25-01113"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Steger</surname><given-names>C.</given-names></name>
</person-group><article-title>An Unbiased Detector of Curvilinear Structures</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>1998</year><volume>20</volume><fpage>113</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1109/34.659930</pub-id></element-citation></ref><ref id="B8-sensors-25-01113"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Steger</surname><given-names>C.</given-names></name>
</person-group><article-title>Extraction of curved lines from images</article-title><source>Proceedings of the 13th International Conference on Pattern Recognition</source><conf-loc>Vienna, Austria</conf-loc><conf-date>25&#x02013;29 August 1996</conf-date><volume>Volume 2</volume><fpage>251</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1109/ICPR.1996.546827</pub-id></element-citation></ref><ref id="B9-sensors-25-01113"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Steger</surname><given-names>C.</given-names></name>
</person-group><article-title>Extracting curvilinear structures: A differential geometric approach</article-title><source>Proceedings of the Computer Vision&#x02014;ECCV &#x02018;96&#x02019;</source><conf-loc>Cambridge, UK</conf-loc><conf-date>14&#x02013;18 April 1996</conf-date><volume>Volume 1064</volume><fpage>630</fpage><lpage>641</lpage><pub-id pub-id-type="doi">10.1007/BFb0015573</pub-id></element-citation></ref><ref id="B10-sensors-25-01113"><label>10.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Abidin</surname><given-names>Z.Z.</given-names></name>
<name><surname>Asmai</surname><given-names>S.A.</given-names></name>
<name><surname>Abas</surname><given-names>Z.A.</given-names></name>
<name><surname>Zakaria</surname><given-names>N.A.</given-names></name>
<name><surname>Ibrahim</surname><given-names>S.N.</given-names></name>
</person-group><article-title>Development of Edge Detection for Image Segmentation</article-title><source>IOP Conference Series: Materials Science and Engineering, Proceedings of the 2nd Joint Conference on Green Engineering Technology &#x00026; Applied Computing 2020, Bangkok, Thailand, 4&#x02013;5 February 2020</source><publisher-name>IOP Publishing</publisher-name><publisher-loc>Bristol, UK</publisher-loc><year>2020</year><volume>Volume 864</volume><fpage>012058</fpage><pub-id pub-id-type="doi">10.1088/1757-899x/864/1/012058</pub-id></element-citation></ref><ref id="B11-sensors-25-01113"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bo</surname><given-names>Q.</given-names></name>
<name><surname>Hou</surname><given-names>B.</given-names></name>
<name><surname>Miao</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Lu</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Laser stripe center extraction method base on hessian matrix improved by stripe width precise calculation</article-title><source>Opt. Lasers Eng.</source><year>2024</year><volume>172</volume><fpage>107896</fpage><pub-id pub-id-type="doi">10.1016/j.optlaseng.2023.107896</pub-id></element-citation></ref><ref id="B12-sensors-25-01113"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Ma</surname><given-names>L.</given-names></name>
<name><surname>Long</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Deng</surname><given-names>H.</given-names></name>
<name><surname>Yan</surname><given-names>F.</given-names></name>
<name><surname>Gu</surname><given-names>Q.</given-names></name>
</person-group><article-title>Hardware-oriented algorithm for high-speed laser centerline extraction based on Hessian matrix</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2021</year><volume>70</volume><fpage>5010514</fpage><pub-id pub-id-type="doi">10.1109/TIM.2021.3081163</pub-id></element-citation></ref><ref id="B13-sensors-25-01113"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Zhou</surname><given-names>J.</given-names></name>
<name><surname>Huang</surname><given-names>F.</given-names></name>
<name><surname>Liu</surname><given-names>L.</given-names></name>
</person-group><article-title>Sub-pixel extraction of laser stripe center using an improved gray-gravity method</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>814</elocation-id><pub-id pub-id-type="doi">10.3390/s17040814</pub-id><pub-id pub-id-type="pmid">28394288</pub-id>
</element-citation></ref><ref id="B14-sensors-25-01113"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>P.</given-names></name>
<name><surname>Yang</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Xia</surname><given-names>H.</given-names></name>
</person-group><article-title>Accurate extraction method of multi-laser stripes for stereo-vision based handheld scanners in complex circumstances</article-title><source>Opt. Laser Technol.</source><year>2025</year><volume>181</volume><fpage>111605</fpage><pub-id pub-id-type="doi">10.1016/j.optlastec.2024.111605</pub-id></element-citation></ref><ref id="B15-sensors-25-01113"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>F.</given-names></name>
<name><surname>Ma</surname><given-names>L.</given-names></name>
</person-group><article-title>Adaptive bidirectional gray-scale center of gravity extraction algorithm of laser stripes</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>9567</elocation-id><pub-id pub-id-type="doi">10.3390/s22249567</pub-id><pub-id pub-id-type="pmid">36559947</pub-id>
</element-citation></ref><ref id="B16-sensors-25-01113"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yin</surname><given-names>X.-Q.</given-names></name>
<name><surname>Tao</surname><given-names>W.</given-names></name>
<name><surname>Feng</surname><given-names>Y.-Y.</given-names></name>
<name><surname>Gao</surname><given-names>Q.</given-names></name>
<name><surname>He</surname><given-names>Q.-Z.</given-names></name>
<name><surname>Zhao</surname><given-names>H.</given-names></name>
</person-group><article-title>Laser stripe extraction method in industrial environments utilizing self-adaptive convolution technique</article-title><source>Appl. Opt.</source><year>2017</year><volume>56</volume><fpage>2653</fpage><lpage>2660</lpage><pub-id pub-id-type="doi">10.1364/AO.56.002653</pub-id><pub-id pub-id-type="pmid">28375225</pub-id>
</element-citation></ref><ref id="B17-sensors-25-01113"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ye</surname><given-names>C.</given-names></name>
<name><surname>Feng</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>Q.</given-names></name>
<name><surname>Wang</surname><given-names>C.</given-names></name>
<name><surname>Pan</surname><given-names>B.</given-names></name>
<name><surname>Xie</surname><given-names>Y.</given-names></name>
<name><surname>Hu</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
</person-group><article-title>Laser stripe segmentation and centerline extraction based on 3D scanning imaging</article-title><source>Appl. Opt.</source><year>2022</year><volume>61</volume><fpage>5409</fpage><lpage>5418</lpage><pub-id pub-id-type="doi">10.1364/AO.457427</pub-id><pub-id pub-id-type="pmid">36256108</pub-id>
</element-citation></ref><ref id="B18-sensors-25-01113"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>H.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Diao</surname><given-names>K.</given-names></name>
<name><surname>Luo</surname><given-names>C.</given-names></name>
</person-group><article-title>An enhanced centerline extraction algorithm for complex stripes in linear laser scanning measurement</article-title><source>Precis. Eng.</source><year>2024</year><volume>91</volume><fpage>199</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1016/j.precisioneng.2024.09.006</pub-id></element-citation></ref><ref id="B19-sensors-25-01113"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Cai</surname><given-names>Z.</given-names></name>
<name><surname>Kong</surname><given-names>L.</given-names></name>
<name><surname>An</surname><given-names>H.</given-names></name>
</person-group><article-title>AG-Unet: A robust laser stripe extraction method in line-laser imaging</article-title><source>Proceedings of the Advanced Optical Manufacturing Technologies and Applications 2024; and Fourth International Forum of Young Scientists on Advanced Optical Manufacturing (AOMTA and YSAOM 2024)</source><conf-loc>Xi&#x02019;an, China</conf-loc><conf-date>5&#x02013;7 July 2024</conf-date><publisher-name>SPIE</publisher-name><publisher-loc>Bellingham, WA, USA</publisher-loc><volume>Volume 13280</volume><fpage>380</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1117/12.3048247</pub-id></element-citation></ref><ref id="B20-sensors-25-01113"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Usamentiaga</surname><given-names>R.</given-names></name>
<name><surname>Molleda</surname><given-names>J.</given-names></name>
<name><surname>Garc&#x000ed;a</surname><given-names>D.F.</given-names></name>
</person-group><article-title>Fast and robust laser stripe extraction for 3D reconstruction in industrial environments</article-title><source>Mach. Vis. Appl.</source><year>2012</year><volume>23</volume><fpage>179</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1007/s00138-010-0288-6</pub-id></element-citation></ref><ref id="B21-sensors-25-01113"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jiang</surname><given-names>H.</given-names></name>
<name><surname>Fu</surname><given-names>W.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
</person-group><article-title>Extraction of laser stripe centerlines from translucent optical components using a multi-scale attention deep neural network</article-title><source>Meas. Sci. Technol.</source><year>2024</year><volume>35</volume><fpage>085404</fpage><pub-id pub-id-type="doi">10.1088/1361-6501/ad480c</pub-id></element-citation></ref><ref id="B22-sensors-25-01113"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>H.</given-names></name>
<name><surname>Guo</surname><given-names>Y.</given-names></name>
<name><surname>Dong</surname><given-names>H.</given-names></name>
<name><surname>Zhu</surname><given-names>M.</given-names></name>
<name><surname>Wu</surname><given-names>H.</given-names></name>
<name><surname>Ke</surname><given-names>Y.</given-names></name>
</person-group><article-title>A denoising and restoration method of weld laser stripe image for robotic multi-layer multi-pass welding based on generative adversarial networks</article-title><source>J. Manuf. Process.</source><year>2025</year><volume>133</volume><fpage>1183</fpage><lpage>1195</lpage><pub-id pub-id-type="doi">10.1016/j.jmapro.2024.12.001</pub-id></element-citation></ref><ref id="B23-sensors-25-01113"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xiao</surname><given-names>R.</given-names></name>
<name><surname>Cao</surname><given-names>Q.</given-names></name>
<name><surname>Chen</surname><given-names>S.</given-names></name>
</person-group><article-title>A novel laser stripe key point tracker based on self-supervised learning and improved KCF for robotic welding seam tracking</article-title><source>J. Manuf. Process.</source><year>2024</year><volume>127</volume><fpage>660</fpage><lpage>670</lpage><pub-id pub-id-type="doi">10.1016/j.jmapro.2024.07.140</pub-id></element-citation></ref><ref id="B24-sensors-25-01113"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Brenner</surname><given-names>M.</given-names></name>
<name><surname>Reyes</surname><given-names>N.H.</given-names></name>
<name><surname>Susnjak</surname><given-names>T.</given-names></name>
<name><surname>Barczak</surname><given-names>A.L.C.</given-names></name>
</person-group><article-title>RGB-D and thermal sensor fusion: A systematic literature review</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>82410</fpage><lpage>82442</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3301119</pub-id></element-citation></ref><ref id="B25-sensors-25-01113"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Guo</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>D.</given-names></name>
<name><surname>Tang</surname><given-names>B.</given-names></name>
<name><surname>Luo</surname><given-names>B.</given-names></name>
</person-group><article-title>Multi-sensors image fusion method for non-destructive inspection in vertical-cavity surface-emitting lasers</article-title><source>Int. J. Mach. Learn. Cybern.</source><year>2024</year><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1007/s13042-024-02464-1</pub-id></element-citation></ref><ref id="B26-sensors-25-01113"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>L.</given-names></name>
<name><surname>Xu</surname><given-names>P.</given-names></name>
</person-group><article-title>Visual Sensing and Depth Perception for Welding Robots and Their Industrial Applications</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>9700</elocation-id><pub-id pub-id-type="doi">10.3390/s23249700</pub-id><pub-id pub-id-type="pmid">38139548</pub-id>
</element-citation></ref><ref id="B27-sensors-25-01113"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tian</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>L.</given-names></name>
<name><surname>Yuan</surname><given-names>G.</given-names></name>
<name><surname>Feng</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
</person-group><article-title>Automatic identification of multi-type weld seam based on vision sensor with silhouette-mapping</article-title><source>IEEE Sens. J.</source><year>2020</year><volume>21</volume><fpage>5402</fpage><lpage>5412</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2020.3034382</pub-id></element-citation></ref><ref id="B28-sensors-25-01113"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Abzal</surname><given-names>A.</given-names></name>
<name><surname>Saadatseresht</surname><given-names>M.</given-names></name>
<name><surname>Varshosaz</surname><given-names>M.</given-names></name>
<name><surname>Remondino</surname><given-names>F.</given-names></name>
</person-group><article-title>Development of an automatic map drawing system for ancient bas-reliefs</article-title><source>J. Cult. Herit.</source><year>2020</year><volume>45</volume><fpage>204</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1016/j.culher.2020.03.009</pub-id></element-citation></ref><ref id="B29-sensors-25-01113"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wan</surname><given-names>M.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Zhao</surname><given-names>H.</given-names></name>
<name><surname>Jia</surname><given-names>H.</given-names></name>
<name><surname>Yu</surname><given-names>L.</given-names></name>
</person-group><article-title>Robust and accurate sub-pixel extraction method of laser stripes in complex circumstances</article-title><source>Appl. Opt.</source><year>2021</year><volume>60</volume><fpage>11196</fpage><lpage>11204</lpage><pub-id pub-id-type="doi">10.1364/AO.444730</pub-id><pub-id pub-id-type="pmid">35201108</pub-id>
</element-citation></ref><ref id="B30-sensors-25-01113"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sheng</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>Fast and accurate centerline extraction algorithm for a laser stripe applied for shoe outsole inspection</article-title><source>Appl. Opt.</source><year>2023</year><volume>62</volume><fpage>314</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1364/AO.476939</pub-id><pub-id pub-id-type="pmid">36630229</pub-id>
</element-citation></ref><ref id="B31-sensors-25-01113"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Du</surname><given-names>H.</given-names></name>
<name><surname>Zhao</surname><given-names>X.</given-names></name>
<name><surname>Yu</surname><given-names>D.</given-names></name>
<name><surname>Shi</surname><given-names>H.</given-names></name>
<name><surname>Zhou</surname><given-names>Z.</given-names></name>
</person-group><article-title>Research on Point Cloud Acquisition and Calibration of Deep Hole Inner Surfaces Based on Collimated Ring Laser Beams</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>5790</elocation-id><pub-id pub-id-type="doi">10.3390/s24175790</pub-id><pub-id pub-id-type="pmid">39275701</pub-id>
</element-citation></ref><ref id="B32-sensors-25-01113"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>X.</given-names></name>
<name><surname>Du</surname><given-names>H.</given-names></name>
<name><surname>Yu</surname><given-names>D.</given-names></name>
</person-group><article-title>Improving Measurement Accuracy of Deep Hole Measurement Instruments through Perspective Transformation</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>3158</elocation-id><pub-id pub-id-type="doi">10.3390/s24103158</pub-id><pub-id pub-id-type="pmid">38794012</pub-id>
</element-citation></ref><ref id="B33-sensors-25-01113"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>J.</given-names></name>
</person-group><article-title>The analysis and application of Prim algorithm, Kruskal algorithm, Boruvka algorithm</article-title><source>Appl. Comput. Eng.</source><year>2023</year><volume>19</volume><fpage>84</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.54254/2755-2721/19/20231012</pub-id></element-citation></ref><ref id="B34-sensors-25-01113"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lammich</surname><given-names>P.</given-names></name>
<name><surname>Neumann</surname><given-names>R.</given-names></name>
</person-group><article-title>A framework for verifying depth-first search algorithms</article-title><source>Proceedings of the 2015 Conference on Certified Programs and Proofs</source><conf-loc>Mumbai, India</conf-loc><conf-date>13&#x02013;14 January 2015</conf-date><fpage>137</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1145/2676724.2693165</pub-id></element-citation></ref><ref id="B35-sensors-25-01113"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yeap</surname><given-names>P.L.</given-names></name>
<name><surname>Wong</surname><given-names>Y.M.</given-names></name>
<name><surname>Ong</surname><given-names>A.L.K.</given-names></name>
<name><surname>Tuan</surname><given-names>J.K.L.</given-names></name>
<name><surname>Pang</surname><given-names>E.P.P.</given-names></name>
<name><surname>Park</surname><given-names>S.Y.</given-names></name>
<name><surname>Lee</surname><given-names>J.C.L.</given-names></name>
<name><surname>Tan</surname><given-names>H.Q.</given-names></name>
</person-group><article-title>Predicting dice similarity coefficient of deformably registered contours using Siamese neural network</article-title><source>Phys. Med. Biol.</source><year>2023</year><volume>68</volume><elocation-id>155016</elocation-id><pub-id pub-id-type="doi">10.1088/1361-6560/ace6f0</pub-id></element-citation></ref><ref id="B36-sensors-25-01113"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>Y.</given-names></name>
<name><surname>Hu</surname><given-names>G.</given-names></name>
<name><surname>Ji</surname><given-names>C.</given-names></name>
<name><surname>Xiong</surname><given-names>H.</given-names></name>
</person-group><article-title>Glass-cutting medical images via a mechanical image segmentation method based on crack propagation</article-title><source>Nat. Commun.</source><year>2020</year><volume>11</volume><fpage>5669</fpage><pub-id pub-id-type="doi">10.1038/s41467-020-19392-7</pub-id><pub-id pub-id-type="pmid">33168802</pub-id>
</element-citation></ref><ref id="B37-sensors-25-01113"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cunningham</surname><given-names>R.J.</given-names></name>
<name><surname>Harding</surname><given-names>P.J.</given-names></name>
<name><surname>Loram</surname><given-names>I.D.</given-names></name>
</person-group><article-title>Real-time ultrasound segmentation, analysis and visualisation of deep cervical muscle structure</article-title><source>IEEE Trans. Med. Imaging</source><year>2016</year><volume>36</volume><fpage>653</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1109/TMI.2016.2623819</pub-id><pub-id pub-id-type="pmid">27831867</pub-id>
</element-citation></ref><ref id="B38-sensors-25-01113"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Suman</surname><given-names>A.A.</given-names></name>
<name><surname>Aktar</surname><given-names>M.N.</given-names></name>
<name><surname>Asikuzzaman</surname><given-names>M.</given-names></name>
<name><surname>Webb</surname><given-names>A.L.</given-names></name>
<name><surname>Perriman</surname><given-names>D.M.</given-names></name>
<name><surname>Pickering</surname><given-names>M.R.</given-names></name>
</person-group><article-title>Segmentation and reconstruction of cervical muscles using knowledge-based grouping adaptation and new step-wise registration with discrete cosines</article-title><source>Comput. Methods Biomech. Biomed. Eng. Imaging Vis.</source><year>2019</year><volume>7</volume><fpage>12</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1080/21681163.2017.1356751</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01113-f001"><label>Figure 1</label><caption><p>Analysis of laser stripe image features and geometric centerline extraction in a smooth deep hole. (<bold>a</bold>) Contour image of the laser stripe in a smooth deep hole; (<bold>b</bold>) 3D distribution of grayscale intensity in the laser stripe image of a smooth deep hole; (<bold>c</bold>) extracted laser stripe centerline in a smooth deep hole using the Steger algorithm; (<bold>d</bold>) locally magnified view of the extracted laser stripe centerline in a smooth deep hole.</p></caption><graphic xlink:href="sensors-25-01113-g001" position="float"/></fig><fig position="float" id="sensors-25-01113-f002"><label>Figure 2</label><caption><p>Analysis of laser stripe image features and geometric centerline extraction in petal-shaped structures. (<bold>a</bold>) Contour image of the laser stripe in a petal-shaped structure; (<bold>b</bold>) 3D distribution of grayscale intensity in the laser stripe image of a petal-shaped structure; (<bold>c</bold>) extracted laser stripe centerline in a petal-shaped structure using the Steger algorithm; (<bold>d</bold>) locally magnified view of the extracted laser stripe centerline in a petal-shaped structure.</p></caption><graphic xlink:href="sensors-25-01113-g002" position="float"/></fig><fig position="float" id="sensors-25-01113-f003"><label>Figure 3</label><caption><p>Analysis of laser stripe image features and geometric centerline extraction in internal gears. (<bold>a</bold>) Contour image of the laser stripe in an internal gear; (<bold>b</bold>) 3D distribution of grayscale intensity in the laser stripe image of an internal gear; (<bold>c</bold>) extracted laser stripe centerline in an internal gear using the Steger algorithm; (<bold>d</bold>) locally magnified view of the extracted laser stripe centerline in an internal gear.</p></caption><graphic xlink:href="sensors-25-01113-g003" position="float"/></fig><fig position="float" id="sensors-25-01113-f004"><label>Figure 4</label><caption><p>Analysis of laser stripe image features and geometric centerline extraction in rectangular splines. (<bold>a</bold>) Contour image of the laser stripe in a rectangular spline; (<bold>b</bold>) 3D distribution of grayscale intensity in the laser stripe image of a rectangular spline; (<bold>c</bold>) extracted laser stripe centerline in a rectangular spline using the Steger algorithm; (<bold>d</bold>) locally magnified view of the extracted laser stripe centerline in a rectangular spline.</p></caption><graphic xlink:href="sensors-25-01113-g004" position="float"/></fig><fig position="float" id="sensors-25-01113-f005"><label>Figure 5</label><caption><p>Analysis of laser stripe image features and geometric centerline extraction in internal octagons. (<bold>a</bold>) Contour image of the laser stripe in an internal octagon; (<bold>b</bold>) 3D distribution of grayscale intensity in the laser stripe image of an internal octagon; (<bold>c</bold>) extracted laser stripe centerline in an internal octagon using the Steger algorithm; (<bold>d</bold>) locally magnified view of the extracted laser stripe centerline in an internal octagon.</p></caption><graphic xlink:href="sensors-25-01113-g005" position="float"/></fig><fig position="float" id="sensors-25-01113-f006"><label>Figure 6</label><caption><p>Minimum spanning tree generation example.</p></caption><graphic xlink:href="sensors-25-01113-g006" position="float"/></fig><fig position="float" id="sensors-25-01113-f007"><label>Figure 7</label><caption><p>DFS path search example in MST.</p></caption><graphic xlink:href="sensors-25-01113-g007" position="float"/></fig><fig position="float" id="sensors-25-01113-f008"><label>Figure 8</label><caption><p>Extraction results of the petal-shaped laser stripe centerline.</p></caption><graphic xlink:href="sensors-25-01113-g008" position="float"/></fig><fig position="float" id="sensors-25-01113-f009"><label>Figure 9</label><caption><p>Extraction results of the internal gear laser stripe centerline.</p></caption><graphic xlink:href="sensors-25-01113-g009a" position="float"/><graphic xlink:href="sensors-25-01113-g009b" position="float"/></fig><fig position="float" id="sensors-25-01113-f010"><label>Figure 10</label><caption><p>Extraction results of the rectangular spline laser stripe centerline.</p></caption><graphic xlink:href="sensors-25-01113-g010" position="float"/></fig><fig position="float" id="sensors-25-01113-f011"><label>Figure 11</label><caption><p>Extraction results of the internal octagonal laser stripe centerline.</p></caption><graphic xlink:href="sensors-25-01113-g011" position="float"/></fig><fig position="float" id="sensors-25-01113-f012"><label>Figure 12</label><caption><p>Laser stripe centerline extraction results based on MST and DFS compared with manual extraction results.</p></caption><graphic xlink:href="sensors-25-01113-g012" position="float"/></fig><table-wrap position="float" id="sensors-25-01113-t001"><object-id pub-id-type="pii">sensors-25-01113-t001_Table 1</object-id><label>Table 1</label><caption><p>DSC and HD numerical table of different types of laser stripes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Laser Stripe</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Petal-Shaped</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inner Gear</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Spline</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Internal Octagon</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hausdorff Distance</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.3821</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.6414</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.0000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9653</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dice Similarity Coefficient (pixel)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9986</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9987</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9953</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9992</td></tr></tbody></table></table-wrap></floats-group></article>