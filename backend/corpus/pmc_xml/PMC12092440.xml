<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40400750</article-id><article-id pub-id-type="pmc">PMC12092440</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2025.1541546</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Toronto Ethnically Diverse face database: a multi-faceted stimulus set</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Latif</surname><given-names>Menahal</given-names></name><xref rid="c001" ref-type="corresp">
<sup>*</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/2907920/overview"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Sugden</surname><given-names>Nicole</given-names></name><uri xlink:href="http://loop.frontiersin.org/people/210694/overview"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>O'Hagan</surname><given-names>Maire L.</given-names></name><uri xlink:href="http://loop.frontiersin.org/people/3033517/overview"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Moulson</surname><given-names>Margaret C.</given-names></name><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib></contrib-group><aff><institution>Department of Psychology, Toronto Metropolitan University</institution>, <addr-line>Toronto, ON</addr-line>, <country>Canada</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Birgitta Dresp-Langley, Centre National de la Recherche Scientifique (CNRS), France</p></fn><fn fn-type="edited-by"><p>Reviewed by: Jessie J. Peissig, California State University, Fullerton, United States</p><p>Gijs Bijlstra, Radboud University, Netherlands</p></fn><corresp id="c001">*Correspondence: Menahal Latif <email>menahal.latif@torontomu.ca</email></corresp></author-notes><pub-date pub-type="epub"><day>07</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>16</volume><elocation-id>1541546</elocation-id><history><date date-type="received"><day>08</day><month>12</month><year>2024</year></date><date date-type="accepted"><day>11</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025 Latif, Sugden, O'Hagan and Moulson.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Latif, Sugden, O'Hagan and Moulson</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>Face stimuli are often used in psychological and neuroimaging research to assess perceptual, cognitive, social, and emotional processes. Many available face databases, however, have limited diversity in ethnicity, emotional expression, gaze direction, and/or pose, which constrains their utility to specific contexts. Having a diverse face database can mitigate these biases and may help researchers investigate novel topics that examine the effects of ethnicity on these processes. The Toronto Ethnically Diverse (TED) face database is designed to provide an open-access set of 271 unique White, Black, East-Asian, South-Asian, South-East Asian, Middle Eastern, Multi-racial, and Indigenous adult models. The TED database includes diversity in race, gender, pose, gaze direction, and three emotion variations (neutral, open-mouth happiness, closed-mouth happiness). Validation data of the stimuli based on judgments of the emotional expressions showed high inter-rater reliability and high accuracy as measured by proportion correct and Cohen's kappa scores. Intensity, and genuineness ratings are also presented for each model. The validation results for TED suggest that this face database consists of models displaying their intended emotions with high fidelity. This database will be useful to researchers seeking to study underrepresented groups and to other broad groups of researchers who are studying face perception.</p></abstract><kwd-group><kwd>face stimuli</kwd><kwd>face perception</kwd><kwd>ethnically diverse</kwd><kwd>emotion expression</kwd><kwd>eye gaze</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that financial support was received for the research and/or publication of this article. This work was supported by the Social Sciences and Humanities Research Council (SSHRC) under the Insight Development Grant 430-2011-0344. This work was supported with a grant provided by the Office of the Dean of Arts, Toronto Metropolitan University.</funding-statement></funding-group><counts><fig-count count="4"/><table-count count="5"/><equation-count count="0"/><ref-count count="46"/><page-count count="12"/><word-count count="8775"/></counts><custom-meta-group><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Perception Science</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>Perceiving human faces is essential for supporting social interaction and maintaining social bonds. Faces convey information regarding the identity, age, sex, race, and emotional state of an individual. They are often the first visual cues available to a perceiver and signal important social information (Little et al., <xref rid="B26" ref-type="bibr">2011</xref>). Studies involving face stimuli investigate high-level vision, identity recognition, emotion perception, social categorization (i.e., age, race, and gender), and trait evaluation (i.e., attractiveness, and/or trustworthiness). Thus, numerous studies in the field of Psychology require face stimuli to investigate novel questions about human interaction. Existing face databases vary across one or more of the following dimensions: race, gender, pose, gaze direction, and emotional expression. Most of these face databases, however, offer diversity in only a few of the above dimensions, limiting researchers from investigating topics that require stimuli that intersect on many dimensions. To date, there are limited publicly available face databases with a substantial number of faces from multiple underrepresented groups coupled with diversity in pose, gaze, and expression. The current study aims to describe the development and validation of the Toronto Ethnically Diverse (TED) face database, a face stimulus set that contains variation along a number of these dimensions, which we hope will facilitate novel research on diverse populations.</p><p>Psychological experiments involving face stimuli use face databases that contain predominantly White and/or Western populations. This limits the generalizability of research findings as a growing literature shows that stimuli portraying individuals from diverse ethnic backgrounds can have a profound impact on the perceptions and actions of participants (Conley et al., <xref rid="B10" ref-type="bibr">2018</xref>; Zebrowitz et al., <xref rid="B45" ref-type="bibr">2010</xref>). To examine the perceptual, emotional, and social processes involved when perceiving faces, diverse face databases are needed that reflect the demographics of study participants. As of 2005, there were 28 databases commonly used in face perception research, but only six of them consisted of non-White faces (Gross, <xref rid="B20" ref-type="bibr">2005</xref>). Only two of these face databases consisted of models from various ethnic backgrounds, whereas the other four face databases were strictly limited to East Asian populations. Recently, more databases have been made available containing faces of models from diverse ethnic backgrounds (Coffman et al., <xref rid="B8" ref-type="bibr">2015</xref>; Conley et al., <xref rid="B10" ref-type="bibr">2018</xref>; Dalrymple et al., <xref rid="B11" ref-type="bibr">2013</xref>; Egger et al., <xref rid="B14" ref-type="bibr">2011</xref>; Langner et al., <xref rid="B25" ref-type="bibr">2010</xref>; Pickron et al., <xref rid="B34" ref-type="bibr">2024</xref>; van der Schalk et al., <xref rid="B41" ref-type="bibr">2011</xref>). For example, the racially diverse affective expression (RADIATE) face database is a database designed to reflect the racial demographics in the United States derived from the annual census (Conley et al., <xref rid="B10" ref-type="bibr">2018</xref>). This database contains facial expressions of Black, White, Hispanic, and Asian adults. The Chicago Face Database is a free resource consisting of high-resolution, standardized photographs of Black and White male and female adult models (Ma et al., <xref rid="B27" ref-type="bibr">2015</xref>). Extensions of the Chicago Face Database include CFD-MR and CFD-INDIA which consist of multiracial and Indian male and female adult models (Ma et al., <xref rid="B28" ref-type="bibr">2021</xref>; Lakshmi et al., <xref rid="B23" ref-type="bibr">2021</xref>). The American Multiracial Faces Database (AMFD) is a collection of photographs of self-reported multiracial individuals with accompanying ratings by na&#x000ef;ve observers (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>). The NimStim set of facial expressions (Tottenham et al., <xref rid="B40" ref-type="bibr">2009</xref>) and the 10k US Adult Faces Database (Bainbridge et al., <xref rid="B1" ref-type="bibr">2013</xref>) are databases that contain majority White faces with a smaller number of Black, East Asian, and Latinx faces. The Multi-Racial Mega-Resolution database (MR2) contains 74 images of men and women of European, African, and East Asian descent (Strohminger et al., <xref rid="B39" ref-type="bibr">2016</xref>).</p><p>Although there has been a positive change in the ethnic diversity in available face databases, there are still several limitations in existing face databases, which limit the research questions and populations that can be examined. Across face databases, White faces still tend to be the most well-represented; this is especially true for face databases containing variability in other characteristics (e.g., age, pose, emotional expression, gaze direction; Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>; Ebner et al., <xref rid="B13" ref-type="bibr">2010</xref>; Gross, <xref rid="B20" ref-type="bibr">2005</xref>; Langner et al., <xref rid="B25" ref-type="bibr">2010</xref>). Relatedly, even databases that contain ethnic diversity are limited in the ethnicities available. Most diverse datasets focus on East Asian and Black populations. There are very few face databases that contain images of individuals from understudied groups such as South Asian, Southeast Asian, Middle Eastern, Latinx, and multiracial (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>; Conley et al., <xref rid="B10" ref-type="bibr">2018</xref>; DeBruine and Jones, <xref rid="B12" ref-type="bibr">2017</xref>; Gross, <xref rid="B20" ref-type="bibr">2005</xref>; Pickron et al., <xref rid="B34" ref-type="bibr">2024</xref>). Due to the overrepresentation of White faces and the dominance of male faces in various datasets, only a small number of databases consist of racially diverse female faces. Such biases present in face databases can further perpetuate the social invisibility experienced by women from underrepresented groups (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>; Gross, <xref rid="B20" ref-type="bibr">2005</xref>; Neel and Lassetter, <xref rid="B32" ref-type="bibr">2019</xref>; Sesko and Biernat, <xref rid="B36" ref-type="bibr">2010</xref>). This is important as targets' race and gender interact to influence social perception and thus have implications for the generalizability of research dominated by White, male faces (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>).</p><p>Several face databases include faces portraying different facial expressions; however, many of these pose limitations due to the number of models and/or the homogeneity of race (Conley et al., <xref rid="B10" ref-type="bibr">2018</xref>). The Facial Recognition Technology (FERET) database presents variations in face pose but only consists of two facial expressions (i.e., neutral and smiling) for the majority White models (Barson, <xref rid="B3" ref-type="bibr">2003</xref>; Gross, <xref rid="B20" ref-type="bibr">2005</xref>). Several other face databases present variation in pose, expression, and other setting characteristics, but contain limited racial diversity. For example, the CAS-PEAL is a database that only contains faces of Chinese models, the CMU PIE database contains faces of majority White models, and the Chicago database only contains photographs of Black and White models (Gao et al., <xref rid="B19" ref-type="bibr">2008</xref>; Ma et al., <xref rid="B27" ref-type="bibr">2015</xref>; Sim et al., <xref rid="B37" ref-type="bibr">2002</xref>). The NIMH-ChEFS face database consists of child models portraying gaze variations (i.e., directed or averted gaze), however, the models are majority White faces (Egger et al., <xref rid="B14" ref-type="bibr">2011</xref>; Gross, <xref rid="B20" ref-type="bibr">2005</xref>). RADIATE is one of the first databases to include individuals from multiple underrepresented groups (i.e., Latinx) portraying 8 emotional expressions; however, this face database does not include variation in pose and gaze direction (Conley et al., <xref rid="B10" ref-type="bibr">2018</xref>). Although many of the above face databases present variations in model attributes, they are limited in the ethnicities represented in the database and/or variations across dimensions like emotion, pose, and eye gaze direction (see <xref rid="T1" ref-type="table">Table 1</xref> for a list of the commonly used ethnically diverse face databases and their characteristics).</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Commonly used face databases with images of non-White models available as of march 2025.</p></caption><table frame="box" rules="all"><thead><tr style="background-color:#919498;color:#ffffff"><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Database</bold>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Ethnicities</bold>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Expression</bold>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<bold>No. of expressions</bold>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Pose</bold>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Gaze</bold>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Gender</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Year</bold>
</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">The Montreal Set of Facial Displays of Emotion</td><td valign="top" align="left" rowspan="1" colspan="1">French Canadian, Chinese, and sub-Saharan African</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2000</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">CMU Pose, Illumination, and Expression (PIE) Database</td><td valign="top" align="left" rowspan="1" colspan="1">White and East Asian</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">3</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">N/A</td><td valign="top" align="center" rowspan="1" colspan="1">2000</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">The MUCT landmarked face database</td><td valign="top" align="left" rowspan="1" colspan="1">Cross-section of races (not specified)</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">2</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2008</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">NimStim</td><td valign="top" align="left" rowspan="1" colspan="1">East Asian, Black, Latinx, and White</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">8 (open-mouth variations for all except surprise)</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2009</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Radboud Faces Database (RaFD)</td><td valign="top" align="left" rowspan="1" colspan="1">White and Moroccan Dutch</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">8</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2010</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Tarr Lab Face Database</td><td valign="top" align="left" rowspan="1" colspan="1">East Asian, Black, Latinx, White, and multiracial</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">8 (emotions of mostly White and East-Asian faces)</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2012</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">The Amsterdam Dynamic Facial Expression Set (ADFES)</td><td valign="top" align="left" rowspan="1" colspan="1">White and Middle Eastern</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">10</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2012</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">10k US Adult Faces Database</td><td valign="top" align="left" rowspan="1" colspan="1">East Asian, Black, Latinx, and White</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">Not specified</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2013</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">The Chicago Face Database</td><td valign="top" align="left" rowspan="1" colspan="1">Black and White</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">1 (neutral)</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2015</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MultiRacial Mega-Resolution database (MR2)</td><td valign="top" align="left" rowspan="1" colspan="1">East Asian, Black, and White</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">1 (neutral)</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2016</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Racially Diverse Affective Expression (RADIATE)</td><td valign="top" align="left" rowspan="1" colspan="1">East Asian, Black, Latinx, and White</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">8 (open-mouth variations)</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2018</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">The Chicago Face Database Multiracial (CFD-MR)</td><td valign="top" align="left" rowspan="1" colspan="1">Multiracial</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">1 (neutral)</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2020</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">The Chicago Face Database India (CFD-INDIA)</td><td valign="top" align="left" rowspan="1" colspan="1">South Asian (Indian)</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">1 (neutral)</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2020</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">American Multiracial Faces Database (AMFD)</td><td valign="top" align="left" rowspan="1" colspan="1">Majority Asian-White and Latinx-White Multiracial faces</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">2</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2020</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Diverse Face Images (DFI)</td><td valign="top" align="left" rowspan="1" colspan="1">White, South Asian, Latin, Black, East-Asian</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">1 (neutral)</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="center" rowspan="1" colspan="1">2023</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Toronto Ethnically Diverse (TED) face database</td><td valign="top" align="left" rowspan="1" colspan="1">East Asian, Black, South East Asian, South Asian, Latinx, White, Middle Eastern, Indo-Caribbean, Indigenous, and multiracial</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">2 (open-mouth variation for happiness)</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="center" rowspan="1" colspan="1">2025</td></tr></tbody></table></table-wrap><p>This lack of databases containing ethnically diverse faces with variations among emotional expression, pose, and gaze direction leads some researchers to employ computer-generated faces to investigate facial recognition. Computer-generated faces are created by morphing real faces into a composite face representing the race of interest or by generating artificial faces using software algorithms (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>; Naples et al., <xref rid="B31" ref-type="bibr">2015</xref>; Vetter and Walker, <xref rid="B42" ref-type="bibr">2011</xref>). Chen et al. (<xref rid="B7" ref-type="bibr">2021</xref>) conducted a systematic review investigating the literature on multiracial person perception and found that 84% of published studies have relied on computer-generated faces to investigate the recognition of multiracial faces. Using computer-generated faces can allow for increased experimental control and standardization while allowing researchers to ask novel questions about the recognition of faces from underrepresented groups. However, artificially generated faces are highly controlled and can lack the natural variability present within human faces, thereby limiting the ecological validity of research findings (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>; Gross, <xref rid="B20" ref-type="bibr">2005</xref>). Additionally, real face images as compared to artificial images can elicit different racial categorizations, as well as divergent ratings on dimensions of trustworthiness, competence, and aggression (Balas and Pacella, <xref rid="B2" ref-type="bibr">2017</xref>; Naples et al., <xref rid="B31" ref-type="bibr">2015</xref>; Vetter and Walker, <xref rid="B42" ref-type="bibr">2011</xref>). Artificial faces are also more poorly remembered than real faces. This has been attributed to the frequent exposure to real faces, which contributes to an out-group disadvantage for the memory of artificial faces (Balas and Pacella, <xref rid="B2" ref-type="bibr">2017</xref>). In sum, these findings suggest that the artificiality of computer-generated faces can minimize the variability associated with identity characteristics leading to altered perceptions of faces, thereby making it less ideal for researchers to employ computer-generated faces to study ethnically diverse populations (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>).</p><p>The limitations of existing face databases and computer-generated faces suggest there is still a need for face databases that provide ethnic diversity as well as variation along other dimensions, like emotional expression, pose, and gaze direction. The goal of the current study was to create and validate a large database consisting of photographs varying in gender, race, pose, expression, and gaze direction. The Toronto Ethnically Diverse (TED) face database is a collection of 271 faces of real individuals from diverse racial backgrounds that have been rated on valence, intensity, genuineness, and accuracy of emotional expressions. It is composed of faces of adult models from multiple ethnic backgrounds (i.e., East Asian, Black, Southeast Asian, South Asian, Latinx, White, Middle Eastern, Indo-Caribbean, Indigenous, and multiracial), the majority of whom are women. Faces vary in pose (frontal, three-quarters, profile, chin up, chin down), eye gaze (open, closed, gaze left, gaze right, gaze up, gaze down), and emotional expression (neutral, open-mouth happiness, closed-mouth happiness). Thus, the TED face database improves upon some limitations in pre-existing datasets and can facilitate novel studies that use faces to investigate perceptual, emotional, and social processes by providing a large, standardized face database containing variations of emotion, gaze, and pose in ethnically diverse models.</p></sec><sec id="s2"><title>Method</title><sec><title>Database development</title><p>Models were recruited from a large public university in Canada and the surrounding community. They participated in the study in exchange for either partial course credit or compensation of $15 for their participation. The study ad was posted online and specified that individuals must be 18 years of age or over to participate. Please note that we informed volunteers that their photographs would be shared with others for research purposes without any accompanying personal information. Participants who opted out of sharing their photographs were welcome to complete the study without their data being recorded.</p><p>Our 271 volunteers (<italic>M</italic>age = 21.39, <italic>SD</italic> = 5.3; 199 women, 72 men) consisted of ~5% Black, 9% East Asian, 2% LatinX, 3% Middle Eastern, 20% South Asian, 5% South East Asian, 5% Indo Caribbean, 46% White, 7% Multi-racial, and &#x0003c; 1% Indigenous. Ethnicity was self-reported based on the census categories of the 2014 Canadian census. <xref rid="T2" ref-type="table">Table 2</xref> shows the demographic characteristics of the models in this face database.</p><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Stimuli demographic information.</p></caption><table frame="box" rules="all"><thead><tr style="background-color:#919498;color:#ffffff"><th valign="top" align="left" colspan="2" rowspan="1">
<bold>Demographic characteristics</bold>
</th></tr></thead><tbody><tr style="background-color:#dee1e1;"><td valign="top" align="left" rowspan="1" colspan="1">
<bold>Age</bold>
</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>Years</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;Range</td><td valign="top" align="left" rowspan="1" colspan="1">17&#x02013;61</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;Mean</td><td valign="top" align="left" rowspan="1" colspan="1">21.39</td></tr><tr style="background-color:#dee1e1;"><td valign="top" align="left" rowspan="1" colspan="1">
<bold>Gender</bold>
</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>Frequency</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;Men</td><td valign="top" align="left" rowspan="1" colspan="1">72</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;Women</td><td valign="top" align="left" rowspan="1" colspan="1">199</td></tr><tr style="background-color:#dee1e1;"><td valign="top" align="left" rowspan="1" colspan="1">
<bold>Ethnicity</bold>
</td><td valign="top" align="left" rowspan="1" colspan="1">
<bold>Frequency</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;Black</td><td valign="top" align="left" rowspan="1" colspan="1">13</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;12 women</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;1 man</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;East Asian</td><td valign="top" align="left" rowspan="1" colspan="1">24</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;17 women</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;7 men</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;LatinX</td><td valign="top" align="left" rowspan="1" colspan="1">5</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;4 women</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;1 man</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;Middle Eastern</td><td valign="top" align="left" rowspan="1" colspan="1">7</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;6 women</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;1 man</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;South Asian</td><td valign="top" align="left" rowspan="1" colspan="1">53</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;33 women</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;20 men</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;South East Asian</td><td valign="top" align="left" rowspan="1" colspan="1">13</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;10 women</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;3 men</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;Indo Caribbean</td><td valign="top" align="left" rowspan="1" colspan="1">13</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;10 women</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;3 men</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;White</td><td valign="top" align="left" rowspan="1" colspan="1">124</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;90 women</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;34 men</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;Multiracial</td><td valign="top" align="left" rowspan="1" colspan="1">18</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;16 women</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;2 men</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;Indigenous</td><td valign="top" align="left" rowspan="1" colspan="1">1</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x000a0;1 woman</td></tr></tbody></table><table-wrap-foot><p>Ethnicity information for five models was missing.</p></table-wrap-foot></table-wrap><sec><title>Procedure and measures</title><p>Volunteers read and signed a photo release form permitting the use of the photographs for academic research and educational purposes. Volunteers also completed a demographic questionnaire, including questions about age, ethnic background, country of birth, previous countries of residence, and gender.</p><p>Prior to being photographed, models were asked to remove any accessories that would visually separate them from the other models (e.g., glasses, headbands, hats). Models were photographed against a white wall and draped with a black scarf to hide clothing and reduce any potential reflected hues on the models' faces.</p><p>Full color photographs were taken with a EOS Rebel T3i camera (affixed to a tripod) and photoshoot lights (placed behind the camera). Participants sat on a stool in front of a white background while facing the camera. A wooden arm with a marker, to align the model's face, was fixed to the closest wall. Research assistants were instructed to swing the arm out to 90 degrees and align the center marker with the eyes of the model before taking the photograph. This alignment was specific for each pose. For example, for the chin-up pose research assistants were asked to align model's bottom lip with the marker on the wooden arm before taking the photograph. See <xref rid="F1" ref-type="fig">Figure 1</xref> for a diagram of the setup of the photoshoot.</p><fig position="float" id="F1"><label>Figure 1</label><caption><p>Technical setup of the photoshoot.</p></caption><graphic xlink:href="fpsyg-16-1541546-g0001" position="float"/></fig><p>Volunteers were photographed expressing a neutral expression and two happy expressions: an open- and closed-lip smile. Frontal, chin up, and chin down photographs were taken of the models. Photographs were taken of the models head-on, at a three-quarter angle, and of their profile. Further, models posed various gaze conditions: eyes open looking forward, eyes toward the left, eyes toward the right, eyes closed, eyes up, and eyes down. However, not all gaze conditions were fulfilled for each emotion, nor head position. Thus, across dimensions of emotion, head pose, and eye gaze, the number of photographs for each model ranged between 38 and 46 photographs.</p><p>To obtain representations of happy and neutral emotional expressions, we asked volunteers to pose the expressions in a way that felt natural to them. These &#x0201c;free posed&#x0201d; expressions offer an alternative to both posed facial expressions (in which participants are asked to move facial muscles in particular ways) and spontaneous facial expressions (expressions that occur in response to natural events). To achieve free-posed expressions, volunteers were asked to think of a time in their life that they felt either neutral or happy and practiced displaying the emotion in a mirror. To further help volunteers display the intended emotion, the experimenter read scenarios that were intended to elicit the specific emotion. Volunteers were not given feedback on their emotional expressions and the experimenter refrained from explaining how to pose (see <xref rid="s11" ref-type="sec">Supplementary material</xref> on Open Science Framework [OSF]). Participants were given as much time as needed until they were ready to have their photo taken. To achieve the eye gaze and pose variations, participants were given the following instructions: now please [turn your head to the right/look to the top-right corner], still expressing [closed-mouth happiness] as naturally as possible. Once the participant was ready, one photograph was taken per condition.</p><p>Using Adobe Photoshop (v2014.0.1), all photographs were resized to 5,184 &#x000d7; 3,456 pixels, such that the target face and core facial features were approximately centered in the image. Photographs were then cropped in a headshot format using the rectangular cropping tool. Example stimuli are displayed in <xref rid="F2" ref-type="fig">Figure 2</xref>.</p><fig position="float" id="F2"><label>Figure 2</label><caption><p>Example stimuli from Model 015. Top left image shows the model with a neutral expression in frontal pose and with open eyes. Top right image shows the model with a neutral expression in frontal pose and with closed eyes. Middle left image shows the model expressing open-mouth happiness in a frontal pose and with open eyes. Middle right image shows the model with a neutral expression in frontal pose and with gaze direction left. Bottom left image shows the model expressing closed-mouth happiness in a frontal pose and with open eyes. Bottom right image shows the model with a neutral expression in frontal pose and with gaze direction right.</p></caption><graphic xlink:href="fpsyg-16-1541546-g0002" position="float"/></fig></sec></sec><sec><title>Stimulus validation</title><p>Only front facing photographs were used in the current validation study. We obtained stimulus ratings from 502 undergraduate students at Toronto Metropolitan University who received partial course credit for their participation (<italic>M</italic>age = 19.45, <italic>SD</italic> = 3.21). The sample included 89 men (17.8%), 407 women (81.1%), two gender-fluid individuals (0.4%), and four non-binary individuals (0.8%). According to their self-reported ethnicity, participants were East Asian (<italic>n</italic> = 69), Black (<italic>n</italic> = 23), Southeast Asian (<italic>n</italic> = 64), South Asian (<italic>n</italic> = 77), Latinx (<italic>n</italic> = 16), White (<italic>n</italic> = 164), Middle Eastern or North African (<italic>n</italic> = 43), Indo-Caribbean (<italic>n</italic> = 11), and Multiracial (<italic>n</italic> = 32). Three participants did not specify their ethnicity. Our sample goal was set to obtain ~40 raters per face. We determined this goal based on previous research indicating that face-based ratings become stable at about 40 independent observations, or earlier depending on the particular attribute (Coffman et al., <xref rid="B8" ref-type="bibr">2015</xref>). Thus, we ensured that each photograph was rated by at least 40 participants.</p><sec><title>Procedure</title><p>Participants were asked to view and rate the photographs to assess whether they depicted the intended emotional expressions. They completed the survey online through Qualtrics. After providing informed consent, participants were asked to provide demographic information, including their age, gender, ethnic group, country of birth, and, if not born in Canada, the year moved to Canada. Participants were then shown one photograph at a time and were asked to give four ratings. The first question was: &#x0201c;What emotion is this face presenting?&#x0201d; with the following answer choices: Anger, Disgust, Fear, Happiness, Neutral, Sad, Surprise, or None of the listed emotions. The second question was: &#x0201c;What is the valence of the emotion? (Is it a positive or negative emotion?)&#x0201d; with the following options: Negative, Neutral, or Positive. Next, they were asked to &#x0201c;Rate the Intensity of the Emotion&#x0201d; on a scale from 1 (neutral) to 5 (extremely intense). Lastly, participants were asked &#x0201c;How genuine is this emotion?&#x0201d;, which they rated on a scale from 1 (not genuine at all) to 5 (very genuine). Each participant rated ~90 photos (i.e., 30 unique identities/models displaying two different emotions with three total variations). This number is an approximation as not all identities had photographs for the full set of emotions. Each set of 30 unique models contained both male and female models of different ethnicities. The photographs were presented in random order. Each face was rated by ~50 raters on average (range: 41&#x02013;84). Participants' ratings were only included in the analyses presented below if they completed at least 50% of the survey.</p><p>Instructions to access the face database and all data files are posted online on the Open Science Framework (<ext-link xlink:href="https://osf.io/6vdn2/?view_only=14a1fcd1f3dd434bbfe6a4028fd96400" ext-link-type="uri">https://osf.io/6vdn2/?view_only=14a1fcd1f3dd434bbfe6a4028fd96400</ext-link>). On this site, researchers can find the descriptive statistics for each model (<xref rid="s11" ref-type="sec">Supplementary Tables</xref> on OSF) and a readme file, as well as instructions to contact the Brain and Early Experiences (BEE) Lab at Toronto Metropolitan University at <email>beelab@torontomu.ca</email> to gain access to the images in the database.</p></sec></sec></sec><sec sec-type="results" id="s3"><title>Results</title><p>The Toronto Ethnically Diverse (TED) face database includes 271 unique individuals who each posed with several variations of a neutral expression and a happy expression. These variations included pose and gaze direction. For the stimulus validation, only direct gaze frontal view photos were rated. The total number of photographs that were rated is 805 (270 neutral, 268 open-mouth happiness, 267 closed-mouth happiness).</p><p>To determine whether emotional expression, ethnicity, or their interaction impacted our other dependent measures, analyzed the descriptive statistics, and then conducted a two-way analysis of variance on valence, intensity, and genuineness ratings. These results are discussed below.</p><sec><title>Inter-rater reliability</title><p>First, two measures, proportion correct and Cohen's kappa (Cohen, <xref rid="B9" ref-type="bibr">1960</xref>), were calculated for each of the 805 stimuli, modeled after the analyses of the NimStim Set of Facial Expressions and the American Multi-racial Face Database (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>; Tottenham et al., <xref rid="B40" ref-type="bibr">2009</xref>). For each stimulus (photograph), proportion correct was calculated by comparing the number of participants who endorsed the correct target expression to the total number of participants who rated that photograph. Though proportion correct is often reported in examinations of facial expression (Ekman and Friesen, <xref rid="B15" ref-type="bibr">1975</xref>; Mandal, <xref rid="B29" ref-type="bibr">1987</xref>; Bieh et al., <xref rid="B6" ref-type="bibr">1997</xref>; Beaupr&#x000e9; and Hess, <xref rid="B4" ref-type="bibr">2005</xref>; Wang and Markham, <xref rid="B43" ref-type="bibr">1999</xref>), Tottenham et al. (<xref rid="B40" ref-type="bibr">2009</xref>) suggest that Cohen's kappa (Cohen, <xref rid="B9" ref-type="bibr">1960</xref>) may be a better dependent variable for evaluations of face databases since proportion correct does not consider false positives (Erwin et al., <xref rid="B16" ref-type="bibr">1992</xref>). Therefore, kappa scores, a measure of agreement between participants' labels and models' intended expressions adjusted for agreement due to chance, were used to estimate agreement between selected labels and intended expressions. These scores were calculated across models within each survey, independently for open- and closed-mouth conditions. Endorsements of &#x0201c;none of the above&#x0201d; were counted as incorrect.</p><p>Average ratings for each of the three emotional expressions are presented in <xref rid="T3" ref-type="table">Table 3</xref> and <xref rid="F3" ref-type="fig">Figure 3</xref> and average ratings separated by face ethnicity are presented in <xref rid="F4" ref-type="fig">Figure 4</xref> (see <xref rid="s11" ref-type="sec">Supplementary Tables</xref> on OSF for proportion correct and kappa score for individual photographs and for other descriptive statistics by ethnicity). Overall, proportion correct (Mean = 0.79, <italic>SD</italic> = 0.23, Median = 0.88) and kappa scores (Mean = 0.88, <italic>SD</italic>= 0.14, Median = 0.93) were high, indicating that stimuli accurately conveyed their intended expressions (Landis and Koch, <xref rid="B24" ref-type="bibr">1977</xref>). Kappa scores per model ranged from 0.6 to 1 for 93% of the models (i.e., 251 of the total 271 models), reflecting general agreement between participants' labels and models' expressions, adjusting for agreement due to chance (Cohen, <xref rid="B9" ref-type="bibr">1960</xref>; Landis and Koch, <xref rid="B24" ref-type="bibr">1977</xref>). Of the mean proportion correct scores, 54% (147/271) were above 0.70.</p><table-wrap position="float" id="T3"><label>Table 3</label><caption><p>Descriptive statistics for each of the three emotional expressions.</p></caption><table frame="box" rules="all"><thead><tr style="background-color:#919498;color:#ffffff"><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Stimuli</bold>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Measures</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>
<italic>M</italic>
</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>
<italic>SD</italic>
</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Range</bold>
</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Neutral</td><td valign="top" align="left" rowspan="1" colspan="1">Accuracy (%)</td><td valign="top" align="center" rowspan="1" colspan="1">0.68</td><td valign="top" align="center" rowspan="1" colspan="1">0.19</td><td valign="top" align="center" rowspan="1" colspan="1">0.07 to 1</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Kappa</td><td valign="top" align="center" rowspan="1" colspan="1">0.81</td><td valign="top" align="center" rowspan="1" colspan="1">0.12</td><td valign="top" align="center" rowspan="1" colspan="1">0.31 to 99</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Valence (&#x02212;1 to +1)</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;0.18</td><td valign="top" align="center" rowspan="1" colspan="1">0.25</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;0.78 to 0.82</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Intensity (1&#x02013;5)</td><td valign="top" align="center" rowspan="1" colspan="1">1.92</td><td valign="top" align="center" rowspan="1" colspan="1">0.23</td><td valign="top" align="center" rowspan="1" colspan="1">1 to 2.73</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Genuineness (1&#x02013;5)</td><td valign="top" align="center" rowspan="1" colspan="1">3.14</td><td valign="top" align="center" rowspan="1" colspan="1">0.24</td><td valign="top" align="center" rowspan="1" colspan="1">2.4 to 4.2</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Open smile</td><td valign="top" align="left" rowspan="1" colspan="1">Accuracy (%)</td><td valign="top" align="center" rowspan="1" colspan="1">0.90</td><td valign="top" align="center" rowspan="1" colspan="1">0.19</td><td valign="top" align="center" rowspan="1" colspan="1">0 to 1</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Kappa</td><td valign="top" align="center" rowspan="1" colspan="1">0.94</td><td valign="top" align="center" rowspan="1" colspan="1">0.12</td><td valign="top" align="center" rowspan="1" colspan="1">0.31 to 1</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Valence (&#x02212;1 to +1)</td><td valign="top" align="center" rowspan="1" colspan="1">0.86</td><td valign="top" align="center" rowspan="1" colspan="1">0.22</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;0.38 to 1</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Intensity (1&#x02013;5)</td><td valign="top" align="center" rowspan="1" colspan="1">3.52</td><td valign="top" align="center" rowspan="1" colspan="1">0.57</td><td valign="top" align="center" rowspan="1" colspan="1">1 to 4.62</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Genuineness (1&#x02013;5)</td><td valign="top" align="center" rowspan="1" colspan="1">3.76</td><td valign="top" align="center" rowspan="1" colspan="1">0.57</td><td valign="top" align="center" rowspan="1" colspan="1">1 to 4.64</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Closed smile</td><td valign="top" align="left" rowspan="1" colspan="1">Accuracy (%)</td><td valign="top" align="center" rowspan="1" colspan="1">0.80</td><td valign="top" align="center" rowspan="1" colspan="1">0.24</td><td valign="top" align="center" rowspan="1" colspan="1">0 to 1</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Kappa</td><td valign="top" align="center" rowspan="1" colspan="1">0.89</td><td valign="top" align="center" rowspan="1" colspan="1">0.13</td><td valign="top" align="center" rowspan="1" colspan="1">0.25 to 1</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Valence (&#x02212;1 to +1)</td><td valign="top" align="center" rowspan="1" colspan="1">0.71</td><td valign="top" align="center" rowspan="1" colspan="1">0.30</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;0.74 to 1</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Intensity (1&#x02013;5)</td><td valign="top" align="center" rowspan="1" colspan="1">2.82</td><td valign="top" align="center" rowspan="1" colspan="1">0.51</td><td valign="top" align="center" rowspan="1" colspan="1">1 to 3.82</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Genuineness (1&#x02013;5)</td><td valign="top" align="center" rowspan="1" colspan="1">3.44</td><td valign="top" align="center" rowspan="1" colspan="1">0.51</td><td valign="top" align="center" rowspan="1" colspan="1">1 to 4.46</td></tr></tbody></table><table-wrap-foot><p><italic>N</italic> = 502.</p></table-wrap-foot></table-wrap><fig position="float" id="F3"><label>Figure 3</label><caption><p>Mean proportion correct and kappa scores for each of the three emotional expressions. Error bars represent standard error of the mean.</p></caption><graphic xlink:href="fpsyg-16-1541546-g0003" position="float"/></fig><fig position="float" id="F4"><label>Figure 4</label><caption><p>Mean proportion correct and kappa scores for each of the 10 model ethnic groups. Error bars represent standard error of the mean.</p></caption><graphic xlink:href="fpsyg-16-1541546-g0004" position="float"/></fig><sec><title>Proportion correct</title><p>The two-way analysis of variance on the proportion correct scores found a significant effect of emotion, <italic>F</italic><sub>(2,802)</sub> = 81.63, <italic>p</italic> &#x0003c; 0.001, <italic>n2</italic> = 0.17. Open-mouth happy faces were rated more accurately (<italic>M</italic> = 0.90, <italic>SD</italic> = 0.19) than closed-mouth happy faces (<italic>M</italic> = 0.80, <italic>SD</italic> = 0.24), <italic>t</italic><sub>(509)</sub> = 5.55, <italic>p</italic> &#x0003c; 0.001, and neutral faces (<italic>M</italic> = 0.68, <italic>SD</italic> = 0.19), <italic>t</italic><sub>(535)</sub> = 13.94, <italic>p</italic> &#x0003c; 0.001. Closed-mouth happy faces were also rated more accurately than neutral faces, <italic>t</italic><sub>(502)</sub> = 6.67, <italic>p</italic> &#x0003c; 0.001. We did not find a significant effect of ethnicity or an interaction between emotion and ethnicity.</p></sec><sec><title>Kappa scores</title><p>Similar to findings for proportion correct, the two-way analysis of variance on the kappa scores found a significant effect of emotion, <italic>F</italic><sub>(2,802)</sub> = 68.45, <italic>p</italic> &#x0003c; 0.001, <italic>n2</italic> = 0.15. Open-mouth happy faces (<italic>M</italic> = 0.94, <italic>SD</italic> = 0.12) were rated as having significantly higher agreement than closed-mouth happy faces (<italic>M</italic> = 0.89, <italic>SD</italic> = 0.13), <italic>t</italic><sub>(532)</sub> = 4.28, <italic>p</italic> &#x0003c; 0.001, and neutral faces (<italic>M</italic> = 0.81, <italic>SD</italic> = 0.12), <italic>t</italic><sub>(536)</sub> = 11.71, <italic>p</italic> &#x0003c; 0.001. Closed-mouth happy faces were also rated as having significantly higher agreement than neutral faces, <italic>t</italic><sub>(534)</sub> = 7.23, <italic>p</italic> &#x0003c; 0.001. In contrast to findings for proportion correct, we found a significant effect of ethnicity on kappa scores, <italic>F</italic><sub>(9,775)</sub> = 2.31, <italic>p</italic> &#x0003c; 0.050, <italic>n</italic>2 = 0.02. Multi-racial faces (<italic>M</italic> = 0.92, <italic>SD</italic> = 0.08) had significantly higher agreement than South-Asian faces (<italic>M</italic> = 0.89, <italic>SD</italic> = 0.12), <italic>t</italic><sub>(204)</sub> = 1.86, <italic>p</italic> &#x0003c; 0.050, White faces (<italic>M</italic> = 0.88, <italic>SD</italic> = 0.13), <italic>t</italic><sub>(419)</sub> = 1.81, <italic>p</italic> &#x0003c; 0.050, East-Asian faces (<italic>M</italic> = 0.86, <italic>SD</italic> = 0.17), <italic>t</italic><sub>(122)</sub> = 2.19, <italic>p</italic> &#x0003c; 0.050, Middle Eastern faces (<italic>M</italic> = 0.84, <italic>SD</italic> = 0.13), <italic>t</italic><sub>(71)</sub> = 3.50, <italic>p</italic> &#x0003c; 0.010, Indo-Caribbean faces (<italic>M</italic> = 0.84, <italic>SD</italic> = 0.18), <italic>t</italic><sub>(88)</sub> = 2.83, <italic>p</italic> &#x0003c; 0.010, and Black faces (<italic>M</italic> = 0.83, <italic>SD</italic> = 0.18), <italic>t</italic><sub>(92)</sub> = 3.02, <italic>p</italic> &#x0003c; 0.010. Southeast-Asian faces (<italic>M</italic> = 0.89, <italic>SD</italic> = 0.11) had significantly higher agreement than Middle-Eastern faces (<italic>M</italic> = 0.84, <italic>SD</italic> = 0.13), <italic>t</italic><sub>(56)</sub> = 1.68, <italic>p</italic> &#x0003c; 0.050, and Black faces (<italic>M</italic> = 0.83, <italic>SD</italic> = 0.18), <italic>t</italic><sub>(77)</sub> = 1.80, <italic>p</italic> &#x0003c; 0.050. South-Asian faces (<italic>M</italic> = 0.89, <italic>SD</italic> = 0.12) had significantly higher agreement than Indo-Caribbean faces (<italic>M</italic> = 0.84, <italic>SD</italic> = 0.18), <italic>t</italic><sub>(188)</sub> = 1.95, <italic>p</italic> &#x0003c; 0.050, and Black faces (<italic>M</italic> = 0.83, <italic>SD</italic> = 0.18), <italic>t</italic><sub>(192)</sub> = 2.25, <italic>p</italic> &#x0003c; 0.050. Similarly, White faces (<italic>M</italic> = 0.88, <italic>SD</italic> = 0.13) had significantly higher agreement than Indo-Caribbean faces (<italic>M</italic> = 0.84, <italic>SD</italic> = 0.18), <italic>t</italic><sub>(403)</sub> = 1.97, <italic>p</italic> &#x0003c; 0.050, and Black faces (<italic>M</italic> = 0.83, <italic>SD</italic> = 0.18), <italic>t</italic><sub>(407)</sub> = 2.32, <italic>p</italic> &#x0003c; 0.050. We did not find a significant interaction between emotion and ethnicity.</p><p>The confusion matrix (<xref rid="T4" ref-type="table">Table 4</xref>) depicts the average proportion of target and non-target labels endorsed for each expression, revealing patterns of misidentifications. This matrix demonstrates that expressions were rarely identified as &#x0201c;none of the above&#x0201d; (endorsement of &#x0201c;none of the above&#x0201d; across expressions ranged from 0.02 to 0.05). Overall, participants accurately endorsed target labels for each expression. However, faces displaying a neutral expression were most often mislabeled as &#x0201c;sad&#x0201d; or &#x0201c;anger&#x0201d; expressions (endorsement of both &#x0201c;sad&#x0201d; or &#x0201c;anger&#x0201d; was 0.08). Faces displaying an open-mouth happy expression were occasionally identified as neutral or surprise (endorsement of both &#x0201c;neutral&#x0201d; or &#x0201c;surprise&#x0201d; was 0.03) and faces with closed-mouth happy expressions were most often mislabeled as neutral (endorsement of &#x0201c;neutral&#x0201d; was 0.11). These results are consistent with previous face databases (Tottenham et al., <xref rid="B40" ref-type="bibr">2009</xref>; Langner et al., <xref rid="B25" ref-type="bibr">2010</xref>; Conley et al., <xref rid="B10" ref-type="bibr">2018</xref>) that confirm that neutral faces are often mislabeled as sad or anger. Also, the higher identification rate for happy than neutral expressions found in the current study is consistent with previous research (Hare et al., <xref rid="B22" ref-type="bibr">2005</xref>). As well, emotion recognition studies suggest that closed-mouth happy expressions are most often mislabeled as neutral (Beaupr&#x000e9; and Hess, <xref rid="B5" ref-type="bibr">2006</xref>; Ekman and Friesen, <xref rid="B15" ref-type="bibr">1975</xref>).</p><table-wrap position="float" id="T4"><label>Table 4</label><caption><p>Confusion matrix depicting proportion of responses for each emotion label (<italic>SD</italic>).</p></caption><table frame="box" rules="all"><thead><tr style="background-color:#919498;color:#ffffff"><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Photograph</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Anger</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Disgust</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Fear</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Happiness</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Neutral</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Sad</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Surprise</bold>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<bold>None of the listed emotions</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>N/A</bold>
</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Neutral</td><td valign="top" align="center" rowspan="1" colspan="1">0.08</td><td valign="top" align="center" rowspan="1" colspan="1">0.03</td><td valign="top" align="center" rowspan="1" colspan="1">0.03</td><td valign="top" align="center" rowspan="1" colspan="1">0.05</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.66</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">0.08</td><td valign="top" align="center" rowspan="1" colspan="1">0.01</td><td valign="top" align="left" rowspan="1" colspan="1">0.05</td><td valign="top" align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">(0.10)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.04)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.04)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.11)</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>(0.18)</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">(0.10)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.03)</td><td valign="top" align="left" rowspan="1" colspan="1">(0.04)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.02)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Happiness (Open)</td><td valign="top" align="center" rowspan="1" colspan="1">0.00</td><td valign="top" align="center" rowspan="1" colspan="1">0.01</td><td valign="top" align="center" rowspan="1" colspan="1">0.00</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.89</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">0.03</td><td valign="top" align="center" rowspan="1" colspan="1">0.00</td><td valign="top" align="center" rowspan="1" colspan="1">0.03</td><td valign="top" align="left" rowspan="1" colspan="1">0.02</td><td valign="top" align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">(0.01)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.02)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.01)</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>(0.17)</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">(0.06)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.01)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.08)</td><td valign="top" align="left" rowspan="1" colspan="1">(0.05)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.01)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Happiness (Closed)</td><td valign="top" align="center" rowspan="1" colspan="1">0.01</td><td valign="top" align="center" rowspan="1" colspan="1">0.01</td><td valign="top" align="center" rowspan="1" colspan="1">0.01</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.79</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">0.11</td><td valign="top" align="center" rowspan="1" colspan="1">0.02</td><td valign="top" align="center" rowspan="1" colspan="1">0.01</td><td valign="top" align="left" rowspan="1" colspan="1">0.04</td><td valign="top" align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">(0.02)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.03)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.01)</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>(0.23)</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">(0.16)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.03)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.03)</td><td valign="top" align="left" rowspan="1" colspan="1">(0.05)</td><td valign="top" align="center" rowspan="1" colspan="1">(0.01)</td></tr></tbody></table><table-wrap-foot><p><italic>N</italic> = 502. The bold values represent average proportion of target labels endorsed for each expression.</p></table-wrap-foot></table-wrap></sec></sec><sec><title>Valence</title><p>Participants' valence responses (negative/neutral/positive) were coded as negative = &#x02212;1, neutral = 0, positive = +1. We found a significant effect of emotion, <italic>F</italic><sub>(2,802)</sub> = 167.79, <italic>p</italic> &#x0003c; 0.001, <italic>n2</italic> = 0.76. Open-mouth happy faces were rated as significantly more positive (<italic>M</italic> = 0.86, <italic>SD</italic> = 0.22) than closed-mouth happy faces (<italic>M</italic> = 0.71, <italic>SD</italic> = 0.30), <italic>t</italic><sub>(496)</sub> = 6.61, <italic>p</italic> &#x0003c; 0.001 and neutral faces (<italic>M</italic> = &#x02212;0.18, <italic>SD</italic> = 0.25), <italic>t</italic><sub>(499)</sub> = &#x02212;51.32, <italic>p</italic> &#x0003c; 0.001. As well, closed-mouth happy faces were rated as significantly more positive than neutral faces, <italic>t</italic><sub>(498)</sub> = &#x02212;37.59, <italic>p</italic> &#x0003c; 0.001. We did not find a significant effect of ethnicity on valence ratings or an interaction between emotion and ethnicity.</p></sec><sec><title>Intensity</title><p>There was also a significant effect of emotion on intensity ratings, <italic>F</italic><sub>(2,802)</sub> = 806.5, <italic>p</italic> &#x0003c; 0.001, <italic>n2</italic> = 0.67. Open-mouth happy faces (<italic>M</italic> = 3.52, <italic>SD</italic> = 0.57) were rated as significantly more intense than closed-mouth happy (<italic>M</italic> = 2.82, <italic>SD</italic> = 0.51), <italic>t</italic><sub>(527)</sub> = 14.82, <italic>p</italic> &#x0003c; 0.001, and neutral faces (<italic>M</italic> = 1.92, <italic>SD</italic> = 0.23), <italic>t</italic><sub>(351)</sub> = 42.32, <italic>p</italic> &#x0003c; 0.001. Closed-mouth happy faces were also rated significantly more intense than neutral faces, <italic>t</italic><sub>(370)</sub> = 26.28, <italic>p</italic> &#x0003c; 0.001. We did not find a significant effect of ethnicity on intensity ratings or an interaction between emotion and ethnicity.</p></sec><sec><title>Genuineness</title><p>There was also a significant effect of emotion on genuineness ratings, <italic>F</italic><sub>(2,802)</sub> = 121, <italic>p</italic> &#x0003c; 0.001, <italic>n2</italic> = 0.23. Open-mouth happy faces (<italic>M</italic> = 3.76, <italic>SD</italic> = 0.57) were rated as significantly more genuine than closed-mouth happy (<italic>M</italic> = 3.44, <italic>SD</italic> = 0.51), <italic>t</italic><sub>(526)</sub> = 6.88, <italic>p</italic> &#x0003c; 0.001, and neutral faces (<italic>M</italic> = 3.14, <italic>SD</italic> = 0.24), <italic>t</italic><sub>(358)</sub> = 16.38, <italic>p</italic> &#x0003c; 0.001. Closed mouth happy faces were also rated significantly more genuine than neutral faces, <italic>t</italic><sub>(379)</sub> = 8.70, <italic>p</italic> &#x0003c; 0.001. We did not find a significant effect of ethnicity or an interaction between emotion and ethnicity. <xref rid="T5" ref-type="table">Table 5</xref> reports the descriptive statistics for each rating dimension collapsed across all stimuli in TED.</p><table-wrap position="float" id="T5"><label>Table 5</label><caption><p>Descriptive statistics for entire face database.</p></caption><table frame="box" rules="all"><thead><tr style="background-color:#919498;color:#ffffff"><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Measures</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>
<italic>M</italic>
</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>
<italic>SD</italic>
</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Range</bold>
</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Accuracy (%)</td><td valign="top" align="center" rowspan="1" colspan="1">0.79</td><td valign="top" align="center" rowspan="1" colspan="1">0.23</td><td valign="top" align="center" rowspan="1" colspan="1">0 to 1</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Kappa</td><td valign="top" align="center" rowspan="1" colspan="1">0.88</td><td valign="top" align="center" rowspan="1" colspan="1">0.13</td><td valign="top" align="center" rowspan="1" colspan="1">0.25 to 1</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Valence (&#x02212;1 to +1)</td><td valign="top" align="center" rowspan="1" colspan="1">0.46</td><td valign="top" align="center" rowspan="1" colspan="1">0.52</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;0.78 to 1</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Intensity (1&#x02013;5)</td><td valign="top" align="center" rowspan="1" colspan="1">2.75</td><td valign="top" align="center" rowspan="1" colspan="1">0.80</td><td valign="top" align="center" rowspan="1" colspan="1">0 to 4.62</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Genuineness (1&#x02013;5)</td><td valign="top" align="center" rowspan="1" colspan="1">3.45</td><td valign="top" align="center" rowspan="1" colspan="1">0.53</td><td valign="top" align="center" rowspan="1" colspan="1">0 to 4.64</td></tr></tbody></table><table-wrap-foot><p><italic>N</italic> = 502.</p></table-wrap-foot></table-wrap></sec></sec><sec sec-type="discussion" id="s4"><title>Discussion</title><p>Face perception is an essential process for day-to-day social interactions. Researchers who are interested in studying face perception and related impression formation processes frequently rely on convenience samples of stimuli. While there are a number of high-quality face databases available to researchers, there are very few with ethnically diverse models that also present variations in emotional expression, gaze direction, and pose. Increasing the number of stimuli can add to the heterogeneity of existing face databases, which will correspondingly improve our field's ability to produce generalizable results and empirically address a broader set of issues (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>). In this paper, we present the Toronto Ethnically Diverse (TED) face database, a collection of 271 unique models with accompanying ratings by a diverse sample of participants. We provide neutral and smiling versions of faces for researchers to have additional flexibility in their research questions and methods. The TED face database is an open-access resource for researchers interested in psychological processes involving face processing and social perception.</p><p>The validation results for TED suggest that this face database consists of models displaying their intended emotions with high fidelity: Mean proportion correct was 0.79 across the entire face database. Similarly, the mean kappa score, which measures agreement between participants' labels and models' intended expressions, taking into account incorrect judgments, was 0.88. This reflects high agreement among participants that stimuli conveyed their intended expressions.</p><p>Despite overall high inter-rater agreement, the ethnicity of the face impacted kappa scores for emotion ratings. Indigenous, Multi-racial, South-Asian, Southeast Asian, Latin, and White faces showed the highest kappa scores (i.e., the highest agreement among participants that the stimuli conveyed their intended emotion), with Black and Indo-Caribbean faces showing the lowest kappa scores. Although it is uncertain what the source of these ethnic group differences is, it is important to note that average kappa scores for all ethnic groups were high, ranging from 0.83 to 0.93, in line with existing databases (Conley et al., <xref rid="B10" ref-type="bibr">2018</xref>; Tottenham et al., <xref rid="B40" ref-type="bibr">2009</xref>). For researchers concerned with using only those faces with the highest consensus, we recommend they use the available descriptive statistics for each model to make the best decision on model selection for their research.</p><p>Valence, intensity, and genuineness ratings support the finding of high emotion recognition accuracy. For valence, participants rated open-mouth happy faces more positively than neutral faces and closed-mouth happy faces, and closed-mouth happy faces were rated more positively than neutral faces. This is consistent with previous face databases that confirm neutral valence for neutral faces and positive valence for both closed and open-mouth happy faces (Conley et al., <xref rid="B10" ref-type="bibr">2018</xref>; Langner et al., <xref rid="B25" ref-type="bibr">2010</xref>; Tottenham et al., <xref rid="B40" ref-type="bibr">2009</xref>). We also found that open-mouth happy faces were rated as more intense than both closed-mouth happy faces and neutral faces, and closed-mouth happy faces were rated as more intense than neutral faces. Overall, the TED stimuli were rated as moderately intense (<italic>M</italic> = 2.75) consistent with previous face databases that contain neutral and happy expressions (Langner et al., <xref rid="B25" ref-type="bibr">2010</xref>; Palermo and Coltheart, <xref rid="B33" ref-type="bibr">2004</xref>). For genuineness, open-mouth happy faces were rated as more genuine depictions of the intended expression than closed-mouth happy and neutral faces, and closed-mouth happy faces were rated as more genuine than neutral faces. These results are consistent with previous literature suggesting an increase in perceived genuineness for open-mouth happy expressions as compared to closed-mouth happy expressions (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>; Langner et al., <xref rid="B25" ref-type="bibr">2010</xref>; Wang et al., <xref rid="B44" ref-type="bibr">2017</xref>).</p><p>Although accuracy, valence, intensity, and genuineness results demonstrate that overall, the TED face database consists of models displaying their intended emotions, there was variability among models in how accurately their emotional expressions were identified. The comprehensive TED face database is provided because of the need for representative stimuli of individuals of color; however, we also provide <xref rid="s11" ref-type="sec">Supplementary Tables</xref> on the Open Science Framework with proportion correct, kappa, mean valence, mean intensity, and mean genuineness scores for each of the 805 stimuli that were validated, so that researchers can choose the subset of stimuli most appropriate for their research question.</p><p>There are a few limitations of the TED face database. The TED face database is limited in the range of emotional expressions represented. While it includes three emotional variations&#x02014;neutral, open-mouth happiness, and closed-mouth happiness&#x02014;it lacks other fundamental emotions such as anger, sadness, and disgust. This may restrict the types of research questions that can be posed regarding emotional expressions across diverse stimuli. Future research would benefit from the inclusion of ethnically diverse models exhibiting a broader spectrum of emotional expressions. Despite its limitations in emotional diversity, the TED face database remains one of the most diverse resources available for researchers investigating ethnically diverse stimuli. It offers a range of variations in other key dimensions, such as pose and gaze direction.</p><p>Moreover, the TED face database did not employ a fully crossed design, meaning that each face was not rated by an equal number of raters with the same demographic composition (i.e., an equal number of own-race and other-race raters). Although this would have been ideal, we were limited by the demographic composition of the undergraduate student population at Toronto Metropolitan University As a result, we are unable to draw conclusions about whether validation results would have differed for own-race vs. other-race raters. We recommend that researchers consider this limitation and the potential impact of the other-race effect, wherein individuals tend to recognize faces of their own race more accurately (Meissner and Brigham, <xref rid="B30" ref-type="bibr">2001</xref>), when using the TED database in their own research. Future research should explore the influence of both face ethnicity and rater ethnicity on validation outcomes.</p><p>Another limitation of the TED face database is that it is demographically skewed in terms of age, ethnicity, and gender. Although there are models available of various ages, the TED face database mainly includes young adults. The number of models belonging to an age group outside of young adults is low; this may limit the potential questions researchers can ask. It will be important for future research to collect ethnically diverse models of various ages. Moreover, White faces are still over-represented in the database. However, the TED adds substantial diversity to existing face databases, by providing many representations of ethnically diverse models from various backgrounds displaying different poses. In the future, we hope that researchers will curate additional databases that increase the representation of other types of ethnically diverse individuals, including individuals from Indigenous populations. The gender composition of the TED face database is predominantly women due to the composition of the undergraduate psychology research pool where many of the models in the TED database were recruited, which reflects the gender imbalance of undergraduate programs in psychology (Gruber et al., <xref rid="B21" ref-type="bibr">2021</xref>). Although this gender imbalance is not ideal, this database presents a valuable resource given that the documented bias in the literature is in the opposite direction&#x02014;it relies on predominantly male face databases (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>). We hope that the availability of the TED face database will help researchers address this bias in their future work; combining faces from the TED database with existing databases containing more male faces will allow researchers to implement research designs balanced on gender.</p><p>An advantage of the TED face database is that images were rated using a semi-forced choice design, allowing participants to choose across eight options (angry, disgust, fear, happy, neutral, sad, surprised, or &#x0201c;none of the above&#x0201d;) for each expression. Consistent with the NimStim (Tottenham et al., <xref rid="B40" ref-type="bibr">2009</xref>) methods, the &#x0201c;none of the above&#x0201d; choice was included because strict forced choice tasks can inflate correct labeling. However, other research suggests that the subtle complexities of expressions may not fully be captured with this design and that a combination of forced choice, freely chosen, or spectrum (i.e., slightly happy, moderately happy, very happy) labels may be more appropriate for rating faces (Russell, <xref rid="B35" ref-type="bibr">1994</xref>; Conley et al., <xref rid="B10" ref-type="bibr">2018</xref>).</p><p>The TED face database addresses several limitations of available face databases. Previous research has relied heavily on White, male, or computer-generated faces. The TED directly addresses these limitations by substantially increasing the number of real models, who are predominantly women and from ethnically diverse backgrounds. Although existing face databases include some ethnic diversity, non-White individuals represented in those face databases are predominantly Black or East Asian (Conley et al., <xref rid="B10" ref-type="bibr">2018</xref>; Lakshmi et al., <xref rid="B23" ref-type="bibr">2021</xref>). The inclusion of multiple faces of South Asian, Southeast Asian, LatinX, and Middle Eastern descent may facilitate studies on ethnic minorities outside of Black and East Asian backgrounds. This is particularly important because these groups face prejudice and discrimination (e.g., French et al., <xref rid="B17" ref-type="bibr">2013</xref>; Frey and Roysircar, <xref rid="B18" ref-type="bibr">2006</xref>), yet remain largely understudied in the face perception literature. Thus, this may reduce the bias present within face databases and encourage research on the social invisibility experienced by women from underrepresented groups (Neel and Lassetter, <xref rid="B32" ref-type="bibr">2019</xref>). This database may also facilitate research on the discrimination of individuals with intersecting race and gender identities (e.g., South Asian men). This is important as targets' race and gender can influence social perception (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>), leading to divergent ratings on dimensions of trustworthiness, attractiveness, and dominance (Strachan et al., <xref rid="B38" ref-type="bibr">2017</xref>; Zuckerman and Kieffer, <xref rid="B46" ref-type="bibr">1994</xref>). By providing a set of real faces, the TED face database also addresses the reliance on computer-generated stimuli due to a lack of databases containing ethnically diverse faces with variations in pose, gaze, and emotional expression. The artificiality of computer-generated faces minimizes the variability within identity characteristics leading to altered perceptions of faces (Chen et al., <xref rid="B7" ref-type="bibr">2021</xref>). Thus, researchers using TED can avoid such limitations by conducting experiments using ethnically diverse real faces.</p><p>Our main objective in developing the Toronto Ethnically Diverse (TED) face database was to create a large, ethnically diverse set of faces displaying different facial expressions and pose variations. The database contains 271 models with varying facial expressions, gaze directions, and poses available in color, offering researchers flexibility to combine TED stimuli with other facial expression databases. Diverse face databases of this nature may prove useful in examining psychological processes by providing representative stimuli that reflect the ethnicities of research participants and for testing questions specific to the effects of in-group vs. out-group membership on psychological processes. The TED face database is an open access tool that is available for free use to all academic researchers. By providing this tool, we hope to combat existing biases in the face perception literature and to contribute to advancing knowledge across the psychological literatures of face perception, impression formation, and intergroup relations.</p></sec></body><back><sec sec-type="data-availability" id="s5"><title>Data availability statement</title><p>The datasets presented in this study can be found in online repositories. The names of the repository/repositories and accession number(s) can be found in the article/<xref rid="s11" ref-type="sec">Supplementary material</xref>.</p></sec><sec sec-type="ethics-statement" id="s6"><title>Ethics statement</title><p>The studies involving humans were approved by Toronto Metropolitan University Research Ethics Board. The studies were conducted in accordance with the local legislation and institutional requirements. The participants provided their written informed consent to participate in this study. Written informed consent was obtained from the individual(s) for the publication of any potentially identifiable images or data included in this article.</p></sec><sec sec-type="author-contributions" id="s7"><title>Author contributions</title><p>ML: Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing. NS: Writing &#x02013; review &#x00026; editing. MO'H: Writing &#x02013; review &#x00026; editing. MM: Writing &#x02013; review &#x00026; editing.</p></sec><sec sec-type="COI-statement" id="conf1"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="ai-statement" id="s9"><title>Generative AI statement</title><p>The author(s) declare that no Gen AI was used in the creation of this manuscript.</p></sec><sec sec-type="disclaimer" id="s10"><title>Publisher's note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><sec sec-type="supplementary-material" id="s11"><title>Supplementary material</title><p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://osf.io/6vdn2/?view_only=14a1fcd1f3dd434bbfe6a4028fd96400" ext-link-type="uri">https://osf.io/6vdn2/?view_only=14a1fcd1f3dd434bbfe6a4028fd96400</ext-link></p></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bainbridge</surname><given-names>W. A.</given-names></name><name><surname>Isola</surname><given-names>P.</given-names></name><name><surname>Oliva</surname><given-names>A.</given-names></name></person-group> (<year>2013</year>). <article-title>The intrinsic memorability of face photographs</article-title>. <source>J. Exp. Psychol. Gen.</source>
<volume>142</volume>, <fpage>1323</fpage>&#x02013;<lpage>1334</lpage>. <pub-id pub-id-type="doi">10.1037/a0033872</pub-id><pub-id pub-id-type="pmid">24246059</pub-id>
</mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balas</surname><given-names>B.</given-names></name><name><surname>Pacella</surname><given-names>J.</given-names></name></person-group> (<year>2017</year>). <article-title>Trustworthiness perception is disrupted in artificial faces</article-title>. <source>Comput. Hum. Behav.</source>
<volume>77</volume>, <fpage>240</fpage>&#x02013;<lpage>248</lpage>. <pub-id pub-id-type="doi">10.1016/j.chb.2017.08.045</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barson</surname><given-names>J. V.</given-names></name></person-group> (<year>2003</year>). <article-title>Facial recognition technology (FERET)</article-title>. <source>Aviat. Space Environ. Med.</source>
<volume>74</volume>, <fpage>590</fpage>&#x02013;<lpage>591</lpage>.<pub-id pub-id-type="pmid">12751595</pub-id>
</mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beaupr&#x000e9;</surname><given-names>M. G.</given-names></name><name><surname>Hess</surname><given-names>U.</given-names></name></person-group> (<year>2005</year>). <article-title>Cross-cultural emotion recognition among Canadian Ethnic Groups</article-title>. <source>J. Cross-Cult. Psychol.</source>
<volume>36</volume>, <fpage>355</fpage>&#x02013;<lpage>370</lpage>. <pub-id pub-id-type="doi">10.1177/0022022104273656</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beaupr&#x000e9;</surname><given-names>M. G.</given-names></name><name><surname>Hess</surname><given-names>U.</given-names></name></person-group> (<year>2006</year>). <article-title>An ingroup advantage for confidence in emotion recognition judgments: the moderating effect of familiarity with the expressions of outgroup members</article-title>. <source>Pers. Soc. Psychol. Bull.</source>
<volume>32</volume>, <fpage>16</fpage>&#x02013;<lpage>26</lpage>. <pub-id pub-id-type="doi">10.1177/0146167205277097</pub-id><pub-id pub-id-type="pmid">16317185</pub-id>
</mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bieh</surname><given-names>M.</given-names></name><name><surname>Matsumoto</surname><given-names>D.</given-names></name><name><surname>Ekman</surname><given-names>P.</given-names></name><name><surname>Hearn</surname><given-names>V.</given-names></name><name><surname>Heider</surname><given-names>K.</given-names></name><name><surname>Kudoh</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>1997</year>). <article-title>Matsumoto and Ekman's Japanese and Caucasian Facial Expressions of Emotion (JACFEE): reliability data and cross-national differences</article-title>. <source>J. Nonverbal. Behav</source>. <volume>21</volume>:<fpage>3</fpage>. <pub-id pub-id-type="doi">10.1023/a:1024902500935</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J. M.</given-names></name><name><surname>Norman</surname><given-names>J. B.</given-names></name><name><surname>Nam</surname><given-names>Y.</given-names></name></person-group> (<year>2021</year>). <article-title>Broadening the stimulus set: introducing the American multiracial faces database</article-title>. <source>Behav. Res. Methods</source>
<volume>53</volume>, <fpage>371</fpage>&#x02013;<lpage>389</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-020-01447-8</pub-id><pub-id pub-id-type="pmid">32705658</pub-id>
</mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coffman</surname><given-names>M. C.</given-names></name><name><surname>Trubanova</surname><given-names>A.</given-names></name><name><surname>Richey</surname><given-names>J. A.</given-names></name><name><surname>White</surname><given-names>S. W.</given-names></name><name><surname>Kim-Spoon</surname><given-names>J.</given-names></name><name><surname>Ollendick</surname><given-names>T. H.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Validation of the NIMH-ChEFS adolescent face stimulus set in an adolescent, parent, and health professional sample</article-title>. <source>Int. J. Methods Psychiatr. Res.</source>
<volume>24</volume>, <fpage>275</fpage>&#x02013;<lpage>286</lpage>. <pub-id pub-id-type="doi">10.1002/mpr.1490</pub-id><pub-id pub-id-type="pmid">26359940</pub-id>
</mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J.</given-names></name></person-group> (<year>1960</year>). <article-title>A coefficient of agreement for nominal scales</article-title>. <source>Educ. Psychol. Meas.</source>
<volume>20</volume>, <fpage>37</fpage>&#x02013;<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1177/001316446002000104</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conley</surname><given-names>M. I.</given-names></name><name><surname>Dellarco</surname><given-names>D. V.</given-names></name><name><surname>Rubien-Thomas</surname><given-names>E.</given-names></name><name><surname>Cohen</surname><given-names>A. O.</given-names></name><name><surname>Cervera</surname><given-names>A.</given-names></name><name><surname>Tottenham</surname><given-names>N.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>The racially diverse affective expression (RADIATE) face stimulus set</article-title>. <source>Psychiatry Res.</source>
<volume>270</volume>, <fpage>1059</fpage>&#x02013;<lpage>1067</lpage>. <pub-id pub-id-type="doi">10.1016/j.psychres.2018.04.066</pub-id><pub-id pub-id-type="pmid">29910020</pub-id>
</mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalrymple</surname><given-names>K. A.</given-names></name><name><surname>Gomez</surname><given-names>J.</given-names></name><name><surname>Duchaine</surname><given-names>B.</given-names></name></person-group> (<year>2013</year>). <article-title>The dartmouth database of children's faces: acquisition and validation of a new face stimulus set</article-title>. <source>PLoS ONE</source>
<volume>8</volume>:<fpage>e79131</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0079131</pub-id><pub-id pub-id-type="pmid">24244434</pub-id>
</mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeBruine</surname><given-names>L.</given-names></name><name><surname>Jones</surname><given-names>B.</given-names></name></person-group> (<year>2017</year>). <source>Face Research Lab London Set</source>. Figshare Dataset. <pub-id pub-id-type="doi">10.6084/m9.figshare.5047666.v5</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebner</surname><given-names>N. C.</given-names></name><name><surname>Riediger</surname><given-names>M.</given-names></name><name><surname>Lindenberger</surname><given-names>U.</given-names></name></person-group> (<year>2010</year>). <article-title>FACES&#x02014;a database of facial expressions in young, middle-aged, and older women and men: development and validation</article-title>. <source>Behav. Res. Methods</source>
<volume>42</volume>, <fpage>351</fpage>&#x02013;<lpage>362</lpage>. <pub-id pub-id-type="doi">10.3758/BRM.42.1.351</pub-id><pub-id pub-id-type="pmid">20160315</pub-id>
</mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egger</surname><given-names>H. L.</given-names></name><name><surname>Pine</surname><given-names>D. S.</given-names></name><name><surname>Nelson</surname><given-names>E.</given-names></name><name><surname>Leibenluft</surname><given-names>E.</given-names></name><name><surname>Ernst</surname><given-names>M.</given-names></name><name><surname>Towbin</surname><given-names>K. E.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>The NIMH Child Emotional Faces Picture Set (NIMH-ChEFS): a new set of children's facial emotion stimuli</article-title>. <source>Int. J. Methods Psychiatr. Res.</source>
<volume>20</volume>, <fpage>145</fpage>&#x02013;<lpage>156</lpage>. <pub-id pub-id-type="doi">10.1002/mpr.343</pub-id><pub-id pub-id-type="pmid">22547297</pub-id>
</mixed-citation></ref><ref id="B15"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P.</given-names></name><name><surname>Friesen</surname><given-names>W. V.</given-names></name></person-group> (<year>1975</year>). <source>Unmasking the Face: A Guide to Recognizing Emotions from Facial Clues</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Prentice-Hall</publisher-name>.</mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erwin</surname><given-names>R. J.</given-names></name><name><surname>Gur</surname><given-names>R. C.</given-names></name><name><surname>Gur</surname><given-names>R. E.</given-names></name><name><surname>Skolnick</surname><given-names>B.</given-names></name><name><surname>Mawhinney-Hee</surname><given-names>M.</given-names></name><name><surname>Smailis</surname><given-names>J.</given-names></name></person-group> (<year>1992</year>). <article-title>Facial emotion discrimination: I. Task construction and behavioral findings in normal subjects</article-title>. <source>Psychiatry Res.</source>
<volume>42</volume>, <fpage>231</fpage>&#x02013;<lpage>240</lpage>. <pub-id pub-id-type="doi">10.1016/0165-1781(92)90115-J</pub-id><pub-id pub-id-type="pmid">1496055</pub-id>
</mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>French</surname><given-names>A. R.</given-names></name><name><surname>Franz</surname><given-names>T. M.</given-names></name><name><surname>Phelan</surname><given-names>L. L.</given-names></name><name><surname>Blaine</surname><given-names>B. E.</given-names></name></person-group> (<year>2013</year>). <article-title>Reducing muslim/arab stereotypes through evaluative conditioning</article-title>. <source>J. Soc. Psychol.</source>
<volume>153</volume>, <fpage>6</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1080/00224545.2012.706242</pub-id><pub-id pub-id-type="pmid">23421001</pub-id>
</mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frey</surname><given-names>L. L.</given-names></name><name><surname>Roysircar</surname><given-names>G.</given-names></name></person-group> (<year>2006</year>). <article-title>South Asian and East Asian international students' perceived prejudice, acculturation, and frequency of help resource utilization</article-title>. <source>J. Multicult. Couns. Devel.</source>
<volume>34</volume>, <fpage>208</fpage>&#x02013;<lpage>222</lpage>. <pub-id pub-id-type="doi">10.1002/j.2161-1912.2006.tb00040.x</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>W.</given-names></name><name><surname>Cao</surname><given-names>B.</given-names></name><name><surname>Shan</surname><given-names>S.</given-names></name><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Zhou</surname><given-names>D.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><etal/></person-group>. (<year>2008</year>). <article-title>The CAS-PEAL large-scale chinese face database and baseline evaluations</article-title>. <source>IEEE Trans. Syst. Man Cybern. A: Syst. Hum.</source>
<volume>38</volume>, <fpage>149</fpage>&#x02013;<lpage>161</lpage>. <pub-id pub-id-type="doi">10.1109/TSMCA.2007.909557</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>R.</given-names></name></person-group> (<year>2005</year>). <article-title>&#x0201c;Face databases,&#x0201d;</article-title> in <source>Handbook of Face Recognition</source>, eds. S. Z. Li, and A. K. Jain (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>), <fpage>301</fpage>&#x02013;<lpage>327</lpage>.</mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gruber</surname><given-names>J.</given-names></name><name><surname>Mendle</surname><given-names>J.</given-names></name><name><surname>Lindquist</surname><given-names>K. A.</given-names></name><name><surname>Schmader</surname><given-names>T.</given-names></name><name><surname>Clark</surname><given-names>L. A.</given-names></name><name><surname>Bliss-Moreau</surname><given-names>E.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>The future of women in psychological science</article-title>. <source>Perspect. Psychol. Sci.</source>
<volume>16</volume>, <fpage>483</fpage>&#x02013;<lpage>516</lpage>. <pub-id pub-id-type="doi">10.1177/1745691620952789</pub-id><pub-id pub-id-type="pmid">32901575</pub-id>
</mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hare</surname><given-names>T. A.</given-names></name><name><surname>Tottenham</surname><given-names>N.</given-names></name><name><surname>Davidson</surname><given-names>M. C.</given-names></name><name><surname>Glover</surname><given-names>G. H.</given-names></name><name><surname>Casey</surname><given-names>B. J.</given-names></name></person-group> (<year>2005</year>). <article-title>Contributions of amygdala and striatal activity in emotion regulation</article-title>. <source>Biol. Psychiatry</source>
<volume>57</volume>, <fpage>624</fpage>&#x02013;<lpage>632</lpage>. <pub-id pub-id-type="doi">10.1016/j.biopsych.2004.12.038</pub-id><pub-id pub-id-type="pmid">15780849</pub-id>
</mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakshmi</surname><given-names>A.</given-names></name><name><surname>Wittenbrink</surname><given-names>B.</given-names></name><name><surname>Correll</surname><given-names>J.</given-names></name><name><surname>Ma</surname><given-names>D. S.</given-names></name></person-group> (<year>2021</year>). <article-title>The india face set: international and cultural boundaries impact face impressions and perceptions of category membership</article-title>. <source>Front. Psychol.</source>
<volume>12</volume>:<fpage>627678</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2021.627678</pub-id><pub-id pub-id-type="pmid">33643159</pub-id>
</mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landis</surname><given-names>J. R.</given-names></name><name><surname>Koch</surname><given-names>G. G.</given-names></name></person-group> (<year>1977</year>). <article-title>An application of hierarchical kappa-type statistics in the assessment of majority agreement among multiple observers</article-title>. <source>Biometrics</source>
<volume>33</volume>, <fpage>363</fpage>&#x02013;<lpage>374</lpage>. <pub-id pub-id-type="doi">10.2307/2529786</pub-id><pub-id pub-id-type="pmid">884196</pub-id>
</mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langner</surname><given-names>O.</given-names></name><name><surname>Dotsch</surname><given-names>R.</given-names></name><name><surname>Bijlstra</surname><given-names>G.</given-names></name><name><surname>Wigboldus</surname><given-names>D. H. J.</given-names></name><name><surname>Hawk</surname><given-names>S. T.</given-names></name><name><surname>van Knippenberg</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>Presentation and validation of the Radboud Faces Database</article-title>. <source>Cogn. Emot.</source>
<volume>24</volume>, <fpage>1377</fpage>&#x02013;<lpage>1388</lpage>. <pub-id pub-id-type="doi">10.1080/02699930903485076</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Little</surname><given-names>A. C.</given-names></name><name><surname>Jones</surname><given-names>B. C.</given-names></name><name><surname>DeBruine</surname><given-names>L. M.</given-names></name></person-group> (<year>2011</year>). <article-title>Facial attractiveness: evolutionary based research</article-title>. <source>Philos. Trans. R Soc. Lond. B Biol. Sci.</source>
<volume>366</volume>, <fpage>1638</fpage>&#x02013;<lpage>1659</lpage>. <pub-id pub-id-type="doi">10.1098/rstb.2010.0404</pub-id><pub-id pub-id-type="pmid">21536551</pub-id>
</mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>D. S.</given-names></name><name><surname>Correll</surname><given-names>J.</given-names></name><name><surname>Wittenbrink</surname><given-names>B.</given-names></name></person-group> (<year>2015</year>). <article-title>The Chicago face database: a free stimulus set of faces and norming data</article-title>. <source>Behav. Res. Methods</source>
<volume>47</volume>, <fpage>1122</fpage>&#x02013;<lpage>1135</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-014-0532-5</pub-id><pub-id pub-id-type="pmid">25582810</pub-id>
</mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>D. S.</given-names></name><name><surname>Kantner</surname><given-names>J.</given-names></name><name><surname>Wittenbrink</surname><given-names>B.</given-names></name></person-group> (<year>2021</year>). <article-title>Chicago face database: multiracial expansion</article-title>. <source>Behav. Res. Methods</source>
<volume>53</volume>, <fpage>1289</fpage>&#x02013;<lpage>1300</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-020-01482-5</pub-id><pub-id pub-id-type="pmid">33037599</pub-id>
</mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mandal</surname><given-names>M. K.</given-names></name></person-group> (<year>1987</year>). <article-title>Decoding of facial emotions, in terms of expressiveness, by schizophrenics and depressives</article-title>. <source>Psychiatry</source>
<volume>50</volume>, <fpage>371</fpage>&#x02013;<lpage>376</lpage>. <pub-id pub-id-type="doi">10.1080/00332747.1987.11024368</pub-id><pub-id pub-id-type="pmid">3423162</pub-id>
</mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meissner</surname><given-names>C. A.</given-names></name><name><surname>Brigham</surname><given-names>J. C.</given-names></name></person-group> (<year>2001</year>). <article-title>Thirty years of investigating the own-race bias in memory for faces: a meta-analytic review</article-title>. <source>Psychol. Public Policy Law</source>
<volume>7</volume>, <fpage>3</fpage>&#x02013;<lpage>35</lpage>. <pub-id pub-id-type="doi">10.1037/1076-8971.7.1.3</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naples</surname><given-names>A.</given-names></name><name><surname>Nguyen-Phuc</surname><given-names>A.</given-names></name><name><surname>Coffman</surname><given-names>M.</given-names></name><name><surname>Kresse</surname><given-names>A.</given-names></name><name><surname>Faja</surname><given-names>S.</given-names></name><name><surname>Bernier</surname><given-names>R.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>A computer-generated animated face stimulus set for psychophysiological research</article-title>. <source>Behav. Res. Methods</source>
<volume>47</volume>, <fpage>562</fpage>&#x02013;<lpage>570</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-014-0491-x</pub-id><pub-id pub-id-type="pmid">25028164</pub-id>
</mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neel</surname><given-names>R.</given-names></name><name><surname>Lassetter</surname><given-names>B.</given-names></name></person-group> (<year>2019</year>). <article-title>The stigma of perceived irrelevance: an affordance-management theory of interpersonal invisibility</article-title>. <source>Psychol. Rev.</source>
<volume>126</volume>, <fpage>634</fpage>&#x02013;<lpage>659</lpage>. <pub-id pub-id-type="doi">10.1037/rev0000143</pub-id><pub-id pub-id-type="pmid">30688473</pub-id>
</mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palermo</surname><given-names>R.</given-names></name><name><surname>Coltheart</surname><given-names>M.</given-names></name></person-group> (<year>2004</year>). <article-title>Photographs of facial expression: accuracy, response times, and ratings of intensity</article-title>. <source>Behav. Res. Methods Instrum. Comput.</source>
<volume>36</volume>, <fpage>634</fpage>&#x02013;<lpage>638</lpage>. <pub-id pub-id-type="doi">10.3758/BF03206544</pub-id><pub-id pub-id-type="pmid">15641409</pub-id>
</mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pickron</surname><given-names>C. B.</given-names></name><name><surname>Brown</surname><given-names>A. J.</given-names></name><name><surname>Hudac</surname><given-names>C. M.</given-names></name><name><surname>Scott</surname><given-names>L. S.</given-names></name></person-group> (<year>2024</year>). <article-title>Diverse Face Images (DFI): Validated for racial representation and eye gaze</article-title>. <source>Behav. Res. Methods</source>
<volume>56</volume>, <fpage>8801</fpage>&#x02013;<lpage>8819</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-024-02504-2</pub-id><pub-id pub-id-type="pmid">39285143</pub-id>
</mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>J. A.</given-names></name></person-group> (<year>1994</year>). <article-title>Is there universal recognition of emotion from facial expression? A review of the cross-cultural studies</article-title>. <source>Psychol. Bull.</source>
<volume>115</volume>, <fpage>102</fpage>&#x02013;<lpage>141</lpage>. <pub-id pub-id-type="doi">10.1037/0033-2909.115.1.102</pub-id><pub-id pub-id-type="pmid">8202574</pub-id>
</mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sesko</surname><given-names>A. K.</given-names></name><name><surname>Biernat</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>Prototypes of race and gender: the invisibility of Black women</article-title>. <source>J. Exp. Soc. Psychol.</source>
<volume>46</volume>, <fpage>356</fpage>&#x02013;<lpage>360</lpage>. <pub-id pub-id-type="doi">10.1016/j.jesp.2009.10.016</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sim</surname><given-names>T.</given-names></name><name><surname>Baker</surname><given-names>S.</given-names></name><name><surname>Bsat</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>). <article-title>&#x0201c;The CMU pose, illumination, and expression (PIE) database,&#x0201d;</article-title> in <source>Proceedings of Fifth IEEE International Conference on Automatic Face Gesture Recognition</source> (<publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>53</fpage>&#x02013;<lpage>58</lpage>. <pub-id pub-id-type="doi">10.1109/AFGR.2002.1004130</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strachan</surname><given-names>J. W. A.</given-names></name><name><surname>Kirkham</surname><given-names>A. J.</given-names></name><name><surname>Manssuer</surname><given-names>L. R.</given-names></name><name><surname>Over</surname><given-names>H.</given-names></name><name><surname>Tipper</surname><given-names>S. P.</given-names></name></person-group> (<year>2017</year>). <article-title>Incidental learning of trust from eye-gaze: effects of race and facial trustworthiness</article-title>. <source>Vis. Cogn</source>. <volume>25</volume>, <fpage>802</fpage>&#x02013;<lpage>814</lpage>. <pub-id pub-id-type="doi">10.1080/13506285.2017.1338321</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strohminger</surname><given-names>N.</given-names></name><name><surname>Gray</surname><given-names>K.</given-names></name><name><surname>Chituc</surname><given-names>V.</given-names></name><name><surname>Heffner</surname><given-names>J.</given-names></name><name><surname>Schein</surname><given-names>C.</given-names></name><name><surname>Heagins</surname><given-names>T. B.</given-names></name></person-group> (<year>2016</year>). <article-title>The MR2: a multi-racial, mega-resolution database of facial stimuli</article-title>. <source>Behav. Res. Methods</source>
<volume>48</volume>, <fpage>1197</fpage>&#x02013;<lpage>1204</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-015-0641-9</pub-id><pub-id pub-id-type="pmid">26311590</pub-id>
</mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tottenham</surname><given-names>N.</given-names></name><name><surname>Tanaka</surname><given-names>J. W.</given-names></name><name><surname>Leon</surname><given-names>A. C.</given-names></name><name><surname>McCarry</surname><given-names>T.</given-names></name><name><surname>Nurse</surname><given-names>M.</given-names></name><name><surname>Hare</surname><given-names>T. A.</given-names></name><etal/></person-group>. (<year>2009</year>). <article-title>The NimStim set of facial expressions: judgments from untrained research participants</article-title>. <source>Psychiatry Res.</source>
<volume>168</volume>, <fpage>242</fpage>&#x02013;<lpage>249</lpage>. <pub-id pub-id-type="doi">10.1016/j.psychres.2008.05.006</pub-id><pub-id pub-id-type="pmid">19564050</pub-id>
</mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Schalk</surname><given-names>J.</given-names></name><name><surname>Hawk</surname><given-names>S. T.</given-names></name><name><surname>Fischer</surname><given-names>A. H.</given-names></name><name><surname>Doosje</surname><given-names>B.</given-names></name></person-group> (<year>2011</year>). <article-title>Moving faces, looking places: validation of the amsterdam dynamic facial expression set (ADFES)</article-title>. <source>Emotion</source>
<volume>11</volume>, <fpage>907</fpage>&#x02013;<lpage>920</lpage>. <pub-id pub-id-type="doi">10.1037/a0023853</pub-id><pub-id pub-id-type="pmid">21859206</pub-id>
</mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vetter</surname><given-names>T.</given-names></name><name><surname>Walker</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <source>Computer-Generated Images in Face Perception</source>. Oxford: Oxford University Press. <pub-id pub-id-type="doi">10.1093/oxfordhb/9780199559053.013.0020</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Markham</surname><given-names>R.</given-names></name></person-group> (<year>1999</year>). <article-title>The development of a series of photographs of chinese facial expressions of emotion</article-title>. <source>J. Cross Cult. Psychol.</source>
<volume>30</volume>, <fpage>397</fpage>&#x02013;<lpage>410</lpage>. <pub-id pub-id-type="doi">10.1177/0022022199030004001</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Xu</surname><given-names>Z.</given-names></name><name><surname>Cui</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Ouyang</surname><given-names>C.</given-names></name></person-group> (<year>2017</year>). <article-title>Does a big Duchenne smile really matter on e-commerce websites? An eye-tracking study in China</article-title>. <source>Electron. Commer. Res.</source>
<volume>17</volume>, <fpage>609</fpage>&#x02013;<lpage>626</lpage>. <pub-id pub-id-type="doi">10.1007/s10660-016-9237-4</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zebrowitz</surname><given-names>L. A.</given-names></name><name><surname>Kikuchi</surname><given-names>M.</given-names></name><name><surname>Fellous</surname><given-names>J.-M.</given-names></name></person-group> (<year>2010</year>). <article-title>Facial resemblance to emotions: group differences, impression effects, and race stereotypes</article-title>. <source>J. Pers. Soc. Psychol.</source>
<volume>98</volume>, <fpage>175</fpage>&#x02013;<lpage>189</lpage>. <pub-id pub-id-type="doi">10.1037/a0017990</pub-id><pub-id pub-id-type="pmid">20085393</pub-id>
</mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuckerman</surname><given-names>M.</given-names></name><name><surname>Kieffer</surname><given-names>S. C.</given-names></name></person-group> (<year>1994</year>). <article-title>Race differences in face-ism: does facial prominence imply dominance?</article-title>
<source>J. Pers. Soc. Psychol.</source>
<volume>66</volume>, <fpage>86</fpage>&#x02013;<lpage>92</lpage>. <pub-id pub-id-type="doi">10.1037/0022-3514.66.1.86</pub-id></mixed-citation></ref></ref-list></back></article>