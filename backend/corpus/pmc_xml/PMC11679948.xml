<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11679948</article-id><article-id pub-id-type="doi">10.3390/s24248020</article-id><article-id pub-id-type="publisher-id">sensors-24-08020</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Depth Prediction Improvement for Near-Field iToF Lidar in Low-Speed Motion State</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5375-9510</contrib-id><name><surname>Nagiub</surname><given-names>Mena</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-24-08020" ref-type="aff">1</xref><xref rid="af2-sensors-24-08020" ref-type="aff">2</xref><xref rid="af3-sensors-24-08020" ref-type="aff">3</xref><xref rid="c1-sensors-24-08020" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Beuth</surname><given-names>Thorsten</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af4-sensors-24-08020" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><name><surname>Sistu</surname><given-names>Ganesh</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-24-08020" ref-type="aff">2</xref><xref rid="af3-sensors-24-08020" ref-type="aff">3</xref><xref rid="af5-sensors-24-08020" ref-type="aff">5</xref></contrib><contrib contrib-type="author"><name><surname>Gotzig</surname><given-names>Heinrich</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af6-sensors-24-08020" ref-type="aff">6</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8383-2635</contrib-id><name><surname>Eising</surname><given-names>Ciar&#x000e1;n</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-24-08020" ref-type="aff">2</xref><xref rid="af3-sensors-24-08020" ref-type="aff">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Stateczny</surname><given-names>Andrzej</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-24-08020"><label>1</label>Department of Front Camera, Valeo Schalter und Sensoren GmbH, 74321 Bietigheim-Bissingen, Germany</aff><aff id="af2-sensors-24-08020"><label>2</label>Department of Electronic &#x00026; Computer Engineer, University of Limerick, V94 T9PX Limerick, Ireland; <email>ganesh.sistu@valeo.com</email> (G.S.); <email>ciaran.eising@ul.ie</email> (C.E.)</aff><aff id="af3-sensors-24-08020"><label>3</label>Data-Driven Computer Engineering (D2iCE) Research Centre, University of Limerick, V94 T9PX Limerick, Ireland</aff><aff id="af4-sensors-24-08020"><label>4</label>Department of Detection Systems, Valeo Detection Systems GmbH, 74321 Bietigheim-Bissingen, Germany; <email>thorsten.beuth@valeo.com</email></aff><aff id="af5-sensors-24-08020"><label>5</label>Department of Computer Vision, Valeo Vision Systems, NW1 186G Galway, Ireland</aff><aff id="af6-sensors-24-08020"><label>6</label>Deptartment of Driving Assistance, Valeo Schalter und Sensoren GmbH, 74321 Bietigheim-Bissingen, Germany; <email>heinrich.gotzig@valeo.com</email></aff><author-notes><corresp id="c1-sensors-24-08020"><label>*</label>Correspondence: <email>mena.nagiub@valeo.com</email></corresp></author-notes><pub-date pub-type="epub"><day>16</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2024</year></pub-date><volume>24</volume><issue>24</issue><elocation-id>8020</elocation-id><history><date date-type="received"><day>22</day><month>10</month><year>2024</year></date><date date-type="rev-recd"><day>04</day><month>12</month><year>2024</year></date><date date-type="accepted"><day>12</day><month>12</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 by the authors.</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Current deep learning-based phase unwrapping techniques for iToF Lidar sensors focus mainly on static indoor scenarios, ignoring motion blur in dynamic outdoor scenarios. Our paper proposes a two-stage semi-supervised method to unwrap ambiguous depth maps affected by motion blur in dynamic outdoor scenes. The method trains on static datasets to learn unwrapped depth map prediction and then adapts to dynamic datasets using continuous learning methods. Additionally, blind deconvolution is introduced to mitigate the blur. The combined use of these methods produces high-quality depth maps with reduced blur noise.</p></abstract><kwd-group><kwd>near field</kwd><kwd>Lidar</kwd><kwd>iTOF</kwd><kwd>depth correction</kwd><kwd>estimation</kwd><kwd>ambiguity</kwd><kwd>blurriness</kwd><kwd>motion blur</kwd><kwd>blind deconvolution</kwd><kwd>continuous learning</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-24-08020"><title>1. Introduction</title><p>Automated driving systems face challenges in low-speed scenarios [<xref rid="B1-sensors-24-08020" ref-type="bibr">1</xref>] due to the combination of static and slow-moving objects and crowded scenes, which complicates perception [<xref rid="B2-sensors-24-08020" ref-type="bibr">2</xref>]. Sparse sensors like ultrasonic and short-range radars handle tasks like parking, while fisheye cameras provide broader views with dense depth data, requiring additional depth estimation [<xref rid="B3-sensors-24-08020" ref-type="bibr">3</xref>,<xref rid="B4-sensors-24-08020" ref-type="bibr">4</xref>]. Sensor fusion merges sparse and dense data, improving perception in such scenarios [<xref rid="B3-sensors-24-08020" ref-type="bibr">3</xref>,<xref rid="B5-sensors-24-08020" ref-type="bibr">5</xref>].</p><p>Emerging solutions like Indirect Time-Of-Flight (iTOF) lidar, such as Valeo&#x02019;s Near-Field Lidars (NFLs) [<xref rid="B6-sensors-24-08020" ref-type="bibr">6</xref>], offer dense depth information essential for low-speed driving. The Amplitude-Modulated Continuous Wave (AMCW)-based lidar [<xref rid="B7-sensors-24-08020" ref-type="bibr">7</xref>] paired with CMOS 2D imagers [<xref rid="B8-sensors-24-08020" ref-type="bibr">8</xref>,<xref rid="B9-sensors-24-08020" ref-type="bibr">9</xref>] generates dense short-range point clouds, addressing key requirements.</p><p>However, iTOF technology faces depth wrapping issues, where depth measurements are constrained to specific depth cycles, creating ambiguous depth maps. This problem worsens in dynamic environments due to motion blur, which introduces noise and increases depth ambiguity. Addressing depth ambiguity and motion blur together is critical for advancing low-speed driving perception.</p><sec id="sec1dot1-sensors-24-08020"><title>1.1. Contributions</title><p>State-of-the-art deep learning methods address depth unwrapping for static indoor scenes, where motion blur is not a concern. In contrast, other methods tackle motion blur in iTOF sensors without addressing depth unwrapping. However, motion blur significantly affects depth measurements; moving objects leave trailing traces at various depth locations, increasing ambiguity. This highlights a strong link between solving motion blur and efficient depth unwrapping. Notably, no existing approach addresses both depth wrapping and motion blur in a single task.</p><p>Our paper presents several contributions:<list list-type="bullet"><list-item><p>We propose a new method to unwrap iTOF depth maps during motion and reduce motion blur artifacts, serving as a benchmark for future research. This method extends the NFL sensor depth range to the third and fourth depth cycles, enabling coverage up to 25 m, surpassing current methods limited to the second depth cycle.</p></list-item><list-item><p>We introduce our new Atrous Spatial Inception Pyramid Module (ASIPM) to estimate the blur transformation of vertices for the overall context based on the study conducted by Huo et al. [<xref rid="B10-sensors-24-08020" ref-type="bibr">10</xref>], a Visual Attention Network to estimate the blur transformation of vertices per specific objects, and an Inverse Blur Tensor Module to compensate for the effect of motion blur efficiently.</p></list-item><list-item><p>Unlike existing approaches that require ambient light frames, our method exclusively uses laser measurements, making it ideal for visually challenging environments like smoke, dust, or darkness.</p></list-item></list></p></sec><sec id="sec1dot2-sensors-24-08020"><title>1.2. Paper Structure</title><p>Our paper is structured as follows: We begin with an introduction to the problem&#x02019;s environment in <xref rid="sec1-sensors-24-08020" ref-type="sec">Section 1</xref>. Then, in <xref rid="sec2-sensors-24-08020" ref-type="sec">Section 2</xref>, we explain the problem and how motion complicates depth wrapping by introducing motion blur artifacts to the depth map. <xref rid="sec3-sensors-24-08020" ref-type="sec">Section 3</xref> explores the research already conducted to solve these problems. Following that, in <xref rid="sec4-sensors-24-08020" ref-type="sec">Section 4</xref>, we explain our method to resolve the problems using a computer vision model. <xref rid="sec5-sensors-24-08020" ref-type="sec">Section 5</xref> presents our method&#x02019;s results, comparing them to those of other state-of-the-art methods. Finally, in <xref rid="sec6-sensors-24-08020" ref-type="sec">Section 6</xref>, we conclude and comment on the results.</p></sec></sec><sec id="sec2-sensors-24-08020"><title>2. Problem Description</title><p>The iTOF AMCW Lidar measures object distances by calculating the phase shift angle between transmitted and received laser signals [<xref rid="B9-sensors-24-08020" ref-type="bibr">9</xref>]. This is achieved using multiple Differential Correlation Samples (DCSs) captured at varying phases, e.g., <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mo>&#x02218;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msup><mml:mn>90</mml:mn><mml:mo>&#x02218;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msup><mml:mn>180</mml:mn><mml:mo>&#x02218;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msup><mml:mn>270</mml:mn><mml:mo>&#x02218;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. The resulting phase shift angles matrix corresponds to the depth of each point in the point cloud. The depth <italic toggle="yes">d</italic> is calculated using (<xref rid="FD1-sensors-24-08020" ref-type="disp-formula">1</xref>) based on the phase shift angle <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c6;</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the amplitude modulation frequency <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. When <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c6;</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is within 0 to <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the depth measurement is within the unambiguous range <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>O</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. However, angles exceeding <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are wrapped due to the sinusoidal nature of the phase shift, leading to a phase-wrapping effect that creates ambiguous depth maps.
<disp-formula id="FD1-sensors-24-08020"><label>(1)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>O</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#x000b7;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>&#x003c6;</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Multi-modulation frequency methods [<xref rid="B11-sensors-24-08020" ref-type="bibr">11</xref>] have been developed to unwrap ambiguous depth maps accurately. This technique captures groups of DCS frames at different modulation frequencies, typically using two frequencies. The resulting ambiguous depth maps are then compared to produce a final unwrapped depth map.</p><p>However, this method has drawbacks: it increases the active time of laser components, leading to potential overheating and reduced lifespan, and it raises the power consumption of the sensor device.</p><p>In iTOF sensors, motion causes the time-shifting of DCS frames, leading to motion blur in depth maps [<xref rid="B8-sensors-24-08020" ref-type="bibr">8</xref>,<xref rid="B9-sensors-24-08020" ref-type="bibr">9</xref>,<xref rid="B12-sensors-24-08020" ref-type="bibr">12</xref>,<xref rid="B13-sensors-24-08020" ref-type="bibr">13</xref>]. Unlike traditional camera motion blur, which results from light integration during long exposures [<xref rid="B10-sensors-24-08020" ref-type="bibr">10</xref>,<xref rid="B14-sensors-24-08020" ref-type="bibr">14</xref>], iTOF motion blur arises from the sensor&#x02019;s operational principle [<xref rid="B15-sensors-24-08020" ref-type="bibr">15</xref>].</p><p>The sensor reconstructs depth maps by correlating multiple time-shifted DCS frames. In dynamic scenes, objects may change position between frames, causing the reconstructed point cloud to include trailing traces and motion blur. <xref rid="sensors-24-08020-f001" ref-type="fig">Figure 1</xref> comprehensively explains the related motion blur case in iTOF sensors. <xref rid="sensors-24-08020-f002" ref-type="fig">Figure 2</xref> illustrates this phenomenon and shows examples from our recorded dataset of these issues, highlighting a known limitation of iTOF sensors. Addressing this problem is critical for accurate depth correction.</p><p>The sensor&#x02019;s CMOS imager captures grayscale images using ambient light alongside DCS frames. However, because the frames are captured sequentially in different time slots, time shifts may occur between the DCS frames and the grayscale image.</p><p>When the sensor or environment is in motion, the perspective viewport of the imager causes non-uniform spatial transformations of objects. This leads to spatial and dimensional changes when comparing the ambiguous depth frame with the grayscale frame. <xref rid="sensors-24-08020-f003" ref-type="fig">Figure 3</xref> illustrates this phenomenon.</p><p>Non-uniform temporal and spatial transformations complicate the prediction of the unwrapped depth map from the grayscale frame. <xref rid="sensors-24-08020-f004" ref-type="fig">Figure 4</xref> provides examples of these transformations from the dataset.</p></sec><sec id="sec3-sensors-24-08020"><title>3. Related Works</title><sec id="sec3dot1-sensors-24-08020"><title>3.1. Motion Blur Noise Removal</title><p>The primary issue addressed in our method is motion blur. Gao et al. [<xref rid="B15-sensors-24-08020" ref-type="bibr">15</xref>] studied motion blur in iToF sensors, categorizing it into half-frame (two out of four DCS) and full-frame blur (four out of four DCS), and proposed a mathematical model for correction. While their method effectively removes motion blur, it does not address the depth wrapping problem simultaneously. Their approach, using a 40 MHz modulation frequency, can remove blur for objects within a range of 1 m to 2 m, which corresponds to the maximum unambiguous range for this frequency, as per Equation (<xref rid="FD1-sensors-24-08020" ref-type="disp-formula">1</xref>).</p><p>Huo et al. [<xref rid="B10-sensors-24-08020" ref-type="bibr">10</xref>] proposed a motion blur correction method using blind non-uniform motion deblurring with an Atrous Spatial Pyramid Deformable Convolution kernel. This kernel, based on residual inception [<xref rid="B16-sensors-24-08020" ref-type="bibr">16</xref>], includes four modulation kernels with varying dilation rates and offset maps. Although their method is designed for compensating prolonged exposure time in traditional cameras, their concept inspired the development of our Atrous Spatial Inception Pyramid Module (ASIPM) and the Visual Attention Network, as detailed in <xref rid="sec4dot1dot2-sensors-24-08020" ref-type="sec">Section 4.1.2</xref>.</p><p>Chang et al. [<xref rid="B17-sensors-24-08020" ref-type="bibr">17</xref>] addressed motion blur and light saturation noise using a recurrent architecture designed for traditional cameras. However, this method is not suitable for iTOF sensors, as the motion blur noise in iTOF sensors differs from that in traditional cameras, as explained in <xref rid="sec2-sensors-24-08020" ref-type="sec">Section 2</xref>.</p><p>Argaw et al. [<xref rid="B18-sensors-24-08020" ref-type="bibr">18</xref>] and Zhang et al. [<xref rid="B19-sensors-24-08020" ref-type="bibr">19</xref>] addressed motion blur in traditional cameras using optical flow estimation and the correction of low Signal-to-Noise Ratio (SNR) sharp images paired with high-SNR blurred, respectively. These methods rely on fusing a sharp low-SNR image with a blurred high-SNR image. Optical flow compensates for motion blur by adjusting for transformations in the opposite direction of the flow. However, this approach does not work for iTOF sensors, where the optical flow cannot compensate for motion blur due to the correlation of the four DCS frames creating the ambiguous depth map. Despite this, these methods influenced our approach, using grayscale as a sharp, low-SNR source and the ambiguous depth map as a high-SNR blurred source.</p><p>The presented methods are designed for 2D frames from traditional cameras, whereas our method focuses on 3D motion blur reduction using sharp grayscale images and ambiguous depth maps. We train the model on both static and dynamic datasets. While prior methods target 2D images, our solution employs an inverse blur block to address motion blur along the X and Y axes and resolves z-axis blur through phase unwrapping, effectively transforming the 3D blurring problem into a simpler 2D context.</p></sec><sec id="sec3dot2-sensors-24-08020"><title>3.2. Phase Unwrapping in Motion State</title><p>We address the challenge of phase unwrapping in iTOF Lidar systems, where ambiguity often obscures point cloud details across multiple cycles. While existing methods effectively unwrap ambiguous depth maps in static scenarios [<xref rid="B20-sensors-24-08020" ref-type="bibr">20</xref>,<xref rid="B21-sensors-24-08020" ref-type="bibr">21</xref>], they are unsuitable for motion states where motion blur introduces additional noise. Our proposed method specifically targets phase unwrapping under these dynamic conditions.</p><p>Wang et al. [<xref rid="B22-sensors-24-08020" ref-type="bibr">22</xref>] used their VNet model, based on the UNet architecture, to unwrap ambiguous depth phases in Fringe Projection Profilometry (FPP), a Lidar technology different from AMCW iTOF Lidar. Similarly, Qiao et al. [<xref rid="B23-sensors-24-08020" ref-type="bibr">23</xref>] proposed an architecture for noise removal and depth map unwrapping in static scenes. Schelling et al. [<xref rid="B24-sensors-24-08020" ref-type="bibr">24</xref>] accurately predicted optical flow from iTOF Lidar during motion but limited their approach to the first cycle of unwrapped depth maps. All these methods are tailored for indoor scenes only.</p><p>Jung et al. [<xref rid="B21-sensors-24-08020" ref-type="bibr">21</xref>] proposed the <italic toggle="yes">Wild ToFu</italic> method, which combines grayscale and DCS frames to produce unwrapped depth maps. However, it requires both inputs, making it unsuitable for night vision. Unlike traditional cameras, Lidar uses laser pulses to function in visually challenging environments such as smoke, dust, or darkness, where visible light fails [<xref rid="B25-sensors-24-08020" ref-type="bibr">25</xref>]. Additionally, <italic toggle="yes">Wild ToFu</italic> is designed for static indoor scenes, while our method targets outdoor scenes, addressing challenges like varied lighting, dynamic elements, large-scale settings, and complex geometry [<xref rid="B26-sensors-24-08020" ref-type="bibr">26</xref>].</p><p>Our method unwraps phases beyond the third cycle during motion while mitigating motion blur noise. It operates without requiring grayscale images, making it ideal for low-light scenarios such as night driving and tunnels. Furthermore, it can function with just two DCS frames, reducing stress on laser components.</p></sec><sec id="sec3dot3-sensors-24-08020"><title>3.3. Improving Model Predictions Using Continuous Learning</title><p>Our semi-supervised learning method begins by training the model on static scenes to leverage their sharpness, enabling it to predict unwrapped sharp depth maps from ambiguous ones. Subsequently, the model uses this learned ability to predict sharp depth maps from ambiguous, motion-blurred depth maps in dynamic scenes.</p><p>Continuous learning techniques support this approach by starting with a simple dataset and gradually progressing to more complex ones. While effective, continuous learning often encounters the challenge of catastrophic forgetting. Wang et al. [<xref rid="B27-sensors-24-08020" ref-type="bibr">27</xref>] addressed this issue by training small parameter sets, known as prompts, to enhance continuous learning. Rehearsal-based continuous learning, which retains copies of old datasets, can be demanding. To overcome this, Smith et al. [<xref rid="B28-sensors-24-08020" ref-type="bibr">28</xref>] proposed a rehearsal-free method using knowledge distillation and parameter regularization.</p><p>Our approach addresses catastrophic forgetting using knowledge distillation, combining multiple methods: the rehearsal technique, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">L</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> regularization, and model regularization through dropout.</p></sec></sec><sec id="sec4-sensors-24-08020"><title>4. Depth Unwrapping with Motion Deblurring Method</title><p>Our proposed method addresses three simultaneous challenges: unwrapping depth map phases during motion, mitigating motion blur artifacts, and handling time shifts across frames. Given this complexity, we provide a formal background to lay the foundation for the design of our solution.</p><sec id="sec4dot1-sensors-24-08020"><title>4.1. Method Formal Background</title><sec id="sec4dot1dot1-sensors-24-08020"><title>4.1.1. Handling Grayscale Time Shift</title><p>The grayscale image is captured with a time difference from the ambiguity and ground truth depth maps. We utilize spherical coordinates to simplify the mathematical representations. This time shift induces non-uniform spatial transformations for grayscale pixels, approximated as an affine transformation matrix. The Spatial Transformer Network (STN) [<xref rid="B29-sensors-24-08020" ref-type="bibr">29</xref>] addresses this issue by training the network through unsupervised learning to predict the inverse affine transformation matrix, thereby correcting pixel transformations. We represent the time shift in the grayscale image as follows:<disp-formula id="FD2-sensors-24-08020"><label>(2)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">f</italic> represents the grayscale image without affine transformation, <italic toggle="yes">g</italic> is the time-shifted grayscale image captured by the sensor, and <italic toggle="yes">A</italic> is the affine matrix representing non-uniform transformation due to time shift. Equation (3a,b) show how the inverse matrix <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is derived, ensuring that the final corrected grayscale image, denoted by <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, is used as input for the depth unwrapping network.
<disp-formula id="FD3a-sensors-24-08020"><label>(3a)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD3b-sensors-24-08020"><label>(3b)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Our model utilizes the STN network to predict the inverse affine transformation matrix <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> used to correct the grayscale image time shift [<xref rid="B29-sensors-24-08020" ref-type="bibr">29</xref>].</p></sec><sec id="sec4dot1dot2-sensors-24-08020"><title>4.1.2. Handling Ambiguous Depth Map Blurriness</title><p>Equation (<xref rid="FD4-sensors-24-08020" ref-type="disp-formula">4</xref>) defines motion blur as a convolution between a time-based sequence of blur kernels and the original image with additional noise [<xref rid="B30-sensors-24-08020" ref-type="bibr">30</xref>,<xref rid="B31-sensors-24-08020" ref-type="bibr">31</xref>].
<disp-formula id="FD4-sensors-24-08020"><label>(4)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <italic toggle="yes">f</italic> represents the original image without motion blur, <italic toggle="yes">k</italic> is the unknown blur kernels tensor, <italic toggle="yes">g</italic> is the image with motion blur noise, and <italic toggle="yes">n</italic> is the additive Gaussian noise. Equation (<xref rid="FD5-sensors-24-08020" ref-type="disp-formula">5</xref>) defines the blur effect in the frequency domain.
<disp-formula id="FD5-sensors-24-08020"><label>(5)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>F</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <italic toggle="yes">G</italic> represents the motion-blurred image in the frequency domain, <italic toggle="yes">K</italic> denotes the motion blur kernel in the frequency domain, <italic toggle="yes">F</italic> is the sharp image in the frequency domain, and <italic toggle="yes">N</italic> is additive Gaussian noise in the frequency domain. Balancing the removal of blur and the preservation of image details is important when designing anti-blurring filters. Gaussian noise typically manifests as high-frequency variations in pixel values, and anti-blurring filters often aim to enhance high-frequency components while attenuating low-frequency components associated with blur. Since it is not required to keep Gaussian noise, then we can simplify Equation (<xref rid="FD5-sensors-24-08020" ref-type="disp-formula">5</xref>) by neglecting Gaussian noise; hence, we can use Equation (<xref rid="FD6-sensors-24-08020" ref-type="disp-formula">6</xref>) [<xref rid="B31-sensors-24-08020" ref-type="bibr">31</xref>].
<disp-formula id="FD6-sensors-24-08020"><label>(6)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Using blind deconvolution [<xref rid="B31-sensors-24-08020" ref-type="bibr">31</xref>], we try to find the term <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, representing all optical flow transformation kernels per pixel to reverse the blur effect according to Equation (<xref rid="FD7-sensors-24-08020" ref-type="disp-formula">7</xref>).
<disp-formula id="FD7-sensors-24-08020"><label>(7)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>&#x000b7;</mml:mo><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Mathematically, it is challenging to calculate <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> due to the non-invertible nature of the convolution operator. We propose a deep learning model that uses Equation (<xref rid="FD8-sensors-24-08020" ref-type="disp-formula">8</xref>) to estimate <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> such that it minimizes the difference between the predicted sharp image <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the actual sharp image <italic toggle="yes">f</italic>.
<disp-formula id="FD8-sensors-24-08020"><label>(8)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02217;</mml:mo><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Equation (<xref rid="FD9-sensors-24-08020" ref-type="disp-formula">9</xref>) presents the optimization problem, where <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula> is the deep learning model predicting the final depth image.
<disp-formula id="FD9-sensors-24-08020"><label>(9)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02248;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Equation (<xref rid="FD10-sensors-24-08020" ref-type="disp-formula">10</xref>) illustrates how the model predicts the final depth map from the motion-blurred image, using an unsupervised approach to predict the inverse blur kernel <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the ground truth image.
<disp-formula id="FD10-sensors-24-08020"><label>(10)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02217;</mml:mo><mml:mi>g</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02248;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Our ASIPM, the Visual Attention Network, and the inverse blur tensor modules represent the model <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula> used to predict the inverse motion blur tensor. As illustrated in <xref rid="sensors-24-08020-f005" ref-type="fig">Figure 5</xref>, the ASIPM module operates on the ambiguous depth map; it aims to extract features about the optical flow of vertices of the point cloud in a general frame context. On the other hand, the visual attention network takes the ambiguous depth map as input. It implements a simple visual attention mechanism for the input to extract the optical flow of point cloud vertices on the object&#x02019;s scope. The inverse blur tensor module inputs the concatenation of the output of the visual attention and ASIPM modules. It predicts the tensor of three kernels representing the inverse optical flow features for the point cloud vertices.</p></sec><sec id="sec4dot1dot3-sensors-24-08020"><title>4.1.3. Phase Unwrapping in the Dynamic State</title><p>Current depth correction techniques for AMCW sensors primarily address static datasets [<xref rid="B32-sensors-24-08020" ref-type="bibr">32</xref>] (i.e., where the sensor and the scene&#x02019;s contents are not moving at all), overlooking dynamic scenarios (i.e., where the sensor and the scene&#x02019;s contents are moving) where motion blur impacts depth maps. State-of-the-art models can accurately predict sharp depth maps in static scenarios, such as when a vehicle is parked inside a garage, where there is no motion for the vehicle or the other objects in the scene.</p><p>We hypothesize that if the model is trained to accurately predict sharp unwrapped depth maps from static scenes with no motion blur, then using continuous learning, we can build on this skill to make it able to predict unwrapped depth maps with reduced motion blur from dynamic scenes.</p><p>Our proposed model&#x02019;s prediction accuracy can be represented by multivariate probability density functions that define the relationship between the input ambiguous depth map and the ground truth unwrapped depth map. To achieve accurate prediction, we aim to make the likelihood of predicting a sharp, unwrapped depth map from a dynamic scene similar to predicting a sharp, unwrapped depth map from a static scene. Let the probability density function <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represent prediction accuracy in the static dataset <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> by capturing the correspondence between the static input <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> representing the static scene ambiguous depth map and the corresponding prediction space <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> representing the ground truth sharp unwrapped depth map. Similarly, <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents prediction accuracy in the dynamic dataset <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> by capturing the relationship between the dynamic input <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> representing the dynamic scene ambiguous depth map and the dynamic prediction space <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> representing the ground truth sharp unwrapped depth map. We can express the model&#x02019;s prediction in a static state using Bayes&#x02019; theorem:<disp-formula id="FD11-sensors-24-08020"><label>(11)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the joint probability of predicting the ground truth static depth map <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and using the static input ambiguous depth map <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the likelihood that the static input <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> would result in predicting the ground truth static depth map <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the marginal accuracy of the model to predict the static ground truth depth map <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, regardless of which input <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is used. Similarly, the model&#x02019;s prediction in a dynamic state can be expressed as follows:<disp-formula id="FD12-sensors-24-08020"><label>(12)</label><mml:math id="mm52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the joint probability of predicting the ground truth dynamic depth map <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and using the dynamic input ambiguous depth map <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the likelihood that the dynamic input <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> would result in predicting the ground truth dynamic depth map <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the marginal accuracy of the model to predict the ground truth dynamic depth map <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, regardless of which input <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is used.</p><p>If the prediction accuracy of the probability density function in the dynamic dataset closely resembles that of the static dataset, and assuming the accuracy of predicted ground truth sharp unwrapped static depth maps <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is similar to that of predicted ground truth sharp unwrapped dynamic depth maps <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, then the model employed for precise predictions in the static dataset could also be effective for the dynamic dataset. This assumption is bold since having two statistical models dependent on different input variables but can give similar prediction accuracy is challenging. However, this assumption holds when the dynamic state model is trained with random batches from the static dataset and the vehicle moves at low speeds, minimizing time shifts within the range of <bold>0</bold> km/h to <bold>30</bold> km/h, as typically seen in parking scenarios. This technique ensures that a similarity condition is employed for the two probability density functions. This condition is essential for the assumption to hold [<xref rid="B33-sensors-24-08020" ref-type="bibr">33</xref>]. Equation (13a&#x02013;e) show our mathematical induction that inputs from the dynamic dataset can be used to predict sharp unwrapped depth maps with an accuracy approximately equivalent to that of the static dataset.
<disp-formula id="FD13a-sensors-24-08020"><label>(13a)</label><mml:math id="mm64" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mo lspace="0pt">:</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02248;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD13b-sensors-24-08020"><label>(13b)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mo lspace="0pt">:</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02243;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD13c-sensors-24-08020"><label>(13c)</label><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x02235;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD13d-sensors-24-08020"><label>(13d)</label><mml:math id="mm67" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x02235;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD13e-sensors-24-08020"><label>(13e)</label><mml:math id="mm68" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x02234;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02248;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>We aim to achieve equivalence between the likelihood of accurate prediction for the dynamic depth map <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from the input <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and accurate prediction for the static depth map <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from the input <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD14-sensors-24-08020"><label>(14)</label><mml:math id="mm73" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02243;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Our hypothesis, as described in <xref rid="sec4dot1dot3-sensors-24-08020" ref-type="sec">Section 4.1.3</xref>, is that this assumption can hold if we can make the continuous learning able to equalize the likelihood of accurately predicting <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from input <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with that of accurately predicting <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from input <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This equilibrium can be realized if the static deep learning model <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> can produce similar predictions to the dynamic deep learning model <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The model <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> should be trained using the dataset <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to achieve this equilibrium, where <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the function correlating the original static dataset <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to the dynamic dataset <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Notably, <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02286;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> ensures that learning from the dynamic dataset does not cause catastrophic forgetting of the static dataset; refer to <xref rid="sec3dot3-sensors-24-08020" ref-type="sec">Section 3.3</xref> to understand the approach.</p></sec></sec><sec id="sec4dot2-sensors-24-08020"><title>4.2. Method Implementation Details</title><p>The proposed solution integrates a continuous learning framework for adapting from static to dynamic datasets and a computer vision model for depth unwrapping in dynamic scenes, employing techniques outlined in <xref rid="sec4dot1-sensors-24-08020" ref-type="sec">Section 4.1</xref>.</p><sec id="sec4dot2dot1-sensors-24-08020"><title>4.2.1. Proposed Model Architecture</title><p>The architecture of the model we propose here is based on the autoencoder architecture as illustrated in <xref rid="sensors-24-08020-f005" ref-type="fig">Figure 5</xref>.</p><p>The model comprises two branches:<list list-type="bullet"><list-item><p><bold>Main Branch, Unwrapped Depth Map Prediction Branch:</bold> an autoencoder architecture with four stages:</p><list list-type="bullet"><list-item><p>The <bold>TofRegNetMotion input encoder</bold> is the first input stage. It is designed to deal with ambiguous depth map details to extract the overall depth context. It comprises four subsequent vanilla ResNet blocks [<xref rid="B34-sensors-24-08020" ref-type="bibr">34</xref>].</p></list-item><list-item><p>The <bold>ResNet Preactivation backbone</bold> is the following stage, designed for detailed depth feature extraction. The stage is composed of eight subsequent Preactivation ResNet blocks [<xref rid="B35-sensors-24-08020" ref-type="bibr">35</xref>].</p></list-item><list-item><p>The <bold>TofRegNetMotion output decoder</bold> is designed to predict unwrapped depth maps and reduce motion blur simultaneously. It fuses the feature block extracted with the ResNet Preactivation backbone and the features extracted by the motion blur inverse tensor prediction branch to perform the prediction task. The module is composed of four subsequent up-sampling residual blocks.</p></list-item><list-item><p>The <bold>depth regression head</bold> predicts the final detailed unwrapped depth map with reduced motion blur. It uses the output of the TofRegNetMotion input encoder stage and the TofRegNetMotion output decoder stage through skip connections. The module comprises a transposed convolution layer followed by two convolution layers.</p></list-item></list></list-item><list-item><p><bold>Motion Blur Inverse Tensor Prediction Branch:</bold> Consists of three primary components, as in <xref rid="sensors-24-08020-f005" ref-type="fig">Figure 5</xref>:</p><list list-type="bullet"><list-item><p>The <bold>ASIPM module</bold> processes the wrapped depth map to predict the general context optical flow of the point cloud vertices. It comprises four different CNN kernels, based on residual inception [<xref rid="B16-sensors-24-08020" ref-type="bibr">16</xref>], with four different dilation levels (one, two, four, and eight dilation kernels) followed by a feature fusion CNN kernel composed of an adaptive average pooling layer followed by two convolution layers, a batch normalization layer, and a ReLU activation layer.</p></list-item><list-item><p>The <bold>Visual Attention Network module</bold> processes the wrapped depth map simultaneously. It implements a simple visual attention mechanism [<xref rid="B36-sensors-24-08020" ref-type="bibr">36</xref>,<xref rid="B37-sensors-24-08020" ref-type="bibr">37</xref>] for the input. The module comprises three subsequent convolution layers followed by a self-attention layer.</p></list-item><list-item><p>The <bold>Inverse Blur Tensor module</bold> generates the final motion blur inverse tensor feature map. It concatenates the output of the visual attention and ASIPM modules. Then, it applies cascaded convolution kernels to estimate the final inverse blur tensor. The module comprises a concatenation layer followed by three dilation convolution kernels, each of which has a dilation size of two.</p></list-item></list></list-item></list></p><p>The output tensor of the inverse blur tensor module serves as an additional feature map for the TofRegNetMotion output decoder stage in the main branch.</p><p>Furthermore, an alternative architecture with an additional branch may enhance the accuracy of predicted motion blur inverse tensors by utilizing the grayscale image as a sharp, low-SNR depth map synchronized with the depth-wrapped depth map through an STN network [<xref rid="B29-sensors-24-08020" ref-type="bibr">29</xref>]. If activated in our proposed model, the output of the STN will be concatenated with the output of the ASIPM and Visual Attention Network modules. Refer to the ablation study in <xref rid="sec5dot4-sensors-24-08020" ref-type="sec">Section 5.4</xref> and Table 3 to observe the impact of removing this branch from the model.</p></sec><sec id="sec4dot2dot2-sensors-24-08020"><title>4.2.2. The Continuous Learning Framework</title><p>As <xref rid="sensors-24-08020-f006" ref-type="fig">Figure 6</xref> illustrates, the continuous learning mechanism comprises two phases. Initially, the model is trained on the static dataset to predict static unwrapped sharp depth maps. Following this, it undergoes training on the dynamic dataset to predict such maps under motion. To address catastrophic forgetting, three methods are employed.</p><list list-type="bullet"><list-item><p>Dropout layers are integrated into the model, activated solely during training with the dynamic dataset. These layers aim to prevent overfitting to the dynamic dataset, thereby retaining knowledge about the static dataset and promoting generalization [<xref rid="B38-sensors-24-08020" ref-type="bibr">38</xref>].</p></list-item><list-item><p>The rehearsal technique involves randomly selecting batches from the static dataset during training with the dynamic dataset. This selection process ensures representation from the entire static dataset, preventing overfitting to specific scenarios. By incorporating batches from the static dataset, the model retains previous knowledge while learning from the dynamic dataset [<xref rid="B39-sensors-24-08020" ref-type="bibr">39</xref>].</p></list-item><list-item><p>L2 regularization is applied by amplifying the loss function for static dataset batches during training with the dynamic dataset, effectively penalizing inaccurate predictions [<xref rid="B40-sensors-24-08020" ref-type="bibr">40</xref>]. When training the model with the dynamic dataset, the loss function for static dataset batches is multiplied by a factor greater than 1.0. This over-penalization ensures that the model prioritizes fitting to the static dataset, thereby retaining its knowledge even during training with dynamic data [<xref rid="B40-sensors-24-08020" ref-type="bibr">40</xref>].</p></list-item></list><p>Although the continuous learning technique proves to be effective, it could be computationally expensive and time-consuming. This drawback can be solved thanks to vehicle connectivity, modern cloud computing platforms, and advanced parallel computing architectures like Tensor Processing Units (TPUs) from Google and Tensor Core GPUs from Nvidia; these architectures are designed to serve intensive AI training workloads at acceptable power consumption requirements, while also achieving highly satisfactory computational performance and acceptable execution time frames.</p></sec><sec id="sec4dot2dot3-sensors-24-08020"><title>4.2.3. Loss Functions</title><p>The loss functions used for each training phase are a key component for training the model. The loss functions are mainly composed of four components. The first component is the Huber loss [<xref rid="B24-sensors-24-08020" ref-type="bibr">24</xref>] function,
<disp-formula id="FD15-sensors-24-08020"><label>(15)</label><mml:math id="mm86" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>&#x003b4;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi>if</mml:mi><mml:mspace width="4.pt"/><mml:mrow><mml:mo>|</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>|</mml:mo></mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mo>(</mml:mo><mml:mo>|</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mi>&#x003b4;</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>otherwise</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This function is used as the mainstream for penalizing the model; it is very suitable for handling outliers and smooth depth. The other major component is the structural similarity index measure loss [<xref rid="B41-sensors-24-08020" ref-type="bibr">41</xref>],
<disp-formula id="FD16-sensors-24-08020"><label>(16)</label><mml:math id="mm87" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>SSIM</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003bc;</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are selected empirically to be <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>01</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>03</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. In addition to this, we apply our depth-guided loss function, which is defined as follows:<disp-formula id="FD17-sensors-24-08020"><label>(17)</label><mml:math id="mm92" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>dg</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>top</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>gradient</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>smooth</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></inline-formula> are selected empirically to be <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. The <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>top</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> loss function is defined as,
<disp-formula id="FD18-sensors-24-08020"><label>(18)</label><mml:math id="mm100" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>top</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></inline-formula> defines the number of top maximum elements for the input and is empirically selected to be 1200. The <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>gradient</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> loss function is defined as,
<disp-formula id="FD19-sensors-24-08020"><label>(19)</label><mml:math id="mm103" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>gradient</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:mo>&#x02207;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mo>&#x02207;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
and the <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>smooth</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> loss function is defined as,
<disp-formula id="FD20-sensors-24-08020"><label>(20)</label><mml:math id="mm105" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>smooth</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:mo>&#x02207;</mml:mo><mml:msub><mml:mover accent="true"><mml:msub><mml:mi>y</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x02207;</mml:mo><mml:msub><mml:mover accent="true"><mml:msub><mml:mi>y</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02225;</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This depth-guided loss function removes depth noise caused by significant changes in the gradient. It also ensures that large continuous depth segments are homogeneous point clouds. In addition, it tackles a common problem in computer vision: the accuracy of depth prediction decreases dramatically as the object&#x02019;s depth increases.</p><p>The last component of the loss function is the scale-invariant loss,
<disp-formula id="FD21-sensors-24-08020"><label>(21)</label><mml:math id="mm106" display="block" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="1" displaystyle="false"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>siv</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The scale-invariant loss addresses depth saturation scenarios where depth regions become undefined. It ensures that in the presence of depth discontinuities, the model can estimate depth based on object shapes and surroundings, regardless of the object&#x02019;s absolute depth, which is unavailable in saturation cases.</p><p>The overall loss function is composed of the previously stated components, as follows:<disp-formula id="FD22-sensors-24-08020"><label>(22)</label><mml:math id="mm107" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>&#x003b4;</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>dg</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>siv</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>SSIM</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where weights <italic toggle="yes">a</italic>, <italic toggle="yes">b</italic>, <italic toggle="yes">c</italic>, and <italic toggle="yes">d</italic> are selected empirically to be <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>40.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>45.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>25.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>40.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. These values are selected to be much greater than <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to avoid the vanishing gradient problem.</p><p>The same loss function trains the static and dynamic datasets phases. However, during the dynamic dataset phase, to apply L2 regularization, when the batches selected from the static dataset are used to train the model, the overall loss function is multiplied by a factor:<disp-formula id="FD23-sensors-24-08020"><label>(23)</label><mml:math id="mm113" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:math></inline-formula> is empirically selected to be <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">1</mml:mn><mml:mo>.</mml:mo><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec4dot2dot4-sensors-24-08020"><title>4.2.4. Learning Rate Decay Scheduling</title><p>Training a complex model requires careful hyperparameter selection, with the learning rate being crucial. Constant rates lead to quick convergence to local minima; techniques like learning rate decay scheduling solve this issue. Kaichao et al. [<xref rid="B42-sensors-24-08020" ref-type="bibr">42</xref>] discuss scheduling algorithms, showing the efficacy of learning rate decay in transfer tasks. Similarly, Jinia et al. [<xref rid="B43-sensors-24-08020" ref-type="bibr">43</xref>] explore scheduling methods. Our experiments identify step decay as optimal, starting with a high learning rate and gradually reducing it.</p></sec></sec></sec><sec id="sec5-sensors-24-08020"><title>5. Experiments and Results</title><p>We used the Valeo NFL sensor [<xref rid="B6-sensors-24-08020" ref-type="bibr">6</xref>], provided by Valeo Schalter und Sensoren GmbH, Bietigheim-Bissingen, Germany which uses the ESPROS imager [<xref rid="B8-sensors-24-08020" ref-type="bibr">8</xref>], provided by ESPROS Photonics AG, Sargans, Switzerland. Our experiments were based on our internally recorded datasets for static and dynamic scenarios.</p><sec id="sec5dot1-sensors-24-08020"><title>5.1. Training Details</title><p>Both the pretraining and training phases utilized the Adam optimizer with default values. The learning rate followed a stepwise decay algorithm, starting at <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>.</mml:mo><mml:mn mathvariant="bold">0007</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and reaching a minimum of <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>.</mml:mo><mml:mn mathvariant="bold">0003</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for both phases. The static dataset underwent training for <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mn mathvariant="bold">30</mml:mn></mml:mrow></mml:math></inline-formula> epochs with a decay rate every <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">six</mml:mi></mml:mrow></mml:math></inline-formula> steps, while the dynamic dataset was trained for <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mn mathvariant="bold">30</mml:mn></mml:mrow></mml:math></inline-formula> epochs with a decay rate every <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">ten</mml:mi></mml:mrow></mml:math></inline-formula> steps. A <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mn mathvariant="bold">25</mml:mn></mml:mrow></mml:math></inline-formula>% dropout rate was applied during the dynamic dataset training phase. Both training phases used a batch size of <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mn mathvariant="bold">16</mml:mn></mml:mrow></mml:math></inline-formula>. The model was implemented using <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">PyTorch</mml:mi></mml:mrow></mml:math></inline-formula> version 2.2.0 LTS and trained using an <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Nvidia</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="bold">RTX</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mn mathvariant="bold">3060</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="bold">GPU</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> from Nvidia, purchased from Stuttgart, Germany.</p></sec><sec id="sec5dot2-sensors-24-08020"><title>5.2. Datasets</title><p>Two datasets were collected. The static dataset included scenarios where scenes were captured while the vehicle and environment were not in motion. Conversely, the dynamic dataset involved scenarios for parking out, parking in, and driving in low-speed areas, where both the vehicle and environmental objects were in motion.</p><p>The NFL sensor generated unwrapped depth maps by applying the dual-frequency modulation method [<xref rid="B11-sensors-24-08020" ref-type="bibr">11</xref>]. These maps were used as ground truth maps. The resulting ground truth map was then recorded alongside the associated DCS and grayscale frames to create the dataset samples. <xref rid="sensors-24-08020-f007" ref-type="fig">Figure 7</xref> shows samples of both datasets.</p><p>We defined our dataset samples by the keyframe. A keyframe is a distinct, non-repeated frame composed of an ambiguous depth map, a corresponding ground truth unwrapped depth map, a corresponding grayscale frame, and a corresponding laser amplitude frame. According to the sensor&#x02019;s recording sequence, these components were recorded from the same time reference point but in different time shifts. Data augmentation was applied to the training keyframes. Each keyframe was flipped horizontally, vertically, and rotated <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mn mathvariant="bold">180</mml:mn></mml:mrow></mml:math></inline-formula>&#x000b0; around the z-axis, generating four different frames, the original one and three other augmented frames. The static dataset contained <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:math></inline-formula> K keyframes for training, <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:mn mathvariant="bold">120</mml:mn></mml:mrow></mml:math></inline-formula> keyframes for validation, and <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:mn mathvariant="bold">120</mml:mn></mml:mrow></mml:math></inline-formula> keyframes for testing. The dynamic dataset contained <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">3</mml:mn><mml:mo>.</mml:mo><mml:mn mathvariant="bold">3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> K keyframes for training, <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mn mathvariant="bold">430</mml:mn></mml:mrow></mml:math></inline-formula> keyframes for validation, and <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mn mathvariant="bold">430</mml:mn></mml:mrow></mml:math></inline-formula> keyframes for testing. The datasets were captured using both two-DCS frames and four-DCS frames per modulation frequency.</p></sec><sec sec-type="results" id="sec5dot3-sensors-24-08020"><title>5.3. Results</title><sec id="sec5dot3dot1-sensors-24-08020"><title>5.3.1. Comparative Analysis</title><p>As we illustrated in our literature review, our proposed method is the first one designed to tackle the problems of ambiguous depth map unwrapping during motion and reducing the motion blur simultaneously. We showed in <xref rid="sec3-sensors-24-08020" ref-type="sec">Section 3</xref> that the ambiguous depth measurement and motion blur trailing traces are two tightly coupled problems that impact each other. A solution that tackles the depth ambiguity problem during motion should also consider motion blur removal to improve depth accuracy.</p><p>So far, only two methods have made partial progress in addressing these challenges. The first, developed by Gao et al. [<xref rid="B15-sensors-24-08020" ref-type="bibr">15</xref>], focuses on motion blur reduction but does not provide a solution for depth unwrapping. It can only operate within the unambiguous range. The second, developed by Jung et al. [<xref rid="B21-sensors-24-08020" ref-type="bibr">21</xref>], known as Wild ToFu, does offer a solution to unwrap ambiguous depth maps, but it is limited to static indoor scenes. This analysis explains why we cannot compare our work to the existing state-of-the-art.</p><p>However, we tried to benchmark our proposed model results using the static dataset versus Wild ToFu results to evaluate the model&#x02019;s performance against the state-of-the-art even for the static scenes. Considering that Wild ToFu is designed for indoor scenes with a different sensor, our method is designed for outdoor scenes based on the Valeo NFL sensor; we had to make some adaptations to the model of Wild ToFu to be compatible with our static dataset labels. The Wild ToFu method works directly on the four DCS frames. Also, the sensor&#x02019;s frame size used with Wild ToFu is <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. On the other hand, our method operates directly on the ambiguous depth map of frame size <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>320</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>240</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. So, we have adapted the input layer of Wild ToFu to be compatible with our labels.</p><p>Despite the challenges posed by the outdoor environment, such as longer measured distances, sunlight interference, and environmental factors, our method has proven its adaptability and superiority. <xref rid="sensors-24-08020-t001" ref-type="table">Table 1</xref> provides a comprehensive comparison between the adapted Wild ToFu method and our proposed method, trained on the static dataset of our sensor. The results demonstrate that our method outperforms the adapted Wild ToFu; note that GS refers to the grayscale frame.</p><p>On the other hand, for motion blur reduction, as illustrated previously, no prior state-of-the-art method has been proposed to solve the problem while unwrapping the ambiguous depth; hence, we cannot provide any comparative analysis for this part, and our model acts as a baseline for further developments in this space.</p></sec><sec id="sec5dot3dot2-sensors-24-08020"><title>5.3.2. Quantitative Analysis</title><p>We start first with the quantitative results of the model. <xref rid="sensors-24-08020-t002" ref-type="table">Table 2</xref> summarizes the performance of our method when it is used in dynamic scenarios, in addition to the common prediction error measurements for the depth estimation function. Upon initial analysis, it becomes apparent that the model&#x02019;s accuracy is lower in dynamic scenarios compared to static scenarios. However, it is essential to acknowledge that ground truth data in dynamic scenarios may be affected by motion blur, potentially influencing accuracy assessments. To ensure a fair evaluation, the model&#x02019;s performance is primarily assessed in static scenarios where motion blur is absent. The model&#x02019;s strong performance in static scenarios, with the absence of motion blur, indicates its adaptability and potential to address this challenge, enabling better performance in dynamic scenarios.</p><p>The model&#x02019;s proficiency in static scenarios, characterized by the absence of motion blur, provides insights into its behavior during low-speed driving in dynamic scenarios. Results in <xref rid="sensors-24-08020-t001" ref-type="table">Table 1</xref> demonstrate high accuracy in static scenarios, suggesting the potential for even better performance in dynamic situations with reduced motion blur. This outlook supports the hypothesis that the model&#x02019;s accuracy is influenced by its ability to handle motion blur effectively.</p><p>Additionally, factors such as depth filters applied in ground truth and ambiguous depth maps, irregularities like blooming, and multi-path interference impact the captured level of detail. The prediction model captures more information than the ground truth, leading to anomalies in the RMSE (Root Mean Square Error) metric calculation. A qualitative analysis will further explore the impact of these factors on the model&#x02019;s performance. The reasons for the model&#x02019;s low accuracy in dynamic scenarios can be summarized as follows:<list list-type="order"><list-item><p>The ground truth is affected by motion blur, while predicted maps address this issue, resulting in a higher error rate between the prediction and the ground truth.</p></list-item><list-item><p>The model&#x02019;s predictions may capture more details than those detected by the ground truth, contributing to error calculation discrepancies.</p></list-item><list-item><p>External factors like blooming and sunlight interference impact both ground truth capture and prediction, although this falls beyond the scope of this paper.</p></list-item></list></p><p>In the <xref rid="sec5dot3dot3-sensors-24-08020" ref-type="sec">Section 5.3.3</xref>, we delve into these findings and hypotheses. Despite the initial interpretation suggesting modest model performance, a deeper investigation reveals that our model surpasses the state-of-the-art method using dual modulation frequency in dynamic scenarios.</p></sec><sec id="sec5dot3dot3-sensors-24-08020"><title>5.3.3. Qualitative Analysis</title><p>We initiate our qualitative analysis by identifying frames with the highest RMSE and MAE (Mean Absolute Error). <xref rid="sensors-24-08020-f008" ref-type="fig">Figure 8</xref> illustrates the selected frames for further examination. The figure describes a trace of 500 test frames captured by the sensor and corrected using our method. The graphs in the frame show the RMSE and MAE between the corrected frames and the ground truth. In the figure, we have identified five different groups of frames (i.e., frames groups a, b, c, d, and e), and each group shares a common reason for high RMSE and MAE values. The qualitative analysis focused on analyzing these reasons and providing thorough explanations and justifications. The significance of our findings lies in the potential to improve sensor technology. Due to space constraints, we will present a subset of samples from each group for review.</p><p>It is important to note that the primary reason for the high RMSE and MAE values observed between the corrected and ground truth frames stems from using an iToF Lidar sensor to capture the ground truth frames that implement a depth cutoff filter for point cloud depth values exceeding 25 m. Our method does not utilize this depth filter, allowing for the detection of objects beyond 25 m. Also, the ground truth is susceptible to errors. It does not perfectly reflect &#x0201c;reality&#x0201d; due to motion blur since there is no state-of-the-art solution for this problem in iTOF Lidar sensors, even though it offers accurate unwrapped depth maps. Therefore, the recreated point cloud by our proposed model might exhibit a closer resemblance to reality than the ground truth despite displaying &#x0201c;inaccurate&#x0201d; results. Additionally, the resolution of the CMOS imager in the NFL sensor used for predictions and ground truth is <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>320</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>240</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels, which is relatively low compared to state-of-the-art RGB and grayscale camera frames. This limitation helps explain the low resolution of the provided frames. We plan to tackle these ground truth issues in future work, as will be discussed in <xref rid="sec6-sensors-24-08020" ref-type="sec">Section 6</xref>.</p><p><bold>Group A Frames:</bold> This group exemplifies the impact of motion blur on depth prediction accuracy. Despite closely matching point clouds between the corrected frames and the ground truth, the corrected frames exhibit more uniform point clouds. A sample of Group <bold>A</bold> is illustrated in <xref rid="sensors-24-08020-f009" ref-type="fig">Figure 9</xref>. The vehicle&#x02019;s outer frame and the wheels are more rigid and defined&#x02014;this effect of homogeneity in the corrected point cloud results in high differences in the RMSE and MAE.</p><p><bold>Group B Frames:</bold> Here, extensive blurriness noise corrupts the ground truth&#x02019;s point cloud. The model partially smooths the depth, resulting in a more uniform point cloud with acceptable pixel gradient transitions, leading to differences between the ground truth and the prediction. A sample of Group <bold>B</bold> is illustrated in <xref rid="sensors-24-08020-f010" ref-type="fig">Figure 10</xref>. The wheels in the real world are of homogeneous depth; it is clear that the ground truth is very noisy, with very high gradients between almost every two neighboring pixels. The model was able to smooth the depth to some extent, which resulted in a homogeneous point cloud with acceptable gradients between neighbor pixels, but the details are not very sharp. This kind of noise is expected due to the scale of the blurred area.</p><p><bold>Group C Frames:</bold> This sample showcases elevated RMSE and MAE due to the model&#x02019;s enhanced performance. The model not only corrects blurriness but also extracts additional details from ambiguous depth maps beyond the ground truth. A sample of Group <bold>C</bold> is illustrated in <xref rid="sensors-24-08020-f011" ref-type="fig">Figure 11</xref>. The model not only corrected the blurriness but also extracted more details from the ambiguous depth maps compared to the ground truth. Here, we show the grayscale to show that extracted details are not just noise due to the model&#x02019;s poor performance but are real objects. This is mainly due to the depth filter implemented in the sensor for capturing the ground truth point cloud frames.</p><p><bold>Group D Frames:</bold> Here, the model surpasses the ground truth by identifying more objects than are visible in the ground truth. This over-performance results in higher RMSE and MAE values. A sample of Group <bold>D</bold> is illustrated in <xref rid="sensors-24-08020-f012" ref-type="fig">Figure 12</xref>. The model was able to extract more objects than what is visible by the ground truth. The grayscale image shows that these are real objects and not just noise. This over-performance leads to higher RMSE and MAE values.</p><p><bold>Group E Frames:</bold> Similar to Group D, the model exceeds the ground truth by identifying additional objects and generating a more uniform point cloud by filling holes in the depth segment. This over-performance leads to higher RMSE and MAE values. A sample of Group <bold>E</bold> is illustrated in <xref rid="sensors-24-08020-f013" ref-type="fig">Figure 13</xref>. The model generated a homogeneous point cloud where the holes in the depth segment were removed. The grayscale image shows that these are real objects and not just noise. This over-performance leads to higher RMSE and MAE values.</p><p>An interesting observation is the higher accuracy and quality of results obtained using two-DCS compared to four-DCS. This can be attributed to the fact that two-DCS ambiguous depth maps experience less motion blur since they are composed of fewer superimposed DCS frames. Consequently, two-DCS ambiguous depth maps are generally less noisy and suffer from fewer multi-path interferences.</p></sec></sec><sec id="sec5dot4-sensors-24-08020"><title>5.4. Ablation Study</title><p>This ablation study was designed to ensure that each component in the framework contributed to the model&#x02019;s overall performance. Two main activities were undertaken to conduct the study. Firstly, the importance of each component of the modules was examined. Secondly, the significance of continuous learning was evaluated.</p><p>To verify the hypothesis stated in <xref rid="sec4dot1dot3-sensors-24-08020" ref-type="sec">Section 4.1.3</xref>, an ablation study was performed to analyze the impact of removing each component on the model&#x02019;s performance. The study included experiments conducted on both the two-DCS and four-DCS datasets involving the following scenarios:<list list-type="order"><list-item><p>Full working model with all components intact.</p></list-item><list-item><p>Full model without utilizing the spatial transformation network.</p></list-item><list-item><p>Model without the spatial transformation network and the inverse blur kernel estimation network, operating only the core pipeline using the ambiguous depth map as input.</p></list-item><list-item><p>Model with the spatial transformation network but without the inverse blur kernel estimation network, operating only the core pipeline using a grayscale image instead of the ambiguous depth map as input.</p></list-item><list-item><p>Model without the spatial transformation network and the inverse blur kernel estimation network, operating only the core pipeline using a grayscale image instead of the ambiguous depth map as input.</p></list-item></list></p><p>The experiment results summarized in <xref rid="sensors-24-08020-t003" ref-type="table">Table 3</xref> highlight the primary improvements in system performance. The most significant enhancement is observed when utilizing the ambiguous depth map as input, as employing grayscale led to notably higher RMSE and MAE values. Moreover, the model&#x02019;s performance is substantially improved when the STN and ASIPM modules are used in conjunction. Removing these modules increased depth errors, particularly evident with the four-DCS datasets. Additionally, the model&#x02019;s performance with two-DCS surpasses that with four-DCS, which was expected due to the lower blurriness noise in the input and fewer synchronization issues between the ambiguous depth map and grayscale. This enabled the STN to operate more effectively.</p><p>We carried out another activity to study the impact of continual learning. We compared the training results of the model using the static dataset followed by the dynamic dataset versus training the system using the dynamic dataset directly and without any static dataset. To perform this activity, we trained the system with different configurations:<list list-type="order"><list-item><p>For the normal case (i.e., the static dataset followed by the dynamic dataset), we used the default training configuration as described in <xref rid="sec5dot1-sensors-24-08020" ref-type="sec">Section 5.1</xref>.</p></list-item><list-item><p>For the dynamic dataset only, we used the training configurations as follows: using the Adam optimizer, the values of <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> were 0.9 and 0.999, respectively. The learning rate was scheduled to decay using the stepwise algorithm, where the starting rate was 0.0007 and the minimum rate was 0.0003. We performed 30 epochs but with a stepwise decay of 10 steps. We used dropout with a 25% rate and a batch size of 16.</p></list-item></list></p><p>In <xref rid="sensors-24-08020-f014" ref-type="fig">Figure 14</xref>, two cases are compared: (a) a learning process that remains stagnant without improvement, and (b) a successful training process using the static dataset followed by the dynamic dataset. Case (b) demonstrates that pretraining the model on static scenarios improves performance and learning curve progress in dynamic scenarios. This improvement supports the hypothesis that the model benefits from static scenario predictions relevant to certain dynamic situations. The proposed continual learning method contributes to improved performance in dynamic scenarios.</p></sec></sec><sec sec-type="discussion" id="sec6-sensors-24-08020"><title>6. Discussion</title><p>This paper introduced a method to predict unwrap iTOF Lidar&#x02019;s ambiguous depth maps in dynamic environments affected by motion blur and frame mis-synchronization. The method can predict unwrapped depth maps with reduced motion blur effects. The study investigated the hypothesis that training the model in static scenarios can improve its performance in dynamic scenarios by addressing motion blur. The proposed method incorporates the STN module for synchronization correction and the ASIPM module for predicting the inverse blur kernel tensor. The model is trained semi-supervised, reducing reliance on noise in the ground truth. Experimental results and ablation studies validate the hypothesis, with the model even outperforming the ground truth in some instances. These findings underscore the proposed model&#x02019;s and training framework&#x02019;s effectiveness in mitigating depth prediction issues inherent in the ground truth. In our future work, we plan to use a synchronized, better-performing sensor to generate the ground truth with accurate depth maps and no motion blur effect. We also plan to extend our dataset to include more postures of the sensor mounting on the ego vehicle. In addition, we plan to conduct sensor power measurements to study the impact of using only two-DCS frames on power reduction.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, M.N.; methodology, M.N.; software, M.N.; validation, G.S., T.B. and C.E.; formal analysis, M.N.; investigation, M.N.; resources, M.N.; data curation, M.N.; writing&#x02014;original draft preparation, M.N.; writing&#x02014;review and editing, G.S., T.B., C.E. and H.G.; visualization, M.N.; supervision, G.S., T.B., C.E. and H.G.; project administration, G.S., T.B., C.E. and H.G.; funding acquisition, G.S., T.B., C.E. and H.G. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Dataset is available upon request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Authors Mena Nagiub, Thorsten Beuth, Heinrich Gotzig were employed by the company Valeo Schalter und Sensoren GmbH. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">MDPI</td><td align="left" valign="middle" rowspan="1" colspan="1">Multidisciplinary Digital Publishing Institute</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">iToF</td><td align="left" valign="middle" rowspan="1" colspan="1">Indirect Time-of-Flight</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Lidar</td><td align="left" valign="middle" rowspan="1" colspan="1">Light Detection and Ranging</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NFL</td><td align="left" valign="middle" rowspan="1" colspan="1">Near-Field Lidar</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AMCW</td><td align="left" valign="middle" rowspan="1" colspan="1">Amplitude-Modulated Continuous Wave</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CMOS</td><td align="left" valign="middle" rowspan="1" colspan="1">Complementary Metal-Oxide Semiconductor</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ASIPM</td><td align="left" valign="middle" rowspan="1" colspan="1">Atrous Spatial Inception Pyramid Module</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DCS</td><td align="left" valign="middle" rowspan="1" colspan="1">Differential Correlation Samples</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SNR</td><td align="left" valign="middle" rowspan="1" colspan="1">Signal Noise Ratio</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FPP</td><td align="left" valign="middle" rowspan="1" colspan="1">Fringe Projection Profilometry</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">STN</td><td align="left" valign="middle" rowspan="1" colspan="1">Spatial Transformer Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GS</td><td align="left" valign="middle" rowspan="1" colspan="1">Grayscale</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RMSE</td><td align="left" valign="middle" rowspan="1" colspan="1">Root Mean Square Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MAE</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Absolute Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SqRel</td><td align="left" valign="middle" rowspan="1" colspan="1">Squared Relative Difference</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AbsRel</td><td align="left" valign="middle" rowspan="1" colspan="1">Absolute Relative Difference</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SiLog</td><td align="left" valign="middle" rowspan="1" colspan="1">Scale-Invariant Logarithmic Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">W/o</td><td align="left" valign="middle" rowspan="1" colspan="1">Without</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-24-08020"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Eising</surname><given-names>C.</given-names></name>
<name><surname>Horgan</surname><given-names>J.</given-names></name>
<name><surname>Yogamani</surname><given-names>S.</given-names></name>
</person-group><article-title>Near-Field Perception for Low-Speed Vehicle Automation Using Surround-View Fisheye Cameras</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>23</volume><fpage>13976</fpage><lpage>13993</lpage><pub-id pub-id-type="doi">10.1109/TITS.2021.3127646</pub-id></element-citation></ref><ref id="B2-sensors-24-08020"><label>2.</label><element-citation publication-type="book"><std>IEEE Std 2846-2022</std><source>IEEE Standard for Assumptions in Safety-Related Models for Automated Driving Systems</source><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2022</year><fpage>1</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1109/IEEESTD.2022.9761121</pub-id></element-citation></ref><ref id="B3-sensors-24-08020"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kumar</surname><given-names>V.R.</given-names></name>
<name><surname>Eising</surname><given-names>C.</given-names></name>
<name><surname>Witt</surname><given-names>C.</given-names></name>
<name><surname>Yogamani</surname><given-names>S.K.</given-names></name>
</person-group><article-title>Surround-View Fisheye Camera Perception for Automated Driving: Overview, Survey &#x00026; Challenges</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2023</year><volume>24</volume><fpage>3638</fpage><lpage>3659</lpage><pub-id pub-id-type="doi">10.1109/TITS.2023.3235057</pub-id></element-citation></ref><ref id="B4-sensors-24-08020"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Reddy Cenkeramaddi</surname><given-names>L.</given-names></name>
<name><surname>Bhatia</surname><given-names>J.</given-names></name>
<name><surname>Jha</surname><given-names>A.</given-names></name>
<name><surname>Kumar Vishkarma</surname><given-names>S.</given-names></name>
<name><surname>Soumya</surname><given-names>J.</given-names></name>
</person-group><article-title>A Survey on Sensors for Autonomous Systems</article-title><source>Proceedings of the 2020 15th IEEE Conference on Industrial Electronics and Applications (ICIEA)</source><conf-loc>Kristiansand, Norway</conf-loc><conf-date>9&#x02013;13 November 2020</conf-date><fpage>1182</fpage><lpage>1187</lpage><pub-id pub-id-type="doi">10.1109/ICIEA48937.2020.9248282</pub-id></element-citation></ref><ref id="B5-sensors-24-08020"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Gong</surname><given-names>Y.</given-names></name>
<name><surname>Lu</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Jin</surname><given-names>D.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>Multi-Modal Fusion Technology Based on Vehicle Information: A Survey</article-title><source>IEEE Trans. Intell. Veh.</source><year>2023</year><volume>8</volume><fpage>3605</fpage><lpage>3619</lpage><pub-id pub-id-type="doi">10.1109/TIV.2023.3268051</pub-id></element-citation></ref><ref id="B6-sensors-24-08020"><label>6.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Valeo</collab>
</person-group><article-title>Valeo Near Field LiDAR</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://levelfivesupplies.com/wp-content/uploads/2021/09/Valeo-Mobility-Kit-near-field-LiDAR-data-sheet.pdf" ext-link-type="uri">https://levelfivesupplies.com/wp-content/uploads/2021/09/Valeo-Mobility-Kit-near-field-LiDAR-data-sheet.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2022-12-09">(accessed on 9 December 2022)</date-in-citation></element-citation></ref><ref id="B7-sensors-24-08020"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Roriz</surname><given-names>R.</given-names></name>
<name><surname>Cabral</surname><given-names>J.</given-names></name>
<name><surname>Gomes</surname><given-names>T.</given-names></name>
</person-group><article-title>Automotive LiDAR Technology: A Survey</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>23</volume><fpage>6282</fpage><lpage>6297</lpage><pub-id pub-id-type="doi">10.1109/TITS.2021.3086804</pub-id></element-citation></ref><ref id="B8-sensors-24-08020"><label>8.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>ESPROS Photonics Corporation</collab>
</person-group><article-title>DATASHEET&#x02013;epc660-3D TOF Imager 320 &#x000d7;240 Pixel, V2.19</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://www.espros.com/downloads/01_Chips/Datasheet_epc660.pdf" ext-link-type="uri">https://www.espros.com/downloads/01_Chips/Datasheet_epc660.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2022-12-09">(accessed on 9 December 2022)</date-in-citation></element-citation></ref><ref id="B9-sensors-24-08020"><label>9.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Texas Instruments Incorporated</collab>
</person-group><article-title>OPT8241 3D Time-of-Flight Sensor Datasheet-SBAS704B</article-title><year>2015</year><comment>Available online: <ext-link xlink:href="https://www.ti.com/product/OPT8241" ext-link-type="uri">https://www.ti.com/product/OPT8241</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2022-12-09">(accessed on 9 December 2022)</date-in-citation></element-citation></ref><ref id="B10-sensors-24-08020"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Huo</surname><given-names>D.</given-names></name>
<name><surname>Masoumzadeh</surname><given-names>A.</given-names></name>
<name><surname>Yang</surname><given-names>Y.H.</given-names></name>
</person-group><article-title>Blind Non-Uniform Motion Deblurring using Atrous Spatial Pyramid Deformable Convolution and Deblurring-Reblurring Consistency</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#x02013;20 June 2022</conf-date><fpage>436</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1109/CVPRW56347.2022.00059</pub-id></element-citation></ref><ref id="B11-sensors-24-08020"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hanto</surname><given-names>D.</given-names></name>
<name><surname>Pratomo</surname><given-names>H.</given-names></name>
<name><surname>Rianaris</surname><given-names>A.</given-names></name>
<name><surname>Setiono</surname><given-names>A.</given-names></name>
<name><surname>Sartika</surname><given-names>S.</given-names></name>
<name><surname>Syahadi</surname><given-names>M.</given-names></name>
<name><surname>Pristianto</surname><given-names>E.J.</given-names></name>
<name><surname>Kurniawan</surname><given-names>D.</given-names></name>
<name><surname>Bayuwati</surname><given-names>D.</given-names></name>
<name><surname>Adinanta</surname><given-names>H.</given-names></name>
<etal/>
</person-group><article-title>Time of Flight Lidar Employing Dual-Modulation Frequencies Switching for Optimizing Unambiguous Range Extension and High Resolution</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2023</year><volume>72</volume><fpage>7001408</fpage><pub-id pub-id-type="doi">10.1109/TIM.2023.3235450</pub-id></element-citation></ref><ref id="B12-sensors-24-08020"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bamji</surname><given-names>C.</given-names></name>
<name><surname>Godbaz</surname><given-names>J.</given-names></name>
<name><surname>Oh</surname><given-names>M.</given-names></name>
<name><surname>Mehta</surname><given-names>S.</given-names></name>
<name><surname>Payne</surname><given-names>A.</given-names></name>
<name><surname>Ortiz</surname><given-names>S.</given-names></name>
<name><surname>Nagaraja</surname><given-names>S.</given-names></name>
<name><surname>Perry</surname><given-names>T.</given-names></name>
<name><surname>Thompson</surname><given-names>B.</given-names></name>
</person-group><article-title>A Review of Indirect Time-of-Flight Technologies</article-title><source>IEEE Trans. Electron Devices</source><year>2022</year><volume>69</volume><fpage>2779</fpage><lpage>2793</lpage><pub-id pub-id-type="doi">10.1109/TED.2022.3145762</pub-id></element-citation></ref><ref id="B13-sensors-24-08020"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bulczak</surname><given-names>D.</given-names></name>
<name><surname>Lambers</surname><given-names>M.</given-names></name>
<name><surname>Kolb</surname><given-names>A.</given-names></name>
</person-group><article-title>Quantified, Interactive Simulation of AMCW ToF Camera Including Multipath Effects</article-title><source>Sensors</source><year>2017</year><volume>18</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3390/s18010013</pub-id><pub-id pub-id-type="pmid">29271888</pub-id>
</element-citation></ref><ref id="B14-sensors-24-08020"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Solomon</surname><given-names>C.</given-names></name>
<name><surname>Breckon</surname><given-names>T.</given-names></name>
</person-group><article-title>Enhancement</article-title><source>Fundamentals of Digital Image Processing: A practical Approach with Examples in Matlab</source><publisher-name>John Wiley &#x00026; Sons, Ltd.</publisher-name><publisher-loc>Hoboken, NJ, USA</publisher-loc><year>2011</year><fpage>85</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1002/9780470689776</pub-id></element-citation></ref><ref id="B15-sensors-24-08020"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gao</surname><given-names>J.</given-names></name>
<name><surname>Gao</surname><given-names>X.</given-names></name>
<name><surname>Nie</surname><given-names>K.</given-names></name>
<name><surname>Gao</surname><given-names>Z.</given-names></name>
<name><surname>Xu</surname><given-names>J.</given-names></name>
</person-group><article-title>A Deblurring Method for Indirect Time-of-Flight Depth Sensor</article-title><source>IEEE Sens. J.</source><year>2023</year><volume>23</volume><fpage>2718</fpage><lpage>2726</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2022.3229687</pub-id></element-citation></ref><ref id="B16-sensors-24-08020"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Herrmann</surname><given-names>C.</given-names></name>
<name><surname>Willersinn</surname><given-names>D.</given-names></name>
<name><surname>Beyerer</surname><given-names>J.</given-names></name>
</person-group><article-title>Residual vs. inception vs. classical networks for low-resolution face recognition</article-title><source>Proceedings of the Image Analysis: 20th Scandinavian Conference, SCIA 2017</source><conf-loc>Troms&#x000f8;, Norway</conf-loc><conf-date>12&#x02013;14 June 2017</conf-date><comment>Proceedings, Part II 20</comment><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2017</year><fpage>377</fpage><lpage>388</lpage></element-citation></ref><ref id="B17-sensors-24-08020"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chang</surname><given-names>M.</given-names></name>
<name><surname>Yang</surname><given-names>C.</given-names></name>
<name><surname>Feng</surname><given-names>H.</given-names></name>
<name><surname>Xu</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>Q.</given-names></name>
</person-group><article-title>Beyond Camera Motion Blur Removing: How to Handle Outliers in Deblurring</article-title><source>IEEE Trans. Comput. Imaging</source><year>2021</year><volume>7</volume><fpage>463</fpage><lpage>474</lpage><pub-id pub-id-type="doi">10.1109/TCI.2021.3076886</pub-id></element-citation></ref><ref id="B18-sensors-24-08020"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Argaw</surname><given-names>D.M.</given-names></name>
<name><surname>Kim</surname><given-names>J.</given-names></name>
<name><surname>Rameau</surname><given-names>F.</given-names></name>
<name><surname>Cho</surname><given-names>J.W.</given-names></name>
<name><surname>Kweon</surname><given-names>I.S.</given-names></name>
</person-group><article-title>Optical flow estimation from a single motion-blurred image</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Virtual</conf-loc><conf-date>2&#x02013;9 February 2021</conf-date><volume>Volume 35</volume><fpage>891</fpage><lpage>900</lpage></element-citation></ref><ref id="B19-sensors-24-08020"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Zhen</surname><given-names>A.</given-names></name>
<name><surname>Stevenson</surname><given-names>R.L.</given-names></name>
</person-group><article-title>Deep motion blur removal using noisy/blurry image pairs</article-title><source>J. Electron. Imaging</source><year>2021</year><volume>30</volume><fpage>033022</fpage><pub-id pub-id-type="doi">10.1117/1.JEI.30.3.033022</pub-id></element-citation></ref><ref id="B20-sensors-24-08020"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nagiub</surname><given-names>M.</given-names></name>
<name><surname>Beuth</surname><given-names>T.</given-names></name>
<name><surname>Sistu</surname><given-names>G.</given-names></name>
<name><surname>Gotzig</surname><given-names>H.</given-names></name>
<name><surname>Eising</surname><given-names>C.</given-names></name>
</person-group><article-title>Near Field iToF LIDAR Depth Improvement from Limited Number of Shots</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2304.07047</pub-id></element-citation></ref><ref id="B21-sensors-24-08020"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Jung</surname><given-names>H.</given-names></name>
<name><surname>Brasch</surname><given-names>N.</given-names></name>
<name><surname>Leonardis</surname><given-names>A.</given-names></name>
<name><surname>Navab</surname><given-names>N.</given-names></name>
<name><surname>Busam</surname><given-names>B.</given-names></name>
</person-group><article-title>Wild ToFu: Improving Range and Quality of Indirect Time-of-Flight Depth with RGB Fusion in Challenging Environments</article-title><source>Proceedings of the 2021 International Conference on 3D Vision (3DV)</source><conf-loc>Virtual</conf-loc><conf-date>1&#x02013;3 December 2021</conf-date><fpage>239</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1109/3DV53792.2021.00034</pub-id></element-citation></ref><ref id="B22-sensors-24-08020"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Chen</surname><given-names>T.</given-names></name>
<name><surname>Shi</surname><given-names>M.</given-names></name>
<name><surname>Zhu</surname><given-names>D.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
</person-group><article-title>Single-frequency and accurate phase unwrapping method using deep learning</article-title><source>Opt. Lasers Eng.</source><year>2023</year><volume>162</volume><fpage>107409</fpage><pub-id pub-id-type="doi">10.1016/j.optlaseng.2022.107409</pub-id></element-citation></ref><ref id="B23-sensors-24-08020"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qiao</surname><given-names>X.</given-names></name>
<name><surname>Ge</surname><given-names>C.</given-names></name>
<name><surname>Deng</surname><given-names>P.</given-names></name>
<name><surname>Wei</surname><given-names>H.</given-names></name>
<name><surname>Poggi</surname><given-names>M.</given-names></name>
<name><surname>Mattoccia</surname><given-names>S.</given-names></name>
</person-group><article-title>Depth Restoration in Under-Display Time-of-Flight Imaging</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>45</volume><fpage>5668</fpage><lpage>5683</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3209905</pub-id><pub-id pub-id-type="pmid">36155477</pub-id>
</element-citation></ref><ref id="B24-sensors-24-08020"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Schelling</surname><given-names>M.</given-names></name>
<name><surname>Hermosilla</surname><given-names>P.</given-names></name>
<name><surname>Ropinski</surname><given-names>T.</given-names></name>
</person-group><article-title>Weakly-Supervised Optical Flow Estimation for Time-of-Flight</article-title><source>Proceedings of the 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>2&#x02013;7 January 2023</conf-date><fpage>2134</fpage><lpage>2143</lpage><pub-id pub-id-type="doi">10.1109/WACV56688.2023.00217</pub-id></element-citation></ref><ref id="B25-sensors-24-08020"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Jokela</surname><given-names>M.</given-names></name>
<name><surname>Pyyk&#x000f6;nen</surname><given-names>P.</given-names></name>
<name><surname>Kutila</surname><given-names>M.</given-names></name>
<name><surname>Kauvo</surname><given-names>K.</given-names></name>
</person-group><article-title>LiDAR Performance Review in Arctic Conditions</article-title><source>Proceedings of the 2019 IEEE 15th International Conference on Intelligent Computer Communication and Processing (ICCP)</source><conf-loc>Cluj-Napoca, Romania</conf-loc><conf-date>5&#x02013;7 September 2019</conf-date><fpage>27</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1109/ICCP48234.2019.8959554</pub-id></element-citation></ref><ref id="B26-sensors-24-08020"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vyas</surname><given-names>P.</given-names></name>
<name><surname>Saxena</surname><given-names>C.</given-names></name>
<name><surname>Badapanda</surname><given-names>A.</given-names></name>
<name><surname>Goswami</surname><given-names>A.</given-names></name>
</person-group><article-title>Outdoor monocular depth estimation: A research review</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2205.01399</pub-id></element-citation></ref><ref id="B27-sensors-24-08020"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Lee</surname><given-names>C.Y.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Sun</surname><given-names>R.</given-names></name>
<name><surname>Ren</surname><given-names>X.</given-names></name>
<name><surname>Su</surname><given-names>G.</given-names></name>
<name><surname>Perot</surname><given-names>V.</given-names></name>
<name><surname>Dy</surname><given-names>J.</given-names></name>
<name><surname>Pfister</surname><given-names>T.</given-names></name>
</person-group><article-title>Learning to Prompt for Continual Learning</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#x02013;24 June 2022</conf-date><fpage>139</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1109/CVPR52688.2022.00024</pub-id></element-citation></ref><ref id="B28-sensors-24-08020"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Smith</surname><given-names>J.</given-names></name>
<name><surname>Tian</surname><given-names>J.</given-names></name>
<name><surname>Hsu</surname><given-names>Y.C.</given-names></name>
<name><surname>Kira</surname><given-names>Z.</given-names></name>
</person-group><article-title>A Closer Look at Rehearsal-Free Continual Learning</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2203.17269</pub-id></element-citation></ref><ref id="B29-sensors-24-08020"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jaderberg</surname><given-names>M.</given-names></name>
<name><surname>Simonyan</surname><given-names>K.</given-names></name>
<name><surname>Zisserman</surname><given-names>A.</given-names></name>
</person-group><article-title>Spatial transformer networks</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2015</year><volume>28</volume><pub-id pub-id-type="doi">10.48550/arXiv.1506.02025</pub-id></element-citation></ref><ref id="B30-sensors-24-08020"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>K.</given-names></name>
<name><surname>Ren</surname><given-names>W.</given-names></name>
<name><surname>Luo</surname><given-names>W.</given-names></name>
<name><surname>Lai</surname><given-names>W.S.</given-names></name>
<name><surname>Stenger</surname><given-names>B.</given-names></name>
<name><surname>Yang</surname><given-names>M.H.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
</person-group><article-title>Deep image deblurring: A survey</article-title><source>Int. J. Comput. Vis.</source><year>2022</year><volume>130</volume><fpage>2103</fpage><lpage>2130</lpage><pub-id pub-id-type="doi">10.1007/s11263-022-01633-5</pub-id></element-citation></ref><ref id="B31-sensors-24-08020"><label>31.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Solomon</surname><given-names>C.</given-names></name>
<name><surname>Breckon</surname><given-names>T.</given-names></name>
</person-group><article-title>Blind deconvolution</article-title><source>Fundamentals of Digital Image Processing: A Practical Approach with Examples in Matlab</source><publisher-name>John Wiley &#x00026; Sons, Ltd.</publisher-name><publisher-loc>Hoboken, NJ, USA</publisher-loc><year>2011</year><fpage>156</fpage><lpage>158</lpage></element-citation></ref><ref id="B32-sensors-24-08020"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Verdi&#x000e9;</surname><given-names>Y.</given-names></name>
<name><surname>Song</surname><given-names>J.</given-names></name>
<name><surname>Mas</surname><given-names>B.</given-names></name>
<name><surname>Busam</surname><given-names>B.</given-names></name>
<name><surname>Leonardis</surname><given-names>A.</given-names></name>
<name><surname>McDonagh</surname><given-names>S.</given-names></name>
</person-group><article-title>Cromo: Cross-modal learning for monocular depth estimation</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#x02013;24 June 2022</conf-date><fpage>3927</fpage><lpage>3937</lpage></element-citation></ref><ref id="B33-sensors-24-08020"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Riley</surname><given-names>K.F.</given-names></name>
<name><surname>Hobson</surname><given-names>M.P.</given-names></name>
<name><surname>Bence</surname><given-names>S.J.</given-names></name>
</person-group><source>Mathematical Methods for Physics and Engineering: A Comprehensive Guide</source><publisher-name>Cambridge University Press</publisher-name><publisher-loc>Cambridge, UK</publisher-loc><year>2006</year><comment>Volumes 1255&#x02013;1271</comment><fpage>377</fpage><lpage>388</lpage></element-citation></ref><ref id="B34-sensors-24-08020"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="B35-sensors-24-08020"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gao</surname><given-names>H.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
<name><surname>Yao</surname><given-names>D.</given-names></name>
<name><surname>Li</surname><given-names>C.</given-names></name>
</person-group><article-title>Hyperspectral Image Classification With Pre-Activation Residual Attention Network</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>176587</fpage><lpage>176599</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2957163</pub-id></element-citation></ref><ref id="B36-sensors-24-08020"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>White</surname><given-names>J.</given-names></name>
<name><surname>Ruiz-Serra</surname><given-names>J.</given-names></name>
<name><surname>Petrie</surname><given-names>S.</given-names></name>
<name><surname>Kameneva</surname><given-names>T.</given-names></name>
<name><surname>McCarthy</surname><given-names>C.</given-names></name>
</person-group><article-title>Self-Attention Based Vision Processing for Prosthetic Vision</article-title><source>Proceedings of the 2023 45th Annual International Conference of the IEEE Engineering in Medicine &#x00026; Biology Society (EMBC)</source><conf-loc>Sydney, Australia</conf-loc><conf-date>24&#x02013;27 July 2023</conf-date><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/EMBC40787.2023.10341053</pub-id></element-citation></ref><ref id="B37-sensors-24-08020"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Zhou</surname><given-names>W.</given-names></name>
<name><surname>Jia</surname><given-names>Y.</given-names></name>
</person-group><article-title>Attention GAN for Multipath Error Removal From ToF Sensors</article-title><source>IEEE Sens. J.</source><year>2022</year><volume>22</volume><fpage>19713</fpage><lpage>19721</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2022.3203759</pub-id></element-citation></ref><ref id="B38-sensors-24-08020"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mai</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>R.</given-names></name>
<name><surname>Jeong</surname><given-names>J.</given-names></name>
<name><surname>Quispe</surname><given-names>D.</given-names></name>
<name><surname>Kim</surname><given-names>H.</given-names></name>
<name><surname>Sanner</surname><given-names>S.</given-names></name>
</person-group><article-title>Online continual learning in image classification: An empirical survey</article-title><source>Neurocomputing</source><year>2022</year><volume>469</volume><fpage>28</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2021.10.021</pub-id></element-citation></ref><ref id="B39-sensors-24-08020"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lomonaco</surname><given-names>V.</given-names></name>
<name><surname>Pellegrini</surname><given-names>L.</given-names></name>
<name><surname>Rodriguez</surname><given-names>P.</given-names></name>
<name><surname>Caccia</surname><given-names>M.</given-names></name>
<name><surname>She</surname><given-names>Q.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Jodelet</surname><given-names>Q.</given-names></name>
<name><surname>Wang</surname><given-names>R.</given-names></name>
<name><surname>Mai</surname><given-names>Z.</given-names></name>
<name><surname>Vamithzquez</surname><given-names>D.</given-names></name>
<etal/>
</person-group><article-title>CVPR 2020 continual learning in computer vision competition: Approaches, results, current challenges and future directions</article-title><source>Artif. Intell.</source><year>2022</year><volume>303</volume><fpage>103635</fpage><pub-id pub-id-type="doi">10.1016/j.artint.2021.103635</pub-id></element-citation></ref><ref id="B40-sensors-24-08020"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>De Lange</surname><given-names>M.</given-names></name>
<name><surname>Aljundi</surname><given-names>R.</given-names></name>
<name><surname>Masana</surname><given-names>M.</given-names></name>
<name><surname>Parisot</surname><given-names>S.</given-names></name>
<name><surname>Jia</surname><given-names>X.</given-names></name>
<name><surname>Leonardis</surname><given-names>A.</given-names></name>
<name><surname>Slabaugh</surname><given-names>G.</given-names></name>
<name><surname>Tuytelaars</surname><given-names>T.</given-names></name>
</person-group><article-title>A Continual Learning Survey: Defying Forgetting in Classification Tasks</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>44</volume><fpage>3366</fpage><lpage>3385</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2021.3057446</pub-id><pub-id pub-id-type="pmid">33544669</pub-id>
</element-citation></ref><ref id="B41-sensors-24-08020"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yan</surname><given-names>X.</given-names></name>
<name><surname>Gilani</surname><given-names>S.Z.</given-names></name>
<name><surname>Qin</surname><given-names>H.</given-names></name>
<name><surname>Mian</surname><given-names>A.</given-names></name>
</person-group><article-title>Structural similarity loss for learning to fuse multi-focus images</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>6647</elocation-id><pub-id pub-id-type="doi">10.3390/s20226647</pub-id><pub-id pub-id-type="pmid">33233568</pub-id>
</element-citation></ref><ref id="B42-sensors-24-08020"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>You</surname><given-names>K.</given-names></name>
<name><surname>Long</surname><given-names>M.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Jordan</surname><given-names>M.I.</given-names></name>
</person-group><article-title>How does learning rate decay help modern neural networks?</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1908.01878</pub-id></element-citation></ref><ref id="B43-sensors-24-08020"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Konar</surname><given-names>J.</given-names></name>
<name><surname>Khandelwal</surname><given-names>P.</given-names></name>
<name><surname>Tripathi</surname><given-names>R.</given-names></name>
</person-group><article-title>Comparison of Various Learning Rate Scheduling Techniques on Convolutional Neural Network</article-title><source>Proceedings of the 2020 IEEE International Students&#x02019; Conference on Electrical, Electronics and Computer Science (SCEECS)</source><conf-loc>Bhopal, India</conf-loc><conf-date>22&#x02013;23 February 2020</conf-date><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/SCEECS48394.2020.94</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-24-08020-f001"><label>Figure 1</label><caption><p>The motion blur effect occurs due to the correlation of four subsequent DCS frames. These four DCS frames are captured one after another, with an inter-frame interval time. When the sensor or environment moves, the object&#x02019;s position in each DCS frame changes accordingly. Consequently, when the four DCS frames are correlated, a blurred image of the object is formed. The motion blur of the iTOF is unique because it appears in the form of blurred objects or trailing traces.</p></caption><graphic xlink:href="sensors-24-08020-g001" position="float"/></fig><fig position="float" id="sensors-24-08020-f002"><label>Figure 2</label><caption><p>Samples from the dataset display the blur effect in the ambiguous depth maps. The top three frames depict the ambiguous depth maps of three different frames. The objects inside the ellipses are affected by motion blur noise. The bottom three frames illustrate ambiguous depth maps where objects inside the boxes are blurred and exhibit trailing traces. For instance, each vehicle wheel displays several trailing traces.</p></caption><graphic xlink:href="sensors-24-08020-g002" position="float"/></fig><fig position="float" id="sensors-24-08020-f003"><label>Figure 3</label><caption><p>Due to a time shift between capturing the DCS samples forming the ambiguous depth map and the grayscale image, along with the sensor&#x02019;s motion and changes in the environment, the size and position of each object change between the grayscale frame and the corresponding ambiguous depth map. This alteration causes a non-uniform spatial transformation of objects.</p></caption><graphic xlink:href="sensors-24-08020-g003" position="float"/></fig><fig position="float" id="sensors-24-08020-f004"><label>Figure 4</label><caption><p>Samples from the dataset demonstrate the non-uniform spatial transformation of objects. The top row displays the ambiguous depth map, the middle row shows the corresponding ground truth depth map, and the bottom row depicts the corresponding grayscale images. The three columns represent three frames captured at subsequent time slots. Each blue box indicates that the car&#x02019;s object in the ambiguous depth map is synchronized with the ground truth in the same frame. Meanwhile, the yellow box illustrates how the object&#x02019;s position is shifted between the ambiguous depth map and the grayscale for the same frame.</p></caption><graphic xlink:href="sensors-24-08020-g004" position="float"/></fig><fig position="float" id="sensors-24-08020-f005"><label>Figure 5</label><caption><p>The model architecture comprises two branches. The first branch is the Main Unwrapped Depth Map Prediction Branch, an autoencoder architecture with four stages: the TofRegNetMotion input feature extraction stage for extracting overall depth context; ResNet Preactivation backbone for detailed feature extraction; TofRegNetMotion output decoder backbone for simultaneous prediction of unwrapped depth map and removal of motion blur; and depth regression head for the final detailed prediction of unwrapped depth map. The other branch is the Motion Blur Inverse Tensor Prediction Branch. This consists of three primary components: The ASIPM and Visual Attention Network modules process the wrapped depth map simultaneously, and the Inverse Blur Tensor module generates the final motion blur inverse tensor feature map. An additional option branch for the grayscale synchronization branch based on the STN network may be used in concatenation with the ASIPM and Visual Attention Network.</p></caption><graphic xlink:href="sensors-24-08020-g005" position="float"/></fig><fig position="float" id="sensors-24-08020-f006"><label>Figure 6</label><caption><p><bold>Continuous training framework.</bold> The model is trained only using the static dataset during the pretraining phase. During the training phase, the model is trained using the dynamic dataset and a subset of the static dataset, represented by random batches.</p></caption><graphic xlink:href="sensors-24-08020-g006" position="float"/></fig><fig position="float" id="sensors-24-08020-f007"><label>Figure 7</label><caption><p><bold>Samples from the 4-DCS dataset and 2-DCS dataset.</bold> Images in row (<bold>a</bold>) show ambiguous depth maps created using 2-DCS frames; images in row (<bold>b</bold>) show the corresponding ambiguous depth created using 4-DCS frames; images in row (<bold>c</bold>) show corresponding ground depth maps with correct depth; and images at row (<bold>d</bold>) show corresponding grayscale frames.</p></caption><graphic xlink:href="sensors-24-08020-g007" position="float"/></fig><fig position="float" id="sensors-24-08020-f008"><label>Figure 8</label><caption><p>The analysis focuses on the test frames of the 2-DCS dataset, with selected frame groups based on RMSE and MAE values. The targeted groups comprise frames with high RMSE and MAE values. As per the selection criteria, frames are categorized into groups: (a) frames 0 to 6; (b) frames 8 to 18; (c) frames 56 to 62; (d) frames 90 to 115; and (e) frames 250 to 260. The most significant metric graphs in the figure are the RMSE and MAE graphs; the other metric graphs (i.e., iRMSE, iMAE, SqRel, AbsSqRel, and SiL) are of very small value, and so they are superimposed over each other near a value of 0.</p></caption><graphic xlink:href="sensors-24-08020-g008" position="float"/></fig><fig position="float" id="sensors-24-08020-f009"><label>Figure 9</label><caption><p>A sample from Group A frames. The upper row shows the predicted unwrapped depth maps in 2D and 3D views. The lower row shows the ground truth unwrapped depth maps in 2D and 3D views. As shown in the figure, the vehicle point cloud highlighted inside the yellow box in the corrected depth map in the upper row has less motion blur trailing traces and more homogeneous sharp depth than the corresponding vehicle point cloud in the ground truth in the lower row, while maintaining the same level of detail for the point cloud of the scene. The yellow boxes in the 2D frames compare the correction results between the corrected depth and ground truth frames. The Green lines show the cross-pending corrected areas in the point cloud in the 3D point cloud.</p></caption><graphic xlink:href="sensors-24-08020-g009" position="float"/></fig><fig position="float" id="sensors-24-08020-f010"><label>Figure 10</label><caption><p>A sample from Group B frames. The upper row shows the predicted unwrapped depth maps in 2D and 3D views. The lower row shows the ground truth unwrapped depth maps in 2D and 3D views. As shown in the figure, the ground truth point cloud suffers badly from motion blur trailing traces to the extent that there are several shadows for the wheels. On the other hand, the model could reduce the motion blur trailing traces and sharpen the details of the objects like the wheels. The yellow boxes in the 2D frames compare the correction results between the corrected depth and ground truth frames. The Green lines show the cross-pending corrected areas in the point cloud in the 3D point cloud.</p></caption><graphic xlink:href="sensors-24-08020-g010" position="float"/></fig><fig position="float" id="sensors-24-08020-f011"><label>Figure 11</label><caption><p>A sample from Group C frames. The upper row shows the predicted unwrapped depth maps in 2D and 3D views. The lower row shows the ground truth unwrapped depth maps in 2D and 3D views. In addition, we show the grayscale frame capture along with the ground truth. The vehicle point cloud inside the yellow box in the corrected depth map shows a sharper point cloud with reduced motion blur and reduced trailing traces compared to the vehicle point cloud in the ground truth. The blue box shows the captured detailed comparison between the corrected depth map and the ground truth map to the grayscale frame. It is evident that the corrected depth map contains more details than the ground truth, and these details are not just noise or prediction error, but they represent valid details compared to the grayscale frame. The yellow boxes in the 2D frames compare the correction results between the corrected depth and ground truth frames. The Green lines show the cross-pending corrected areas in the point cloud in the 3D point cloud.</p></caption><graphic xlink:href="sensors-24-08020-g011" position="float"/></fig><fig position="float" id="sensors-24-08020-f012"><label>Figure 12</label><caption><p>A sample from Group D frames. The upper row shows the predicted unwrapped depth maps in 2D view and 3D view. The lower row shows the ground truth unwrapped depth maps in 2D view and 3D view. In addition, we show the grayscale frame capture along with the ground truth. The blue and yellow boxes in the corrected depth maps show captured details not detected by the ground truth. Compared to the grayscale, it is not just a prediction error; they are accurate details not captured in the ground truth. The yellow boxes in the 2D frames compare the correction results between the corrected depth and ground truth frames. The Green lines show the cross-pending corrected areas in the point cloud in the 3D point cloud.</p></caption><graphic xlink:href="sensors-24-08020-g012" position="float"/></fig><fig position="float" id="sensors-24-08020-f013"><label>Figure 13</label><caption><p>A sample from Group E frames. The upper row shows the predicted unwrapped depth maps in 2D and 3D views. The lower row shows the ground truth unwrapped depth maps in 2D and 3D views. In addition, we show the grayscale frame capture along with the ground truth. Comparing the point clouds of the objects inside the blue, yellow, and green boxes in the corrected and ground truth depth map, the corrected depth map has more details and a sharper point cloud. Comparing the corrected depth map to the grayscale, it is evident that it captures more valid details with less motion blur noise than the ground truth. The yellow boxes in the 2D frames compare the correction results between the corrected depth and ground truth frames. The Green lines show the cross-pending corrected areas in the point cloud in the 3D point cloud.</p></caption><graphic xlink:href="sensors-24-08020-g013" position="float"/></fig><fig position="float" id="sensors-24-08020-f014"><label>Figure 14</label><caption><p>The diagram compares the training performance using (<bold>a</bold>) the dynamic dataset only, versus (<bold>b</bold>) the static dataset followed by the dynamic dataset. The diagram shows that the learning curve converges better when using the static dataset followed by the dynamic dataset, compared to using the dynamic dataset only.</p></caption><graphic xlink:href="sensors-24-08020-g014" position="float"/></fig><table-wrap position="float" id="sensors-24-08020-t001"><object-id pub-id-type="pii">sensors-24-08020-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparative analysis between our method and closest state-of-the-art methods on the outdoor static dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Input</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE<break/>
(m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AbsRel</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SqRel</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Modified Wild ToFu</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>iToF Wrapped Depth Map</bold>
<break/>
<bold>(4-DCS + GS)</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.85</td><td align="center" valign="middle" rowspan="1" colspan="1">1.57</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>iToF Wrapped Depth Map</bold>
<break/>
<bold>(4-DCS + GS)</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.23</td><td align="center" valign="middle" rowspan="1" colspan="1">0.42</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>iToF Wrapped Depth Map</bold>
<break/>
<bold>(2-DCS + GS)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.43</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-08020-t002"><object-id pub-id-type="pii">sensors-24-08020-t002_Table 2</object-id><label>Table 2</label><caption><p>Quantitative analysis for the results of the method using different datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Prediction<break/>
Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Correction<break/>
Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE<break/>
(m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">iRMSE<break/>
(1/m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SqRel</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AbsRel</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SiLog</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAE<break/>
(m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">iMAE<break/>
(1/m)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Two-DCS</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Static</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">99%</td><td align="center" valign="middle" rowspan="1" colspan="1">99%</td><td align="center" valign="middle" rowspan="1" colspan="1">1.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.31</td><td align="center" valign="middle" rowspan="1" colspan="1">0.43</td><td align="center" valign="middle" rowspan="1" colspan="1">0.26</td><td align="center" valign="middle" rowspan="1" colspan="1">0.17</td><td align="center" valign="middle" rowspan="1" colspan="1">0.28</td><td align="center" valign="middle" rowspan="1" colspan="1">0.043</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Dynamic</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.69%</td><td align="center" valign="middle" rowspan="1" colspan="1">89.45%</td><td align="center" valign="middle" rowspan="1" colspan="1">3.89</td><td align="center" valign="middle" rowspan="1" colspan="1">0.29</td><td align="center" valign="middle" rowspan="1" colspan="1">0.021</td><td align="center" valign="middle" rowspan="1" colspan="1">0.012</td><td align="center" valign="middle" rowspan="1" colspan="1">0.43</td><td align="center" valign="middle" rowspan="1" colspan="1">2.31</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0017</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Four-DCS</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Static</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">99%</td><td align="center" valign="middle" rowspan="1" colspan="1">99%</td><td align="center" valign="middle" rowspan="1" colspan="1">1.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.31</td><td align="center" valign="middle" rowspan="1" colspan="1">0.42</td><td align="center" valign="middle" rowspan="1" colspan="1">0.24</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.25</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Dynamic</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.74%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.13%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0024</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-08020-t003"><object-id pub-id-type="pii">sensors-24-08020-t003_Table 3</object-id><label>Table 3</label><caption><p>Ablation study analysis between different implementations of our method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Prediction Frame</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Correction Frame</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">RMSE<break/>(m)</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">iRMSE<break/>(1/m)</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">SqRel</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">AbsRel</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">SiLog</th></tr><tr><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Top 90%<break/>Precision</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Top 90%<break/>Precision</th></tr></thead><tbody><tr><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<bold>Two-DCS</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Full Mode</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.69%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.69%</td><td align="center" valign="middle" rowspan="1" colspan="1">89.45%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.81%</td><td align="center" valign="middle" rowspan="1" colspan="1">3.88</td><td align="center" valign="middle" rowspan="1" colspan="1">2.92</td><td align="center" valign="middle" rowspan="1" colspan="1">0.021</td><td align="center" valign="middle" rowspan="1" colspan="1">0.012</td><td align="center" valign="middle" rowspan="1" colspan="1">0.42</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>W/o STN</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">88.74%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.71%</td><td align="center" valign="middle" rowspan="1" colspan="1">88.61%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.88%</td><td align="center" valign="middle" rowspan="1" colspan="1">4.16</td><td align="center" valign="middle" rowspan="1" colspan="1">3.29</td><td align="center" valign="middle" rowspan="1" colspan="1">0.029</td><td align="center" valign="middle" rowspan="1" colspan="1">0.016</td><td align="center" valign="middle" rowspan="1" colspan="1">0.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>W/o Inverse</bold>
<break/>
<bold>Blur</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">87.74%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.63%</td><td align="center" valign="middle" rowspan="1" colspan="1">87.41%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.72%</td><td align="center" valign="middle" rowspan="1" colspan="1">4.35</td><td align="center" valign="middle" rowspan="1" colspan="1">3.94</td><td align="center" valign="middle" rowspan="1" colspan="1">0.030</td><td align="center" valign="middle" rowspan="1" colspan="1">0.017</td><td align="center" valign="middle" rowspan="1" colspan="1">1.05</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Using GS</bold>
<break/>
<bold>Only</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">71.50%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.17%</td><td align="center" valign="middle" rowspan="1" colspan="1">71.48%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.17%</td><td align="center" valign="middle" rowspan="1" colspan="1">7.22</td><td align="center" valign="middle" rowspan="1" colspan="1">8.53</td><td align="center" valign="middle" rowspan="1" colspan="1">0.210</td><td align="center" valign="middle" rowspan="1" colspan="1">0.086</td><td align="center" valign="middle" rowspan="1" colspan="1">6.16</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Using GS</bold>
<break/>
<bold>w/o STN</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.36%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.18%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.33%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.18%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.220</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.087</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.17</td></tr><tr><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<bold>Four-DCS</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Full Mode</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.47%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.43%</td><td align="center" valign="middle" rowspan="1" colspan="1">89.13%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.81%</td><td align="center" valign="middle" rowspan="1" colspan="1">4.03</td><td align="center" valign="middle" rowspan="1" colspan="1">2.99</td><td align="center" valign="middle" rowspan="1" colspan="1">0.023</td><td align="center" valign="middle" rowspan="1" colspan="1">0.013</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>W/o STN</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">84.70%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.50%</td><td align="center" valign="middle" rowspan="1" colspan="1">85.18%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.02%</td><td align="center" valign="middle" rowspan="1" colspan="1">4.76</td><td align="center" valign="middle" rowspan="1" colspan="1">4.14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.043</td><td align="center" valign="middle" rowspan="1" colspan="1">0.022</td><td align="center" valign="middle" rowspan="1" colspan="1">1.77</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>W/o Inverse</bold>
<break/>
<bold>Blur</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">86.23%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.10%</td><td align="center" valign="middle" rowspan="1" colspan="1">87.05%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.00%</td><td align="center" valign="middle" rowspan="1" colspan="1">4.38</td><td align="center" valign="middle" rowspan="1" colspan="1">3.45</td><td align="center" valign="middle" rowspan="1" colspan="1">0.028</td><td align="center" valign="middle" rowspan="1" colspan="1">0.016</td><td align="center" valign="middle" rowspan="1" colspan="1">0.70</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Using GS</bold>
<break/>
<bold>Only</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">71.36%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.18%</td><td align="center" valign="middle" rowspan="1" colspan="1">71.35%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.18%</td><td align="center" valign="middle" rowspan="1" colspan="1">7.20</td><td align="center" valign="middle" rowspan="1" colspan="1">8.53</td><td align="center" valign="middle" rowspan="1" colspan="1">0.220</td><td align="center" valign="middle" rowspan="1" colspan="1">0.086</td><td align="center" valign="middle" rowspan="1" colspan="1">6.17</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Using GS</bold>
<break/>
<bold>w/o STN</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.30%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.88%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.58%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.22%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.082</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.041</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.44</td></tr></tbody></table></table-wrap></floats-group></article>