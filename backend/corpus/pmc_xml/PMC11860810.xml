<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Tomography</journal-id><journal-id journal-id-type="iso-abbrev">Tomography</journal-id><journal-id journal-id-type="publisher-id">tomography</journal-id><journal-title-group><journal-title>Tomography</journal-title></journal-title-group><issn pub-type="ppub">2379-1381</issn><issn pub-type="epub">2379-139X</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39997996</article-id><article-id pub-id-type="pmc">PMC11860810</article-id>
<article-id pub-id-type="doi">10.3390/tomography11020013</article-id><article-id pub-id-type="publisher-id">tomography-11-00013</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Impact of Deep Learning 3D CT Super-Resolution on AI-Based Pulmonary Nodule Characterization</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kim</surname><given-names>Dongok</given-names></name><xref rid="af1-tomography-11-00013" ref-type="aff">1</xref><xref rid="af2-tomography-11-00013" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Ahn</surname><given-names>Chulkyun</given-names></name><xref rid="af2-tomography-11-00013" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5695-4976</contrib-id><name><surname>Kim</surname><given-names>Jong Hyo</given-names></name><xref rid="af1-tomography-11-00013" ref-type="aff">1</xref><xref rid="af2-tomography-11-00013" ref-type="aff">2</xref><xref rid="af3-tomography-11-00013" ref-type="aff">3</xref><xref rid="af4-tomography-11-00013" ref-type="aff">4</xref><xref rid="c1-tomography-11-00013" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Tyrrell</surname><given-names>Pascal N.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-tomography-11-00013"><label>1</label>Department of Applied Bioengineering, Graduate School of Convergence Science and Technology, Seoul National University, Seoul 08826, Republic of Korea; <email>dongk@snu.ac.kr</email></aff><aff id="af2-tomography-11-00013"><label>2</label>ClariPi Research, ClariPi Inc., Seoul 03088, Republic of Korea; <email>rnd3456@claripi.com</email></aff><aff id="af3-tomography-11-00013"><label>3</label>Department of Radiology, Seoul National University Hospital and College of Medicine, Seoul 03080, Republic of Korea</aff><aff id="af4-tomography-11-00013"><label>4</label>Center for Medical-IT Convergence Technology Research, Advanced Institutes of Convergence Technology, Suwon 16229, Republic of Korea</aff><author-notes><corresp id="c1-tomography-11-00013"><label>*</label>Correspondence: <email>kimjhyo@snu.ac.kr</email></corresp></author-notes><pub-date pub-type="epub"><day>27</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>11</volume><issue>2</issue><elocation-id>13</elocation-id><history><date date-type="received"><day>28</day><month>11</month><year>2024</year></date><date date-type="rev-recd"><day>01</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>15</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Background/Objectives: Correct pulmonary nodule volumetry and categorization is paramount for accurate diagnosis in lung cancer screening programs. CT scanners with slice thicknesses of multiple millimetres are still common worldwide, and slice thickness has an adverse effect on the accuracy of the pulmonary nodule volumetry. Methods: We propose a deep learning based super-resolution technique to generate thin-slice CT images from thick-slice CT images. Analysis of the lung nodule volumetry and categorization accuracy was performed using commercially available AI-based lung cancer screening software. Results: The accuracy of pulmonary nodule categorization increased from 72.7 percent to 94.5 percent when thick-slice CT images were converted to generated-thin-slice CT images. Conclusions: Applying the super-resolution-based slice generation on thick-slice CT images prior to automatic nodule evaluation significantly increases the accuracy of pulmonary nodule volumetry and corresponding pulmonary nodule category.</p></abstract><kwd-group><kwd>deep learning</kwd><kwd>computed tomography</kwd><kwd>lung nodules</kwd><kwd>super-resolution</kwd><kwd>slice thickness</kwd></kwd-group><funding-group><award-group><funding-source>Ministry of Health and Welfare</funding-source><award-id>H502-24-1001</award-id></award-group><funding-statement>This study was performed with financial support from the Ministry of Health and Welfare (MOHW) in 2024, under grant no. (H502-24-1001).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-tomography-11-00013"><title>1. Introduction</title><p>National lung screening programs initiated in Europe have increased the demand for chest computed tomography (CT) scans, and when it comes to pulmonary nodule volumetry, slice thickness in CT images is a key factor [<xref rid="B1-tomography-11-00013" ref-type="bibr">1</xref>]. Unfortunately, not all CT scans are taken with uniform thin-slice thickness, and therefore a method to neutralize the effect of heterogeneous slice thickness is desired. Here, we introduce a deep learning-based super-resolution method to convert thick-slice CT images to generated-thin-slice CT images.</p><p>Advancements in deep learning, particularly in super-resolution techniques, have significantly surpassed traditional interpolation methods in image quality restoration. Dong et al. [<xref rid="B2-tomography-11-00013" ref-type="bibr">2</xref>] pioneered one of the earliest super-resolution methods using a three-layer convolutional neural network (CNN), achieving superior results compared to conventional interpolation techniques. However, the shallow depth of the network constrained its ability to effectively recover high-frequency image details. To address this limitation, Kim et al. [<xref rid="B3-tomography-11-00013" ref-type="bibr">3</xref>,<xref rid="B4-tomography-11-00013" ref-type="bibr">4</xref>] incorporated ResNet principles, designing a 32-layer CNN with skip connections to mitigate the vanishing gradient problem. Zhang et al. [<xref rid="B5-tomography-11-00013" ref-type="bibr">5</xref>] further refined the approach by introducing a residual-in-residual structure, resulting in high peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values.</p><p>Ledig et al. [<xref rid="B6-tomography-11-00013" ref-type="bibr">6</xref>] highlighted the limitations of relying solely on the L1 loss function, which tends to produce blurred outputs by prioritizing averaged pixel values and sacrificing high-frequency details. Additionally, PSNR and SSIM metrics do not always correlate well with perceived image quality. To address these issues, they proposed a perceptual loss based on feature maps from a pre-trained VGG network [<xref rid="B7-tomography-11-00013" ref-type="bibr">7</xref>] and utilized a generative adversarial network (GAN) [<xref rid="B8-tomography-11-00013" ref-type="bibr">8</xref>] for training. In this framework, the generator created super-resolved (SR) images from low-resolution (LR) inputs, while the discriminator differentiated between SR images and high-resolution (HR) ground truths. This adversarial training enabled the generator to produce images nearly indistinguishable from HR images, leading to the development of a Super-resolution Generative Adversarial Network (SRGAN). Although SRGAN did not achieve the highest PSNR and SSIM scores, it generated SR images with perceptual quality closer to HR images.</p><p>Based on SRGAN, Wang et al. [<xref rid="B9-tomography-11-00013" ref-type="bibr">9</xref>] introduced Enhanced SRGAN (ESRGAN), incorporating residual-in-residual dense blocks without batch normalization, which further improved image quality. Subsequently, Real-ESRGAN [<xref rid="B10-tomography-11-00013" ref-type="bibr">10</xref>] added degradations such as noise, blur, and JPEG compression during training to enhance robustness for real-world applications.</p><p>In the medical imaging domain, researchers have actively adapted deep learning-enabled super-resolution techniques to advance the field. Park et al. [<xref rid="B11-tomography-11-00013" ref-type="bibr">11</xref>,<xref rid="B12-tomography-11-00013" ref-type="bibr">12</xref>,<xref rid="B13-tomography-11-00013" ref-type="bibr">13</xref>] utilized residual networks to up-sample in the z-direction and generate thin CT slices, leveraging this technology to explore radiomic features in chest CT. More specifically, they explored whether SR methods improve radiomic feature reproducibility across varying CT slice thicknesses. A total dataset of one hundred series of CT images from lung cancer patients with 1 mm, 3 mm and 5 mm slices thicknesses were used to train the model. The developed model was then used to convert 3 mm and 5 mm thick slices to 1 mm thin slices. Radiomic reproducibility decreased with increased slice thickness, but the SR technique standardized radiomic feature extraction and improved diagnostic consistency.</p><p>Yun et al. [<xref rid="B14-tomography-11-00013" ref-type="bibr">14</xref>,<xref rid="B15-tomography-11-00013" ref-type="bibr">15</xref>] generated thin CT slices to achieve higher quality orbital bone reconstructions. They identified critical issues in the field of 3D orbital bone reconstruction, which were the aliasing effects and structural disconnections caused by thick CT slice thickness. The orbital bone, especially its thin structures like the medial wall and orbital floor, is prone to fractures, but thick-slice CT images fail to accurately represent these fine structures, hampering the creation of precise 3D models for surgical implants. To convert thick slices to thin slices, orbital bone edge-aware loss was implemented where it specifically targeted the orbital bone region using a mask that identified bone edges, ensuring the preservation of thin and cortical bone structures. The network architecture had a 2D CNN with six convolutional layers for feature extraction and up-sampling layers for inter-slice resolution enhancement. Skip connections between the layers maintained low-level features, such as textures and gradients, crucial for retaining edge details. The proposed network offered a robust solution for improving inter-slice resolution in facial CT imaging, particularly for orbital bone reconstruction.</p><p>Nakamoto et al. [<xref rid="B16-tomography-11-00013" ref-type="bibr">16</xref>] also showed a method used to generate thin-slice CT images from thicker-slice CT images. Their virtual thin-slice technique used a conditional generative adversarial network framework, comprising an encoder&#x02013;decoder-style generator inspired by U-Net and a discriminator designed for 3D data processing. The generator employed skip connections to retain spatial information, enabling the reconstruction of high-resolution thin-slice CT images from thick-slice CT inputs. It used 3D convolutional layers to handle the volumetric nature of CT data, optimizing the output through adversarial loss, which promoted realistic image generation, and L1 loss, which minimized pixel-wise intensity differences between generated and true thin-slice images. The discriminator, also built with 3D convolutional layers, distinguished real thin-slice images from generated ones and incorporated conditioning labels, such as slice intervals, to improve the accuracy of the super-resolution process. Together, the generator and discriminator were trained to produce isotropic thin-slice images with a voxel size of 1 &#x000d7; 1 &#x000d7; 1 mm, ensuring high fidelity and realistic morphology for volumetric CT image reconstruction. This enhanced the visibility of anatomical details in CT images, particularly in high-contrast regions like bone. However, they concluded that it may have had diagnostic limitations for subtle abnormalities, highlighting the need for further refinement, due to the tendency of GANs to normalize slight deviations, such as mild compression fractures, which could affect diagnostic utility.</p><p>Iwano et al. [<xref rid="B17-tomography-11-00013" ref-type="bibr">17</xref>,<xref rid="B18-tomography-11-00013" ref-type="bibr">18</xref>] used same model to show how the measurement of solid size in early-stage lung adenocarcinoma affected in thick-slice CT images and virtual 3D thin-slice CT images. They concluded that the conversion of thick-slice CT images to thin-slice CT images improved the detection and measurement of pulmonary nodules, thereby increasing staging accuracy and possibly providing better prognosis prediction.</p><p>Building upon these advancements, we propose a deep learning-based three-dimensional (3D) super-resolution method to generate thin-slice CT images from heterogeneous thick-slice CT images. Following on, a commercially available AI-based lung cancer screening software (ClariPulmo, v2.0.0, ClariPi, Seoul, Republic of Korea) [<xref rid="B19-tomography-11-00013" ref-type="bibr">19</xref>] was utilized to evaluate the consistency of pulmonary nodule categorization by measuring pulmonary nodule volumes and classifying the nodules in accordance with well-established Lung-RADS v2022 published by the American College of Radiology on thick-, generated-thin- and thin-slice CT images.</p></sec><sec id="sec2-tomography-11-00013"><title>2. Materials and Methods</title><sec id="sec2dot1-tomography-11-00013"><title>2.1. Training Dataset</title><p>For training the deep learning model for CT slice generation, we used the National Lung Screening Trial (NLST) open dataset. CT images with slice thicknesses of 1.0 mm were selected, which consisted of 18,243 images from the Siemens Sensation 16 scanner, 2931 images from the Canon Aquilion scanner, 2707 images from the GE LightSpeed 16, scanner and 4747 images from the Philips MX8000 scanner, resulting in a total of 28,628 chest CT images (<xref rid="tomography-11-00013-t001" ref-type="table">Table 1</xref>).</p></sec><sec id="sec2dot2-tomography-11-00013"><title>2.2. Test Dataset</title><p>For testing the deep learning model for CT slice generation, we used the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) open dataset. LIDC-IDRI contains 1018 series, and among them we selected thin-slice CT images which had slice thicknesses of 1.0 mm or less, and that resulted in 83 series and 304 nodules. Lung-RADS v2022 categorises pulmonary nodules between 2, 3, 4A, and 4B, based on its solid volume and total volume, and those volumes of interest mostly range between 80 and 350 mm<sup>3</sup>. Therefore, we further filtered the nodule sizes to be in that range, which left us with a total of 40 series and 55 nodules to be used as the test dataset (<xref rid="tomography-11-00013-f001" ref-type="fig">Figure 1</xref>).</p></sec><sec id="sec2dot3-tomography-11-00013"><title>2.3. Model Architecture</title><p>The base of our super-resolution framework was influenced by ESRGAN, with necessary modification made to suit medical imaging applications. Unlike natural images, CT images are acquired in controlled environments, resulting in consistent image quality. Consequently, the introduction of degradations such as noise, blur, and JPEG compression, as implemented in Real-ESRGAN, was deemed unnecessary for this application.</p><p>Traditional super-resolution networks are typically designed for single-image super-resolution. However, CT images possess volumetric properties, necessitating adaptations to handle 3D data. To accommodate this, we modified the network&#x02019;s convolutions to process 3D arrays, such as 512 &#x000d7; 512 &#x000d7; 16 arrays. Due to GPU memory constraints, CT volumes were partitioned into segments of 16 slices before being input into the network.</p><p>The network architecture, depicted in <xref rid="tomography-11-00013-f002" ref-type="fig">Figure 2</xref>A, is centered around a generator network, shown in <xref rid="tomography-11-00013-f002" ref-type="fig">Figure 2</xref>B, responsible for converting LR inputs into SR outputs. The generator consisted of 23 ResBlocks as its core building units. Each ResBlock incorporated a central skip connection and comprised two small blocks. Each small block included five alternating 3D CNNs and four leaky rectified linear units (LReLUs), with dense connections preceding each 3D CNN to facilitate residual information flow. Three-dimensional CNN incorporates 64 channels, a kernel size of 3, a stride of 1, and padding of 1. This design improved information propagation efficiency and enhanced the model&#x02019;s ability to learn complex features. Local residual connections within each small block stabilized the training process by mitigating the vanishing gradient problem commonly observed in deep networks. Additionally, a global residual connection linked the input of the first ResBlock to the output of the last ResBlock, forming a residual-in-residual structure. This dual-level residual architecture enhanced the network&#x02019;s ability to capture and synthesize high-frequency details critical for reconstructing high-resolution images with fine textures and sharp edges.</p><p>Following the ResBlocks&#x02019; processing, the network applied a 3D CNN and a custom-designed VoxelShuffle-Slice Generation (SG) module, illustrated in <xref rid="tomography-11-00013-f002" ref-type="fig">Figure 2</xref>D, to enhance spatial resolution in the depth dimension while preserving learned features. The VoxelShuffle-SG module, adapted from PyTorch&#x02019;s PixelShuffle module [<xref rid="B20-tomography-11-00013" ref-type="bibr">20</xref>], was tailored for 3D data processing. Whereas PixelShuffle transforms tensors from the channel domain to the spatial domain in two dimensions, VoxelShuffle-SG extends this functionality to 3D arrays, transferring tensors from the channel domain to the depth dimension. For instance, an input tensor of shape (batch, channel &#x000d7; r<sup>2</sup>, height, width) processed by PixelShuffle resulted in an output tensor of shape (batch, channel, height &#x000d7; r, width &#x000d7; r), where r represents the scaling factor. Similarly, when VoxelShuffle-SG processed an input tensor of shape (batch, channel &#x000d7; r, height, width, depth), the output tensor was reshaped to (batch, channel, height, width, depth &#x000d7; r). Subsequently, two 3D CNN layers generated the final thin-slice CT images.</p><p>The generated-thin-slice images were then fed into a discriminator network, shown in <xref rid="tomography-11-00013-f002" ref-type="fig">Figure 2</xref>C, alongside the corresponding ground truth images. The discriminator was designed to distinguish between generated (synthetic) and ground truth (real) thin-slice CT images. Its architecture comprised eight pairs of 3D CNN and LReLU layers, followed by a fully connected dense layer producing 1024 output tensors. These tensors passed through an LReLU activation, followed by another fully connected layer outputting a single value, which was then passed through a sigmoid function. An output of one indicated that the discriminator classified the input as ground truth, while an output of zero indicated that it classified the input as generated. The discriminator was trained adversarially against the generator, driving the generator to produce synthetic thin-slice CT images that became increasingly indistinguishable from the ground truth. Ideally, by the end of training, the generator would produce synthetic images perceptually identical to the ground truth, rendering the discriminator unable to differentiate between the two.</p></sec><sec id="sec2dot4-tomography-11-00013"><title>2.4. Training</title><p>To generate thin-slice from thick-slice CT, first, the image needs to be reformatted in the coronal plane. Due to GPU memory limitations, up to 16 slices of coronal CT images were put into the network for slice generation. Training was conducted using 28,628 chest CT images with an initial slice thickness of 1.0 mm, with each series containing approximately 300 slices on average. To simulate thick-slice images, every four consecutive slices were averaged to produce a single slice, effectively reducing the number of slices per series from 300 to 75 and increasing the slice thickness from 1.0 mm to 4.0 mm. These low-resolution, thick-slice CT images were then input into the network to restore the thin-slice images.</p><p>Comparisons between the original 1.0 mm thin-slice CT images and the generated-thin-slice images were performed using three loss functions: mean squared error (MSE) loss (1), perceptual loss (2), and adversarial loss (3). The equations for these loss functions are provided below:<disp-formula id="FD1-tomography-11-00013"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>D</mml:mi><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-tomography-11-00013"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>G</mml:mi><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-tomography-11-00013"><label>(3)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">r</italic> is the scaling factor, which is 4 for this study; <italic toggle="yes">D</italic> is depth; <italic toggle="yes">H</italic> is height; <italic toggle="yes">W</italic> is width; <italic toggle="yes">I<sup>LR</sup></italic> is a thick-slice CT image; <italic toggle="yes">I<sup>GT</sup></italic> is a thin-slice CT image; <italic toggle="yes">G<sub>&#x003b8;</sub></italic> is the generator function; <italic toggle="yes">D<sub>&#x003b8;</sub></italic> is the discriminator function; <italic toggle="yes">&#x003c6;<sub>i<italic toggle="yes">,</italic>j</sub></italic> is the feature map obtained after the <italic toggle="yes">j</italic>-th convolution and before the <italic toggle="yes">i</italic>-th max-pooling layer in the VGG network; and <italic toggle="yes">D<sub>i<italic toggle="yes">,</italic>j</sub></italic>, <italic toggle="yes">H<sub>i<italic toggle="yes">,</italic>j</sub></italic>, and <italic toggle="yes">W<sub>i<italic toggle="yes">,</italic>j</sub></italic> are the dimensions of the respective feature map.</p><p>The MSE loss is a pixel-wise metric that quantifies the average squared difference between the estimated values and the actual values, effectively measuring the square of the Euclidean distance between them. Although minimizing MSE reduces the overall difference in voxel values between the predicted and actual images, relying solely on MSE can lead to blurry textures due to the suppression of high-frequency details. To mitigate this issue, perceptual and adversarial losses were introduced.</p><p>The perceptual loss utilized the VGG16 network, a classification model originally developed for the ImageNet dataset challenge. Let <italic toggle="yes">&#x003c6;<sub>i<italic toggle="yes">,</italic>j</sub></italic> represent the feature map obtained after the j-th convolution and before the i-th max-pooling layer in the VGG network, with <italic toggle="yes">D<sub>i<italic toggle="yes">,</italic>j</sub></italic>, <italic toggle="yes">H<sub>i<italic toggle="yes">,</italic>j</sub></italic> and <italic toggle="yes">W<sub>i<italic toggle="yes">,</italic>j</sub></italic> denoting the dimensions of the respective feature map. As the VGG network was designed for natural 2D images, it could not be directly applied to 3D CT images. To address this, the CT volumes were split into individual slices, which were then sequentially fed into the VGG network. The loss was calculated by summing and averaging over all slices. By minimizing the Euclidean distance between the feature representations of the ground truth thin-slice images and the generated-thin-slice images, the network was encouraged to produce images that were perceptually similar.</p><p>The adversarial loss was derived from the discriminator in the network, which simultaneously encouraged the generator to produce images more similar to the thin-slice images and trained the discriminator to better distinguish between the generated-thin and thin CT images. This adversarial training was carried out until the loss output saturated in both the training and validation datasets. Upon the completion of training, the model was capable of generating high-quality images that were indistinguishable from the real thin-slice CT images.</p></sec><sec id="sec2dot5-tomography-11-00013"><title>2.5. Evaluation with AI-Based Lung Cancer Screening Software</title><p>There was a total of 55 pulmonary nodules in the test dataset, and they were presented in the form of thick, generated-thin- and thin-slice CT images. ClariPulmo was used to evaluate their pulmonary nodule volumetry and corresponding pulmonary nodule category. ClariPulmo performs nodule detection, segmentation, and type prediction using deep learning models with CNN architecture and a histogram-based approach for nodule type prediction. The core of the system consists of an end-to-end framework utilizing three modified 3D ResNet architectures for nodule detection and segmentation [<xref rid="B21-tomography-11-00013" ref-type="bibr">21</xref>]. These architectures were enhanced with attention mechanisms and weighted loss functions to improve performance. Following nodule detection and segmentation, each nodule was classified into solid, sub-solid, and GGN based on its internal density distribution. This classification used a histogram-based approach, analyzing the distribution of CT attenuation values within each segmented nodule. After that, each type of nodules was sorted into categories 2, 3, 4A, and 4B, in accordance with Lung-RADS v2022, depending on the solid volume and the total volume of the nodule, as illustrated in <xref rid="tomography-11-00013-f003" ref-type="fig">Figure 3</xref>. The software&#x02019;s graphical user interface is shown on <xref rid="tomography-11-00013-f004" ref-type="fig">Figure 4</xref>.</p></sec></sec><sec sec-type="results" id="sec3-tomography-11-00013"><title>3. Results</title><p>The representative thick-, generated-thin- and thin-slice coronal CT images of solid nodule and (ground glass nodule) GGN are shown in <xref rid="tomography-11-00013-f005" ref-type="fig">Figure 5</xref> and <xref rid="tomography-11-00013-f006" ref-type="fig">Figure 6</xref>. The thick-slice CT images show staircase artifacts on both solid nodule and GGN, making accurate volume measurement and corresponding categorization nearly impossible, whereas generated-thin-slice CT images show image quality on par with thin-slice CT images, exhibiting clear nodule/GGN structures and well-defined pulmonary vessels delineations.</p><p>A total of 55 nodules from 40 series of test datasets were presented in thick-, generated-thin- and thin-slice CT images. These nodules were analyzed using ClariPulmo, and by setting nodule categories from thin-slice CT images as the ground truth, nodule categories from thick-slice and generated-thin-slice CT images were compared using a confusion matrix, as shown in <xref rid="tomography-11-00013-f007" ref-type="fig">Figure 7</xref>. From the confusion matrix, we can see that thick-slice CT images mis-categorized 15 out of 55 nodules, whereas generated-thin-slice CT images mis-categorized 3 out of 55 nodules. Nodule categorization was further analyzed to observe the category accuracy of thick-slice and generated-thin-slice CT images with respect to the ground truth thin-slice CT images. Among 55 nodules, 2 nodules were correctly categorized by thick-slice CT images but incorrectly categorized by generated-thin-slice CT images, 14 nodules were incorrectly categorized by thick-slice CT images but correctly categorized by generated-thin-slice CT images, 1 nodule was incorrectly categorized by both, and 38 nodules were correctly categorized by both. Therefore, thick-slice CT images had a categorization accuracy of 72.7% and generated-thin-slice CT images had a categorization accuracy of 94.5% (<xref rid="tomography-11-00013-t002" ref-type="table">Table 2</xref>).</p></sec><sec sec-type="discussion" id="sec4-tomography-11-00013"><title>4. Discussion</title><p>The accurate measurement of lung nodule volume is essential for proper categorization in accordance with Lung-RADS v2022. Our results indicate that via using automatic categorization software, lung nodules from thick-slice CT images were misclassified more frequently than those from generated-thin-slice CT images.</p><p>CT scans with old CT scanners typically produce images with slice thicknesses between 3 and 5 mm to avoid noise present with thin slices. Even with newer-generation CT scanners, thick-slice images are often stored in Picture Archiving and Communication Systems (PACSs) in preference to thin-slice images for optimizing storage efficiency. In either scenario, the slice thickness is inadequate for AI-based lung cancer screening software to segment and correctly categorize pulmonary nodules present in the chest CT. Similar outcomes were found for attempting to manually segment and correctly categorize pulmonary nodules, which have not been covered in this study, but we have covered them in a separate study, which has been submitted to another journal for a review.</p><p>To overcome the slice thickness issue, we implemented a deep learning-enabled super-resolution technique to intake thick-slice CT images and forward pass generated-thin-slice CT images. In detail, we modified our deep learning model based on ESRGAN to accommodate 3D CT images by adjusting the input channels and replacing the PixelShuffle module with the VoxelShuffle-SG module, which enabled the model to generate thin-slice CT images and recover both nodule volume and texture faithfully. Therefore, thick-slice CT images can be converted to generated-thin-slice CT images and produce accurate categorization of pulmonary nodules by ClariPulmo. This study showed that the automatic nodule mis-categorization decreased from 15 cases down to merely 3 cases out of 55 cases examined when thick-slice CT images were converted to generated-thin-slice CT images. The outcome indicated that applying super-resolution-based slice generation to thick-slice CT images prior to automatic pulmonary nodule categorization significantly increased the categorization accuracy with little to no side-effects.</p><p>This study has certain limitations. Although training was conducted using data from four major CT vendors (Siemens Healthineers, GE Healthcare, Canon Medical Systems, and Philips Healthcare), all images were sourced from the NLST open dataset. Similarly, the test data originated from the LIDC-IDRI open dataset. Future work should include validating the model&#x02019;s performance on CT images from other institutions and with various CT scanners to ensure generalizability.</p></sec><sec sec-type="conclusions" id="sec5-tomography-11-00013"><title>5. Conclusions</title><p>This study aimed to address the challenges posed by heterogeneous slice thickness in CT imaging, which can negatively impact the accuracy of automated pulmonary nodule categorization. To mitigate this issue, we employed a deep learning-enabled 3D super-resolution method to convert thick-slice CT images into uniform, generated-thin-slice CT images. The effectiveness of this approach was evaluated by comparing pulmonary nodule categorization outcomes from both thick-slice and generated-thin-slice CT images using AI-based lung cancer screening software, with reference thin-slice CT images serving as the gold standard. The results demonstrated a notable improvement in categorization accuracy, with thick-slice CT images achieving 72.7% accuracy, while the generated-thin-slice CT images attained a higher accuracy of 94.5%. These findings underscore the potential of our proposed approach to enhance diagnostic precision in scenarios where only thick-slice CT images are available. In conclusion, the conversion of thick-slice CT images into generated-thin-slice CT images represents a promising strategy for improving the reliability of pulmonary nodule categorization.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, D.K. and J.H.K.; methodology, D.K.; software, D.K. and C.A.; validation, D.K., C.A. and J.H.K.; formal analysis, D.K.; investigation, D.K.; resources, D.K. and C.A.; data curation, D.K.; writing&#x02014;original draft preparation, D.K. and C.A.; writing&#x02014;review and editing, J.H.K.; visualization, D.K.; supervision, J.H.K.; project administration, J.H.K. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Ethical review and approval were waived since this study used open dataset only. This study was conducted according to the relevant guidelines and regulations.</p></notes><notes><title>Informed Consent Statement</title><p>Patient consent was not needed since this study used open datasets only.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The LIDC-IDRI CT image dataset used for the evaluation is available at the following URL: <uri xlink:href="https://www.cancerimagingarchive.net/collection/lidc-idri/">https://www.cancerimagingarchive.net/collection/lidc-idri/</uri>, accessed on 10 September 2024. The NLST CT image dataset used for training the model is available at the following URL: <uri xlink:href="https://www.cancerimagingarchive.net/collection/nlst/">https://www.cancerimagingarchive.net/collection/nlst/</uri>, accessed on 2 June 2024.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>D.K. is employed by the company ClariPi, Inc. C.A. is employed by the company ClariPi, Inc. J.H.K. is CEO of the company ClariPi, Inc.</p></notes><ref-list><title>References</title><ref id="B1-tomography-11-00013"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Petrou</surname><given-names>M.</given-names></name>
<name><surname>Quint</surname><given-names>L.E.</given-names></name>
<name><surname>Nan</surname><given-names>B.</given-names></name>
<name><surname>Baker</surname><given-names>L.H.</given-names></name>
</person-group><article-title>Pulmonary Nodule Volumetric Measurement Variability as a Function of CT Slice Thickness and Nodule Morphology</article-title><source>Am. J. Roentgenol.</source><year>2007</year><volume>188</volume><fpage>306</fpage><lpage>312</lpage><pub-id pub-id-type="doi">10.2214/AJR.05.1063</pub-id><pub-id pub-id-type="pmid">17242235</pub-id>
</element-citation></ref><ref id="B2-tomography-11-00013"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dong</surname><given-names>C.</given-names></name>
<name><surname>Loy</surname><given-names>C.C.</given-names></name>
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Tang</surname><given-names>X.</given-names></name>
</person-group><article-title>Image Super-Resolution Using Deep Convolutional Networks</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2015</year><volume>38</volume><fpage>297</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2015.2439281</pub-id><pub-id pub-id-type="pmid">26761735</pub-id>
</element-citation></ref><ref id="B3-tomography-11-00013"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>J.</given-names></name>
<name><surname>Lee</surname><given-names>J.K.</given-names></name>
<name><surname>Lee</surname><given-names>K.M.</given-names></name>
</person-group><article-title>Accurate Image Super-Resolution Using Very Deep Convolutional Networks</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>26 June&#x02013;1 July 2016</conf-date><fpage>1646</fpage><lpage>1654</lpage></element-citation></ref><ref id="B4-tomography-11-00013"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lim</surname><given-names>B.</given-names></name>
<name><surname>Son</surname><given-names>S.</given-names></name>
<name><surname>Kim</surname><given-names>H.</given-names></name>
<name><surname>Nah</surname><given-names>S.</given-names></name>
<name><surname>Lee</surname><given-names>K.M.</given-names></name>
</person-group><article-title>Enhanced Deep Residual Networks for Single Image Super-Resolution</article-title><source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;26 July 2017</conf-date><fpage>1132</fpage><lpage>1140</lpage></element-citation></ref><ref id="B5-tomography-11-00013"><label>5.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>K.</given-names></name>
<name><surname>Li</surname><given-names>K.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Zhong</surname><given-names>B.</given-names></name>
<name><surname>Fu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Image Super-Resolution Using Very Deep Residual Channel Attention Networks</article-title><source>Computer Vision&#x02014;ECCV 2018</source><person-group person-group-type="editor">
<name><surname>Ferrari</surname><given-names>V.</given-names></name>
<name><surname>Hebert</surname><given-names>M.</given-names></name>
<name><surname>Sminchisescu</surname><given-names>C.</given-names></name>
<name><surname>Weiss</surname><given-names>Y.</given-names></name>
</person-group><comment>Lecture Notes in Computer Science</comment><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2018</year><volume>Volume 11211</volume><fpage>294</fpage><lpage>310</lpage><isbn>978-3-030-01233-5</isbn></element-citation></ref><ref id="B6-tomography-11-00013"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ledig</surname><given-names>C.</given-names></name>
<name><surname>Theis</surname><given-names>L.</given-names></name>
<name><surname>Huszar</surname><given-names>F.</given-names></name>
<name><surname>Caballero</surname><given-names>J.</given-names></name>
<name><surname>Cunningham</surname><given-names>A.</given-names></name>
<name><surname>Acosta</surname><given-names>A.</given-names></name>
<name><surname>Aitken</surname><given-names>A.</given-names></name>
<name><surname>Tejani</surname><given-names>A.</given-names></name>
<name><surname>Totz</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<etal/>
</person-group><article-title>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;27 July 2017</conf-date></element-citation></ref><ref id="B7-tomography-11-00013"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Simonyan</surname><given-names>K.</given-names></name>
<name><surname>Zisserman</surname><given-names>A.</given-names></name>
</person-group><article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1409.1556</pub-id></element-citation></ref><ref id="B8-tomography-11-00013"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Goodfellow</surname><given-names>I.</given-names></name>
<name><surname>Pouget-Abadie</surname><given-names>J.</given-names></name>
<name><surname>Mirza</surname><given-names>M.</given-names></name>
<name><surname>Xu</surname><given-names>B.</given-names></name>
<name><surname>Warde-Farley</surname><given-names>D.</given-names></name>
<name><surname>Ozair</surname><given-names>S.</given-names></name>
<name><surname>Courville</surname><given-names>A.</given-names></name>
<name><surname>Bengio</surname><given-names>Y.</given-names></name>
</person-group><article-title>Generative Adversarial Networks</article-title><source>Commun. ACM</source><year>2020</year><volume>63</volume><fpage>139</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1145/3422622</pub-id></element-citation></ref><ref id="B9-tomography-11-00013"><label>9.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Yu</surname><given-names>K.</given-names></name>
<name><surname>Wu</surname><given-names>S.</given-names></name>
<name><surname>Gu</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Dong</surname><given-names>C.</given-names></name>
<name><surname>Qiao</surname><given-names>Y.</given-names></name>
<name><surname>Loy</surname><given-names>C.C.</given-names></name>
</person-group><article-title>ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</article-title><source>Computer Vision&#x02014;ECCV 2018 Workshops</source><person-group person-group-type="editor">
<name><surname>Leal-Taix&#x000e9;</surname><given-names>L.</given-names></name>
<name><surname>Roth</surname><given-names>S.</given-names></name>
</person-group><comment>Lecture Notes in Computer Science</comment><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2019</year><volume>Volume 11133</volume><fpage>63</fpage><lpage>79</lpage><isbn>978-3-030-11020-8</isbn></element-citation></ref><ref id="B10-tomography-11-00013"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Xie</surname><given-names>L.</given-names></name>
<name><surname>Dong</surname><given-names>C.</given-names></name>
<name><surname>Shan</surname><given-names>Y.</given-names></name>
</person-group><article-title>Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data</article-title><source>Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#x02013;17 October 2021</conf-date><fpage>1905</fpage><lpage>1914</lpage></element-citation></ref><ref id="B11-tomography-11-00013"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Goo</surname><given-names>J.M.</given-names></name>
</person-group><article-title>Deep Learning&#x02013;Based Super-Resolution Algorithm: Potential in the Management of Subsolid Nodules</article-title><source>Radiology</source><year>2021</year><volume>299</volume><fpage>220</fpage><lpage>221</lpage><pub-id pub-id-type="doi">10.1148/radiol.2021204463</pub-id><pub-id pub-id-type="pmid">33561378</pub-id>
</element-citation></ref><ref id="B12-tomography-11-00013"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Park</surname><given-names>S.</given-names></name>
<name><surname>Lee</surname><given-names>S.M.</given-names></name>
<name><surname>Kim</surname><given-names>W.</given-names></name>
<name><surname>Park</surname><given-names>H.</given-names></name>
<name><surname>Jung</surname><given-names>K.-H.</given-names></name>
<name><surname>Do</surname><given-names>K.-H.</given-names></name>
<name><surname>Seo</surname><given-names>J.B.</given-names></name>
</person-group><article-title>Computer-Aided Detection of Subsolid Nodules at Chest CT: Improved Performance with Deep Learning&#x02013;Based CT Section Thickness Reduction</article-title><source>Radiology</source><year>2021</year><volume>299</volume><fpage>211</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1148/radiol.2021203387</pub-id><pub-id pub-id-type="pmid">33560190</pub-id>
</element-citation></ref><ref id="B13-tomography-11-00013"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Park</surname><given-names>S.</given-names></name>
<name><surname>Lee</surname><given-names>S.M.</given-names></name>
<name><surname>Do</surname><given-names>K.-H.</given-names></name>
<name><surname>Lee</surname><given-names>J.-G.</given-names></name>
<name><surname>Bae</surname><given-names>W.</given-names></name>
<name><surname>Park</surname><given-names>H.</given-names></name>
<name><surname>Jung</surname><given-names>K.-H.</given-names></name>
<name><surname>Seo</surname><given-names>J.B.</given-names></name>
</person-group><article-title>Deep Learning Algorithm for Reducing CT Slice Thickness: Effect on Reproducibility of Radiomic Features in Lung Cancer</article-title><source>Korean J. Radiol.</source><year>2019</year><volume>20</volume><fpage>1431</fpage><pub-id pub-id-type="doi">10.3348/kjr.2019.0212</pub-id><pub-id pub-id-type="pmid">31544368</pub-id>
</element-citation></ref><ref id="B14-tomography-11-00013"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Yun</surname><given-names>H.R.</given-names></name>
<name><surname>Lee</surname><given-names>M.J.</given-names></name>
<name><surname>Hong</surname><given-names>H.</given-names></name>
<name><surname>Shim</surname><given-names>K.W.</given-names></name>
<name><surname>Jeon</surname><given-names>J.</given-names></name>
</person-group><article-title>Improvement of Inter-Slice Resolution Based on 2D CNN with Thin Bone Structure-Aware on Head-and-Neck CT Images</article-title><source>Proceedings of the Medical Imaging 2021: Image Processing</source><publisher-name>SPIE</publisher-name><publisher-loc>Bellingham, WA, USA</publisher-loc><year>2021</year><volume>Volume 11596</volume><fpage>600</fpage><lpage>605</lpage></element-citation></ref><ref id="B15-tomography-11-00013"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yun</surname><given-names>H.R.</given-names></name>
<name><surname>Lee</surname><given-names>M.J.</given-names></name>
<name><surname>Hong</surname><given-names>H.</given-names></name>
<name><surname>Shim</surname><given-names>K.W.</given-names></name>
</person-group><article-title>Inter-Slice Resolution Improvement Using Convolutional Neural Network with Orbital Bone Edge-Aware in Facial CT Images</article-title><source>J. Digit. Imaging</source><year>2022</year><volume>36</volume><fpage>240</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1007/s10278-022-00686-9</pub-id><pub-id pub-id-type="pmid">35995899</pub-id>
</element-citation></ref><ref id="B16-tomography-11-00013"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nakamoto</surname><given-names>A.</given-names></name>
<name><surname>Hori</surname><given-names>M.</given-names></name>
<name><surname>Onishi</surname><given-names>H.</given-names></name>
<name><surname>Ota</surname><given-names>T.</given-names></name>
<name><surname>Fukui</surname><given-names>H.</given-names></name>
<name><surname>Ogawa</surname><given-names>K.</given-names></name>
<name><surname>Masumoto</surname><given-names>J.</given-names></name>
<name><surname>Kudo</surname><given-names>A.</given-names></name>
<name><surname>Kitamura</surname><given-names>Y.</given-names></name>
<name><surname>Kido</surname><given-names>S.</given-names></name>
<etal/>
</person-group><article-title>Three-Dimensional Conditional Generative Adversarial Network-Based Virtual Thin-Slice Technique for the Morphological Evaluation of the Spine</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><elocation-id>12176</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-16637-x</pub-id><pub-id pub-id-type="pmid">35842451</pub-id>
</element-citation></ref><ref id="B17-tomography-11-00013"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Iwano</surname><given-names>S.</given-names></name>
<name><surname>Kamiya</surname><given-names>S.</given-names></name>
<name><surname>Ito</surname><given-names>R.</given-names></name>
<name><surname>Kudo</surname><given-names>A.</given-names></name>
<name><surname>Kitamura</surname><given-names>Y.</given-names></name>
<name><surname>Nakamura</surname><given-names>K.</given-names></name>
<name><surname>Naganawa</surname><given-names>S.</given-names></name>
</person-group><article-title>Measurement of Solid Size in Early-Stage Lung Adenocarcinoma by Virtual 3D Thin-Section CT Applied Artificial Intelligence</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><elocation-id>21709</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-48755-5</pub-id><pub-id pub-id-type="pmid">38066174</pub-id>
</element-citation></ref><ref id="B18-tomography-11-00013"><label>18.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Kudo</surname><given-names>A.</given-names></name>
<name><surname>Kitamura</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Iizuka</surname><given-names>S.</given-names></name>
<name><surname>Simo-Serra</surname><given-names>E.</given-names></name>
</person-group><article-title>Virtual Thin Slice: 3D Conditional GAN-Based Super-Resolution for CT Slice Interval</article-title><source>Medical Image Computing and Computer Assisted Intervention&#x02014;MICCAI 2019</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2019</year></element-citation></ref><ref id="B19-tomography-11-00013"><label>19.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>ClariPulmo</collab>
</person-group><article-title>Fully Automated AI-Powered 3D Reporting Solution for Lung Cancer Screening</article-title><comment>Available online: <ext-link xlink:href="https://claripi.com/claripulmo-2/" ext-link-type="uri">https://claripi.com/claripulmo-2/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-12">(accessed on 12 December 2024)</date-in-citation></element-citation></ref><ref id="B20-tomography-11-00013"><label>20.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>PixelShuffle</collab>
</person-group><article-title>PyTorch 2.5 Documentation</article-title><comment>Available online: <ext-link xlink:href="https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html" ext-link-type="uri">https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-12">(accessed on 12 December 2024)</date-in-citation></element-citation></ref><ref id="B21-tomography-11-00013"><label>21.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Tang</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Xie</surname><given-names>X.</given-names></name>
</person-group><article-title>NoduleNet: Decoupled False Positive Reduction for Pulmonary Nodule Detection and Segmentation</article-title><source>Medical Image Computing and Computer Assisted Intervention&#x02014;MICCAI 2019</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2019</year><fpage>11769</fpage><pub-id pub-id-type="doi">10.1007/978-3-030-32226-7_30</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="tomography-11-00013-f001"><label>Figure 1</label><caption><p>A flow diagram of pulmonary nodule inclusion for the test dataset.</p></caption><graphic xlink:href="tomography-11-00013-g001" position="float"/></fig><fig position="float" id="tomography-11-00013-f002"><label>Figure 2</label><caption><p>(<bold>A</bold>) Overall network architecture, (<bold>B</bold>) generator model, (<bold>C</bold>) discriminator model, and (<bold>D</bold>) VoxelShuffle-SG.</p></caption><graphic xlink:href="tomography-11-00013-g002" position="float"/></fig><fig position="float" id="tomography-11-00013-f003"><label>Figure 3</label><caption><p>Simplified Lung-RADS v2022 lung nodule categorization criteria for solid, sub-solid, and GGN lung nodules.</p></caption><graphic xlink:href="tomography-11-00013-g003" position="float"/></fig><fig position="float" id="tomography-11-00013-f004"><label>Figure 4</label><caption><p>ClariPulmo graphical user interface.</p></caption><graphic xlink:href="tomography-11-00013-g004" position="float"/></fig><fig position="float" id="tomography-11-00013-f005"><label>Figure 5</label><caption><p>Coronal CT images of (<bold>A</bold>) thick-slice, (<bold>B</bold>) generated-thin-slice, and (<bold>C</bold>) thin-slice presenting solid nodules as shown by the arrows.</p></caption><graphic xlink:href="tomography-11-00013-g005" position="float"/></fig><fig position="float" id="tomography-11-00013-f006"><label>Figure 6</label><caption><p>Coronal CT images of (<bold>A</bold>) thick-slice, (<bold>B</bold>) generated-thin-slice, and (<bold>C</bold>) thin-slice presenting GGNs as shown by the arrows.</p></caption><graphic xlink:href="tomography-11-00013-g006" position="float"/></fig><fig position="float" id="tomography-11-00013-f007"><label>Figure 7</label><caption><p>Confusion matrix based on Lung-RADS v2022 lung nodule classification for (<bold>A</bold>) thick-slice&#x02013;thin-slice CT image pair and (<bold>B</bold>) generated-thin-slice&#x02013;thin-slice CT image pair.</p></caption><graphic xlink:href="tomography-11-00013-g007" position="float"/></fig><table-wrap position="float" id="tomography-11-00013-t001"><object-id pub-id-type="pii">tomography-11-00013-t001_Table 1</object-id><label>Table 1</label><caption><p>Parameters of training datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Manufacturer</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Siemens <break/>Healthineers</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Canon<break/>Medical Systems</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GE<break/>Healthcare</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Philips<break/>Healthcare</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Model</td><td align="center" valign="middle" rowspan="1" colspan="1">Sensation 16</td><td align="center" valign="middle" rowspan="1" colspan="1">Aquilion</td><td align="center" valign="middle" rowspan="1" colspan="1">LightSpeed 16</td><td align="center" valign="middle" rowspan="1" colspan="1">MX8000</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Tube Voltage (kV)</td><td align="center" valign="middle" rowspan="1" colspan="1">120</td><td align="center" valign="middle" rowspan="1" colspan="1">120</td><td align="center" valign="middle" rowspan="1" colspan="1">120</td><td align="center" valign="middle" rowspan="1" colspan="1">120</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Slice Thickness (mm)</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Reconstruction Kernel</td><td align="center" valign="middle" rowspan="1" colspan="1">B30f, B45f, B50f, B80f</td><td align="center" valign="middle" rowspan="1" colspan="1">FC01, FC02, FC53</td><td align="center" valign="middle" rowspan="1" colspan="1">Standard, Lung</td><td align="center" valign="middle" rowspan="1" colspan="1">C</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of Slices</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18,243</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2931</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2707</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4747</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tomography-11-00013-t002"><object-id pub-id-type="pii">tomography-11-00013-t002_Table 2</object-id><label>Table 2</label><caption><p>Categorization outcomes using ClariPulmo.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Thick Correct/<break/>Generated-Thin<break/>Incorrect</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Thick Incorrect/Generated-Thin Correct</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Thick Incorrect/Generated-Thin Incorrect</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Thick Correct/Generated-Thin Correct</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Thick-Slice<break/>Categorization<break/>Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Generated-Thin-Slice <break/>Categorization Accuracy</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.5%</td></tr></tbody></table></table-wrap></floats-group></article>