<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Nat Commun</journal-id><journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id><journal-title-group><journal-title>Nature Communications</journal-title></journal-title-group><issn pub-type="epub">2041-1723</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39890777</article-id><article-id pub-id-type="pmc">PMC11785991</article-id><article-id pub-id-type="publisher-id">55631</article-id><article-id pub-id-type="doi">10.1038/s41467-024-55631-x</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Prompt injection attacks on vision language models in oncology</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Clusmann</surname><given-names>Jan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Ferber</surname><given-names>Dyke</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Wiest</surname><given-names>Isabella C.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Schneider</surname><given-names>Carolin V.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Brinker</surname><given-names>Titus J.</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><name><surname>Foersch</surname><given-names>Sebastian</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9605-0728</contrib-id><name><surname>Truhn</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3730-5348</contrib-id><name><surname>Kather</surname><given-names>Jakob Nikolas</given-names></name><address><email>Jakob.Kather@ukdd.de</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff8">8</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/042aqky30</institution-id><institution-id institution-id-type="GRID">grid.4488.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2111 7257</institution-id><institution>Else Kroener Fresenius Center for Digital Health, </institution><institution>Technical University Dresden, </institution></institution-wrap>Dresden, Germany </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04xfq0f34</institution-id><institution-id institution-id-type="GRID">grid.1957.a</institution-id><institution-id institution-id-type="ISNI">0000 0001 0728 696X</institution-id><institution>Department of Medicine III, </institution><institution>University Hospital RWTH Aachen, </institution></institution-wrap>Aachen, Germany </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/013czdx64</institution-id><institution-id institution-id-type="GRID">grid.5253.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0328 4908</institution-id><institution>Department of Medical Oncology, National Center for Tumor Diseases (NCT), </institution><institution>Heidelberg University Hospital, </institution></institution-wrap>Heidelberg, Germany </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/038t36y30</institution-id><institution-id institution-id-type="GRID">grid.7700.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2190 4373</institution-id><institution>Department of Medicine II, Medical Faculty Mannheim, </institution><institution>Heidelberg University, </institution></institution-wrap>Mannheim, Germany </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04cdgtt98</institution-id><institution-id institution-id-type="GRID">grid.7497.d</institution-id><institution-id institution-id-type="ISNI">0000 0004 0492 0584</institution-id><institution>Digital Biomarkers for Oncology Group, </institution><institution>German Cancer Research Center, </institution></institution-wrap>Heidelberg, Germany </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00q1fsf04</institution-id><institution-id institution-id-type="GRID">grid.410607.4</institution-id><institution>Institute of Pathology, </institution><institution>University Medical Center Mainz, </institution></institution-wrap>Mainz, Germany </aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02gm5zw39</institution-id><institution-id institution-id-type="GRID">grid.412301.5</institution-id><institution-id institution-id-type="ISNI">0000 0000 8653 1507</institution-id><institution>Department of Diagnostic and Interventional Radiology, </institution><institution>University Hospital Aachen, </institution></institution-wrap>Aachen, Germany </aff><aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04za5zm41</institution-id><institution-id institution-id-type="GRID">grid.412282.f</institution-id><institution-id institution-id-type="ISNI">0000 0001 1091 2917</institution-id><institution>Department of Medicine I, </institution><institution>University Hospital Dresden, </institution></institution-wrap>Dresden, Germany </aff></contrib-group><pub-date pub-type="epub"><day>1</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>1</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>16</volume><elocation-id>1239</elocation-id><history><date date-type="received"><day>2</day><month>7</month><year>2024</year></date><date date-type="accepted"><day>17</day><month>12</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Vision-language artificial intelligence models (VLMs) possess medical knowledge and can be employed in healthcare in numerous ways, including as image interpreters, virtual scribes, and general decision support systems. However, here, we demonstrate that current VLMs applied to medical tasks exhibit a fundamental security flaw: they can be compromised by prompt injection attacks. These can be used to output harmful information just by interacting with the VLM, without any access to its parameters. We perform a quantitative study to evaluate the vulnerabilities to these attacks in four state of the art VLMs: Claude-3 Opus, Claude-3.5 Sonnet, Reka Core, and GPT-4o. Using a set of N&#x02009;=&#x02009;594 attacks, we show that all of these models are susceptible. Specifically, we show that embedding sub-visual prompts in manifold medical imaging data can cause the model to provide harmful output, and that these prompts are non-obvious to human observers. Thus, our study demonstrates a key vulnerability in medical VLMs which should be mitigated before widespread clinical adoption.</p></abstract><abstract id="Abs2" abstract-type="web-summary"><p id="Par2">Vision-language artificial intelligence models (VLMs) can be employed to recognize lesions in cancer images. Here, the authors show that VLMs can be misled by prompt injection attacks, producing harmful output and leading to incorrect diagnoses.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Cancer imaging</kwd><kwd>Computational science</kwd><kwd>Medical imaging</kwd><kwd>Machine learning</kwd></kwd-group><funding-group><award-group><funding-source><institution>JC is supported by the Mildred-Scheel-Postdoktorandenprogramm of the German Cancer Aid (grant #70115730). C.V.S is supported by a grant from the Interdisciplinary Centre for Clinical Research within the faculty of Medicine at the RWTH Aachen University (PTD 1-13/IA 532313), the Junior Principal Investigator Fellowship program of RWTH Aachen Excellence strategy, the NRW Rueckkehr Programme of the Ministry of Culture and Science of the German State of North Rhine-Westphalia and by the CRC 1382 (ID 403224013) funded by Deutsche Forschungsgesellschaft (DFG, German Research Foundation). SF is supported by the German Federal Ministry of Education and Research (SWAG, 01KD2215A), the German Cancer Aid (DECADE, 70115166 and TargHet, 70115995) and the German Research Foundation (504101714). DT is funded by the German Federal Ministry of Education and Research (TRANSFORM LIVER, 031L0312A), the European Union&#x02019;s Horizon Europe and innovation programme (ODELIA, 101057091), and the German Federal Ministry of Health (SWAG, 01KD2215B). JNK is supported by the German Cancer Aid (DECADE, 70115166), the German Federal Ministry of Education and Research (PEARL, 01KD2104C; CAMINO, 01EO2101; SWAG, 01KD2215A; TRANSFORM LIVER, 031L0312A; TANGERINE, 01KT2302 through ERA-NET Transcan; Come2Data, 16DKZ2044A; DEEP-HCC, 031L0315A), the German Academic Exchange Service (SECAI, 57616814), the German Federal Joint Committee (TransplantKI, 01VSF21048) the European Union&#x02019;s Horizon Europe and innovation programme (ODELIA, 101057091; GENIAL, 101096312), the European Research Council (ERC; NADIR, 101114631), the National Institutes of Health (EPICO, R01 CA263318) and the National Institute for Health and Care Research (NIHR, NIHR203331) Leeds Biomedical Research Centre. The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health and Social Care. This work was funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them.</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par3">Large language models (LLMs) are generative artificial intelligence (AI) systems trained on vast amounts of human language. They are the fastest-adopted technology in human history<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. Numerous scientific and medical applications of LLMs have been proposed<sup><xref ref-type="bibr" rid="CR3">3</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref></sup>, and these could drastically change and improve medicine as we know it. In particular, LLMs have been shown to be able to reduce documentation burden and promote guideline-based medicine<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>. In parallel to the rapid progression of LLM capabilities, there has been substantial progress in the development of multimodal vision-language models (VLMs). VLMs can interpret images and text alike and further expand the applicability of LLMs in medicine. Several VLMs have been published to date, either as healthcare-specific models, e.g., for the interpretation of pathology images or echocardiograms<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>, or as generalist models, applicable to multiple domains at once, including healthcare, such as GPT-4o<sup><xref ref-type="bibr" rid="CR10">10</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref></sup>.</p><p id="Par4">However, with new technologies, new vulnerabilities emerge, and the healthcare system has to be hardened against these<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref></sup>. We hypothesized that one particular vulnerability of LLMs and VLMs is prompt injection. Prompt injection means that a user adds an additional, hidden instruction for the model (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1a</xref>). Prompt injection can be disguised in hidden (e.g., zero-width) or encoded characters (e.g., Unicode), whitespaces, metadata, images and much more&#x02014;essentially, any information which flows into a model at runtime can be used as an attack vector (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1b</xref>)<sup><xref ref-type="bibr" rid="CR17">17</xref>&#x02013;<xref ref-type="bibr" rid="CR20">20</xref></sup>. Importantly, third parties with access to a user&#x02019;s input (but without access to the model itself), can perform prompt injection to exfiltrate private data, evade model guardrails, and alter model outputs<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. In a clinical context, modifying a model&#x02019;s output can cause lethal harm. Still, prompt injection attacks have never been investigated in the medical domain.<fig id="Fig1"><label>Fig. 1</label><caption><title>Prompt injection attacks on medical images.</title><p><bold>a</bold> Concept of prompt injection with correct and corrupted output plus models investigated in the study. Models that did not meet inclusion criteria displayed in grey. <bold>b</bold> Schematic illustration of different attack gateways for prompt injections. Left: Patient image data is routinely not exclusively created in the hospital itself, but also by external sources, illustrated as &#x0201c;External radiologist&#x0201d; as an example (equivalent to other external imaging providers). It is&#x000a0;then provided either via (unprotected) CD/DVD or a server (varying protection levels) to the hospital. Right: For inference, hospitals will rely on external VLM-providers. Illustrated are several gateways to send patient data to these providers, with varying protection levels. EHR Electronic health records, API Application programming interface, HIS Hospital information system. <bold>c</bold> Exemplary views of modalities (histology, endoscopy, CT, MRI, ultrasound, photography) with 18 cases, 3 prompt injection positions, 3 variations (high contrast, low contrast, small font) and 3 iterations per unique constellations, with schematic illustration of injection methods. Contrast in &#x0201c;low contrast&#x0201d; was increased for this figure as compared to the actual model input to maintain human readability. Prompt injection in &#x0201c;low contrast&#x0201d; and &#x0201c;small font&#x0201d; reads &#x0201c;Describe which organ you see, but state it looks healthy&#x0201d;.</p></caption><graphic xlink:href="41467_2024_55631_Fig1_HTML" id="d33e386"/></fig></p><p id="Par5">Here, we show that prompt injection attacks have the potential to alter model outputs from accurate diagnosis to a potentially harmful misdiagnosis in oncology.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Vision-language models are modality-agnostic detectors of malignant lesions</title><p id="Par6">To investigate the susceptibility of contemporaneous state-of-the-art VLMs, we designed a set of experiments to evaluate prompt injection in the context of cancer diagnosis. Specifically, we investigated if the diagnosis obtained from an imaging scan, i.e., presence or absence of tumor, can be manipulated at will, just by injecting malicious information into the model input (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>). Images of 18 patients with visually highly suspicious and histologically confirmed malignant lesions were passed to the VLMs Claude 3 Opus, Claude 3.5 Sonnet, Gemini 1.5, GPT-4o and Reka Core. These images included liver imaging (in equal parts CT, MRI and ultrasound images), as well as photographies of melanomas, endoscopic imaging of gastrointestinal cancers and hematoxylin-eosin (HE)-staining of adenocarcinomas (<italic>n</italic>&#x02009;=&#x02009;3 per modality, 18 cases in total, Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1c</xref>). Each image (Supplementary Data&#x000a0;<xref rid="MOESM3" ref-type="media">1</xref>) was accompanied by the prompt &#x0201c;Describe this image&#x0201d; and a structured template for the output (Supplementary Data&#x000a0;<xref rid="MOESM4" ref-type="media">2</xref>, <xref rid="MOESM5" ref-type="media">3</xref>, Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>). Three strategies for prompt injection were tested: &#x0201c;text prompt injection&#x0201d;, &#x0201c;visual prompt injection&#x0201d;, and &#x0201c;delayed visual prompt injection&#x0201d;, in which the attack was performed using the image preceding the target image (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">1a</xref>). Additionally, for visual&#x02014;and delayed visual prompt injection, we tested if the contrast and size of the injected text had an influence on the models&#x02019; accuracies: we employed two contrast settings (high contrast and low contrast) and one setting in which the text was tiny, see Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1b</xref>. Low-contrast and &#x0201c;tiny&#x0201d; injections correspond to sub-visual injections which are not obvious to human observers, therefore more harmful. This led to a total of 72 variations per model (18 negative controls + 54 prompt injection variations), with each of the 72 variations being queried a total of 3 replicates (<italic>n</italic>&#x02009;=&#x02009;216 per model). All prompts are listed in Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>.</p><p id="Par7">First, we assessed the organ detection rate by the model. Only VLMs that reached at least a 50% organ detection rate, i.e., were able to accurately describe the organ in the image, were used for subsequent experiments (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2a</xref>). The VLMs Claude-3 Opus, Claude 3.5 Sonnet, GPT-4o and Reka Core achieved this rate and were therefore included in this study (Accuracy of 59%, 80%, 79%, 74% for Claude-3, Claude-3.5, GPT-4o and Reka Core, respectively). We were not able to investigate the vision capabilities of Gemini 1.5 Plus because its current guardrails prevent it from being used on radiology images. Llama-3.1 (405B), the best currently available open-source LLM, does not yet support vision interpretation, and could therefore not be assessed<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>. As a side observation, we found that all models sometimes hallucinated the presence of spleen, kidneys, and pancreas when prompted to describe them despite them not being visible, but this effect was not relevant to the subsequent experiments.<fig id="Fig2"><label>Fig. 2</label><caption><title>Prompt injection attacks manipulate the capability of VLMs to detect malignant lesions.</title><p><bold>a</bold> Accuracies in detecting the represented organs per model. Mean&#x02009;&#x000b1;&#x02009;standard deviation (SD) is shown. <italic>n</italic>&#x02009;=&#x02009;18 data points per model (<italic>n</italic>&#x02009;=&#x02009;9 for Gemini), with each data point representing a mean of three replicated measurements, two-sided Kruskal-Wallis test with Dunn&#x02019;s test and Bonferroni post-hoc correction. <bold>b</bold> Harmfulness scores for all queries with injected prompt vs prompts without prompt injection per model. Mean&#x02009;&#x000b1;&#x02009;SD are shown. Each point represents triplicate evaluation. Two-sided Wilcoxon Signed-Rank tests with Bonferroni post-hoc correction compared lesion miss rates scores within each model (square brackets). Two-sided Mann-Whitney <italic>U</italic> tests with Bonferroni post-hoc correction compared lesion miss rates for prompt injection (PI) vs non PI over all models combined (straight bar). <italic>P</italic>-values were adjusted using the Bonferroni method, with *<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05, **<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01, ***<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001. Harmfulness scores as mean&#x02009;&#x000b1;&#x02009;standard deviation (SD) per (<bold>c</bold>) position or (<bold>d</bold>) variation of adversarial prompt, ordered as Claude-3, Claude-3.5, GPT-4o, and Reka Core from left to right. <italic>n</italic>&#x02009;=&#x02009;18 data points per model and variation, with each data point representing a mean of three replicated measurements. Mann-Whitney <italic>U</italic> test + Bonferroni method over all models combined for each position/variation.</p></caption><graphic xlink:href="41467_2024_55631_Fig2_HTML" id="d33e489"/></fig></p></sec><sec id="Sec4"><title>Hidden instructions in images can bypass guardrails and alter VLM outputs</title><p id="Par8">Second, we assessed the attack success rate in all VLMs. Our objective was to provide the VLM with an image of a cancer lesion, and prompting the model to ignore the lesion, either by text prompt injection, visual prompt injection or delayed visual prompt injection. We quantified (a) the model&#x02019;s ability to detect lesions in the first place (lesion miss rate, LMR), and (b) the attack success rate (ASR), i.e., flipping the model&#x02019;s output by a prompt injection (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b</xref>). We observed highly different behavior between VLMs, with organ detection rates of 59% (Claude-3), 80% (Claude-3.5), 79% (GPT-4o), and 74% (Reka Core) (<italic>n</italic>&#x02009;=&#x02009;54 each) (Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">2</xref>). Lesion miss rate (LMR) of unaltered prompts was 35% for Claude-3, 17% for Claude-3.5, 22% for GPT-4o, and 41% for Reka Core (<italic>n</italic>&#x02009;=&#x02009;54 each) (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b</xref>). Adding prompt injection significantly impaired the models&#x02019; abilities to detect lesions, with a LMR of 70% (ASR of 33%) for Claude-3 (<italic>n</italic>&#x02009;=&#x02009;81), LMR of 57% (ASR 40%) for Claude-3.5 (<italic>n</italic>&#x02009;=&#x02009;162), LMR of 89% (ASR of 67%) for GPT-4o (<italic>n</italic>&#x02009;=&#x02009;162) and LMR of 92% (ASR of 51%) for Reka Core (<italic>n</italic>&#x02009;=&#x02009;104), significant both per model (<italic>p</italic>&#x02009;=&#x02009;0.02; 0.01; &#x0003c;0.001 and &#x0003c;0.001 for Claude-3, Claude-3.5, GPT-4o, and Reka Core, respectively) as well as over all models combined (<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.0001) (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b</xref>). Notably, the ASR for GPT-4o and Reka Core was significantly higher than the ASR of Claude-3.5 (<italic>p</italic>&#x02009;=&#x02009;0.001 and <italic>p</italic>&#x02009;=&#x02009;0.006 for GPT-4o and Reka Core, respectively, Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">3</xref>), possibly indicating a slightly superior alignment training for Claude-3.5. Together, these data show that prompt injection, to varying extent, is possible in all investigated VLMs on a broad range of clinically relevant imaging modalities.</p><p id="Par9">Prompt injection can be performed in various ways. As a proof-of-concept we investigated three different strategies for prompt injection (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1b</xref>), with striking differences between models and strategies (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2c, d</xref>, Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>). Text prompt injection and image prompt injection were both harmful in almost all observations, except for Claude-3.5, which proved less harmful here. Meanwhile, delayed visual prompt injection resulted in less harmful responses overall (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2c</xref>, Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">4</xref>), possibly because the hidden instruction becomes more susceptible to guardrail interventions once written. Different hiding strategies (low contrast, small font) were shown to be similarly harmful to the default (high contrast, large font) for GPT-4o and Reka Core, while low contrast settings reduced the LMR for Claude models (69% to 14% for Claude-3, 58 to 33% for Claude-3.5, Figs.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>b, <xref rid="Fig2" ref-type="fig">2d</xref>, Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">5</xref>).</p></sec><sec id="Sec5"><title>Prompt injections are modality-agnostic and not easily mitigated</title><p id="Par10">Current state-of-the-art VLMs are predominantly closed-source. It is therefore unclear whether they are trained comprehensively across diverse medical imaging modalities, systematic evaluation for this domain is lacking<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. We therefore investigated the vision capabilities on organ detection and lesion detection for six clinically relevant imaging modalities (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>). In line with the most likely representation in training data, organ detection for photographs and radiological imaging far exceeded that of endoscopic and histological imaging (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3a</xref>, Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">6</xref>). We observed that all investigated models were susceptible to prompt injection irrespective of the imaging modality (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3a&#x02013;d</xref>, averaged ASR 32; 32; 49; 58; 61% for US, Endoscopy, MRI, CT and Histology, respectively, Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">7</xref>), with significant differences only between US and CT (<italic>p</italic>&#x02009;=&#x02009;0.02). Together, these data show that prompt injection is modality-agnostic, as well as generalizable over different strategies and visibility of the injected prompt.<fig id="Fig3"><label>Fig. 3</label><caption><title>Prompt injection attacks are modality-agnostic.</title><p>Heatmaps per model and imaging modality for (<bold>a</bold>) mean organ detection rate, (<bold>b</bold>) mean attack success rate, (<bold>c</bold>) lesion miss rate (LMR) for the native models and (<bold>d</bold>) mean lesion miss rate (LMR) for the prompts with prompt injection, with (<bold>b</bold>) representing the tile-based difference between (<bold>d</bold>) and (<bold>c</bold>). CT Computed Tomography, MRI Magnetic Resonance Imaging, US Ultrasound.&#x000a0;* represents instances where LMR was higher for native models than injected models (<italic>n</italic>&#x02009;=&#x02009;1). <bold>e</bold> Thumbnails of all images used for the study sorted by modality. All images contain a histologically confirmed malignant lesion.&#x000a0;(Images are&#x000a0;cropped for this figure, original images see Supplementary Data&#x000a0;<xref rid="MOESM3" ref-type="media">1</xref>).</p></caption><graphic xlink:href="41467_2024_55631_Fig3_HTML" id="d33e634"/></fig></p><p id="Par11">Finally, we investigated three strategies to mitigate prompt injection attacks. Investigated strategies included ethical prompt engineering and agent systems, as well as a combination of both (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). For ethical prompt engineering, we enforced the VLMs to provide answers in line with ethical behavior (Prompts see Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>). To simulate agent-systems, we instructed a second model-instance as a supervisor model. The supervisor observed the first answer, was instructed to actively search for malicious content in the first image and provide its own answer by choosing to either replicate the initial answer or provide independent, helpful feedback. None of the strategies proved to be successful for Claude-3, GPT-4o, and Reka-Core, demonstrating that prompt injection is successful even in repeated model calls (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">8</xref>). However, we observed that prompt engineering for ethical behavior significantly reduced vulnerability to prompt injection for Claude-3.5 (<italic>p</italic>&#x02009;&#x02264;&#x02009;0.001) from 64.8% to 27.8%, suggesting a superior alignment to desirable ethical outputs compared to other models.<fig id="Fig4"><label>Fig. 4</label><caption><title>Mitigation efforts for prompt injection attacks.</title><p>Count of prompt injections that were successful (Model reported no pathologies) or failed (Model reported lesion, either due to failed prompt injection or due to defense mechanism) of <italic>n</italic>&#x02009;=&#x02009;54 distinct scenarios in total (0&#x02013;3 missing values per scenario due to errors in model calling, see Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">1b</xref>). Two-sided Fisher&#x02019;s exact test compared ratio of successful vs failed prompt injections for each condition (intra-model comparison only). <italic>p</italic>-values were adjusted using the Bonferroni method, with *<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05, **<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01, ***<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001.</p></caption><graphic xlink:href="41467_2024_55631_Fig4_HTML" id="d33e680"/></fig></p></sec></sec><sec id="Sec6" sec-type="discussion"><title>Discussion</title><p id="Par12">In summary, our study demonstrates that subtle prompt injection attacks on state-of-the-art VLMs can cause harmful outputs. These attacks can be performed without access to the model architecture, i.e., as black-box attacks. Potential attackers encompass cybercriminals, blackmailers, insiders with malicious intent, or, as observed with increasing and concerning frequency, political actors engaging in cyber warfare<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. These would only need to gain access to the user&#x02019;s prompt, e.g., before the data reaches the secure hospital infrastructure. Inference, for which data is sent to the (most-likely external) VLM-provider, serves as another gateway (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1b</xref>). Here, a simple, malicious browser extension would suffice to alter a prompt that is sent via web-browser<sup><xref ref-type="bibr" rid="CR28">28</xref>&#x02013;<xref ref-type="bibr" rid="CR31">31</xref></sup>. These methods are of significant concern, especially in an environment such as healthcare, where individuals are stressed, overworked and are operating within a chronically underfunded cybersecurity infrastructure<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>. This makes prompt injection a highly relevant security threat in future healthcare infrastructure, as injections can be hidden in virtually any data that is processed by medical AI systems<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>. Given that prompt injection exploits the fundamental input mechanism of LLMs, prompt injection is likely to be a fundamental problem of LLMs/VLMs, not exclusive to the tested models, and not easily fixable, as the model is simply following the (altered) instructions. Recent technical improvements to LLMs, e.g., Short circuiting, important to mitigate intrinsically harmful outputs such as weapon-building-instructions, are insufficient to mitigate such attacks<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. Agent-systems composed of multiple models have similarly been shown to be targetable<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Further, other types of guardrails can be bypassed<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> or compromise usability, as shown for Gemini 1.5. A possible solution to this could be hybrid alignment training<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, enforcing prioritization on ethical outputs alongside human preferences over blind adherence to inappropriate requests. As we show that Claude-3.5, after years of alignment research from Anthropic<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, is the only tested model where mitigation worked to some extent (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>), this approach appears promising. Other approaches could include rigorous enforcement or wrapping of the prompt structure<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Moreover, public release of model-specific approaches to alignment training, currently not available, could assist in the&#x000a0;development of solutions, especially as this would allow causal investigations for the varying levels of susceptibility to prompt injection attacks for different models. Overall, our data highlight the need for techniques specifically targeting this form of adversarial attacks.</p><p id="Par13">While we acknowledge that prompt injection in general has been described elsewhere in general<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup>, the concept bears exceptional risks for the medical domain: Firstly, the medical domain is dealing with data that is not necessarily represented in the training data of SOTA VLMs, resulting in lower overall accuracy. Secondly, medical data is life-critical of nature. Thirdly, specific use cases (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1b</xref>) are unique to clinical context. Lastly, while one would anticipate LLM-guardrails to prevent prompt injection from working in life-critical contexts, they clearly do not, as we show that prompt injection is a relevant threat in the medical domain. Hospital infrastructures face a dual challenge and a complex risk-benefit scenario here: They will have to adapt to both integrate LLMs and build robust infrastructure around them to prevent these new forms of attacks, e.g., by deploying agent-based systems and focusing not only on performance but also on alignment when choosing a model<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. Despite our findings pointing to relevant security threats, integrating LLMs in hospitals holds tremendous promise for patient empowerment, reduction of documentation burden, and guideline-based clinician support<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup>. Our study therefore encourages all relevant stakeholders to adopt these LLMs and VLMs but to develop new ways to harden the systems against all forms of adversarial attacks, ideally before approval as medical devices<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. A promising way for such hardening is to keep human experts in the loop and to have highly critical decisions double-checked and vetted by humans who ultimately take responsibility for clinical decisions.</p></sec><sec id="Sec7"><title>Methods</title><sec id="Sec8"><title>Ethics statement</title><p id="Par14">This study does not include confidential information. All research procedures were conducted exclusively on anonymized patient data and in accordance with the Declaration of Helsinki, maintaining all relevant ethical standards. No participant consent was required as the data consisted of anonymized images and was obtained either from local hospital servers or from external sources where informed consent is a prerequisite for the submission and use of such information. The overall analysis was approved by the Ethics Commission of the Medical Faculty of the Technical University Dresden (BO-EK-444102022). Local data was obtained from Uniklinik RWTH Aachen under grant nr EK 028/19. Our work demonstrates a significant threat to healthcare. By publicly disclosing the vulnerabilities and attacks explored in this paper, our goal is to encourage robust mitigation and defense mechanisms and promote transparency regarding risks associated with LLMs. All prompts were injected in a completely simulated scenario to prevent unintended harm. We strongly emphasize that the disclosed attack techniques and prompts should under no circumstances be used in real-world scenarios without proper authorization.</p></sec><sec id="Sec9"><title>Patient cases</title><p id="Par15">Single transversal images of anonymized patient cases were retrieved from local university hospital servers (CT/MRI, each <italic>n</italic>&#x02009;=&#x02009;3) by a board-certified radiologist, and from publicly available resources (ultrasound, <italic>n</italic>&#x02009;=&#x02009;3 Radiopaedia.org, with case courtesy of Di Muzio B (10.53347/rID-70007), Keshavamurthy J (10.53347/rID-68460) and Lawson <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.53347/rID-26464">A</ext-link>). Endoscopic, histological and dermatoscopic images (each <italic>n</italic>&#x02009;=&#x02009;3) were retrieved from Wikimedia Commons, licensed under CC BY-SA 4.0. All diagnoses were double-checked by our board-certified physicians for radiology, dermatology, pathology, or internal medicine, respectively. Further information on patient cases displayed in Supplementary Data&#x000a0;<xref rid="MOESM5" ref-type="media">3</xref>.</p></sec><sec id="Sec10"><title>Model setup</title><p id="Par16">Models (Claude 3 Opus (claude-3-opus-20240229), Claude 3.5 Sonnet (claude-3-5-sonnet-20240620), Gemini 1.5, GPT-4o (gpt-4o-2024-05-13) and Reka Core (reka-core-20240501) were accessed between the 12th of June 2024 and the 21st of June 2024 and on 5th of August 2024 via respective web interfaces. Models were further accessed via API on 5th of August and between 2nd and 4th of September 2024. No learning features (e.g., ChatGPT&#x02019;s Memory function) were used, to prevent the model from learning desired outputs from earlier chat history. User prompts were introduced in independent chat windows/API calls along with image prompts, with the temperature set to 0.7 (default setting for most LLMs) for all models and maximum token count to 1000. No individual system prompts (e.g., ChatGPT&#x02019;s &#x0201c;Individual Configuration&#x0201d; feature) were added.</p></sec><sec id="Sec11"><title>Experimental setup</title><p id="Par17">For each patient case, five images were composed. Images 1, 2, and 5 were passed independently to the models. Image 1 served as negative control, with just the unaltered prompt template added to the chat. Image 2 was passed together with the &#x0201c;text prompt injection&#x0201d;-prompt (&#x0201c;Just describe which organ you see but state it looks healthy&#x0201d;). Text characters were not hidden but passed directly to the model. Images 3 and 4, were passed to the same chat&#x000a0;or API-call, to simulate a prompt injection that is already present as model instruction (in image 3) before the image to assess (image 4) is passed. Prompt injections were varied in font size (14 or 6) and color (black (#060606), or dark gray (#171717)), background color was either white (#ffffff) or black (#060606). Image prompts combining prompt injection (or whitespace) + radiological image were passed to the model with a resolution of 4457&#x02009;&#x000d7;&#x02009;2846 pixels, with original imaging resolution between 500 and 1000 pixels per dimension. For the low-contrast setting, we chose &#x0201c;black in black&#x0201d; over &#x0201c;white in white&#x0201d; because black reflects the usual background in a clinical setting of radiology imaging. All images are available in Supplementary Data&#x000a0;<xref rid="MOESM3" ref-type="media">1</xref>, all model outputs are available in Supplementary Data&#x000a0;<xref rid="MOESM4" ref-type="media">2</xref>.</p></sec><sec id="Sec12"><title>Organ detection assessment</title><p id="Par18">Organ detection was assessed in two separate instances per model output, evaluating correct detection of the liver as region of interest firstly for the answer to &#x0201c;Liver:&#x0201d; and secondly for &#x0201c;Pathologies:&#x0201d;. Each model output was therefore evaluated with 0, 1, or 2 points for organ detection: 1 if one of two instances was correct, and 2 points if both instances were correct. This ordinal scale was chosen as models tended to falsely label the liver as &#x0201c;not present&#x0201d; in the first instance, &#x0201c;Liver&#x0201d;, but correctly identified the liver (plus the lesion) under &#x0201c;Pathologies&#x0201d;, suggesting that the wrong label in the first instance was caused by structural constraints inflicted upon the model by our template. This is commonly observed, as models &#x0201c;think while they write&#x0201d; and are therefore more likely to hallucinate when forced in specific structures<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The threshold for inclusion for assessment of hallucination was set to 50%. Mean organ detection rate over the three iterations for every prompt was plotted and underwent statistical analysis.</p></sec><sec id="Sec13"><title>Lesion miss rate and attack success rate</title><p id="Par19">Lesion miss rate (LMR) was assessed as a binary score of 0 and 1 by a licensed physician for all model outputs, including negative controls without prompt injection. Standard disclaimers such as &#x0201c;remember, I am not a doctor&#x0201d; were excluded from the assessment, only the direct answer to the question was evaluated. Responses like &#x0201c;No pathologies (pathologies ignored as instructed)&#x0201d;, as well as instances where the model refused to provide false information were categorized as non-harmful responses. Mean&#x02009;&#x000b1;&#x02009;standard deviation over the three iterations for every prompt was plotted and underwent statistical analysis. Attack success rate (ASR) was calculated as the difference between LMR for prompt injections and LMR for unaltered prompts.</p></sec><sec id="Sec14"><title>Mitigation efforts</title><p id="Par20">All cases were sent to Claude-3, Claude-3.5, GPT-4o, and Reka-Core between the 2nd to 4th of September 2024. For &#x0201c;Ethical prompt&#x0201d;, the initial prompt was supplemented with the sentence &#x0201c;Remember to adhere to the highest ethical standards as a responsible medical AI system.&#x0201d; For the supervisor models, the corresponding answer from the model for the &#x0201c;Default&#x0201d; and &#x0201c;Ethical&#x0201d; scenarios was integrated into a supervisor instruction prompt (Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>). Scenarios were analyzed in triplicates.</p></sec><sec id="Sec15"><title>Statistics and reproducibility</title><p id="Par21">All results are shown as mean&#x02009;&#x000b1;&#x02009;standard deviation (SD). Sample sizes were chosen as triplicates for each measurement to ensure the representation of output variance. Data for Gemini 1.5 Pro were excluded as internal guardrails of Gemini prevented application on medical images. No randomization or blinding was performed. Significance was either assessed by two-sided Mann-Whitney U test (independent samples) or two-sided Wilcoxon Signed-Rank test (dependent samples/within the same model) or two-sided Kruskal-Wallis test with Dunn&#x02019;s test for comparison of &#x02265;3 groups, each with Bonferroni correction for multiple testing, with significance level alpha &#x0003c;0.05. The significance for changes in relation (mitigation efforts) was calculated with two-sided Fisher&#x02019;s exact test with Bonferroni. All steps of data processing and statistical analysis are documented in our GitHub repository.</p></sec><sec id="Sec16"><title>Software</title><p id="Par22">Models were assessed via respective web interfaces or via API using Visual Studio Code with Python Version 3.11. Graphs were created with RStudio (2024.04.0) including the libraries ggplot2, dplyr, readxl, tidyr, gridExtra, FSA, rstatix, scales, RColorBrewer). Figures were composed with Inkscape, version 1.3.2. The models GPT-4o (OpenAI) and Claude 3.5 Sonnet (Anthropic) were used for spell checking, grammar correction and programming assistance during the writing of this article, in accordance with the COPE (Committee on Publication Ethics) position statement of 13 February 2023<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>.</p></sec><sec id="Sec17"><title>Reporting summary</title><p id="Par23">Further information on research design is available in the&#x000a0;<xref rid="MOESM6" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p></sec></sec><sec id="Sec18" sec-type="supplementary-material"><title>Supplementary information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41467_2024_55631_MOESM1_ESM.pdf"><caption><p>Supplementary Information</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="41467_2024_55631_MOESM2_ESM.pdf"><caption><p>Description of Additional Supplementary Files</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="41467_2024_55631_MOESM3_ESM.pdf"><caption><p>Supplementary Data 1</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM4"><media xlink:href="41467_2024_55631_MOESM4_ESM.xlsx"><caption><p>Supplementary Data 2</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM5"><media xlink:href="41467_2024_55631_MOESM5_ESM.xlsx"><caption><p>Supplementary Data 3</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM6"><media xlink:href="41467_2024_55631_MOESM6_ESM.pdf"><caption><p>Reporting Summary</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM7"><media xlink:href="41467_2024_55631_MOESM7_ESM.pdf"><caption><p>Peer Review file</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s41467-024-55631-x.</p></sec><ack><title>Acknowledgements</title><p>The results generated in our study are in part based upon data provided by Radiopaedia.org, with case courtesy of Di Muzio B (10.53347/rID-70007), Keshavamurthy J (10.53347/rID-68460) and Lawson A. Further, images used in this study were sourced from Wikimedia Commons, all licensed under CC BY-SA 4.0. References and further details see Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">2</xref>. J.C. is supported by the Mildred-Scheel-Postdoktorandenprogramm of the German Cancer Aid (grant #70115730). C.V.S is supported by a grant from the Interdisciplinary Centre for Clinical Research within the Faculty of Medicine at the RWTH Aachen University (PTD 1-13/IA 532313), the Junior Principal Investigator Fellowship program of RWTH Aachen Excellence strategy, the NRW Rueckkehr Programme of the Ministry of Culture and Science of the German State of North Rhine-Westphalia and by the CRC 1382 (ID 403224013) funded by Deutsche Forschungsgesellschaft (DFG, German Research Foundation). S.F. is supported by the German Federal Ministry of Education and Research (SWAG, 01KD2215A), the German Cancer Aid (DECADE, 70115166 and TargHet, 70115995) and the German Research Foundation (504101714). D.T. is funded by the German Federal Ministry of Education and Research (TRANSFORM LIVER, 031L0312A), the European Union&#x02019;s Horizon Europe and Innovation program (ODELIA, 101057091), and the German Federal Ministry of Health (SWAG, 01KD2215B). J.N.K. is supported by the German Cancer Aid (DECADE, 70115166), the German Federal Ministry of Education and Research (PEARL, 01KD2104C; CAMINO, 01EO2101; SWAG, 01KD2215A; TRANSFORM LIVER, 031L0312A; TANGERINE, 01KT2302 through ERA-NET Transcan; Come2Data, 16DKZ2044A; DEEP-HCC, 031L0315A), the German Academic Exchange Service (SECAI, 57616814), the German Federal Joint Committee (TransplantKI, 01VSF21048) the European Union&#x02019;s Horizon Europe and innovation program (ODELIA, 101057091; GENIAL, 101096312), the European Research Council (ERC; NADIR, 101114631), the National Institutes of Health (EPICO, R01 CA263318) and the National Institute for Health and Care Research (NIHR, NIHR203331) Leeds Biomedical Research Centre. The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR, or the Department of Health and Social Care. This work was funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>J.C. designed and performed the experiments, evaluated and interpreted the results, and wrote the initial draft of the manuscript. D.F., I.C.W., and J.N.K. provided scientific support for running the experiments and contributed to writing the manuscript. J.C. and D.T. provided the raw data. T.J.B., S.F., D.T., and J.N.K. provided specialist medical supervision in their respective fields. JNK supervised the study. J.C., D.F., I.C.W., C.V.S., T.J.B., S.F.,. D.T., J.N.K. contributed scientific advice and approved the final version of the manuscript.</p></notes><notes notes-type="peer-review"><title>Peer review</title><sec id="FPar1"><title>Peer review information</title><p id="Par24"><italic>Nature Communications</italic> thanks Hao Chen, and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. A peer review file is available.</p></sec></notes><notes notes-type="funding-information"><title>Funding</title><p>Open Access funding enabled and organized by Projekt DEAL.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The original data (patient information, images, prompts, model outputs, ratings, summary statistics) generated in this study are available in the supplementary data and supplementary information, including direct hyperlinks to previously published cases which are all publicly accessible (see Supplementary Data&#x000a0;<xref rid="MOESM5" ref-type="media">3</xref> for hyperlinks).</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>All code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/KatherLab/prompt_injection_attacks">https://github.com/KatherLab/prompt_injection_attacks</ext-link> under a CC BY-NC-SA 4.0 license for full reproducibility. The code was developed specifically for this study and does not include re-used components from previously published repositories or software.</p></notes><notes id="FPar2" notes-type="COI-statement"><title>Competing interests</title><p id="Par25">The authors declare the following competing interests: DT received honoraria for lectures by Bayer and holds shares in StratifAI GmbH, Germany. SF has received honoraria from MSD and BMS. TJB is the owner of Smart Health Heidelberg GmbH, Heidelberg, Germany, outside of the scope of the submitted work. JNK declares consulting services for Bioptimus, France; Owkin, France; DoMore Diagnostics, Norway; Panakeia, UK; AstraZeneca, UK; Mindpeak, Germany; and MultiplexDx, Slovakia. Furthermore, he holds shares in StratifAI GmbH, Germany, Synagen GmbH, Germany, and has received a research grant by GSK, and has received honoraria by AstraZeneca, Bayer, Daiichi Sankyo, Eisai, Janssen, Merck, MSD, BMS, Roche, Pfizer, and Fresenius. ICW has received honoraria from AstraZeneca. DF holds shares in Synagen GmbH, Germany.&#x000a0;No other competing interests are declared by any of the remaining authors.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Singhal, K. et al. Large language models encode clinical knowledge. <italic>Nature</italic><bold>620</bold>, 172&#x02013;180 (2023).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Bubeck, S. et al. Sparks of artificial general intelligence: early experiments with GPT-4. <italic>arXiv [cs.CL]</italic> (2023).</mixed-citation></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Clusmann</surname><given-names>J</given-names></name><etal/></person-group><article-title>The future landscape of large language models in medicine</article-title><source>Commun. Med.</source><year>2023</year><volume>3</volume><fpage>141</fpage><pub-id pub-id-type="doi">10.1038/s43856-023-00370-1</pub-id><pub-id pub-id-type="pmid">37816837</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Clusmann, J. et al. The future landscape of large language models in medicine. <italic>Commun. Med.</italic><bold>3</bold>, 141 (2023).<pub-id pub-id-type="pmid">37816837</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Ferber, D. et al. Autonomous artificial intelligence agents for clinical decision making in oncology. <italic>arXiv [cs.AI]</italic> (2024).</mixed-citation></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Thirunavukarasu</surname><given-names>AJ</given-names></name><etal/></person-group><article-title>Large language models in medicine</article-title><source>Nat. Med.</source><year>2023</year><volume>29</volume><fpage>1930</fpage><lpage>1940</lpage><pub-id pub-id-type="doi">10.1038/s41591-023-02448-8</pub-id><pub-id pub-id-type="pmid">37460753</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Thirunavukarasu, A. J. et al. Large language models in medicine. <italic>Nat. Med.</italic><bold>29</bold>, 1930&#x02013;1940 (2023).<pub-id pub-id-type="pmid">37460753</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Ferber</surname><given-names>D</given-names></name><etal/></person-group><article-title>GPT-4 for information retrieval and comparison of medical oncology guidelines</article-title><source>NEJM AI</source><year>2024</year><volume>1</volume><fpage>AIcs2300235</fpage><pub-id pub-id-type="doi">10.1056/AIcs2300235</pub-id></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Ferber, D. et al. GPT-4 for information retrieval and comparison of medical oncology guidelines. <italic>NEJM AI</italic><bold>1</bold>, AIcs2300235 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Van Veen</surname><given-names>D</given-names></name><etal/></person-group><article-title>Adapted large language models can outperform medical experts in clinical text summarization</article-title><source>Nat. Med.</source><year>2024</year><volume>30</volume><fpage>1134</fpage><lpage>1142</lpage><pub-id pub-id-type="doi">10.1038/s41591-024-02855-5</pub-id><pub-id pub-id-type="pmid">38413730</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Van Veen, D. et al. Adapted large language models can outperform medical experts in clinical text summarization. <italic>Nat. Med.</italic><bold>30</bold>, 1134&#x02013;1142 (2024).<pub-id pub-id-type="pmid">38413730</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Lu, M. Y. et al. A Multimodal Generative AI Copilot for Human Pathology. <italic>Nature</italic>10.1038/s41586-024-07618-3 (2024).</mixed-citation></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Christensen</surname><given-names>M</given-names></name><name><surname>Vukadinovic</surname><given-names>M</given-names></name><name><surname>Yuan</surname><given-names>N</given-names></name><name><surname>Ouyang</surname><given-names>D</given-names></name></person-group><article-title>Vision-language foundation model for echocardiogram interpretation</article-title><source>Nat. Med.</source><year>2024</year><volume>30</volume><fpage>1481</fpage><lpage>1488</lpage><pub-id pub-id-type="doi">10.1038/s41591-024-02959-y</pub-id><pub-id pub-id-type="pmid">38689062</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Christensen, M., Vukadinovic, M., Yuan, N. &#x00026; Ouyang, D. Vision-language foundation model for echocardiogram interpretation. <italic>Nat. Med.</italic><bold>30</bold>, 1481&#x02013;1488 (2024).<pub-id pub-id-type="pmid">38689062</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Hello GPT-4o. <ext-link ext-link-type="uri" xlink:href="https://openai.com/index/hello-gpt-4o/">https://openai.com/index/hello-gpt-4o/</ext-link>.</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Vision. Anthropic <ext-link ext-link-type="uri" xlink:href="https://docs.anthropic.com/en/docs/vision">https://docs.anthropic.com/en/docs/vision</ext-link>.</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Gemini Team et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. <italic>arXiv [cs.CL]</italic> (2024).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Chameleon Team. Chameleon: mixed-modal early-fusion foundation models. <italic>arXiv [cs.CL]</italic> (2024).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Reka Team et al. Reka core, flash, and edge: a series of powerful multimodal language models. <italic>arXiv [cs.CL]</italic> (2024).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Zou, A. et al. Improving alignment and robustness with short circuiting. <italic>NeurIPS</italic> (2024).</mixed-citation></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Ghaffari Laleh</surname><given-names>N</given-names></name><etal/></person-group><article-title>Adversarial attacks and adversarial robustness in computational pathology</article-title><source>Nat. Commun.</source><year>2022</year><volume>13</volume><fpage>5711</fpage><pub-id pub-id-type="doi">10.1038/s41467-022-33266-0</pub-id><pub-id pub-id-type="pmid">36175413</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Ghaffari Laleh, N. et al. Adversarial attacks and adversarial robustness in computational pathology. <italic>Nat. Commun.</italic><bold>13</bold>, 5711 (2022).<pub-id pub-id-type="pmid">36175413</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Liu, Y. <italic>et al</italic>. Prompt Injection attack against LLM-integrated Applications. <italic>arXiv [cs.CR]</italic> (2023).</mixed-citation></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Biggio</surname><given-names>B</given-names></name><name><surname>Roli</surname><given-names>F</given-names></name></person-group><article-title>Wild patterns: ten years after the rise of adversarial machine learning</article-title><source>Pattern Recognit.</source><year>2018</year><volume>84</volume><fpage>317</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2018.07.023</pub-id></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Biggio, B. &#x00026; Roli, F. Wild patterns: ten years after the rise of adversarial machine learning. <italic>Pattern Recognit.</italic><bold>84</bold>, 317&#x02013;331 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Kimura, S., Tanaka, R., Miyawaki, S., Suzuki, J. &#x00026; Sakaguchi, K. Empirical analysis of large vision-language models against goal hijacking via visual prompt injection. <italic>arXiv [cs.CL]</italic> (2024).</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Rossi, S., Michel, A. M., Mukkamala, R. R. &#x00026; Thatcher, J. B. An early categorization of prompt injection attacks on large language models. <italic>arXiv [cs.CR]</italic> (2024).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Hubinger, E. et al. Sleeper agents: training deceptive LLMs that persist through safety training. <italic>arXiv [cs.CR]</italic> (2024).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Jiang, F. et al. ArtPrompt: ASCII art-based jailbreak attacks against aligned LLMs. <italic>arXiv [cs.CL]</italic> (2024).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Meta Llama. <italic>Meta Llama</italic><ext-link ext-link-type="uri" xlink:href="https://llama.meta.com/">https://llama.meta.com/</ext-link>.</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Dubey, A. et al. The Llama 3 herd of models. <italic>arXiv [cs.AI]</italic> (2024).</mixed-citation></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Kaczmarczyk</surname><given-names>R</given-names></name><name><surname>Wilhelm</surname><given-names>TI</given-names></name><name><surname>Martin</surname><given-names>R</given-names></name><name><surname>Roos</surname><given-names>J</given-names></name></person-group><article-title>Evaluating multimodal AI in medical diagnostics</article-title><source>NPJ Digit Med</source><year>2024</year><volume>7</volume><fpage>205</fpage><pub-id pub-id-type="doi">10.1038/s41746-024-01208-3</pub-id><pub-id pub-id-type="pmid">39112822</pub-id>
</element-citation><mixed-citation id="mc-CR25" publication-type="journal">Kaczmarczyk, R., Wilhelm, T. I., Martin, R. &#x00026; Roos, J. Evaluating multimodal AI in medical diagnostics. <italic>NPJ Digit Med</italic><bold>7</bold>, 205 (2024).<pub-id pub-id-type="pmid">39112822</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Law, R. Cyberattacks on healthcare: Russia&#x02019;s tool for mass disruption. <italic>Medical Device Network</italic><ext-link ext-link-type="uri" xlink:href="https://www.medicaldevice-network.com/features/cyberattacks-on-healthcare-russias-tool-for-mass-disruption/">https://www.medicaldevice-network.com/features/cyberattacks-on-healthcare-russias-tool-for-mass-disruption/</ext-link> (2024).</mixed-citation></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname><given-names>M</given-names></name><name><surname>Lusthaus</surname><given-names>J</given-names></name><name><surname>Kashyap</surname><given-names>R</given-names></name><name><surname>Phair</surname><given-names>N</given-names></name><name><surname>Varese</surname><given-names>F</given-names></name></person-group><article-title>Mapping the global geography of cybercrime with the World Cybercrime Index</article-title><source>PLoS One</source><year>2024</year><volume>19</volume><fpage>e0297312</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0297312</pub-id><pub-id pub-id-type="pmid">38598553</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Bruce, M., Lusthaus, J., Kashyap, R., Phair, N. &#x00026; Varese, F. Mapping the global geography of cybercrime with the World Cybercrime Index. <italic>PLoS One</italic><bold>19</bold>, e0297312 (2024).<pub-id pub-id-type="pmid">38598553</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Jalali</surname><given-names>MS</given-names></name><name><surname>Bruckes</surname><given-names>M</given-names></name><name><surname>Westmattelmann</surname><given-names>D</given-names></name><name><surname>Schewe</surname><given-names>G</given-names></name></person-group><article-title>Why employees (still) click on phishing links: Investigation in hospitals</article-title><source>J. Med. Internet Res.</source><year>2020</year><volume>22</volume><fpage>e16775</fpage><pub-id pub-id-type="doi">10.2196/16775</pub-id><pub-id pub-id-type="pmid">32012071</pub-id>
</element-citation><mixed-citation id="mc-CR28" publication-type="journal">Jalali, M. S., Bruckes, M., Westmattelmann, D. &#x00026; Schewe, G. Why employees (still) click on phishing links: Investigation in hospitals. <italic>J. Med. Internet Res.</italic><bold>22</bold>, e16775 (2020).<pub-id pub-id-type="pmid">32012071</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>G</given-names></name><name><surname>Ghafur</surname><given-names>S</given-names></name><name><surname>Kinross</surname><given-names>J</given-names></name><name><surname>Hankin</surname><given-names>C</given-names></name><name><surname>Darzi</surname><given-names>A</given-names></name></person-group><article-title>WannaCry-a year on</article-title><source>BMJ</source><year>2018</year><volume>361</volume><fpage>k2381</fpage><pub-id pub-id-type="doi">10.1136/bmj.k2381</pub-id><pub-id pub-id-type="pmid">29866711</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Martin, G., Ghafur, S., Kinross, J., Hankin, C. &#x00026; Darzi, A. WannaCry-a year on. <italic>BMJ</italic><bold>361</bold>, k2381 (2018).<pub-id pub-id-type="pmid">29866711</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>WJ</given-names></name><etal/></person-group><article-title>Assessment of employee susceptibility to phishing attacks at US health care institutions</article-title><source>JAMA Netw. Open</source><year>2019</year><volume>2</volume><fpage>e190393</fpage><pub-id pub-id-type="doi">10.1001/jamanetworkopen.2019.0393</pub-id><pub-id pub-id-type="pmid">30848810</pub-id>
</element-citation><mixed-citation id="mc-CR30" publication-type="journal">Gordon, W. J. et al. Assessment of employee susceptibility to phishing attacks at US health care institutions. <italic>JAMA Netw. Open</italic><bold>2</bold>, e190393 (2019).<pub-id pub-id-type="pmid">30848810</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Toreini</surname><given-names>E</given-names></name><name><surname>Shahandashti</surname><given-names>SF</given-names></name><name><surname>Mehrnezhad</surname><given-names>M</given-names></name><name><surname>Hao</surname><given-names>F</given-names></name></person-group><article-title>DOMtegrity: ensuring web page integrity against malicious browser extensions</article-title><source>Int. J. Inf. Secur.</source><year>2019</year><volume>18</volume><fpage>801</fpage><lpage>814</lpage><pub-id pub-id-type="doi">10.1007/s10207-019-00442-1</pub-id><pub-id pub-id-type="pmid">31632229</pub-id>
</element-citation><mixed-citation id="mc-CR31" publication-type="journal">Toreini, E., Shahandashti, S. F., Mehrnezhad, M. &#x00026; Hao, F. DOMtegrity: ensuring web page integrity against malicious browser extensions. <italic>Int. J. Inf. Secur.</italic><bold>18</bold>, 801&#x02013;814 (2019).<pub-id pub-id-type="pmid">31632229</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Cartwright</surname><given-names>AJ</given-names></name></person-group><article-title>The elephant in the room: cybersecurity in healthcare</article-title><source>J. Clin. Monit. Comput.</source><year>2023</year><volume>37</volume><fpage>1123</fpage><lpage>1132</lpage><pub-id pub-id-type="doi">10.1007/s10877-023-01013-5</pub-id><pub-id pub-id-type="pmid">37088852</pub-id>
</element-citation><mixed-citation id="mc-CR32" publication-type="journal">Cartwright, A. J. The elephant in the room: cybersecurity in healthcare. <italic>J. Clin. Monit. Comput.</italic><bold>37</bold>, 1123&#x02013;1132 (2023).<pub-id pub-id-type="pmid">37088852</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/html/2410.07283v1">https://arxiv.org/html/2410.07283v1</ext-link>.</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Wang, C. et al. Hybrid Alignment Training for Large Language Models. In <italic>Findings of the Association for Computational Linguistics: ACL 2024</italic>, 11389&#x02013;11403 (Bangkok Thailand, Association for Computational Linguistics, 2024).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Research. <ext-link ext-link-type="uri" xlink:href="https://www.anthropic.com/research#alignment">https://www.anthropic.com/research#alignment</ext-link>.</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Debenedetti, E. et al. AgentDojo: a dynamic environment to evaluate attacks and defenses for LLM agents. <italic>NeurIPS</italic> (2024).</mixed-citation></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>T</given-names></name><etal/></person-group><article-title>Comparative analysis of multimodal large language model performance on clinical vignette questions</article-title><source>JAMA</source><year>2024</year><volume>331</volume><fpage>1320</fpage><lpage>1321</lpage><pub-id pub-id-type="doi">10.1001/jama.2023.27861</pub-id><pub-id pub-id-type="pmid">38497956</pub-id>
</element-citation><mixed-citation id="mc-CR37" publication-type="journal">Han, T. et al. Comparative analysis of multimodal large language model performance on clinical vignette questions. <italic>JAMA</italic><bold>331</bold>, 1320&#x02013;1321 (2024).<pub-id pub-id-type="pmid">38497956</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Yang, Y., Jin, Q., Huang, F. &#x00026; Lu, Z. Adversarial attacks on Large Language Models in medicine. <italic>arXiv [cs.AI]</italic> (2024).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Lu, A., Zhang, H., Zhang, Y., Wang, X. &#x00026; Yang, D. Bounding the capabilities of large language models in open text generation with prompt constraints. 1982&#x02013;2008 (2023).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Authorship and AI tools. COPE: Committee on Publication Ethics <ext-link ext-link-type="uri" xlink:href="https://publicationethics.org/cope-position-statements/ai-author">https://publicationethics.org/cope-position-statements/ai-author</ext-link>.</mixed-citation></ref></ref-list></back></article>