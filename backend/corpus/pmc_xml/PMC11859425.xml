<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006311</article-id><article-id pub-id-type="pmc">PMC11859425</article-id><article-id pub-id-type="doi">10.3390/s25041081</article-id><article-id pub-id-type="publisher-id">sensors-25-01081</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Sub-Pixel Measurement Platform Using Twist-Angle Analysis in Two-Dimensional Planes</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lyu</surname><given-names>Jiangbo</given-names></name><xref rid="af1-sensors-25-01081" ref-type="aff">1</xref><xref rid="af2-sensors-25-01081" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Kong</surname><given-names>Wenchao</given-names></name><xref rid="af1-sensors-25-01081" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Yan</given-names></name><xref rid="af1-sensors-25-01081" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Pi</surname><given-names>Yazhi</given-names></name><xref rid="af1-sensors-25-01081" ref-type="aff">1</xref><xref rid="c1-sensors-25-01081" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Cao</surname><given-names>Zizheng</given-names></name><xref rid="af1-sensors-25-01081" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Della Corte</surname><given-names>Francesco</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01081"><label>1</label>Peng Cheng Laboratory, Shenzhen 518055, China</aff><aff id="af2-sensors-25-01081"><label>2</label>Department of Electronic and Information Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China</aff><author-notes><corresp id="c1-sensors-25-01081"><label>*</label>Correspondence: <email>piyzh@pcl.ac.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>11</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1081</elocation-id><history><date date-type="received"><day>15</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>08</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>10</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Arrayed ultraviolet (UV) LED light sources have been widely applied in various semiconductor processes, ranging from photopolymerization to lithography. In practical cases, based on data provided by manufacturers, calibration of individual UV LEDs is often needed before their real usage in high-precision applications. In this paper, we present a high-precision, automated light source measurement platform, which can be applied to the performance evaluation of various types of light sources. In order to minimize errors introduced by the automated measurement system, the platform employs a sub-pixel measurement technique, along with a twist-angle method, to perform multiple measurements and analyses of the spatial intensity distribution of the light source on a given plane. Through noise analysis of repeated measurements, the platform&#x02019;s effectiveness and reliability are validated within a certain tolerance range. The high-precision automated light source measurement platform demonstrates excellent performance in the precise control and data acquisition of complex light sources. The light source dataset derived from the test results can provide guidance for the optimization of light sources in fields such as lighting, imaging, and lithography.</p></abstract><kwd-group><kwd>arrayed UV LED</kwd><kwd>measurement platform</kwd><kwd>twist-angle analysis</kwd><kwd>sub-pixel</kwd><kwd>spatial intensity distribution</kwd><kwd>response time</kwd></kwd-group><funding-group><award-group><funding-source>PCL</funding-source></award-group><funding-statement>This work was supported by the Major Key Project of PCL.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01081"><title>1. Introduction</title><p>With the rapid development of industrial technologies, advanced manufacturing techniques have been widely applied across various fields. Light source technology, particularly efficient and precise light sources, has become a core driving force behind these technological advancements [<xref rid="B1-sensors-25-01081" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01081" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01081" ref-type="bibr">3</xref>]. As an emerging light source technology, arrayed UV LED light sources are gradually replacing traditional light sources due to their high luminous efficiency, long lifespan, superior controllability, and environmental friendliness [<xref rid="B4-sensors-25-01081" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01081" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01081" ref-type="bibr">6</xref>]. They have found extensive applications in fields such as microelectronics manufacturing [<xref rid="B7-sensors-25-01081" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01081" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01081" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01081" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01081" ref-type="bibr">11</xref>], photolithography [<xref rid="B12-sensors-25-01081" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01081" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01081" ref-type="bibr">14</xref>], and curing processes [<xref rid="B15-sensors-25-01081" ref-type="bibr">15</xref>]. However, in practical applications, the data provided by suppliers are not guaranteed to meet the requirements of high-precision applications directly. Consequently, calibration is usually necessary before utilizing UV LEDs for such purposes. The performance of light sources in these applications directly impacts process quality, system reliability, and overall efficiency [<xref rid="B16-sensors-25-01081" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01081" ref-type="bibr">17</xref>]. Therefore, the precise characterization of light sources is crucial to ensure their optimal performance in practical applications.</p><p>However, existing light source testing platforms often face challenges, such as insufficient resolution and the inability to comprehensively capture critical parameters of the light source [<xref rid="B18-sensors-25-01081" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-01081" ref-type="bibr">19</xref>], including spatial intensity distribution and response time. The limitations of traditional platforms often result in measurements that lack precision or fail to reflect the actual performance of light sources in certain applications. Therefore, the development of a high-precision, highly automated testing platform has become a pressing technical challenge in the field of light source testing.</p><p>To address this issue, extensive research has been conducted on measurement techniques, and the application of super-resolution imaging has been explored to enhance the accuracy of light source measurements. As digital images serve as two-dimensional signal records, higher-resolution digital images are often required in many applications [<xref rid="B5-sensors-25-01081" ref-type="bibr">5</xref>,<xref rid="B20-sensors-25-01081" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-01081" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01081" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-01081" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-01081" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-01081" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-01081" ref-type="bibr">26</xref>]. The need for resolution enhancement becomes particularly critical for the precise representation of the spatial intensity distribution of light sources [<xref rid="B26-sensors-25-01081" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01081" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-01081" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-01081" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-01081" ref-type="bibr">30</xref>]. Currently, there are two main approaches to achieving high-resolution images. The first involves reducing the pixel size [<xref rid="B31-sensors-25-01081" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-01081" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-01081" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-01081" ref-type="bibr">34</xref>]. However, as the pixel size decreases, new challenges arise, including a lower signal-to-noise ratio, increased pixel crosstalk, and reduced readout speed [<xref rid="B35-sensors-25-01081" ref-type="bibr">35</xref>]. The second approach involves employing non-rectangular pixel layouts, increasing the camera focal length, or enlarging the chip size [<xref rid="B36-sensors-25-01081" ref-type="bibr">36</xref>]. Yet, due to limitations in sensor technology and manufacturing processes, these methods are generally not considered effective solutions. Specifically, modern CCD and CMOS sensor manufacturing processes are primarily based on rectangular pixel layouts. Non-rectangular layouts introduce increased complexity in photolithography, etching, and alignment, resulting in higher defect rates and lower yields. Additionally, current sensor technology is predominantly optimized for rectangular arrays, making the effective production of non-rectangular layouts challenging [<xref rid="B37-sensors-25-01081" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-01081" ref-type="bibr">38</xref>].</p><p>Due to the limitations of hardware devices, super-resolution (SR) techniques are still required to achieve higher resolution. These techniques utilize signal processing methods to generate high-resolution (HR) images from multiple observed low-resolution (LR) images. The advantage of SR methods lies in their ability to overcome the limitations of LR observations without the cost to enhance hardware capabilities [<xref rid="B39-sensors-25-01081" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-01081" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-01081" ref-type="bibr">41</xref>]. Given that light source measurement devices aim to capture spatial intensity distributions, a non-uniform interpolation approach is employed to reconstruct images and achieve super-resolution [<xref rid="B34-sensors-25-01081" ref-type="bibr">34</xref>,<xref rid="B42-sensors-25-01081" ref-type="bibr">42</xref>,<xref rid="B43-sensors-25-01081" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-01081" ref-type="bibr">44</xref>]. This method represents the most intuitive approach for SR image reconstruction and is widely employed in practice due to its broad applicability. By estimating relative motion information, an HR image is obtained at non-uniformly spaced sampling points. Subsequently, a uniformly spaced sampling grid is generated following either a direct or iterative reconstruction procedure. Gross [<xref rid="B45-sensors-25-01081" ref-type="bibr">45</xref>] applied the generalized multi-channel sampling theorem introduced by Papoulis [<xref rid="B46-sensors-25-01081" ref-type="bibr">46</xref>] and Brown [<xref rid="B47-sensors-25-01081" ref-type="bibr">47</xref>] to perform non-uniform interpolation on spatially shifted LR images. This interpolation process was followed by a deblurring step under the assumption of precisely known relative displacements. Komatsu et al. [<xref rid="B48-sensors-25-01081" ref-type="bibr">48</xref>] developed a method for obtaining high-resolution images from multiple simultaneous camera captures using the Landweber algorithm. They employed block-matching to measure relative motion. However, identical apertures in all cameras imposed strict constraints on their placement and scene configuration. This issue was mitigated by equipping the cameras with varying apertures. Hardie et al. [<xref rid="B49-sensors-25-01081" ref-type="bibr">49</xref>] proposed a real-time infrared image registration and SR reconstruction technique, employing a gradient-based registration algorithm to estimate displacements and a weighted nearest-neighbor interpolation approach. Ultimately, Wiener filtering was used to suppress blur and system noise. Shah and Zakhor [<xref rid="B29-sensors-25-01081" ref-type="bibr">29</xref>] adopted the Landweber algorithm for the SR enhancement of color video sequences, but to address registration inaccuracies, they evaluated multiple candidate motion estimates rather than a single motion vector per pixel, using both brightness and color information to compute the motion field. Nguyen and Milanfar [<xref rid="B50-sensors-25-01081" ref-type="bibr">50</xref>] presented an efficient wavelet-based SR reconstruction algorithm. By exploiting the interlaced sampling grid inherent in SR, they derived a wavelet interpolation method tailored for interlaced two-dimensional data, reducing computation while maintaining reconstruction quality.</p><p>We propose a highly automated and high-precision light source testing platform. This platform employs a non-uniform interpolation method to integrate multiple low-resolution images, achieving high-resolution outputs enabling high-resolution outputs and precise characterization of spatial intensity distribution. Additionally, it is capable of measuring critical parameters of light sources, including response time. By implementing repeated measurement techniques, the platform effectively suppresses noise. The spatial intensity distribution obtained exhibits a Gaussian profile, confirming the platform&#x02019;s effectiveness and reliability. The platform is not only applicable for testing ultraviolet LED array light sources but also capable of accurately measuring the key parameters of traditional laser light sources, making it suitable for performance evaluation across various types of light sources. The platform outputs test datasets for spatial intensity distribution and response time, providing essential data support for subsequent light source applications or optimizations. It offers significant potential for a wide range of applications.</p></sec><sec id="sec2-sensors-25-01081"><title>2. Materials and Methods</title><sec id="sec2dot1-sensors-25-01081"><title>2.1. Measurement Setup</title><p>The structural schematic of the light source testing device is shown in <xref rid="sensors-25-01081-f001" ref-type="fig">Figure 1</xref>. The device adopts a vertical structural design to ensure the stable control of the relative position between the light source and the receiver.</p><p>The main components of the light source testing device include a displacement stage, a displacement stage controller (Wuhan RED STAR YANG TECHNOLOGY Co., Ltd., Wuhan, China), a receiver (UV radiometer or UV photodetector, Shenzhen Linshang Technology Co., Ltd., Shenzhen, China), a light source module, a housing, and a computer. The distance between the emitting surface of the light source module and the plane where the receiver is positioned is 100 mm. The displacement stage has a travel range of 1000 mm &#x000d7; 1000 mm, which is sufficient to cover the spatial intensity distribution range of most types of light sources. The UV radiometer enables high-sensitivity measurements, featuring an aperture diameter of 10 mm. The protective housing has dimensions of 335 mm &#x000d7; 620 mm &#x000d7; 500 mm, and the light source module measures 160 mm &#x000d7; 160 mm &#x000d7; 122 mm.</p><p>The light source is a UV array LED light source, mounted on a fixed bracket above the testing platform. Its structural schematic is shown in <xref rid="sensors-25-01081-f002" ref-type="fig">Figure 2</xref>. In <xref rid="sensors-25-01081-f002" ref-type="fig">Figure 2</xref>a, the light source device measures 160 mm &#x000d7; 160 mm, with the LED array occupying a central region of 94 mm &#x000d7; 94 mm. The UV array LED emits light with a central wavelength of 365 nm, and the array consists of 8 &#x000d7; 8 LEDs. <xref rid="sensors-25-01081-f002" ref-type="fig">Figure 2</xref>b shows the cross-sectional intensity distribution data provided by the supplier, obtained using an integrating sphere. In contrast, the objective of the light source measurement system is to capture the spatial intensity distribution in specific directions confined with a specific plan area, rendering a direct comparison between the two datasets impractical. According to the supplier&#x02019;s data, the polar luminous intensity distribution conforms to the Lambertian source model. Therefore, we assume that the emission characteristics of the packaged LED adhere to the Lambertian source model [<xref rid="B19-sensors-25-01081" ref-type="bibr">19</xref>], with its intensity given by Equation (1).<disp-formula id="FD1-sensors-25-01081"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the viewing angle, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the luminous intensity along the surface normal of the light source, <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is directly proportional to the irradiance, and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents an important parameter describing the directional characteristics of the LED light source. It is determined by the LED&#x02019;s half-power angle and is used to control the distribution of the LED light intensity at different angles.</p><p>These LEDs are evenly spaced, with a center-to-center distance of 12.5 mm between adjacent LEDs. The luminous intensity of the LEDs is controlled using pulse-width modulation (PWM) technology, with each LED offering 16 intensity levels. The central wavelength of the light source module is approximately 365 nm. The module employs an air-cooling system for heat dissipation, and the UV LED chip configuration employs the single-chip design, which not only possesses a low level of thermal effects but also significantly enhances the stability and reliability of the light source. The reduction in thermal effects subsequently enhances the stability and reliability of the light source, ensuring consistent performance and extended operational lifespan.</p></sec><sec id="sec2dot2-sensors-25-01081"><title>2.2. Testing Methods</title><p>Based on a standardized testing platform, a light source control driving algorithm has been developed. By coordinating light source control, displacement stage control, and receiver data acquisition, the algorithm enables the communication and control of various UV array LED light sources, movement of the two-dimensional displacement stage, and data reception and storage by the receiver. This achieves the measurement of the spatial intensity distribution and temporal response of the light source. The workflow of the light source control driving algorithm is shown in <xref rid="sensors-25-01081-f003" ref-type="fig">Figure 3</xref>. <xref rid="sensors-25-01081-f003" ref-type="fig">Figure 3</xref>a illustrates the testing process for spatial intensity distribution, where <italic toggle="yes">num</italic> denotes the LED index and <italic toggle="yes">NUM</italic> refers to the total number of light source LEDs (<italic toggle="yes">NUM</italic> = 64). In <xref rid="sensors-25-01081-f003" ref-type="fig">Figure 3</xref>a, the process consists of two nested loops: the inner loop switches the LEDs and collects data, while the outer loop updates the spatial position of the receiver and then repeats the inner loop. The UV radiometer, integrated with the two-dimensional translation stage, enables full control of the scanning path, allowing it to adapt flexibly to measurement areas of various shapes and sizes. A non-uniform interpolation method is employed in the testing process of spatial intensity distribution, as described by Equation (2).<disp-formula id="FD2-sensors-25-01081"><label>(2)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the high-resolution image, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the low-resolution image, and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the index of the low-resolution image sequence. This method leverages a single-pixel information acquisition approach, combining the illumination area of the light source with a radiometer featuring a circular aperture. By utilizing a two-dimensional displacement stage to scan the entire plane point by point, a complete image frame can be generated upon the completion of the scan. This method allows for the collection of multiple low-resolution image frames within the testing area, which can then be used to reconstruct high-resolution images, achieving super-resolution.</p><p><xref rid="sensors-25-01081-f003" ref-type="fig">Figure 3</xref>b illustrates the response time testing process. The UV photodetector measured the transient time response of the UV array LED light source through its flicker behavior.</p><p>The sampling point distribution following the process outlined in <xref rid="sensors-25-01081-f003" ref-type="fig">Figure 3</xref>a is shown in <xref rid="sensors-25-01081-f004" ref-type="fig">Figure 4</xref>a. The black lines represent the grid of the scanning area, while the green circles indicate the positions where the UV radiometer collected spatial intensity distribution data. These collected data are considered a single image <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, arranged in a grid pattern. The grid length was 10 mm, indicating that the resolution of this image was 10 mm.</p><p>To further enhance the resolution, sub-pixel precision shifting and sampling [<xref rid="B51-sensors-25-01081" ref-type="bibr">51</xref>] were performed based on the data collected in <xref rid="sensors-25-01081-f004" ref-type="fig">Figure 4</xref>a. The resulting schematic is shown in <xref rid="sensors-25-01081-f004" ref-type="fig">Figure 4</xref>b. The green filled circles represent the first image frame collected in <xref rid="sensors-25-01081-f004" ref-type="fig">Figure 4</xref>a, while the blue circles correspond to the second image frame <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> obtained through sub-pixel sampling. Since the collected data were sampled based on spatial coordinates, the two frames of data could be directly integrated into a reconstruction image using a sub-pixel interpolation method. The reconstructed image demonstrates a significantly improved resolution compared to the original.</p><p>Based on the detachable convenience of the light source module and the precise positioning control of the two-dimensional displacement stage, it is important to emphasize here that light source module rotation has been introduced in addition to the aforementioned two image frames (<inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>). The rotation was achieved by adjusting the height of the carrier stage and placing a precision-machined calibration block beneath the light source, both with micron-level accuracy, ensuring controlled and precise rotation. This involved a relative rotation of the data acquisition plane, enabling the collection of two additional image frames: the grid sampling and sub-pixel sampling data after a rotation of angle <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The schematic of these four frames is shown in <xref rid="sensors-25-01081-f004" ref-type="fig">Figure 4</xref>c. By reconstructing the image from these four low-resolution frames using a non-uniform interpolation method, super-resolution imaging <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> was achieved.</p><p>By repeatedly measuring the spatial intensity distribution within the testing area, the impact of noise on the results was significantly reduced, thereby improving the signal-to-noise ratio and significantly enhancing the reliability of the testing outcomes. Compared to pixel arrays, this approach greatly reduces the hardware complexity and substantially simplifies costs.</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-01081"><title>3. Results and Discussion</title><p>The light source testing apparatus is capable of measuring key parameters of the light source, including the spatial intensity distribution, temporal response, and other critical characteristics.</p><sec id="sec3dot1-sensors-25-01081"><title>3.1. Spatial Intensity Distribution</title><p>During the grid data acquisition process, the measurement area was set to 260 mm &#x000d7; 260 mm, and the LED intensity level was configured to level 16. The first frame <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> measurement results of a specific LED are shown in <xref rid="sensors-25-01081-f005" ref-type="fig">Figure 5</xref>a. The total measurement time depends on the area to be scanned on the receiver&#x02019;s surface. For each pixel, the data acquisition time was set to 3 s, while the displacement stage required 2 s to move 1 cm. Given that the LED array consisted of 64 light sources and the measurement grid comprised 27 &#x000d7; 27 pixels, the estimated time to complete a full measurement cycle for one frame was approximately 39 h. <xref rid="sensors-25-01081-f005" ref-type="fig">Figure 5</xref>a illustrates the spatial intensity distribution of the LED in the array, with the results represented as a two-dimensional grid. The horizontal and vertical axes represent the spatial coordinates in the x- and y-directions, respectively. The color bar on the right ranges from blue to deep red, indicating irradiance values from low to high within the two-dimensional spatial region. This visual representation uses color to depict the numerical values of the spatial intensity distribution of light. The spot diameter was approximately 100 mm, from which the beam angle of the LED was determined to be 60&#x000b0;.</p><p>The second frame image <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was obtained using sub-pixel sampling for data acquisition. Although the resolution of the equipment itself was limited, sub-pixel sampling captured the details between pixels, effectively simulating smaller spatial units. This approach enhanced measurement accuracy without increasing the actual hardware sampling costs. The test results are shown in <xref rid="sensors-25-01081-f005" ref-type="fig">Figure 5</xref>b. By utilizing sub-pixel precision shifting, the second frame image captured data from the remaining blank regions. To avoid image distortion, the pixel points are refined in <xref rid="sensors-25-01081-f005" ref-type="fig">Figure 5</xref>b, thereby enhancing the visual effect. Compared to <xref rid="sensors-25-01081-f005" ref-type="fig">Figure 5</xref>a, the points in <xref rid="sensors-25-01081-f005" ref-type="fig">Figure 5</xref>b are more densely distributed within the same spatial coordinates. Specifically, the number of pixels increased from 27 &#x000d7; 27 to 27 &#x000d7; 27 + 26 &#x000d7; 26, thereby improving the resolution.</p><p>To more accurately reproduce the distribution of the light source from different perspectives, the rotation of the measurement plane was introduced. When collecting the third and fourth frames (<inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>), the light source was rotated by 8&#x000b0;, causing the pixel points originally aligned in the same row or column to no longer overlap. At different spatial positions, the angles between the UV radiometer and the LEDs varied, allowing for the acquisition of more independent and non-interfering measurement points even within the same scanning area. This effectively increased the resolution of the data acquisition. Rotating the light source by 8&#x000b0; is equivalent to a &#x02212;8&#x000b0; rotation of the testing plane. During data visualization, the data collected for the third and fourth frames must be rotated and combined with the first two frames using a non-uniform interpolation method to reconstruct a new image <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The reconstructed image can achieve sub-micron resolution. The results are shown in <xref rid="sensors-25-01081-f005" ref-type="fig">Figure 5</xref>c. By stacking multiple frames of images, the integrity of the data was ensured, and the results are more closely aligned with the true distribution. The rotation matrix is provided in Equation (3), and the coordinate transformation formula is shown in Equation (4).<disp-formula id="FD3-sensors-25-01081"><label>(3)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mfenced></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mfenced></mml:mtd><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-01081"><label>(4)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mfenced></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mfenced></mml:mtd><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where (<inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) are the new coordinates after rotation, (<inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) are the original coordinates of each point, and (<inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) is the center of rotation.</p><p>To verify whether the repeated measurement results follow a Gaussian distribution, the mean of the three measured datasets was calculated, followed by interpolation to generate a 3D plot, as shown in <xref rid="sensors-25-01081-f005" ref-type="fig">Figure 5</xref>d. <xref rid="sensors-25-01081-f005" ref-type="fig">Figure 5</xref>d depicts the 3D data distribution, while <xref rid="sensors-25-01081-f005" ref-type="fig">Figure 5</xref>e shows the data distribution of a specific cross-section. Ideally, the irradiance density distribution should exhibit a Gaussian profile, as described by Equation (5).<disp-formula id="FD5-sensors-25-01081"><label>(5)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the peak intensity, x denotes the position, <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the mean, and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> refers to the standard deviation.</p><p>The cross-sectional data distribution was further analyzed by calculating its mean and standard deviation, with the corresponding formulas given in Equations (6) and (7). The resulting mean and standard deviation were 2.29 and 2.75, respectively. After calculating the mean and standard deviation of the sampled data, all measured values were found to lie within the &#x003bc;(x) &#x000b1; 3&#x003c3;(x) range. Considering that this range encompasses approximately 99.73% of random fluctuations under an assumed normal distribution, these results indicate that the data variability in this study fell within an acceptable range and thus demonstrates a high level of overall reliability. This finding not only confirms the conformity of the light source to the Lambertian model but also, from a statistical perspective, demonstrates the high repeatability of the measurement system.<disp-formula id="FD6-sensors-25-01081"><label>(6)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-01081"><label>(7)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msqrt><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The super-resolution results were analyzed using the Fourier method, as shown in <xref rid="sensors-25-01081-f005" ref-type="fig">Figure 5</xref>f. The spatial light intensity distribution exhibits a pronounced central symmetry and generally aligns with the characteristics of a Gaussian distribution. The intensity is highest at the center and gradually decreases as the spatial distance increases. This indicates a high degree of energy concentration in the light source, and the spatial distribution matches theoretical expectations. By performing multiple repeated measurements and averaging the results, the impact of random noise was significantly reduced, effectively suppressing background noise levels and accentuating the signals&#x02019; primary peak characteristics. The Fourier transform results indicate that the light intensity was predominantly concentrated in low-frequency components, further confirming the effectiveness and reliability of the light source testing apparatus. These findings serve as an important reference for evaluating light source performance and optimizing the experimental setup.</p><p>In addition, we compared the performance of different platforms, including resolution and key light source parameters, as shown in <xref rid="sensors-25-01081-t001" ref-type="table">Table 1</xref>. The comparison results indicate that the proposed platform is capable of measuring a more comprehensive set of light source characteristics, thus enabling a more thorough evaluation of the light source properties.</p><p>Based on the aforementioned analysis methods, in addition to the results of a single LED, we also provide the test results for the remaining 15 LEDs. The data are visualized and represented through images, thereby emphasizing the reliability and versatility of the light source testing system. <xref rid="sensors-25-01081-f006" ref-type="fig">Figure 6</xref> presents the spatial intensity distribution of 16 LEDs under grid sampling. <xref rid="sensors-25-01081-f007" ref-type="fig">Figure 7</xref> illustrates the spatial intensity distribution of 16 LEDs under sub-pixel sampling. <xref rid="sensors-25-01081-f008" ref-type="fig">Figure 8</xref> depicts the spatial intensity distribution of 16 LEDs obtained from angular measurements. <xref rid="sensors-25-01081-f009" ref-type="fig">Figure 9</xref> analyzes the super-resolution reconstruction results of 16 LEDs using the Fourier method.</p></sec><sec id="sec3dot2-sensors-25-01081"><title>3.2. Response Time Testing</title><p>When testing the response time, the receiver needed to be replaced with a UV photodetector, which was fixed within the range illuminated by the LED under test. The UV photodetector had a minimum response time of 2 &#x000b5;s and was connected to the KEYSIGHT DSOX1204A oscilloscope. Additionally, the LED under testing was set to blink at a frequency of <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> Hz, allowing the test results to be saved and analyzed via the oscilloscope. The parameters of the oscilloscope used during the measurement were as follows: 4 channels, a 600 MHz bandwidth, and a 5G Sa/s sampling rate.</p><p>The response time under investigation refers to the transient response of the light source, specifically the rising or falling edge. <xref rid="sensors-25-01081-f010" ref-type="fig">Figure 10</xref> presents the averaged rising edge results of 16 LEDs, where the horizontal axis represents the sampling points recorded by the oscilloscope, and the vertical axis indicates the averaged voltage levels of the 16 LEDs. A total of 165 points were identified within the voltage range of 0 V to 2 V. Given that the oscillator&#x02019;s sampling rate was 5 GSa/s, each point corresponds to 0.0002 &#x003bc;s. Therefore, the 165 points on the rising edge represent a rise time of 33 &#x003bc;s.</p><p>In addition, using this light source testing apparatus, it is possible to collect data on various spatial positions of an 8 &#x000d7; 8 UV array LED light source at an intensity level of 16. This yields a rich training dataset, which can be utilized for further improvements and practical applications of the light source.</p></sec></sec><sec sec-type="conclusions" id="sec4-sensors-25-01081"><title>4. Conclusions</title><p>A highly automated and precise light source testing platform has been developed to measure the key parameters of light sources, such as the spatial intensity distribution and temporal response. By employing advanced methodologies, such as sub-pixel displacement and rotation angle adjustments, combined with state-of-the-art techniques, like sub-pixel sampling and non-uniform interpolation, the platform effectively reconstructs low-resolution images into high-resolution representations. This process ensures the accurate characterization of critical light source properties, enabling the precise evaluation and analysis of their performance metrics.</p><p>The spatial intensity distribution results demonstrate that the light source exhibited Gaussian characteristics, with high energy concentration and strong theoretical consistency. The temporal response measurements reveal the transient time response of the light source, providing indispensable information for high-precision applications. Additionally, Fourier-based reconstruction improved the measurement resolution, while noise reduction through repeated measurements enhanced the accuracy, further validating the effectiveness and reliability of the light source testing platform. These advancements lay a solid foundation for the continued development of light source design and evaluation.</p><p>Furthermore, the platform can perform comprehensive spatial and temporal data acquisition for an array of UV LEDs across 16 intensity levels, thereby generating a rich training dataset that can provide valuable opportunities for light source optimization and broader applications.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Y.P. and Z.C.; methodology, Y.P., J.L. and Z.C.; experimental design and study, J.L.; software, J.L.; validation, J.L.; formal analysis, J.L., Y.P. and Y.Z.; literature investigation, J.L.; resources, W.K.; writing&#x02014;original draft preparation, J.L.; writing&#x02014;review and editing, Y.P., Y.Z. and Z.C.; visualization, J.L.; supervision, Y.Z. and Y.P.; project administration, Y.P. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available upon request from the corresponding author. The data are not publicly available due to privacy.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01081"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Muramoto</surname><given-names>Y.</given-names></name>
<name><surname>Kimura</surname><given-names>M.</given-names></name>
<name><surname>Nouda</surname><given-names>S.</given-names></name>
</person-group><article-title>Development and future of ultraviolet light-emitting diodes: UV-LED will replace the UV lamp</article-title><source>Semicond. Sci. Technol.</source><year>2014</year><volume>29</volume><fpage>084004</fpage><pub-id pub-id-type="doi">10.1088/0268-1242/29/8/084004</pub-id></element-citation></ref><ref id="B2-sensors-25-01081"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>L.</given-names></name>
<name><surname>Birr</surname><given-names>T.</given-names></name>
<name><surname>Zywietz</surname><given-names>U.</given-names></name>
<name><surname>Reinhardt</surname><given-names>C.</given-names></name>
<name><surname>Roth</surname><given-names>B.</given-names></name>
</person-group><article-title>Feature size below 100 nm realized by UVLED-based microscope projection photolithography</article-title><source>Light Adv. Manuf.</source><year>2023</year><volume>4</volume><fpage>410</fpage><lpage>419</lpage></element-citation></ref><ref id="B3-sensors-25-01081"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Erickstad</surname><given-names>M.</given-names></name>
<name><surname>Gutierrez</surname><given-names>E.</given-names></name>
<name><surname>Groisman</surname><given-names>A.</given-names></name>
</person-group><article-title>A low-cost low-maintenance ultraviolet lithography light source based on light-emitting diodes</article-title><source>Lab Chip</source><year>2015</year><volume>15</volume><fpage>57</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1039/C4LC00472H</pub-id><pub-id pub-id-type="pmid">25322205</pub-id>
</element-citation></ref><ref id="B4-sensors-25-01081"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shiba</surname><given-names>S.F.</given-names></name>
<name><surname>Jeon</surname><given-names>H.</given-names></name>
<name><surname>Kim</surname><given-names>J.-S.</given-names></name>
<name><surname>Kim</surname><given-names>J.-E.</given-names></name>
<name><surname>Kim</surname><given-names>J.</given-names></name>
</person-group><article-title>3D microlithography using an integrated system of 5-mm uv-leds with a tilt-rotational sample holder</article-title><source>Micromachines</source><year>2020</year><volume>11</volume><elocation-id>157</elocation-id><pub-id pub-id-type="doi">10.3390/mi11020157</pub-id><pub-id pub-id-type="pmid">32024035</pub-id>
</element-citation></ref><ref id="B5-sensors-25-01081"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mudunuri</surname><given-names>S.P.</given-names></name>
<name><surname>Biswas</surname><given-names>S.</given-names></name>
</person-group><article-title>Low resolution face recognition across variations in pose and illumination</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2015</year><volume>38</volume><fpage>1034</fpage><lpage>1040</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2015.2469282</pub-id></element-citation></ref><ref id="B6-sensors-25-01081"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yapici</surname><given-names>M.K.</given-names></name>
<name><surname>Farhat</surname><given-names>I.</given-names></name>
</person-group><article-title>UV-LED exposure system for low-cost photolithography</article-title><source>Opt. Microlithogr. XXVII</source><year>2014</year><volume>9052</volume><fpage>523</fpage><lpage>529</lpage></element-citation></ref><ref id="B7-sensors-25-01081"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>J.</given-names></name>
<name><surname>Yoon</surname><given-names>Y.-K.</given-names></name>
<name><surname>Allen</surname><given-names>M.G.</given-names></name>
</person-group><article-title>Computer numerical control (CNC) lithography: Light-motion synchronized UV-LED lithography for 3D microfabrication</article-title><source>J. Micromech. Microeng.</source><year>2016</year><volume>26</volume><fpage>035003</fpage><pub-id pub-id-type="doi">10.1088/0960-1317/26/3/035003</pub-id></element-citation></ref><ref id="B8-sensors-25-01081"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bing</surname><given-names>C.Y.</given-names></name>
<name><surname>Mohanan</surname><given-names>A.A.</given-names></name>
<name><surname>Saha</surname><given-names>T.</given-names></name>
<name><surname>Ramanan</surname><given-names>R.N.</given-names></name>
<name><surname>Parthiban</surname><given-names>R.</given-names></name>
<name><surname>Ramakrishnan</surname><given-names>N.</given-names></name>
</person-group><article-title>Microfabrication of surface acoustic wave device using UV LED photolithography technique</article-title><source>Microelectron. Eng.</source><year>2014</year><volume>122</volume><fpage>9</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/j.mee.2014.03.011</pub-id></element-citation></ref><ref id="B9-sensors-25-01081"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shiba</surname><given-names>S.F.</given-names></name>
<name><surname>Tan</surname><given-names>J.Y.</given-names></name>
<name><surname>Kim</surname><given-names>J.</given-names></name>
</person-group><article-title>Multidirectional UV-LED lithography using an array of high-intensity UV-LEDs and tilt-rotational sample holder for 3-D microfabrication</article-title><source>Micro Nano Syst. Lett.</source><year>2020</year><volume>8</volume><fpage>5</fpage><pub-id pub-id-type="doi">10.1186/s40486-020-00107-y</pub-id></element-citation></ref><ref id="B10-sensors-25-01081"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huntington</surname><given-names>M.D.</given-names></name>
<name><surname>Odom</surname><given-names>T.W.</given-names></name>
</person-group><article-title>A portable, benchtop photolithography system based on a solid-state light source</article-title><source>Small</source><year>2011</year><volume>7</volume><fpage>3144</fpage><lpage>3147</lpage><pub-id pub-id-type="doi">10.1002/smll.201101209</pub-id><pub-id pub-id-type="pmid">21901830</pub-id>
</element-citation></ref><ref id="B11-sensors-25-01081"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>L.</given-names></name>
<name><surname>Zywietz</surname><given-names>U.</given-names></name>
<name><surname>Birr</surname><given-names>T.</given-names></name>
<name><surname>Duderstadt</surname><given-names>M.</given-names></name>
<name><surname>Overmeyer</surname><given-names>L.</given-names></name>
<name><surname>Roth</surname><given-names>B.</given-names></name>
<name><surname>Reinhardt</surname><given-names>C.</given-names></name>
</person-group><article-title>UV-LED projection photolithography for high-resolution functional photonic components</article-title><source>Microsyst. Nanoeng.</source><year>2021</year><volume>7</volume><fpage>64</fpage><pub-id pub-id-type="doi">10.1038/s41378-021-00286-7</pub-id><pub-id pub-id-type="pmid">34567776</pub-id>
</element-citation></ref><ref id="B12-sensors-25-01081"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kang</surname><given-names>Y.H.</given-names></name>
<name><surname>Oh</surname><given-names>S.S.</given-names></name>
<name><surname>Kim</surname><given-names>Y.-S.</given-names></name>
<name><surname>Choi</surname><given-names>C.-G.</given-names></name>
</person-group><article-title>Fabrication of antireflection nanostructures by hybrid nano-patterning lithography</article-title><source>Microelectron. Eng.</source><year>2010</year><volume>87</volume><fpage>125</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1016/j.mee.2009.06.006</pub-id></element-citation></ref><ref id="B13-sensors-25-01081"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kontio</surname><given-names>J.M.</given-names></name>
<name><surname>Simonen</surname><given-names>J.</given-names></name>
<name><surname>Tommila</surname><given-names>J.</given-names></name>
<name><surname>Pessa</surname><given-names>M.</given-names></name>
</person-group><article-title>Arrays of metallic nanocones fabricated by UV-nanoimprint lithography</article-title><source>Microelectron. Eng.</source><year>2010</year><volume>87</volume><fpage>1711</fpage><lpage>1715</lpage><pub-id pub-id-type="doi">10.1016/j.mee.2009.08.015</pub-id></element-citation></ref><ref id="B14-sensors-25-01081"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Stuerzebecher</surname><given-names>L.</given-names></name>
<name><surname>Harzendorf</surname><given-names>T.</given-names></name>
<name><surname>Vogler</surname><given-names>U.</given-names></name>
<name><surname>Zeitner</surname><given-names>U.D.</given-names></name>
<name><surname>Voelkel</surname><given-names>R.</given-names></name>
</person-group><article-title>Advanced mask aligner lithography: Fabrication of periodic patterns using pinhole array mask and Talbot effect</article-title><source>Opt. Express</source><year>2010</year><volume>18</volume><fpage>19485</fpage><lpage>19494</lpage><pub-id pub-id-type="doi">10.1364/OE.18.019485</pub-id><pub-id pub-id-type="pmid">20940844</pub-id>
</element-citation></ref><ref id="B15-sensors-25-01081"><label>15.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Dreyer</surname><given-names>C.</given-names></name>
<name><surname>Mildner</surname><given-names>F.</given-names></name>
</person-group><article-title>Application of LEDs for UV-curing</article-title><source>III-Nitride Ultraviolet Emitters: Technology and Applications</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2016</year><fpage>415</fpage><lpage>434</lpage></element-citation></ref><ref id="B16-sensors-25-01081"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Shiba</surname><given-names>S.F.</given-names></name>
<name><surname>Beavers</surname><given-names>J.</given-names></name>
<name><surname>Laramore</surname><given-names>D.</given-names></name>
<name><surname>Lindstrom</surname><given-names>B.</given-names></name>
<name><surname>Brovles</surname><given-names>J.</given-names></name>
<name><surname>Gaither</surname><given-names>C.</given-names></name>
<name><surname>Hieber</surname><given-names>T.</given-names></name>
<name><surname>Kim</surname><given-names>J.</given-names></name>
</person-group><article-title>UV-LED lithography system and characterization</article-title><source>Proceedings of the 2020 IEEE 15th International Conference on Nano/Micro Engineered and Molecular System (NEMS)</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>27&#x02013;30 September 2020</conf-date><fpage>73</fpage><lpage>76</lpage></element-citation></ref><ref id="B17-sensors-25-01081"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zollner</surname><given-names>C.J.</given-names></name>
<name><surname>DenBaars</surname><given-names>S.</given-names></name>
<name><surname>Speck</surname><given-names>J.</given-names></name>
<name><surname>Nakamura</surname><given-names>S.</given-names></name>
</person-group><article-title>Germicidal ultraviolet LEDs: A review of applications and semiconductor technologies</article-title><source>Semicond. Sci. Technol.</source><year>2021</year><volume>36</volume><fpage>123001</fpage><pub-id pub-id-type="doi">10.1088/1361-6641/ac27e7</pub-id></element-citation></ref><ref id="B18-sensors-25-01081"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Ye</surname><given-names>X.</given-names></name>
<name><surname>Han</surname><given-names>Q.</given-names></name>
<name><surname>Qi</surname><given-names>F.</given-names></name>
<name><surname>Luo</surname><given-names>H.</given-names></name>
<name><surname>Shi</surname><given-names>H.</given-names></name>
<name><surname>Xiong</surname><given-names>W.</given-names></name>
</person-group><article-title>Research on calibration and data processing method of dynamic target monitoring spectrometer</article-title><source>Proceedings of the Second Symposium on Novel Technology of X-Ray Imaging</source><conf-loc>Hefei, China</conf-loc><conf-date>26&#x02013;28 November 2018</conf-date><fpage>632</fpage><lpage>637</lpage></element-citation></ref><ref id="B19-sensors-25-01081"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Xiong</surname><given-names>J.</given-names></name>
<name><surname>Hu</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>Q.</given-names></name>
</person-group><article-title>Implementation and uniformity calibration of LED array for photodynamic therapy</article-title><source>J. Innov. Opt. Health Sci.</source><year>2022</year><volume>15</volume><fpage>2240004</fpage><pub-id pub-id-type="doi">10.1142/S1793545822400041</pub-id></element-citation></ref><ref id="B20-sensors-25-01081"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Greenspan</surname><given-names>H.</given-names></name>
</person-group><article-title>Super-resolution in medical imaging</article-title><source>Comput. J.</source><year>2009</year><volume>52</volume><fpage>43</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1093/comjnl/bxm075</pub-id></element-citation></ref><ref id="B21-sensors-25-01081"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bai</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Ding</surname><given-names>M.</given-names></name>
<name><surname>Ghanem</surname><given-names>B.</given-names></name>
</person-group><article-title>Sod-mtgan: Small object detection via multi-task generative adversarial network</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#x02013;14 September 2018</conf-date><fpage>206</fpage><lpage>221</lpage></element-citation></ref><ref id="B22-sensors-25-01081"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lobanov</surname><given-names>A.P.</given-names></name>
</person-group><article-title>Resolution limits in astronomical images</article-title><source>arXiv</source><year>2005</year><pub-id pub-id-type="arxiv">astro-ph/0503225</pub-id></element-citation></ref><ref id="B23-sensors-25-01081"><label>23.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Lillesand</surname><given-names>T.</given-names></name>
<name><surname>Kiefer</surname><given-names>R.W.</given-names></name>
<name><surname>Chipman</surname><given-names>J.</given-names></name>
</person-group><source>Remote Sensing and Image Interpretation</source><publisher-name>John Wiley &#x00026; Sons</publisher-name><publisher-loc>Hoboken, NJ, USA</publisher-loc><year>2015</year></element-citation></ref><ref id="B24-sensors-25-01081"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Girshick</surname><given-names>R.</given-names></name>
<name><surname>Donahue</surname><given-names>J.</given-names></name>
<name><surname>Darrell</surname><given-names>T.</given-names></name>
<name><surname>Malik</surname><given-names>J.</given-names></name>
</person-group><article-title>Region-based convolutional networks for accurate object detection and segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2015</year><volume>38</volume><fpage>142</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2015.2437384</pub-id></element-citation></ref><ref id="B25-sensors-25-01081"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Swaminathan</surname><given-names>A.</given-names></name>
<name><surname>Wu</surname><given-names>M.</given-names></name>
<name><surname>Liu</surname><given-names>K.R.</given-names></name>
</person-group><article-title>Digital image forensics via intrinsic fingerprints</article-title><source>IEEE Trans. Inf. Forensics Secur.</source><year>2008</year><volume>3</volume><fpage>101</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1109/TIFS.2007.916010</pub-id></element-citation></ref><ref id="B26-sensors-25-01081"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bashir</surname><given-names>S.M.A.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Khan</surname><given-names>M.</given-names></name>
<name><surname>Niu</surname><given-names>Y.</given-names></name>
</person-group><article-title>A comprehensive review of deep learning-based single image super-resolution</article-title><source>PeerJ Comput. Sci.</source><year>2021</year><volume>7</volume><fpage>e621</fpage><pub-id pub-id-type="doi">10.7717/peerj-cs.621</pub-id><pub-id pub-id-type="pmid">34322592</pub-id>
</element-citation></ref><ref id="B27-sensors-25-01081"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>R.</given-names></name>
<name><surname>Yuan</surname><given-names>Y.</given-names></name>
<name><surname>Huang</surname><given-names>R.</given-names></name>
<name><surname>Luo</surname><given-names>J.</given-names></name>
</person-group><article-title>Video super-resolution with spatial-temporal transformer encoder</article-title><source>Proceedings of the 2022 IEEE International Conference on Multimedia and Expo (ICME)</source><conf-loc>Taipei, Taiwan</conf-loc><conf-date>18&#x02013;22 July 2022</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B28-sensors-25-01081"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>P.</given-names></name>
</person-group><article-title>Spatio-temporal fusion network for video super-resolution</article-title><source>Proceedings of the 2021 International Joint Conference on Neural Networks (IJCNN)</source><conf-loc>Shenzhen, China</conf-loc><conf-date>18&#x02013;22 July 2021</conf-date><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="B29-sensors-25-01081"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shah</surname><given-names>N.R.</given-names></name>
<name><surname>Zakhor</surname><given-names>A.</given-names></name>
</person-group><article-title>Resolution enhancement of color video sequences</article-title><source>IEEE Trans. Image Process.</source><year>1999</year><volume>8</volume><fpage>879</fpage><lpage>885</lpage><pub-id pub-id-type="doi">10.1109/83.766865</pub-id><pub-id pub-id-type="pmid">18267501</pub-id>
</element-citation></ref><ref id="B30-sensors-25-01081"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Thawakar</surname><given-names>O.</given-names></name>
<name><surname>Patil</surname><given-names>P.W.</given-names></name>
<name><surname>Dudhane</surname><given-names>A.</given-names></name>
<name><surname>Murala</surname><given-names>S.</given-names></name>
<name><surname>Kulkarni</surname><given-names>U.</given-names></name>
</person-group><article-title>Image and video super resolution using recurrent generative adversarial network</article-title><source>Proceedings of the 2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</source><conf-loc>Taipei, Taiwan</conf-loc><conf-date>18&#x02013;21 September 2019</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B31-sensors-25-01081"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Panchenko</surname><given-names>E.</given-names></name>
<name><surname>Wesemann</surname><given-names>L.</given-names></name>
<name><surname>G&#x000f3;mez</surname><given-names>D.E.</given-names></name>
<name><surname>James</surname><given-names>T.D.</given-names></name>
<name><surname>Davis</surname><given-names>T.J.</given-names></name>
<name><surname>Roberts</surname><given-names>A.</given-names></name>
</person-group><article-title>Ultracompact camera pixel with integrated plasmonic color filters</article-title><source>Adv. Opt. Mater.</source><year>2019</year><volume>7</volume><fpage>1900893</fpage><pub-id pub-id-type="doi">10.1002/adom.201900893</pub-id></element-citation></ref><ref id="B32-sensors-25-01081"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pejovi&#x00107;</surname><given-names>V.</given-names></name>
<name><surname>Lee</surname><given-names>J.</given-names></name>
<name><surname>Georgitzikis</surname><given-names>E.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Kim</surname><given-names>J.H.</given-names></name>
<name><surname>Lieberman</surname><given-names>I.</given-names></name>
<name><surname>Malinowski</surname><given-names>P.E.</given-names></name>
<name><surname>Heremans</surname><given-names>P.</given-names></name>
<name><surname>Cheyns</surname><given-names>D.</given-names></name>
</person-group><article-title>Thin-film photodetector optimization for high-performance short-wavelength infrared imaging</article-title><source>IEEE Electron Device Lett.</source><year>2021</year><volume>42</volume><fpage>1196</fpage><lpage>1199</lpage><pub-id pub-id-type="doi">10.1109/LED.2021.3093081</pub-id></element-citation></ref><ref id="B33-sensors-25-01081"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Morimoto</surname><given-names>K.</given-names></name>
<name><surname>Ardelean</surname><given-names>A.</given-names></name>
<name><surname>Wu</surname><given-names>M.-L.</given-names></name>
<name><surname>Ulku</surname><given-names>A.C.</given-names></name>
<name><surname>Antolovic</surname><given-names>I.M.</given-names></name>
<name><surname>Bruschini</surname><given-names>C.</given-names></name>
<name><surname>Charbon</surname><given-names>E.</given-names></name>
</person-group><article-title>A megapixel time-gated SPAD image sensor for 2D and 3D imaging applications</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1912.12910</pub-id><pub-id pub-id-type="doi">10.1364/OPTICA.386574</pub-id></element-citation></ref><ref id="B34-sensors-25-01081"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>S.</given-names></name>
<name><surname>Bose</surname><given-names>N.K.</given-names></name>
</person-group><article-title>Reconstruction of 2-D bandlimited discrete signals from nonuniform samples</article-title><source>IEE Proc. F (Radar Signal Process.)</source><year>1990</year><volume>137</volume><fpage>197</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1049/ip-f-2.1990.0030</pub-id></element-citation></ref><ref id="B35-sensors-25-01081"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rogalski</surname><given-names>A.</given-names></name>
<name><surname>Martyniuk</surname><given-names>P.</given-names></name>
<name><surname>Kopytko</surname><given-names>M.</given-names></name>
</person-group><article-title>Challenges of small-pixel infrared detectors: A review</article-title><source>Rep. Prog. Phys.</source><year>2016</year><volume>79</volume><fpage>046501</fpage><pub-id pub-id-type="doi">10.1088/0034-4885/79/4/046501</pub-id><pub-id pub-id-type="pmid">27007242</pub-id>
</element-citation></ref><ref id="B36-sensors-25-01081"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Komatsu</surname><given-names>T.</given-names></name>
<name><surname>Aizawa</surname><given-names>K.</given-names></name>
<name><surname>Igarashi</surname><given-names>T.</given-names></name>
<name><surname>Saito</surname><given-names>T.</given-names></name>
</person-group><article-title>Signal-processing based method for acquiring very high resolution images with multiple cameras and its theoretical analysis</article-title><source>IEE Proc. I (Commun. Speech Vis.)</source><year>1993</year><volume>140</volume><fpage>19</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1049/ip-i-2.1993.0005</pub-id></element-citation></ref><ref id="B37-sensors-25-01081"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yue</surname><given-names>L.</given-names></name>
<name><surname>Shen</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Yuan</surname><given-names>Q.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
</person-group><article-title>Image super-resolution: The techniques, applications, and future</article-title><source>Signal Process.</source><year>2016</year><volume>128</volume><fpage>389</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1016/j.sigpro.2016.05.002</pub-id></element-citation></ref><ref id="B38-sensors-25-01081"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>W.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Tian</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Xue</surname><given-names>J.-H.</given-names></name>
<name><surname>Liao</surname><given-names>Q.</given-names></name>
</person-group><article-title>Deep learning for single image super-resolution: A brief review</article-title><source>IEEE Trans. Multimed.</source><year>2019</year><volume>21</volume><fpage>3106</fpage><lpage>3121</lpage><pub-id pub-id-type="doi">10.1109/TMM.2019.2919431</pub-id></element-citation></ref><ref id="B39-sensors-25-01081"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gibson</surname><given-names>G.M.</given-names></name>
<name><surname>Johnson</surname><given-names>S.D.</given-names></name>
<name><surname>Padgett</surname><given-names>M.J.</given-names></name>
</person-group><article-title>Single-pixel imaging 12 years on: A review</article-title><source>Opt. Express</source><year>2020</year><volume>28</volume><fpage>28190</fpage><lpage>28208</lpage><pub-id pub-id-type="doi">10.1364/OE.403195</pub-id><pub-id pub-id-type="pmid">32988095</pub-id>
</element-citation></ref><ref id="B40-sensors-25-01081"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bishara</surname><given-names>W.</given-names></name>
<name><surname>Su</surname><given-names>T.-W.</given-names></name>
<name><surname>Coskun</surname><given-names>A.F.</given-names></name>
<name><surname>Ozcan</surname><given-names>A.</given-names></name>
</person-group><article-title>Lensfree on-chip microscopy over a wide field-of-view using pixel super-resolution</article-title><source>Opt. Express</source><year>2010</year><volume>18</volume><fpage>11181</fpage><lpage>11191</lpage><pub-id pub-id-type="doi">10.1364/OE.18.011181</pub-id><pub-id pub-id-type="pmid">20588977</pub-id>
</element-citation></ref><ref id="B41-sensors-25-01081"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>Q.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
<name><surname>Ding</surname><given-names>T.</given-names></name>
<name><surname>Zuo</surname><given-names>C.</given-names></name>
</person-group><article-title>The dynamic super-resolution phase imaging based on low-cost lensfree system</article-title><source>Proceedings of the Sixth International Conference on Optical and Photonic Engineering (icOPEN 2018)</source><conf-loc>Shanghai, China</conf-loc><conf-date>8&#x02013;11 May 2018</conf-date><fpage>314</fpage><lpage>318</lpage></element-citation></ref><ref id="B42-sensors-25-01081"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Clark</surname><given-names>J.</given-names></name>
<name><surname>Palmer</surname><given-names>M.</given-names></name>
<name><surname>Lawrence</surname><given-names>P.</given-names></name>
</person-group><article-title>A transformation method for the reconstruction of functions from nonuniformly spaced samples</article-title><source>IEEE Trans. Acoust. Speech Signal Process.</source><year>1985</year><volume>33</volume><fpage>1151</fpage><lpage>1165</lpage><pub-id pub-id-type="doi">10.1109/TASSP.1985.1164714</pub-id></element-citation></ref><ref id="B43-sensors-25-01081"><label>43.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Shukla</surname><given-names>A.</given-names></name>
<name><surname>Merugu</surname><given-names>S.</given-names></name>
<name><surname>Jain</surname><given-names>K.</given-names></name>
</person-group><article-title>A technical review on image super-resolution techniques</article-title><source>Advances in Cybernetics, Cognition, and Machine Learning for Communication Technologies</source><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2020</year><fpage>543</fpage><lpage>565</lpage></element-citation></ref><ref id="B44-sensors-25-01081"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Park</surname><given-names>S.C.</given-names></name>
<name><surname>Park</surname><given-names>M.K.</given-names></name>
<name><surname>Kang</surname><given-names>M.G.</given-names></name>
</person-group><article-title>Super-resolution image reconstruction: A technical overview</article-title><source>IEEE Signal Process. Mag.</source><year>2003</year><volume>20</volume><fpage>21</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1109/MSP.2003.1203207</pub-id></element-citation></ref><ref id="B45-sensors-25-01081"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ur</surname><given-names>H.</given-names></name>
<name><surname>Gross</surname><given-names>D.</given-names></name>
</person-group><article-title>Improved resolution from subpixel shifted pictures</article-title><source>CVGIP Graph. Models Image Process.</source><year>1992</year><volume>54</volume><fpage>181</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/1049-9652(92)90065-6</pub-id></element-citation></ref><ref id="B46-sensors-25-01081"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Papoulis</surname><given-names>A.</given-names></name>
</person-group><article-title>Generalized sampling expansion</article-title><source>IEEE Trans. Circuits Syst.</source><year>1977</year><volume>24</volume><fpage>652</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1109/TCS.1977.1084284</pub-id></element-citation></ref><ref id="B47-sensors-25-01081"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Brown</surname><given-names>J.</given-names></name>
</person-group><article-title>Multi-channel sampling of low-pass signals</article-title><source>IEEE Trans. Circuits Syst.</source><year>1981</year><volume>28</volume><fpage>101</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1109/TCS.1981.1084954</pub-id></element-citation></ref><ref id="B48-sensors-25-01081"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Landweber</surname><given-names>L.</given-names></name>
</person-group><article-title>An iteration formula for Fredholm integral equations of the first kind</article-title><source>Am. J. Math.</source><year>1951</year><volume>73</volume><fpage>615</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.2307/2372313</pub-id></element-citation></ref><ref id="B49-sensors-25-01081"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alam</surname><given-names>M.S.</given-names></name>
<name><surname>Bognar</surname><given-names>J.G.</given-names></name>
<name><surname>Hardie</surname><given-names>R.C.</given-names></name>
<name><surname>Yasuda</surname><given-names>B.J.</given-names></name>
</person-group><article-title>Infrared image registration and high-resolution reconstruction using multiple translationally shifted aliased video frames</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2000</year><volume>49</volume><fpage>915</fpage><lpage>923</lpage><pub-id pub-id-type="doi">10.1109/19.872908</pub-id></element-citation></ref><ref id="B50-sensors-25-01081"><label>50.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Nguyen</surname><given-names>N.</given-names></name>
<name><surname>Milanfar</surname><given-names>P.</given-names></name>
</person-group><article-title>An efficient wavelet-based algorithm for image superresolution</article-title><source>Proceedings of the 2000 International Conference on Image Processing (Cat. No. 00CH37101)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>10&#x02013;13 September 2000</conf-date><fpage>351</fpage><lpage>354</lpage></element-citation></ref><ref id="B51-sensors-25-01081"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Park</surname><given-names>M.K.</given-names></name>
<name><surname>Lee</surname><given-names>E.S.</given-names></name>
<name><surname>Park</surname><given-names>J.Y.</given-names></name>
<name><surname>Kang</surname><given-names>M.G.</given-names></name>
<name><surname>Kim</surname><given-names>J.</given-names></name>
</person-group><article-title>Discrete cosine transform based high-resolution image reconstruction considering the inaccurate subpixel motion information</article-title><source>Opt. Eng.</source><year>2002</year><volume>41</volume><fpage>370</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1117/1.1431552</pub-id></element-citation></ref><ref id="B52-sensors-25-01081"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ben-Ezra</surname><given-names>M.</given-names></name>
<name><surname>Zomet</surname><given-names>A.</given-names></name>
<name><surname>Nayar</surname><given-names>S.K.</given-names></name>
</person-group><article-title>Video super-resolution using controlled subpixel detector shifts</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2005</year><volume>27</volume><fpage>977</fpage><lpage>987</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2005.129</pub-id><pub-id pub-id-type="pmid">15943428</pub-id>
</element-citation></ref><ref id="B53-sensors-25-01081"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bishara</surname><given-names>W.</given-names></name>
<name><surname>Sikora</surname><given-names>U.</given-names></name>
<name><surname>Mudanyali</surname><given-names>O.</given-names></name>
<name><surname>Su</surname><given-names>T.-W.</given-names></name>
<name><surname>Yaglidere</surname><given-names>O.</given-names></name>
<name><surname>Luckhart</surname><given-names>S.</given-names></name>
<name><surname>Ozcan</surname><given-names>A.</given-names></name>
</person-group><article-title>Holographic pixel super-resolution in portable lensless on-chip microscopy using a fiber-optic array</article-title><source>Lab Chip</source><year>2011</year><volume>11</volume><fpage>1276</fpage><lpage>1279</lpage><pub-id pub-id-type="doi">10.1039/c0lc00684j</pub-id><pub-id pub-id-type="pmid">21365087</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01081-f001"><label>Figure 1</label><caption><p>Schematic of the light source testing setup.</p></caption><graphic xlink:href="sensors-25-01081-g001" position="float"/></fig><fig position="float" id="sensors-25-01081-f002"><label>Figure 2</label><caption><p>(<bold>a</bold>) Schematic of the UV LED array light source. (<bold>b</bold>) Polar luminous intensity distribution curve.</p></caption><graphic xlink:href="sensors-25-01081-g002" position="float"/></fig><fig position="float" id="sensors-25-01081-f003"><label>Figure 3</label><caption><p>Workflow of the light source control and driving algorithm. (<bold>a</bold>) Spatial intensity distribution measurement process. (<bold>b</bold>) Temporal response measurement process.</p></caption><graphic xlink:href="sensors-25-01081-g003" position="float"/></fig><fig position="float" id="sensors-25-01081-f004"><label>Figure 4</label><caption><p>Schematic diagram of measurement method. (<bold>a</bold>) Grid sampling. (<bold>b</bold>) Sub-pixel sampling. (<bold>c</bold>) Multi-frame superposition.</p></caption><graphic xlink:href="sensors-25-01081-g004" position="float"/></fig><fig position="float" id="sensors-25-01081-f005"><label>Figure 5</label><caption><p>Spatial intensity distribution results. (<bold>a</bold>) Grid sampling. (<bold>b</bold>) Sub-pixel sampling. (<bold>c</bold>) Multi-frame superposition. (<bold>d</bold>) Three-dimensional irradiance data distribution. (<bold>e</bold>) Irradiance data distribution of a specific cross-section. (<bold>f</bold>) Super-resolution reconstruction analyzed using the Fourier method.</p></caption><graphic xlink:href="sensors-25-01081-g005" position="float"/></fig><fig position="float" id="sensors-25-01081-f006"><label>Figure 6</label><caption><p>Grid sampling spatial intensity distribution results of 16 LEDs.</p></caption><graphic xlink:href="sensors-25-01081-g006" position="float"/></fig><fig position="float" id="sensors-25-01081-f007"><label>Figure 7</label><caption><p>Sub-pixel sampling spatial intensity distribution results of 16 LEDs.</p></caption><graphic xlink:href="sensors-25-01081-g007" position="float"/></fig><fig position="float" id="sensors-25-01081-f008"><label>Figure 8</label><caption><p>Multi-frame superposition spatial intensity distribution results of 16 LEDs.</p></caption><graphic xlink:href="sensors-25-01081-g008" position="float"/></fig><fig position="float" id="sensors-25-01081-f009"><label>Figure 9</label><caption><p>Super-resolution reconstruction results of 16 LEDs analyzed using the Fourier method.</p></caption><graphic xlink:href="sensors-25-01081-g009" position="float"/></fig><fig position="float" id="sensors-25-01081-f010"><label>Figure 10</label><caption><p>The rising edge of the response time waveform.</p></caption><graphic xlink:href="sensors-25-01081-g010" position="float"/></fig><table-wrap position="float" id="sensors-25-01081-t001"><object-id pub-id-type="pii">sensors-25-01081-t001_Table 1</object-id><label>Table 1</label><caption><p>The performance comparison among different measurement platforms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Resolution</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Spatial Intensity <break/>Distribution</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Response Time</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Subpixel detector shift [<xref rid="B52-sensors-25-01081" ref-type="bibr">52</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">~200 &#x003bc;m</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Holographic pixel super-resolution [<xref rid="B53-sensors-25-01081" ref-type="bibr">53</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;1 &#x003bc;m</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">This work</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0003c;1 &#x003bc;m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td></tr></tbody></table></table-wrap></floats-group></article>