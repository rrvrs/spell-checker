<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title-group><journal-title>bioRxiv</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39975030</article-id><article-id pub-id-type="pmc">PMC11838456</article-id>
<article-id pub-id-type="doi">10.1101/2025.01.31.634335</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Generating Correlated Data for Omics Simulation</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-2048-9398</contrib-id><name><surname>Yang</surname><given-names>Jianing</given-names></name><aff id="A1">Institute for Translational Medicine and Therapeutics, University of Pennsylvania</aff><aff id="A2">Chronobiology and Sleep Institute, University of Pennsylvania</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0139-7658</contrib-id><name><surname>Grant</surname><given-names>Gregory R.</given-names></name><aff id="A3">Institute for Translational Medicine and Therapeutics, University of Pennsylvania</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-6980-0079</contrib-id><name><surname>Brooks</surname><given-names>Thomas G.</given-names></name><aff id="A4">Institute for Translational Medicine and Therapeutics, University of Pennsylvania</aff></contrib></contrib-group><pub-date pub-type="epub"><day>06</day><month>2</month><year>2025</year></pub-date><elocation-id>2025.01.31.634335</elocation-id><permissions><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</ext-link>, which allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator. The license allows for commercial use.</license-p></license></permissions><self-uri content-type="pdf">nihpp-2025.01.31.634335.pdf</self-uri><abstract id="ABS1"><p id="P1">Simulation of realistic omics data is a key input for benchmarking studies that help users obtain optimal computational pipelines. Omics data involves large numbers of measured features on each samples and these measures are generally correlated with each other. However, simulation too often ignores these correlations, perhaps due to the inconvenience and computational hurdles of doing so. To alleviate this, we describe in detail three approaches for quickly generating omics-scale data with correlated measures which mimic real data sets. These approaches all are based on a Gaussian copula approach with a covariance matrix that decomposes into a diagonal part and a low-rank part. We use these approaches to demonstrate the importance of including correlation in two benchmarking applications. First, we show that variance of results from the popular DESeq2 method increases when dependence is included. Second, we demonstrate that CYCLOPS, a method for inferring circadian time of collection from transcriptomics, improves in performance when given gene-gene dependencies in some circumstances. We provide an R package, dependentsimr, that has efficient implementations of these methods and can generate dependent data with arbitrary distributions, including discrete (binary, ordered categorical, Poisson, negative binomial), continuous (normal), or with an empirical distribution.</p></abstract><funding-group><funding-statement>JY received funding from National Institute of Neurological Disorders and Stroke (5R01NS048471). TB and GG received funding support from the National Center for Advancing Translational Sciences Grant (5UL1TR000003). The funders had no role in this research, the decision to publish, or the preparation of this manuscript.</funding-statement></funding-group></article-meta></front><body><sec id="S1"><title>Introduction</title><p id="P2">Omics data typically has far fewer samples than measurements per sample. This creates dual challenges in generating realistic simulated data for the purposes of benchmarking. First, there isn&#x02019;t enough data to be able to compute a dependence structure (e.g., a full-rank correlation matrix). Second, generating omicsscale data with a specified correlation matrix is slow due to the typical <inline-formula><mml:math id="M1" display="inline"><mml:mi>O</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula> nature of these algorithms, where <inline-formula><mml:math id="M2" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of measurements per sample. Moreover, there is a lack of practical guidance on how to generate simulated data with realistic dependence. These often mean that simulators assume independence of the measurements, which does not reflect reality.</p><p id="P3">Here, we give an introduction to the theory and practice of generating dependent data and describe three related solutions which all offer good performance and ease-of-use even for large omics-scale problems. This expands off a discussion we originally wrote as part of a larger discussion on best practices in omics benchmarking (<xref rid="R6" ref-type="bibr">Brooks et al. 2024</xref>). Our goal here is to produce guidelines that show that generating correlated data does not have to be onerous and instead should be considered a baseline requirement when simulating data.</p><p id="P4">We present three methods that operate by inferring a covariance matrix that decomposes into a diagonal part and a low-rank part. Using a Gaussian copula (<xref rid="R11" ref-type="bibr">Nelsen 1998</xref>) approach (also referred to as NORTA, for &#x0201c;normal to anything&#x0201d; (<xref rid="R7" ref-type="bibr">Cario and Nelson 1997</xref>)), the marginal (univariate) distributions can have realistic forms. These solutions operate by taking a real dataset and mimicking it. For ease of use, we implement this in an R package which supports normal, Poisson, DESeq2-based (negative binomial with sample-specific size factors), and empirical (for ordinal data) marginal distributions.</p><p id="P5">We implemented three different strategies for determining the diagonal and low-rank parts of the covariance matrix. First, the &#x02018;PCA&#x02019; method uses principal component analysis (PCA) and picks the low-rank part such that the simulated data has the same variance in the top <inline-formula><mml:math id="M3" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> PCA components of the reference dataset. Second, the &#x02018;spiked Wishart&#x02019; method fits <inline-formula><mml:math id="M4" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> components such that simulations with the same number of samples as the reference dataset will have, on average, the same PCA component variances as the reference. Unlike &#x02018;PCA&#x02019;, these variances are computed with resepect to the simulated data&#x02019;s own PCA and not using the PCA weights of the reference dataset. Third, the &#x02018;corpcor&#x02019; method uses the popular R library <monospace>corpcor</monospace> (<xref rid="R13" ref-type="bibr">Sch&#x000e4;fer and Strimmer 2005</xref>; <xref rid="R12" ref-type="bibr">Opgen-Rhein and Strimmer 2007</xref>), which implements a James-Stein type shrinkage estimator for the covariance matrix as a linear interpolation of the sample covariance matrix and a diagonal matrix. No method exactly captures the input data, indicating room for future research, but all improve upon the common approach of assuming independence.</p><p id="P6">We show two applications which demonstrate the effects of including dependence of measurements in simulated data when benchmarking computational pipelines. First, we simulate RNA-seq data with differential expression between two conditions. Using DESeq2 to determine the differentially expressed genes, we found that dependence had little impact on the accuracy of reported <inline-formula><mml:math id="M5" display="inline"><mml:mi>p</mml:mi></mml:math></inline-formula>-values but increased the variance of those estimates. Second, we simulated a time series of RNA-seq data points and used the CYCLOPS method (<xref rid="R1" ref-type="bibr">Anafi et al. 2017</xref>) to infer collection time from the RNA-seq data, without time labels. Depending upon settings used, performance of CYCLOPS dependent substantially on the dependence structure of the data, and surprisingly showed worst performance when given data with independent genes.</p></sec><sec id="S2"><title>Results</title><p id="P7">Assume that we have a reference dataset <inline-formula><mml:math id="M6" display="inline"><mml:mi>X</mml:mi></mml:math></inline-formula> given by an <inline-formula><mml:math id="M7" display="inline"><mml:mi>p</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:math></inline-formula> data matrix of <inline-formula><mml:math id="M8" display="inline"><mml:mi>p</mml:mi></mml:math></inline-formula> features measured in each of <inline-formula><mml:math id="M9" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> independent samples. We want to capture correlations between the <inline-formula><mml:math id="M10" display="inline"><mml:mi>p</mml:mi></mml:math></inline-formula> features, which could represent gene expressions, protein abundances, or other measured values. We refer to these features as genes for simplicity. Our goal is to generate simulated data with the same <inline-formula><mml:math id="M11" display="inline"><mml:mi>p</mml:mi></mml:math></inline-formula> genes, the same marginal distributions of each gene as in <inline-formula><mml:math id="M12" display="inline"><mml:mi>X</mml:mi></mml:math></inline-formula> and realistic gene-gene dependence.</p><sec id="S3"><title>Multivariate normal distribution</title><p id="P8">We first discuss the simplest case, where our data set is multivariate normally distributed. The distribution <inline-formula><mml:math id="M13" display="inline"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> is the multivariate normal distribution with mean vector <inline-formula><mml:math id="M14" display="inline"><mml:mi>&#x003bc;</mml:mi></mml:math></inline-formula> and covariance matrix <inline-formula><mml:math id="M15" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula>. Here <inline-formula><mml:math id="M16" display="inline"><mml:mi>&#x003bc;</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="M17" display="inline"><mml:mi>p</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> column vector and <inline-formula><mml:math id="M18" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> is a <inline-formula><mml:math id="M19" display="inline"><mml:mi>p</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>p</mml:mi></mml:math></inline-formula> matrix. In order for this to work, <inline-formula><mml:math id="M20" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> must be symmetric and positive semi-definite, meaning that all of its eigenvalues are non-negative. This matrix is conceptually simple, since <inline-formula><mml:math id="M21" display="inline"><mml:msub><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> gives the the covariance of <inline-formula><mml:math id="M22" display="inline"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M23" display="inline"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> when <inline-formula><mml:math id="M24" display="inline"><mml:mi>x</mml:mi><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula>. More specifically, this is the population covariance matrix of <inline-formula><mml:math id="M25" display="inline"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula>, which does not generally equal the sample covariance matrix. Indeed, if <inline-formula><mml:math id="M26" display="inline"><mml:mi>X</mml:mi></mml:math></inline-formula> is <inline-formula><mml:math id="M27" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> samples from <inline-formula><mml:math id="M28" display="inline"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula>, then <inline-formula><mml:math id="M29" display="inline"><mml:mover accent="true"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo>&#x02254;</mml:mo><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>&#x0203e;</mml:mi></mml:mover><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>&#x0203e;</mml:mi></mml:mover><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> is the sample covariance matrix where <inline-formula><mml:math id="M30" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>&#x0203e;</mml:mi></mml:mover></mml:math></inline-formula> is the average of the <inline-formula><mml:math id="M31" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> samples. For large <inline-formula><mml:math id="M32" display="inline"><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.5em"/><mml:mover accent="true"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:math></inline-formula> will closely approximate <inline-formula><mml:math id="M33" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula>, but we care primarily about the situation where <inline-formula><mml:math id="M34" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> is small. In particular, <inline-formula><mml:math id="M35" display="inline"><mml:mover accent="true"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:math></inline-formula> is at most a rank <inline-formula><mml:math id="M36" display="inline"><mml:mi>n</mml:mi><mml:mspace width="0.5em"/><mml:mo>&#x02013;</mml:mo><mml:mspace width="0.5em"/><mml:mn>1</mml:mn></mml:math></inline-formula> matrix while <inline-formula><mml:math id="M37" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> could be up to rank <inline-formula><mml:math id="M38" display="inline"><mml:mi>p</mml:mi></mml:math></inline-formula>. This means that <inline-formula><mml:math id="M39" display="inline"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="M40" display="inline"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> are quite different distributions: every sample from <inline-formula><mml:math id="M41" display="inline"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:math></inline-formula> is contained in an <inline-formula><mml:math id="M42" display="inline"><mml:mi>n</mml:mi><mml:mspace width="0.5em"/><mml:mo>&#x02013;</mml:mo><mml:mspace width="0.5em"/><mml:mn>1</mml:mn></mml:math></inline-formula> dimensional plane. If <inline-formula><mml:math id="M43" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> is small, then this is very unlike real data, which typically is close to <inline-formula><mml:math id="M44" display="inline"><mml:mi>p</mml:mi></mml:math></inline-formula> dimensional.</p><p id="P9">The difficulty then is that we only know <inline-formula><mml:math id="M45" display="inline"><mml:mover accent="true"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:math></inline-formula> from our reference data set, but we need to choose a <inline-formula><mml:math id="M46" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> with which we can simulate data and the obvious choice of <inline-formula><mml:math id="M47" display="inline"><mml:mover accent="true"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:math></inline-formula> is inadequate. The most common choice is to assume <inline-formula><mml:math id="M48" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> is a diagonal matrix. This is the situation we want to avoid where the generated data is independent: <inline-formula><mml:math id="M49" display="inline"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M50" display="inline"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> have zero covariance unless <inline-formula><mml:math id="M51" display="inline"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:math></inline-formula>. However, this has some nice properties, such as being simple and fast to simulate (just generate univariate normal data for each variable).</p><p id="P10">We describe three alternative approaches in <xref rid="S8" ref-type="sec">Methods</xref> of how to choose a <inline-formula><mml:math id="M52" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> that will produce data similar to the input data. All three of these rely on a specific form of <inline-formula><mml:math id="M53" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula>, namely that
<disp-formula id="FD1">
<mml:math id="M54" display="block"><mml:mo>&#x02211;</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>
</disp-formula>
where <inline-formula><mml:math id="M55" display="inline"><mml:mi>D</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="M56" display="inline"><mml:mi>p</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>p</mml:mi></mml:math></inline-formula> diagonal matrix and <inline-formula><mml:math id="M57" display="inline"><mml:mi>P</mml:mi></mml:math></inline-formula> is <inline-formula><mml:math id="M58" display="inline"><mml:mi>p</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi></mml:math></inline-formula> for some <inline-formula><mml:math id="M59" display="inline"><mml:mi>k</mml:mi><mml:mo>&#x0226a;</mml:mo><mml:mi>p</mml:mi></mml:math></inline-formula>. This is a combination of an independent part (the diagonal matrix) and a low-rank part (<inline-formula><mml:math id="M60" display="inline"><mml:mi>P</mml:mi><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>). The low-rank part is also simple to generate data for. If <inline-formula><mml:math id="M61" display="inline"><mml:mi>x</mml:mi></mml:math></inline-formula> is a vector of <inline-formula><mml:math id="M62" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> independent univariate standard normal values, then <inline-formula><mml:math id="M63" display="inline"><mml:mi>P</mml:mi><mml:mi>x</mml:mi><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula>. The choice of specific <inline-formula><mml:math id="M64" display="inline"><mml:mi>D</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M65" display="inline"><mml:mi>P</mml:mi></mml:math></inline-formula> matrices is more in-depth and we leave the details for the <xref rid="S8" ref-type="sec">Methods</xref> section. We have three alternative means for doing so, which we refer to as the PCA, spiked Wishart, and corpcor methods.</p><p id="P11">Lastly, we emphasize that multivariate normal distributions do not capture all, or even most, types of possible dependence. Indeed, we see this even in the 2-dimensional case where it is well known that correlation describes only a linear relationship between two variables while in reality they may have much more complex relations. In higher dimensions, the problem is only worse. So any method based off multivariate normal distributions are making large assumptions about distribution. However, it is necessary to make some assumption like this. In the next section, though, we see that &#x0201c;normal&#x0201d; part is actually not a large obstacle.</p></sec><sec id="S4"><title>Gaussian copula</title><p id="P12">Building on the multivariate normal distribution, a popular approach to describe dependence in a highdimensional settings is called the Gaussian copula approach. The idea of this approach is that by normalizing and later reversing the normalization, data that does not fit a normal distribution can still have its dependence structure described using a multivariate normal distribution. This allows the marginal (i.e., univariate) distributions of each genes to be specified separately from the dependence between genes. This operates first by normalizing each gene by fitting a distribution (such as a normal distribution, Poisson, negative binomial, or other form), and then applying the fit cumulative distribution function (CDF) to the observed values. Finally, those are fed to a standard normal distribution&#x02019;s inverse CDF to obtain values that are approximately normally distributed. These values are then used to compute a covariance matrix <inline-formula><mml:math id="M66" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> and the data is assumed to follow a multivariate normal distribution in <inline-formula><mml:math id="M67" display="inline"><mml:mi>p</mml:mi></mml:math></inline-formula> dimensions with that covariance matrix.</p><p id="P13">Here, we describe the approach using the form of covariance matrix <inline-formula><mml:math id="M68" display="inline"><mml:mo>&#x02211;</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> as above. Once data is obtained <inline-formula><mml:math id="M69" display="inline"><mml:mi>Z</mml:mi><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula>, then one can undo the normalization process to obtain data with the same marginal distributions as the fit marginal distributions but with dependence determined by <inline-formula><mml:math id="M70" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula>. We describe this in detail:</p><list list-type="order" id="L2"><list-item><p id="P14">Fit marginal distributions to each feature in <inline-formula><mml:math id="M71" display="inline"><mml:mi>X</mml:mi></mml:math></inline-formula> to determine CDFs <inline-formula><mml:math id="M72" display="inline"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each feature.</p></list-item><list-item><p id="P15">Transform <inline-formula><mml:math id="M73" display="inline"><mml:mi>X</mml:mi></mml:math></inline-formula> to normalized values by <inline-formula><mml:math id="M74" display="inline"><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></inline-formula> where <inline-formula><mml:math id="M75" display="inline"><mml:mi>&#x003a6;</mml:mi></mml:math></inline-formula> is the CDF of the standard normal distribution.</p></list-item><list-item><p id="P16">Compute <inline-formula><mml:math id="M76" display="inline"><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.5em"/><mml:mi>U</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.5em"/><mml:mi>W</mml:mi></mml:math></inline-formula> matrices from <inline-formula><mml:math id="M77" display="inline"><mml:mi>X</mml:mi></mml:math></inline-formula> by one of three methods (see <xref rid="S8" ref-type="sec">Methods</xref>).</p></list-item><list-item><p id="P17">Generate <inline-formula><mml:math id="M78" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> i.i.d. standard normally distributed values <inline-formula><mml:math id="M79" display="inline"><mml:mi>u</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M80" display="inline"><mml:mi>p</mml:mi></mml:math></inline-formula> i.i.d standard normally distributed values <inline-formula><mml:math id="M81" display="inline"><mml:mi>v</mml:mi></mml:math></inline-formula>.</p></list-item><list-item><p id="P18">Set <inline-formula><mml:math id="M82" display="inline"><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mi>W</mml:mi><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mi>v</mml:mi></mml:math></inline-formula>.</p></list-item><list-item><p id="P19">Output the vector <inline-formula><mml:math id="M83" display="inline"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> where <inline-formula><mml:math id="M84" display="inline"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003a6;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></inline-formula>.</p></list-item></list><p id="P20">The generated data <inline-formula><mml:math id="M85" display="inline"><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> has covariance matrix <inline-formula><mml:math id="M86" display="inline"><mml:mo>&#x02211;</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>U</mml:mi><mml:mi>W</mml:mi><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, where <inline-formula><mml:math id="M87" display="inline"><mml:mi>P</mml:mi><mml:mo>&#x02254;</mml:mo><mml:mi>U</mml:mi><mml:msqrt><mml:mi>W</mml:mi></mml:msqrt></mml:math></inline-formula>. Moreover, we require that <inline-formula><mml:math id="M88" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> satisfies that <inline-formula><mml:math id="M89" display="inline"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02211;</mml:mo><mml:msubsup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is approximately 1. That guarantees that the output <inline-formula><mml:math id="M90" display="inline"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> has each entry with the same marginal distributions <inline-formula><mml:math id="M91" display="inline"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as was originally fit and inherits gene-gene dependence from <inline-formula><mml:math id="M92" display="inline"><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>. This method is computationally efficient, taking hardly any more time or memory than simulations without dependence.</p></sec><sec id="S5"><title>Comparison to real data</title><p id="P21">To compare the three simulation methods with a real data set, we chose as a references data set 12 mouse cortex RNA-seq samples from accession GSE151923 (<xref rid="R15" ref-type="bibr">Wang et al. 2022</xref>). We then simulated data mimicking this reference using all three simulation methods (PCA, spiked Wishart, and corpcor) as well as a simulation with independent genes. We repeated the simulations, each of 12 samples, a total of 8 times to estimate variance. For the PCA method, we used <inline-formula><mml:math id="M93" display="inline"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> dimensions and for the spiked Wishart, <inline-formula><mml:math id="M94" display="inline"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>11</mml:mn></mml:math></inline-formula>. The coprcor method always uses the full data matrix, analogous to <inline-formula><mml:math id="M95" display="inline"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>11</mml:mn></mml:math></inline-formula>. Note that PCA method must use a rank <inline-formula><mml:math id="M96" display="inline"><mml:mi>k</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>11</mml:mn></mml:math></inline-formula> in order to generate full-rank data, see <xref rid="S8" ref-type="sec">Methods</xref>, so these parameters are not directly comparable across methods.</p><p id="P22">Simulated data captures the genes&#x02019; mean and variance accurately (<xref rid="F1" ref-type="fig">Figure 1 a</xref>&#x02013;<xref rid="F1" ref-type="fig">b</xref>). Next, we compared to the real data set when projected onto the top two principal components of the real data set (<xref rid="F1" ref-type="fig">Figure 1 c</xref>). The simulations with dependence are distributed around the entire space like the real data, but the independent simulations have unrealistically low variance in these components, clustering tightly around the origin.</p><p id="P23">Then, we computed the gene-gene correlation on pairs of high-expressed genes (at least 300 mean reads). The simulation with independence showed the least levels of gene-gene correlations (<xref rid="F1" ref-type="fig">Figure 1 d</xref>). However, the PCA method overshot the reference data set and the spiked Wishart and corpcor methods only slightly improved upon the independent simulation.</p><p id="P24">Lastly, we compared the variances of principal components on each data set (<xref rid="F1" ref-type="fig">Figure 1 e</xref>). These were computed separately for each data set, unlike (<xref rid="F1" ref-type="fig">Figure 1 c</xref>) which used the reference data set&#x02019;s PCA weights for all data sets. The independent data has much lower variance than the real data set in the top four principal components. The spiked Wishart method comes closest to the real data set, as it optimizes for fitting these values. Surprisingly, the corpcor method performs only somewhat better than the independent method. The PCA method puts a large amount of variance into the first two components (due to using <inline-formula><mml:math id="M97" display="inline"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>) and then undershoots the other components.</p></sec><sec id="S6"><title>DESeq2 application</title><p id="P25">We benchmarked DESeq2 (<xref rid="R10" ref-type="bibr">Love, Huber, and Anders 2014</xref>), a popular differential expression analysis tool, using data sets simulated with dependence and ones simulated without dependence to compare its performances on both. DESeq2 presents an interesting case because several aspects of it assume independence of genes and so may be adversely affected by gene-gene dependence. First, the independent filtering step (<xref rid="R4" ref-type="bibr">Bourgon, Gentleman, and Huber 2010</xref>) assumes independence but has been reported to be robust to typical gene-gene dependence. Relatedly, the false discovery rate (FDR) (<xref rid="R3" ref-type="bibr">Benjamini and Hochberg 1995</xref>) allows only certain forms of dependence. Lastly, DESeq2&#x02019;s empirical Bayes steps could possibly be affected by gene dependence.</p><p id="P26">We used a fly whole body RNA-Seq data set GSE81142 and selected samples of male flies without treatment and after at least 2 hours of feeding to simulate 5 &#x0201c;control&#x0201d; samples. We then randomly selected 5% of the genes to be differentially expressed, with absolute <inline-formula><mml:math id="M98" display="inline"><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> fold change uniformly distributed between 0.2 and 2.0, either up or down regulated chosen randomly, and simulated 5 &#x0201c;experimental&#x0201d; samples. This was repeated 20 times for each of four dependence conditions (independent, PCA, Wishart, and corpcor).</p><p id="P27">Finally, we ran DESeq2 on each simulated 5 vs 5 experiment and compared the output FDR with the true percentages of genes that are differential expressed (<xref rid="F2" ref-type="fig">Figure 2 a</xref>&#x02013;<xref rid="F2" ref-type="fig">d</xref>). We observed that DESeq2 is anti-conservative on all data sets, with similar mean true FDRs for each estimated FDR cutoff. However, there was a greater variance in the performance of DESeq2 on the data sets simulated with dependence, indicating that it performs less consistently on data sets with gene-gene dependence, as in real data sets.</p><p id="P28">To demonstrate the application of our simulation method for another organism, we also simulated data sets using mouse cortex data set GSE151923 (<xref rid="R15" ref-type="bibr">Wang et al. 2022</xref>) and selected samples from male mice. We then simulated differential expression experiments as above and observed a similar result (<xref rid="F2" ref-type="fig">Figure 2e</xref>&#x02013;<xref rid="F2" ref-type="fig">f</xref>) as for the fly whole body data sets.</p></sec><sec id="S7"><title>CYCLOPS application</title><p id="P29">We next used our simulation method to benchmark CYCLOPS (<xref rid="R1" ref-type="bibr">Anafi et al. 2017</xref>), which infers relative times for a set of unlabeled samples using an autoencoder to identify circular structures. We chose a mouse cortex time series data set GSE151565, which contains a total of 77 samples every 3 hours, for 36 hours. We computed the dependence structure of the genes as well as the variances of marginal distributions using the 12 time point 0 samples and computed the means of gene expressions at each time point. We then used these to simulate 20 time series data sets for each of the independent, PCA, Wishart, and corpcor simulation methods.</p><p id="P30">We ran CYCLOPS on each data set with a list of cyclic mouse genes (from (<xref rid="R16" ref-type="bibr">Zhang et al. 2014</xref>), JTK p-value &#x0003c; 0.05), which yielded an estimated relative time for each sample. We evaluated CYCLOPS&#x02019; performance compared to true circadian time using the circular correlation (<xref rid="R8" ref-type="bibr">Fisher and Lee 1983</xref>), defined as follows:
<disp-formula id="FD2">
<mml:math id="M99" display="block"><mml:mi>&#x003c1;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mtext>sin</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace width="0.5em"/><mml:mtext>sin</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mtext>sin</mml:mtext><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mtext>sin</mml:mtext><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math>
</disp-formula>
where <inline-formula><mml:math id="M100" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of samples, <inline-formula><mml:math id="M101" display="inline"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M102" display="inline"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the true time and CYCLOPS-estimated time, respectively, for the <inline-formula><mml:math id="M103" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula>-th sample. <inline-formula><mml:math id="M104" display="inline"><mml:mi>&#x003c1;</mml:mi></mml:math></inline-formula> has value between &#x02212;1 and 1, and a <inline-formula><mml:math id="M105" display="inline"><mml:mo>|</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:mo>|</mml:mo></mml:math></inline-formula> close to 1 indicates accurate predictions by CYCLOPS.</p><p id="P31">By default, CYCLOPS performs dimension reduction so that each dimension (called an &#x0201c;eigengene&#x0201d;) contains at least 3% of the total variance. We found that CYCLOPS performance depended significantly on this parameter, with the default producing good performance across all simulations. However, when dropping CYCLOPS to require just 2% variance in each eigengene, we found that its performance depends significantly on the dependence structure of the simulated time series data (<xref rid="F3" ref-type="fig">Figure 3 a</xref>). At that setting, CYCLOPS performance is much higher in the PCA method and moderately improved in Wishart method, compared to the independent method. This difference is likely driven by the difference in the number of eigengenes used (<xref rid="F3" ref-type="fig">Figure 3 b</xref>), which is a measure of how much dependence is present in the data set. This demonstrates that the correlational structure of the transcriptome can have a major impact on performance.</p></sec></sec><sec id="S8"><title>Methods</title><p id="P32">Below, we describe the three methods for selecting the components of the covariance matrix <inline-formula><mml:math id="M106" display="inline"><mml:mo>&#x02211;</mml:mo><mml:mo>&#x02254;</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>.</p><sec id="S9"><title>PCA method</title><p id="P33">The first of our three methods attempts to match the top <inline-formula><mml:math id="M107" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> PCA components of the normalized reference dataset <inline-formula><mml:math id="M108" display="inline"><mml:mi>Z</mml:mi></mml:math></inline-formula>. Specifically, let <inline-formula><mml:math id="M109" display="inline"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> be the left singular vectors of <inline-formula><mml:math id="M110" display="inline"><mml:mi>Z</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="M111" display="inline"><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the corresponding top <inline-formula><mml:math id="M112" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> signular values. This method computes <inline-formula><mml:math id="M113" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> such that <inline-formula><mml:math id="M114" display="inline"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, i.e. that the variance in the direction of <inline-formula><mml:math id="M115" display="inline"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> exactly matches of the reference dataset&#x02019;s variance in that same direction. One solution is to use the sample covariance matrix, but that is not full rank and would match for all <inline-formula><mml:math id="M116" display="inline"><mml:mi>i</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>n</mml:mi></mml:math></inline-formula> instead of just <inline-formula><mml:math id="M117" display="inline"><mml:mi>i</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>k</mml:mi></mml:math></inline-formula>. Instead, we use the following:
<list list-type="order" id="L4"><list-item><p id="P34">Compute <inline-formula><mml:math id="M118" display="inline"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b4;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mo>&#x02113;</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:msubsup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02113;</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02113;</mml:mo><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="M119" display="inline"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mo>&#x02113;</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:msubsup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02113;</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02113;</mml:mo><mml:mo>&#x02113;</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> where <inline-formula><mml:math id="M120" display="inline"><mml:msub><mml:mrow><mml:mi>&#x003b4;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the Kronecker delta and <inline-formula><mml:math id="M121" display="inline"><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>Z</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> is the covariance matrix of <inline-formula><mml:math id="M122" display="inline"><mml:mi>Z</mml:mi></mml:math></inline-formula>.</p></list-item><list-item><p id="P35">Solve <inline-formula><mml:math id="M123" display="inline"><mml:mi>A</mml:mi><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mi>B</mml:mi></mml:math></inline-formula> and set <inline-formula><mml:math id="M124" display="inline"><mml:mi>W</mml:mi></mml:math></inline-formula> to be the diagonal matrix with <inline-formula><mml:math id="M125" display="inline"><mml:mi>w</mml:mi></mml:math></inline-formula> along its diagonal.</p></list-item><list-item><p id="P36">Set <inline-formula><mml:math id="M126" display="inline"><mml:mi>U</mml:mi></mml:math></inline-formula> to be the <inline-formula><mml:math id="M127" display="inline"><mml:mi>p</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi></mml:math></inline-formula> matrix with columns <inline-formula><mml:math id="M128" display="inline"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></list-item><list-item><p id="P37">Set <inline-formula><mml:math id="M129" display="inline"><mml:mi>D</mml:mi></mml:math></inline-formula> to be the diagonal matrix with <inline-formula><mml:math id="M130" display="inline"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>U</mml:mi><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:math></inline-formula>, which is the remaining variance.</p></list-item></list>
Steps 1 and 2 give that <inline-formula><mml:math id="M131" display="inline"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02211;</mml:mo><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for <inline-formula><mml:math id="M132" display="inline"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:math></inline-formula>. Step 4 ensures that <inline-formula><mml:math id="M133" display="inline"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02211;</mml:mo><mml:msubsup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> for <inline-formula><mml:math id="M134" display="inline"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:math></inline-formula>.</p></sec><sec id="S10"><title>Spiked Wishart method</title><p id="P39">The second method also makes use of PCA but has a different objective. If <inline-formula><mml:math id="M135" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> samples are drawn from <inline-formula><mml:math id="M136" display="inline"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> then we want the variances of their PCA components to match those of the reference dataset. Specifically, let <inline-formula><mml:math id="M137" display="inline"><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> be the <inline-formula><mml:math id="M138" display="inline"><mml:mi>n</mml:mi><mml:mspace width="0.5em"/><mml:mo>&#x02013;</mml:mo><mml:mspace width="0.5em"/><mml:mn>1</mml:mn></mml:math></inline-formula> non-zero singular values of <inline-formula><mml:math id="M139" display="inline"><mml:mi>Z</mml:mi><mml:mspace width="0.5em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is always approximately zero due to the normalization procedure), and let <inline-formula><mml:math id="M140" display="inline"><mml:msubsup><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> be the singular values of <inline-formula><mml:math id="M141" display="inline"><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> where <inline-formula><mml:math id="M142" display="inline"><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> has <inline-formula><mml:math id="M143" display="inline"><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> columns each iid <inline-formula><mml:math id="M144" display="inline"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula>. Then we want to choose <inline-formula><mml:math id="M145" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> such that <inline-formula><mml:math id="M146" display="inline"><mml:mi>E</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each <inline-formula><mml:math id="M147" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="M148" display="inline"><mml:mi>E</mml:mi><mml:mo>[</mml:mo><mml:mi>Y</mml:mi><mml:mo>]</mml:mo></mml:math></inline-formula> denotes the expectation of the random variable <inline-formula><mml:math id="M149" display="inline"><mml:mi>Y</mml:mi></mml:math></inline-formula>.</p><p id="P40">Since the distribution of the <inline-formula><mml:math id="M150" display="inline"><mml:msubsup><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> does not have a known analytic solution, we approximate this situation with the spiked Wishart distribution. The rank <inline-formula><mml:math id="M151" display="inline"><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> Wishart distribution is that of the sample covariance matrix of <inline-formula><mml:math id="M152" display="inline"><mml:mi>Y</mml:mi></mml:math></inline-formula> where the <inline-formula><mml:math id="M153" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> columns of <inline-formula><mml:math id="M154" display="inline"><mml:mi>Y</mml:mi></mml:math></inline-formula> are iid <inline-formula><mml:math id="M155" display="inline"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula>. The spiked Wishart is the special case where <inline-formula><mml:math id="M156" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> has <inline-formula><mml:math id="M157" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> arbitrary eigenvalues and the remaining are all equal to a constant. Note that the singular values of <inline-formula><mml:math id="M158" display="inline"><mml:mi>Y</mml:mi></mml:math></inline-formula> are the square roots of the eigenvalues of its sample covariance matrix. While our case has <inline-formula><mml:math id="M159" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> non-diagonal, <inline-formula><mml:math id="M160" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> may be diagonalized by orthogonal rotations due the sepctral theorem, and orthogonal rotations do not change the singular values of <inline-formula><mml:math id="M161" display="inline"><mml:mi>Y</mml:mi></mml:math></inline-formula>. Therefore, the distribution of singular values is not affected by the assumption that <inline-formula><mml:math id="M162" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> is diagonal. Moreover, for the form <inline-formula><mml:math id="M163" display="inline"><mml:mo>&#x02211;</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>U</mml:mi><mml:mi>W</mml:mi><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> where <inline-formula><mml:math id="M164" display="inline"><mml:mi>p</mml:mi></mml:math></inline-formula> is very large, each column of <inline-formula><mml:math id="M165" display="inline"><mml:mi>U</mml:mi></mml:math></inline-formula> is typically very close to orthogonal to any <inline-formula><mml:math id="M166" display="inline"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, a standard basis vector. Therefore, when <inline-formula><mml:math id="M167" display="inline"><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>I</mml:mi></mml:math></inline-formula> for some constant <inline-formula><mml:math id="M168" display="inline"><mml:mi>c</mml:mi></mml:math></inline-formula>, we can approximate <inline-formula><mml:math id="M169" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> as having <inline-formula><mml:math id="M170" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> arbitrary eigenvalues from <inline-formula><mml:math id="M171" display="inline"><mml:mi>W</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M172" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> remaining eigenvalues all equal to <inline-formula><mml:math id="M173" display="inline"><mml:mi>c</mml:mi></mml:math></inline-formula> corresponding to <inline-formula><mml:math id="M174" display="inline"><mml:mi>D</mml:mi></mml:math></inline-formula>. This is a spiked Wishart distribution.</p><p id="P41">However, the spiked Wishart distribution also has no known analytic solution for the distribution of its eigenvalues either. Therefore, we use an efficient sampling and stochastic gradient descent method that we recently described (<xref rid="R5" ref-type="bibr">Brooks 2024</xref>). Since the dataset has been normalized, <inline-formula><mml:math id="M175" display="inline"><mml:mi>c</mml:mi></mml:math></inline-formula> will be close to one and <inline-formula><mml:math id="M176" display="inline"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02211;</mml:mo><mml:msubsup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02248;</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>.</p><p id="P42">Specifically, we do:</p><list list-type="order" id="L6"><list-item><p id="P43">Set <inline-formula><mml:math id="M177" display="inline"><mml:mi>U</mml:mi></mml:math></inline-formula> to be the <inline-formula><mml:math id="M178" display="inline"><mml:mi>p</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi></mml:math></inline-formula> matrix with columns <inline-formula><mml:math id="M179" display="inline"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the left singular vectors of <inline-formula><mml:math id="M180" display="inline"><mml:mi>Z</mml:mi></mml:math></inline-formula>.</p></list-item><list-item><p id="P44">Compute <inline-formula><mml:math id="M181" display="inline"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M182" display="inline"><mml:mi>c</mml:mi></mml:math></inline-formula> by stochastic gradient descent minimizing <inline-formula><mml:math id="M183" display="inline"><mml:mrow><mml:munder><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>E</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> for <inline-formula><mml:math id="M184" display="inline"><mml:mo>&#x02211;</mml:mo></mml:math></inline-formula> diagonal with entries <inline-formula><mml:math id="M185" display="inline"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> (<xref rid="R5" ref-type="bibr">Brooks 2024</xref>).</p></list-item><list-item><p id="P45">Set <inline-formula><mml:math id="M186" display="inline"><mml:mi>W</mml:mi></mml:math></inline-formula> diagonal with the entries <inline-formula><mml:math id="M187" display="inline"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></list-item><list-item><p id="P46">Set <inline-formula><mml:math id="M188" display="inline"><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>I</mml:mi></mml:math></inline-formula></p></list-item></list></sec><sec id="S11"><title>Corpcor method</title><p id="P47">The <monospace>corpcor</monospace> package (<xref rid="R13" ref-type="bibr">Sch&#x000e4;fer and Strimmer 2005</xref>; <xref rid="R12" ref-type="bibr">Opgen-Rhein and Strimmer 2007</xref>) computes a JamesStein type shrinkage estimator for the covariance matrix. For large <inline-formula><mml:math id="M189" display="inline"><mml:mi>p</mml:mi></mml:math></inline-formula>, this greatly improves the estimate of the covariance matrix by introducing a little bias towards zero correlations and equal variances of genes. It computes optimal values of <inline-formula><mml:math id="M190" display="inline"><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="M191" display="inline"><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, its two regularization coefficients. It then uses <inline-formula><mml:math id="M192" display="inline"><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to linearly interpolate the sample covariance matrix towards the identity matrix <inline-formula><mml:math id="M193" display="inline"><mml:mi>I</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M194" display="inline"><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to interpolate the vector of variances towards the median variance value. Since the sample covariance matrix is rank at most <inline-formula><mml:math id="M195" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula>, we again obtain a matrix of the form <inline-formula><mml:math id="M196" display="inline"><mml:mo>&#x02211;</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>U</mml:mi><mml:mi>W</mml:mi><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>.</p><p id="P48">This algorithm is:</p><list list-type="order" id="L8"><list-item><p id="P49">Compute the <inline-formula><mml:math id="M197" display="inline"><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M198" display="inline"><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> values from <monospace>corpcor::estimate.lambda</monospace> and <monospace>corpcor::estimate.lambda.var</monospace> functions on <inline-formula><mml:math id="M199" display="inline"><mml:mi>Z</mml:mi></mml:math></inline-formula>, respectively.</p></list-item><list-item><p id="P50">Set <inline-formula><mml:math id="M200" display="inline"><mml:mi>D</mml:mi></mml:math></inline-formula> to be diagonal with <inline-formula><mml:math id="M201" display="inline"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:msqrt><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">med</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> where <inline-formula><mml:math id="M202" display="inline"><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the standard deviation of the <inline-formula><mml:math id="M203" display="inline"><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. and <inline-formula><mml:math id="M204" display="inline"><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the median of the <inline-formula><mml:math id="M205" display="inline"><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></list-item><list-item><p id="P51">Set <inline-formula><mml:math id="M206" display="inline"><mml:mi>U</mml:mi></mml:math></inline-formula> to be <inline-formula><mml:math id="M207" display="inline"><mml:msqrt><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>&#x003bb;</mml:mi></mml:msqrt><mml:mi>S</mml:mi><mml:mi>Z</mml:mi><mml:mo>/</mml:mo><mml:msqrt><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:msqrt></mml:math></inline-formula> where <inline-formula><mml:math id="M208" display="inline"><mml:mi>S</mml:mi></mml:math></inline-formula> is the diagonal matrix with <inline-formula><mml:math id="M209" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula>.</p></list-item><list-item><p id="P52">Set <inline-formula><mml:math id="M210" display="inline"><mml:mi>W</mml:mi></mml:math></inline-formula> to the identity.</p></list-item></list></sec></sec><sec id="S12"><title>Discussion</title><p id="P53">We described the well-known Gaussian copula approach and recommended a specific form of covariance matrix which is well tailored to omics data simulation. We developed three methods using this form of covariance matrix which can be used to mimic a reference data set for simulation. All of these methods use a multivariate normal distribution as an intermediate step and therefore substantially restrict the kinds of dependence that can be simulated. However, when operating in a high-dimensional space some simplification is likely required.</p><p id="P54">To encourage adoption of dependence in simulated omics data, we developed <monospace>dependentsimr</monospace>, an R package that generates omics-scale data with realistic correlation. This implementation is efficient and simple, requiring just two lines of code to fit a model to a reference data set and then simulate data from it. We demonstrated this package on RNA-seq data, using the DESeq2 method to fit negative binomial marginal distributions. However, this package is actually quite general and supports normal, Poisson, negative binomial, and arbitrary ordered discrete distributions using the empirical CDF. Moreover, it can support multi-modal data such as is increasingly common in multi-omics.</p><p id="P55">We demonstrated the importance of including gene-gene dependence in simulated data by two application benchmarks. In the first, DESeq2 results were substantially more variable when simulating with gene-gene dependence. In the second, CYCLOPS performance in estimating circadian phases depends upon genegene dependence, was sensitive to dependence structure of the data.</p><p id="P56">Our comparisons to a real dataset show that none of our three methods are able to exactly capture all aspects of the real dataset. In particular, the gene-gene correlations were too high in the PCA method and too low in the spiked Wishart and corpcor methods. Surprisingly, the spiked Wishart and corpcor methods improved in this metric only slightly compared to the simulations with independent genes. These observations demonstrate that there is room for future improvements over independent data in these techniques, possibly incorporating more recent developments in copulae (<xref rid="R9" ref-type="bibr">Gr&#x000f6;&#x000df;er and Okhrin 2022</xref>). Possibly, this could demonstrate the limitations of methods based on the multivariate normal distribution or of the low-rank approximation used by all three of our methods. Nonetheless, these methods represent significant improvements by other metrics and we recommend the inclusion of some dependence in nearly every simulated omics dataset.</p><sec id="S13"><title>Alternatives</title><p id="P57">We highlight some alternative approaches and software packages that have been taken below.</p><p id="P58">The R package SPsimSeq (<xref rid="R2" ref-type="bibr">Assefa, Vandesompele, and Thas 2020</xref>) provides a dedicated RNA-seq and singlecell RNA-seq simulator using a Gaussian copula approach to simulate gene dependence. In contrast to this package, it uses WGCNA to determine the correlation matrix, which is a gene network approach. However, this method takes significant computational resources. Indeed, the <monospace>SPsimseq</monospace> paper generated data for just 5000 genes based on a randomly sampled 5000 gene subset of the RNA-seq data and our attempts to use <monospace>SPsimseq</monospace> to generate a full sample exhausted the memory of a 24GB computer. In contrast, our method runs in seconds to generate a 40,000 gene samples on the same computer. <monospace>SPsimseq</monospace> is more specialized and full-featured for RNA-seq simulation, providing, for example, native differential expression (DE) options. In comparison, our <monospace>dependentsimr</monospace> package requires manually setting marginal expression values to inject DE, but also supports other marginal distributions for situations outside of RNA-seq.</p><p id="P59">The <underline>scDesign2</underline> simulator (<xref rid="R14" ref-type="bibr">Sun et al. 2021</xref>) for single-cell RNA-seq also uses Gaussian copula and, like our method, uses the approach of estimating the correlation matrix from the normalized dataset. However, it limits this correlation matrix to top-expressed genes. Since correlation is most discernible in high-expressed genes, this approach is reasonable but requires making certain arbitrary cutoffs that our methods avoid.</p><p id="P60">Other Gaussian copula-based R packages that may be applicable, at least for datasets with smaller numbers of features, include <monospace>bindata</monospace>, <monospace>Genord</monospace>, and <monospace>SimMultiCorrData</monospace>, the last of these being the most comprehensive. The <monospace>bigsimr</monospace> package provides faster implementations of these methods to scale up to omics-level data. However, even this is computationally demanding; their paper references generating 20,000-dimensional vectors in &#x0201c;under an hour&#x0201d; using 16 threads. The <monospace>copula</monospace> package provides even more flexible dependence options through use of copulas. All of these packages provide more flexibility in specifying dependence than our package, which can only mimic existing datasets, and therefore the longer run-times may be unavoidable for use cases where researchers need to parameterize the dependence structure.</p></sec></sec></body><back><ack id="S15"><title>Funding statement and competing interests</title><p id="P62">JY received funding from National Institute of Neurological Disorders and Stroke (5R01NS048471). TB and GG received funding support from the National Center for Advancing Translational Sciences Grant (5UL1TR000003). The funders had no role in this research, the decision to publish, or the preparation of this manuscript.</p></ack><fn-group><fn fn-type="COI-statement" id="FN1"><p id="P63">The authors declare no competing interests.</p></fn></fn-group><sec sec-type="data-availability" id="S14"><title>Data availability</title><p id="P61">Source code for all simulations and figures in this plot is available at <ext-link xlink:href="http://github.com/itmat/dependent_sim_paper/" ext-link-type="uri">github.com/itmat/dependent_sim_paper/</ext-link>. Source code for the <monospace>dependentsimr</monospace> package is available at <ext-link xlink:href="http://github.com/tgbrooks/dependent_sim" ext-link-type="uri">github.com/tgbrooks/dependent_sim</ext-link>. All data used is available with accession numbers GSE151923, GSE81142, GSE151565.</p></sec><ref-list><title>References</title><ref id="R1"><mixed-citation publication-type="journal"><name><surname>Anafi</surname><given-names>Ron C</given-names></name>, <name><surname>Francey</surname><given-names>Lauren J</given-names></name>, <name><surname>Hogenesch</surname><given-names>John B</given-names></name>, and <name><surname>Kim</surname><given-names>Junhyong</given-names></name>. <year>2017</year>. &#x0201c;<article-title>CYCLOPS Reveals Human Transcriptional Rhythms in Health and Disease</article-title>.&#x0201d; <source>Proc. Natl. Acad. Sci. U. S. A.</source>
<volume>114</volume> (<issue>20</issue>): <fpage>5312</fpage>&#x02013;<lpage>17</lpage>.<pub-id pub-id-type="pmid">28439010</pub-id>
</mixed-citation></ref><ref id="R2"><mixed-citation publication-type="journal"><name><surname>Assefa</surname><given-names>Alemu Takele</given-names></name>, <name><surname>Vandesompele</surname><given-names>Jo</given-names></name>, and <name><surname>Thas</surname><given-names>Olivier</given-names></name>. <year>2020</year>. &#x0201c;<article-title>SPsimSeq: Semi-Parametric Simulation of Bulk and Single-Cell RNA-sequencing Data</article-title>.&#x0201d; <source>Bioinformatics</source>
<volume>36</volume> (<issue>10</issue>): <fpage>3276</fpage>&#x02013;<lpage>78</lpage>.<pub-id pub-id-type="pmid">32065619</pub-id>
</mixed-citation></ref><ref id="R3"><mixed-citation publication-type="journal"><name><surname>Benjamini</surname><given-names>Yoav</given-names></name>, and <name><surname>Hochberg</surname><given-names>Yosef</given-names></name>. <year>1995</year>. &#x0201c;<article-title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing</article-title>.&#x0201d; <source>Journal of the Royal Statistical Society. Series B (Methodological)</source>
<volume>57</volume> (<issue>1</issue>): <fpage>289</fpage>&#x02013;<lpage>300</lpage>. <comment><ext-link xlink:href="http://www.jstor.org/stable/2346101" ext-link-type="uri">http://www.jstor.org/stable/2346101</ext-link>.</comment></mixed-citation></ref><ref id="R4"><mixed-citation publication-type="journal"><name><surname>Bourgon</surname><given-names>Richard</given-names></name>, <name><surname>Gentleman</surname><given-names>Robert</given-names></name>, and <name><surname>Huber</surname><given-names>Wolfgang</given-names></name>. <year>2010</year>. &#x0201c;<article-title>Independent Filtering Increases Detection Power for High-Throughput Experiments</article-title>.&#x0201d; <source>Proc. Natl. Acad. Sci. U. S. A.</source>
<volume>107</volume> (<issue>21</issue>): <fpage>9546</fpage>&#x02013;<lpage>51</lpage>.<pub-id pub-id-type="pmid">20460310</pub-id>
</mixed-citation></ref><ref id="R5"><mixed-citation publication-type="webpage"><name><surname>Brooks</surname><given-names>Thomas G.</given-names></name>
<year>2024</year>. &#x0201c;<source>Sampling Spiked Wishart Eigenvalues</source>.&#x0201d; <comment><ext-link xlink:href="https://arxiv.org/abs/2410.05280" ext-link-type="uri">https://arxiv.org/abs/2410.05280</ext-link>.</comment></mixed-citation></ref><ref id="R6"><mixed-citation publication-type="journal"><name><surname>Brooks</surname><given-names>Thomas G.</given-names></name>, <name><surname>Lahens</surname><given-names>Nicholas F.</given-names></name>, <name><surname>Mr&#x0010d;ela</surname><given-names>Antonijo</given-names></name>, and <name><surname>Grant</surname><given-names>Gregory R.</given-names></name>. <year>2024</year>. &#x0201c;<article-title>Challenges and Best Practices in Omics Benchmarking</article-title>.&#x0201d; <source>Nature Reviews Genetics</source>
<volume>25</volume> (<issue>5</issue>): <fpage>326</fpage>&#x02013;<lpage>39</lpage>. <pub-id pub-id-type="doi">10.1038/s41576-023-00679-6</pub-id>.</mixed-citation></ref><ref id="R7"><mixed-citation publication-type="other"><name><surname>Cario</surname><given-names>Marne C.</given-names></name>, and <name><surname>Nelson</surname><given-names>Barry L.</given-names></name>. <year>1997</year>. &#x0201c;<source>Modeling and Generating Random Vectors with Arbitrary Marginal Distributions and Correlation Matrix</source>.&#x0201d;</mixed-citation></ref><ref id="R8"><mixed-citation publication-type="journal"><name><surname>Fisher</surname><given-names>N. I.</given-names></name>, and <name><surname>Lee</surname><given-names>A. J.</given-names></name>. <year>1983</year>. &#x0201c;<article-title>A correlation coefficient for circular data</article-title>.&#x0201d; <source>Biometrika</source>
<volume>70</volume> (<issue>2</issue>): <fpage>327</fpage>&#x02013;<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1093/biomet/70.2.327</pub-id>.</mixed-citation></ref><ref id="R9"><mixed-citation publication-type="journal"><name><surname>Gr&#x000f6;&#x000df;er</surname><given-names>Joshua</given-names></name>, and <name><surname>Okhrin</surname><given-names>Ostap</given-names></name>. <year>2022</year>. &#x0201c;<article-title>Copulae: An Overview and Recent Developments</article-title>.&#x0201d; <source>WIREs Computational Statistics</source>
<volume>14</volume> (<issue>3</issue>): <fpage>e1557</fpage>. <comment>https://doi.org/</comment><pub-id pub-id-type="doi">10.1002/wics.1557</pub-id>.</mixed-citation></ref><ref id="R10"><mixed-citation publication-type="journal"><name><surname>Love</surname><given-names>Michael I</given-names></name>, <name><surname>Huber</surname><given-names>Wolfgang</given-names></name>, and <name><surname>Anders</surname><given-names>Simon</given-names></name>. <year>2014</year>. &#x0201c;<article-title>Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2</article-title>.&#x0201d; <source>Genome Biol</source>. <volume>15</volume> (<issue>12</issue>): <fpage>550</fpage>.<pub-id pub-id-type="pmid">25516281</pub-id>
</mixed-citation></ref><ref id="R11"><mixed-citation publication-type="book"><name><surname>Nelsen</surname><given-names>Roger</given-names></name>. <year>1998</year>. <part-title>An Introduction to Copulas</part-title>. <source>Lecture Notes in Statistics</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>.</mixed-citation></ref><ref id="R12"><mixed-citation publication-type="journal"><name><surname>Opgen-Rhein</surname><given-names>Rainer</given-names></name>, and <name><surname>Strimmer</surname><given-names>Korbinian</given-names></name>. <year>2007</year>. <source>Statistical Applications in Genetics and Molecular Biology</source>
<volume>6</volume> (<issue>1</issue>). <comment>https://doi.org/</comment>doi:<pub-id pub-id-type="doi">10.2202/1544-6115.1252</pub-id>.</mixed-citation></ref><ref id="R13"><mixed-citation publication-type="journal"><name><surname>Sch&#x000e4;fer</surname><given-names>Juliane</given-names></name>, and <name><surname>Strimmer</surname><given-names>Korbinian</given-names></name>. <year>2005</year>. <source>Statistical Applications in Genetics and Molecular Biology</source>
<volume>4</volume> (<issue>1</issue>). <comment>https://doi.org/</comment>doi:<pub-id pub-id-type="doi">10.2202/1544-6115.1175</pub-id>.</mixed-citation></ref><ref id="R14"><mixed-citation publication-type="journal"><name><surname>Sun</surname><given-names>Tianyi</given-names></name>, <name><surname>Song</surname><given-names>Dongyuan</given-names></name>, <name><surname>Li</surname><given-names>Wei Vivian</given-names></name>, and <name><surname>Li</surname><given-names>Jingyi Jessica</given-names></name>. <year>2021</year>. &#x0201c;<article-title>scDesign2: A Transparent Simulator That Generates High-Fidelity Single-Cell Gene Expression Count Data with Gene Correlations Captured</article-title>.&#x0201d; <source>Genome Biol</source>. <volume>22</volume> (<issue>1</issue>): <fpage>163</fpage>.<pub-id pub-id-type="pmid">34034771</pub-id>
</mixed-citation></ref><ref id="R15"><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Nan</given-names></name>, <name><surname>Langfelder</surname><given-names>Peter</given-names></name>, <name><surname>Stricos</surname><given-names>Matthew</given-names></name>, <name><surname>Ramanathan</surname><given-names>Lalini</given-names></name>, <name><surname>Richman</surname><given-names>Jeffrey B</given-names></name>, <name><surname>Vaca</surname><given-names>Raymond</given-names></name>, <name><surname>Plascencia</surname><given-names>Mary</given-names></name>, <etal/>
<year>2022</year>. &#x0201c;<article-title>Mapping Brain Gene Coexpression in Daytime Transcriptomes Unveils Diurnal Molecular Networks and Deciphers Perturbation Gene Signatures</article-title>.&#x0201d; <source>Neuron</source>
<volume>110</volume> (<issue>20</issue>): <fpage>3318</fpage>&#x02013;<lpage>3338.e9</lpage>.<pub-id pub-id-type="pmid">36265442</pub-id>
</mixed-citation></ref><ref id="R16"><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Ray</given-names></name>, <name><surname>Lahens</surname><given-names>Nicholas F</given-names></name>, <name><surname>Ballance</surname><given-names>Heather I</given-names></name>, <name><surname>Hughes</surname><given-names>Michael E</given-names></name>, and <name><surname>Hogenesch</surname><given-names>John B</given-names></name>. <year>2014</year>. &#x0201c;<article-title>A Circadian Gene Expression Atlas in Mammals: Implications for Biology and Medicine</article-title>.&#x0201d; <source>Proc. Natl. Acad. Sci. U. S. A.</source>
<volume>111</volume> (<issue>45</issue>): <fpage>16219</fpage>&#x02013;<lpage>24</lpage>.<pub-id pub-id-type="pmid">25349387</pub-id>
</mixed-citation></ref></ref-list></back><floats-group><fig position="float" id="F1"><label>Figure 1:</label><caption><p id="P64">Comparison to real data run on a mouse cortex data set from GSE151923. (a-b) Comparison of gene (a) mean expression and (b) variance, log-scaled in real and PCA simulated data. The line of equality is marked in black. Points are colored according to the density of points in their region. Wishart and corpcor methods give similar results (not shown). (c) Quantile-quantile plot comparing correlation values of gene pairs from real data and simulated data (both with and without dependence). Genes with at least 300 reads were used. Values on the diagonal line indicate a match between the simulated and real data sets. (d) Projections onto the top two principal components of the real data set for both real and simulated data. All 8 simulations (96 samples for each simulation) shown. (e) Principal component analysis was performed on all data sets and the variance captured by the top components is shown. Unlike (d), these components were fit from each data set considered separately instead of reusing the weights from the real data.</p></caption><graphic xlink:href="nihpp-2025.01.31.634335v1-f0001" position="float"/></fig><fig position="float" id="F2"><label>Figure 2:</label><caption><p id="P65">Performance of DESeq2 on simulated datasets. (a-d) Comparison of true false discovery proportions and DESeq2 reported False Discovery Rates, plotted on a log scale, for data sets simulated from the fly whole body data set (GSE81142), (a) without dependence, (b) using PCA, (c) using Wishart and (d) using corpcor. Diagonal line represents perfect estimation of FDR. (e-h) Comparison of true false discovery proportions and DESeq2 reported FDR for data sets simulated from the mouse cortex data set (GSE151923), (e) without dependence, (f) using PCA, (g) using Wishart and (h) using corpcor.</p></caption><graphic xlink:href="nihpp-2025.01.31.634335v1-f0002" position="float"/></fig><fig position="float" id="F3"><label>Figure 3:</label><caption><p id="P66">Performance of CYCLOPS on simulated time series data sets based on mouse cortex data set (GSE151565), using eigengenes of at least 2% variance. (a) Absolute circular correlations between true phases and CYCLOPS estimated phases on the simulated data sets. (b) The number of eigengenes used by CYCLOPS; the dotted line indicates the number of eigengenes used by CYCLOPS on the real data (5). (c-f) Examples of CYCLOPS estimated phases on the simulated data sets. CYCLOPS shows good performance when it separates out points by color (true circadian time).</p></caption><graphic xlink:href="nihpp-2025.01.31.634335v1-f0003" position="float"/></fig></floats-group></article>