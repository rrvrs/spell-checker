<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="review-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006322</article-id><article-id pub-id-type="pmc">PMC11859923</article-id><article-id pub-id-type="doi">10.3390/s25041093</article-id><article-id pub-id-type="publisher-id">sensors-25-01093</article-id><article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group></article-categories><title-group><article-title>Recent Advances in Deep Learning-Based Spatiotemporal Fusion Methods for Remote Sensing Images</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lian</surname><given-names>Zilong</given-names></name><xref rid="af1-sensors-25-01093" ref-type="aff">1</xref><xref rid="af2-sensors-25-01093" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5771-4168</contrib-id><name><surname>Zhan</surname><given-names>Yulin</given-names></name><xref rid="af1-sensors-25-01093" ref-type="aff">1</xref><xref rid="c1-sensors-25-01093" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8762-1878</contrib-id><name><surname>Zhang</surname><given-names>Wenhao</given-names></name><xref rid="af2-sensors-25-01093" ref-type="aff">2</xref><xref rid="af3-sensors-25-01093" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Zhangjie</given-names></name><xref rid="af1-sensors-25-01093" ref-type="aff">1</xref><xref rid="af2-sensors-25-01093" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Wenbo</given-names></name><xref rid="af2-sensors-25-01093" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Xuhan</given-names></name><xref rid="af1-sensors-25-01093" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Dobrini&#x00107;</surname><given-names>Dino</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Ga&#x00161;parovi&#x00107;</surname><given-names>Mateo</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01093"><label>1</label>Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100094, China; <email>kingsleylin@stumail.nciae.edu.cn</email> (Z.L.); <email>wangzhangjie12@163.com</email> (Z.W.); <email>huangxuhan24@mails.ucas.ac.cn</email> (X.H.)</aff><aff id="af2-sensors-25-01093"><label>2</label>School of Remote Sensing and Information Engineering, North China Institute of Aerospace Engineering, Langfang 065000, China; <email>zhangwh@radi.ac.cn</email> (W.Z.); <email>liu2728210141@163.com</email> (W.L.)</aff><aff id="af3-sensors-25-01093"><label>3</label>Hebei Collaborative Innovation Center for Aerospace Remote Sensing Information Processing and Application, Langfang 065000, China</aff><author-notes><corresp id="c1-sensors-25-01093"><label>*</label>Correspondence: <email>zhanyl@radi.ac.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1093</elocation-id><history><date date-type="received"><day>13</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>07</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>10</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Remote sensing images captured by satellites play a critical role in Earth observation (EO). With the advancement of satellite technology, the number and variety of remote sensing satellites have increased, which provide abundant data for precise environmental monitoring and effective resource management. However, existing satellite imagery often faces a trade-off between spatial and temporal resolutions. It is challenging for a single satellite to simultaneously capture images with high spatial and temporal resolutions. Consequently, spatiotemporal fusion techniques, which integrate images from different sensors, have garnered significant attention. Over the past decade, research on spatiotemporal fusion has achieved remarkable progress. Nevertheless, traditional fusion methods often encounter difficulties when dealing with complicated fusion scenarios. With the development of computer science, deep learning models, such as convolutional neural networks (CNNs), generative adversarial networks (GANs), Transformers, and diffusion models, have recently been introduced into the field of spatiotemporal fusion, resulting in efficient and accurate algorithms. These algorithms exhibit various strengths and limitations, which require further analysis and comparison. Therefore, this paper reviews the literature on deep learning-based spatiotemporal fusion methods, analyzes and compares existing deep learning-based fusion algorithms, summarizes current challenges in this field, and proposes possible directions for future studies.</p></abstract><kwd-group><kwd>multi-sensor data fusion</kwd><kwd>deep learning</kwd><kwd>remote sensing images</kwd><kwd>temporal resolution</kwd><kwd>spatial resolution</kwd></kwd-group><funding-group><award-group><funding-source>Common Application Support Platform for National Civil Space Infrastructure Land Observation Satellites</funding-source><award-id>2017-000052-73-01-001735</award-id></award-group><award-group><funding-source>Major Project of the High-Resolution Earth Observation System</funding-source><award-id>30-Y60B01-9003-22/23</award-id></award-group><award-group><funding-source>North China Institute of Aerospace Engineering Foundation for Doctoral Research</funding-source><award-id>YKY-2024-87</award-id></award-group><funding-statement>This research was funded by the Common Application Support Platform for National Civil Space Infrastructure Land Observation Satellites, grant number 2017-000052-73-01-001735; the Major Project of the High-Resolution Earth Observation System, grant number 30-Y60B01-9003-22/23; and the North China Institute of Aerospace Engineering Foundation for Doctoral Research, grant number YKY-2024-87.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01093"><title>1. Introduction</title><p>With the development of remote sensing technology, remote sensing images captured by satellites have been widely used in fields such as agriculture [<xref rid="B1-sensors-25-01093" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01093" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01093" ref-type="bibr">3</xref>], ecology [<xref rid="B4-sensors-25-01093" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01093" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01093" ref-type="bibr">6</xref>], and Earth surface observation [<xref rid="B7-sensors-25-01093" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01093" ref-type="bibr">8</xref>]. However, current satellite images still fall short of meeting the need for high-resolution observations in dense time series and large areas. Due to constraints in sensor technology and the cost of satellite launches, there is a trade-off between the temporal and spatial resolutions of remote sensing satellites, which makes it difficult for satellites to achieve both high spatial and temporal resolutions at the same time. Additionally, restricted by swath width and weather conditions, it is challenging for high-spatial-resolution satellites to capture seamless images in large areas. Therefore, an economical and effective approach is to perform spatiotemporal fusion (STF) for remote sensing images to obtain images with both high spatial and high temporal resolutions. Spatiotemporal fusion is a technique that integrates high-spatial-resolution but low-temporal-resolution images with high-temporal-resolution but low-spatial-resolution images to create synthetic images with both high spatial and high temporal resolutions.</p><p>Over the past decade, traditional spatiotemporal fusion research has made significant progress, and a series of traditional fusion methods have been developed (<xref rid="sensors-25-01093-f001" ref-type="fig">Figure 1</xref>). According to fusion mechanisms, traditional spatiotemporal fusion methods can be classified into unmixing-based, weight function-based, Bayesian-based, learning-based, and hybrid methods [<xref rid="B9-sensors-25-01093" ref-type="bibr">9</xref>]. Each type of method is based on different principles. For example, unmixing-based methods perform spatiotemporal fusion using the linear spectral mixing theory [<xref rid="B10-sensors-25-01093" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01093" ref-type="bibr">11</xref>]. Weight function-based methods establish a relationship between high-resolution and low-resolution pixels through weight functions [<xref rid="B12-sensors-25-01093" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01093" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01093" ref-type="bibr">14</xref>]. Bayesian methods regard spatiotemporal fusion as a maximum a posteriori problem [<xref rid="B15-sensors-25-01093" ref-type="bibr">15</xref>]. Learning-based methods construct spatial and temporal relationships using machine learning algorithms [<xref rid="B16-sensors-25-01093" ref-type="bibr">16</xref>]. Hybrid methods combine multiple traditional methods to improve fusion accuracy [<xref rid="B17-sensors-25-01093" ref-type="bibr">17</xref>]. Despite the distinct characteristics of these methods, they still face obstacles in practical applications. Traditional methods often rely on prior knowledge for model construction and require specific adjustments when applied to different regions. These limitations have hindered the further development and broader adoption of traditional methods.</p><p>Therefore, to address the challenges associated with traditional fusion methods, deep learning techniques have been introduced into the field of spatiotemporal fusion. Compared to traditional methods, deep learning models excel in automated feature extraction, nonlinear modeling, and model generalization. These advantages endow deep learning models with significant potential for improving fusion performance. In the early stages, deep learning-based fusion algorithms mainly employed simple backpropagation networks [<xref rid="B18-sensors-25-01093" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-01093" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01093" ref-type="bibr">20</xref>] to improve traditional methods. By utilizing multi-layer network structures and nonlinear activation functions, these networks automatically extracted features and modeled nonlinear relationships between images. However, simple backpropagation networks could not further improve fusion results in complex scenarios. Consequently, more advanced models, such as convolutional neural networks (CNNs) [<xref rid="B21-sensors-25-01093" ref-type="bibr">21</xref>], generative adversarial networks (GANs) [<xref rid="B22-sensors-25-01093" ref-type="bibr">22</xref>], Transformers [<xref rid="B23-sensors-25-01093" ref-type="bibr">23</xref>], and diffusion models [<xref rid="B24-sensors-25-01093" ref-type="bibr">24</xref>], have gradually been applied in the field of spatiotemporal fusion. Compared to traditional methods, deep learning-based models demonstrate higher accuracy and efficiency in handling diverse fusion scenarios and have progressively become mainstream methods in spatiotemporal fusion (<xref rid="sensors-25-01093-f002" ref-type="fig">Figure 2</xref>).</p><p>Despite the significant advantages and potential of deep learning methods in spatiotemporal fusion, current review studies predominantly focus on traditional methods [<xref rid="B9-sensors-25-01093" ref-type="bibr">9</xref>,<xref rid="B25-sensors-25-01093" ref-type="bibr">25</xref>], while systematic reviews of deep learning approaches remain relatively scarce. This lack of comprehensive reviews has hindered the further development of deep learning-based spatiotemporal fusion methods. In response, this paper conducts a comprehensive survey of existing deep learning-based fusion methods, provides an in-depth analysis of various deep learning-based approaches, and thoroughly reviews the research progress and current status of these methods. Then, a quantitative analysis of the evaluation and application of deep learning-based fusion methods is carried out based on specific methods and examples. By categorizing and comparing different methods, this paper identifies the key challenges currently faced in the field of deep learning-based spatiotemporal fusion and offers perspectives on future research directions. These analyses and summaries aim to serve as references for subsequent research on spatiotemporal fusion, fostering further exploration and development of deep learning in this field.</p></sec><sec id="sec2-sensors-25-01093"><title>2. Deep Learning-Based Spatiotemporal Fusion Methods</title><p>Before the advent of deep learning methods, traditional spatiotemporal fusion approaches faced problems such as complexity and difficulty in model design and application. For instance, constrained by linear spectral mixing theory, unmixing-based methods lack variability within coarse pixels and require prior classification, greatly limiting their applicability. Weight function-based methods rely heavily on prior knowledge in model design, resulting in reduced stability. Bayesian methods are computationally intensive, making them inefficient when capturing large-scale or high-resolution images. Learning-based methods rely heavily on complex hand-crafted features and, therefore, exhibit poor stability. Hybrid methods increase computational complexity and the difficulty of parameter tuning, often resulting in error propagation. These limitations have significantly constrained the performance and broader adoption of traditional spatiotemporal fusion methods.</p><p>To address the challenges in traditional spatiotemporal fusion, deep learning techniques have been introduced. Based on neural network architectures, these methods fall into four categories: convolutional neural network (CNN)-based, generative adversarial network (GAN)-based, Transformer-based, and diffusion-based methods. Each category leverages unique principles to enhance the accuracy and applicability of spatiotemporal fusion. CNNs improve fusion performance by handling image details and extracting features through local receptive fields and weight-sharing mechanisms. GANs generate high-quality images from limited data through adversarial learning between generators and discriminators, enhancing applicability. Transformers, with self-attention mechanisms, boost efficiency and accuracy in handling long temporal sequences and global spatial relationships. Diffusion models use diffusion and denoising processes to produce realistic, stable images. The adoption of deep learning techniques overcomes the limitations of traditional fusion methods and broadens their applicability in complex scenarios. This section categorizes and summarizes CNN-based, GAN-based, Transformer-based, and diffusion-based methods (<xref rid="sensors-25-01093-f003" ref-type="fig">Figure 3</xref>), providing an in-depth analysis of their strengths and weaknesses.</p><sec id="sec2dot1-sensors-25-01093"><title>2.1. CNN-Based Fusion Methods</title><p>Originally proposed by [<xref rid="B30-sensors-25-01093" ref-type="bibr">30</xref>], convolutional neural networks are designed for feature extraction. CNNs are a type of deep learning architecture that is particularly suitable for image-related tasks. Using convolutional layers, a CNN can extract local features from input images. The core advantage of a CNN is that it can automatically learn image features without manual feature extraction. In spatiotemporal fusion, CNNs effectively address the limitations of traditional methods in three areas: (1) Traditional methods rely on hand-crafted features, making it difficult to capture complex spatiotemporal characteristics, while CNN-based methods improve feature extraction efficiency and accuracy through automated learning. (2) Traditional methods use linear or simple nonlinear models that struggle to represent intricate spatial and temporal relationships, whereas CNNs, with multiple convolutional layers and nonlinear activation functions, provide strong nonlinear modeling capabilities. (3) Traditional methods suffer from low computational efficiency when processing large-scale data, whereas CNN-based methods excel in handling such data through parallel computation, thus enhancing spatiotemporal fusion performance.</p><p>Compared to traditional fusion methods, CNN-based methods offer clear advantages in feature extraction, fusion accuracy, and processing efficiency. Subsequent studies have further enhanced CNNs by incorporating residual connections and attention mechanisms, resulting in even greater performance. For example, residual blocks allow for deeper network layers to extract more complex features [<xref rid="B31-sensors-25-01093" ref-type="bibr">31</xref>], and attention mechanisms reduce feature redundancy, improving computational efficiency [<xref rid="B32-sensors-25-01093" ref-type="bibr">32</xref>]. Therefore, the CNN-based spatiotemporal fusion methods summarized in this paper are categorized into conventional CNN methods, residual-based CNN methods, and attention-based CNN methods, as shown in <xref rid="sensors-25-01093-t001" ref-type="table">Table 1</xref>.</p><sec id="sec2dot1dot1-sensors-25-01093"><title>2.1.1. Conventional CNN Methods</title><p>Conventional CNN methods use the basic CNN structure to perform fusion tasks. By processing each layer (<xref rid="sensors-25-01093-f004" ref-type="fig">Figure 4</xref>), these methods automatically capture local features from input images. The input layer receives the original image data as pixel values for each band. Convolutional layers then extract local features like edges, lines, and textures. Activation layers apply nonlinear functions to the convolution outputs, enabling the network to learn more complex features. Pooling layers reduce data dimensions and computational costs. Finally, fully connected layers integrate all features, and the output layer produces the predicted image.</p><p>Through these operations, conventional CNN-based fusion methods mitigate the complexity and instability associated with hand-crafted features in traditional methods. For example, STFDCNN [<xref rid="B33-sensors-25-01093" ref-type="bibr">33</xref>] introduces a fusion model combining a nonlinear mapping CNN (NLMCNN) and a super-resolution CNN (SRCNN). NLMCNN trains feature extraction filters, nonlinear mapping filters, and reconstruction filters to extract features from coarse images, map them into residual feature maps, and reconstruct downsampled fine images. SRCNN correlates the output image with the original fine image. During training, NLMCNN learns the relationship between coarse and downsampled fine images, while SRCNN learns the relationship between the downsampled and original fine images. During prediction, STFDCNN uses the transition image from NLMCNN as input to the trained SRCNN to generate the predicted fine image. By separating spatial and temporal relationships into separate learning processes within two convolutional networks, STFDCNN significantly enhances fusion accuracy.</p><p>However, as the first conventional CNN-based spatiotemporal fusion model of its kind, STFDCNN has limitations. Subsequent studies focused on addressing these flaws. Zheng et al. [<xref rid="B35-sensors-25-01093" ref-type="bibr">35</xref>] noted that STFDCNN&#x02019;s three hidden layers struggle to capture complex nonlinear relationships and proposed the VDCN model. VDCN trains a deep NLM CNN between coarse images and downsampled fine images, followed by a deep multi-scale super-resolution (MSSR) CNN that correlates downsampled and original fine images. During prediction (<xref rid="sensors-25-01093-f005" ref-type="fig">Figure 5</xref>), MSSR CNN divides the downsampling into two stages to mitigate resolution gap issues. Also based on SRCNN, ESRCNN [<xref rid="B37-sensors-25-01093" ref-type="bibr">37</xref>] resamples images to a uniform resolution before fusion. STFDCNN, focusing on nonlinear mappings, neglects temporal change information. To address this, DL-SDFM [<xref rid="B41-sensors-25-01093" ref-type="bibr">41</xref>] generates feature maps that capture both temporal changes and spatial information. Using a two-stream convolutional network, DL-SDFM predicts phenological and land-cover changes separately. To improve robustness against phenological changes, LSTM-SRCNN [<xref rid="B82-sensors-25-01093" ref-type="bibr">82</xref>] integrates long short-term memory (LSTM) networks with CNNs, assessing model performance across different phenological scenarios. Similarly, TSSTFN [<xref rid="B83-sensors-25-01093" ref-type="bibr">83</xref>] employs LSTM to capture long-term dependencies. These conventional CNN-based methods progressively resolve issues related to nonlinear mapping, spatial detail reconstruction, and phenological changes, significantly enhancing spatiotemporal fusion accuracy and robustness.</p><p>Despite the improvements made by the above methods, conventional CNN-based approaches still have areas needing enhancement. Subsequent research has further improved fusion performance. For instance, feature-level fusion in these methods can cause high-frequency detail loss and image smoothing. To address this, MCDNet [<xref rid="B34-sensors-25-01093" ref-type="bibr">34</xref>] employs a multi-scale mechanism and dilated convolutions to extract edge information while using a composite loss function to reduce smoothing. Since conventional CNNs process limited temporal information, LTSC3D [<xref rid="B36-sensors-25-01093" ref-type="bibr">36</xref>] introduces a three-dimensional fully convolutional spatiotemporal fusion model based on multidimensional datasets (MDDs). To enhance fusion accuracy in heterogeneous regions, StfNet [<xref rid="B39-sensors-25-01093" ref-type="bibr">39</xref>] independently learns spatial and temporal information, leveraging structural similarity and texture features between images. These advancements have refined existing models and addressed spatial and temporal information loss to some extent.</p><p>Differences between sensors present another challenge for conventional CNN-based spatiotemporal fusion models. Variations in spectral (<xref rid="sensors-25-01093-f006" ref-type="fig">Figure 6</xref>) and geometric characteristics across sensors can introduce biases between images, affecting fusion accuracy. To mitigate this, BiaSTF [<xref rid="B43-sensors-25-01093" ref-type="bibr">43</xref>] uses convolutional networks to learn sensor biases, significantly reducing spectral and spatial distortions. Another method, MUSTFN [<xref rid="B38-sensors-25-01093" ref-type="bibr">38</xref>], improves performance through multi-level and multi-scale feature extraction. It preserves spatial details and weights neighboring pixels to address information loss caused by Landsat-7 scan-line corrector (SLC) failure and cloud occlusion. By leveraging multi-level feature extraction and automated learning, conventional CNN-based methods can effectively integrate data from different sensors, minimizing sensor discrepancies and enhancing fusion accuracy and stability.</p><p>Through the aforementioned studies, conventional CNN-based fusion methods have been extensively explored, prompting researchers to apply them in practical scenarios. To meet application needs, improved spatiotemporal fusion models have been proposed. For example, MSTTIFN [<xref rid="B40-sensors-25-01093" ref-type="bibr">40</xref>] enhances fusion for land surface temperature by extracting multi-scale features and texture information, addressing input noise propagation and information loss. CIG-STF [<xref rid="B42-sensors-25-01093" ref-type="bibr">42</xref>] integrates change detection with spatiotemporal fusion to improve performance in regions with land-cover changes. These studies show that improved CNN-based fusion methods offer strong adaptability and excellent performance across diverse applications.</p><p>Conventional CNN-based methods have advanced in nonlinear mapping, spatial detail reconstruction, and sensor inconsistencies, showing good performance in certain fusion scenarios. However, as spatiotemporal fusion scenarios become more complex and data volumes increase, these methods still have drawbacks in capturing details and handling diverse applications. To address this, residual-based CNN methods have been developed, improving feature extraction and spatiotemporal fusion accuracy with deeper networks and skip connections.</p></sec><sec id="sec2dot1dot2-sensors-25-01093"><title>2.1.2. Residual-Based CNN Methods</title><p>Residual-based CNN methods refer to convolutional neural networks with residual blocks. Originally proposed by [<xref rid="B31-sensors-25-01093" ref-type="bibr">31</xref>], residual blocks are designed to create skip connections between different layers of a CNN, which solves the problem of gradient vanishing and model degradation in deep networks. As shown in <xref rid="sensors-25-01093-f007" ref-type="fig">Figure 7</xref>, residual connections add the input to the output of the subsequent layers, allowing gradients to propagate directly to previous layers. This structure helps the network learn identity mappings, increases depth while maintaining efficient training, and improves model expressiveness and generalization.</p><p>Similar to conventional CNN-based models, residual-based CNN models aim to tackle the shortcomings of traditional spatiotemporal fusion methods. In 2018, Tan et al. [<xref rid="B26-sensors-25-01093" ref-type="bibr">26</xref>] proposed the first residual-based CNN fusion network, DCSTFN, which uses residual blocks to find a direct nonlinear mapping between coarse and fine images. The architecture of DCSTFN includes three parts, as shown in <xref rid="sensors-25-01093-f008" ref-type="fig">Figure 8</xref>: first, a shared network expands coarse images; then, a sub-network extracts features from fine images; finally, the extracted features of the fine and coarse images are fused using deconvolution. These convolutional and deconvolutional layers greatly improve the accuracy and robustness of spatiotemporal fusion. However, DCSTFN still has flaws. The use of the mean squared error (MSE) as a loss function often leads to blurred predictions. To address this, the enhanced DCSTFN (EDCSTFN) [<xref rid="B45-sensors-25-01093" ref-type="bibr">45</xref>] introduces a composite loss function that preserves high-frequency information and reduces blurriness. In EDCSTFN, spectral information is derived from spectrum changes between the reference and prediction dates. Additionally, to reduce information loss from the direct summation of feature maps in DCSTFN, DMNet [<xref rid="B49-sensors-25-01093" ref-type="bibr">49</xref>] employs skip connections within a multi-scale feature extraction framework to preserve temporal variations and spatial details. To handle large variations in spatial resolution, MSISR-STF [<xref rid="B85-sensors-25-01093" ref-type="bibr">85</xref>] integrates Graph Neural Networks (GNNs) with residual convolutional networks to find similar pixels between coarse and fine images, aggregating them into graph-structured information to enhance the super-resolution process. By integrating skip connections, residual-based CNN methods have successfully addressed blurriness and information loss in traditional approaches.</p><p>In addition to enhancing traditional spatiotemporal fusion methods, residual-based CNN fusion models have significantly improved conventional approaches like STFDCNN, StfNet, and DL-SDFM. For instance, Li et al. [<xref rid="B47-sensors-25-01093" ref-type="bibr">47</xref>] introduced a residual CNN model in STFDCNN to reduce redundant computations by merging two transitional images. Peng et al. [<xref rid="B51-sensors-25-01093" ref-type="bibr">51</xref>] applied residual blocks in STF3DCNN to optimize data structures and improve computational efficiency for long time-series fusion. To address StfNet&#x02019;s issue of global fusion parameters failing to capture local variations, STFMCNN [<xref rid="B67-sensors-25-01093" ref-type="bibr">67</xref>] incorporated a multi-scale two-stream residual network, enhancing local change feature extraction. Moreover, ResStf [<xref rid="B53-sensors-25-01093" ref-type="bibr">53</xref>] improved StfNet by using skip connections and a single image pair for spatiotemporal fusion, solving the challenge of obtaining suitable image pairs. In response to DL-SDFM&#x02019;s limitations in detecting phenological changes, HDLSFM [<xref rid="B55-sensors-25-01093" ref-type="bibr">55</xref>] applied a super-resolution residual network to process both phenological and land-cover changes. Furthermore, to mitigate blurriness and high computational costs in CNN methods, residual-based techniques like STFRDN [<xref rid="B60-sensors-25-01093" ref-type="bibr">60</xref>], STFDSC [<xref rid="B44-sensors-25-01093" ref-type="bibr">44</xref>], and a dual-branch network [<xref rid="B46-sensors-25-01093" ref-type="bibr">46</xref>] integrate dense residual blocks, depthwise separable convolution, and a selection kernel mechanism, respectively. These advancements have significantly improved CNN fusion models by optimizing computational efficiency, enhancing local feature extraction, simplifying input requirements, addressing blurriness, and reducing computational intensity.</p><p>With the development of residual-based CNN fusion methods, spatial information extraction has significantly improved. As a result, some studies now focus on enhancing temporal information extraction in CNN models. Advancements in residual-based CNNs have encouraged researchers to explore their potential in addressing temporal feature extraction challenges in spatiotemporal fusion. In the fusion model by Hoque et al. [<xref rid="B48-sensors-25-01093" ref-type="bibr">48</xref>], a U-Net [<xref rid="B86-sensors-25-01093" ref-type="bibr">86</xref>] architecture with residual blocks (<xref rid="sensors-25-01093-f009" ref-type="fig">Figure 9</xref>) enhances temporal feature extraction. Most spatiotemporal fusion methods require the reference and predicted dates to be close, but this is difficult due to cloud cover or rain. To address this, Jia et al. [<xref rid="B50-sensors-25-01093" ref-type="bibr">50</xref>] applied a temporal constraint mechanism to a residual convolutional fusion model, accounting for differences between the reference and predicted dates. Due to limited temporal information, many fusion methods cannot reconstruct abrupt land-cover changes. Xiong et al. [<xref rid="B52-sensors-25-01093" ref-type="bibr">52</xref>] addressed this by using enhanced residual dense networks and modified temporal sequences to reduce reflectance differences and improve prediction accuracy. The introduction of residual CNNs has strengthened the ability of spatiotemporal fusion models to extract temporal information and expanded their application across various data types.</p><p>In recent years, increases in the volume and resolution of remote sensing image data have introduced new challenges to spatiotemporal fusion at the data level. Residual-based CNN methods are applied to address feature degradation and limited generalization in different sensor combinations. Image pairs from Landsat and MODIS are commonly used in spatiotemporal fusion studies, but the large spatial resolution difference often leads to feature degradation. To solve this, TSDTSF [<xref rid="B54-sensors-25-01093" ref-type="bibr">54</xref>] improves coarse image features using residual convolution and feature transformation, while DPSTFN [<xref rid="B56-sensors-25-01093" ref-type="bibr">56</xref>] adopts a progressive fusion scheme to enhance MODIS data resolution. Models trained with Landsat and MODIS pairs often generalize poorly to other satellite data. To address this, Htitiou et al. [<xref rid="B57-sensors-25-01093" ref-type="bibr">57</xref>] and Wei et al. [<xref rid="B58-sensors-25-01093" ref-type="bibr">58</xref>] developed residual-based fusion models using Landsat-8 and Sentinel-2 pairs, and PMS and WFV pairs from GF-1, respectively. The residual convolutional models in [<xref rid="B62-sensors-25-01093" ref-type="bibr">62</xref>,<xref rid="B64-sensors-25-01093" ref-type="bibr">64</xref>] utilize image pairs from Luojia-01 and VIIRS DNB nighttime sensors, as well as high-resolution PlanetScope and UAV sensors. These residual-based CNN methods have enhanced feature quality and generalization in spatiotemporal fusion models across different sensors, establishing a foundation for applying residual convolution methods in various scenarios.</p><p>Studies have shown significant improvements in residual-based CNN fusion models for spatial and temporal information extraction and data processing. These advancements highlight the potential of residual convolutional methods across various scenarios, driving further research on specific fusion applications. For example, Wei et al. [<xref rid="B59-sensors-25-01093" ref-type="bibr">59</xref>] proposed MOST, an image mosaicking method using residual-based CNNs for color adjustment. Fu et al. [<xref rid="B66-sensors-25-01093" ref-type="bibr">66</xref>] presented STFNet for tropical cyclone intensity estimation. ACFNet [<xref rid="B61-sensors-25-01093" ref-type="bibr">61</xref>] and BASNet [<xref rid="B63-sensors-25-01093" ref-type="bibr">63</xref>] are residual-based methods for ice lake extraction and flood classification, respectively. STTFN [<xref rid="B65-sensors-25-01093" ref-type="bibr">65</xref>] uses skip connections in convolutional networks to reduce spatial detail loss in surface temperature fusion. These applications demonstrate the broad applicability and potential of residual-based CNN spatiotemporal fusion methods across various fields.</p><p>Residual-based CNN models for spatiotemporal fusion have significantly improved feature extraction and generalization performance, leading to widespread application and development across various domains. However, as spatiotemporal fusion demands become more complex and refined, these methods face limitations in capturing long-range dependencies and intricate temporal relationships. To overcome these challenges, researchers have integrated attention mechanisms into CNNs to further enhance the accuracy of spatiotemporal fusion models.</p></sec><sec id="sec2dot1dot3-sensors-25-01093"><title>2.1.3. Attention-Based CNN Methods</title><p>An attention mechanism [<xref rid="B32-sensors-25-01093" ref-type="bibr">32</xref>] enhances a model&#x02019;s ability to focus on key features or important regions by dynamically assigning weights, improving the efficiency and effectiveness of information extraction. Attention-based convolutional networks (<xref rid="sensors-25-01093-f010" ref-type="fig">Figure 10</xref>) combine the strengths of conventional CNNs and attention mechanisms, adjusting focus to key regions or channels in input images. By introducing spatial or channel attention modules, attention-based CNNs improve performance in various image tasks, including spatiotemporal fusion.</p><p>Compared to conventional and residual-based CNNs, attention-based CNNs handle large resolution disparities and complex temporal relationships more effectively, making them key to optimizing fusion performance. For example, PDCNN [<xref rid="B72-sensors-25-01093" ref-type="bibr">72</xref>] uses an attention-based pseudo-Siamese network to extract features from both high- and low-resolution images. Sun et al. [<xref rid="B74-sensors-25-01093" ref-type="bibr">74</xref>] replaced traditional pixel similarity measurements with a learnable attention module to better utilize input image pairs. Ran et al. [<xref rid="B77-sensors-25-01093" ref-type="bibr">77</xref>] introduced SIFnet, an attention-based model that captures resolution differences. STF-EGFA [<xref rid="B76-sensors-25-01093" ref-type="bibr">76</xref>] incorporates an edge feature extraction module with attention to refine feature alignment. SCRnet [<xref rid="B78-sensors-25-01093" ref-type="bibr">78</xref>] uses a spatial-channel attention mechanism to optimize feature fusion. MANet [<xref rid="B80-sensors-25-01093" ref-type="bibr">80</xref>] addresses missing spatial details with separate sub-networks and a residual channel attention upsampling module. These methods enhance spatiotemporal fusion performance by focusing on critical features.</p><p>Attention-based CNN methods excel in handling temporal variations, long-term dependencies, and complex spatiotemporal relationships. Non-attention-based approaches often struggle with low accuracy in regions with significant temporal changes. To address this, AMNet [<xref rid="B68-sensors-25-01093" ref-type="bibr">68</xref>] integrates attention and multi-scale mechanisms to better capture temporal variations. ASRCNN [<xref rid="B70-sensors-25-01093" ref-type="bibr">70</xref>] and RCAN [<xref rid="B71-sensors-25-01093" ref-type="bibr">71</xref>] use attention-based CNNs to improve long-term NDVI reconstruction accuracy in heterogeneous regions. These improvements highlight the superiority of attention-based CNN fusion methods in managing complex spatial and temporal relationships. Consequently, more recent attention-based fusion methods have enhanced their ability to handle complex surface variations across various datasets.</p><p>Convolutional networks with attention mechanisms can focus on regions that vary widely between images, giving fusion methods based on attention-based CNNs greater robustness and precision in handling complex terrains and surface features. For example, DSTFN [<xref rid="B69-sensors-25-01093" ref-type="bibr">69</xref>] integrates residual dense blocks and attention mechanisms to improve performance in abrupt change scenarios and produce high-resolution time series data. CAFE [<xref rid="B73-sensors-25-01093" ref-type="bibr">73</xref>] uses multiple processing units with a cross-attention mechanism to capture temporal variations and spatial information, adapting feature weights from spatial and spectral domains. These advancements significantly enhance the adaptability and accuracy of attention-based CNNs in managing surface changes, improving spatiotemporal fusion models across various image scales and resolutions.</p><p>Attention-based CNN fusion methods retain fine details from high-resolution images and large-scale patterns from low-resolution images, which is crucial for fusing data with different resolution scales. For example, ECPW-STFN [<xref rid="B79-sensors-25-01093" ref-type="bibr">79</xref>] uses a convolutional attention enhancement module to reduce dependence on the number of input images. DSTFNet [<xref rid="B75-sensors-25-01093" ref-type="bibr">75</xref>] introduces an attention-driven dual-branch network, where the spatial branch extracts scale information. RCAN-FSDAF [<xref rid="B81-sensors-25-01093" ref-type="bibr">81</xref>] integrates attention mechanisms with traditional fusion methods to correct spatial discrepancies between images of different resolutions. These studies improve the adaptability and accuracy of attention-based CNN fusion techniques across various resolutions and image scales.</p><p>Attention-based CNN techniques have greatly enhanced the efficiency and versatility of spatiotemporal fusion models by refining feature extraction and fusion in complex scenarios, such as varying resolutions, temporal fluctuations, and land-cover discrepancies. However, despite overcoming some challenges faced by traditional CNNs, they continue to struggle to produce high-quality spatiotemporal fusion images. As a result, fusion techniques using generative adversarial networks have gained significant attention in recent spatiotemporal fusion research.</p></sec></sec><sec id="sec2dot2-sensors-25-01093"><title>2.2. GAN-Based Fusion Methods</title><p>A generative adversarial network (GAN), originally proposed by [<xref rid="B22-sensors-25-01093" ref-type="bibr">22</xref>], is a generative model initially used for image generation, denoising, restoration, and conversion. A GAN consists of a generator and a discriminator (<xref rid="sensors-25-01093-f011" ref-type="fig">Figure 11</xref>), where the generator learns to produce realistic data, while the discriminator differentiates between generated and real data. The training process is a two-person zero-sum game [<xref rid="B27-sensors-25-01093" ref-type="bibr">27</xref>], where both the generator and discriminator improve simultaneously. When the generator produces data indistinguishable from real samples and the discriminator can no longer differentiate, the network is considered well trained.</p><p>Convolutional fusion methods still face challenges like overfitting and poor noise management, which stem from the inherent limitations of convolutional networks. These issues can be addressed with unsupervised learning models like Autoencoders, which are effective in pan-sharpening [<xref rid="B88-sensors-25-01093" ref-type="bibr">88</xref>] and have been applied to spatiotemporal fusion. For instance, Chen et al. [<xref rid="B67-sensors-25-01093" ref-type="bibr">67</xref>] proposed a conditional Variational Autoencoder-based model for better feature extraction and dimensionality reduction. However, Autoencoders can suffer from blurring effects due to their deterministic encoding-decoding processes. To overcome these limitations, recent research has introduced GAN-based fusion methods, which use adversarial training to improve robustness and generate high-precision fusion images, even from sparse or missing data. The generative mechanism of GANs provides strong generalization abilities, reducing overfitting and data distribution issues. Compared to CNN-based methods, GAN-based approaches yield better fusion outcomes with limited data, enhancing adaptability and generalizability in spatiotemporal fusion. GAN-based spatiotemporal fusion methods are summarized in <xref rid="sensors-25-01093-t002" ref-type="table">Table 2</xref>.</p><p>GAN-based fusion methods were initially designed to enhance the fusion accuracy of CNN-based models by improving feature fusion effectiveness and image generation quality. For example, convolutional fusion methods like STFDCNN and StfNet require separate feature extraction and fusion processes, increasing complexity. To address this, STFGAN [<xref rid="B94-sensors-25-01093" ref-type="bibr">94</xref>] introduces an end-to-end adversarial generative network framework, enhancing fusion efficiency through generator and discriminator optimization. PSTAF-GAN [<xref rid="B100-sensors-25-01093" ref-type="bibr">100</xref>] combines GANs with attention mechanisms to integrate feature extraction and fusion, improving efficiency and accuracy. CNN-based methods often overlook sensor discrepancies, so SSTSTF [<xref rid="B91-sensors-25-01093" ref-type="bibr">91</xref>] incorporates a modular GAN that accounts for spectral, spatial, and sensor differences. Additionally, SMPG [<xref rid="B92-sensors-25-01093" ref-type="bibr">92</xref>] integrates a pixel-matching module to address vanishing gradients and insufficient training data. By refining CNN-based models with GANs, these methods improve fusion quality and efficiency while reducing data dependence.</p><p>Another approach to reducing data dependence is minimizing the number of required inputs in spatiotemporal fusion models. Most traditional fusion models require at least three images and impose strict quality standards on reference images, limiting the broad applicability of spatiotemporal fusion. To address this, GAN-based methods focus on reducing reliance on both the quantity and quality of input images. For example, Tan et al. [<xref rid="B27-sensors-25-01093" ref-type="bibr">27</xref>] proposed the GANSTFM model, which uses fine images as conditional inputs and requires only one pair of images, improving the flexibility of spatiotemporal fusion compared to methods that require three or five images (<xref rid="sensors-25-01093-f012" ref-type="fig">Figure 12</xref>). Inspired by GANSTFM, recent studies have developed models like GASTFM [<xref rid="B104-sensors-25-01093" ref-type="bibr">104</xref>], which also uses just one pair of images, and TLSRSTF [<xref rid="B96-sensors-25-01093" ref-type="bibr">96</xref>], which integrates a mid-resolution image transition module to extract spatial information with fewer inputs.</p><p>However, fusion methods using fewer images as input may also overlook surface changes in near-real-time monitoring. To address this, OPGAN [<xref rid="B108-sensors-25-01093" ref-type="bibr">108</xref>] enhances the temporal change recognition capability of single-pair fusion models by incorporating temporal variations from different time points. Resolution differences between input images also pose a challenge in traditional fusion methods, where significant disparities hinder spatial information extraction from coarse images. CycleGAN-STF [<xref rid="B89-sensors-25-01093" ref-type="bibr">89</xref>] addresses this issue by improving spatial information extraction with a cycle-generative adversarial network and an enhanced loss function. In response to the need for high-resolution spatiotemporal fusion, Liu et al. [<xref rid="B103-sensors-25-01093" ref-type="bibr">103</xref>] proposed the StarFusion model, combining traditional methods with super-resolution GANs to merge medium- and high-resolution images. To reduce errors and increase robustness, RSFN [<xref rid="B90-sensors-25-01093" ref-type="bibr">90</xref>] improves fusion quality by filtering input noise. Through adversarial training, these GAN-based methods have significantly enhanced spatiotemporal fusion effectiveness in scenarios with limited input data and noise interference.</p><p>Nevertheless, current GAN-based methods have certain drawbacks, and the following studies aim to address these issues. For example, to reduce image stitching seams in GANSTFM, Weng et al. [<xref rid="B107-sensors-25-01093" ref-type="bibr">107</xref>] proposed an improved method. In response to sensor errors affecting the fusion results in GANSTFM, Wu et al. [<xref rid="B99-sensors-25-01093" ref-type="bibr">99</xref>] introduced EDRGAN-STF, which uses degraded resolution versions of input images to rectify the fusion model. To better balance spatial and temporal feature extraction, MCBAM-GAN [<xref rid="B93-sensors-25-01093" ref-type="bibr">93</xref>] incorporates multi-level feature extraction, fusion, and multi-scale reconstruction into GAN-based models. Meanwhile, MLFF-GAN [<xref rid="B106-sensors-25-01093" ref-type="bibr">106</xref>] enhances sensor discrepancy processing with multi-layered feature extraction techniques. Additionally, to address the loss of spatial details and image blurring caused by neglecting shallow or low-dimensional features, AMS-STF [<xref rid="B95-sensors-25-01093" ref-type="bibr">95</xref>] adopts an adaptive multi-scale pyramid network for better feature recognition at different scales. These advancements have greatly improved the performance of GAN-based fusion models in feature extraction, image generation, and handling sensor errors.</p><p>Some studies have contributed to the refinement and expansion of GAN-based spatiotemporal fusion methods. For example, Jiang et al. [<xref rid="B98-sensors-25-01093" ref-type="bibr">98</xref>] introduced DRCGAN, a GAN-based model for fusing optical and radar images. MOSTGAN [<xref rid="B102-sensors-25-01093" ref-type="bibr">102</xref>], which is based on MOST, uses GANs for color adjustment in image stitching. DSFN [<xref rid="B97-sensors-25-01093" ref-type="bibr">97</xref>] utilizes GANs for spatiotemporal fusion of land-surface temperature. Additionally, several studies have applied GANs to spatial&#x02013;spectral&#x02013;temporal fusion [<xref rid="B101-sensors-25-01093" ref-type="bibr">101</xref>,<xref rid="B105-sensors-25-01093" ref-type="bibr">105</xref>]. These methods have demonstrated the significant flexibility and potential of GAN-based approaches in addressing various complex tasks across different domains.</p><p>Compared to CNN-based methods, spatiotemporal fusion models based on generative adversarial networks (GANs) have significantly enhanced image generation quality and feature fusion effectiveness. However, as the demand for different fusion scenarios grows and data complexity increases, GAN-based methods still encounter challenges related to stability and long-term dependencies. This has led to a growing interest in Transformer-based spatiotemporal fusion models, which have emerged as a promising new research direction in the field, offering the potential for improved handling of long-range dependencies and complex data relationships.</p></sec><sec id="sec2dot3-sensors-25-01093"><title>2.3. Transformer-Based Fusion Methods</title><p>Transformers [<xref rid="B23-sensors-25-01093" ref-type="bibr">23</xref>], originally developed for natural language processing, use a self-attention mechanism and an encoder&#x02013;decoder architecture (<xref rid="sensors-25-01093-f013" ref-type="fig">Figure 13</xref>) to capture long-range dependencies and improve training efficiency. Their multi-head attention extracts multiple feature layers. Adapted for computer vision tasks [<xref rid="B109-sensors-25-01093" ref-type="bibr">109</xref>], Transformers are widely used in spatiotemporal fusion due to their flexibility and expressiveness.</p><p>Handling time-series data has been a challenge for many deep learning-based fusion methods. While Recurrent Neural Networks (RNNs), such as LSTM and GRUs, are commonly used for spatiotemporal fusion [<xref rid="B82-sensors-25-01093" ref-type="bibr">82</xref>,<xref rid="B83-sensors-25-01093" ref-type="bibr">83</xref>], they struggle with capturing long-range dependencies and suffer from inefficiencies in sequential processing. Transformer-based methods were introduced to overcome these limitations, utilizing self-attention and parallel computation to capture long-term relationships and improve training efficiency.</p><p>Transformers offer several advantages over CNNs and GANs. For example, GANs suffer from issues like vanishing gradients and mode collapse [<xref rid="B110-sensors-25-01093" ref-type="bibr">110</xref>], and their generators often focus too much on local features, making it difficult to capture long-term dependencies across time or space. In contrast, Transformer-based spatiotemporal fusion models address these issues by extending temporal dependencies and ensuring more stable training. With the help of multi-head attention, Transformers excel at capturing long-term relationships, improving overall training stability. Transformer-based fusion methods are summarized in <xref rid="sensors-25-01093-t003" ref-type="table">Table 3</xref>.</p><p>Transformer-based fusion methods have significantly improved processing flexibility, temporal change accuracy, and feature mapping capabilities compared to CNN-based models. CNNs struggle with limited receptive fields, making it difficult to capture global information. For instance, STF-Trans [<xref rid="B115-sensors-25-01093" ref-type="bibr">115</xref>] uses a serialized embedding approach of a Transformer and a dual-stream feature extraction framework to better capture deep features. Convolutional models also struggle to capture temporal variations across different spatial scales. STM-STFNet [<xref rid="B116-sensors-25-01093" ref-type="bibr">116</xref>] addresses this by employing the Swin Transformer to extract global information and learn temporal change features. Additionally, MSFusion [<xref rid="B111-sensors-25-01093" ref-type="bibr">111</xref>] combines Transformer and CNN modules and uses self-attention to capture global change information, improving temporal feature extraction. These advancements demonstrate the effectiveness of Transformer-based spatiotemporal fusion methods in overcoming the limitations of convolutional models.</p><p>Transformer-based fusion methods offer several improvements over GAN-based models, particularly in reducing input noise, lowering computational costs, and enhancing channel feature extraction. For instance, DBTT-FM [<xref rid="B117-sensors-25-01093" ref-type="bibr">117</xref>] uses a dual-branch Transformer to extract texture features and applies a composite loss function to reduce noise in generated images. MSNet [<xref rid="B28-sensors-25-01093" ref-type="bibr">28</xref>] utilizes Transformers to capture local and global temporal changes, minimizing noise by merging coarse and fine features. EMSNet [<xref rid="B114-sensors-25-01093" ref-type="bibr">114</xref>] enhances this by using Transformer embedding and dilated convolutions to extract temporal information and reduce the number of input images. Additionally, GAN methods often require high computational resources, which Transformer-based approaches help mitigate. For example, SwinSTFM [<xref rid="B112-sensors-25-01093" ref-type="bibr">112</xref>] uses shifted windows and self-attention mechanisms to reduce redundancy and computational costs. Furthermore, SMSTFM [<xref rid="B113-sensors-25-01093" ref-type="bibr">113</xref>] improves feature extraction by incorporating multi-band fusion and three-dimensional convolutions, capturing both spatial and spectral features more effectively. These advancements show that Transformer-based methods outperform GAN-based models while minimizing hardware requirements, thus enhancing the efficiency and adaptability of spatiotemporal data processing.</p><p>Transformer-based fusion methods offer significant improvements in flexibility, accuracy in capturing temporal variations, and the ability to map global features. They have successfully addressed challenges such as reducing input noise, lowering computational costs, and enhancing feature extraction compared to CNN- and GAN-based methods. However, research on Transformers for spatiotemporal fusion remains limited, indicating substantial potential for further advancement. Future studies should focus on exploring Transformer models in various spatiotemporal contexts and refining their architectures to better harness their capabilities for handling complex spatiotemporal data.</p></sec><sec id="sec2dot4-sensors-25-01093"><title>2.4. Diffusion-Based Fusion Methods</title><p>Recently, diffusion models have gained attention in computer vision [<xref rid="B118-sensors-25-01093" ref-type="bibr">118</xref>], particularly Denoising Diffusion Probabilistic Models (DDPMs) [<xref rid="B24-sensors-25-01093" ref-type="bibr">24</xref>]. A diffusion model is a generative model that gradually adds random noise to data through a diffusion Markov process. Its training involves two phases: a diffusion process and a denoising process (<xref rid="sensors-25-01093-f014" ref-type="fig">Figure 14</xref>). In the diffusion process, noise is progressively added to real data samples, while in the denoising process, pure noise is gradually removed to recover the original data. By alternating between these two stages, diffusion models learn to generate data that closely resemble the original samples. Given their strong generative capabilities, diffusion models have been applied to various image tasks, such as super-resolution [<xref rid="B119-sensors-25-01093" ref-type="bibr">119</xref>], image denoising [<xref rid="B120-sensors-25-01093" ref-type="bibr">120</xref>], and image restoration [<xref rid="B121-sensors-25-01093" ref-type="bibr">121</xref>]. As a result, some studies have started exploring diffusion models for spatiotemporal fusion, as shown in <xref rid="sensors-25-01093-t004" ref-type="table">Table 4</xref>.</p><p>Huang et al. introduced a diffusion model into the field of spatiotemporal fusion and proposed STFDiff [<xref rid="B29-sensors-25-01093" ref-type="bibr">29</xref>]. In response to the issues of spatial, spectral, and temporal uncertainties in current deep learning-based fusion methods and the problem of mode collapse in GAN-based fusion models, STFDiff integrates a diffusion model and a dual-stream U-Net to better predict noise in each time step. The process of spatiotemporal fusion is regarded as a conditional diffusion process in STFDiff, where fine images from reference dates and coarse images from predicted dates serve as the conditional input, while the target images serve as the original input. Experiments with STFDiff have shown that this diffusion-based fusion method outperforms others based on CNNs, GANs, and Transformers, demonstrating great potential for applications. To address sensor and scale errors in existing spatiotemporal fusion methods, Ma et al. utilized the concept of the conditional diffusion model and proposed DiffSTF [<xref rid="B122-sensors-25-01093" ref-type="bibr">122</xref>]. DiffSTF takes the structural information from fine images in reference dates and the spectral information from coarse images in predicted dates as the dual conditions for training. Similar to DiffSTF, Wei et al. [<xref rid="B123-sensors-25-01093" ref-type="bibr">123</xref>] proposed a diffusion-based fusion method, DiffSTSF, to blend images from GF-1 2-meter panchromatic, 8-meter multispectral and 16-meter wide-field cameras. To improve the fusion results from previous work [<xref rid="B58-sensors-25-01093" ref-type="bibr">58</xref>], a multi-conditional diffusion model was utilized to achieve better results than existing CNN-based methods. In DiffSTSF, the diffusion process is regarded as a degradation process that models the downscaling explicitly, while the backward denoise process is considered the fusion process.</p><p>These studies demonstrate the improvements and advancements of diffusion-based methods over current deep learning-based models, as well as their great potential in the field of spatiotemporal fusion. However, existing research on diffusion-based fusion models remains lacking. Therefore, future studies should focus on further improving the performance and applicability of diffusion models to maximize their potential in spatiotemporal fusion.</p></sec></sec><sec id="sec3-sensors-25-01093"><title>3. Evaluations and Applications</title><p>Deep learning-based spatiotemporal fusion methods are applied in fields like crop classification, land-cover mapping, and change detection. Comparing their performance and efficiency is crucial for selecting the right model for specific needs. This section explores their applications, compares their performance, and evaluates their adaptability.</p><sec id="sec3dot1-sensors-25-01093"><title>3.1. Method Comparisons</title><p>Evaluating spatiotemporal fusion methods is crucial to determine their effectiveness in remote sensing applications. This assessment focuses on performance, which examines the accuracy and reliability of fused results, and on computational efficiency, which considers processing time and resource use. A comprehensive comparison of these aspects helps identify the best method for specific needs. In this section, we compare CNN-based, GAN-based, Transformer-based, and diffusion model-based fusion methods to assess their accuracy and efficiency.</p><sec id="sec3dot1dot1-sensors-25-01093"><title>3.1.1. Performance Evaluation</title><p>Performance evaluation is essential to assess how well spatiotemporal fusion methods preserve spatial, temporal, and spectral details. Since the effectiveness of deep learning models can vary based on parameters, training times, and hardware, we conducted a statistical analysis of the fusion performance metrics of four open-source spatiotemporal fusion models (<xref rid="sensors-25-01093-t005" ref-type="table">Table 5</xref>) using publicly available datasets for an objective comparison.</p><p>We compared the performance of the CNN-based, GAN-based, Transformer-based, and diffusion-based fusion models using two publicly available datasets and five evaluation metrics. The Coleambally Irrigation Area (CIA) dataset pertains to southern New South Wales (NSW, Australia; 145.0675&#x000b0; S, 34.0034&#x000b0; E) and consists of Landsat-7 and MODIS image pairs, capturing phenological changes over a season. The Lower Gwydir Catchment (LGC) dataset pertains to northern NSW (29.0855&#x000b0; S, 149.2815&#x000b0; E) and includes Landsat-5 and MODIS pairs, covering crop growth cycles and a flood event. Examples of cropland areas from the CIA dataset and a flood event from the LGC dataset are shown in <xref rid="sensors-25-01093-f015" ref-type="fig">Figure 15</xref>. The evaluation metrics include the root mean square error (RMSE), structural similarity index (SSIM), and correlation coefficient (CC) for spatial accuracy, as well as the spectral angle mapper (SAM) and relative dimensionless global error in synthesis (ERGAS) for spectral accuracy. The RMSE measures the difference between the predicted and reference values, the SSIM evaluates the structural similarity, the CC assesses the linear relationship between the predicted and reference images, the SAM measures the spectral similarity, and the ERGAS evaluates the overall spectral quality. Note that since STFDiff appears only once in the literature, its box plot has no height.</p><p>As shown in <xref rid="sensors-25-01093-t006" ref-type="table">Table 6</xref> and <xref rid="sensors-25-01093-f016" ref-type="fig">Figure 16</xref>, the RMSE decreases consistently across the CIA dataset as the spatiotemporal fusion models advance from CNN to diffusion, highlighting that the reduction in the pixel-level RMSE is a key focus of these models. However, significant fluctuations in the SSIM suggest that these models struggle with global representation. For the correlation coefficient (CC), the Transformer and diffusion models outperform the CNN and GAN models, demonstrating that more complex models better preserve spatiotemporal correlations by learning intricate nonlinear mappings. In terms of the SAM, the Transformer and diffusion models show greater advantages. However, the performance of the ERGAS fluctuates for all models, likely due to deep learning models using small data blocks for training, limiting their ability to leverage global information. The CNN model, with its limited receptive field, shows poorer ERGAS performance.</p><p>Similar to the CIA dataset, the indicators in the LGC dataset reflect the performance of the four models, as shown in <xref rid="sensors-25-01093-t007" ref-type="table">Table 7</xref> and <xref rid="sensors-25-01093-f017" ref-type="fig">Figure 17</xref>. The diffusion-based model consistently outperforms the others in all aspects. In terms of the RMSE and SSIM, the more advanced models show better performance. However, the correlation coefficient (CC) for the GAN-based model is lower than that of the others, possibly due to the GAN&#x02019;s emphasis on the adversarial process, which may neglect learning correlations between predicted and real images, leading to texture and detail discrepancies. In contrast, the diffusion-based method improves the CC by more effectively learning the data distribution. Regarding the SAM and ERGAS, all models exhibit significant fluctuations, similar to their performance on the CIA dataset.</p></sec><sec id="sec3dot1dot2-sensors-25-01093"><title>3.1.2. Computational Efficiency</title><p>Computational efficiency is crucial for the practical deployment of spatiotemporal fusion methods, especially in large-scale remote sensing tasks that require processing vast amounts of high-resolution data. In deep learning, the size of model parameters is the primary factor affecting training and inference time, as seen in <xref rid="sensors-25-01093-t008" ref-type="table">Table 8</xref>. CNN-based models are mainly affected by the number of layers and the size of convolutional kernels. GAN-based methods, due to simultaneous training of the generator and discriminator, require longer training times compared to CNN-based methods. Transformer models, due to the self-attention mechanism, consume large amounts of memory and require longer training times. Diffusion models, which require thousands of diffusion steps for each image, significantly increase training time.</p><p>Additionally, training time is influenced by the number of iterations required for model convergence. CNN-based methods, with their local receptive fields and shared weights, generally have shorter training times. In contrast, Transformer, GAN, and diffusion models have slower convergence speeds. GAN-based models experience slower convergence due to the instability in adversarial training, while diffusion models also suffer from slower loss reduction due to the iterative nature of the diffusion process. These factors contribute to the extended training times of GAN and diffusion models.</p></sec></sec><sec id="sec3dot2-sensors-25-01093"><title>3.2. Model Applicability</title><p>Model applicability examines the suitability of deep learning-based fusion models across various scenarios and datasets. This section assesses the adaptability of CNN, GAN, Transformer, and diffusion models through quantitative analysis of their evaluation metrics. It also evaluates their feasibility in different tasks and conditions, considering how data heterogeneity (such as variations in data types, quality, or resolution) impacts their performance. This analysis helps determine which models are best suited for specific spatiotemporal fusion applications and the challenges posed by different real-world data scenarios.</p><sec id="sec3dot2dot1-sensors-25-01093"><title>3.2.1. Feasibility for Different Scenarios</title><p>For the CIA dataset, which features small farmland areas with notable spatial heterogeneity over a single growing season, deep learning-based fusion methods face challenges related to both spatial and temporal variations. CNN models are less suitable due to their focus on local spatial patterns, which may not be sufficient to capture the dataset&#x02019;s high spatial diversity. While CNNs can extract texture and spectral features, they struggle to model complex relationships between varying crop types and environmental factors. GAN models are more appropriate for this dataset, as they excel at synthesizing spatial patterns, which helps capture the diversity of conditions in the farmland regions. Transformer models are highly effective due to their ability to model long-range dependencies across both spatial and temporal dimensions, addressing spatial heterogeneity and complex relationships in crop dynamics. Diffusion models, while computationally intensive, can enhance spatial feature generation and handle heterogeneity through their iterative refinement process, although they may struggle to capture the rapid changes in crop dynamics during the season.</p><p>For the LGC dataset, which spans an entire year of crop growth with significant temporal changes, including a flood event in a mountainous region, deep learning models face unique challenges. CNN models are limited due to their inability to capture long-range temporal dependencies and complex spatial patterns, especially in areas with large elevation changes and extreme events like floods. GAN models can generate high-resolution images but struggle to accurately model temporal sequences, particularly during abrupt environmental changes. Transformer models excel in this scenario, as their ability to capture long-range temporal dependencies allows them to effectively model the entire crop growth cycle and extreme events. Although computationally intensive, diffusion models are highly effective for this dataset, as they can iteratively refine image quality and handle the complex spatiotemporal variations caused by the mountainous terrain and flood events.</p></sec><sec id="sec3dot2dot2-sensors-25-01093"><title>3.2.2. Impact of Data Heterogeneity</title><p>The CIA dataset is marked by significant data heterogeneity, with variations in crop types, soil conditions, and farming practices across small farmland regions. This introduces considerable spatial and temporal variability, which is challenging for deep learning models. CNN-based methods struggle with this high heterogeneity because they are limited in their ability to capture long-range spatial dependencies, making them less effective in diverse environments with significant land-cover variation. GANs also face difficulties in modeling the complex temporal variations in crop growth across different farming regions, and their inability to capture long-term dependencies further limits their performance in such settings. Transformer models, however, are more capable of handling this heterogeneity due to their ability to model complex relationships between different crop types, seasonal changes, and environmental factors over time. Diffusion models, with their iterative refinement process, also show promise in addressing data heterogeneity. They progressively refine predictions while preserving key spatial features, improving the robustness of the model, and helping to mitigate inconsistencies caused by the dataset&#x02019;s heterogeneous conditions.</p></sec></sec><sec id="sec3dot3-sensors-25-01093"><title>3.3. Practical Applications</title><p>Spatiotemporal fusion techniques based on deep learning have been increasingly utilized to overcome the limitations of traditional remote sensing data, offering improved spatial and temporal resolutions for a wide range of applications. In particular, applications in crop classification, land-cover classification, vegetation monitoring, and change detection have benefited from deep learning-based methods (<xref rid="sensors-25-01093-f018" ref-type="fig">Figure 18</xref>). These applications provide valuable insights for environmental management and agricultural planning.</p><sec id="sec3dot3dot1-sensors-25-01093"><title>3.3.1. Crop Classification</title><p>In crop classification, mixed pixels in low-resolution images often blur the spectral characteristics of different land-cover types, leading to lower classification accuracy. Time-series remote sensing datasets generated through spatiotemporal fusion help monitor high-frequency changes, offering advantages in crop classification. For example, Zhan et al. [<xref rid="B83-sensors-25-01093" ref-type="bibr">83</xref>] proposed the CNN-based TSSTFN fusion model to generate multi-temporal high-resolution NDVI, improving classification accuracy between soybean and corn. Their experiments showed a significant improvement in the kappa coefficient, rising from 69.2% before fusion to 74.22&#x02013;82.44% after fusion. This highlights that the construction of high-resolution time-series NDVI through CNN-based fusion methods plays a crucial role in improving crop classification accuracy by leveraging the spectral and textural features of remote sensing images. CNNs are well suited for capturing local spatial features such as shape, texture, and spectral patterns, making them effective in distinguishing different crop types. Additionally, the relatively simple architecture and fast training speed of CNNs make them efficient for processing large-scale remote sensing datasets with reduced computational costs.</p></sec><sec id="sec3dot3dot2-sensors-25-01093"><title>3.3.2. Land-Cover Classification</title><p>Land-cover classification, a key component of Earth observation systems, plays an essential role in climate and ecological studies. High-resolution imagery generated by spatiotemporal fusion methods improves land-cover classification accuracy and supports time-series analysis. Similar to crop classification, CNN-based fusion methods have been effectively applied to enhance land-cover classification accuracy across various landscapes. Studies have shown significant improvements in performance with deep learning-based models [<xref rid="B79-sensors-25-01093" ref-type="bibr">79</xref>]. For example, AMSDFNet achieved a 2&#x02013;+3% increase in both overall pixel accuracy (PA) and mean intersection over union (mIOU) compared to milestone methods [<xref rid="B124-sensors-25-01093" ref-type="bibr">124</xref>]. DSTFNet improved the F1 score from 0.865 to 0.909 and demonstrated better transferability than direct classification using U-Net [<xref rid="B75-sensors-25-01093" ref-type="bibr">75</xref>]. These CNN-based spatiotemporal fusion methods outperform non-fusion methods in land-cover classification, offering advantages for managing the complexity and diversity of land-cover categories. However, for more intricate scenarios like land-cover classification, GAN-based, Transformer-based, and diffusion-based methods are more suitable and generally perform better than CNN-based fusion models [<xref rid="B29-sensors-25-01093" ref-type="bibr">29</xref>,<xref rid="B105-sensors-25-01093" ref-type="bibr">105</xref>].</p></sec><sec id="sec3dot3dot3-sensors-25-01093"><title>3.3.3. Vegetation Monitoring</title><p>Dense time-series remote sensing imagery is essential for continuous and reliable phenological monitoring, particularly for tracking growth processes. However, obtaining high-resolution, dense, and cloud-free imagery remains a challenge. To overcome this, spatiotemporal fusion is increasingly applied in vegetation monitoring, with the Normalized Difference Vegetation Index (NDVI) being the most commonly used vegetation index. Many studies utilize spatiotemporal fusion to generate dense time-series NDVI images for long-term, high-resolution vegetation monitoring. CNN-based models typically outperform traditional fusion methods in terms of accuracy, showing improvements in the root mean square error (RMSE) and structural similarity index (SSIM), especially in areas with phenological changes and shadows, like forests. This highlights the robustness of CNNs in spatiotemporal fusion for crop monitoring, both at the pixel and feature levels. Additionally, Transformer-based methods, known for their superior ability to capture temporal dependencies, are also effective for vegetation monitoring, although they incur significant computational costs.</p></sec><sec id="sec3dot3dot4-sensors-25-01093"><title>3.3.4. Change Detection</title><p>Time-series land-cover imagery is essential for surface change monitoring, as it provides high-resolution images that capture subtle changes in the Earth&#x02019;s surface. Spatiotemporal fusion techniques that generate dense, high-resolution land-cover time-series imagery play a key role in improving change detection accuracy. For example, the GAN-based fusion method SMPG [<xref rid="B92-sensors-25-01093" ref-type="bibr">92</xref>] employs spatiotemporal fusion for change detection in snow areas. By providing detailed spatial and temporal information, high-resolution synthetic images produced through spatiotemporal fusion can significantly enhance change detection accuracy. Experimental results showed that the SMPG method achieved change detection error rates of 0.17% and 0.84% in two regions, outperforming other fusion methods with error rates of 0.68% and 1.89%. The superior performance of GAN-based fusion models in change detection is attributed to their ability to generate more realistic and accurate synthetic images by effectively learning the underlying data distribution, thus improving the detection of subtle surface changes.</p></sec></sec></sec><sec id="sec4-sensors-25-01093"><title>4. Current Issues and Future Directions</title><p>Current studies on deep learning-based spatiotemporal fusion methods have shown considerable progress in feature extraction, spatiotemporal modeling, and computational efficiency. However, several critical challenges persist. Existing models struggle to precisely detect subtle and ephemeral changes. The discrepancies in spectral and spatial alignment among sensors also hinder the precise extraction and integration of image features. Additionally, the absence of standardized benchmark datasets and evaluation metrics impedes the generalization and uniform assessment of these methods. The complexity of deep learning models also results in higher computational costs, and variations in input imagery and network structures introduce uncertainty into existing spatiotemporal fusion models. Therefore, this section discusses the current challenges facing deep learning-based spatiotemporal fusion methods and proposes possible future directions for addressing these issues.</p><sec id="sec4dot1-sensors-25-01093"><title>4.1. Land-Cover Changes</title><p>Despite the strengths of CNNs, GANs, and Transformers in spatiotemporal fusion, they struggle to capture subtle or abrupt land-cover changes. In CNN-based methods, downsampling in convolutional layers can miss small or transient variations, making it difficult to detect changes in small regions. While GANs improve global consistency and fine detail extraction, their generators focus on overall image quality, often neglecting subtle changes. Transformers, although capable of capturing long-term dependencies with self-attention, struggle to focus on small or sudden changes.</p><p>Therefore, future deep learning research on spatiotemporal fusion should prioritize identifying small or transient changes by incorporating techniques like multi-scale feature extraction and adaptive attention mechanisms. Multi-scale feature extraction, such as using multi-scale convolutional networks or hierarchical pyramid structures [<xref rid="B125-sensors-25-01093" ref-type="bibr">125</xref>], helps models capture changes across various spatial and temporal scales, improving accuracy in detecting subtle variations. This approach allows for precise detection of both local and large-scale changes, making it especially effective in complex and dynamic environments where changes vary in intensity across regions. Additionally, integrating adaptive attention mechanisms offers a promising way to enhance model performance. These mechanisms enable the model to dynamically focus on regions of interest, prioritizing areas with significant or transient changes while minimizing the computational resources spent on less relevant areas. Furthermore, attention mechanisms improve a model&#x02019;s robustness against noisy or incomplete data and make fusion methods more adaptable to varying complexities in land-cover changes. This can provide more reliable and actionable insights for applications such as land-use change detection, crop monitoring, and climate change assessment. Future research in this area will be crucial for developing more efficient and accurate deep learning-based spatiotemporal fusion models.</p></sec><sec id="sec4dot2-sensors-25-01093"><title>4.2. Sensor Differences</title><p>Sensor differences, including reflectance and geometric registration discrepancies [<xref rid="B126-sensors-25-01093" ref-type="bibr">126</xref>], significantly impact the accuracy and reliability of spatiotemporal fusion outcomes. Reflectance inconsistencies arise from variations in bandwidth, spectral response functions, and atmospheric conditions, resulting in different reflectance values for the same surface feature across sensors [<xref rid="B127-sensors-25-01093" ref-type="bibr">127</xref>]. Such reflectance discrepancies, especially between coarse- and fine-resolution images, can bias fusion results and degrade their quality. Geometric registration errors, caused by misalignment between corresponding locations, result from differences in sensor viewing angles, swath widths, and geometric correction techniques [<xref rid="B128-sensors-25-01093" ref-type="bibr">128</xref>]. In deep learning-based fusion methods, especially CNNs, model performance is heavily reliant on accurate training data, and misregistration between sensor images can cause substantial inaccuracies [<xref rid="B35-sensors-25-01093" ref-type="bibr">35</xref>].</p><p>Future research on deep learning-based spatiotemporal fusion should focus on minimizing sensor-induced errors and developing models that can effectively address reflectance and spatial registration discrepancies [<xref rid="B129-sensors-25-01093" ref-type="bibr">129</xref>]. One promising direction is to design adaptive fusion algorithms that dynamically adjust for sensor-specific discrepancies by learning the sensor-to-sensor mapping during training. Additionally, spatial and spectral alignment methods could be improved by leveraging deep learning models that learn complex geometric transformations. For example, integrating spatial transformer networks (STNs) into spatiotemporal fusion models could enable the models to automatically learn and apply optimal geometric transformations, addressing misalignments caused by satellite or sensor motion, varying pixel sizes, and other registration issues. Another promising approach is to incorporate physical models into deep learning frameworks. By embedding domain-specific physical knowledge into the neural network structure, models could maintain consistency with known physical laws while learning the fusion task. Future studies should focus on enhancing the robustness of deep learning-based fusion models by developing effective algorithms that address sensor discrepancies.</p></sec><sec id="sec4dot3-sensors-25-01093"><title>4.3. Datasets and Assessment Metrics</title><p>Benchmark datasets and evaluation metrics are crucial for advancing and comparing deep learning models. Benchmark datasets are essential for training and validating model performance, with their scale and diversity directly affecting model accuracy [<xref rid="B65-sensors-25-01093" ref-type="bibr">65</xref>]. Larger and more varied datasets significantly improve model performance, particularly for complex models with many parameters. Evaluation metrics guide the formulation of loss functions and assess feature extraction and integration capabilities. In deep learning-based fusion methods, these metrics influence the design of loss functions and the training process. Thus, creating standardized benchmark datasets and unified evaluation metrics is key to improving deep learning-based spatiotemporal fusion algorithms and their practical applications.</p><p>However, existing spatiotemporal fusion studies face challenges due to insufficient benchmark datasets (<xref rid="sensors-25-01093-t009" ref-type="table">Table 9</xref>) and a lack of unified evaluation metrics (<xref rid="sensors-25-01093-f019" ref-type="fig">Figure 19</xref>). Current research relies on limited and specific datasets, which introduce biases in geographic representation, temporal duration, and sensor types, thus restricting the generalizability and applicability of the models [<xref rid="B62-sensors-25-01093" ref-type="bibr">62</xref>]. Additionally, the evaluation metrics used in spatiotemporal fusion research are diverse and cover various dimensions, such as spectral, spatial, and visual quality. This diversity complicates comprehensive evaluations and comparisons of fusion results across studies [<xref rid="B130-sensors-25-01093" ref-type="bibr">130</xref>]. For instance, some studies use only the RMSE and r, which measure spectral feature accuracy, neglecting spatial features [<xref rid="B131-sensors-25-01093" ref-type="bibr">131</xref>]. Furthermore, redundancy exists in current evaluation metrics; for example, some studies use both the RMSE and AAD, which are highly correlated, leading to unnecessary duplication [<xref rid="B130-sensors-25-01093" ref-type="bibr">130</xref>].</p><p>Hence, future research should focus on enhancing the diversity and scale of datasets by incorporating a broader range of geographic regions, temporal durations, and sensor types to improve model generalization. Establishing benchmark datasets and standardized evaluation metrics will enhance the accuracy and applicability of future spatiotemporal fusion research. Furthermore, standardized assessment metrics should be developed for comprehensive model evaluation and comparison. For instance, Zhu et al. [<xref rid="B130-sensors-25-01093" ref-type="bibr">130</xref>] proposed a novel framework for assessing spatiotemporal fusion performance by incorporating four indicators from both spatial and spectral aspects. Their study also designed a visual polar coordinate chart, enabling cross-comparison of different fusion methods while considering input data and surface features. Additionally, Guo et al. [<xref rid="B134-sensors-25-01093" ref-type="bibr">134</xref>] introduced a new evaluation metric, the SSAM, which simultaneously evaluates both the spatial and spectral accuracy of fused imagery. Compared to existing metrics, the SSAM offers a more comprehensive and intuitive evaluation of fusion image quality, thereby facilitating cross-comparison studies of various spatiotemporal fusion methods. Therefore, future studies focusing on benchmark datasets and standardized evaluation metrics will be pivotal in advancing deep learning-based spatiotemporal fusion.</p></sec><sec id="sec4dot4-sensors-25-01093"><title>4.4. Efficiency and Uncertainty</title><p>Deep learning-based fusion methods often involve complex network architectures with a large number of parameters, leading to high computational costs and long training times. CNN- and GAN-based methods typically include hundreds of thousands of parameters, while Transformer models can exceed a million parameters. This limitation affects their feasibility for large-scale and real-time applications [<xref rid="B112-sensors-25-01093" ref-type="bibr">112</xref>]. Additionally, these methods are sensitive to input image quality and data noise, which introduces uncertainty and instability into fusion results [<xref rid="B27-sensors-25-01093" ref-type="bibr">27</xref>]. The transferability of models to new datasets or different regions remains challenging, as models trained on specific datasets may not generalize well to others due to variations in sensor characteristics and environmental conditions. Furthermore, the repeatability of results can be influenced by factors such as random initialization and hyperparameter tuning, leading to variations in performance across different training runs. These challenges further complicate the deployment of deep learning-based fusion methods in real-world applications.</p><p>Future deep learning-based spatiotemporal fusion methods should prioritize improving efficiency through algorithm optimization, model structure enhancements, and parallel computation. One approach is to develop adaptive models that adjust computational requirements based on data quality and input noise. For example, uncertainty-aware networks could be employed to automatically highlight reliable regions of input data while minimizing the influence of noisy or missing areas. Additionally, incremental learning techniques, such as lifelong learning or transfer learning [<xref rid="B135-sensors-25-01093" ref-type="bibr">135</xref>], can help models continuously learn from new data without forgetting previous knowledge. Online updating mechanisms [<xref rid="B136-sensors-25-01093" ref-type="bibr">136</xref>] could allow models to adapt in real time to incoming data streams. Federated learning approaches might also be beneficial, enabling decentralized models to train across multiple devices or locations, preserving privacy while efficiently processing large, diverse datasets. Furthermore, future innovations should explore model compression and optimization techniques using GPUs or parallel computing to reduce computational costs for practical applications.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01093"><title>5. Conclusions</title><p>Over the past decade, spatiotemporal fusion techniques for remote sensing images have become relatively mature, and deep learning methods have been extensively applied in this field. Deep learning-based spatiotemporal fusion algorithms demonstrate great advantages in both accuracy and efficiency. Yet, they also face limitations and drawbacks. This paper comprehensively reviews existing deep learning-based fusion methods, categorizing them based on neural network model principles, and outlines the research progress and current trends in this area. Through a detailed analysis of different algorithms, this paper identifies the remaining challenges in deep learning-based spatiotemporal fusion research and provides an outlook on potential future directions. The main contributions of this paper are as follows:<list list-type="order"><list-item><p>This paper provides a detailed classification of existing deep learning-based spatiotemporal fusion methods based on network structures and categorizes them into four main types: convolutional neural network-based methods, generative adversarial network-based methods, Transformer-based methods, and diffusion-based methods. This paper analyzes and compares the different principles, advantages, and disadvantages of each deep learning-based method and outlines the evolution and development of research in this area. As neural network models are increasingly being applied in spatiotemporal fusion, the comprehensive analysis and summary presented in this paper serve as a helpful resource for future research on deep learning-based spatiotemporal fusion methods.</p></list-item><list-item><p>This paper provides an in-depth exploration of deep learning-based spatiotemporal fusion methods, presenting application examples, performance evaluations, and method comparisons to assess their effectiveness and computational efficiency. By evaluating four deep learning-based fusion models from CNN-based, GAN-based, Transformer-based, and diffusion-based methods, this paper offers valuable insights into the strengths and limitations of various approaches, considering different scenarios and the impact of data heterogeneity. The analysis highlights the importance of model adaptability, computational efficiency, and robustness to data variations, providing a solid foundation for improving the performance and scalability of deep learning-based spatiotemporal fusion methods.</p></list-item><list-item><p>This paper identifies four challenges currently faced in deep learning-based spatiotemporal fusion studies. Difficulties in recognizing land-cover changes and the insufficient consideration of sensor differences are common obstacles for deep learning-based fusion models. The limited data scale, the lack of variety in spatiotemporal fusion datasets, the incompleteness and redundancy of evaluation metrics, and the low computational efficiency and uncertainty of deep learning-based models are important issues that future studies need to tackle. In response to these challenges, this paper proposes several potential solutions and provides useful references for subsequent research and applications of deep learning-based spatiotemporal fusion methods.</p></list-item></list></p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Z.L. and Y.Z.; methodology, Z.L. and Y.Z.; writing&#x02014;original draft preparation, Z.L.; writing&#x02014;review and editing, Y.Z. and W.Z.; visualization, Z.W., W.L. and X.H.; supervision, Y.Z. and W.Z.; project administration, Y.Z.; funding acquisition, Y.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Not applicable.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01093"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mewes</surname><given-names>B.</given-names></name>
<name><surname>Schumann</surname><given-names>A.H.</given-names></name>
</person-group><article-title>An agent-based extension for object-based image analysis for the delineation of irrigated agriculture from remote sensing data</article-title><source>Int. J. Remote Sens.</source><year>2019</year><volume>40</volume><fpage>4623</fpage><lpage>4641</lpage><pub-id pub-id-type="doi">10.1080/01431161.2019.1569788</pub-id></element-citation></ref><ref id="B2-sensors-25-01093"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Luo</surname><given-names>J.</given-names></name>
<name><surname>Xia</surname><given-names>L.</given-names></name>
<name><surname>Wu</surname><given-names>T.</given-names></name>
<name><surname>Gao</surname><given-names>L.</given-names></name>
<name><surname>Dong</surname><given-names>W.</given-names></name>
<name><surname>Hu</surname><given-names>X.</given-names></name>
<name><surname>Hai</surname><given-names>Y.</given-names></name>
</person-group><article-title>Geo-parcel-based crop classification in very-high-resolution images via hierarchical perception</article-title><source>Int. J. Remote Sens.</source><year>2020</year><volume>41</volume><fpage>1603</fpage><lpage>1624</lpage><pub-id pub-id-type="doi">10.1080/01431161.2019.1673916</pub-id></element-citation></ref><ref id="B3-sensors-25-01093"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aneece</surname><given-names>I.</given-names></name>
<name><surname>Thenkabail</surname><given-names>P.S.</given-names></name>
<name><surname>McCormick</surname><given-names>R.</given-names></name>
<name><surname>Alifu</surname><given-names>H.</given-names></name>
<name><surname>Foley</surname><given-names>D.</given-names></name>
<name><surname>Oliphant</surname><given-names>A.J.</given-names></name>
<name><surname>Teluguntla</surname><given-names>P.</given-names></name>
</person-group><article-title>Machine Learning and New-Generation Spaceborne Hyperspectral Data Advance Crop Type Mapping</article-title><source>Photogramm. Eng. Remote Sens.</source><year>2024</year><volume>90</volume><fpage>687</fpage><lpage>698</lpage><pub-id pub-id-type="doi">10.14358/PERS.24-00026R2</pub-id></element-citation></ref><ref id="B4-sensors-25-01093"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liang</surname><given-names>L.</given-names></name>
<name><surname>Tan</surname><given-names>B.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Kang</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
</person-group><article-title>Identifying the Driving Factors of Urban Land Surface Temperature</article-title><source>Photogramm. Eng. Remote Sens.</source><year>2022</year><volume>88</volume><fpage>233</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.14358/PERS.21-00043R3</pub-id></element-citation></ref><ref id="B5-sensors-25-01093"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Al-Doski</surname><given-names>J.</given-names></name>
<name><surname>Hassan</surname><given-names>F.M.</given-names></name>
<name><surname>Mossa</surname><given-names>H.A.</given-names></name>
<name><surname>Najim</surname><given-names>A.A.</given-names></name>
</person-group><article-title>Incorporation of digital elevation model, normalized difference vegetation index, and Landsat-8 data for land use land cover mapping</article-title><source>Photogramm. Eng. Remote Sens.</source><year>2022</year><volume>88</volume><fpage>507</fpage><lpage>516</lpage><pub-id pub-id-type="doi">10.14358/PERS.21-00082R2</pub-id></element-citation></ref><ref id="B6-sensors-25-01093"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lakshmi Priya</surname><given-names>G.</given-names></name>
<name><surname>Chandra Mouli</surname><given-names>P.</given-names></name>
<name><surname>Domnic</surname><given-names>S.</given-names></name>
<name><surname>Chemmalar Selvi</surname><given-names>G.</given-names></name>
<name><surname>Cho</surname><given-names>B.K.</given-names></name>
</person-group><article-title>Hyperspectral image classification using Walsh Hadamard transform-based key band selection and deep convolutional neural networks</article-title><source>Int. J. Remote Sens.</source><year>2024</year><volume>45</volume><fpage>1220</fpage><lpage>1249</lpage><pub-id pub-id-type="doi">10.1080/01431161.2024.2307324</pub-id></element-citation></ref><ref id="B7-sensors-25-01093"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Byerlay</surname><given-names>R.A.</given-names></name>
<name><surname>Nambiar</surname><given-names>M.K.</given-names></name>
<name><surname>Nazem</surname><given-names>A.</given-names></name>
<name><surname>Nahian</surname><given-names>M.R.</given-names></name>
<name><surname>Biglarbegian</surname><given-names>M.</given-names></name>
<name><surname>Aliabadi</surname><given-names>A.A.</given-names></name>
</person-group><article-title>Measurement of land surface temperature from oblique angle airborne thermal camera observations</article-title><source>Int. J. Remote Sens.</source><year>2020</year><volume>41</volume><fpage>3119</fpage><lpage>3146</lpage><pub-id pub-id-type="doi">10.1080/01431161.2019.1699672</pub-id></element-citation></ref><ref id="B8-sensors-25-01093"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alpers</surname><given-names>W.</given-names></name>
<name><surname>Kong</surname><given-names>W.</given-names></name>
<name><surname>Zeng</surname><given-names>K.</given-names></name>
<name><surname>Chan</surname><given-names>P.W.</given-names></name>
</person-group><article-title>On the physical mechanism causing strongly enhanced radar backscatter in C-Band SAR images of convective rain over the ocean</article-title><source>Int. J. Remote Sens.</source><year>2024</year><volume>45</volume><fpage>3827</fpage><lpage>3845</lpage><pub-id pub-id-type="doi">10.1080/01431161.2024.2354131</pub-id></element-citation></ref><ref id="B9-sensors-25-01093"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<name><surname>Cai</surname><given-names>F.</given-names></name>
<name><surname>Tian</surname><given-names>J.</given-names></name>
<name><surname>Williams</surname><given-names>T.</given-names></name>
</person-group><article-title>Spatiotemporal Fusion of Multisource Remote Sensing Data: Literature Survey, Taxonomy, Principles, Applications, and Future Directions</article-title><source>Remote Sens.</source><year>2018</year><volume>10</volume><elocation-id>527</elocation-id><pub-id pub-id-type="doi">10.3390/rs10040527</pub-id></element-citation></ref><ref id="B10-sensors-25-01093"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhukov</surname><given-names>B.</given-names></name>
<name><surname>Oertel</surname><given-names>D.</given-names></name>
<name><surname>Lanzl</surname><given-names>F.</given-names></name>
<name><surname>Reinhackel</surname><given-names>G.</given-names></name>
</person-group><article-title>Unmixing-based multisensor multiresolution image fusion</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>1999</year><volume>37</volume><fpage>1212</fpage><lpage>1226</lpage><pub-id pub-id-type="doi">10.1109/36.763276</pub-id></element-citation></ref><ref id="B11-sensors-25-01093"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>M.</given-names></name>
<name><surname>Wu</surname><given-names>C.</given-names></name>
<name><surname>Huang</surname><given-names>W.</given-names></name>
<name><surname>Niu</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>C.</given-names></name>
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Hao</surname><given-names>P.</given-names></name>
</person-group><article-title>An improved high spatial and temporal data fusion approach for combining Landsat and MODIS data to generate daily synthetic Landsat imagery</article-title><source>Inf. Fusion</source><year>2016</year><volume>31</volume><fpage>14</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2015.12.005</pub-id></element-citation></ref><ref id="B12-sensors-25-01093"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gao</surname><given-names>F.</given-names></name>
<name><surname>Masek</surname><given-names>J.</given-names></name>
<name><surname>Schwaller</surname><given-names>M.</given-names></name>
<name><surname>Hall</surname><given-names>F.</given-names></name>
</person-group><article-title>On the blending of the Landsat and MODIS surface reflectance: Predicting daily Landsat surface reflectance</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2006</year><volume>44</volume><fpage>2207</fpage><lpage>2218</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2006.872081</pub-id></element-citation></ref><ref id="B13-sensors-25-01093"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Gao</surname><given-names>F.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Masek</surname><given-names>J.G.</given-names></name>
</person-group><article-title>An enhanced spatial and temporal adaptive reflectance fusion model for complex heterogeneous regions</article-title><source>Remote Sens. Environ.</source><year>2010</year><volume>114</volume><fpage>2610</fpage><lpage>2623</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2010.05.032</pub-id></element-citation></ref><ref id="B14-sensors-25-01093"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hilker</surname><given-names>T.</given-names></name>
<name><surname>Wulder</surname><given-names>M.A.</given-names></name>
<name><surname>Coops</surname><given-names>N.C.</given-names></name>
<name><surname>Linke</surname><given-names>J.</given-names></name>
<name><surname>McDermid</surname><given-names>G.</given-names></name>
<name><surname>Masek</surname><given-names>J.G.</given-names></name>
<name><surname>Gao</surname><given-names>F.</given-names></name>
<name><surname>White</surname><given-names>J.C.</given-names></name>
</person-group><article-title>A new data fusion model for high spatial- and temporal-resolution mapping of forest disturbance based on Landsat and MODIS</article-title><source>Remote Sens. Environ.</source><year>2009</year><volume>113</volume><fpage>1613</fpage><lpage>1627</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2009.03.007</pub-id></element-citation></ref><ref id="B15-sensors-25-01093"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>A.</given-names></name>
<name><surname>Bo</surname><given-names>Y.</given-names></name>
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
<name><surname>Guo</surname><given-names>P.</given-names></name>
<name><surname>Bi</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
</person-group><article-title>Blending multi-resolution satellite sea surface temperature (SST) products using Bayesian maximum entropy method</article-title><source>Remote Sens. Environ.</source><year>2013</year><volume>135</volume><fpage>52</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2013.03.021</pub-id></element-citation></ref><ref id="B16-sensors-25-01093"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>B.</given-names></name>
<name><surname>Song</surname><given-names>H.</given-names></name>
</person-group><article-title>Spatiotemporal Reflectance Fusion via Sparse Representation</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2012</year><volume>50</volume><fpage>3707</fpage><lpage>3716</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2012.2186638</pub-id></element-citation></ref><ref id="B17-sensors-25-01093"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<name><surname>Helmer</surname><given-names>E.H.</given-names></name>
<name><surname>Gao</surname><given-names>F.</given-names></name>
<name><surname>Liu</surname><given-names>D.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Lefsky</surname><given-names>M.A.</given-names></name>
</person-group><article-title>A flexible spatiotemporal method for fusing satellite images with different resolutions</article-title><source>Remote Sens. Environ.</source><year>2016</year><volume>172</volume><fpage>165</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2015.11.016</pub-id></element-citation></ref><ref id="B18-sensors-25-01093"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Moosavi</surname><given-names>V.</given-names></name>
<name><surname>Talebi</surname><given-names>A.</given-names></name>
<name><surname>Mokhtari</surname><given-names>M.H.</given-names></name>
<name><surname>Shamsi</surname><given-names>S.R.F.</given-names></name>
<name><surname>Niazi</surname><given-names>Y.</given-names></name>
</person-group><article-title>A wavelet-artificial intelligence fusion approach (WAIFA) for blending Landsat and MODIS surface temperature</article-title><source>Remote Sens. Environ.</source><year>2015</year><volume>169</volume><fpage>243</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2015.08.015</pub-id></element-citation></ref><ref id="B19-sensors-25-01093"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Deng</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Huang</surname><given-names>G.B.</given-names></name>
<name><surname>Zhao</surname><given-names>B.</given-names></name>
<name><surname>Lauren</surname><given-names>P.</given-names></name>
</person-group><article-title>Fast and Accurate Spatiotemporal Fusion Based Upon Extreme Learning Machine</article-title><source>IEEE Geosci. Remote Sensing Lett.</source><year>2016</year><volume>13</volume><fpage>2039</fpage><lpage>2043</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2016.2622726</pub-id></element-citation></ref><ref id="B20-sensors-25-01093"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fung</surname><given-names>C.H.</given-names></name>
<name><surname>Wong</surname><given-names>M.S.</given-names></name>
<name><surname>Chan</surname><given-names>P.W.</given-names></name>
</person-group><article-title>Spatio-Temporal Data Fusion for Satellite Images Using Hopfield Neural Network</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><elocation-id>2077</elocation-id><pub-id pub-id-type="doi">10.3390/rs11182077</pub-id></element-citation></ref><ref id="B21-sensors-25-01093"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Krizhevsky</surname><given-names>A.</given-names></name>
<name><surname>Sutskever</surname><given-names>I.</given-names></name>
<name><surname>Hinton</surname><given-names>G.E.</given-names></name>
</person-group><article-title>ImageNet classification with deep convolutional neural networks</article-title><source>Commun. ACM</source><year>2017</year><volume>60</volume><fpage>84</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1145/3065386</pub-id></element-citation></ref><ref id="B22-sensors-25-01093"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Goodfellow</surname><given-names>I.</given-names></name>
<name><surname>Pouget-Abadie</surname><given-names>J.</given-names></name>
<name><surname>Mirza</surname><given-names>M.</given-names></name>
<name><surname>Xu</surname><given-names>B.</given-names></name>
<name><surname>Warde-Farley</surname><given-names>D.</given-names></name>
<name><surname>Ozair</surname><given-names>S.</given-names></name>
<name><surname>Courville</surname><given-names>A.</given-names></name>
<name><surname>Bengio</surname><given-names>Y.</given-names></name>
</person-group><person-group person-group-type="editor">
<name><surname>Ghahramani</surname><given-names>Z.</given-names></name>
<name><surname>Welling</surname><given-names>M.</given-names></name>
<name><surname>Cortes</surname><given-names>C.</given-names></name>
<name><surname>Lawrence</surname><given-names>N.</given-names></name>
<name><surname>Weinberger</surname><given-names>K.</given-names></name>
</person-group><article-title>Generative Adversarial Nets</article-title><source>Proceedings of the Advances in Neural Information Processing Systems</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>8&#x02013;13 December 2014</conf-date><publisher-name>Curran Associates, Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2014</year><volume>Volume 27</volume></element-citation></ref><ref id="B23-sensors-25-01093"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Vaswani</surname><given-names>A.</given-names></name>
<name><surname>Shazeer</surname><given-names>N.</given-names></name>
<name><surname>Parmar</surname><given-names>N.</given-names></name>
<name><surname>Uszkoreit</surname><given-names>J.</given-names></name>
<name><surname>Jones</surname><given-names>L.</given-names></name>
<name><surname>Gomez</surname><given-names>A.N.</given-names></name>
<name><surname>Kaiser</surname><given-names>L.U.</given-names></name>
<name><surname>Polosukhin</surname><given-names>I.</given-names></name>
</person-group><person-group person-group-type="editor">
<name><surname>Guyon</surname><given-names>I.</given-names></name>
<name><surname>Luxburg</surname><given-names>U.V.</given-names></name>
<name><surname>Bengio</surname><given-names>S.</given-names></name>
<name><surname>Wallach</surname><given-names>H.</given-names></name>
<name><surname>Fergus</surname><given-names>R.</given-names></name>
<name><surname>Vishwanathan</surname><given-names>S.</given-names></name>
<name><surname>Garnett</surname><given-names>R.</given-names></name>
</person-group><article-title>Attention is All you Need</article-title><source>Proceedings of the Advances in Neural Information Processing Systems</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>4&#x02013;9 December 2017</conf-date><publisher-name>Curran Associates, Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2017</year><volume>Volume 30</volume></element-citation></ref><ref id="B24-sensors-25-01093"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Ho</surname><given-names>J.</given-names></name>
<name><surname>Jain</surname><given-names>A.</given-names></name>
<name><surname>Abbeel</surname><given-names>P.</given-names></name>
</person-group><article-title>Denoising Diffusion Probabilistic Models</article-title><source>Advances in Neural Information Processing Systems</source><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2020</year><volume>Volume 33</volume><fpage>6840</fpage><lpage>6851</lpage></element-citation></ref><ref id="B25-sensors-25-01093"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Belgiu</surname><given-names>M.</given-names></name>
<name><surname>Stein</surname><given-names>A.</given-names></name>
</person-group><article-title>Spatiotemporal Image Fusion in Remote Sensing</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><elocation-id>818</elocation-id><pub-id pub-id-type="doi">10.3390/rs11070818</pub-id></element-citation></ref><ref id="B26-sensors-25-01093"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>Z.</given-names></name>
<name><surname>Yue</surname><given-names>P.</given-names></name>
<name><surname>Di</surname><given-names>L.</given-names></name>
<name><surname>Tang</surname><given-names>J.</given-names></name>
</person-group><article-title>Deriving High Spatiotemporal Remote Sensing Images Using Deep Convolutional Network</article-title><source>Remote Sens.</source><year>2018</year><volume>10</volume><elocation-id>1066</elocation-id><pub-id pub-id-type="doi">10.3390/rs10071066</pub-id></element-citation></ref><ref id="B27-sensors-25-01093"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>Z.</given-names></name>
<name><surname>Gao</surname><given-names>M.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Jiang</surname><given-names>L.</given-names></name>
</person-group><article-title>A Flexible Reference-Insensitive Spatiotemporal Fusion Model for Remote Sensing Images Using Conditional Generative Adversarial Network</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2021</year><volume>60</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2021.3050551</pub-id></element-citation></ref><ref id="B28-sensors-25-01093"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Cao</surname><given-names>D.</given-names></name>
<name><surname>Peng</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>C.</given-names></name>
</person-group><article-title>MSNet: A Multi-Stream Fusion Network for Remote Sensing Spatiotemporal Fusion Based on Transformer and Convolution</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>3724</elocation-id><pub-id pub-id-type="doi">10.3390/rs13183724</pub-id></element-citation></ref><ref id="B29-sensors-25-01093"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>H.</given-names></name>
<name><surname>He</surname><given-names>W.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Xia</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
</person-group><article-title>STFDiff: Remote sensing image spatiotemporal fusion with diffusion models</article-title><source>Inf. Fusion</source><year>2024</year><volume>111</volume><fpage>102505</fpage><pub-id pub-id-type="doi">10.1016/j.inffus.2024.102505</pub-id></element-citation></ref><ref id="B30-sensors-25-01093"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>LeCun</surname><given-names>Y.</given-names></name>
<name><surname>Boser</surname><given-names>B.</given-names></name>
<name><surname>Denker</surname><given-names>J.S.</given-names></name>
<name><surname>Henderson</surname><given-names>D.</given-names></name>
<name><surname>Howard</surname><given-names>R.E.</given-names></name>
<name><surname>Hubbard</surname><given-names>W.</given-names></name>
<name><surname>Jackel</surname><given-names>L.D.</given-names></name>
</person-group><article-title>Backpropagation Applied to Handwritten Zip Code Recognition</article-title><source>Neural Comput.</source><year>1989</year><volume>1</volume><fpage>541</fpage><lpage>551</lpage><pub-id pub-id-type="doi">10.1162/neco.1989.1.4.541</pub-id></element-citation></ref><ref id="B31-sensors-25-01093"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="B32-sensors-25-01093"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mnih</surname><given-names>V.</given-names></name>
<name><surname>Heess</surname><given-names>N.</given-names></name>
<name><surname>Graves</surname><given-names>A.</given-names></name>
<name><surname>Kavukcuoglu</surname><given-names>K.</given-names></name>
</person-group><article-title>Recurrent Models of Visual Attention</article-title><source>Proceedings of the Advances in Neural Information Processing Systems</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>8&#x02013;13 December 2014</conf-date><person-group person-group-type="editor">
<name><surname>Ghahramani</surname><given-names>Z.</given-names></name>
<name><surname>Welling</surname><given-names>M.</given-names></name>
<name><surname>Cortes</surname><given-names>C.</given-names></name>
<name><surname>Lawrence</surname><given-names>N.</given-names></name>
<name><surname>Weinberger</surname><given-names>K.</given-names></name>
</person-group><publisher-name>Curran Associates, Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2014</year><volume>Volume 27</volume></element-citation></ref><ref id="B33-sensors-25-01093"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>H.</given-names></name>
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Wang</surname><given-names>G.</given-names></name>
<name><surname>Hang</surname><given-names>R.</given-names></name>
<name><surname>Huang</surname><given-names>B.</given-names></name>
</person-group><article-title>Spatiotemporal Satellite Image Fusion Using Deep Convolutional Neural Networks</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2018</year><volume>11</volume><fpage>821</fpage><lpage>829</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2018.2797894</pub-id></element-citation></ref><ref id="B34-sensors-25-01093"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Yang</surname><given-names>C.</given-names></name>
<name><surname>Peng</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
</person-group><article-title>A Multi-Cooperative Deep Convolutional Neural Network for Spatiotemporal Satellite Image Fusion</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2021</year><volume>14</volume><fpage>10174</fpage><lpage>10188</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2021.3113163</pub-id></element-citation></ref><ref id="B35-sensors-25-01093"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>Y.</given-names></name>
<name><surname>Song</surname><given-names>H.</given-names></name>
<name><surname>Sun</surname><given-names>L.</given-names></name>
<name><surname>Wu</surname><given-names>Z.</given-names></name>
<name><surname>Jeon</surname><given-names>B.</given-names></name>
</person-group><article-title>Spatiotemporal Fusion of Satellite Images via Very Deep Convolutional Networks</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><elocation-id>2701</elocation-id><pub-id pub-id-type="doi">10.3390/rs11222701</pub-id></element-citation></ref><ref id="B36-sensors-25-01093"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Peng</surname><given-names>M.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Sun</surname><given-names>X.</given-names></name>
<name><surname>Cen</surname><given-names>Y.</given-names></name>
<name><surname>Zhao</surname><given-names>X.</given-names></name>
</person-group><article-title>A Synchronous Long Time-Series Completion Method Using 3-D Fully Convolutional Neural Networks</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2022</year><volume>19</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2021.3055847</pub-id></element-citation></ref><ref id="B37-sensors-25-01093"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shao</surname><given-names>Z.</given-names></name>
<name><surname>Cai</surname><given-names>J.</given-names></name>
<name><surname>Fu</surname><given-names>P.</given-names></name>
<name><surname>Hu</surname><given-names>L.</given-names></name>
<name><surname>Liu</surname><given-names>T.</given-names></name>
</person-group><article-title>Deep learning-based fusion of Landsat-8 and Sentinel-2 images for a harmonized surface reflectance product</article-title><source>Remote Sens. Environ.</source><year>2019</year><volume>235</volume><fpage>111425</fpage><pub-id pub-id-type="doi">10.1016/j.rse.2019.111425</pub-id></element-citation></ref><ref id="B38-sensors-25-01093"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qin</surname><given-names>P.</given-names></name>
<name><surname>Huang</surname><given-names>H.</given-names></name>
<name><surname>Tang</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>C.</given-names></name>
</person-group><article-title>MUSTFN: A spatiotemporal fusion method for multi-scale and multi-sensor remote sensing images based on a convolutional neural network</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2022</year><volume>115</volume><fpage>103113</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2022.103113</pub-id></element-citation></ref><ref id="B39-sensors-25-01093"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Deng</surname><given-names>C.</given-names></name>
<name><surname>Chanussot</surname><given-names>J.</given-names></name>
<name><surname>Hong</surname><given-names>D.</given-names></name>
<name><surname>Zhao</surname><given-names>B.</given-names></name>
</person-group><article-title><italic toggle="yes">StfNet</italic>: A Two-Stream Convolutional Neural Network for Spatiotemporal Image Fusion</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2019</year><volume>57</volume><fpage>6552</fpage><lpage>6564</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2019.2907310</pub-id></element-citation></ref><ref id="B40-sensors-25-01093"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Shao</surname><given-names>Z.</given-names></name>
<name><surname>Huang</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>D.</given-names></name>
</person-group><article-title>Spatiotemporal Temperature Fusion Based on a Deep Convolutional Network</article-title><source>Photogramm Eng Remote Sens.</source><year>2022</year><volume>88</volume><fpage>93</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.14358/PERS.21-00023R2</pub-id></element-citation></ref><ref id="B41-sensors-25-01093"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jia</surname><given-names>D.</given-names></name>
<name><surname>Song</surname><given-names>C.</given-names></name>
<name><surname>Cheng</surname><given-names>C.</given-names></name>
<name><surname>Shen</surname><given-names>S.</given-names></name>
<name><surname>Ning</surname><given-names>L.</given-names></name>
<name><surname>Hui</surname><given-names>C.</given-names></name>
</person-group><article-title>A Novel Deep Learning-Based Spatiotemporal Fusion Method for Combining Satellite Images with Different Resolutions Using a Two-Stream Convolutional Neural Network</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><elocation-id>698</elocation-id><pub-id pub-id-type="doi">10.3390/rs12040698</pub-id></element-citation></ref><ref id="B42-sensors-25-01093"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>You</surname><given-names>M.</given-names></name>
<name><surname>Meng</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Shao</surname><given-names>F.</given-names></name>
<name><surname>Fu</surname><given-names>R.</given-names></name>
</person-group><article-title>CIG-STF: Change Information Guided Spatiotemporal Fusion for Remote Sensing Images</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2024</year><volume>62</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2024.3491111</pub-id></element-citation></ref><ref id="B43-sensors-25-01093"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>L.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Plaza</surname><given-names>A.</given-names></name>
</person-group><article-title>A new sensor bias-driven spatio-temporal fusion model based on convolutional neural networks</article-title><source>Sci. China Inf. Sci.</source><year>2020</year><volume>63</volume><fpage>140302</fpage><pub-id pub-id-type="doi">10.1007/s11432-019-2805-y</pub-id></element-citation></ref><ref id="B44-sensors-25-01093"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Liang</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>M.</given-names></name>
</person-group><article-title>A New Spatial&#x02013;Temporal Depthwise Separable Convolutional Fusion Network for Generating Landsat 8-Day Surface Reflectance Time Series over Forest Regions</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>2199</elocation-id><pub-id pub-id-type="doi">10.3390/rs14092199</pub-id></element-citation></ref><ref id="B45-sensors-25-01093"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>Z.</given-names></name>
<name><surname>Di</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Guo</surname><given-names>L.</given-names></name>
<name><surname>Gao</surname><given-names>M.</given-names></name>
</person-group><article-title>An Enhanced Deep Convolutional Model for Spatiotemporal Image Fusion</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><elocation-id>2898</elocation-id><pub-id pub-id-type="doi">10.3390/rs11242898</pub-id></element-citation></ref><ref id="B46-sensors-25-01093"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Wu</surname><given-names>F.</given-names></name>
<name><surname>Cao</surname><given-names>D.</given-names></name>
</person-group><article-title>Dual-Branch Remote Sensing Spatiotemporal Fusion Network Based on Selection Kernel Mechanism</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>4282</elocation-id><pub-id pub-id-type="doi">10.3390/rs14174282</pub-id></element-citation></ref><ref id="B47-sensors-25-01093"><label>47.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>C.</given-names></name>
<name><surname>Yan</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Plaza</surname><given-names>A.</given-names></name>
<name><surname>Li</surname><given-names>B.</given-names></name>
</person-group><article-title>A New Spatio-Temporal Fusion Method for Remotely Sensed Data Based on Convolutional Neural Networks</article-title><source>Proceedings of the IGARSS 2019&#x02014;2019 IEEE International Geoscience and Remote Sensing Symposium</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>28 July&#x02013;2 August 2019</conf-date><fpage>835</fpage><lpage>838</lpage><pub-id pub-id-type="doi">10.1109/IGARSS.2019.8898524</pub-id></element-citation></ref><ref id="B48-sensors-25-01093"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hoque</surname><given-names>M.R.U.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Kwan</surname><given-names>C.</given-names></name>
<name><surname>Koperski</surname><given-names>K.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>ArithFusion: An Arithmetic Deep Model for Temporal Remote Sensing Image Fusion</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>6160</elocation-id><pub-id pub-id-type="doi">10.3390/rs14236160</pub-id></element-citation></ref><ref id="B49-sensors-25-01093"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Peng</surname><given-names>Y.</given-names></name>
<name><surname>Dong</surname><given-names>M.</given-names></name>
</person-group><article-title>DMNet: A Network Architecture Using Dilated Convolution and Multiscale Mechanisms for Spatiotemporal Fusion of Remote Sensing Images</article-title><source>IEEE Sensors J.</source><year>2020</year><volume>20</volume><fpage>12190</fpage><lpage>12202</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2020.3000249</pub-id></element-citation></ref><ref id="B50-sensors-25-01093"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jia</surname><given-names>D.</given-names></name>
<name><surname>Cheng</surname><given-names>C.</given-names></name>
<name><surname>Shen</surname><given-names>S.</given-names></name>
<name><surname>Ning</surname><given-names>L.</given-names></name>
</person-group><article-title>Multitask Deep Learning Framework for Spatiotemporal Fusion of NDVI</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2021.3140144</pub-id></element-citation></ref><ref id="B51-sensors-25-01093"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Peng</surname><given-names>M.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Sun</surname><given-names>X.</given-names></name>
<name><surname>Cen</surname><given-names>Y.</given-names></name>
<name><surname>Zhao</surname><given-names>X.</given-names></name>
</person-group><article-title>A Fast Three-Dimensional Convolutional Neural Network-Based Spatiotemporal Fusion Method (STF3DCNN) Using a Spatial-Temporal-Spectral Dataset</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><elocation-id>3888</elocation-id><pub-id pub-id-type="doi">10.3390/rs12233888</pub-id></element-citation></ref><ref id="B52-sensors-25-01093"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xiong</surname><given-names>S.</given-names></name>
<name><surname>Du</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ouyang</surname><given-names>S.</given-names></name>
<name><surname>Cui</surname><given-names>W.</given-names></name>
</person-group><article-title>Fusing Landsat-7, Landsat-8 and Sentinel-2 surface reflectance to generate dense time series images with 10m spatial resolution</article-title><source>Int. J. Remote. Sens.</source><year>2022</year><volume>43</volume><fpage>1630</fpage><lpage>1654</lpage><pub-id pub-id-type="doi">10.1080/01431161.2022.2047240</pub-id></element-citation></ref><ref id="B53-sensors-25-01093"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
</person-group><article-title>Spatiotemporal Fusion of Remote Sensing Image Based on Deep Learning</article-title><source>J. Sens.</source><year>2020</year><volume>2020</volume><fpage>8873079</fpage><pub-id pub-id-type="doi">10.1155/2020/8873079</pub-id></element-citation></ref><ref id="B54-sensors-25-01093"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fang</surname><given-names>S.</given-names></name>
<name><surname>Meng</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Cao</surname><given-names>Y.</given-names></name>
</person-group><article-title>Two-stream spatiotemporal image fusion network based on difference transformation</article-title><source>J. Appl. Remote Sens.</source><year>2022</year><volume>16</volume><fpage>038506</fpage><pub-id pub-id-type="doi">10.1117/1.JRS.16.038506</pub-id></element-citation></ref><ref id="B55-sensors-25-01093"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jia</surname><given-names>D.</given-names></name>
<name><surname>Cheng</surname><given-names>C.</given-names></name>
<name><surname>Song</surname><given-names>C.</given-names></name>
<name><surname>Shen</surname><given-names>S.</given-names></name>
<name><surname>Ning</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>T.</given-names></name>
</person-group><article-title>A Hybrid Deep Learning-Based Spatiotemporal Fusion Method for Combining Satellite Images with Different Resolutions</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>645</elocation-id><pub-id pub-id-type="doi">10.3390/rs13040645</pub-id></element-citation></ref><ref id="B56-sensors-25-01093"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cai</surname><given-names>J.</given-names></name>
<name><surname>Huang</surname><given-names>B.</given-names></name>
<name><surname>Fung</surname><given-names>T.</given-names></name>
</person-group><article-title>Progressive spatiotemporal image fusion with deep neural networks</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2022</year><volume>108</volume><fpage>102745</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2022.102745</pub-id></element-citation></ref><ref id="B57-sensors-25-01093"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Htitiou</surname><given-names>A.</given-names></name>
<name><surname>Boudhar</surname><given-names>A.</given-names></name>
<name><surname>Benabdelouahab</surname><given-names>T.</given-names></name>
</person-group><article-title>Deep Learning-Based Spatiotemporal Fusion Approach for Producing High-Resolution NDVI Time-Series Datasets</article-title><source>Can. J. Remote. Sens.</source><year>2021</year><volume>47</volume><fpage>182</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1080/07038992.2020.1865141</pub-id></element-citation></ref><ref id="B58-sensors-25-01093"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wei</surname><given-names>J.</given-names></name>
<name><surname>Yang</surname><given-names>H.</given-names></name>
<name><surname>Tang</surname><given-names>W.</given-names></name>
<name><surname>Li</surname><given-names>Q.</given-names></name>
</person-group><article-title>Spatiotemporal-Spectral Fusion for Gaofen-1 Satellite Images</article-title><source>IEEE Geosci. Remote Sensing Lett.</source><year>2022</year><volume>19</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2021.3111961</pub-id></element-citation></ref><ref id="B59-sensors-25-01093"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wei</surname><given-names>J.</given-names></name>
<name><surname>Tang</surname><given-names>W.</given-names></name>
<name><surname>He</surname><given-names>C.</given-names></name>
</person-group><article-title>Enblending Mosaicked Remote Sensing Images with Spatiotemporal Fusion of Convolutional Neural Networks</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2021</year><volume>14</volume><fpage>5891</fpage><lpage>5902</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2021.3082619</pub-id></element-citation></ref><ref id="B60-sensors-25-01093"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Erdem</surname><given-names>F.</given-names></name>
<name><surname>Avdan</surname><given-names>U.</given-names></name>
</person-group><article-title>STFRDN: A residual dense network for remote sensing image spatiotemporal fusion</article-title><source>Int. J. Remote Sens.</source><year>2023</year><volume>44</volume><fpage>3259</fpage><lpage>3277</lpage><pub-id pub-id-type="doi">10.1080/01431161.2023.2221800</pub-id></element-citation></ref><ref id="B61-sensors-25-01093"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>F.</given-names></name>
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Yu</surname><given-names>B.</given-names></name>
</person-group><article-title>ACFNet: A Feature Fusion Network for Glacial Lake Extraction Based on Optical and Synthetic Aperture Radar Images</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>5091</elocation-id><pub-id pub-id-type="doi">10.3390/rs13245091</pub-id></element-citation></ref><ref id="B62-sensors-25-01093"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xiao</surname><given-names>J.</given-names></name>
<name><surname>Aggarwal</surname><given-names>A.K.</given-names></name>
<name><surname>Rage</surname><given-names>U.K.</given-names></name>
<name><surname>Katiyar</surname><given-names>V.</given-names></name>
<name><surname>Avtar</surname><given-names>R.</given-names></name>
</person-group><article-title>Deep Learning-Based Spatiotemporal Fusion of Unmanned Aerial Vehicle and Satellite Reflectance Images for Crop Monitoring</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>85600</fpage><lpage>85614</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3297513</pub-id></element-citation></ref><ref id="B63-sensors-25-01093"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bai</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>W.</given-names></name>
<name><surname>Yang</surname><given-names>Z.</given-names></name>
<name><surname>Yu</surname><given-names>J.</given-names></name>
<name><surname>Zhao</surname><given-names>B.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Yang</surname><given-names>H.</given-names></name>
<name><surname>Mas</surname><given-names>E.</given-names></name>
<name><surname>Koshimura</surname><given-names>S.</given-names></name>
</person-group><article-title>Enhancement of Detecting Permanent Water and Temporary Water in Flood Disasters by Fusing Sentinel-1 and Sentinel-2 Imagery Using Deep Learning Algorithms: Demonstration of Sen1Floods11 Benchmark Datasets</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>2220</elocation-id><pub-id pub-id-type="doi">10.3390/rs13112220</pub-id></element-citation></ref><ref id="B64-sensors-25-01093"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zeng</surname><given-names>Y.</given-names></name>
<name><surname>Gao</surname><given-names>B.</given-names></name>
<name><surname>Liu</surname><given-names>P.</given-names></name>
<name><surname>Zhao</surname><given-names>X.</given-names></name>
</person-group><article-title>Spatiotemporal Fusion for Nighttime Light Remote Sensing Images With Multivariate Activation Function</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2024</year><volume>21</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2023.3324480</pub-id></element-citation></ref><ref id="B65-sensors-25-01093"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yin</surname><given-names>Z.</given-names></name>
<name><surname>Wu</surname><given-names>P.</given-names></name>
<name><surname>Foody</surname><given-names>G.M.</given-names></name>
<name><surname>Wu</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Du</surname><given-names>Y.</given-names></name>
<name><surname>Ling</surname><given-names>F.</given-names></name>
</person-group><article-title>Spatiotemporal Fusion of Land Surface Temperature Based on a Convolutional Neural Network</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2021</year><volume>59</volume><fpage>1808</fpage><lpage>1822</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2020.2999943</pub-id></element-citation></ref><ref id="B66-sensors-25-01093"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fu</surname><given-names>R.</given-names></name>
<name><surname>Hu</surname><given-names>H.</given-names></name>
<name><surname>Wu</surname><given-names>N.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Jin</surname><given-names>W.</given-names></name>
</person-group><article-title>Spatiotemporal fusion convolutional neural network: Tropical cyclone intensity estimation from multisource remote sensing images</article-title><source>J. Appl. Remote Sens.</source><year>2024</year><volume>18</volume><fpage>018501</fpage><pub-id pub-id-type="doi">10.1117/1.JRS.18.018501</pub-id></element-citation></ref><ref id="B67-sensors-25-01093"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Shi</surname><given-names>K.</given-names></name>
<name><surname>Ge</surname><given-names>Y.</given-names></name>
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
</person-group><article-title>Spatiotemporal Remote Sensing Image Fusion Using Multiscale Two-Stream Convolutional Neural Networks</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2021.3069116</pub-id></element-citation></ref><ref id="B68-sensors-25-01093"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Peng</surname><given-names>Y.</given-names></name>
<name><surname>Dong</surname><given-names>M.</given-names></name>
</person-group><article-title>Spatiotemporal Fusion of Remote Sensing Images using a Convolutional Neural Network with Attention and Multiscale Mechanisms</article-title><source>Int. J. Remote. Sens.</source><year>2021</year><volume>42</volume><fpage>1973</fpage><lpage>1993</lpage><pub-id pub-id-type="doi">10.1080/01431161.2020.1809742</pub-id></element-citation></ref><ref id="B69-sensors-25-01093"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Lin</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>T.</given-names></name>
<name><surname>Cheng</surname><given-names>Q.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Shen</surname><given-names>H.</given-names></name>
</person-group><article-title>Fusing Landsat 8 and Sentinel-2 data for 10-m dense time-series imagery using a degradation-term constrained deep network</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2022</year><volume>108</volume><fpage>102738</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2022.102738</pub-id></element-citation></ref><ref id="B70-sensors-25-01093"><label>70.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ao</surname><given-names>Z.</given-names></name>
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Xin</surname><given-names>Q.</given-names></name>
</person-group><article-title>Constructing 10-m NDVI Time Series From Landsat 8 and Sentinel 2 Images Using Convolutional Neural Networks</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2021</year><volume>18</volume><fpage>1461</fpage><lpage>1465</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2020.3003322</pub-id></element-citation></ref><ref id="B71-sensors-25-01093"><label>71.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Cui</surname><given-names>D.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Peng</surname><given-names>J.</given-names></name>
</person-group><article-title>Applying deep-learning enhanced fusion methods for improved NDVI reconstruction and long-term vegetation cover study: A case of the Danjiang River Basin</article-title><source>Ecol. Indic.</source><year>2023</year><volume>155</volume><fpage>111088</fpage><pub-id pub-id-type="doi">10.1016/j.ecolind.2023.111088</pub-id></element-citation></ref><ref id="B72-sensors-25-01093"><label>72.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Yang</surname><given-names>C.</given-names></name>
<name><surname>Peng</surname><given-names>Y.</given-names></name>
<name><surname>Du</surname><given-names>J.</given-names></name>
</person-group><article-title>A Pseudo-Siamese Deep Convolutional Neural Network for Spatiotemporal Satellite Image Fusion</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2022</year><volume>15</volume><fpage>1205</fpage><lpage>1220</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2022.3143464</pub-id></element-citation></ref><ref id="B73-sensors-25-01093"><label>73.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>L.</given-names></name>
<name><surname>Shen</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Nan</surname><given-names>F.</given-names></name>
</person-group><article-title>CAFE: A Cross-Attention Based Adaptive Weighting Fusion Network for MODIS and Landsat Spatiotemporal Fusion</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2023</year><volume>20</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2023.3286463</pub-id></element-citation></ref><ref id="B74-sensors-25-01093"><label>74.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>H.</given-names></name>
<name><surname>Xiao</surname><given-names>W.</given-names></name>
</person-group><article-title>Similarity Weight Learning: A New Spatial and Temporal Satellite Image Fusion Framework</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3161070</pub-id></element-citation></ref><ref id="B75-sensors-25-01093"><label>75.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cai</surname><given-names>Z.</given-names></name>
<name><surname>Hu</surname><given-names>Q.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Wei</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Zeng</surname><given-names>Y.</given-names></name>
<name><surname>Yin</surname><given-names>G.</given-names></name>
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>You</surname><given-names>L.</given-names></name>
<etal/>
</person-group><article-title>Improving agricultural field parcel delineation with a dual branch spatiotemporal fusion network by integrating multimodal satellite data</article-title><source>ISPRS J. Photogramm. Remote. Sens.</source><year>2023</year><volume>205</volume><fpage>34</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2023.09.021</pub-id></element-citation></ref><ref id="B76-sensors-25-01093"><label>76.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cheng</surname><given-names>F.</given-names></name>
<name><surname>Fu</surname><given-names>Z.</given-names></name>
<name><surname>Tang</surname><given-names>B.</given-names></name>
<name><surname>Huang</surname><given-names>L.</given-names></name>
<name><surname>Huang</surname><given-names>K.</given-names></name>
<name><surname>Ji</surname><given-names>X.</given-names></name>
</person-group><article-title>STF-EGFA: A Remote Sensing Spatiotemporal Fusion Network with Edge-Guided Feature Attention</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>3057</elocation-id><pub-id pub-id-type="doi">10.3390/rs14133057</pub-id></element-citation></ref><ref id="B77-sensors-25-01093"><label>77.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ran</surname><given-names>Q.</given-names></name>
<name><surname>Wang</surname><given-names>Q.</given-names></name>
<name><surname>Zheng</surname><given-names>K.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>Multiscale Attention Spatiotemporal Fusion Model Based on Pyramidal Network Constraints</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2024</year><volume>21</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2024.3432938</pub-id></element-citation></ref><ref id="B78-sensors-25-01093"><label>78.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lei</surname><given-names>D.</given-names></name>
<name><surname>Huang</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>W.</given-names></name>
</person-group><article-title>SCRNet: An efficient spatial channel attention residual network for spatiotemporal fusion</article-title><source>J. Appl. Remote Sens.</source><year>2022</year><volume>16</volume><fpage>036512</fpage><pub-id pub-id-type="doi">10.1117/1.JRS.16.036512</pub-id></element-citation></ref><ref id="B79-sensors-25-01093"><label>79.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Tan</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
</person-group><article-title>Enhanced wavelet based spatiotemporal fusion networks using cross-paired remote sensing images</article-title><source>ISPRS J. Photogramm. Remote Sens.</source><year>2024</year><volume>211</volume><fpage>281</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2024.04.016</pub-id></element-citation></ref><ref id="B80-sensors-25-01093"><label>80.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cao</surname><given-names>H.</given-names></name>
<name><surname>Luo</surname><given-names>X.</given-names></name>
<name><surname>Peng</surname><given-names>Y.</given-names></name>
<name><surname>Xie</surname><given-names>T.</given-names></name>
</person-group><article-title>MANet: A Network Architecture for Remote Sensing Spatiotemporal Fusion Based on Multiscale and Attention Mechanisms</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>4600</elocation-id><pub-id pub-id-type="doi">10.3390/rs14184600</pub-id></element-citation></ref><ref id="B81-sensors-25-01093"><label>81.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cui</surname><given-names>D.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Zhao</surname><given-names>C.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
</person-group><article-title>A Novel Remote Sensing Spatiotemporal Data Fusion Framework Based on the Combination of Deep-Learning Downscaling and Traditional Fusion Algorithm</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2024</year><volume>17</volume><fpage>7957</fpage><lpage>7970</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2024.3382136</pub-id></element-citation></ref><ref id="B82-sensors-25-01093"><label>82.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>Z.</given-names></name>
<name><surname>Diao</surname><given-names>C.</given-names></name>
<name><surname>Li</surname><given-names>B.</given-names></name>
</person-group><article-title>A Robust Hybrid Deep Learning Model for Spatiotemporal Image Fusion</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>5005</elocation-id><pub-id pub-id-type="doi">10.3390/rs13245005</pub-id></element-citation></ref><ref id="B83-sensors-25-01093"><label>83.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhan</surname><given-names>W.</given-names></name>
<name><surname>Luo</surname><given-names>F.</given-names></name>
<name><surname>Luo</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>Y.</given-names></name>
<name><surname>Yin</surname><given-names>Z.</given-names></name>
<name><surname>Wu</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>P.</given-names></name>
</person-group><article-title>Time-Series-Based Spatiotemporal Fusion Network for Improving Crop Type Mapping</article-title><source>Remote Sens.</source><year>2024</year><volume>16</volume><elocation-id>235</elocation-id><pub-id pub-id-type="doi">10.3390/rs16020235</pub-id></element-citation></ref><ref id="B84-sensors-25-01093"><label>84.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wei</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>L.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Huang</surname><given-names>Y.</given-names></name>
</person-group><article-title>An Experimental Study of the Accuracy and Change Detection Potential of Blending Time Series Remote Sensing Images with Spatiotemporal Fusion</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>3763</elocation-id><pub-id pub-id-type="doi">10.3390/rs15153763</pub-id></element-citation></ref><ref id="B85-sensors-25-01093"><label>85.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>X.</given-names></name>
<name><surname>Feng</surname><given-names>R.</given-names></name>
<name><surname>Fan</surname><given-names>J.</given-names></name>
<name><surname>Han</surname><given-names>W.</given-names></name>
<name><surname>Yu</surname><given-names>S.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
</person-group><article-title>MSISR-STF: Spatiotemporal Fusion via Multilevel Single-Image Super-Resolution</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>5675</elocation-id><pub-id pub-id-type="doi">10.3390/rs15245675</pub-id></element-citation></ref><ref id="B86-sensors-25-01093"><label>86.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Ronneberger</surname><given-names>O.</given-names></name>
<name><surname>Fischer</surname><given-names>P.</given-names></name>
<name><surname>Brox</surname><given-names>T.</given-names></name>
</person-group><article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title><source>Medical Image Computing and Computer-Assisted Intervention&#x02014;MICCAI 2015</source><person-group person-group-type="editor">
<name><surname>Navab</surname><given-names>N.</given-names></name>
<name><surname>Hornegger</surname><given-names>J.</given-names></name>
<name><surname>Wells</surname><given-names>W.M.</given-names></name>
<name><surname>Frangi</surname><given-names>A.F.</given-names></name>
</person-group><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2015</year><volume>Volume 9351</volume><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></element-citation></ref><ref id="B87-sensors-25-01093"><label>87.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>Z.</given-names></name>
<name><surname>Song</surname><given-names>Y.N.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Yang</surname><given-names>X.</given-names></name>
<name><surname>Lian</surname><given-names>W.</given-names></name>
<name><surname>Dai</surname><given-names>H.N.</given-names></name>
</person-group><article-title>Precious Metal Price Prediction Based on Deep Regularization Self-Attention Regression</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>2178</fpage><lpage>2187</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2962202</pub-id></element-citation></ref><ref id="B88-sensors-25-01093"><label>88.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Talbi</surname><given-names>F.</given-names></name>
<name><surname>Chikr Elmezouar</surname><given-names>M.</given-names></name>
<name><surname>Boutellaa</surname><given-names>E.</given-names></name>
<name><surname>Alim</surname><given-names>F.</given-names></name>
</person-group><article-title>Vector-Quantized Variational AutoEncoder for pansharpening</article-title><source>Int. J. Remote Sens.</source><year>2023</year><volume>44</volume><fpage>6329</fpage><lpage>6349</lpage><pub-id pub-id-type="doi">10.1080/01431161.2023.2265542</pub-id></element-citation></ref><ref id="B89-sensors-25-01093"><label>89.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Feng</surname><given-names>R.</given-names></name>
<name><surname>Liu</surname><given-names>P.</given-names></name>
<name><surname>Han</surname><given-names>W.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
</person-group><article-title>CycleGAN-STF: Spatiotemporal Fusion via CycleGAN-Based Image Generation</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2020</year><volume>59</volume><fpage>5851</fpage><lpage>5865</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2020.3023432</pub-id></element-citation></ref><ref id="B90-sensors-25-01093"><label>90.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>Z.</given-names></name>
<name><surname>Gao</surname><given-names>M.</given-names></name>
<name><surname>Yuan</surname><given-names>J.</given-names></name>
<name><surname>Jiang</surname><given-names>L.</given-names></name>
<name><surname>Duan</surname><given-names>H.</given-names></name>
</person-group><article-title>A Robust Model for MODIS and Landsat Image Fusion Considering Input Noise</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3145086</pub-id></element-citation></ref><ref id="B91-sensors-25-01093"><label>91.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ma</surname><given-names>Y.</given-names></name>
<name><surname>Wei</surname><given-names>J.</given-names></name>
<name><surname>Tang</surname><given-names>W.</given-names></name>
<name><surname>Tang</surname><given-names>R.</given-names></name>
</person-group><article-title>Explicit and stepwise models for spatiotemporal fusion of remote sensing images with deep neural networks</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2021</year><volume>105</volume><fpage>102611</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2021.102611</pub-id></element-citation></ref><ref id="B92-sensors-25-01093"><label>92.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Gu</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Gao</surname><given-names>F.</given-names></name>
<name><surname>Jiang</surname><given-names>T.</given-names></name>
<name><surname>Ren</surname><given-names>R.</given-names></name>
</person-group><article-title>An Improved Spatiotemporal Fusion Algorithm for Monitoring Daily Snow Cover Changes with High Spatial Resolution</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3224126</pub-id></element-citation></ref><ref id="B93-sensors-25-01093"><label>93.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Yang</surname><given-names>G.</given-names></name>
<name><surname>Deng</surname><given-names>F.</given-names></name>
<name><surname>Qian</surname><given-names>Y.</given-names></name>
<name><surname>Fan</surname><given-names>Y.</given-names></name>
</person-group><article-title>MCBAM-GAN: The Gan Spatiotemporal Fusion Model Based on Multiscale and CBAM for Remote Sensing Images</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>1583</elocation-id><pub-id pub-id-type="doi">10.3390/rs15061583</pub-id></element-citation></ref><ref id="B94-sensors-25-01093"><label>94.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Song</surname><given-names>Y.</given-names></name>
<name><surname>Han</surname><given-names>C.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
</person-group><article-title>Remote Sensing Image Spatiotemporal Fusion Using a Generative Adversarial Network</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2021</year><volume>59</volume><fpage>4273</fpage><lpage>4286</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2020.3010530</pub-id></element-citation></ref><ref id="B95-sensors-25-01093"><label>95.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pan</surname><given-names>X.</given-names></name>
<name><surname>Deng</surname><given-names>M.</given-names></name>
<name><surname>Ao</surname><given-names>Z.</given-names></name>
<name><surname>Xin</surname><given-names>Q.</given-names></name>
</person-group><article-title>An Adaptive Multiscale Generative Adversarial Network for the Spatiotemporal Fusion of Landsat and MODIS Data</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>5128</elocation-id><pub-id pub-id-type="doi">10.3390/rs15215128</pub-id></element-citation></ref><ref id="B96-sensors-25-01093"><label>96.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Fang</surname><given-names>S.</given-names></name>
<name><surname>Guo</surname><given-names>Q.</given-names></name>
<name><surname>Cao</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
</person-group><article-title>A Two-Layers Super-Resolution Based Generation Adversarial Spatiotemporal Fusion Model</article-title><source>Proceedings of the IGARSS 2022&#x02014;2022 IEEE International Geoscience and Remote Sensing Symposium</source><conf-loc>Kuala Lumpur, Malaysia</conf-loc><conf-date>17&#x02013;22 July 2022</conf-date><fpage>891</fpage><lpage>894</lpage><pub-id pub-id-type="doi">10.1109/IGARSS46834.2022.9883547</pub-id></element-citation></ref><ref id="B97-sensors-25-01093"><label>97.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>W.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Jiang</surname><given-names>M.</given-names></name>
<name><surname>Yuan</surname><given-names>Q.</given-names></name>
</person-group><article-title>Supervised and self-supervised learning-based cascade spatiotemporal fusion framework and its application</article-title><source>ISPRS J. Photogramm. Remote. Sens.</source><year>2023</year><volume>203</volume><fpage>19</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2023.07.022</pub-id></element-citation></ref><ref id="B98-sensors-25-01093"><label>98.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jiang</surname><given-names>M.</given-names></name>
<name><surname>Shen</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep-Learning-Based Spatio-Temporal-Spectral Integrated Fusion of Heterogeneous Remote Sensing Images</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3188998</pub-id></element-citation></ref><ref id="B99-sensors-25-01093"><label>99.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>Y.</given-names></name>
<name><surname>Feng</surname><given-names>S.</given-names></name>
<name><surname>Huang</surname><given-names>M.</given-names></name>
</person-group><article-title>An enhanced spatiotemporal fusion model with degraded fine-resolution images via relativistic generative adversarial networks</article-title><source>Geocarto Int.</source><year>2023</year><volume>38</volume><fpage>2153931</fpage><pub-id pub-id-type="doi">10.1080/10106049.2022.2153931</pub-id></element-citation></ref><ref id="B100-sensors-25-01093"><label>100.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Meng</surname><given-names>X.</given-names></name>
<name><surname>Shao</surname><given-names>F.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
</person-group><article-title>PSTAF-GAN: Progressive Spatio-Temporal Attention Fusion Method Based on Generative Adversarial Network</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3161563</pub-id></element-citation></ref><ref id="B101-sensors-25-01093"><label>101.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Huang</surname><given-names>M.</given-names></name>
<name><surname>Feng</surname><given-names>S.</given-names></name>
</person-group><article-title>Multiresolution generative adversarial networks with bidirectional adaptive-stage progressive guided fusion for remote sensing image</article-title><source>Int. J. Digit. Earth</source><year>2023</year><volume>16</volume><fpage>2962</fpage><lpage>2997</lpage><pub-id pub-id-type="doi">10.1080/17538947.2023.2241441</pub-id></element-citation></ref><ref id="B102-sensors-25-01093"><label>102.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ma</surname><given-names>Y.</given-names></name>
<name><surname>Wei</surname><given-names>J.</given-names></name>
<name><surname>Huang</surname><given-names>X.</given-names></name>
</person-group><article-title>Balancing Colors of Nonoverlapping Mosaicking Images with Generative Adversarial Networks</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2022</year><volume>19</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2021.3126261</pub-id></element-citation></ref><ref id="B103-sensors-25-01093"><label>103.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Tan</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
</person-group><article-title>A Hybrid Spatiotemporal Fusion Method for High Spatial Resolution Imagery: Fusion of Gaofen-1 and Sentinel-2 over Agricultural Landscapes</article-title><source>J. Remote Sens.</source><year>2024</year><volume>4</volume><fpage>0159</fpage><pub-id pub-id-type="doi">10.34133/remotesensing.0159</pub-id></element-citation></ref><ref id="B104-sensors-25-01093"><label>104.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shang</surname><given-names>C.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Yin</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Du</surname><given-names>Y.</given-names></name>
<name><surname>Ling</surname><given-names>F.</given-names></name>
</person-group><article-title>Spatiotemporal Reflectance Fusion Using a Generative Adversarial Network</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2021.3065418</pub-id></element-citation></ref><ref id="B105-sensors-25-01093"><label>105.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>W.</given-names></name>
<name><surname>Ren</surname><given-names>K.</given-names></name>
<name><surname>Meng</surname><given-names>X.</given-names></name>
<name><surname>Yang</surname><given-names>G.</given-names></name>
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Zhu</surname><given-names>L.</given-names></name>
<name><surname>Peng</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>Generating high-resolution hyperspectral time series datasets based on unsupervised spatial-temporal-spectral fusion network incorporating a deep prior</article-title><source>Inf. Fusion</source><year>2024</year><volume>111</volume><fpage>102499</fpage><pub-id pub-id-type="doi">10.1016/j.inffus.2024.102499</pub-id></element-citation></ref><ref id="B106-sensors-25-01093"><label>106.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>B.</given-names></name>
<name><surname>Liu</surname><given-names>P.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>He</surname><given-names>G.</given-names></name>
<name><surname>Chen</surname><given-names>L.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
</person-group><article-title>MLFF-GAN: A Multilevel Feature Fusion With GAN for Spatiotemporal Remote Sensing Images</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3169916</pub-id></element-citation></ref><ref id="B107-sensors-25-01093"><label>107.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Weng</surname><given-names>C.</given-names></name>
<name><surname>Zhan</surname><given-names>Y.</given-names></name>
<name><surname>Gu</surname><given-names>X.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Guo</surname><given-names>H.</given-names></name>
<name><surname>Lian</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Zhao</surname><given-names>X.</given-names></name>
</person-group><article-title>The Spatially Seamless Spatiotemporal Fusion Model Based on Generative Adversarial Networks</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2024</year><volume>17</volume><fpage>12760</fpage><lpage>12771</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2024.3381185</pub-id></element-citation></ref><ref id="B108-sensors-25-01093"><label>108.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Huang</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
</person-group><article-title>Remote Sensing Image Spatiotemporal Fusion via a Generative Adversarial Network With One Prior Image Pair</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3171331</pub-id></element-citation></ref><ref id="B109-sensors-25-01093"><label>109.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dosovitskiy</surname><given-names>A.</given-names></name>
<name><surname>Beyer</surname><given-names>L.</given-names></name>
<name><surname>Kolesnikov</surname><given-names>A.</given-names></name>
<name><surname>Weissenborn</surname><given-names>D.</given-names></name>
<name><surname>Zhai</surname><given-names>X.</given-names></name>
<name><surname>Unterthiner</surname><given-names>T.</given-names></name>
<name><surname>Dehghani</surname><given-names>M.</given-names></name>
<name><surname>Minderer</surname><given-names>M.</given-names></name>
<name><surname>Heigold</surname><given-names>G.</given-names></name>
<name><surname>Gelly</surname><given-names>S.</given-names></name>
<etal/>
</person-group><article-title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2010.11929</pub-id></element-citation></ref><ref id="B110-sensors-25-01093"><label>110.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gulrajani</surname><given-names>I.</given-names></name>
<name><surname>Ahmed</surname><given-names>F.</given-names></name>
<name><surname>Arjovsky</surname><given-names>M.</given-names></name>
<name><surname>Dumoulin</surname><given-names>V.</given-names></name>
<name><surname>Courville</surname><given-names>A.C.</given-names></name>
</person-group><article-title>Improved Training of Wasserstein GANs</article-title><source>Proceedings of the Advances in Neural Information Processing Systems</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>4&#x02013;9 December 2017</conf-date><volume>Volume 30</volume></element-citation></ref><ref id="B111-sensors-25-01093"><label>111.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>G.</given-names></name>
<name><surname>Qian</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Tang</surname><given-names>B.</given-names></name>
<name><surname>Qi</surname><given-names>R.</given-names></name>
<name><surname>Lu</surname><given-names>Y.</given-names></name>
<name><surname>Geng</surname><given-names>J.</given-names></name>
</person-group><article-title>MSFusion: Multistage for Remote Sensing Image Spatiotemporal Fusion Based on Texture Transformer and Convolutional Neural Network</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2022</year><volume>15</volume><fpage>4653</fpage><lpage>4666</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2022.3179415</pub-id></element-citation></ref><ref id="B112-sensors-25-01093"><label>112.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>G.</given-names></name>
<name><surname>Jiao</surname><given-names>P.</given-names></name>
<name><surname>Hu</surname><given-names>Q.</given-names></name>
<name><surname>Xiao</surname><given-names>L.</given-names></name>
<name><surname>Ye</surname><given-names>Z.</given-names></name>
</person-group><article-title>SwinSTFM: Remote Sensing Spatiotemporal Fusion Using Swin Transformer</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3182809</pub-id></element-citation></ref><ref id="B113-sensors-25-01093"><label>113.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Fang</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
</person-group><article-title>Spatiotemporal Fusion Model of Remote Sensing Images Combining Single-Band and Multi-Band Prediction</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>4936</elocation-id><pub-id pub-id-type="doi">10.3390/rs15204936</pub-id></element-citation></ref><ref id="B114-sensors-25-01093"><label>114.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Cao</surname><given-names>D.</given-names></name>
<name><surname>Xiang</surname><given-names>M.</given-names></name>
</person-group><article-title>Enhanced Multi-Stream Remote Sensing Spatiotemporal Fusion Network Based on Transformer and Dilated Convolution</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>4544</elocation-id><pub-id pub-id-type="doi">10.3390/rs14184544</pub-id></element-citation></ref><ref id="B115-sensors-25-01093"><label>115.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Benzenati</surname><given-names>T.</given-names></name>
<name><surname>Kallel</surname><given-names>A.</given-names></name>
<name><surname>Kessentini</surname><given-names>Y.</given-names></name>
</person-group><article-title>STF-Trans: A two-stream spatiotemporal fusion transformer for very high resolution satellites images</article-title><source>Neurocomputing</source><year>2024</year><volume>563</volume><fpage>126868</fpage><pub-id pub-id-type="doi">10.1016/j.neucom.2023.126868</pub-id></element-citation></ref><ref id="B116-sensors-25-01093"><label>116.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qian</surname><given-names>Z.</given-names></name>
<name><surname>Yue</surname><given-names>L.</given-names></name>
<name><surname>Xie</surname><given-names>X.</given-names></name>
<name><surname>Yuan</surname><given-names>Q.</given-names></name>
<name><surname>Shen</surname><given-names>H.</given-names></name>
</person-group><article-title>A Dual-Perspective Spatiotemporal Fusion Model for Remote Sensing Images by Discriminative Learning of the Spatial and Temporal Mapping</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2024</year><volume>17</volume><fpage>12505</fpage><lpage>12520</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2024.3426944</pub-id></element-citation></ref><ref id="B117-sensors-25-01093"><label>117.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Qian</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>G.</given-names></name>
<name><surname>Jiang</surname><given-names>H.</given-names></name>
</person-group><article-title>Super-Resolution Reconstruction Model of Spatiotemporal Fusion Remote Sensing Image Based on Double Branch Texture Transformers and Feedback Mechanism</article-title><source>Electronics</source><year>2022</year><volume>11</volume><elocation-id>2497</elocation-id><pub-id pub-id-type="doi">10.3390/electronics11162497</pub-id></element-citation></ref><ref id="B118-sensors-25-01093"><label>118.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ramesh</surname><given-names>A.</given-names></name>
<name><surname>Pavlov</surname><given-names>M.</given-names></name>
<name><surname>Goh</surname><given-names>G.</given-names></name>
<name><surname>Gray</surname><given-names>S.</given-names></name>
<name><surname>Voss</surname><given-names>C.</given-names></name>
<name><surname>Radford</surname><given-names>A.</given-names></name>
<name><surname>Chen</surname><given-names>M.</given-names></name>
<name><surname>Sutskever</surname><given-names>I.</given-names></name>
</person-group><article-title>Zero-Shot Text-to-Image Generation</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Virtual</conf-loc><conf-date>18&#x02013;24 July 2021</conf-date><fpage>8821</fpage><lpage>8831</lpage></element-citation></ref><ref id="B119-sensors-25-01093"><label>119.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Moser</surname><given-names>B.B.</given-names></name>
<name><surname>Shanbhag</surname><given-names>A.S.</given-names></name>
<name><surname>Raue</surname><given-names>F.</given-names></name>
<name><surname>Frolov</surname><given-names>S.</given-names></name>
<name><surname>Palacio</surname><given-names>S.</given-names></name>
<name><surname>Dengel</surname><given-names>A.</given-names></name>
</person-group><article-title>Diffusion Models, Image Super-Resolution and Everything: A Survey</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2024</year><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2024.3476671</pub-id></element-citation></ref><ref id="B120-sensors-25-01093"><label>120.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kulikov</surname><given-names>V.</given-names></name>
<name><surname>Yadin</surname><given-names>S.</given-names></name>
<name><surname>Kleiner</surname><given-names>M.</given-names></name>
<name><surname>Michaeli</surname><given-names>T.</given-names></name>
</person-group><article-title>Sinddm: A single image denoising diffusion model</article-title><source>Proceedings of the International Conference on Machine Learning, PMLR</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>30 November&#x02013;1 December 2023</conf-date><fpage>17920</fpage><lpage>17930</lpage></element-citation></ref><ref id="B121-sensors-25-01093"><label>121.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>Y.</given-names></name>
<name><surname>Jin</surname><given-names>X.</given-names></name>
<name><surname>Lan</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Zeng</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
</person-group><article-title>Diffusion Models for Image Restoration and Enhancement&#x02014;A Comprehensive Survey</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2308.09388</pub-id></element-citation></ref><ref id="B122-sensors-25-01093"><label>122.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ma</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Q.</given-names></name>
<name><surname>Wei</surname><given-names>J.</given-names></name>
</person-group><article-title>Spatiotemporal Fusion via Conditional Diffusion Model</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2024</year><volume>21</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2024.3378715</pub-id></element-citation></ref><ref id="B123-sensors-25-01093"><label>123.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wei</surname><given-names>J.</given-names></name>
<name><surname>Gan</surname><given-names>L.</given-names></name>
<name><surname>Tang</surname><given-names>W.</given-names></name>
<name><surname>Li</surname><given-names>M.</given-names></name>
<name><surname>Song</surname><given-names>Y.</given-names></name>
</person-group><article-title>Diffusion models for spatio-temporal-spectral fusion of homogeneous Gaofen-1 satellite platforms</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2024</year><volume>128</volume><fpage>103752</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2024.103752</pub-id></element-citation></ref><ref id="B124-sensors-25-01093"><label>124.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Han</surname><given-names>W.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Dong</surname><given-names>Y.</given-names></name>
<name><surname>Fan</surname><given-names>R.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
</person-group><article-title>Geological Remote Sensing Interpretation Using Deep Learning Feature and an Adaptive Multisource Data Fusion Network</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3183080</pub-id></element-citation></ref><ref id="B125-sensors-25-01093"><label>125.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Elizar</surname><given-names>E.</given-names></name>
<name><surname>Zulkifley</surname><given-names>M.A.</given-names></name>
<name><surname>Muharar</surname><given-names>R.</given-names></name>
<name><surname>Zaman</surname><given-names>M.H.M.</given-names></name>
<name><surname>Mustaza</surname><given-names>S.M.</given-names></name>
</person-group><article-title>A Review on Multiscale-Deep-Learning Applications</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>7384</elocation-id><pub-id pub-id-type="doi">10.3390/s22197384</pub-id><pub-id pub-id-type="pmid">36236483</pub-id>
</element-citation></ref><ref id="B126-sensors-25-01093"><label>126.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Swain</surname><given-names>R.</given-names></name>
<name><surname>Paul</surname><given-names>A.</given-names></name>
<name><surname>Behera</surname><given-names>M.D.</given-names></name>
</person-group><article-title>Spatio-temporal fusion methods for spectral remote sensing: A comprehensive technical review and comparative analysis</article-title><source>Trop. Ecol.</source><year>2023</year><volume>65</volume><fpage>356</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1007/s42965-023-00318-5</pub-id></element-citation></ref><ref id="B127-sensors-25-01093"><label>127.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xue</surname><given-names>J.</given-names></name>
<name><surname>Leung</surname><given-names>Y.</given-names></name>
<name><surname>Fung</surname><given-names>T.</given-names></name>
</person-group><article-title>A Bayesian Data Fusion Approach to Spatio-Temporal Fusion of Remotely Sensed Images</article-title><source>Remote Sens.</source><year>2017</year><volume>9</volume><elocation-id>1310</elocation-id><pub-id pub-id-type="doi">10.3390/rs9121310</pub-id></element-citation></ref><ref id="B128-sensors-25-01093"><label>128.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<name><surname>Qiu</surname><given-names>Y.</given-names></name>
<name><surname>Song</surname><given-names>H.</given-names></name>
<name><surname>Rao</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Cao</surname><given-names>X.</given-names></name>
<name><surname>Cui</surname><given-names>X.</given-names></name>
</person-group><article-title>Sensitivity of six typical spatiotemporal fusion methods to different influential factors: A comparative study for a normalized difference vegetation index time series reconstruction</article-title><source>Remote Sens. Environ.</source><year>2021</year><volume>252</volume><fpage>112130</fpage><pub-id pub-id-type="doi">10.1016/j.rse.2020.112130</pub-id></element-citation></ref><ref id="B129-sensors-25-01093"><label>129.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Hong</surname><given-names>D.</given-names></name>
<name><surname>Gao</surname><given-names>L.</given-names></name>
<name><surname>Yao</surname><given-names>J.</given-names></name>
<name><surname>Zheng</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
<name><surname>Chanussot</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep learning in multimodal remote sensing data fusion: A comprehensive review</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2022</year><volume>112</volume><fpage>102926</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2022.102926</pub-id></element-citation></ref><ref id="B130-sensors-25-01093"><label>130.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<name><surname>Zhan</surname><given-names>W.</given-names></name>
<name><surname>Zhou</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Liang</surname><given-names>Z.</given-names></name>
<name><surname>Xu</surname><given-names>S.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
</person-group><article-title>A novel framework to assess all-round performances of spatiotemporal fusion models</article-title><source>Remote Sens. Environ.</source><year>2022</year><volume>274</volume><fpage>113002</fpage><pub-id pub-id-type="doi">10.1016/j.rse.2022.113002</pub-id></element-citation></ref><ref id="B131-sensors-25-01093"><label>131.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>D.</given-names></name>
<name><surname>Shi</surname><given-names>W.</given-names></name>
<name><surname>Qian</surname><given-names>F.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Cai</surname><given-names>C.</given-names></name>
</person-group><article-title>Monitoring the spatiotemporal change of Dongting Lake wetland by integrating Landsat and MODIS images, from 2001 to 2020</article-title><source>Ecol. Inform.</source><year>2022</year><volume>72</volume><fpage>101848</fpage><pub-id pub-id-type="doi">10.1016/j.ecoinf.2022.101848</pub-id></element-citation></ref><ref id="B132-sensors-25-01093"><label>132.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Emelyanova</surname><given-names>I.V.</given-names></name>
<name><surname>McVicar</surname><given-names>T.R.</given-names></name>
<name><surname>Van Niel</surname><given-names>T.G.</given-names></name>
<name><surname>Li</surname><given-names>L.T.</given-names></name>
<name><surname>Van Dijk</surname><given-names>A.I.</given-names></name>
</person-group><article-title>Assessing the accuracy of blending Landsat&#x02014;MODIS surface reflectances in two landscapes with contrasting spatial and temporal dynamics: A framework for algorithm selection</article-title><source>Remote Sens. Environ.</source><year>2013</year><volume>133</volume><fpage>193</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2013.02.007</pub-id></element-citation></ref><ref id="B133-sensors-25-01093"><label>133.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>He</surname><given-names>L.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Plaza</surname><given-names>A.</given-names></name>
</person-group><article-title>Spatio-temporal fusion for remote sensing data: An overview and new benchmark</article-title><source>Sci. China Inf. Sci.</source><year>2020</year><volume>63</volume><fpage>140301</fpage><pub-id pub-id-type="doi">10.1007/s11432-019-2785-y</pub-id></element-citation></ref><ref id="B134-sensors-25-01093"><label>134.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>D.</given-names></name>
<name><surname>Shi</surname><given-names>W.</given-names></name>
</person-group><article-title>Object-Level Hybrid Spatiotemporal Fusion: Reaching a Better Tradeoff Among Spectral Accuracy, Spatial Accuracy, and Efficiency</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2023</year><volume>16</volume><fpage>8007</fpage><lpage>8021</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2023.3310195</pub-id></element-citation></ref><ref id="B135-sensors-25-01093"><label>135.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tasar</surname><given-names>O.</given-names></name>
<name><surname>Tarabalka</surname><given-names>Y.</given-names></name>
<name><surname>Alliez</surname><given-names>P.</given-names></name>
</person-group><article-title>Incremental Learning for Semantic Segmentation of Large-Scale Remote Sensing Data</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2019</year><volume>12</volume><fpage>3524</fpage><lpage>3537</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2019.2925416</pub-id></element-citation></ref><ref id="B136-sensors-25-01093"><label>136.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yin</surname><given-names>M.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
</person-group><article-title>A CNN-Transformer Network Combining CBAM for Change Detection in High-Resolution Remote Sensing Images</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>2406</elocation-id><pub-id pub-id-type="doi">10.3390/rs15092406</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01093-f001"><label>Figure 1</label><caption><p>Development of spatiotemporal fusion methods.</p></caption><graphic xlink:href="sensors-25-01093-g001" position="float"/></fig><fig position="float" id="sensors-25-01093-f002"><label>Figure 2</label><caption><p>Yearly paper count of spatiotemporal fusion methods.</p></caption><graphic xlink:href="sensors-25-01093-g002" position="float"/></fig><fig position="float" id="sensors-25-01093-f003"><label>Figure 3</label><caption><p>Example of deep learning-based spatiotemporal fusion (STF) models. (<bold>a</bold>) A CNN-based STF model (e.g., DCSTFN [<xref rid="B26-sensors-25-01093" ref-type="bibr">26</xref>]). (<bold>b</bold>) A GAN-based STF model (e.g., GANSTFN [<xref rid="B27-sensors-25-01093" ref-type="bibr">27</xref>]). (<bold>c</bold>) A Transformer-based STF model (e.g., MSNet [<xref rid="B28-sensors-25-01093" ref-type="bibr">28</xref>]). (<bold>d</bold>) A diffusion-based STF model (e.g., STFDiff [<xref rid="B29-sensors-25-01093" ref-type="bibr">29</xref>]). &#x0201c;F&#x0201d; and &#x0201c;C&#x0201d; represent fine images and coarse images, respectively. Subscripts &#x0201c;1&#x0201d; and &#x0201c;2&#x0201d; represent the reference date <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and the predicted date <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, respectively.</p></caption><graphic xlink:href="sensors-25-01093-g003a" position="float"/><graphic xlink:href="sensors-25-01093-g003b" position="float"/></fig><fig position="float" id="sensors-25-01093-f004"><label>Figure 4</label><caption><p>An example layer of a conventional CNN.</p></caption><graphic xlink:href="sensors-25-01093-g004" position="float"/></fig><fig position="float" id="sensors-25-01093-f005"><label>Figure 5</label><caption><p>Flowchart of VDCN [<xref rid="B35-sensors-25-01093" ref-type="bibr">35</xref>].</p></caption><graphic xlink:href="sensors-25-01093-g005" position="float"/></fig><fig position="float" id="sensors-25-01093-f006"><label>Figure 6</label><caption><p>Spectral response functions of the Landsat series and MODIS [<xref rid="B84-sensors-25-01093" ref-type="bibr">84</xref>].</p></caption><graphic xlink:href="sensors-25-01093-g006" position="float"/></fig><fig position="float" id="sensors-25-01093-f007"><label>Figure 7</label><caption><p>Structure of a typical residual block.</p></caption><graphic xlink:href="sensors-25-01093-g007" position="float"/></fig><fig position="float" id="sensors-25-01093-f008"><label>Figure 8</label><caption><p>Architecture of DCSTFN [<xref rid="B26-sensors-25-01093" ref-type="bibr">26</xref>].</p></caption><graphic xlink:href="sensors-25-01093-g008" position="float"/></fig><fig position="float" id="sensors-25-01093-f009"><label>Figure 9</label><caption><p>A typical U-Net architecture.</p></caption><graphic xlink:href="sensors-25-01093-g009" position="float"/></fig><fig position="float" id="sensors-25-01093-f010"><label>Figure 10</label><caption><p>Example of an attention module in a convolutional network (modified from [<xref rid="B87-sensors-25-01093" ref-type="bibr">87</xref>]). &#x0201c;Conv&#x0201d; denotes a convolution operation with a kernel size of <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-25-01093-g010" position="float"/></fig><fig position="float" id="sensors-25-01093-f011"><label>Figure 11</label><caption><p>A typical structure of a GAN.</p></caption><graphic xlink:href="sensors-25-01093-g011" position="float"/></fig><fig position="float" id="sensors-25-01093-f012"><label>Figure 12</label><caption><p>Three types of input schemes in spatiotemporal fusion. (<bold>a</bold>) Five-image input scheme (e.g., STARFM [<xref rid="B12-sensors-25-01093" ref-type="bibr">12</xref>]). (<bold>b</bold>) Three-image input scheme (e.g., DCSTFN [<xref rid="B26-sensors-25-01093" ref-type="bibr">26</xref>]). (<bold>c</bold>) Two-image input scheme (e.g., GANSTFM [<xref rid="B27-sensors-25-01093" ref-type="bibr">27</xref>]). &#x0201c;F&#x0201d; and &#x0201c;C&#x0201d; represent fine images and coarse images, respectively. Subscripts &#x0201c;1&#x0201d;, &#x0201c;3&#x0201d;, and &#x0201c;2&#x0201d; represent the reference dates <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and the predicted date <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. &#x0201c;?&#x0201d; represents the predicted image.</p></caption><graphic xlink:href="sensors-25-01093-g012" position="float"/></fig><fig position="float" id="sensors-25-01093-f013"><label>Figure 13</label><caption><p>Structure of a Transformer encoder in a Vision Transformer.</p></caption><graphic xlink:href="sensors-25-01093-g013" position="float"/></fig><fig position="float" id="sensors-25-01093-f014"><label>Figure 14</label><caption><p>A typical process of a diffusion model.</p></caption><graphic xlink:href="sensors-25-01093-g014" position="float"/></fig><fig position="float" id="sensors-25-01093-f015"><label>Figure 15</label><caption><p>Examples of cropland areas from the CIA dataset (<bold>a</bold>) and a flood event from the LGC dataset (<bold>b</bold>).</p></caption><graphic xlink:href="sensors-25-01093-g015" position="float"/></fig><fig position="float" id="sensors-25-01093-f016"><label>Figure 16</label><caption><p>Box plots for quantitative performance evaluation on the CIA dataset.</p></caption><graphic xlink:href="sensors-25-01093-g016" position="float"/></fig><fig position="float" id="sensors-25-01093-f017"><label>Figure 17</label><caption><p>Box plots for quantitative performance evaluation on the LGC dataset.</p></caption><graphic xlink:href="sensors-25-01093-g017" position="float"/></fig><fig position="float" id="sensors-25-01093-f018"><label>Figure 18</label><caption><p>Application examples of spatiotemporal fusion. (<bold>a</bold>) Crop classification. (<bold>b</bold>) Land-cover classification. (<bold>c</bold>) Vegetation monitoring. (<bold>d</bold>) Change detection.</p></caption><graphic xlink:href="sensors-25-01093-g018" position="float"/></fig><fig position="float" id="sensors-25-01093-f019"><label>Figure 19</label><caption><p>Literature count of various assessment metrics in deep learning-based spatiotemporal fusion methods.</p></caption><graphic xlink:href="sensors-25-01093-g019" position="float"/></fig><table-wrap position="float" id="sensors-25-01093-t001"><object-id pub-id-type="pii">sensors-25-01093-t001_Table 1</object-id><label>Table 1</label><caption><p>CNN-based spatiotemporal fusion methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th></tr></thead><tbody><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Conventional CNN-based</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">STFDCNN [<xref rid="B33-sensors-25-01093" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">MCDNet [<xref rid="B34-sensors-25-01093" ref-type="bibr">34</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">VDCN [<xref rid="B35-sensors-25-01093" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">LTSC3D [<xref rid="B36-sensors-25-01093" ref-type="bibr">36</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">ESRCNN [<xref rid="B37-sensors-25-01093" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">MUSTFN [<xref rid="B38-sensors-25-01093" ref-type="bibr">38</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">StfNet [<xref rid="B39-sensors-25-01093" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">MSTTIFN [<xref rid="B40-sensors-25-01093" ref-type="bibr">40</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">DL-SDFM [<xref rid="B41-sensors-25-01093" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">CIG-STF [<xref rid="B42-sensors-25-01093" ref-type="bibr">42</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BiaSTF [<xref rid="B43-sensors-25-01093" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td rowspan="13" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Residual CNN-based</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">DCSTFN [<xref rid="B26-sensors-25-01093" ref-type="bibr">26</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">STFDSC [<xref rid="B44-sensors-25-01093" ref-type="bibr">44</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">EDCSTFN [<xref rid="B45-sensors-25-01093" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">Li et al. [<xref rid="B46-sensors-25-01093" ref-type="bibr">46</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">Li et al. [<xref rid="B47-sensors-25-01093" ref-type="bibr">47</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">Hoque et al. [<xref rid="B48-sensors-25-01093" ref-type="bibr">48</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">DMNet [<xref rid="B49-sensors-25-01093" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">MTDL-STF [<xref rid="B50-sensors-25-01093" ref-type="bibr">50</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">STF3DCNN [<xref rid="B51-sensors-25-01093" ref-type="bibr">51</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">ERDN [<xref rid="B52-sensors-25-01093" ref-type="bibr">52</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">ResStf [<xref rid="B53-sensors-25-01093" ref-type="bibr">53</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">TSDTSF [<xref rid="B54-sensors-25-01093" ref-type="bibr">54</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">HDLSFM [<xref rid="B55-sensors-25-01093" ref-type="bibr">55</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">DPSTFN [<xref rid="B56-sensors-25-01093" ref-type="bibr">56</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">Htitiou et al. [<xref rid="B57-sensors-25-01093" ref-type="bibr">57</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">Wei et al. [<xref rid="B58-sensors-25-01093" ref-type="bibr">58</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">MOST [<xref rid="B59-sensors-25-01093" ref-type="bibr">59</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">STFRDN [<xref rid="B60-sensors-25-01093" ref-type="bibr">60</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">ACFNet [<xref rid="B61-sensors-25-01093" ref-type="bibr">61</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">UAV-Net [<xref rid="B62-sensors-25-01093" ref-type="bibr">62</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">BASNet [<xref rid="B63-sensors-25-01093" ref-type="bibr">63</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">Zeng et al. [<xref rid="B64-sensors-25-01093" ref-type="bibr">64</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">STTFN [<xref rid="B65-sensors-25-01093" ref-type="bibr">65</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">STFNet [<xref rid="B66-sensors-25-01093" ref-type="bibr">66</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">STFMCNN [<xref rid="B67-sensors-25-01093" ref-type="bibr">67</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Attentional CNN-based</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">AMNet [<xref rid="B68-sensors-25-01093" ref-type="bibr">68</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">DSTFN [<xref rid="B69-sensors-25-01093" ref-type="bibr">69</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">ASRCNN [<xref rid="B70-sensors-25-01093" ref-type="bibr">70</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">RCAN [<xref rid="B71-sensors-25-01093" ref-type="bibr">71</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">PDCNN [<xref rid="B72-sensors-25-01093" ref-type="bibr">72</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">CAFE [<xref rid="B73-sensors-25-01093" ref-type="bibr">73</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">SL-STIF [<xref rid="B74-sensors-25-01093" ref-type="bibr">74</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">DSTFNet [<xref rid="B75-sensors-25-01093" ref-type="bibr">75</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">STF-EGFA [<xref rid="B76-sensors-25-01093" ref-type="bibr">76</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">SIFnet [<xref rid="B77-sensors-25-01093" ref-type="bibr">77</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">SCRnet [<xref rid="B78-sensors-25-01093" ref-type="bibr">78</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">ECPW-STFN [<xref rid="B79-sensors-25-01093" ref-type="bibr">79</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MANet [<xref rid="B80-sensors-25-01093" ref-type="bibr">80</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RCAN-FSDAF [<xref rid="B81-sensors-25-01093" ref-type="bibr">81</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01093-t002"><object-id pub-id-type="pii">sensors-25-01093-t002_Table 2</object-id><label>Table 2</label><caption><p>GAN-based spatiotemporal fusion methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">CycleGAN -STF [<xref rid="B89-sensors-25-01093" ref-type="bibr">89</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">RSFN [<xref rid="B90-sensors-25-01093" ref-type="bibr">90</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">SSTSTF [<xref rid="B91-sensors-25-01093" ref-type="bibr">91</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">SMPG [<xref rid="B92-sensors-25-01093" ref-type="bibr">92</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">GANSTFM [<xref rid="B27-sensors-25-01093" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">MCBAM-GAN [<xref rid="B93-sensors-25-01093" ref-type="bibr">93</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">STFGAN [<xref rid="B94-sensors-25-01093" ref-type="bibr">94</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">AMS-STF [<xref rid="B95-sensors-25-01093" ref-type="bibr">95</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">TLSRSTF [<xref rid="B96-sensors-25-01093" ref-type="bibr">96</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">DSFN [<xref rid="B97-sensors-25-01093" ref-type="bibr">97</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">DRCGAN [<xref rid="B98-sensors-25-01093" ref-type="bibr">98</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">EDRGAN-STF [<xref rid="B99-sensors-25-01093" ref-type="bibr">99</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">PSTAF-GAN [<xref rid="B100-sensors-25-01093" ref-type="bibr">100</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">BPF-MGAN [<xref rid="B101-sensors-25-01093" ref-type="bibr">101</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">MOSTGAN [<xref rid="B102-sensors-25-01093" ref-type="bibr">102</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">StarFusion [<xref rid="B103-sensors-25-01093" ref-type="bibr">103</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">GASTFN [<xref rid="B104-sensors-25-01093" ref-type="bibr">104</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">Sun et al. [<xref rid="B105-sensors-25-01093" ref-type="bibr">105</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">MLFF-GAN [<xref rid="B106-sensors-25-01093" ref-type="bibr">106</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">Weng et al. [<xref rid="B107-sensors-25-01093" ref-type="bibr">107</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OPGAN [<xref rid="B108-sensors-25-01093" ref-type="bibr">108</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01093-t003"><object-id pub-id-type="pii">sensors-25-01093-t003_Table 3</object-id><label>Table 3</label><caption><p>Transformer-based spatiotemporal fusion methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">MSNet [<xref rid="B28-sensors-25-01093" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">MSFusion [<xref rid="B111-sensors-25-01093" ref-type="bibr">111</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">SwinSTFM [<xref rid="B112-sensors-25-01093" ref-type="bibr">112</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">SMSTFM [<xref rid="B113-sensors-25-01093" ref-type="bibr">113</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">EMSNet [<xref rid="B114-sensors-25-01093" ref-type="bibr">114</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">STF-Trans [<xref rid="B115-sensors-25-01093" ref-type="bibr">115</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DBTT-FM [<xref rid="B100-sensors-25-01093" ref-type="bibr">100</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">STM-STFNet [<xref rid="B116-sensors-25-01093" ref-type="bibr">116</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01093-t004"><object-id pub-id-type="pii">sensors-25-01093-t004_Table 4</object-id><label>Table 4</label><caption><p>Diffusion-based fusion methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">STFDiff [<xref rid="B29-sensors-25-01093" ref-type="bibr">29</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">DiffSTF [<xref rid="B122-sensors-25-01093" ref-type="bibr">122</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DiffSTSF [<xref rid="B123-sensors-25-01093" ref-type="bibr">123</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01093-t005"><object-id pub-id-type="pii">sensors-25-01093-t005_Table 5</object-id><label>Table 5</label><caption><p>Open-source methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Type</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Link</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">CNN-based</td><td align="left" valign="middle" rowspan="1" colspan="1">EDCSTFN</td><td align="left" valign="middle" rowspan="1" colspan="1"><uri xlink:href="https://github.com/theonegis/edcstfn">https://github.com/theonegis/edcstfn</uri><break/>
(accessed on 11 February 2025)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GAN-based</td><td align="left" valign="middle" rowspan="1" colspan="1">GANSTFM</td><td align="left" valign="middle" rowspan="1" colspan="1"><uri xlink:href="https://github.com/theonegis/ganstfm">https://github.com/theonegis/ganstfm</uri><break/>
(accessed on 11 February 2025)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Transformer-based</td><td align="left" valign="middle" rowspan="1" colspan="1">SwinSTFM</td><td align="left" valign="middle" rowspan="1" colspan="1"><uri xlink:href="https://github.com/LouisChen0104/swinstfm">https://github.com/LouisChen0104/swinstfm</uri> (accessed on 11 February 2025)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Diffusion-based</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">STFDiff</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><uri xlink:href="https://github.com/prowDIY/STF">https://github.com/prowDIY/STF</uri><break/>
(accessed on 11 February 2025)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01093-t006"><object-id pub-id-type="pii">sensors-25-01093-t006_Table 6</object-id><label>Table 6</label><caption><p>Quantitative performance evaluation on the CIA dataset. Values in bold represent the model with the optimal average performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">EDCSTFN</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GANSTFM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SwinSTFM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STFDiff</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">RMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">Min</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0217</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0209</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0198</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.0232</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0507</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0357</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0256</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Avg</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0319</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0276</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.0231</bold>
</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">SSIM</td><td align="center" valign="middle" rowspan="1" colspan="1">Min</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7936</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7818</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7579</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.8844</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9094</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8933</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8683</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Avg</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8525</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8471</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8434</bold>
</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">CC</td><td align="center" valign="middle" rowspan="1" colspan="1">Min</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8427</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8169</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8574</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.9018</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8580</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8562</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9190</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Avg</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8517</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8375</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8809</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">SAM</td><td align="center" valign="middle" rowspan="1" colspan="1">Min</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0678</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0532</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0572</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<bold>0.0734</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0839</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1046</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0914</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Avg</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0764</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0879</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0746</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">ERGAS</td><td align="center" valign="middle" rowspan="1" colspan="1">Min</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0677</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1955</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1754</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<bold>1.0732</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max</td><td align="center" valign="middle" rowspan="1" colspan="1">2.6280</td><td align="center" valign="middle" rowspan="1" colspan="1">2.6675</td><td align="center" valign="middle" rowspan="1" colspan="1">2.5728</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Avg</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.6315</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.3399</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.1842</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01093-t007"><object-id pub-id-type="pii">sensors-25-01093-t007_Table 7</object-id><label>Table 7</label><caption><p>Quantitative performance evaluation on the LGC dataset. Values in bold represent the model with the optimal average performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">EDCSTFN</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GANSTFM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SwinSTFM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STFDiff</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">RMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">Min</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0168</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0167</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0174</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<bold>0.0169</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0359</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0319</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0280</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Avg</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0279</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0258</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0222</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">SSIM</td><td align="center" valign="middle" rowspan="1" colspan="1">Min</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7585</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6290</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7470</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.9429</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9585</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8972</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8997</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Avg</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8228</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.7984</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8336</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">CC</td><td align="center" valign="middle" rowspan="1" colspan="1">Min</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7993</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7517</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8065</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.9286</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9195</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8395</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9412</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Avg</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8612</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.7956</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8630</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">SAM</td><td align="center" valign="middle" rowspan="1" colspan="1">Min</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0515</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0593</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0539</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<bold>0.0536</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1382</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1769</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1474</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Avg</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1064</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1275</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1043</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">ERGAS</td><td align="center" valign="middle" rowspan="1" colspan="1">Min</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8180</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1336</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1310</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<bold>0.7258</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max</td><td align="center" valign="middle" rowspan="1" colspan="1">3.2709</td><td align="center" valign="middle" rowspan="1" colspan="1">3.3205</td><td align="center" valign="middle" rowspan="1" colspan="1">3.1502</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Avg</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.6504</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.6581</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.4844</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01093-t008"><object-id pub-id-type="pii">sensors-25-01093-t008_Table 8</object-id><label>Table 8</label><caption><p>Parameters for each model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">EDCSTFN</td><td align="center" valign="middle" rowspan="1" colspan="1">280,000</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GANSTFM</td><td align="center" valign="middle" rowspan="1" colspan="1">4,180,000</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SwinSTFM</td><td align="center" valign="middle" rowspan="1" colspan="1">39,665,893</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">STFDiff</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4,590,000</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01093-t009"><object-id pub-id-type="pii">sensors-25-01093-t009_Table 9</object-id><label>Table 9</label><caption><p>Current open-source spatiotemporal fusion datasets. &#x0201c;Citations&#x0201d; indicate references within deep learning spatiotemporal fusion methods (as of 12 December 2024).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Datasets</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Data Source</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Citations</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CIA [<xref rid="B132-sensors-25-01093" ref-type="bibr">132</xref>]</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">2013</td><td align="center" valign="middle" rowspan="1" colspan="1">Landsat-7|MODIS</td><td align="center" valign="middle" rowspan="1" colspan="1">77</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LGC [<xref rid="B132-sensors-25-01093" ref-type="bibr">132</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Landsat-5|MODIS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">AHB [<xref rid="B133-sensors-25-01093" ref-type="bibr">133</xref>]</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">2020</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Landsat-8|MODIS</td><td align="center" valign="middle" rowspan="1" colspan="1">12</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DX [<xref rid="B133-sensors-25-01093" ref-type="bibr">133</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TJ [<xref rid="B133-sensors-25-01093" ref-type="bibr">133</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td></tr></tbody></table></table-wrap></floats-group></article>