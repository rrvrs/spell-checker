<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Audiol Res</journal-id><journal-id journal-id-type="iso-abbrev">Audiol Res</journal-id><journal-id journal-id-type="publisher-id">audiolres</journal-id><journal-title-group><journal-title>Audiology Research</journal-title></journal-title-group><issn pub-type="ppub">2039-4330</issn><issn pub-type="epub">2039-4349</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40407662</article-id><article-id pub-id-type="pmc">PMC12101201</article-id>
<article-id pub-id-type="doi">10.3390/audiolres15030048</article-id><article-id pub-id-type="publisher-id">audiolres-15-00048</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Sound Localization with Hearables in Transparency Mode</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5138-0988</contrib-id><name><surname>Ausili</surname><given-names>Sebastian A.</given-names></name><xref rid="af1-audiolres-15-00048" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Erthal</surname><given-names>Nathan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af2-audiolres-15-00048" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1150-7400</contrib-id><name><surname>Bennett</surname><given-names>Christopher</given-names></name><xref rid="af2-audiolres-15-00048" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Snapp</surname><given-names>Hillary A.</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-audiolres-15-00048" ref-type="aff">1</xref><xref rid="c1-audiolres-15-00048" ref-type="corresp">*</xref></contrib></contrib-group><aff id="af1-audiolres-15-00048"><label>1</label>Department of Otolaryngology, University of Miami, 1120 NW 14th Street, 5th Floor, Miami, FL 33136, USA; <email>s.ausili@miami.edu</email></aff><aff id="af2-audiolres-15-00048"><label>2</label>Department of Music Engineering, University of Miami, 1314 Miller Dr, Coral Gables, FL 33146, USA; <email>nxe244@miami.edu</email> (N.E.); <email>bennett@miami.edu</email> (C.B.)</aff><author-notes><corresp id="c1-audiolres-15-00048"><label>*</label>Correspondence: <email>hsnapp@miami.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>25</day><month>4</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>6</month><year>2025</year></pub-date><volume>15</volume><issue>3</issue><elocation-id>48</elocation-id><history><date date-type="received"><day>24</day><month>2</month><year>2025</year></date><date date-type="rev-recd"><day>11</day><month>4</month><year>2025</year></date><date date-type="accepted"><day>22</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p><bold>Background:</bold> Transparency mode in hearables aims to maintain environmental awareness by transmitting external sounds through built-in microphones and speakers. While technical assessments have documented acoustic alterations in these devices, their impact on spatial hearing abilities under realistic listening conditions remains unclear. This study aimed to evaluate how transparency mode affects sound localization performance with and without background noise. <bold>Methods:</bold> Ten normal-hearing adults completed sound localization tasks across azimuth (&#x000b1;90&#x000b0;) and elevation (&#x000b1;30&#x000b0;) with and without background noise. Performance was assessed with and without AirPods Pro in transparency mode. Sound localization performance was evaluated through linear regression analysis and mean absolute errors. Head-Related Transfer Function measurements quantified changes in binaural and spectral cues. <bold>Results:</bold> While interaural time differences were largely preserved, transparency mode introduced systematic alterations in level differences (up to 8 dB) and eliminated spectral cues above 5 kHz. These modifications resulted in increased localization errors, particularly for elevation perception and in noise. Mean absolute errors increased from 6.81&#x000b0; to 19.6&#x000b0; in azimuth and from 6.79&#x000b0; to 19.4&#x000b0; in elevation without background noise, with further degradation at lower SNRs (<italic toggle="yes">p</italic> &#x0003c; 0.05). Response times were affected by background noise (<italic toggle="yes">p</italic> &#x0003c; 0.001) but not by device use. <bold>Conclusions:</bold> Current transparency mode implementation significantly compromises spatial hearing abilities, particularly in noisy environments typical of everyday listening situations. These findings highlight the need for technological improvements in maintaining natural spatial cues through transparency mode, as current limitations may impact user safety and communication in real-world environments.</p></abstract><kwd-group><kwd>binaural hearing</kwd><kwd>active hearables</kwd><kwd>AirPods</kwd><kwd>transparency mode</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-audiolres-15-00048"><title>1. Introduction</title><p>The rapid and widespread expansion of hearable devices (e.g., smart wireless electronic devices worn in the ear) represents significant technological advancement in personal audio technology [<xref rid="B1-audiolres-15-00048" ref-type="bibr">1</xref>,<xref rid="B2-audiolres-15-00048" ref-type="bibr">2</xref>,<xref rid="B3-audiolres-15-00048" ref-type="bibr">3</xref>]. Hearables integrate sophisticated functionalities including audio streaming, voice commands, and adaptive noise control, positioning them at the intersection of consumer electronics and assistive hearing technology. With the recent authorization of over-the-counter (OTC) hearing aids by the U.S. Food and Drug Administration (FDA), hearables are uniquely positioned to gain mainstream adoption in the hearing healthcare market, potentially serving millions of individuals with hearing loss [<xref rid="B4-audiolres-15-00048" ref-type="bibr">4</xref>].</p><p>However, the fundamental design of these devices presents significant challenges to natural auditory processing. As it happens with hearing aids and protective hearing devices [<xref rid="B5-audiolres-15-00048" ref-type="bibr">5</xref>,<xref rid="B6-audiolres-15-00048" ref-type="bibr">6</xref>,<xref rid="B7-audiolres-15-00048" ref-type="bibr">7</xref>,<xref rid="B8-audiolres-15-00048" ref-type="bibr">8</xref>], the physical insertion of hearables into the ear canal modifies its acoustic properties, altering both the resonant frequency and the overall sound pressure at the eardrum [<xref rid="B9-audiolres-15-00048" ref-type="bibr">9</xref>,<xref rid="B10-audiolres-15-00048" ref-type="bibr">10</xref>,<xref rid="B11-audiolres-15-00048" ref-type="bibr">11</xref>]. This modification can significantly alter spatial hearing, which relies on precise acoustic cues for accurate sound source localization. These cues include interaural time differences (ITDs) predominant at lower frequencies (&#x0003c;1.5 kHz), interaural level differences (ILDs) at higher frequencies (&#x0003e;3 kHz), and spectral cues above 4 kHz that emerge from complex acoustic interactions with the torso, head, and pinnae [<xref rid="B12-audiolres-15-00048" ref-type="bibr">12</xref>].</p><p>Conventionally, headphones lacked active electronics for real-time audio processing and functioned as passive devices. By sitting in or on the ear, and similar to the effects of passive earplugs, they altered binaural acoustic interactions and disrupted pinna-related spectral cues impacting sound localization [<xref rid="B5-audiolres-15-00048" ref-type="bibr">5</xref>,<xref rid="B13-audiolres-15-00048" ref-type="bibr">13</xref>,<xref rid="B14-audiolres-15-00048" ref-type="bibr">14</xref>]. To address these acoustic modifications, manufacturers have implemented &#x0201c;transparency&#x0201d; or &#x0201c;hear-through&#x0201d; modes that utilize built-in microphones to capture and process ambient sounds in real-time. The theoretical objective of these systems is to achieve acoustic transparency, where the transfer function measured at the eardrum should remain consistent between the unoccluded condition and with the device inserted. This approach has been adopted in both traditional hearing aids and modern consumer hearables to minimize the acoustic disruptions caused by ear canal occlusion, with the goal of preserving natural spatial hearing and maintaining a sound experience similar to that of an open ear [<xref rid="B10-audiolres-15-00048" ref-type="bibr">10</xref>,<xref rid="B14-audiolres-15-00048" ref-type="bibr">14</xref>,<xref rid="B15-audiolres-15-00048" ref-type="bibr">15</xref>,<xref rid="B16-audiolres-15-00048" ref-type="bibr">16</xref>,<xref rid="B17-audiolres-15-00048" ref-type="bibr">17</xref>,<xref rid="B18-audiolres-15-00048" ref-type="bibr">18</xref>,<xref rid="B19-audiolres-15-00048" ref-type="bibr">19</xref>].</p><p>Recent acoustic measurements of commercial hearables&#x02019; transparency features have revealed substantial deviations from natural sound transmission characteristics, showing significant alterations in both monaural and binaural cues [<xref rid="B20-audiolres-15-00048" ref-type="bibr">20</xref>,<xref rid="B21-audiolres-15-00048" ref-type="bibr">21</xref>]. The deviations from natural hearing included processing delays ranging from 0 to 9.7 ms, with some devices showing left-right channel asymmetries up to 9.5 ms, and frequency response deviations of up to 15 dB relative to the open-ear configuration. While some devices maintained better acoustic transparency than others, significant compromises in frequency response accuracy and spatial cue preservation were evident in all studies, highlighting the technical challenges in achieving natural sound transmission through hearables. While these acoustic measurements provide crucial insights into signal modification, complementary perceptual studies have examined user experience through sound quality ratings assessments, demonstrating variable outcomes across different device implementations and listening scenarios [<xref rid="B22-audiolres-15-00048" ref-type="bibr">22</xref>]. However, despite extensive objective measurements, the extent to which these acoustic alterations translate to real-world localization deficits, particularly in the presence of environmental noise, remains unclear. Recent work by Watanabe and Terada (2023) [<xref rid="B11-audiolres-15-00048" ref-type="bibr">11</xref>] provides important behavioral evidence on spatial hearing degradation when using transparency mode for some of the available commercial hearables, demonstrating a significant decline in horizontal sound localization accuracy from 91.5% (open ear) to 68.9% (device with transparency mode). Although this work provided important information on transparency mode&#x02019;s impact on spatial hearing, its evaluation was limited to horizontal-plane localization under ideal acoustic conditions. Critical aspects of real-world listening such as elevation perception and the effects of background noise have yet to be explored.</p><p>The presence of background noise introduces additional challenges to spatial hearing. Processing delays and potential hearables&#x02019; misalignment between ears can affect ITD cues, while environmental noise may interact with the devices&#x02019; output through acoustic leakage, potentially resulting in comb-filtering effects [<xref rid="B20-audiolres-15-00048" ref-type="bibr">20</xref>]. Despite these real-world challenges, current evaluations of hearables&#x02019; transparency modes have predominantly occurred under idealized acoustic conditions, limiting our understanding of their performance in everyday listening scenarios.</p><p>The present study addresses this critical gap by conducting a comprehensive evaluation of how active hearables affect spatial hearing abilities with and without background noise. We employ a dual-methodology approach that uniquely combines objective head-related impulse response (HRIR) measurements with behavioral assessments of localization ability in normal-hearing listeners. By examining performance across different signal-to-noise ratios (SNRs), we quantify how these devices maintain spatial cues under conditions that challenge the auditory system&#x02019;s natural localization mechanisms. This integrated acoustic and behavioral investigation provides insight into the effectiveness of current transparency mode implementations and their implications for maintaining spatial awareness in noisy environments.</p></sec><sec sec-type="methods" id="sec2-audiolres-15-00048"><title>2. Methods</title><sec id="sec2dot1-audiolres-15-00048"><title>2.1. Listeners</title><p>Participants included 10 adults ranging in age from 22 to 35 years (median = 24; 3 female). All participants had normal hearing as confirmed by a standard pure-tone audiogram [<xref rid="B23-audiolres-15-00048" ref-type="bibr">23</xref>], where their hearing was within 20 dB of audiometric zero across the independent standard audiometric test frequencies, 250&#x02013;8000 Hz, and no prior history of hearing impairment. The experimental procedures were approved by the Institutional Review Board of the University of Miami (20211149), and all participants provided informed consent.</p></sec><sec id="sec2dot2-audiolres-15-00048"><title>2.2. Hearable Device</title><p>Participants were fitted with first-generation Apple AirPods Pro (Model A2084, Apple Inc., Cupertino, CA, USA) for the experimental protocol. The transparency mode was set, which permits environmental sound passthrough while maintaining a balanced acoustic profile. At the time of data collection, devices were running the manufacturer&#x02019;s most recent firmware version, ensuring standardized operational parameters. The transparency mode has a minimal audio processing latency to maintain real-time acoustic perception. No individual device-specific modifications were implemented that could introduce variability in the acoustic signal processing, thus maintaining consistent experimental conditions.</p><p>Proper insertion was verified using the built-in &#x02018;Ear Tip Fit Test&#x02019; in the AirPods settings. The process involves playing a specific music track through the earbuds while built-in microphones measure the acoustic response within the ear canal to assess seal quality and leakage. If the fit was not ideal, the AirPods were re-positioned until they passed the check.</p></sec><sec id="sec2dot3-audiolres-15-00048"><title>2.3. Setup</title><p>The experiments took place in a sound-attenuated auditory booth (4.3 &#x000d7; 4.3 &#x000d7; 2 m) equipped with an array of 72 speakers (e301, KEF, Maidstone, UK) arranged in a 1.3 m radius. Target sounds were presented from 47 speakers spanning &#x02212;90&#x000b0; to 90&#x000b0; in azimuth and &#x02212;30&#x000b0; to 30&#x000b0; in elevation. Audio routing was accomplished through a Focusrite RedNet PCIeR (Focusrite, High Wycombe, UK) connected to a set of 3 Sonible d:24 multi-channel amplifiers (Sonible, Graz, Austria) with a sampling rate of 44.1 kHz. Stimuli presentation and the analysis of the responses were implemented in MATLAB (ver. R2022a, The MathWorks, Natick, MA, USA) using the Psychtoolbox-3.0.19 extension [<xref rid="B24-audiolres-15-00048" ref-type="bibr">24</xref>,<xref rid="B25-audiolres-15-00048" ref-type="bibr">25</xref>,<xref rid="B26-audiolres-15-00048" ref-type="bibr">26</xref>]. Lab Streaming Layer library (<uri xlink:href="https://github.com/sccn/labstreaminglayer">https://github.com/sccn/labstreaminglayer</uri>; accessed on 10 March 2020) was implemented for device time synchronization. All levels were measured using a sound pressure level meter (NSRTW mk4, Convergence Instrument, Sherbrooke, QC, Canada) positioned at the center of the speaker array.</p></sec><sec id="sec2dot4-audiolres-15-00048"><title>2.4. Objective HRIR Measures</title><p>To analyze the impact of the hearables on the spatial cues (i.e., ILDs, ITDs, and spectral cues), HRIRs were recorded in silence and noise, with and without the AirPods.</p><sec><title>Stimuli and Procedures</title><p>Binaural recordings were obtained by positioning the dummy head (Neuman KU 100, Georg Neumann GmbH, Berlin, Germany) at the center of the speaker array. The binaural dummy head has physical approximate dimensions of 280 mm &#x000d7; 180 mm &#x000d7; 220 mm, and includes two microphone capsules built into its anatomically realistic pinnae (20 Hz to kHz). Data were registered through a Focusrite RedNet MP8R (Focusrite, High Wycombe, UK) at a sample rate of 44.1 kHz. Proper positioning was verified by ensuring equidistance from speakers at &#x02212;90&#x000b0; and 90&#x000b0;, and equivalent distance from both ears to speakers at 0&#x000b0; and 180&#x000b0;. Logarithmic sine sweeps (15 s, 20 Hz to 20 kHz) were presented at 70 dBA from &#x000b1;90&#x000b0; in steps of 10&#x000b0;. Additional recordings were made at 50, 60 and 70 dBA with constant Gaussian white noise (55 dBA) to achieve signal-to-noise ratios of &#x02212;5 dB, 5 dB, and +15 dB. Binaural HRIRs were then obtained by convolving the recorded sweeps with the inverse of the original sweep [<xref rid="B27-audiolres-15-00048" ref-type="bibr">27</xref>]. All measurements were repeated with Apple AirPods Pro inserted into the dummy head&#x02019;s ears.</p><p>ITDs were extracted through a multi-stage filtering process. First, a linear phase finite impulse response (FIR) bandpass filter (0.1&#x02013;1.5 kHz) was applied to the HRIRs to preserve phase information in the frequency range most relevant for ITD processing. The filtered signals underwent half-wave rectification, followed by a 1.4 kHz Brickwall lowpass filter to remove high-frequency components [<xref rid="B28-audiolres-15-00048" ref-type="bibr">28</xref>]. ITDs were then computed by identifying the peak of the cross-correlation function between the left and right filtered HRIRs from corresponding sweeps [<xref rid="B29-audiolres-15-00048" ref-type="bibr">29</xref>]. The cross-correlation was implemented using MATLAB&#x02019;s xcorr function with a maximum lag parameter of &#x000b1;1 ms, chosen to encompass the physiological limit of human ITDs (~800 &#x000b5;s) [<xref rid="B30-audiolres-15-00048" ref-type="bibr">30</xref>].</p><p>ILDs were extracted by applying an infinite impulse response (IIR) bandpass filter (3&#x02013;20 kHz) to the HRIRs, focusing on the frequency range where head-shadow effects are most prominent. The ILD values were then computed as the difference in root mean square (RMS) amplitude between the filtered left and right HRIRs.</p><p>Lastly, monaural spectral cues were characterized by computing the magnitude responses of the HRIRs for each elevation angle (ranging &#x000b1;30&#x000b0; in steps of 15&#x000b0;) using Fast Fourier Transform (FFT) analysis between 3 kHz and 16 kHz, encompassing the frequency range critical for elevation perception.</p></sec></sec><sec id="sec2dot5-audiolres-15-00048"><title>2.5. Behavioral Localization Measures</title><p>To examine how transparency mode in hearables affects spatial hearing abilities, we conducted behavioral sound localization experiments in normal-hearing listeners under various acoustic conditions. We examined both azimuthal and elevation localization abilities across different signal types and noise conditions.</p><sec sec-type="methods" id="sec2dot5dot1-audiolres-15-00048"><title>2.5.1. Procedures</title><p>Subjects sat comfortably in a chair positioned at the center of the speaker array, facing 0&#x000b0; azimuth and 0&#x000b0; elevation. The room was kept completely dark to eliminate visual cues and prevent response bias from known speaker locations. Head orientation was tracked using a camera system consisting of 6 trackers (Flex 3 Optitrack, Natural Point, Corvallis, OR, USA) and a custom headband with reflective markers (see <xref rid="audiolres-15-00048-f001" ref-type="fig">Figure 1</xref>A,B). Additionally, the headband was equipped with a laser pointer to help participants align their head and eye gaze with a central LED light mounted on the reference loudspeaker.</p><p>For each trial, participants first aligned their laser pointer with the central LED and pressed a button to indicate readiness. Upon button press, the LED turned off and after a randomized delay (200&#x02013;300 ms), the stimulus was presented from a random speaker location within the front hemifield (spanning &#x000b1;90&#x000b0; azimuth and &#x000b1;30&#x000b0; elevation). Participants were instructed to point their head toward the perceived sound source location as quickly and accurately as possible. Once the head orientation was recorded, the central LED reactivated, signaling participants to return to the starting position for the next trial.</p></sec><sec id="sec2dot5dot2-audiolres-15-00048"><title>2.5.2. Test Conditions</title><p>Two primary experimental conditions were tested: normal hearing (NH) and with AirPods Pro in transparency mode. Each condition was evaluated both in silence and with background noise. In silent conditions, the target stimulus consisted of a 150 ms Gaussian white noise burst filtered into three frequency ranges: broadband (BB, 0.2&#x02013;20 kHz), low-pass (LP, 0.2&#x02013;1.5 kHz), and high-pass (HP, 3&#x02013;20 kHz). While LP and HP filtered noise bursts were presented at a fixed level of 60 dBA, the BB noise was tested at three intensities: 50, 60, and 70 dBA.</p><p>For conditions with background noise, the target stimulus was a 150 ms broadband buzzer noise presented at three intensities (50, 60, and 70 dBA) against continuous Gaussian white noise fixed at 55 dBA. This configuration produced three signal-to-noise ratios (SNRs): &#x02212;5, +5, and +15 dB. Background noise was spatially distributed through eight speakers positioned at &#x000b1;30&#x000b0;, &#x000b1;70&#x000b0;, &#x000b1;110&#x000b0;, and &#x000b1;150&#x000b0; in azimuth (0&#x000b0; elevation) surrounding the listener. Target stimuli were presented following a randomized delay of 200&#x02013;300 ms after noise onset.</p></sec><sec id="sec2dot5dot3-audiolres-15-00048"><title>2.5.3. Data Analysis</title><p>Head orientation trajectories were recorded throughout each trial (<xref rid="audiolres-15-00048-f001" ref-type="fig">Figure 1</xref>A). Movement onset (defining reaction time) and offset (defining final response position) were determined using a velocity threshold criterion of 20 degrees/s (<xref rid="audiolres-15-00048-f001" ref-type="fig">Figure 1</xref>B). Sound localization performance was quantified through linear regression analysis using a least-squares error criterion. For both azimuth and elevation dimensions, the relationship between target azimuth/elevation angles (<italic toggle="yes">&#x003b1;</italic><sub>T</sub> and <italic toggle="yes">&#x003b5;</italic><sub>T</sub>, respectively) and response azimuth/elevation angle (<italic toggle="yes">&#x003b1;</italic><sub>R</sub> and <italic toggle="yes">&#x003b5;</italic><sub>R</sub>) was obtained using the following equations:<disp-formula id="FD1-audiolres-15-00048"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="sans-serif">&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="sans-serif">&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mspace width="30pt"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the slope or gain of the best-fit regression line (dimensionless) and <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> the intercept or bias (in degrees). We also computed the coefficient of determination (r<sup>2</sup>), or goodness of fit, for all linear fits. Note that a perfect localization response should yield a gain of 1, bias of 0&#x000b0; and a high coefficient of determination of 1.0.</p><p>Furthermore, to quantify an overall measure of the response accuracy, we also used the mean absolute error (MAE) across trials, according to the following equations:<disp-formula id="FD2-audiolres-15-00048"><label>(2)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="sans-serif">&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="sans-serif">&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mspace width="30pt"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
with <italic toggle="yes">N</italic> representing the total number of trials and <italic toggle="yes">n</italic> representing each individual trial.</p><p>Reaction times (RTs) were calculated as the difference between the onset of the auditory target and the onset of the participant&#x02019;s head movement (measured in milliseconds). As is typical in human response data, RTs followed a positively skewed distribution due to a subset of longer-latency trials. To normalize this distribution and allow for more robust statistical analyses, we transformed reaction times into their reciprocals (1/RT, in s<sup>&#x02212;1</sup>), a measure known as promptness [<xref rid="B31-audiolres-15-00048" ref-type="bibr">31</xref>,<xref rid="B32-audiolres-15-00048" ref-type="bibr">32</xref>,<xref rid="B33-audiolres-15-00048" ref-type="bibr">33</xref>]. This transformation yields a distribution that more closely approximates normality, where higher promptness values reflect faster responses, and lower values reflect slower reactions.</p></sec></sec><sec id="sec2dot6-audiolres-15-00048"><title>2.6. Statistical Analysis</title><p>We analyzed the data using estimation statistics to provide a comprehensive understanding of effect sizes and their precision [<xref rid="B34-audiolres-15-00048" ref-type="bibr">34</xref>]. Specifically, we employed a bias-corrected and accelerated bootstrap resampling technique to estimate the 95% confidence intervals of the mean differences. The analysis was conducted using the DABEST (Data Analysis with Bootstrap-Coupled Estimation) package in MATLAB [<xref rid="B34-audiolres-15-00048" ref-type="bibr">34</xref>]. Throughout the Results section, data distributions are presented using violin plots, which illustrate the full range of the data along with the 25th and 75th percentiles and the median. Individual data points were plotted for each group and tested condition.</p><p>To assess statistical significance, we performed permutation t-tests with 5000 reshuffles of control and test labels for each <italic toggle="yes">p</italic>-value calculation. Due to the non-normality of the data, a permutation approach is used to provide a more robust analysis for the skewed distributions. This method presents a clearer understanding of the interventions&#x02019; effect size and its precision, thereby providing a more reliable inference framework [<xref rid="B34-audiolres-15-00048" ref-type="bibr">34</xref>]. The reported <italic toggle="yes">p</italic>-values represent the probability of observing the observed effect sizes under the null hypothesis of zero difference.</p></sec></sec><sec sec-type="results" id="sec3-audiolres-15-00048"><title>3. Results</title><sec id="sec3dot1-audiolres-15-00048"><title>3.1. Objective HRIR Measures in Quiet</title><p>Analysis of the binaural cues derived from measured HRIRs revealed distinct patterns across device conditions in the horizontal plane (&#x000b1;90&#x000b0;; <xref rid="audiolres-15-00048-f002" ref-type="fig">Figure 2</xref>). Under NH condition, ITDs showed the expected monotonic variation with azimuth, spanning the physiologically relevant range of &#x000b1;700 &#x000b5;s. Due to the acoustic leakage in the low frequencies, the use of AirPods does not alter temporal relationships, demonstrating deviations from NH condition of less than 45 &#x000b5;s across all measured angles. In contrast, ILDs showed substantial alterations when measured through the AirPods compared to NH. The most pronounced deviations manifested as systematic amplification of the natural level differences, with maximum increases of 8.0 dB and 7.5 dB observed at &#x02212;50&#x000b0; and 40&#x000b0; azimuth, respectively. This pattern of ILD enhancement diminished near the median plane, where differences between conditions were constrained to 0.5&#x02013;2.0 dB, suggesting a spatial dependence in the device&#x02019;s impact on interaural intensity cues.</p><p>The monaural spectral cues were substantially altered when comparing NH to the AirPods condition (<xref rid="audiolres-15-00048-f003" ref-type="fig">Figure 3</xref>). As expected, the characteristic spectral notches and peaks above 5 kHz were eliminated by the AirPods.</p></sec><sec id="sec3dot2-audiolres-15-00048"><title>3.2. Sound Localization in Quiet</title><p>Sound localization performance was evaluated in quiet for BB (50, 60, and 70 dBA), LP (60 dBA), and HP (60 dBA) (see Methods). <xref rid="audiolres-15-00048-f004" ref-type="fig">Figure 4</xref> depicts an example of a participant&#x02019;s sound localization results in each hearing condition. The listener accurately localizes horizontal sound sources in the NH condition, achieving g<sub>&#x003b1;</sub> values of 1, b<sub>&#x003b1;</sub> close to 0, r<sup>2</sup><sub>&#x003b1;</sub> of ~1 and MAE<sub>&#x003b1;</sub> &#x02264; 5&#x000b0; across all stimuli. In the AirPods listening condition, azimuth responses are mostly accurate for the LP stimuli, yielding a bigger spread on the responses for BB and HP stimuli. In general, azimuth MAE with AirPods showed a bigger overall MAE than in the NH condition (&#x02265;9&#x000b0;), being the highest for BB and HP (14&#x000b0; and 16&#x000b0;, respectively). In elevation, the listener showed an expected normal-hearing outcome with target&#x02013;response relationships for BB and HP, and with poor localization in LP due to lack of monaural spectral pinna cues in that frequency range. However, when examining the elevation responses with the AirPods, poor performance is observed across all stimuli. In this case, gains were practically 0, demonstrating the absence of a target&#x02013;response relationship, together with large MAEs and low correlation values (i.e., big data spread).</p><p>Overall results for sound localization performance without background noise for all listeners are shown in <xref rid="audiolres-15-00048-f005" ref-type="fig">Figure 5</xref>. In the NH condition, listeners generally demonstrated accurate sound localization in azimuth and elevation in the NH condition. Across all stimuli, azimuth gain was approximately 1 (g<sub>&#x003b1;</sub> = 0.97 &#x000b1; 0.31), bias was close to 0&#x000b0; (b<sub>&#x003b1;</sub> = 0.72&#x000b0; &#x000b1; 1.52&#x000b0;), correlation was high (r<sup>2</sup><sub>&#x003b1;</sub> = 0.97 &#x000b1; 0.01) and overall error was low (MAE<sub>&#x003b1;</sub> = 6.81&#x000b0; &#x000b1; 0.99&#x000b0;). Sound localization in the vertical plane also yielded expected results, as BB and HP yielded high gains (g<sub>&#x003b5;</sub> = 0.93 &#x000b1; 0.05), bias close to 0 (b<sub>&#x003b5;</sub> = 1.42&#x000b0; &#x000b1; 0.96), reasonable correlation (r<sup>2</sup><sub>&#x003b5;</sub> = 0.83 &#x000b1; 0.03) and low error (MAE<sub>&#x003b5;</sub> = 6.79&#x000b0; &#x000b1; 0.59&#x000b0;). Low-frequency vertical sound localization was poor as expected, evidenced by a low regression slope (g<sub>&#x003b5;</sub> = 0.17 &#x000b1; 0.08), low correlation (r<sup>2</sup><sub>&#x003b5;</sub> = 0.13 &#x000b1; 0.07) and high error (MAE<sub>&#x003b5;</sub> = 15&#x000b0; &#x000b1; 1.43&#x000b0;). Data showed no significant differences across BB levels on either condition for azimuth: g<sub>&#x003b1;</sub> (<italic toggle="yes">p</italic> &#x0003e; 0.06), b&#x003b1; (<italic toggle="yes">p</italic> &#x0003e; 0.08), r<sup>2</sup><sub>&#x003b1;</sub> (<italic toggle="yes">p</italic> &#x0003e; 0.06), or MAE<sub>&#x003b1;</sub> (<italic toggle="yes">p</italic> &#x0003e; 0.09). Similarly, there were no significant differences across BB levels for elevation values: g<sub>&#x003b5;</sub> (<italic toggle="yes">p</italic> &#x0003e; 0.26), b&#x003b5; (<italic toggle="yes">p</italic> &#x0003e; 0.33), r<sup>2</sup><sub>&#x003b5;</sub> (<italic toggle="yes">p</italic> &#x0003e; 0.14), MAE<sub>&#x003b5;</sub> (<italic toggle="yes">p</italic> &#x0003e; 0.11). Therefore, BB results were pooled across levels for azimuth and elevation.</p><p>When listening through AirPods, data generally showed a larger spread and higher localization errors than NH. This resulted in significant differences between AirPods and NH r<sup>2</sup><sub>&#x003b1;</sub> for BB (<italic toggle="yes">p</italic> &#x0003c; 0.001) and HP (<italic toggle="yes">p</italic> &#x0003c; 0.001), but not for LP (<italic toggle="yes">p</italic> = 0.103). Moreover, MAEs for the AirPod conditions were higher than NH for BB (<italic toggle="yes">p</italic> &#x0003c; 0.05) and HP (<italic toggle="yes">p</italic> &#x0003c; 0.001) but, again, did not reach significance for LP (<italic toggle="yes">p</italic> = 0.053). A review of the data demonstrates large overall errors of two subjects in the NH condition, which may explain the non-marginal statistical difference for the LP stimuli (see <xref rid="audiolres-15-00048-f005" ref-type="fig">Figure 5</xref>G, LP).</p><p>Noticeably, vertical sound localization gain with the AirPods was poor across stimuli (<italic toggle="yes">g<sub>&#x003b5;</sub></italic> = 0.07 &#x000b1; 0.05) and significantly different than NH for BB and HP (<italic toggle="yes">p</italic> &#x0003c; 0.001). This condition also elicited a significant perceptual change upward in bias for BB and HP (<italic toggle="yes">b<sub>&#x003b5;</sub></italic> = 9.01&#x000b0; &#x000b1; 3.41&#x000b0;; <italic toggle="yes">p</italic> &#x0003c; 0.05). Similarly, due to the spread of the data, there was a substantial drop in BB and HP correlation values (<italic toggle="yes">r</italic><sup>2</sup><italic toggle="yes"><sub>&#x003b5;</sub></italic> = 0.04 &#x000b1; 0.02; <italic toggle="yes">p</italic> &#x0003c; 0.001). Lastly, the same frequencies exhibited a significant increase in error (MAE<italic toggle="yes"><sub>&#x003b5;</sub></italic> = 19.4&#x000b0; &#x000b1; 1.59&#x000b0;; <italic toggle="yes">p</italic> &#x0003c; 0.001), reaching more than double their error in the NH condition.</p></sec><sec id="sec3dot3-audiolres-15-00048"><title>3.3. Objective HRIR Measures in Background Noise</title><p><xref rid="audiolres-15-00048-f006" ref-type="fig">Figure 6</xref> presents the binaural cues measurements obtained for each SNR level in background noise (see methods). ITDs and ILDs remained consistent throughout each SNR level for the NH recordings. With the AirPods, ITDs remain mostly preserved despite decreasing SNR, with the largest deviation of &#x02212;113 &#x000b5;s at 0&#x000b0; for the ITD at &#x02212;5 SNR. However, ILDs exhibited a reduced range, with values between &#x02212;11 dB and 14 dB, compared to the NH range of &#x02212;17 dB to 20 dB. As SNR decreased, there was a noticeable bias approaching +90&#x000b0; at +5 dB (+4 dB at 0&#x000b0; azimuth), and at &#x02212;5 dB SNR, errors shifted toward the medial plane. Additionally, at &#x02212;5 dB SNR, the characteristic sigmoid curve was disrupted between 0&#x000b0; and &#x000b1;10&#x000b0; azimuth.</p></sec><sec id="sec3dot4-audiolres-15-00048"><title>3.4. Sound Localization in Background Noise</title><p>In <xref rid="audiolres-15-00048-f007" ref-type="fig">Figure 7</xref>, the same participant as presented in <xref rid="audiolres-15-00048-f004" ref-type="fig">Figure 4</xref> is shown under different SNR conditions. For NH, the results indicate accurate azimuth localization for +5 dB and +15 dB SNR levels, while minor performance decrease is observed at &#x02212;5 dB SNR, indicated by a decrease in r<sup>2</sup><sub>&#x003b1;</sub> and an increase in MAE<sub>&#x003b1;</sub>. When using AirPods, MAE<sub>&#x003b1;</sub> increased by at least 6&#x000b0; overall when compared to the NH condition. Similar to NH, azimuth localization with AirPods was generally less accurate at lower SNRs, with a larger decrease of 0.12 in r<sup>2</sup>&#x003b1; and an increase of 6&#x000b0; in MAE<sub>&#x003b1;</sub> from +5 dB to &#x02212;5 dB SNR. As expected, vertical localization in noise for NH was consistently lower in accuracy than for azimuth. Performance was relatively close to target values at +15 dB SNR but gradually declined as SNR decreased. With AirPods, localization accuracy in elevation was poor across all SNRs (&#x02212;5, +5, and +15 dB), showing consistently low gain values, indicating a lack of a reliable target-response relationship. These results yield large MAE<sub>&#x003b5;</sub> and low r<sup>2</sup>&#x003b5; values, demonstrating substantial variability and error in elevation localization.</p><p>Overall results for sound localization performance in background noise are shown in <xref rid="audiolres-15-00048-f008" ref-type="fig">Figure 8</xref>. In the NH condition, listeners generally demonstrated accurate azimuth localization across all SNRs, with near-perfect gain (g<sub>&#x003b1;</sub> = 0.99 &#x000b1; 0.06), minimal bias (b<sub>&#x003b1;</sub> = 0.13&#x000b0; &#x000b1; 2.11&#x000b0;), high correlation (r<sup>2</sup><sub>&#x003b1;</sub> = 0.96 &#x000b1; 0.02), and low error (MAE<sub>&#x003b1;</sub> = 7.58&#x000b0; &#x000b1; 1.45&#x000b0;). However, as SNR decreased, a decline in performance was observed in both r<sup>2</sup><sub>&#x003b1;</sub> and MAE<sub>&#x003b1;</sub> (<italic toggle="yes">p</italic> &#x0003c; 0.05). This SNR change appeared to have a larger impact on vertical sound localization. Besides elevation bias (<italic toggle="yes">p</italic> = 0.628), gain (g<sub>&#x003b5;</sub>; <italic toggle="yes">p</italic> &#x0003c; 0.001), goodness of fit (r<sup>2</sup><sub>&#x003b5;</sub>; <italic toggle="yes">p</italic> &#x0003c; 0.001), and mean absolute error (MAE<sub>&#x003b5;</sub>; <italic toggle="yes">p</italic> &#x0003c; 0.05) showed significantly poorer outcomes as SNR decreased.</p><p>Localization in noise with AirPods yielded an overall poorer performance compared to NH. Although azimuth gain was only significantly different at &#x02212;5 dB SNR (<italic toggle="yes">p</italic> &#x0003c; 0.001), MAE<sub>&#x003b1;</sub> and r<sup>2</sup><sub>&#x003b1;</sub> were poorer in all SNRs (<italic toggle="yes">p</italic> &#x0003c; 0.001). This was not the case for azimuth bias between listening conditions (<italic toggle="yes">p</italic> &#x0003e; 0.387). As SNR decreased, increased response variability and errors were reflected in r<sup>2</sup><sub>&#x003b1;</sub> and MAE<sub>&#x003b1;</sub>. The azimuth goodness of fit (r<sup>2</sup><sub>&#x003b1;</sub>) was 0.93 &#x000b1; 0.02 at +15 dB SNR, but significantly dropped to 0.84 &#x000b1; 0.03 at the lowest SNR (<italic toggle="yes">p</italic> &#x0003c; 0.05). Similarly, the MAE<sub>&#x003b1;</sub> increased from 8.2&#x000b0; &#x000b1; 1.1&#x000b0; at +15 dB SNR to 19.6&#x000b0; &#x000b1; 4.6&#x000b0;, showing a significant inverse relationship with SNR.</p><p>As expected, elevation localization with AirPods was significantly worse in noisy conditions than NH. Significant differences were observed across all SNR levels, including reduced gain (g<sub>&#x003b5;</sub> = &#x02212;0.02 &#x000b1; 0.17; <italic toggle="yes">p</italic> &#x0003c; 0.001), increased bias (b<sub>&#x003b5;</sub> = 7.9&#x000b0; &#x000b1; 6.5&#x000b0;; <italic toggle="yes">p</italic> &#x0003c; 0.05), decreased correlation (r<sup>2</sup><sub>&#x003b5;</sub> = 0.07 &#x000b1; 0.07; <italic toggle="yes">p</italic> &#x0003c; 0.001), and higher mean absolute error (MAE<sub>&#x003b5;</sub> = 19.6&#x000b0; &#x000b1; 4.4&#x000b0;; <italic toggle="yes">p</italic> &#x0003c; 0.001). In this condition, SNR did not affect g<sub>&#x003b5;</sub> (<italic toggle="yes">p</italic> = 0.315), b<sub>&#x003b5;</sub> (<italic toggle="yes">p</italic> = 0.714), r<sup>2</sup><sub>&#x003b5;</sub> (<italic toggle="yes">p</italic> = 0.386), or MAE<sub>&#x003b5;</sub> (<italic toggle="yes">p</italic> = 0.787).</p></sec><sec id="sec3dot5-audiolres-15-00048"><title>3.5. Sound Localization Promptness</title><p>Response promptness by group and for sound localization with and without noise is shown in <xref rid="audiolres-15-00048-f009" ref-type="fig">Figure 9</xref>. Listeners displayed increased promptness on the localization tasks performed in quiet (M = 4.77 &#x000b1; 0.15 s<sup>&#x02212;1</sup>) compared to in background noise (M= 3.64 &#x000b1; 0.06 s<sup>&#x02212;1</sup>; <italic toggle="yes">p</italic> &#x0003c; 0.001). Specifically, listeners are faster to initiate the localization response where there is no competing noise. Review of <xref rid="audiolres-15-00048-f009" ref-type="fig">Figure 9</xref> reveals greater variance in the response times of listeners in quiet compared to in noise, likely reflecting the difference in cognitive demand allowing for more fluctuations and individualized differences in response patterns, whereas competing noise requires more focus narrowing the window of response times. There was no significant effect of stimulus type for localization in quiet conditions (<xref rid="audiolres-15-00048-f009" ref-type="fig">Figure 9</xref>, left panel; <italic toggle="yes">p</italic> &#x0003e; 0.05) or by SNR (<xref rid="audiolres-15-00048-f009" ref-type="fig">Figure 9</xref>, right panel; <italic toggle="yes">p</italic> &#x0003e; 0.05).</p></sec></sec><sec sec-type="discussion" id="sec4-audiolres-15-00048"><title>4. Discussion</title><p>Spatial hearing is essential for effective interaction with our environment, as it allows individuals to localize sound sources, navigate safely, and respond appropriately to auditory information. Our study provides a comprehensive assessment of how transparency mode in commercial hearables affects spatial hearing abilities under both ideal and challenging acoustic conditions&#x02014;a critical gap identified in previous research. Through combined objective and behavioral measures, we demonstrate significant alterations in spatial cues and their perceptual consequences.</p><p>As active hearables grow in popularity and technical sophistication, they are increasingly replacing passive headphones that lack spatial awareness capabilities. The development of transparency mode represents a key milestone in this evolution, aimed at restoring acoustic cues critical for environmental perception and user safety. However, consistent with prior technical measurements, our study found that spatial cues were compromised when using AirPods in transparency mode [<xref rid="B11-audiolres-15-00048" ref-type="bibr">11</xref>,<xref rid="B20-audiolres-15-00048" ref-type="bibr">20</xref>,<xref rid="B21-audiolres-15-00048" ref-type="bibr">21</xref>]. The preservation of ITDs can be attributed to acoustic leakage in low frequencies, allowing direct acoustic signals to reach the ear canal. In contrast, ILDs showed systematic deviations from the open ear pattern, particularly in the presence of background noise. These changes were likely influenced by the additional processing required for the real-time pass-through audio algorithm and the effect of background noise on this processing. As expected, monaural pinna cues for elevation were completely disrupted above 5 kHz due to the unavoidable alterations in acoustic interactions between sound waves and the natural shape of the pinnae caused by the hearables.</p><p>Subjects&#x02019; sound localization performance reflected the impact of compromised auditory spatial cues. Vertical sound localization was completely disrupted across all listening conditions with AirPods. The absence of vertical cues is crucial not only for up&#x02013;down localization but also for front&#x02013;back sound discrimination. The safety concerns associated with using hearable devices remain even with the transparency mode, as detecting sounds from behind relies solely on auditory input without visual support. Notably, these devices are primarily used by individuals without hearing loss who typically have no difficulty perceiving sounds from the front or back. Using a hearable in transparency mode can impair this capacity, effectively creating a limitation that would not otherwise exist.</p><p>Azimuth localization was significantly affected for sound stimuli with a high-frequency range (&#x0003e;3 kHz), with some responses showing errors exceeding 40&#x000b0;. These errors were further exacerbated by background noise, particularly at negative SNRs, highlighting potential challenges in real-world environments. Interestingly, response promptness was significantly reduced in the presence of background noise but remained unaffected while listening through hearables (<xref rid="audiolres-15-00048-f009" ref-type="fig">Figure 9</xref>). This finding aligns with previous research showing that noise can degrade performance and increase reaction times during localization tasks [<xref rid="B35-audiolres-15-00048" ref-type="bibr">35</xref>]. In contrast, the acoustic modifications introduced by the hearables&#x02019; transparency mode did not appear to hinder response speed. This suggests that, for NH listeners, these devices do not substantially increase listening effort under the tested conditions. As reaction time can reflect underlying cognitive demand and listening effort [<xref rid="B36-audiolres-15-00048" ref-type="bibr">36</xref>,<xref rid="B37-audiolres-15-00048" ref-type="bibr">37</xref>], these results suggest that background noise imposes a measurable increase in effort, whereas using AirPods, regardless of the condition, might not alter the cognitive load required for spatial hearing.</p><p>The implications of our findings extend beyond consumer applications to the emerging role of hearables as over-the-counter hearing aids. Interestingly, sound localization performance with AirPods in transparency mode is comparable to the levels reported for hearing aid users in the literature [<xref rid="B38-audiolres-15-00048" ref-type="bibr">38</xref>,<xref rid="B39-audiolres-15-00048" ref-type="bibr">39</xref>,<xref rid="B40-audiolres-15-00048" ref-type="bibr">40</xref>]. As active hearables become more accessible and affordable compared to traditional hearing aids, these initial comparable outcomes are promising. However, for a more comprehensive evaluation, listeners with different types of hearing loss should be tested to validate these findings.</p><p>Our integrated acoustic and behavioral approach reveals that while transparency mode enables environmental awareness, it introduces significant compromises in spatial hearing abilities that may impact user safety and listening experience. These findings highlight the need for technological improvements in maintaining natural spatial hearing cues through transparency mode, particularly for high-frequency spectral cues critical for elevation perception. Despite these current limitations, mass-produced hearable technology may offer some of the same benefits as more expensive devices for individuals with hearing impairment, potentially improving accessibility to hearing healthcare. Future research should evaluate spatial hearing performance in listeners with hearing loss, focusing not only on sound localization but also on spatial speech understanding in noise&#x02014;a critical aspect of real-world listening that will help validate these devices&#x02019; effectiveness across diverse user populations and listening environments.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, S.A.A.; methodology, S.A.A.; software, S.A.A. and N.E.; validation, S.A.A. and N.E.; formal analysis, S.A.A. and N.E.; investigation, S.A.A.; resources, S.A.A., C.B. and H.A.S.; data curation, S.A.A. and N.E.; writing&#x02014;original draft preparation, S.A.A. and N.E.; writing&#x02014;review and editing, S.A.A., N.E., C.B. and H.A.S.; visualization, S.A.A. and N.E.; supervision, S.A.A.; project administration, S.A.A. and H.A.S. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>This research was conducted according to the principles expressed in the Declaration of Helsinki and was approved by the Institutional Review Board at the University of Miami (IRB#20211149), approved date: 21 January 2022.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data can be made available upon request, subject to the discretion of the authors.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-audiolres-15-00048"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Plazak</surname><given-names>J.</given-names></name>
<name><surname>Kersten-Oertel</surname><given-names>M.</given-names></name>
</person-group><article-title>A Survey on the Affordances of &#x0201c;Hearables&#x0201d;</article-title><source>Inventions</source><year>2018</year><volume>3</volume><elocation-id>48</elocation-id><pub-id pub-id-type="doi">10.3390/inventions3030048</pub-id></element-citation></ref><ref id="B2-audiolres-15-00048"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Crum</surname><given-names>P.</given-names></name>
</person-group><article-title>Hearables: Here Come the: Technology Tucked inside Your Ears Will Augment Your Daily Life</article-title><source>IEEE Spectr.</source><year>2019</year><volume>56</volume><fpage>38</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1109/MSPEC.2019.8701198</pub-id></element-citation></ref><ref id="B3-audiolres-15-00048"><label>3.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>IDC</collab>
</person-group><article-title>Wearable Devices Market Insights</article-title><comment>Available online: <ext-link xlink:href="https://www.idc.com/promo/wearablevendor" ext-link-type="uri">https://www.idc.com/promo/wearablevendor</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-01-14">(accessed on 14 January 2025)</date-in-citation></element-citation></ref><ref id="B4-audiolres-15-00048"><label>4.</label><element-citation publication-type="gov"><person-group person-group-type="author">
<collab>FDA</collab>
</person-group><article-title>FDA Authorizes First Over-the-Counter Hearing Aid Software</article-title><comment>Available online: <ext-link xlink:href="https://www.fda.gov/news-events/press-announcements/fda-authorizes-first-over-counter-hearing-aid-software" ext-link-type="uri">https://www.fda.gov/news-events/press-announcements/fda-authorizes-first-over-counter-hearing-aid-software</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-01-14">(accessed on 14 January 2025)</date-in-citation></element-citation></ref><ref id="B5-audiolres-15-00048"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kayser</surname><given-names>H.</given-names></name>
<name><surname>Ewert</surname><given-names>S.D.</given-names></name>
<name><surname>Anem&#x000fc;ller</surname><given-names>J.</given-names></name>
<name><surname>Rohdenburg</surname><given-names>T.</given-names></name>
<name><surname>Hohmann</surname><given-names>V.</given-names></name>
<name><surname>Kollmeier</surname><given-names>B.</given-names></name>
</person-group><article-title>Database of Multichannel In-Ear and Behind-the-Ear Head-Related and Binaural Room Impulse Responses</article-title><source>EURASIP J. Adv. Signal Process.</source><year>2009</year><volume>2009</volume><fpage>298605</fpage><pub-id pub-id-type="doi">10.1155/2009/298605</pub-id></element-citation></ref><ref id="B6-audiolres-15-00048"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Durin</surname><given-names>V.</given-names></name>
<name><surname>Carlile</surname><given-names>S.</given-names></name>
<name><surname>Guillon</surname><given-names>P.</given-names></name>
<name><surname>Best</surname><given-names>V.</given-names></name>
<name><surname>Kalluri</surname><given-names>S.</given-names></name>
</person-group><article-title>Acoustic Analysis of the Directional Information Captured by Five Different Hearing Aid Styles</article-title><source>J. Acoust. Soc. Am.</source><year>2014</year><volume>136</volume><fpage>818</fpage><lpage>828</lpage><pub-id pub-id-type="doi">10.1121/1.4883372</pub-id><pub-id pub-id-type="pmid">25096115</pub-id>
</element-citation></ref><ref id="B7-audiolres-15-00048"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zimpfer</surname><given-names>V.</given-names></name>
<name><surname>Sarafian</surname><given-names>D.</given-names></name>
</person-group><article-title>Impact of Hearing Protection Devices on Sound Localization Performance</article-title><source>Front. Neurosci.</source><year>2014</year><volume>8</volume><elocation-id>135</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00135</pub-id><pub-id pub-id-type="pmid">24966807</pub-id>
</element-citation></ref><ref id="B8-audiolres-15-00048"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kroener</surname><given-names>L.</given-names></name>
<name><surname>Garcia</surname><given-names>A.</given-names></name>
<name><surname>Zimpfer</surname><given-names>V.</given-names></name>
<name><surname>Langrenne</surname><given-names>C.</given-names></name>
</person-group><article-title>Hearing Protections: Effects on HRTFs and Localization Accuracy</article-title><source>Proceedings of the 23rd International Congress on Acoustics</source><conf-loc>Aachen, Germany</conf-loc><conf-date>9&#x02013;13 September 2019</conf-date></element-citation></ref><ref id="B9-audiolres-15-00048"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Denk</surname><given-names>F.</given-names></name>
<name><surname>Kollmeier</surname><given-names>B.</given-names></name>
</person-group><article-title>The Hearpiece Database of Individual Transfer Functions of an In-the-Ear Earpiece for Hearing Device Research</article-title><source>Acta Acust.</source><year>2021</year><volume>5</volume><fpage>2</fpage><pub-id pub-id-type="doi">10.1051/aacus/2020028</pub-id></element-citation></ref><ref id="B10-audiolres-15-00048"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liebich</surname><given-names>S.</given-names></name>
<name><surname>Vary</surname><given-names>P.</given-names></name>
</person-group><article-title>Occlusion Effect Cancellation in Headphones and Hearing Devices&#x02014;The Sister of Active Noise Cancellation</article-title><source>IEEE ACM Trans. Audio Speech Lang. Process.</source><year>2022</year><volume>30</volume><fpage>35</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1109/TASLP.2021.3130966</pub-id></element-citation></ref><ref id="B11-audiolres-15-00048"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Watanabe</surname><given-names>H.</given-names></name>
<name><surname>Terada</surname><given-names>T.</given-names></name>
</person-group><article-title>Transparency Mode of Hearable Reduces Your Spatial Hearing: Evaluation and Cancelling Method to Restore Spatial Hearing</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>97952</fpage><lpage>97960</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3312713</pub-id></element-citation></ref><ref id="B12-audiolres-15-00048"><label>12.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Blauert</surname><given-names>J.</given-names></name>
</person-group><source>Spatial Hearing: The Psychophysics of Human Sound Localization</source><edition>6th ed.</edition><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>1996</year><isbn>978-0-262-02413-6</isbn></element-citation></ref><ref id="B13-audiolres-15-00048"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Snapp</surname><given-names>H.</given-names></name>
<name><surname>Vogt</surname><given-names>K.</given-names></name>
<name><surname>Agterberg</surname><given-names>M.J.H.</given-names></name>
</person-group><article-title>Bilateral bone conduction stimulation provides reliable binaural cues for localization</article-title><source>Hear. Res.</source><year>2020</year><volume>388</volume><fpage>107881</fpage><pub-id pub-id-type="doi">10.1016/j.heares.2019.107881</pub-id><pub-id pub-id-type="pmid">31945691</pub-id>
</element-citation></ref><ref id="B14-audiolres-15-00048"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Denk</surname><given-names>F.</given-names></name>
<name><surname>Ernst</surname><given-names>S.M.A.</given-names></name>
<name><surname>Ewert</surname><given-names>S.D.</given-names></name>
<name><surname>Kollmeier</surname><given-names>B.</given-names></name>
</person-group><article-title>Adapting Hearing Devices to the Individual Ear Acoustics: Database and Target Response Correction Functions for Various Device Styles</article-title><source>Trends Hear.</source><year>2018</year><volume>22</volume><fpage>2331216518779313</fpage><pub-id pub-id-type="doi">10.1177/2331216518779313</pub-id><pub-id pub-id-type="pmid">29877161</pub-id>
</element-citation></ref><ref id="B15-audiolres-15-00048"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Denk</surname><given-names>F.</given-names></name>
<name><surname>Heeren</surname><given-names>J.</given-names></name>
<name><surname>Ewert</surname><given-names>S.D.</given-names></name>
<name><surname>Kollmeier</surname><given-names>B.</given-names></name>
<name><surname>Ernst</surname><given-names>S.M.A.</given-names></name>
</person-group><article-title>Controlling the Head Position during Individual HRTF Measurements and Its Effect on Accuracy</article-title><source>Proceedings of the Fortschritte der Akustik&#x02014;DAGA</source><conf-loc>Kiel, Germany</conf-loc><conf-date>6&#x02013;9 March 2017</conf-date></element-citation></ref><ref id="B16-audiolres-15-00048"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>H&#x000e4;rm&#x000e4;</surname><given-names>A.</given-names></name>
<name><surname>Jakka</surname><given-names>J.</given-names></name>
<name><surname>Tikander</surname><given-names>M.</given-names></name>
<name><surname>Karjalainen</surname><given-names>M.</given-names></name>
<name><surname>Lokki</surname><given-names>T.</given-names></name>
<name><surname>Hiipakka</surname><given-names>J.</given-names></name>
<name><surname>Lorho</surname><given-names>G.</given-names></name>
</person-group><article-title>Augmented Reality Audio for Mobile and Wearable Appliances</article-title><source>J. Audio Eng. Soc.</source><year>2004</year><volume>52</volume><fpage>618</fpage><lpage>639</lpage></element-citation></ref><ref id="B17-audiolres-15-00048"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hoffmann</surname><given-names>P.F.</given-names></name>
<name><surname>Christensen</surname><given-names>F.</given-names></name>
<name><surname>Hammersh&#x000f8;i</surname><given-names>D.</given-names></name>
</person-group><article-title>Insert Earphone Calibration for Hear-Through Options</article-title><source>Proceedings of the AES 51st Conference on Loudspeakers and Headphones</source><conf-loc>Helsinki, Finland</conf-loc><conf-date>21&#x02013;24 August 2013</conf-date><fpage>3</fpage><lpage>4</lpage></element-citation></ref><ref id="B18-audiolres-15-00048"><label>18.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Killion</surname><given-names>M.C.</given-names></name>
</person-group><article-title>Design and Evaluation of High-Fidelity Hearing Aids</article-title><source>Ph.D. Thesis</source><publisher-name>Northwestern University</publisher-name><publisher-loc>Evanston, IL, USA</publisher-loc><year>1979</year></element-citation></ref><ref id="B19-audiolres-15-00048"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>R&#x000e4;m&#x000f6;</surname><given-names>J.</given-names></name>
<name><surname>V&#x000e4;lim&#x000e4;ki</surname><given-names>V.</given-names></name>
</person-group><article-title>Digital Augmented Reality Audio Headset</article-title><source>J. Electr. Comput. Eng.</source><year>2012</year><volume>2012</volume><fpage>457374</fpage><pub-id pub-id-type="doi">10.1155/2012/457374</pub-id></element-citation></ref><ref id="B20-audiolres-15-00048"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Denk</surname><given-names>F.</given-names></name>
<name><surname>Schepker</surname><given-names>H.</given-names></name>
<name><surname>Doclo</surname><given-names>S.</given-names></name>
<name><surname>Kollmeier</surname><given-names>B.</given-names></name>
</person-group><article-title>Acoustic Transparency in Hearables&#x02014;Technical Evaluation</article-title><source>J. Audio Eng. Soc.</source><year>2020</year><volume>68</volume><fpage>508</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.17743/jaes.2020.0042</pub-id></element-citation></ref><ref id="B21-audiolres-15-00048"><label>21.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Schlieper</surname><given-names>R.</given-names></name>
<name><surname>Preihs</surname><given-names>S.</given-names></name>
<name><surname>Peissig</surname><given-names>J.</given-names></name>
</person-group><article-title>An Open Dataset of Measured HRTFs Perturbed by Headphones</article-title><source>Audio Engineering Society Convention 152</source><publisher-name>Audio Engineering Society</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2022</year><fpage>593</fpage><lpage>598</lpage></element-citation></ref><ref id="B22-audiolres-15-00048"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schepker</surname><given-names>H.</given-names></name>
<name><surname>Denk</surname><given-names>F.</given-names></name>
<name><surname>Kollmeier</surname><given-names>B.</given-names></name>
<name><surname>Doclo</surname><given-names>S.</given-names></name>
</person-group><article-title>Acoustic Transparency in Hearables&#x02014;Perceptual Sound Quality Evaluations</article-title><source>J. Audio Eng. Soc.</source><year>2020</year><volume>68</volume><fpage>495</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.17743/jaes.2020.0045</pub-id></element-citation></ref><ref id="B23-audiolres-15-00048"><label>23.</label><element-citation publication-type="book"><std>ISO 8253-1</std><source>Acoustics-Audiometric Test Methods&#x02013;Part 1: Pure-Tone Air and Bone Conduction Audiometry</source><publisher-name>International Organization for Standardization</publisher-name><publisher-loc>Geneva, Switzerland</publisher-loc><year>2010</year></element-citation></ref><ref id="B24-audiolres-15-00048"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Brainard</surname><given-names>D.H.</given-names></name>
</person-group><article-title>The Psychophysics Toolbox Short Title: The Psychophysics Toolbox Corresponding Author</article-title><source>Spat. Vis.</source><year>1997</year><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id>
</element-citation></ref><ref id="B25-audiolres-15-00048"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pelli</surname><given-names>D.G.</given-names></name>
</person-group><article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title><source>Spat. Vis.</source><year>1997</year><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id><pub-id pub-id-type="pmid">9176953</pub-id>
</element-citation></ref><ref id="B26-audiolres-15-00048"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kleiner</surname><given-names>M.</given-names></name>
<name><surname>Brainard</surname><given-names>D.</given-names></name>
<name><surname>Pelli</surname><given-names>D.</given-names></name>
<name><surname>Ingling</surname><given-names>A.</given-names></name>
<name><surname>Murray</surname><given-names>R.</given-names></name>
<name><surname>Broussard</surname><given-names>C.</given-names></name>
</person-group><article-title>What&#x02019;s new in Psychtoolbox-3?</article-title><source>Perception</source><year>2007</year><volume>36</volume><fpage>14</fpage></element-citation></ref><ref id="B27-audiolres-15-00048"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Farina</surname><given-names>A.</given-names></name>
</person-group><article-title>Simultaneous Measurement of Impulse Response and Distortion with a Swept-Sine Technique</article-title><source>J. Audio Eng. Soc.</source><year>2000</year><volume>48</volume><fpage>331</fpage><lpage>342</lpage></element-citation></ref><ref id="B28-audiolres-15-00048"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dietz</surname><given-names>M.</given-names></name>
<name><surname>Ewert</surname><given-names>S.D.</given-names></name>
<name><surname>Hohmann</surname><given-names>V.</given-names></name>
</person-group><article-title>Auditory Model Based Direction Estimation of Concurrent Speakers from Binaural Signals</article-title><source>Speech Commun.</source><year>2011</year><volume>53</volume><fpage>592</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1016/j.specom.2010.05.006</pub-id></element-citation></ref><ref id="B29-audiolres-15-00048"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Katz</surname><given-names>B.F.G.</given-names></name>
<name><surname>Noisternig</surname><given-names>M.</given-names></name>
</person-group><article-title>A Comparative Study of Interaural Time Delay Estimation Methods</article-title><source>J. Acoust. Soc. Am.</source><year>2014</year><volume>135</volume><fpage>352w</fpage><lpage>3540</lpage><pub-id pub-id-type="doi">10.1121/1.4875714</pub-id><pub-id pub-id-type="pmid">24437775</pub-id>
</element-citation></ref><ref id="B30-audiolres-15-00048"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Tollin</surname><given-names>D.J.</given-names></name>
<name><surname>Yin</surname><given-names>T.C.T.</given-names></name>
</person-group><article-title>Sound Localization: Neural Mechanisms</article-title><source>Encyclopedia of Neuroscience</source><publisher-name>Elsevier</publisher-name><publisher-loc>Amsterdam, The Netherlands</publisher-loc><year>2009</year><fpage>137</fpage><lpage>144</lpage><isbn>978-0-08-045046-9</isbn></element-citation></ref><ref id="B31-audiolres-15-00048"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Carpenter</surname><given-names>R.H.</given-names></name>
<name><surname>Reddi</surname><given-names>B.A.</given-names></name>
<name><surname>Anderson</surname><given-names>A.J.</given-names></name>
</person-group><article-title>A simple two-stage model predicts response time distributions</article-title><source>J. Physiol.</source><year>2009</year><volume>587</volume><issue-part>Pt 16</issue-part><fpage>4051</fpage><lpage>4062</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2009.173955</pub-id><pub-id pub-id-type="pmid">19564395</pub-id>
</element-citation></ref><ref id="B32-audiolres-15-00048"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Carpenter</surname><given-names>R.H.</given-names></name>
<name><surname>Williams</surname><given-names>M.L.L.</given-names></name>
</person-group><article-title>Neural computation of log likelihood in control of saccadic eye movements</article-title><source>Nature</source><year>1995</year><volume>377</volume><fpage>59</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/377059a0</pub-id><pub-id pub-id-type="pmid">7659161</pub-id>
</element-citation></ref><ref id="B33-audiolres-15-00048"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ausili</surname><given-names>S.A.</given-names></name>
<name><surname>Backus</surname><given-names>B.</given-names></name>
<name><surname>Agterberg</surname><given-names>M.J.H.</given-names></name>
<name><surname>van Opstal</surname><given-names>A.J.</given-names></name>
<name><surname>van Wanrooij</surname><given-names>M.M.</given-names></name>
</person-group><article-title>Sound Localization in Real-Time Vocoded Cochlear-Implant Simulations with Normal-Hearing Listeners</article-title><source>Trends Hear.</source><year>2019</year><volume>23</volume><fpage>2331216519847332</fpage><pub-id pub-id-type="doi">10.1177/2331216519847332</pub-id><pub-id pub-id-type="pmid">31088265</pub-id>
</element-citation></ref><ref id="B34-audiolres-15-00048"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ho</surname><given-names>J.</given-names></name>
<name><surname>Tumkaya</surname><given-names>T.</given-names></name>
<name><surname>Aryal</surname><given-names>S.</given-names></name>
<name><surname>Choi</surname><given-names>H.</given-names></name>
<name><surname>Claridge-Chang</surname><given-names>A.</given-names></name>
</person-group><article-title>Moving beyond P Values: Data Analysis with Estimation Graphics</article-title><source>Nat. Methods</source><year>2019</year><volume>16</volume><fpage>565</fpage><lpage>566</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0470-3</pub-id><pub-id pub-id-type="pmid">31217592</pub-id>
</element-citation></ref><ref id="B35-audiolres-15-00048"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Van Wanrooij</surname><given-names>M.M.</given-names></name>
<name><surname>Bell</surname><given-names>A.H.</given-names></name>
<name><surname>Munoz</surname><given-names>D.P.</given-names></name>
<name><surname>Van Opstal</surname><given-names>A.J.</given-names></name>
</person-group><article-title>The effect of spatial-temporal audiovisual disparities on saccades in a complex scene</article-title><source>Exp. Brain Res.</source><year>2009</year><volume>198</volume><fpage>425</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1007/s00221-009-1815-4</pub-id><pub-id pub-id-type="pmid">19415249</pub-id>
</element-citation></ref><ref id="B36-audiolres-15-00048"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ohlenforst</surname><given-names>B.</given-names></name>
<name><surname>Zekveld</surname><given-names>A.A.</given-names></name>
<name><surname>Jansma</surname><given-names>E.P.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Naylor</surname><given-names>G.</given-names></name>
<name><surname>Lorens</surname><given-names>A.</given-names></name>
<name><surname>Lunner</surname><given-names>T.</given-names></name>
<name><surname>Kramer</surname><given-names>S.E.</given-names></name>
</person-group><article-title>Effects of hearing impairment and hearing aid amplification on listening effort: A systematic review</article-title><source>Ear Hear.</source><year>2017</year><volume>38</volume><fpage>267</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000396</pub-id><pub-id pub-id-type="pmid">28234670</pub-id>
</element-citation></ref><ref id="B37-audiolres-15-00048"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pichora-Fuller</surname><given-names>M.K.</given-names></name>
<name><surname>Kramer</surname><given-names>S.E.</given-names></name>
<name><surname>Eckert</surname><given-names>M.A.</given-names></name>
<name><surname>Edwards</surname><given-names>B.</given-names></name>
<name><surname>Hornsby</surname><given-names>B.W.Y.</given-names></name>
<name><surname>Humes</surname><given-names>L.E.</given-names></name>
<name><surname>Lemke</surname><given-names>U.</given-names></name>
<name><surname>Lunner</surname><given-names>T.</given-names></name>
<name><surname>Matthen</surname><given-names>M.</given-names></name>
<name><surname>Mackersie</surname><given-names>C.L.</given-names></name>
<etal/>
</person-group><article-title>Hearing impairment and cognitive energy: The framework for understanding effortful listening (FUEL)</article-title><source>Ear Hear.</source><year>2016</year><volume>37</volume><fpage>5S</fpage><lpage>27S</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000312</pub-id><pub-id pub-id-type="pmid">27355771</pub-id>
</element-citation></ref><ref id="B38-audiolres-15-00048"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Best</surname><given-names>V.</given-names></name>
<name><surname>Kalluri</surname><given-names>S.</given-names></name>
<name><surname>McLachlan</surname><given-names>S.</given-names></name>
<name><surname>Valentine</surname><given-names>S.</given-names></name>
<name><surname>Edwards</surname><given-names>B.</given-names></name>
<name><surname>Carlile</surname><given-names>S.</given-names></name>
</person-group><article-title>A Comparison of CIC and BTE Hearing Aids for Three-Dimensional Localization of Speech</article-title><source>Int. J. Audiol.</source><year>2010</year><volume>49</volume><fpage>723</fpage><lpage>732</lpage><pub-id pub-id-type="doi">10.3109/14992027.2010.484827</pub-id><pub-id pub-id-type="pmid">20515424</pub-id>
</element-citation></ref><ref id="B39-audiolres-15-00048"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fletcher</surname><given-names>M.D.</given-names></name>
<name><surname>Zgheib</surname><given-names>J.</given-names></name>
</person-group><article-title>Haptic Sound-Localisation for Use in Cochlear Implant and Hearing-Aid Users</article-title><source>Sci. Rep.</source><year>2020</year><volume>10</volume><elocation-id>14171</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-70379-2</pub-id><pub-id pub-id-type="pmid">32843659</pub-id>
</element-citation></ref><ref id="B40-audiolres-15-00048"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>Y.</given-names></name>
<name><surname>Swanson</surname><given-names>J.</given-names></name>
<name><surname>Koehnke</surname><given-names>J.</given-names></name>
<name><surname>Guan</surname><given-names>J.</given-names></name>
</person-group><article-title>Sound Localization of Listeners With Normal Hearing, Impaired Hearing, Hearing Aids, Bone-Anchored Hearing Instruments, and Cochlear Implants: A Review</article-title><source>Am. J. Audiol.</source><year>2022</year><volume>31</volume><fpage>819</fpage><lpage>834</lpage><pub-id pub-id-type="doi">10.1044/2022_AJA-22-00006</pub-id><pub-id pub-id-type="pmid">35917460</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="audiolres-15-00048-f001"><label>Figure 1</label><caption><p>(<bold>A</bold>) Head trajectories for each trial (represented by line), localizing stimuli generated from the sound sources (represented by circles). (<bold>B</bold>) Example measurement of a subject&#x02019;s head orientation and velocity for one trial.</p></caption><graphic xlink:href="audiolres-15-00048-g001" position="float"/></fig><fig position="float" id="audiolres-15-00048-f002"><label>Figure 2</label><caption><p>Binaural cues compared between NH (blue) and AirPods (red) with ITDs (left) and ILDs (right) along the azimuth plane at 0&#x000b0; elevation. Positive ITDs mean sound reaching the right side first, and vice versa. Similarly, positive ILDs mean a louder signal on the right, and vice versa.</p></caption><graphic xlink:href="audiolres-15-00048-g002" position="float"/></fig><fig position="float" id="audiolres-15-00048-f003"><label>Figure 3</label><caption><p>(<bold>A</bold>) Monaural spectral pinna cues for NH and (<bold>B</bold>) AirPods for &#x02212;30&#x000b0; to 30&#x000b0; elevation and (<bold>C</bold>) their difference.</p></caption><graphic xlink:href="audiolres-15-00048-g003" position="float"/></fig><fig position="float" id="audiolres-15-00048-f004"><label>Figure 4</label><caption><p>Example of Subject 9 results for azimuth and elevation localization for different stimuli in each listening condition. Circles and squares represent the individual responses for azimuth and elevation, respectively. The colored line shows the linear regression (NH in blue, AirPods in red), and the dashed line represents a perfect unity line. Each plot displays the gain of the best-fit regression line (g), the bias (b), the correlation coefficient (r<sup>2</sup>), and the mean absolute error (MAE).</p></caption><graphic xlink:href="audiolres-15-00048-g004" position="float"/></fig><fig position="float" id="audiolres-15-00048-f005"><label>Figure 5</label><caption><p>Violin plots presenting each participant&#x02019;s performance (colored circles) for each stimulus, with the 25th and 75th percentiles indicated by the black bars and the median indicated by the white circle. Responses under NH conditions (blue) and AirPods (red). Left column: responses in azimuth for (<bold>A</bold>) Gain, (<bold>C</bold>) Bias, (<bold>E</bold>) r<sup>2</sup>, and (<bold>G</bold>) MAE; right column: responses in elevation for (<bold>B</bold>) Gain, (<bold>D</bold>) Bias, (<bold>F</bold>) r<sup>2</sup>, and (<bold>H</bold>) MAE. * indicates <italic toggle="yes">p</italic>-value &#x0003c; 0.05.</p></caption><graphic xlink:href="audiolres-15-00048-g005" position="float"/></fig><fig position="float" id="audiolres-15-00048-f006"><label>Figure 6</label><caption><p>Binaural cues for normal hearing (NH) and AirPods across all SNR levels. The top row shows interaural time differences (ITDs) for frequencies between 0.1 kHz and 1.5 kHz, while the bottom row displays interaural level differences (ILDs) for frequencies from 3 kHz to 20 kHz.</p></caption><graphic xlink:href="audiolres-15-00048-g006" position="float"/></fig><fig position="float" id="audiolres-15-00048-f007"><label>Figure 7</label><caption><p>Example results for Subject 9 showing azimuth and elevation localization across different SNR levels for each listening condition. Circles represent azimuth responses, while squares denote elevation responses. Colored lines indicate linear regression fits (blue for NH, red for AirPods), with the dashed line representing perfect unity.</p></caption><graphic xlink:href="audiolres-15-00048-g007" position="float"/></fig><fig position="float" id="audiolres-15-00048-f008"><label>Figure 8</label><caption><p>Violin plots of sound localization outcomes in noise for NH and AirPod listening conditions at each SNR level. Left column: responses in azimuth for (<bold>A</bold>) Gain, (<bold>C</bold>) Bias, (<bold>E</bold>) r2, and (<bold>G</bold>) MAE; right column: responses in elevation for (<bold>B</bold>) Gain, (<bold>D</bold>) Bias, (<bold>F</bold>) r2, and (<bold>H</bold>) MAE. * indicates <italic toggle="yes">p</italic>-value &#x0003c; 0.05.</p></caption><graphic xlink:href="audiolres-15-00048-g008" position="float"/></fig><fig position="float" id="audiolres-15-00048-f009"><label>Figure 9</label><caption><p>Violin plots showing response promptness for NH (red) and AirPods (blue) listening conditions during sound localization, both with and without background noise. * indicates <italic toggle="yes">p</italic>-value &#x0003c; 0.05.</p></caption><graphic xlink:href="audiolres-15-00048-g009" position="float"/></fig></floats-group></article>