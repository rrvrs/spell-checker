<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006501</article-id><article-id pub-id-type="pmc">PMC11860719</article-id><article-id pub-id-type="doi">10.3390/s25041272</article-id><article-id pub-id-type="publisher-id">sensors-25-01272</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Contrastive Learning with Global and Local Representation for Mixed-Type Wafer Defect Recognition</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yin</surname><given-names>Shantong</given-names></name></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Yangkun</given-names></name></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Rui</given-names></name><xref rid="c1-sensors-25-01272" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Lazaridis</surname><given-names>Pavlos</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01272">School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen 518055, China; <email>styin0425@gmail.com</email> (S.Y.); <email>zhangyangkun@hit.edu.cn</email> (Y.Z.)</aff><author-notes><corresp id="c1-sensors-25-01272"><label>*</label>Correspondence: <email>r.wang@hit.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>19</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1272</elocation-id><history><date date-type="received"><day>08</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>12</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>18</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Recognizing defect patterns in semiconductor wafer bin maps (WBMs) poses a critical challenge in the integrated circuit (IC) manufacturing industry. The accurate classification and segmentation of these defect patterns are of utmost significance as they are key to tracing the root causes of defects, thereby reducing costs and enhancing both product efficiency and quality. As the manufacturing process grows in complexity, the WBM becomes intricate when multiple defect patterns coexist on a single wafer, making the recognition task increasingly complicated. In addition, traditional supervised learning methods require a large number of labeled samples, which is labor-intensive. In this paper, we present a self-supervised contrastive learning framework for the classification and segmentation of mixed-type WBM defect patterns. Our model incorporates a global module for contrastive learning that captures image-level representations, alongside a local module that targets the comprehension of regional details, which is helpful for the segmentation of defective patterns. Experimental results demonstrate that our model performs effectively in scenarios where there is a limited number of labeled examples and a wealth of unlabeled ones.</p></abstract><kwd-group><kwd>pattern classification and segmentation</kwd><kwd>contrastive learning</kwd><kwd>wafer bin map</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>72101065</award-id></award-group><award-group><funding-source>Guangdong Basic and Applied Basic Research Foundation</funding-source><award-id>2021A1515110336</award-id><award-id>2020A1515110246</award-id></award-group><award-group><funding-source>Shenzhen Science and Technology Program</funding-source><award-id>RCBS20221008093124063</award-id></award-group><funding-statement>This work was supported in part by the National Natural Science Foundation of China under Grant 72101065; in part by the Guangdong Basic and Applied Basic Research Foundation under Grants 2021A1515110336, 2020A1515110246; and in part by the Shenzhen Science and Technology Program under Grant RCBS20221008093124063.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01272"><title>1. Introduction</title><p>The production of semiconductor devices is a complex and highly detailed operation, involving hundreds of steps to fabricate integrated circuit (IC) chips on silicon wafers. After wafer fabrication, the wafer needs to be inspected. Common inspection methods include a probing test, scanning electron microscopy (SEM), transmission electron microscopy (TEM), X-ray inspection, and automatic optical inspection (AOI), each generating information at different levels. For probing tests, the electrical performance of each die is tested using wafer probes. Dies that meet quality requirements are cut off from the wafer and utilized as final qualified chips. As the worldwide need for these devices grows, manufacturers are concentrating their efforts on boosting manufacturing efficiency by employing techniques such as maximizing the number of chips per wafer. Nevertheless, the enhanced complexity of these advanced techniques also gives rise to various defects, which are inevitable during the manufacturing processes. Therefore, the results of wafer test are stored in wafer bin maps (WBMs) for defect analysis, contributing to reducing defect rates and ensuring market competitiveness [<xref rid="B1-sensors-25-01272" ref-type="bibr">1</xref>].</p><p>WBM defect pattern recognition is crucial for detecting systemic manufacturing issues, as the accurate classification and segmentation results of WBM defect patterns can effectively infer the root causes of defects in the manufacturing process [<xref rid="B2-sensors-25-01272" ref-type="bibr">2</xref>]. The defects on the wafer can generally be divided into two categories: random errors and systematic faults. When dealing with wafer defect patterns, the analysis of systematic faults is generally the main focus, as the defect patterns caused by random errors are irregular and difficult to eliminate. As shown in <xref rid="sensors-25-01272-f001" ref-type="fig">Figure 1</xref>a, the typical single defect patterns contain the following: &#x0201c;Center&#x0201d;, &#x0201c;Donut&#x0201d;, &#x0201c;EdgeLoc&#x0201d;, &#x0201c;EdgeRing&#x0201d;, &#x0201c;Loc&#x0201d;, &#x0201c;NearFull&#x0201d;, &#x0201c;Scratch&#x0201d;, and &#x0201c;Random&#x0201d; [<xref rid="B3-sensors-25-01272" ref-type="bibr">3</xref>]. With the ongoing advancement in the IC industry, the complexity of manufacturing raises the likelihood of mixed-type WBM defect patterns appearing on the same wafer [<xref rid="B4-sensors-25-01272" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01272" ref-type="bibr">5</xref>], as shown in <xref rid="sensors-25-01272-f001" ref-type="fig">Figure 1</xref>b. These defect patterns may interact with each other, resulting in more intricate issues. Consequently, accurate classification and segmentation of these mixed-type WBM defect patterns are essential for effective quality control.</p><p>Given the intricate nature of mixed-type WBM defect patterns, manual inspection becomes increasingly labor-intensive and time-consuming [<xref rid="B6-sensors-25-01272" ref-type="bibr">6</xref>]. Alternative solutions are required in industrial production. Recent studies have explored the application of artificial intelligence and computer vision models for defect pattern analysis [<xref rid="B2-sensors-25-01272" ref-type="bibr">2</xref>]. However, existing research mainly focuses on image classification and cannot accurately identify the size and location of mixed-type defect patterns. In addition, training a model to recognize defect patterns often requires a large number of labeled samples and undoubtedly wastes manpower and resources. Self-supervised learning can learn from unlabeled data, which is particularly attractive in situations where it is difficult to obtain large amounts of labeled data [<xref rid="B7-sensors-25-01272" ref-type="bibr">7</xref>]. Although self-supervised learning is currently mainly used for image classification tasks, our work is based on the intuition that the WBM defect pattern segmentation can also benefit from representations learned through self-supervised learning from unlabeled data.</p><p>This study centers on contrastive learning, a branch of self-supervised learning [<xref rid="B8-sensors-25-01272" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01272" ref-type="bibr">9</xref>]. According to our research, most existing contrastive learning approaches prioritize global representations, which are not optimal for segmentation tasks that require pixel-level distinctions [<xref rid="B10-sensors-25-01272" ref-type="bibr">10</xref>]. If local regions of images are extracted for contrastive learning, then local representations can be adequately learned [<xref rid="B11-sensors-25-01272" ref-type="bibr">11</xref>] for segmentation tasks. Consequently, we propose a self-supervised contrastive learning model that integrates both global and local contrastive learning modules, with queue mechanisms introduced to decouple the sample size from mini-batch constraints. Specifically, the global contrastive learning module focuses on the learning of global representations, while the local contrastive learning module is used to learn pixel-level discrimination. With the introduction of queues, each module receives a sufficient number of negative samples, improving the contrastive loss computation. To the best of our knowledge, this is the first application of contrastive learning to both the classification and segmentation of WBM defect patterns. The primary contributions of this paper are as follows:<list list-type="simple"><list-item><label>(1)</label><p>We employ self-supervised contrastive learning for both the classification and segmentation of mixed-type defect patterns. In this way, we can obtain more precise information about the types and locations of different defect patterns.</p></list-item><list-item><label>(2)</label><p>Our proposed model integrates global and local contrastive learning modules, enabling the learning of both image-level and pixel-level features directly from unlabeled data. This approach provides a robust foundation for downstream classification and segmentation tasks using only a limited amount of labeled data.</p></list-item><list-item><label>(3)</label><p>Experimental evaluations on the mixed-type WBM defect pattern dataset demonstrate that our framework outperforms existing self-supervised methods.</p></list-item></list></p><p>The remainder of this paper is organized as follows: <xref rid="sec2-sensors-25-01272" ref-type="sec">Section 2</xref> presents a brief overview of recent studies on mixed-type WBM defect pattern classification and segmentation. <xref rid="sec3-sensors-25-01272" ref-type="sec">Section 3</xref> details the proposed methodology. <xref rid="sec4-sensors-25-01272" ref-type="sec">Section 4</xref> discusses the experimental results and analysis. Finally, <xref rid="sec5-sensors-25-01272" ref-type="sec">Section 5</xref> contains concluding remarks and directions for future research.</p></sec><sec id="sec2-sensors-25-01272"><title>2. Related Works</title><sec id="sec2dot1-sensors-25-01272"><title>2.1. WBM Defect Pattern Recognition</title><p>WBM defect pattern analysis is crucial for identifying the root cause of process failures in semiconductor manufacturing. The current research mainly focuses on the classification of single defect patterns and can be roughly divided into three categories. The first category mainly monitors defect patterns on WBMs by constructing statistical information [<xref rid="B12-sensors-25-01272" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01272" ref-type="bibr">13</xref>], which can successfully separate normal and abnormal graphic patterns, but it is difficult to distinguish different defect patterns. The second category is model-based clustering methods. For example, Wang et al. [<xref rid="B14-sensors-25-01272" ref-type="bibr">14</xref>] used Gaussian EM to detect elliptical and linear patterns and used the shell algorithm to estimate annular patterns. Yuan and Kuo [<xref rid="B15-sensors-25-01272" ref-type="bibr">15</xref>] proposed a mixed model describing the distribution of defects and used Bayesian inference to estimate parameters. The last category is machine learning methods that have received more and more attention in recent years. Wang and Chen [<xref rid="B16-sensors-25-01272" ref-type="bibr">16</xref>] trained a multilayer perceptron (MLP) on a set of hand-crafted spatial filters that reflect the rotational invariance property of WBM. Kulkarni [<xref rid="B3-sensors-25-01272" ref-type="bibr">3</xref>] applied CNNs to classify defect patterns based on a synthesized WBM dataset. Kang et al. [<xref rid="B17-sensors-25-01272" ref-type="bibr">17</xref>] presented a semi-supervised representation learning method, which fully utilizes the information in both unlabeled and labeled wafer maps. Most of the above research relies on large WBM databases and has problems with unsatisfactory performance and high computational costs.</p><p>The composition of mixed-type WBM defect patterns is more complex, as the combination of defect patterns can vary greatly. Thus, research on the classification of mixed-type WBM defect patterns is less extensive than that on single WBM defect patterns. In these studies, Byun and Baek [<xref rid="B18-sensors-25-01272" ref-type="bibr">18</xref>] proposed a new approach using a convolutional autoencoder to initialize the weights of a CNN. Tello et al. [<xref rid="B19-sensors-25-01272" ref-type="bibr">19</xref>] used feature rules to determine whether a wafer map has mixed-type patterns and then used a stochastic general regression neural network (GRNN) and a deep structured CNN for classification. Kyeong and Kim [<xref rid="B20-sensors-25-01272" ref-type="bibr">20</xref>] applied a CNN to classify WBM defects with mixed-type defect patterns. Wang et al. [<xref rid="B5-sensors-25-01272" ref-type="bibr">5</xref>] proposed DC-Net, which used a deformable convolutional network (DC-Net) to classify mixed-type defect patterns. These methods are unable to recognize and locate mixed-type defect patterns on WBMs simultaneously, and the research on the separation of multiple defect patterns is still insufficient.</p><p>Compared with classification, image segmentation can provide more detailed information and better understand the content of the image. For mixed-type WBM defect pattern segmentation, Nag et al. [<xref rid="B21-sensors-25-01272" ref-type="bibr">21</xref>] recently proposed a novel network based on an encoder&#x02013;decoder architecture for the simultaneous classification and segmentation of single and mixed defect patterns. Chiu et al. [<xref rid="B2-sensors-25-01272" ref-type="bibr">2</xref>] proposed a method based on Mask R-CNN [<xref rid="B22-sensors-25-01272" ref-type="bibr">22</xref>] to identify WBM defect patterns. Yan et al. [<xref rid="B23-sensors-25-01272" ref-type="bibr">23</xref>] presented a new U-Net framework that used semantic segmentation methods to segment different defect patterns on WBMs. However, existing research tends to focus more on global image representation while overlooking the powerful auxiliary role that local information plays in image segmentation tasks.</p></sec><sec id="sec2dot2-sensors-25-01272"><title>2.2. Self-Supervised Contrastive Learning</title><p>Self-supervised learning mainly uses auxiliary tasks to mine its own supervision information from large-scale unsupervised data and trains the network through this constructed supervision information [<xref rid="B7-sensors-25-01272" ref-type="bibr">7</xref>,<xref rid="B24-sensors-25-01272" ref-type="bibr">24</xref>]. As shown in <xref rid="sensors-25-01272-f002" ref-type="fig">Figure 2</xref>, the core idea of the self-supervised learning paradigm is to first learn knowledge from unlabeled samples by designing self-supervised signals and then transfer it to downstream tasks. As a branch of self-supervised learning, contrastive learning [<xref rid="B25-sensors-25-01272" ref-type="bibr">25</xref>] focuses on learning the common features between instances of the same class and distinguishing the differences between instances of different classes. Contrastive learning can effectively utilize unlabeled data for training [<xref rid="B9-sensors-25-01272" ref-type="bibr">9</xref>,<xref rid="B26-sensors-25-01272" ref-type="bibr">26</xref>] and has been widely applied in the field of natural image processing. In the field of WBM defect pattern classification, Kwak et al. [<xref rid="B27-sensors-25-01272" ref-type="bibr">27</xref>] applied a method (SWaCo) for safe WBM classification with self-supervised contrastive learning to effectively exploit unlabeled data. Kahng et al. [<xref rid="B28-sensors-25-01272" ref-type="bibr">28</xref>] presented a self-supervised learning framework that fully utilizes unlabeled data to pre-learn rich visual representations for data-efficient WBM defect pattern classification. Hu et al. [<xref rid="B29-sensors-25-01272" ref-type="bibr">29</xref>] proposed a contrastive learning framework for semi-supervised learning and prediction of WBM defect patterns. It can be seen that self-supervised contrastive learning methods do not heavily rely on the number of labeled samples and are mostly used for the classification of WBM defect patterns, but there is still little research on segmentation tasks. The urgent challenge lies in establishing a self-supervised contrastive learning model that possesses both classification and segmentation capabilities for mixed-type WBM defect patterns.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-01272"><title>3. Methodology</title><sec id="sec3dot1-sensors-25-01272"><title>3.1. Overview</title><p>In this paper, we propose a classification and segmentation model for mixed-type WBM defect patterns based on self-supervised contrastive learning. In our problem setting, we consider a set of WBM data including <italic toggle="yes">N</italic> labeled data and <italic toggle="yes">M</italic> unlabeled data, where <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x0226a;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. For simplicity, the unlabeled set is defined as <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mi>u</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, and the labeled set is defined as <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. As shown in <xref rid="sensors-25-01272-f003" ref-type="fig">Figure 3</xref>, the framework consists of a self-supervised pre-training stage and a supervised fine-tuning stage. In pre-training, global contrastive learning module is designed to learn image-level representation, while the local contrastive learning module is used to better understand the structure of local regions, which contributes to the image segmentation task. To distinguish extracted features with different connotations, we apply the concept of a query and a key to describe encoders and decoders, although they have the same structure respectively. A query can be understood as the target to be searched for, and a key is the template. The model is built by matching the query to a dictionary of keys, which is a queue of the encoded representations of data samples that is updated with each mini-batch. In fine-tuning, pre-trained weights are transferred to the downstream classification and segmentation tasks and fine-tuned with a limited number of labeled samples. This section describes the proposed framework in detail.</p></sec><sec id="sec3dot2-sensors-25-01272"><title>3.2. Data&#x000a0;Augmentation</title><p>In contrastive learning, the model is trained by enforcing similarity between positive sample pairs and dissimilarity between negative sample pairs. Therefore, the core of contrastive learning is the construction of positive and negative samples. In practice, different augmented views of the same sample after data augmentation are seen as positive samples, while other samples are defined as negative samples. Supported by data augmentation, the model can learn better feature representations of the data, capture the fundamental structure and relationships between samples, and effectively prevent over-fitting. As shown in <xref rid="sensors-25-01272-f004" ref-type="fig">Figure 4</xref>, we perform data augmentation operators, i.e., random rotation, flipping, and random noise on samples for learning the rotational invariance of WBMs and improving the robustness and generalization ability of the model. Regarding the noise addition operations in data augmentation, given that noise in semiconductor manufacturing environments (such as dust, light variations, etc.) can affect the performance of detection systems, we introduce different types of artificial noise (Gaussian Noise, Salt-and-Pepper Noise, and Poisson Noise) to simulate real-world production noise conditions. Specifically, given unlabeled sample <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mi>u</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, two augmented WBMs <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are generated by data augmentation <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, i.e., <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. These two augmented views are then used for the illustration of subsequent feature extraction.</p></sec><sec id="sec3dot3-sensors-25-01272"><title>3.3. Global Contrastive Learning&#x000a0;Module</title><p>The global contrastive learning module mainly promotes the model to learn the ability of extracting global features. Since global features provide the context information of the image, the designing of this module contributes to the effective comprehension of the overall structure of mixed-type WBMs. Specifically, key and query encoders are used to extract high-dimensional key and query global features. Then, these high-dimensional features are mapped to lower-dimensional hidden spaces for a more compact representation through the global projection head. After that, the global queue that is a unique form of a linear list is employed to dynamically manage key features. The construction of the global contrastive learning module is similar to MoCo v2 [<xref rid="B8-sensors-25-01272" ref-type="bibr">8</xref>]; we provide a detailed description as follows.</p><sec id="sec3dot3dot1-sensors-25-01272"><title>3.3.1. Global Feature&#x000a0;Extraction</title><p>Global features refer to the overall attributes of an image, capable of capturing the global information and understanding the image from a holistic perspective. We apply the concept of a query and a key to describe encoders for distinguishing extracted features with different connotations. As given a series of key features and a query feature, contrastive learning is to find the unique key feature that matches the query feature the most and enforce them to be similar while distancing from others.</p><p>Firstly, a key encoder <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and a query encoder <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are used to obtain global key feature <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and global query feature <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> from the augmented sample instances, respectively. The procedure can be formulated as<disp-formula id="FD1-sensors-25-01272"><label>(1)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>e</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the average value of each channel in the feature map, i.e., the global average pooling. <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the key encoder and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the query encoder, which have the same structure as the downstream encoder <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>In addition, the global projection head <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is an MLP with one hidden layer (with ReLU) in this module, whose presence has been proven to be beneficial to push the dissimilar samples away from each other and obtain more useful information for downstream tasks [<xref rid="B9-sensors-25-01272" ref-type="bibr">9</xref>]. The global projection head can map the high-dimensional features <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> to lower-dimensional hidden spaces for a more compact representation, encouraging our model to learn more abstract and informative features. Therefore, we obtain the final global key feature <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and global query feature <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> by passing <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> through the global projection head <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to further calculate the global contrastive loss. The conversion can be formulated as follows:<disp-formula id="FD2-sensors-25-01272"><label>(2)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>R</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-sensors-25-01272"><label>(3)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>R</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are linear transformation matrices, and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a ReLU nonlinearity.</p><p>Furthermore, the global queue that is a unique form of a linear list for dynamically managing vectors is introduced, as shown in <xref rid="sensors-25-01272-f003" ref-type="fig">Figure 3</xref>. Since the core of contrastive learning is to construct sufficient negative sample pairs, we design the global queue to store abundant negative vectors (i.e., key features) generated by the key encoder. The size of key features in the global queue can far exceed the capacity of a single batch, which can increase the diversity of training and improve the generalization ability of the model. Therefore, the introduction of the queue can decouple the negative vector size from the mini-batch size. In the dynamic management of the global queue, the current mini-batch of encoded key features is enqueued into the queue and the oldest key features are dequeued from the queue. This process ensures that the model is always learning from the latest data by a dynamic and efficient management of key features.</p></sec><sec id="sec3dot3dot2-sensors-25-01272"><title>3.3.2. Global Contrastive&#x000a0;Loss</title><p>Global contrastive loss is a measure of the similarities between global features in a representation space and promotes the model to learn global representations. In contrastive learning, if a query feature (generated by query encoder) is given, only a unique key feature in the global queue can match it. So, the idea of contrastive learning is to compare the query feature with all the key features in the queue and hope that the query feature is as similar as possible to its matching key feature, and then it stays away from others. Consider an encoded query feature <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and a set of encoded samples <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that contains <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> key features in the global queue. Obviously, there is a single key feature (denoted as <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>) that matches <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. Then, the contrastive loss function, called InfoNCE [<xref rid="B30-sensors-25-01272" ref-type="bibr">30</xref>], is used to expect matching pairs to be similar and mismatching pairs to be dissimilar, as follows:<disp-formula id="FD4-sensors-25-01272"><label>(4)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:munderover><mml:mo>&#x02212;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a temperature hyper-parameter, and <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the number of samples from a mini-batch. The sum in the denominator is calculated over one positive key feature <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and other <italic toggle="yes">K</italic> negative key features. Intuitively, the InfoNCE loss tries to classify the query feature <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> as its corresponding positive key feature <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> rather than others.</p></sec></sec><sec id="sec3dot4-sensors-25-01272"><title>3.4. Local Contrastive Learning&#x000a0;Module</title><p>The local contrastive learning module is a local contrastive representation space for computing the contrastive loss associated with the local regions of a WBM. For a WBM with mixed-type defect patterns, the segmentation task is more complex due to the interconnected nature of defects and their variability in quantity, position, and type, etc. While global features can effectively capture the overall information of an image, relying solely on global features to measure WBM defect patterns will result in the neglect of crucial local details. This approach may not be optimal for segmentation tasks that require pixel-level discrimination. To address this issue, we design the local contrastive learning module to learn the representation of local regions. We separate the WBM into multiple regions, selectively identify relevant regions, and subsequently perform position matching and feature extraction for local contrastive learning. This method ensures that the segmentation process benefits from both the global context and the fine-grained local information, enhancing the overall accuracy and effectiveness of defect recognition. The details are as follows:</p><sec id="sec3dot4dot1-sensors-25-01272"><title>3.4.1. Local Region Selection and&#x000a0;Matching</title><p>Randomly selected local regions will be biased towards the more dominant feature categories, as the &#x0201c;background&#x0201d; category accounts for a large proportion in mixed-type WBM defect patterns. Therefore, we need to make the selection of local regions reasonable to contain more valuable feature categories. For example, as shown in <xref rid="sensors-25-01272-f005" ref-type="fig">Figure 5</xref>, flipping and rotating operations are performed on <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and two augmented views <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are obtained respectively. The selected local regions are then matched according to their positions.</p><p>The local region selection and matching is performed by the selection of appropriate regions in <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> at first, and then by the determination of the local regions in <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> that match with the selected regions. The appropriateness of each local region to be selected is evaluated by its weight, which is calculated by a convolutional layer with the kernel size being the same as the local region size <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Specifically, <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is input into the convolutional layer, and the weight of each local region is calculated and recorded by sliding the kernel. We select the local region with the maximum weight from <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, and then the position of the matching local region in <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is determined. To minimize overlap, after each local region selection, the region is excluded so that the centers of subsequently selected regions do not fall into the previously selected local regions. This weighted selection and matching process is repeated <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> times, selecting distinct local regions in each iteration to obtain <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> pairs of matched local regions from images <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec3dot4dot2-sensors-25-01272"><title>3.4.2. Local Feature&#x000a0;Extraction</title><p>Local features <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> that capture the pixel-level discrimination of WBM defect patterns are extracted by the following steps. Firstly, feature maps <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> of a positive pair (<inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>) are obtained from a key and query encoder&#x02013;decoder network. Here, <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are key and query encoders while <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are key and query decoders, which have the same structure as the downstream encoder&#x02013;decoder network, respectively. Then, the <italic toggle="yes">j</italic>-th <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> pair of matching local region feature maps <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is extracted according to the idea of local region selection and matching. After that, average pooling <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is performed on local region feature maps <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, and the results then pass through the local projection head <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Therefore, the final local features can be defined as follows:<disp-formula id="FD5-sensors-25-01272"><label>(5)</label><mml:math id="mm74" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>&#x003bc;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mfenced></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mi>&#x003bc;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-01272"><label>(6)</label><mml:math id="mm75" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>&#x003bc;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mfenced></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mi>&#x003bc;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where feature maps <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> have the same size as the original image. The local projection head <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> can contribute to making the model better capture the differences between local regions. <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math></inline-formula> represents calculating the average value of each channel in feature maps, <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are linear transformation matrices, and <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a ReLU nonlinearity. Similar to the global contrastive learning, the queue is also applied to dynamically store and manage local region vectors in the training of the model, as shown in <xref rid="sensors-25-01272-f003" ref-type="fig">Figure 3</xref>.</p></sec><sec id="sec3dot4dot3-sensors-25-01272"><title>3.4.3. Local Contrastive&#x000a0;Loss</title><p>The local contrastive loss updates the network by forcing the feature representations of matching local regions to be similar and the feature representations of different local regions to be dissimilar. Specifically, considering a local query feature <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and a set of vectors <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mo>&#x00393;</mml:mo></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that contain <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00393;</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> key features in the local queue, there is a single key feature (denoted as <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>) that matches <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. Then, the local contrastive loss can also be calculated by InfoNCE as follows:<disp-formula id="FD7-sensors-25-01272"><label>(7)</label><mml:math id="mm88" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:munderover><mml:mo>&#x02212;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>&#x00393;</mml:mo></mml:msubsup><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi>II</mml:mi></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a temperature hyper-parameter, <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> indicates the number of pairs of matched local regions in a mini-batch, and <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mo>&#x00393;</mml:mo></mml:mrow></mml:math></inline-formula> represents the number of key features in the local queue.</p></sec></sec><sec id="sec3dot5-sensors-25-01272"><title>3.5. Network&#x000a0;Updating</title><sec id="sec3dot5dot1-sensors-25-01272"><title>3.5.1. Self-Supervised&#x000a0;Pre-Training</title><p>In the pre-training period, the total contrastive loss function to update the query encoder parameters <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and query decoder parameters <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is defined as follows:<disp-formula id="FD8-sensors-25-01272"><label>(8)</label><mml:math id="mm94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the global contrastive loss in Equation (<xref rid="FD4-sensors-25-01272" ref-type="disp-formula">4</xref>), and <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the local contrastive loss in Equation (<xref rid="FD7-sensors-25-01272" ref-type="disp-formula">7</xref>).</p><p>Although using queues allows for the size of negative vectors to be larger, it also poses difficulties for updating the key encoder and key decoder via back-propagation, since the gradient should propagate to all samples in the queue. To address this issue, a momentum update approach is utilized for the key encoder and decoder, which accumulates exponential moving averages of the key network to ensure a stable and efficient update manner. To be specific, the query encoder parameters <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and the decoder parameters <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> are updated via back-propagation by Equation (<xref rid="FD8-sensors-25-01272" ref-type="disp-formula">8</xref>), while the key encoder parameters <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and decoder parameters <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> are updated by the momentum equations:<disp-formula id="FD9-sensors-25-01272"><label>(9)</label><mml:math id="mm101" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mo>&#x02190;</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
and<disp-formula id="FD10-sensors-25-01272"><label>(10)</label><mml:math id="mm102" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mo>&#x02190;</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the momentum coefficients. The momentum updating makes parameters <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> evolve more smoothly than <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>. Therefore, although the keys in the queue are generated by different encoders and decoders, the differences between these encoders and decoders can be ignored.</p></sec><sec id="sec3dot5dot2-sensors-25-01272"><title>3.5.2. Supervised&#x000a0;Fine-Tuning</title><p>Supervised fine-tuning refers to training the model on a smaller and task-specific labeled dataset. The purpose of this stage is to make the model adapt to the specific requirements of the given task. In detail, the model retains the knowledge learned in the pre-training stage while also learning additional information relevant to the specific task. In our problem, given labeled data <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, we fine-tune the whole network through a segmentation loss <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and classification loss <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>For the computation of the segmentation loss <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we employ the cross-entropy loss function [<xref rid="B31-sensors-25-01272" ref-type="bibr">31</xref>], which computes the pixel-level difference between segmented defect patterns and the ground truth. However, merely calculating the loss value of pixels may result in over-segmentation. It is still necessary to compute the loss value associated with the type of defect. We use a <italic toggle="yes">C</italic>-bit one-hot value to represent the predicted types of WBM defect patterns and calculate the corresponding binary cross entropy loss <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Since we need to simultaneously focus on the classification and segmentation of WBM defect patterns, the total loss function in the fine-tuning period is defined as follows:<disp-formula id="FD11-sensors-25-01272"><label>(11)</label><mml:math id="mm115" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the segmentation loss and <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the classification loss.</p></sec></sec></sec><sec id="sec4-sensors-25-01272"><title>4. Experimental&#x000a0;Results</title><sec id="sec4dot1-sensors-25-01272"><title>4.1. Data&#x000a0;Description</title><p>A wafer bin map (WBM) is the result of the circuit probing process, recording the spatial distribution of defective chips on the wafer. The WBMs dataset collection system is shown in <xref rid="sensors-25-01272-f006" ref-type="fig">Figure 6</xref>. The primary method of wafer testing involves the coordination between the tester and the probe stage [<xref rid="B32-sensors-25-01272" ref-type="bibr">32</xref>]. During the testing process, the tester cannot directly measure the wafer under test. Instead, it relies on the probes in the probe card to make electrical contact with the pads or bumps on the wafer. The test signals measured by the probes are then sent to the automatic test equipment (ATE) for analysis and judgment, thereby obtaining the electrical characteristics test results for each die on the wafer. All samples are collected in actual industrial scenarios by the wafer probe testing station. We standardized the image size to <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>52</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>52</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>As shown in <xref rid="sensors-25-01272-t001" ref-type="table">Table 1</xref>, the defect types contain 8 kinds of single defect patterns and 29 kinds of mixed-type defect patterns (13 kinds of double mixed-type defect patterns, 12 kinds of triple mixed-type defect patterns, and 4 kinds of quadruple mixed-type defect patterns). We represent each type of defect using the uppercase initial letter of its name as shown in <xref rid="sensors-25-01272-f001" ref-type="fig">Figure 1</xref>. It is worth noting that Random and NearFull defects do not significantly point to specific manufacturing roots, and their impact in practice is minimal. Also, these two types of patterns are not suitable for generating mixed defects with other single defect patterns, so we do not consider the synthesis of these two types. Furthermore, we consider the WBMs of the normal category (C0) into the model training process, and there are 37 kinds of defect patterns and 1 kind of normal category in total. In the final dataset, we collected 19,000 WBMs with 500 instances for each defect pattern. In our problem setting, the original WBMs have only pixel values 0, 1, and 2, so <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mfenced separators="" open="{" close="}"><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mfenced><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> mentioned earlier is the WBM; <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close="}"><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mi>C</mml:mi></mml:msup></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the corresponding ground truth category label, where <italic toggle="yes">C</italic> denotes the number of pattern types; <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the pixel label map of the <italic toggle="yes">i</italic>-th sample; and <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the <italic toggle="yes">C</italic>-bit one-hot label.</p></sec><sec id="sec4dot2-sensors-25-01272"><title>4.2. Model&#x000a0;Evaluation</title><p>In this paper, we focus on both the segmentation and classification performance of mixed-type WBM defect patterns. Specifically, we use <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (pixel predicted accuracy) and <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (Intersection over Union) to evaluate the results of semantic segmentation, which can be defined for a specific class <italic toggle="yes">c</italic> as follows:<disp-formula id="FD12-sensors-25-01272"><label>(12)</label><mml:math id="mm125" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>&#x02229;</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD13-sensors-25-01272"><label>(13)</label><mml:math id="mm126" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>&#x02229;</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>&#x0222a;</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> denote the actual and predicted pixel label maps of the <italic toggle="yes">c</italic>-th pattern in the <italic toggle="yes">i</italic>-th WBM. For mixed-type patterns, the mean <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) and the mean <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) are used to evaluate the model segmentation performance.</p><p>For the evaluation of classification results, we use <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to measure the classification accuracy of mixed-type WBM defect patterns. To evaluate the performance in the false classification of WBMs, <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are employed in our experiments. The definitions of the indices are shown as follows:<disp-formula id="FD14-sensors-25-01272"><label>(14)</label><mml:math id="mm136" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD15-sensors-25-01272"><label>(15)</label><mml:math id="mm137" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD16-sensors-25-01272"><label>(16)</label><mml:math id="mm138" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the true positives, <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the true negatives, <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the false positives, and <inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents false negatives.</p></sec><sec id="sec4dot3-sensors-25-01272"><title>4.3. Performance and Analysis</title><sec id="sec4dot3dot1-sensors-25-01272"><title>4.3.1. Implementation Details</title><p>In this work, the encoders and decoders designed in our model have the same structure as those in DeepLab v3+ [<xref rid="B33-sensors-25-01272" ref-type="bibr">33</xref>]. During the pre-training period, we used an abundant amount of unlabeled WBMs to pre-train the encoder and decoder for 350 epochs with a batch size of 64. We optimized the parameters with Adam optimizer, and the initial learning rate was set as 0.01 with the cosine decay schedule. In addition, we selected 6 local regions of size <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>12</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> from each sample in the local region selection and matching process (i.e., <inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). The hyper-parameter <inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> in Equation (<xref rid="FD8-sensors-25-01272" ref-type="disp-formula">8</xref>) was set to 0.5 to balance the global contrast loss and local contrast loss. Following [<xref rid="B8-sensors-25-01272" ref-type="bibr">8</xref>], the momentum coefficients <inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> were all set to 0.999, and the size of the queues was set to 2048. After pre-training, the model with the lowest loss was saved for the downstream segmentation task.</p><p>In the fine-tuning period, we trained our model using a small amount of labeled WBMs for a total of 150 epochs, with a batch size of 32. The initial learning rate was set to 0.001, which decreased to 98% per epoch. The hyper-parameter <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> in Equation (<xref rid="FD11-sensors-25-01272" ref-type="disp-formula">11</xref>) is used to balance pixel loss and one-hot prediction loss, so we also set it to a default 0.5. In addition, the experiment was implemented on an equipment with an Intel(R) Xeon(R) Gold 6226R @ 2.90 GHz CPU and an NVIDIA RTX A6000 GPU by Python 3.8.3.</p></sec><sec id="sec4dot3dot2-sensors-25-01272"><title>4.3.2. Model Performance</title><p>In our experimental setting, we randomly split the dataset into three subsets: 80% for pre-training, 10% for fine-tuning, and 10% for testing. We obtained the <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for 38 kinds of defect patterns to demonstrate the performance of our model. <xref rid="sensors-25-01272-t002" ref-type="table">Table 2</xref> primarily displays the segmentation and classification results for the normal pattern and the single-type defect patterns, <xref rid="sensors-25-01272-t003" ref-type="table">Table 3</xref> shows the segmentation results for the mixed-type defect patterns, and <xref rid="sensors-25-01272-t004" ref-type="table">Table 4</xref> presents the classification results for the mixed-type defect patterns. As shown in <xref rid="sensors-25-01272-t002" ref-type="table">Table 2</xref>, the horizontal axis represents different categories (C0 to C8), while the vertical axis displays various evaluation metrics for segmentation and classification. The average <inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm156" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm157" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm158" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm159" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of our proposed model for C0&#x02013;C8 are 95.78%, 93.87%, 96.80%, 94.61%, and 94.89%. In <xref rid="sensors-25-01272-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-01272-t004" ref-type="table">Table 4</xref>, to facilitate the presentation of the experimental results, the horizontal axis represents various evaluation metrics for segmentation and classification, respectively, while the vertical axis corresponds to different categories (C9 to C37). The average <inline-formula><mml:math id="mm160" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm161" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm162" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm163" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm164" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for C9&#x02013;C37 are 95.40%, 90.51%, 92.34%, 94.45%, and 94.66%. From an overall perspective, the more the defect classes that appear on the same WBM, the more difficult it is to recognize and segment. However, whether it is a mixture of two defects, three defects, or four defects, our model accurately accomplished the recognition and segmentation of WBMs in the testing set.</p><p><xref rid="sensors-25-01272-f007" ref-type="fig">Figure 7</xref> presents several mixed-type WBM segmentation results achieved with our proposed model. We randomly selected eight experimental samples, each displaying the original mixed-type WBM, label map, and corresponding segmentation output. We can see that the segmentation maps obtained using our model are smooth, complete, and robust to noise. Even in the cases where several mixed defect patterns are overlapped, our model can still distinguish them based on the different characteristics of each defect pattern. For all these defect patterns, the <italic toggle="yes">C</italic> and <inline-formula><mml:math id="mm165" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> classes are best classified and segmented. This is because the location and shape of the <italic toggle="yes">C</italic> and <inline-formula><mml:math id="mm166" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> defect patterns are relatively fixed and easier to distinguish. Even for the <italic toggle="yes">S</italic> class, which may easily interact with other pattern classes, our model still has an excellent segmentation performance. Therefore, our model has great potential and application value in the field of WBMs classification and segmentation.</p></sec><sec id="sec4dot3dot3-sensors-25-01272"><title>4.3.3. Comparison with Other Methods</title><p>In this section, we compare the performance of our proposed model with other self-supervised learning models, including Jigsaw [<xref rid="B34-sensors-25-01272" ref-type="bibr">34</xref>], SimCLR [<xref rid="B9-sensors-25-01272" ref-type="bibr">9</xref>], and MoCo v2 [<xref rid="B8-sensors-25-01272" ref-type="bibr">8</xref>]. These are all relatively successful contrastive learning models. To ensure fairness in comparison, all models used in the experiments have the same dataset and division ratio as in the previous section.</p><p>As shown in <xref rid="sensors-25-01272-t005" ref-type="table">Table 5</xref>, the horizontal axis represents the overall evaluation metrics for segmentation and classification, including <inline-formula><mml:math id="mm167" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm168" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (used for segmentation), and <inline-formula><mml:math id="mm169" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm170" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm171" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (used for classification), while the vertical axis displays the numerical results for each method across these metrics. The overall <inline-formula><mml:math id="mm172" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm173" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm174" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm175" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm176" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> values of our model are as high as 95.59%, 92.19%, 93.57%, 94.43%, and 94.78%, respectively. These indicators are far superior to the other three self-supervised contrastive learning methods. SimCLR performs the worst, with the overall <inline-formula><mml:math id="mm177" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm178" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> values only being 75.85% and 80.41%. This is related to the fact that SimCLR relies more on large batch data, and the model cannot learn more useful knowledge from limited data. Although self-supervised contrastive learning is widely used in the field of image classification, research on segmentation tasks is still somewhat lacking. The other three common contrastive learning methods used for comparison only focus on global representations; thus, they ignore the local information, which is important for segmentation tasks. So, even after the fine-tuning period, their performance is still not excellent. However, our approach is designed to focus on both the global and local information of the image. Therefore, the proposed model can perform well whether the WBM defect patterns are easy or difficult to distinguish.</p></sec></sec><sec sec-type="discussion" id="sec4dot4-sensors-25-01272"><title>4.4. Discussion</title><sec id="sec4dot4dot1-sensors-25-01272"><title>4.4.1. Effectiveness of the Amount of Self-Supervised Unlabeled Data</title><p>Self-supervised pre-training needs to use a large amount of unlabeled data, and an abundance of unlabeled WBMs is readily accessible in the industrial manufacturing process. Therefore, this section investigates the potential of more self-supervised pre-training unlabeled data improving the performance of the model. Building upon our WBM dataset, we randomly utilized 1%, 25%, 50%, 75%, and 100% pre-training unlabeled WBMs. As presented in <xref rid="sensors-25-01272-t006" ref-type="table">Table 6</xref>, the horizontal axis represents different amounts of pre-training unlabeled data, while the vertical axis displays various evaluation metrics for segmentation and classification. The experimental results indicate an overall upward trend as the amount of unlabeled data increases. This is because more unlabeled pre-training data contain richer features, so the model can learn more distinctive feature representations, thereby improving the generalization ability. Hence, it can be anticipated that the proposed method may yield even greater benefits when applied to a larger dataset for self-supervised pre-training.</p></sec><sec id="sec4dot4dot2-sensors-25-01272"><title>4.4.2. Effectiveness of the Amount of Fine-Tuning Labeled Data</title><p>This section examines the influence of utilizing additional labeled data for fine-tuning on model performance. Initially, we expanded the fine-tuning labeled dataset to the same size as the pre-training unlabeled dataset. Then, we fine-tuned the model using varying proportions of labeled data, specifically 1%, 5%, 10%, 25%, 50%, and 100% of the self-supervised unlabeled data. The results, presented in <xref rid="sensors-25-01272-t007" ref-type="table">Table 7</xref>, indicate that as the amount of labeled fine-tuning data increased, the evaluation metrics exhibited an upward trend, although at a diminishing rate. This can be attributed to the model&#x02019;s ability to learn more information with an increasing number of labels but with a diminishing rate for each additional label. When the amount of labeled data reaches 25% of the self-supervised unlabeled data, the model achieves outstanding performance, and almost all of the evaluation metrics reach a fairly satisfactory result. Therefore, our proposed self-supervised contrastive learning model is not heavily dependent on the size of the labeled data but still remains effective even when a large amount of labeled data are available.</p></sec><sec id="sec4dot4dot3-sensors-25-01272"><title>4.4.3. Ablation Study</title><p>In this section, we describe ablation experiments we conducted to investigate the effectiveness of the modules in our proposed method. Our proposed model mainly consists of three parts: global contrastive learning module, local contrastive learning module, and fine-tuning module. We chose to remove one or two modules separately for experimental research. The experimental results are shown in <xref rid="sensors-25-01272-t008" ref-type="table">Table 8</xref>. We present the results of the ablative experiments conducted to assess the effectiveness of different modules. The horizontal axis represents the evaluation metrics, while the vertical axis lists various combinations of the modules. When there is only the global contrastive learning module or only the local contrastive learning module, the performance is slightly inferior to the experimental results obtained from the complete network. This indicates that both the global information and local information are beneficial for our segmentation and recognition task from different perspectives. Therefore, when removing both the global and local modules, the model performance becomes extremely poor. The purpose of the fine-tuning module is to make the self supervised pre-trained model better adapt to the specific downstream task. Hence, when the fine-tuning module is removed, the model does not adapt well to the recognition and segmentation of mixed-type WBM defect patterns. To sum up, each module is proven to be crucial for the performance of the model through the ablation experiments.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01272"><title>5. Conclusions</title><p>In this paper, we introduce a self-supervised contrastive learning model designed for the classification and segmentation of mixed-type WBM defect patterns. The experimental results indicate that our model surpasses other self-supervised contrastive learning approaches. Furthermore, we demonstrate that the performance of our proposed model does not heavily depend on labeled data, but more self-supervised unlabeled pre-training data can improve the model&#x02019;s performance. And the ablation study further assesses the contribution of each model component to the overall performance. In practical scenarios where large amounts of unlabeled WBM data can be easily obtained, our proposed model will have significant real-world applications. However, in the process of chip manufacturing, even minor defects can lead to chip failure or performance degradation. Therefore, the requirements for defect detection are extremely stringent. We will focus on improving the detection performance with a limited number of labeled samples. In future research, we aim to integrate network architecture optimizations with self-supervised learning to increase model evaluation accuracy, reduce processing time, and lower computational resource demands. We also plan to extend the application of our model to other fields, such as medical image segmentation and remote sensing image analysis.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>All authors contributed to the study conception and design. Material preparation, data collection, and analysis were performed by S.Y., Y.Z. and R.W. The first draft of the manuscript was written by S.Y., and all authors commented on previous versions of the manuscript. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The datasets used in this study are available from the corresponding author upon reasonable request. These data were used under license for the current study and cannot be redistributed without permission from the data provider.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01272"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shin</surname><given-names>E.</given-names></name>
<name><surname>Yoo</surname><given-names>C.D.</given-names></name>
</person-group><article-title>Efficient Convolutional Neural Networks for Semiconductor Wafer Bin Map Classification</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>1926</elocation-id><pub-id pub-id-type="doi">10.3390/s23041926</pub-id><pub-id pub-id-type="pmid">36850523</pub-id>
</element-citation></ref><ref id="B2-sensors-25-01272"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chiu</surname><given-names>M.-C.</given-names></name>
<name><surname>Chen</surname><given-names>T.-M.</given-names></name>
</person-group><article-title>Applying Data Augmentation and Mask R-CNN-Based Instance Segmentation Method for Mixed-Type Wafer Maps Defect Patterns Classification</article-title><source>IEEE Trans. Semicond. Manuf.</source><year>2021</year><volume>34</volume><fpage>455</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1109/TSM.2021.3118922</pub-id></element-citation></ref><ref id="B3-sensors-25-01272"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nakazawa</surname><given-names>T.</given-names></name>
<name><surname>Kulkarni</surname><given-names>D.V.</given-names></name>
</person-group><article-title>Wafer Map Defect Pattern Classification and Image Retrieval Using Convolutional Neural Network</article-title><source>IEEE Trans. Semicond. Manuf.</source><year>2018</year><volume>31</volume><fpage>309</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1109/TSM.2018.2795466</pub-id></element-citation></ref><ref id="B4-sensors-25-01272"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>H.</given-names></name>
<name><surname>Kim</surname><given-names>H.</given-names></name>
</person-group><article-title>Semi-Supervised Multi-Label Learning for Classification of Wafer Bin Maps With Mixed-Type Defect Patterns</article-title><source>IEEE Trans. Semicond. Manuf.</source><year>2020</year><volume>33</volume><fpage>653</fpage><lpage>662</lpage><pub-id pub-id-type="doi">10.1109/TSM.2020.3027431</pub-id></element-citation></ref><ref id="B5-sensors-25-01272"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Xu</surname><given-names>C.</given-names></name>
<name><surname>Yang</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
</person-group><article-title>Deformable Convolutional Networks for Efficient Mixed-Type Wafer Defect Pattern Recognition</article-title><source>IEEE Trans. Semicond. Manuf.</source><year>2020</year><volume>33</volume><fpage>587</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1109/TSM.2020.3020985</pub-id></element-citation></ref><ref id="B6-sensors-25-01272"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>C.W.</given-names></name>
<name><surname>Chien</surname><given-names>C.F.</given-names></name>
</person-group><article-title>An intelligent system for wafer bin map defect diagnosis: An empirical study for semiconductor manufacturing</article-title><source>Eng. Appl. Artif. Intell.</source><year>2013</year><volume>26</volume><fpage>1479</fpage><lpage>1486</lpage><pub-id pub-id-type="doi">10.1016/j.engappai.2012.11.009</pub-id></element-citation></ref><ref id="B7-sensors-25-01272"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qian</surname><given-names>Y.</given-names></name>
<name><surname>Tang</surname><given-names>S.-K.</given-names></name>
</person-group><article-title>Multi-Scale Contrastive Learning with Hierarchical Knowledge Synergy for Visible-Infrared Person Re-Identification</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>192</elocation-id><pub-id pub-id-type="doi">10.3390/s25010192</pub-id><pub-id pub-id-type="pmid">39796982</pub-id>
</element-citation></ref><ref id="B8-sensors-25-01272"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Fan</surname><given-names>H.</given-names></name>
<name><surname>Girshick</surname><given-names>R.</given-names></name>
<name><surname>He</surname><given-names>K.</given-names></name>
</person-group><article-title>Improved baselines with momentum contrastive learning</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2003.04297</pub-id></element-citation></ref><ref id="B9-sensors-25-01272"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>T.</given-names></name>
<name><surname>Kornblith</surname><given-names>S.</given-names></name>
<name><surname>Norouzi</surname><given-names>M.</given-names></name>
<name><surname>Hinton</surname><given-names>G.</given-names></name>
</person-group><article-title>A simple framework for contrastive learning of visual representations</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Los Angeles, CA, USA</conf-loc><conf-date>12&#x02013;18 July 2020</conf-date><fpage>1597</fpage><lpage>1607</lpage></element-citation></ref><ref id="B10-sensors-25-01272"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Q.</given-names></name>
</person-group><article-title>Semantic Segmentation of Traffic Scene Based on DeepLabv3+ and Attention Mechanism</article-title><source>Proceedings of the International Conference on Neural Networks, Information and Communication Engineering</source><conf-loc>Guangzhou, China</conf-loc><conf-date>10&#x02013;12 December 2023</conf-date><fpage>542</fpage><lpage>547</lpage></element-citation></ref><ref id="B11-sensors-25-01272"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>G.</given-names></name>
<name><surname>Liu</surname><given-names>R.</given-names></name>
<name><surname>Huang</surname><given-names>H.</given-names></name>
<name><surname>Zhu</surname><given-names>Q.</given-names></name>
<name><surname>Tao</surname><given-names>C.</given-names></name>
</person-group><article-title>Global and Local Contrastive Self-Supervised Learning for Semantic Segmentation of HR Remote Sensing Images</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>5618014</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3147513</pub-id></element-citation></ref><ref id="B12-sensors-25-01272"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tong</surname><given-names>L.I.</given-names></name>
<name><surname>Wang</surname><given-names>C.H.</given-names></name>
<name><surname>Huang</surname><given-names>C.L.</given-names></name>
</person-group><article-title>Monitoring defects in IC fabrication using a Hotelling T/sup 2/ control chart</article-title><source>IEEE Trans. Semicond. Manuf.</source><year>2005</year><volume>18</volume><fpage>140</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1109/TSM.2004.836659</pub-id></element-citation></ref><ref id="B13-sensors-25-01272"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>B.</given-names></name>
<name><surname>Jeong</surname><given-names>Y.S.</given-names></name>
<name><surname>Tong</surname><given-names>S.H.</given-names></name>
<name><surname>Chang</surname><given-names>I.K.</given-names></name>
<name><surname>Jeong</surname><given-names>M.K.</given-names></name>
</person-group><article-title>Step-Down Spatial Randomness Test for Detecting Abnormalities in DRAM Wafers with Multiple Spatial Maps</article-title><source>IEEE Trans. Semicond. Manuf.</source><year>2016</year><volume>29</volume><fpage>57</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1109/TSM.2015.2486383</pub-id></element-citation></ref><ref id="B14-sensors-25-01272"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>C.H.</given-names></name>
<name><surname>Kuo</surname><given-names>W.</given-names></name>
<name><surname>Bensmail</surname><given-names>H.</given-names></name>
</person-group><article-title>Detection and classification of defect patterns on semiconductor wafers</article-title><source>IIE Trans.</source><year>2006</year><volume>38</volume><fpage>1059</fpage><lpage>1068</lpage><pub-id pub-id-type="doi">10.1080/07408170600733236</pub-id></element-citation></ref><ref id="B15-sensors-25-01272"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yuan</surname><given-names>T.</given-names></name>
<name><surname>Kuo</surname><given-names>W.</given-names></name>
</person-group><article-title>Spatial defect pattern recognition on semiconductor wafers using model-based clustering and Bayesian inference</article-title><source>Eur. J. Oper.</source><year>2008</year><volume>190</volume><fpage>228</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/j.ejor.2007.06.007</pub-id></element-citation></ref><ref id="B16-sensors-25-01272"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>R.</given-names></name>
<name><surname>Che</surname><given-names>N.</given-names></name>
</person-group><article-title>Wafer Map Defect Pattern Recognition Using Rotation-Invariant Features</article-title><source>IEEE Trans. Semicond. Manuf.</source><year>2019</year><volume>32</volume><fpage>596</fpage><lpage>604</lpage><pub-id pub-id-type="doi">10.1109/TSM.2019.2944181</pub-id></element-citation></ref><ref id="B17-sensors-25-01272"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kang</surname><given-names>H.</given-names></name>
<name><surname>Kang</surname><given-names>S.</given-names></name>
</person-group><article-title>Semi-supervised rotation-invariant representation learning for wafer map pattern analysis</article-title><source>Eng. Appl. Artif. Intell.</source><year>2023</year><volume>120</volume><fpage>105864</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2023.105864</pub-id></element-citation></ref><ref id="B18-sensors-25-01272"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Byun</surname><given-names>Y.</given-names></name>
<name><surname>Baek</surname><given-names>J.G.</given-names></name>
</person-group><article-title>Mixed Pattern Recognition Methodology on Wafer Maps with Pre-trained Convolutional Neural Networks</article-title><source>Proceedings of the International Conference on Agents and Artificial Intelligence</source><conf-loc>Lisbon, Portugal</conf-loc><conf-date>19&#x02013;21 February 2020</conf-date><fpage>53</fpage><lpage>62</lpage></element-citation></ref><ref id="B19-sensors-25-01272"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tello</surname><given-names>G.</given-names></name>
<name><surname>Al-Jarrah</surname><given-names>O.Y.</given-names></name>
<name><surname>Yoo</surname><given-names>P.D.</given-names></name>
<name><surname>Al-Hammadi</surname><given-names>Y.</given-names></name>
<name><surname>Muhaidat</surname><given-names>S.</given-names></name>
<name><surname>Lee</surname><given-names>U.</given-names></name>
</person-group><article-title>Deep-Structured Machine Learning Model for the Recognition of Mixed-Defect Patterns in Semiconductor Fabrication Processes</article-title><source>IEEE Trans. Semicond. Manuf.</source><year>2018</year><volume>31</volume><fpage>315</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1109/TSM.2018.2825482</pub-id></element-citation></ref><ref id="B20-sensors-25-01272"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kyeong</surname><given-names>K.</given-names></name>
<name><surname>Kim</surname><given-names>H.</given-names></name>
</person-group><article-title>Classification of Mixed-Type Defect Patterns in Wafer Bin Maps Using Convolutional Neural Networks</article-title><source>IEEE Trans. Semicond. Manuf.</source><year>2018</year><volume>31</volume><fpage>395</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1109/TSM.2018.2841416</pub-id></element-citation></ref><ref id="B21-sensors-25-01272"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nag</surname><given-names>S.</given-names></name>
<name><surname>Makwana</surname><given-names>D.</given-names></name>
<name><surname>Mittal</surname><given-names>S.</given-names></name>
<name><surname>Mohan</surname><given-names>C.K.</given-names></name>
</person-group><article-title>Wafersegclassnet&#x02014;A light-weight network for classification and segmentation of semiconductor wafer defects</article-title><source>Comput. Ind.</source><year>2022</year><volume>142</volume><fpage>103720</fpage><pub-id pub-id-type="doi">10.1016/j.compind.2022.103720</pub-id></element-citation></ref><ref id="B22-sensors-25-01272"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Gkioxari</surname><given-names>G.</given-names></name>
<name><surname>Doll&#x000e1;r</surname><given-names>P.</given-names></name>
<name><surname>Girshick</surname><given-names>R.</given-names></name>
</person-group><article-title>Mask r-cnn</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Washington, DC, USA</conf-loc><conf-date>22&#x02013;29 October 2017</conf-date><fpage>33</fpage><lpage>42</lpage></element-citation></ref><ref id="B23-sensors-25-01272"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yan</surname><given-names>J.</given-names></name>
<name><surname>Sheng</surname><given-names>Y.</given-names></name>
<name><surname>Piao</surname><given-names>M.</given-names></name>
</person-group><article-title>Semantic Segmentation-Based Wafer Map Mixed-Type Defect Pattern Recognition</article-title><source>IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst.</source><year>2023</year><volume>42</volume><fpage>4065</fpage><lpage>4074</lpage><pub-id pub-id-type="doi">10.1109/TCAD.2023.3274958</pub-id></element-citation></ref><ref id="B24-sensors-25-01272"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>G.</given-names></name>
</person-group><article-title>Self-Supervised Dam Deformation Anomaly Detection Based on Temporal&#x02013;Spatial Contrast Learning</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>5858</elocation-id><pub-id pub-id-type="doi">10.3390/s24175858</pub-id><pub-id pub-id-type="pmid">39275768</pub-id>
</element-citation></ref><ref id="B25-sensors-25-01272"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kumari</surname><given-names>P.</given-names></name>
<name><surname>Kern</surname><given-names>J.</given-names></name>
<name><surname>Raedle</surname><given-names>M.</given-names></name>
</person-group><article-title>Self-Supervised and Zero-Shot Learning in Multi-Modal Raman Light Sheet Microscopy</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>8143</elocation-id><pub-id pub-id-type="doi">10.3390/s24248143</pub-id><pub-id pub-id-type="pmid">39771883</pub-id>
</element-citation></ref><ref id="B26-sensors-25-01272"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>Z.</given-names></name>
<name><surname>Xiong</surname><given-names>Y.</given-names></name>
<name><surname>Yu</surname><given-names>S.X.</given-names></name>
<name><surname>Lin</surname><given-names>D.</given-names></name>
</person-group><article-title>Unsupervised feature learning via non-parametric instance discrimination</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#x02013;22 June 2018</conf-date><fpage>3733</fpage><lpage>3742</lpage></element-citation></ref><ref id="B27-sensors-25-01272"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kwak</surname><given-names>M.G.</given-names></name>
<name><surname>Lee</surname><given-names>Y.J.</given-names></name>
<name><surname>Kim</surname><given-names>S.B.</given-names></name>
</person-group><article-title>SWaCo: Safe Wafer Bin Map Classification With Self-Supervised Contrastive Learning</article-title><source>IEEE Trans. Semicond. Manuf.</source><year>2023</year><volume>6</volume><fpage>416</fpage><lpage>424</lpage><pub-id pub-id-type="doi">10.1109/TSM.2023.3280891</pub-id></element-citation></ref><ref id="B28-sensors-25-01272"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kahng</surname><given-names>H.</given-names></name>
<name><surname>Kim</surname><given-names>S.B.</given-names></name>
</person-group><article-title>Self-supervised representation learning for wafer bin map defect pattern classification</article-title><source>IEEE Trans. Semicond. Manuf.</source><year>2020</year><volume>34</volume><fpage>74</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1109/TSM.2020.3038165</pub-id></element-citation></ref><ref id="B29-sensors-25-01272"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>H.</given-names></name>
<name><surname>He</surname><given-names>C.</given-names></name>
<name><surname>Li</surname><given-names>P.</given-names></name>
</person-group><article-title>Semi-supervised Wafer Map Pattern Recognition using Domain-Specific Data Augmentation and Contrastive Learning</article-title><source>Proceedings of the 2021 IEEE International Test Conference</source><conf-loc>Anaheim, CA, USA</conf-loc><conf-date>1&#x02013;4 November 2021</conf-date><fpage>113</fpage><lpage>122</lpage></element-citation></ref><ref id="B30-sensors-25-01272"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Oord</surname><given-names>A.V.D.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Vinyals</surname><given-names>O.</given-names></name>
</person-group><article-title>Representation learning with contrastive predictive coding</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1807.03748</pub-id></element-citation></ref><ref id="B31-sensors-25-01272"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Jadon</surname><given-names>S.</given-names></name>
</person-group><article-title>A survey of loss functions for semantic segmentation</article-title><source>Proceedings of the 2020 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology</source><conf-loc>Via del Mar, Chile</conf-loc><conf-date>3&#x02013;5 August 2020</conf-date><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="B32-sensors-25-01272"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mann</surname><given-names>W.R.</given-names></name>
<name><surname>Taber</surname><given-names>F.L.</given-names></name>
<name><surname>Seitzer</surname><given-names>P.W.</given-names></name>
<name><surname>Broz</surname><given-names>J.J.</given-names></name>
</person-group><article-title>The leading edge of production wafer probe test technology</article-title><source>Proceedings of the 2004 International Conferce on Test</source><conf-loc>Charlotte, NC, USA</conf-loc><conf-date>25&#x02013;29 April 2004</conf-date><fpage>1168</fpage><lpage>1195</lpage></element-citation></ref><ref id="B33-sensors-25-01272"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>L.C.</given-names></name>
<name><surname>Papandreou</surname><given-names>G.</given-names></name>
<name><surname>Schroff</surname><given-names>F.</given-names></name>
<name><surname>Adam</surname><given-names>H.</given-names></name>
</person-group><article-title>Rethinking atrous convolution for semantic image segmentation</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1706.05587</pub-id></element-citation></ref><ref id="B34-sensors-25-01272"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Noroozi</surname><given-names>M.</given-names></name>
<name><surname>Favaro</surname><given-names>P.</given-names></name>
</person-group><article-title>Unsupervised learning of visual representations by solving jigsaw puzzles</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>8&#x02013;16 October 2016</conf-date><fpage>69</fpage><lpage>84</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01272-f001"><label>Figure 1</label><caption><p>(<bold>a</bold>) Eight common single-type WBM defect patterns. (<bold>b</bold>) Examples of mixed-type WBM defect patterns.</p></caption><graphic xlink:href="sensors-25-01272-g001" position="float"/></fig><fig position="float" id="sensors-25-01272-f002"><label>Figure 2</label><caption><p>The self-supervised learning paradigm.</p></caption><graphic xlink:href="sensors-25-01272-g002" position="float"/></fig><fig position="float" id="sensors-25-01272-f003"><label>Figure 3</label><caption><p>The overall framework of our proposed model. Note that the figure omits the display of negative samples. In the self-supervised pre-training stage, we pre-train the network using a large amount of unlabeled data <inline-formula><mml:math id="mm179" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mi>u</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. After that, we transfer the knowledge embedded in the query encoder and query decoder to the downstream task and fine-tune the network with few labeled samples <inline-formula><mml:math id="mm180" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-25-01272-g003" position="float"/></fig><fig position="float" id="sensors-25-01272-f004"><label>Figure 4</label><caption><p>Illustration of the data augmentation operators.</p></caption><graphic xlink:href="sensors-25-01272-g004" position="float"/></fig><fig position="float" id="sensors-25-01272-f005"><label>Figure 5</label><caption><p>Process of local region selection and matching.</p></caption><graphic xlink:href="sensors-25-01272-g005" position="float"/></fig><fig position="float" id="sensors-25-01272-f006"><label>Figure 6</label><caption><p>Wafer probe testing station.</p></caption><graphic xlink:href="sensors-25-01272-g006" position="float"/></fig><fig position="float" id="sensors-25-01272-f007"><label>Figure 7</label><caption><p>Examples of visualization results obtained using our proposed model.</p></caption><graphic xlink:href="sensors-25-01272-g007" position="float"/></fig><table-wrap position="float" id="sensors-25-01272-t001"><object-id pub-id-type="pii">sensors-25-01272-t001_Table 1</object-id><label>Table 1</label><caption><p>Description of mixed-type WBM defect patterns.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Single-Type</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">C (C1)</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">D (C2)</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">EL (C3)</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ER (C4)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L (C5)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">S (C6)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NF (C7)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R (C8)</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Two-Mixed-Type</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C + EL (C9)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C + ER (C10)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C + L (C11)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C + S (C12)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D + EL (C13)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D + ER (C14)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D + L (C15)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D + S (C16)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EL + L (C17)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EL + S (C18)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ER + L (C19)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ER + S (C20)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L + S (C21)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Three-Mixed-Type</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C + EL + L (C22)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C + EL + S (C23)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C + ER + L (C24)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C + ER + S (C25)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D + EL + L (C26)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D + EL + S (C27)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D + ER + L (C28)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D + ER + S (C29)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C + L + S (C30)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D + L + S (C31)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EL + L + S (C32)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ER + L + S (C33)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Four-Mixed-Type</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C + L + EL + S (C34)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C + L + ER + S (C35)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D + L + EL + S (C36)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D + L + ER + S (C37)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01272-t002"><object-id pub-id-type="pii">sensors-25-01272-t002_Table 2</object-id><label>Table 2</label><caption><p>Segmentation and classification results for normal and single-type defect patterns (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Category</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">C0</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">C1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">C2</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">C3</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">C4</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">C5</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">C6</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">C7</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">C8</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Segmentation</td><td align="center" valign="middle" rowspan="1" colspan="1">Pacc</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">98.69</td><td align="center" valign="middle" rowspan="1" colspan="1">96.40</td><td align="center" valign="middle" rowspan="1" colspan="1">95.95</td><td align="center" valign="middle" rowspan="1" colspan="1">98.02</td><td align="center" valign="middle" rowspan="1" colspan="1">96.90</td><td align="center" valign="middle" rowspan="1" colspan="1">91.49</td><td align="center" valign="middle" rowspan="1" colspan="1">98.74</td><td align="center" valign="middle" rowspan="1" colspan="1">90.11</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IoU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.46</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Classification</td><td align="center" valign="middle" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">98.75</td><td align="center" valign="middle" rowspan="1" colspan="1">98.67</td><td align="center" valign="middle" rowspan="1" colspan="1">95.83</td><td align="center" valign="middle" rowspan="1" colspan="1">96.89</td><td align="center" valign="middle" rowspan="1" colspan="1">98.41</td><td align="center" valign="middle" rowspan="1" colspan="1">97.04</td><td align="center" valign="middle" rowspan="1" colspan="1">94.54</td><td align="center" valign="middle" rowspan="1" colspan="1">98.72</td><td align="center" valign="middle" rowspan="1" colspan="1">92.32</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">96.66</td><td align="center" valign="middle" rowspan="1" colspan="1">97.24</td><td align="center" valign="middle" rowspan="1" colspan="1">93.46</td><td align="center" valign="middle" rowspan="1" colspan="1">91.89</td><td align="center" valign="middle" rowspan="1" colspan="1">98.74</td><td align="center" valign="middle" rowspan="1" colspan="1">97.30</td><td align="center" valign="middle" rowspan="1" colspan="1">92.07</td><td align="center" valign="middle" rowspan="1" colspan="1">95.41</td><td align="center" valign="middle" rowspan="1" colspan="1">88.74</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.68</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.01</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01272-t003"><object-id pub-id-type="pii">sensors-25-01272-t003_Table 3</object-id><label>Table 3</label><caption><p>Segmentation results for mixed-type defect patterns (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Category</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">C</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">D</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">EL</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">ER</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">L</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">S</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pacc</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IoU</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pacc</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IoU</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pacc</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IoU</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pacc</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IoU</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pacc</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IoU</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pacc</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IoU</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">C9</td><td align="center" valign="middle" rowspan="1" colspan="1">98.57</td><td align="center" valign="middle" rowspan="1" colspan="1">96.12</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.94</td><td align="center" valign="middle" rowspan="1" colspan="1">93.23</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C10</td><td align="center" valign="middle" rowspan="1" colspan="1">98.66</td><td align="center" valign="middle" rowspan="1" colspan="1">96.19</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">97.94</td><td align="center" valign="middle" rowspan="1" colspan="1">95.48</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C11</td><td align="center" valign="middle" rowspan="1" colspan="1">98.49</td><td align="center" valign="middle" rowspan="1" colspan="1">96.10</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.88</td><td align="center" valign="middle" rowspan="1" colspan="1">96.01</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C12</td><td align="center" valign="middle" rowspan="1" colspan="1">98.48</td><td align="center" valign="middle" rowspan="1" colspan="1">96.10</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">91.48</td><td align="center" valign="middle" rowspan="1" colspan="1">89.35</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C13</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.23</td><td align="center" valign="middle" rowspan="1" colspan="1">93.89</td><td align="center" valign="middle" rowspan="1" colspan="1">95.66</td><td align="center" valign="middle" rowspan="1" colspan="1">93.10</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C14</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.31</td><td align="center" valign="middle" rowspan="1" colspan="1">94.08</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">97.91</td><td align="center" valign="middle" rowspan="1" colspan="1">95.47</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C15</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.20</td><td align="center" valign="middle" rowspan="1" colspan="1">93.88</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.79</td><td align="center" valign="middle" rowspan="1" colspan="1">95.12</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C16</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.22</td><td align="center" valign="middle" rowspan="1" colspan="1">93.85</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">91.46</td><td align="center" valign="middle" rowspan="1" colspan="1">89.34</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C17</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.21</td><td align="center" valign="middle" rowspan="1" colspan="1">92.16</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.20</td><td align="center" valign="middle" rowspan="1" colspan="1">95.03</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C18</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.29</td><td align="center" valign="middle" rowspan="1" colspan="1">92.17</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">91.34</td><td align="center" valign="middle" rowspan="1" colspan="1">89.19</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C19</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">97.82</td><td align="center" valign="middle" rowspan="1" colspan="1">95.40</td><td align="center" valign="middle" rowspan="1" colspan="1">96.77</td><td align="center" valign="middle" rowspan="1" colspan="1">95.13</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C20</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">97.77</td><td align="center" valign="middle" rowspan="1" colspan="1">95.35</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">91.22</td><td align="center" valign="middle" rowspan="1" colspan="1">89.15</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.94</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C22</td><td align="center" valign="middle" rowspan="1" colspan="1">98.43</td><td align="center" valign="middle" rowspan="1" colspan="1">96.02</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.32</td><td align="center" valign="middle" rowspan="1" colspan="1">92.20</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.14</td><td align="center" valign="middle" rowspan="1" colspan="1">94.90</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C23</td><td align="center" valign="middle" rowspan="1" colspan="1">98.42</td><td align="center" valign="middle" rowspan="1" colspan="1">96.02</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.32</td><td align="center" valign="middle" rowspan="1" colspan="1">92.21</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">90.19</td><td align="center" valign="middle" rowspan="1" colspan="1">88.03</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C24</td><td align="center" valign="middle" rowspan="1" colspan="1">98.45</td><td align="center" valign="middle" rowspan="1" colspan="1">96.04</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">97.52</td><td align="center" valign="middle" rowspan="1" colspan="1">95.06</td><td align="center" valign="middle" rowspan="1" colspan="1">96.02</td><td align="center" valign="middle" rowspan="1" colspan="1">94.78</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C25</td><td align="center" valign="middle" rowspan="1" colspan="1">98.45</td><td align="center" valign="middle" rowspan="1" colspan="1">96.03</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">97.55</td><td align="center" valign="middle" rowspan="1" colspan="1">95.07</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">90.12</td><td align="center" valign="middle" rowspan="1" colspan="1">87.94</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C26</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.18</td><td align="center" valign="middle" rowspan="1" colspan="1">91.98</td><td align="center" valign="middle" rowspan="1" colspan="1">93.40</td><td align="center" valign="middle" rowspan="1" colspan="1">91.03</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">94.99</td><td align="center" valign="middle" rowspan="1" colspan="1">92.11</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C27</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.15</td><td align="center" valign="middle" rowspan="1" colspan="1">91.90</td><td align="center" valign="middle" rowspan="1" colspan="1">93.39</td><td align="center" valign="middle" rowspan="1" colspan="1">91.03</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">90.10</td><td align="center" valign="middle" rowspan="1" colspan="1">89.90</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C28</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.20</td><td align="center" valign="middle" rowspan="1" colspan="1">91.92</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.18</td><td align="center" valign="middle" rowspan="1" colspan="1">94.89</td><td align="center" valign="middle" rowspan="1" colspan="1">95.03</td><td align="center" valign="middle" rowspan="1" colspan="1">92.12</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C29</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.18</td><td align="center" valign="middle" rowspan="1" colspan="1">91.97</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.15</td><td align="center" valign="middle" rowspan="1" colspan="1">94.87</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">90.10</td><td align="center" valign="middle" rowspan="1" colspan="1">87.89</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C30</td><td align="center" valign="middle" rowspan="1" colspan="1">98.39</td><td align="center" valign="middle" rowspan="1" colspan="1">95.95</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">94.95</td><td align="center" valign="middle" rowspan="1" colspan="1">92.03</td><td align="center" valign="middle" rowspan="1" colspan="1">90.02</td><td align="center" valign="middle" rowspan="1" colspan="1">87.86</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C31</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.11</td><td align="center" valign="middle" rowspan="1" colspan="1">91.86</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">94.92</td><td align="center" valign="middle" rowspan="1" colspan="1">92.02</td><td align="center" valign="middle" rowspan="1" colspan="1">90.01</td><td align="center" valign="middle" rowspan="1" colspan="1">87.85</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C32</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">93.08</td><td align="center" valign="middle" rowspan="1" colspan="1">90.02</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">94.86</td><td align="center" valign="middle" rowspan="1" colspan="1">91.95</td><td align="center" valign="middle" rowspan="1" colspan="1">89.90</td><td align="center" valign="middle" rowspan="1" colspan="1">87.80</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.80</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.90</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.81</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C34</td><td align="center" valign="middle" rowspan="1" colspan="1">97.98</td><td align="center" valign="middle" rowspan="1" colspan="1">95.07</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">92.11</td><td align="center" valign="middle" rowspan="1" colspan="1">89.20</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">93.87</td><td align="center" valign="middle" rowspan="1" colspan="1">90.26</td><td align="center" valign="middle" rowspan="1" colspan="1">88.40</td><td align="center" valign="middle" rowspan="1" colspan="1">85.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C35</td><td align="center" valign="middle" rowspan="1" colspan="1">98.00</td><td align="center" valign="middle" rowspan="1" colspan="1">95.08</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.11</td><td align="center" valign="middle" rowspan="1" colspan="1">93.57</td><td align="center" valign="middle" rowspan="1" colspan="1">93.89</td><td align="center" valign="middle" rowspan="1" colspan="1">90.27</td><td align="center" valign="middle" rowspan="1" colspan="1">88.41</td><td align="center" valign="middle" rowspan="1" colspan="1">85.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C36</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">93.85</td><td align="center" valign="middle" rowspan="1" colspan="1">90.07</td><td align="center" valign="middle" rowspan="1" colspan="1">92.03</td><td align="center" valign="middle" rowspan="1" colspan="1">88.79</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">93.08</td><td align="center" valign="middle" rowspan="1" colspan="1">89.88</td><td align="center" valign="middle" rowspan="1" colspan="1">88.16</td><td align="center" valign="middle" rowspan="1" colspan="1">85.21</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.90</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.25</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01272-t004"><object-id pub-id-type="pii">sensors-25-01272-t004_Table 4</object-id><label>Table 4</label><caption><p>Classification results for mixed-type defect patterns (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Category</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">C9</td><td align="center" valign="middle" rowspan="1" colspan="1">95.62</td><td align="center" valign="middle" rowspan="1" colspan="1">98.67</td><td align="center" valign="middle" rowspan="1" colspan="1">93.30</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C10</td><td align="center" valign="middle" rowspan="1" colspan="1">96.19</td><td align="center" valign="middle" rowspan="1" colspan="1">97.98</td><td align="center" valign="middle" rowspan="1" colspan="1">94.02</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C11</td><td align="center" valign="middle" rowspan="1" colspan="1">94.45</td><td align="center" valign="middle" rowspan="1" colspan="1">97.56</td><td align="center" valign="middle" rowspan="1" colspan="1">93.88</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C12</td><td align="center" valign="middle" rowspan="1" colspan="1">93.87</td><td align="center" valign="middle" rowspan="1" colspan="1">92.43</td><td align="center" valign="middle" rowspan="1" colspan="1">98.14</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C13</td><td align="center" valign="middle" rowspan="1" colspan="1">91.92</td><td align="center" valign="middle" rowspan="1" colspan="1">93.00</td><td align="center" valign="middle" rowspan="1" colspan="1">97.44</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C14</td><td align="center" valign="middle" rowspan="1" colspan="1">93.85</td><td align="center" valign="middle" rowspan="1" colspan="1">94.86</td><td align="center" valign="middle" rowspan="1" colspan="1">95.12</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C15</td><td align="center" valign="middle" rowspan="1" colspan="1">91.68</td><td align="center" valign="middle" rowspan="1" colspan="1">96.33</td><td align="center" valign="middle" rowspan="1" colspan="1">92.88</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C16</td><td align="center" valign="middle" rowspan="1" colspan="1">91.61</td><td align="center" valign="middle" rowspan="1" colspan="1">91.44</td><td align="center" valign="middle" rowspan="1" colspan="1">98.05</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C17</td><td align="center" valign="middle" rowspan="1" colspan="1">94.31</td><td align="center" valign="middle" rowspan="1" colspan="1">97.87</td><td align="center" valign="middle" rowspan="1" colspan="1">93.30</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C18</td><td align="center" valign="middle" rowspan="1" colspan="1">92.41</td><td align="center" valign="middle" rowspan="1" colspan="1">91.55</td><td align="center" valign="middle" rowspan="1" colspan="1">97.20</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C19</td><td align="center" valign="middle" rowspan="1" colspan="1">92.84</td><td align="center" valign="middle" rowspan="1" colspan="1">92.34</td><td align="center" valign="middle" rowspan="1" colspan="1">94.99</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C20</td><td align="center" valign="middle" rowspan="1" colspan="1">92.69</td><td align="center" valign="middle" rowspan="1" colspan="1">90.59</td><td align="center" valign="middle" rowspan="1" colspan="1">96.22</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.74</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C22</td><td align="center" valign="middle" rowspan="1" colspan="1">92.45</td><td align="center" valign="middle" rowspan="1" colspan="1">89.84</td><td align="center" valign="middle" rowspan="1" colspan="1">93.10</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C23</td><td align="center" valign="middle" rowspan="1" colspan="1">89.97</td><td align="center" valign="middle" rowspan="1" colspan="1">95.44</td><td align="center" valign="middle" rowspan="1" colspan="1">92.13</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C24</td><td align="center" valign="middle" rowspan="1" colspan="1">91.93</td><td align="center" valign="middle" rowspan="1" colspan="1">98.01</td><td align="center" valign="middle" rowspan="1" colspan="1">90.74</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C25</td><td align="center" valign="middle" rowspan="1" colspan="1">90.76</td><td align="center" valign="middle" rowspan="1" colspan="1">93.71</td><td align="center" valign="middle" rowspan="1" colspan="1">89.67</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C26</td><td align="center" valign="middle" rowspan="1" colspan="1">90.55</td><td align="center" valign="middle" rowspan="1" colspan="1">91.35</td><td align="center" valign="middle" rowspan="1" colspan="1">97.28</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C27</td><td align="center" valign="middle" rowspan="1" colspan="1">89.68</td><td align="center" valign="middle" rowspan="1" colspan="1">91.55</td><td align="center" valign="middle" rowspan="1" colspan="1">96.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C28</td><td align="center" valign="middle" rowspan="1" colspan="1">92.61</td><td align="center" valign="middle" rowspan="1" colspan="1">90.24</td><td align="center" valign="middle" rowspan="1" colspan="1">98.00</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C29</td><td align="center" valign="middle" rowspan="1" colspan="1">89.77</td><td align="center" valign="middle" rowspan="1" colspan="1">99.01</td><td align="center" valign="middle" rowspan="1" colspan="1">92.74</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C30</td><td align="center" valign="middle" rowspan="1" colspan="1">90.18</td><td align="center" valign="middle" rowspan="1" colspan="1">98.12</td><td align="center" valign="middle" rowspan="1" colspan="1">94.17</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C31</td><td align="center" valign="middle" rowspan="1" colspan="1">89.11</td><td align="center" valign="middle" rowspan="1" colspan="1">94.38</td><td align="center" valign="middle" rowspan="1" colspan="1">96.05</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C32</td><td align="center" valign="middle" rowspan="1" colspan="1">90.56</td><td align="center" valign="middle" rowspan="1" colspan="1">97.35</td><td align="center" valign="middle" rowspan="1" colspan="1">93.69</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.77</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C34</td><td align="center" valign="middle" rowspan="1" colspan="1">88.76</td><td align="center" valign="middle" rowspan="1" colspan="1">88.29</td><td align="center" valign="middle" rowspan="1" colspan="1">95.13</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C35</td><td align="center" valign="middle" rowspan="1" colspan="1">89.52</td><td align="center" valign="middle" rowspan="1" colspan="1">91.30</td><td align="center" valign="middle" rowspan="1" colspan="1">93.27</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C36</td><td align="center" valign="middle" rowspan="1" colspan="1">87.41</td><td align="center" valign="middle" rowspan="1" colspan="1">96.66</td><td align="center" valign="middle" rowspan="1" colspan="1">89.35</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.80</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.06</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01272-t005"><object-id pub-id-type="pii">sensors-25-01272-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison with other methods using limited labeled data on WBM defect pattern recognition and segmentation task (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Methods</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Segmentation (Overall)</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Recognition (Overall)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mPacc</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mIoU</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Jigsaw</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.71</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SimCLR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.13</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MoCo v2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.25</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.78</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01272-t006"><object-id pub-id-type="pii">sensors-25-01272-t006_Table 6</object-id><label>Table 6</label><caption><p>Experimental results with different amounts of pre-training unlabeled data (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Different Amounts of Pre-Training <break/>Unlabeled Data</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">1%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">25%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">50%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">75%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">100%</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(152)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(3800)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(7600)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(11,400)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(15,200)</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Segmentation results (Overall)</td><td align="center" valign="middle" rowspan="1" colspan="1">mPacc</td><td align="center" valign="middle" rowspan="1" colspan="1">31.12</td><td align="center" valign="middle" rowspan="1" colspan="1">40.55</td><td align="center" valign="middle" rowspan="1" colspan="1">72.64</td><td align="center" valign="middle" rowspan="1" colspan="1">89.33</td><td align="center" valign="middle" rowspan="1" colspan="1">95.59</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mIoU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.19</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Classification results (Overall)</td><td align="center" valign="middle" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">27.94</td><td align="center" valign="middle" rowspan="1" colspan="1">38.55</td><td align="center" valign="middle" rowspan="1" colspan="1">70.66</td><td align="center" valign="middle" rowspan="1" colspan="1">86.45</td><td align="center" valign="middle" rowspan="1" colspan="1">93.57</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">28.60</td><td align="center" valign="middle" rowspan="1" colspan="1">41.07</td><td align="center" valign="middle" rowspan="1" colspan="1">70.23</td><td align="center" valign="middle" rowspan="1" colspan="1">85.04</td><td align="center" valign="middle" rowspan="1" colspan="1">94.43</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.78</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01272-t007"><object-id pub-id-type="pii">sensors-25-01272-t007_Table 7</object-id><label>Table 7</label><caption><p>Experimental results with different amounts of fine-tuning labeled data (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Different Amounts of Fine-Tuning <break/>Labeled Data</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">1%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">5%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">10%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">25%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">50%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">100%</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(152)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(760)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1520)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(3800)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(7600)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(15,200)</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Segmentation results (Overall)</td><td align="center" valign="middle" rowspan="1" colspan="1">mPacc</td><td align="center" valign="middle" rowspan="1" colspan="1">56.32</td><td align="center" valign="middle" rowspan="1" colspan="1">70.92</td><td align="center" valign="middle" rowspan="1" colspan="1">88.25</td><td align="center" valign="middle" rowspan="1" colspan="1">95.54</td><td align="center" valign="middle" rowspan="1" colspan="1">95.72</td><td align="center" valign="middle" rowspan="1" colspan="1">95.80</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mIoU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.23</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Classification results (Overall)</td><td align="center" valign="middle" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">57.08</td><td align="center" valign="middle" rowspan="1" colspan="1">71.10</td><td align="center" valign="middle" rowspan="1" colspan="1">87.55</td><td align="center" valign="middle" rowspan="1" colspan="1">93.49</td><td align="center" valign="middle" rowspan="1" colspan="1">93.60</td><td align="center" valign="middle" rowspan="1" colspan="1">93.64</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">58.68</td><td align="center" valign="middle" rowspan="1" colspan="1">75.43</td><td align="center" valign="middle" rowspan="1" colspan="1">89.02</td><td align="center" valign="middle" rowspan="1" colspan="1">94.38</td><td align="center" valign="middle" rowspan="1" colspan="1">94.92</td><td align="center" valign="middle" rowspan="1" colspan="1">95.17</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.39</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.33</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01272-t008"><object-id pub-id-type="pii">sensors-25-01272-t008_Table 8</object-id><label>Table 8</label><caption><p>Results of ablation experiments on exploring the effectiveness of each module (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Modules</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Segmentation (Overall)</th><th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Classification (Overall)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Global</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Local</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fine-Tuning</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mPacc</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mIoU</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.78</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">75.09</td><td align="center" valign="middle" rowspan="1" colspan="1">71.54</td><td align="center" valign="middle" rowspan="1" colspan="1">72.64</td><td align="center" valign="middle" rowspan="1" colspan="1">75.16</td><td align="center" valign="middle" rowspan="1" colspan="1">71.81</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">84.11</td><td align="center" valign="middle" rowspan="1" colspan="1">78.52</td><td align="center" valign="middle" rowspan="1" colspan="1">82.46</td><td align="center" valign="middle" rowspan="1" colspan="1">80.08</td><td align="center" valign="middle" rowspan="1" colspan="1">83.25</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">86.44</td><td align="center" valign="middle" rowspan="1" colspan="1">80.22</td><td align="center" valign="middle" rowspan="1" colspan="1">84.10</td><td align="center" valign="middle" rowspan="1" colspan="1">80.59</td><td align="center" valign="middle" rowspan="1" colspan="1">85.67</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.01</td></tr></tbody></table></table-wrap></floats-group></article>