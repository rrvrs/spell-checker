<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006372</article-id><article-id pub-id-type="pmc">PMC11860109</article-id><article-id pub-id-type="doi">10.3390/s25041143</article-id><article-id pub-id-type="publisher-id">sensors-25-01143</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Acquisition and Modeling of Material Appearance Using a Portable, Low Cost, Device</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2389-0301</contrib-id><name><surname>Marelli</surname><given-names>Davide</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7070-1545</contrib-id><name><surname>Bianco</surname><given-names>Simone</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2878-2131</contrib-id><name><surname>Ciocca</surname><given-names>Gianluigi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="c1-sensors-25-01143" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Song</surname><given-names>Kechen</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Yan</surname><given-names>Yunhui</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01143">Department of Informatics, Systems and Communication, University of Milano-Bicocca, Viale Sarca 336, 20126 Milan, Italy; <email>davide.marelli@unimib.it</email> (D.M.); <email>simone.bianco@unimib.it</email> (S.B.)</aff><author-notes><corresp id="c1-sensors-25-01143"><label>*</label>Correspondence: <email>gianluigi.ciocca@unimib.it</email></corresp></author-notes><pub-date pub-type="epub"><day>13</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1143</elocation-id><history><date date-type="received"><day>21</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>24</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>11</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Material appearance acquisition allows researchers to capture the optical properties of surfaces and use them in different tasks such as material analysis, digital twins reproduction, 3D configurators, augmented and virtual reality, etc. Precise acquisition of such properties requires complex and expensive hardware. In this paper, we aim to answer the following research challenge: Can we design an accurate enough but low-cost and portable device for material appearance acquisition? We present the rationale behind the design of our device using consumer-grade hardware components. Ultimately, our device costs EUR 80 and can acquire surface patches of size 5 &#x000d7; 5 cm with a 40 pix/mm resolution. Our device exploits a traditional RGB camera to capture a surface using 24 different images, each photographed using different lighting conditions. The different lighting conditions are generated by exploiting the LED rings included in our device; specifically, each of the 24 images is acquired by turning on one individual LED at time. We also illustrate the custom processing pipelines developed to support capturing and generating the material data in terms of albedo, normal, and roughness maps. The accuracy of the acquisition process is comprehensively evaluated both quantitatively and qualitatively. Results show that our low-cost device can faithfully acquire different materials. The usefulness of our device is further demonstrated by a textile virtual catalog application that we designed for rendering virtual fabrics on a mobile apparatus.</p></abstract><kwd-group><kwd>material acquisition</kwd><kwd>material measurement</kwd><kwd>material reproduction</kwd><kwd>low-cost device</kwd><kwd>photometric stereo</kwd><kwd>computer graphics</kwd></kwd-group><funding-group><award-group><funding-source>NVIDIA</funding-source></award-group><award-group><funding-source>Department of Informatics, Systems, and Communication of the University of Milano-Bicocca, Italy</funding-source></award-group><funding-statement>This research was supported by grants from NVIDIA and utilized the NVIDIA Quadro RTX 6000. This work was partially supported by the MUR under the grant &#x0201c;Dipartimenti di Eccellenza 2023&#x02013;2027&#x0201d; of the Department of Informatics, Systems, and Communication of the University of Milano-Bicocca, Italy.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01143"><title>1. Introduction</title><p>Material appearance acquisition is the task of characterizing the optical properties of a surface and is a topic of interest in both computer vision and computer graphics fields. This topic is also relevant in the context of Industry 4.0, where it can be exploited for the creation of digital catalogs, digital twins, or reverse engineering of existing materials since it allows for the reproduction of elements that mimic real-world surfaces in the virtual world. The ability to virtually replicate the appearance of a surface enables the provision of interactive catalogs of object materials in order to replace the materials of real objects in 3D configurators for the customization, design, and prototyping of new items.</p><p>The appearance of materials can be modeled by a function. In the case of opaque materials, this function is called the Bidirectional Reflectance Distribution Function (BRDF) [<xref rid="B1-sensors-25-01143" ref-type="bibr">1</xref>], which models the amount of light energy reflected from any incoming direction into any outgoing direction for any given wavelength at a specific point <italic toggle="yes">p</italic> of a surface. It is thus a 4D function that maps any pair of directions over the upper unit hemisphere to a non-negative real number. The BRDF is intended to provide information about how light is scattered by the material at a single point on the surface, and it is commonly used to represent the material properties of uniform surfaces. However, in the real world, most objects are not made of uniform materials, and light is not scattered in the same way across the surface. The spatially varying bidirectional reflectance distribution function (SVBRDF) [<xref rid="B2-sensors-25-01143" ref-type="bibr">2</xref>] models the reflectance by adding a dependency on the surface position, making it a six-dimensional function.</p><p>Physically measuring material properties requires expensive devices such as gonioreflectometers that have long acquisition times and produce large amounts of data. These devices must also be carefully calibrated to align the light emitters with the detector sensors. To overcome these limitations, image-based approaches have been proposed. For example, in [<xref rid="B3-sensors-25-01143" ref-type="bibr">3</xref>], an object of known shape is photographed with a series of images, each capturing light reflected from parts of the surface while photogrammetry is used to measure the camera position. Another image-based technique is photometric stereo (PS) [<xref rid="B4-sensors-25-01143" ref-type="bibr">4</xref>]. It is a method for recovering local surface shape and albedo from multiple images taken from the same viewpoint but under different illumination directions. More recently, deep-learning-based methods have emerged to solve the task of material appearance acquisition. However, these methods require a large amount of training data to obtain suitable methods for estimating (not measuring) the material properties.</p><p>With the need for easy-to-use tools and devices for material acquisition in mind, this paper presents the design and development of a portable, low-cost device for the acquisition of a planar surface&#x02019;s material appearance, which can later be used to create realistic renderings of real surfaces.</p><p>Our device utilizes a conventional RGB camera to capture surface images under 24 distinct lighting conditions. These varying illumination captures are achieved using the integrated LED rings in the device, with each image acquired by activating a single LED at a time. A specifically designed processing pipeline then measures the material appearance by computing the normal map, albedo map, and roughness map that can be used in a computer graphics software to render and reproduce the original material. To this end, the device exploits photometric stereo to capture the surface properties. The advantage of this technique is that it does not require multiple sensors but only additional lighting that can be arranged compactly. PS allows us to seamlessly recover different surface properties. These properties are then used to describe the SVBRDF of the material. This device, which costs about EUR 80 (about USD 89), is validated by quantitative and qualitative evaluations. Finally, the device is used to acquire multiple materials and build a dataset of textiles to be used in a mixed reality catalog application.</p><p>Our device has a wide range of applications across various fields. In digital content creation, it can be used to capture and replicate realistic textures for 3D modeling, gaming, and virtual reality environments. In manufacturing and quality control, it enables precise color and texture matching to ensure consistency and defects detection in products such as fabrics, metals, plastics, and paints. It can be used for visual digital twin creation where the material can be faithfully reproduced, and its appearance can be evaluated with respect to other materials. In e-commerce, it can enhance online shopping experiences by providing accurate visual representations of products in virtual catalogs. The device can also aid in cultural preservation by documenting the appearance of artifacts and historical materials. Furthermore, it has potential uses in scientific research, such as studying surface properties of materials, and in medical fields, where it can assist in analyzing skin textures or other biological surfaces for diagnostic purposes.</p><p>The main contributions of this work are, therefore,</p><list list-type="simple"><list-item><label>-</label><p>Affordability and Accessibility: The entire system costs are low, as it is entirely built utilizing consumer-grade components in order to democratize material acquisition for both experts and non-experts.</p></list-item><list-item><label>-</label><p>Streamlined Hardware Design: The device integrates an RGB camera with LED-based lighting in a compact and portable setup, eliminating the need for expensive or bulky equipment.</p></list-item><list-item><label>-</label><p>Custom Processing Pipeline: We introduce a tailored software pipeline that combines photometric stereo with optimized algorithms to estimate albedo, surface normals, and roughness maps accurately.</p></list-item><list-item><label>-</label><p>Real-World Applicability: We validate the device&#x02019;s practicality through its application in creating a mixed reality textile catalog.</p></list-item></list><p>The paper is organized as follows: <xref rid="sec2-sensors-25-01143" ref-type="sec">Section 2</xref> reviews the existing literature on material appearance acquisition. <xref rid="sec3-sensors-25-01143" ref-type="sec">Section 3</xref> presents the design and methods of the proposed material appearance acquisition device. In <xref rid="sec4-sensors-25-01143" ref-type="sec">Section 4</xref>, we evaluate the accuracy of the device. <xref rid="sec5-sensors-25-01143" ref-type="sec">Section 5</xref> shows an example of the use of our device in a real application. As a use case, we have designed a virtual textile catalog application. Finally, <xref rid="sec6-sensors-25-01143" ref-type="sec">Section 6</xref> concludes the paper with some known limitations and possible solutions for future work.</p></sec><sec id="sec2-sensors-25-01143"><title>2. Related Work</title><p>The existing literature covers the task of material appearance acquisition for both devices and methods. Nowadays, traditional approaches such as the gonioreflectometer are being integrated with newer deep learning techniques. This section describes the most relevant approaches and devices for material appearance acquisition.</p><sec id="sec2dot1-sensors-25-01143"><title>2.1. BRDF Acquisition</title><p>Various devices and methods have been proposed to acquire the BRDF of a material. These methods range from gonioreflectometers [<xref rid="B2-sensors-25-01143" ref-type="bibr">2</xref>,<xref rid="B5-sensors-25-01143" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01143" ref-type="bibr">6</xref>] to image-based [<xref rid="B3-sensors-25-01143" ref-type="bibr">3</xref>,<xref rid="B7-sensors-25-01143" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01143" ref-type="bibr">8</xref>] and catadioptric [<xref rid="B9-sensors-25-01143" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01143" ref-type="bibr">10</xref>] measurement systems. Here, we are instead interested in obtaining spatial information about the material; thus, we focus on SVBRDF acquisition techniques. Setups for SVBRDF acquisition can be categorized into (hemi-)spherical gantry, photometric stereo, LCD light source, flashlight, and others based on the hardware used for acquisition.</p><p>Spherical and hemispherical gantries include the work of Rump et al. [<xref rid="B11-sensors-25-01143" ref-type="bibr">11</xref>]. They used a hemispherical gantry with 151 cameras with the cameras&#x02019; flashes as light sources. Each camera took one image for each flash (total of 151 &#x000d7; 151 = 22,801 pictures). The images were then processed to produce the material representation of the object. Ghosh et al. [<xref rid="B12-sensors-25-01143" ref-type="bibr">12</xref>] proposed three different setups to estimate SVBRDFs of isotropic and anisotropic materials, using up to nine polarized second-order spherical gradient illumination patterns.</p><p>In the category of LCD light source systems, Francken et al. [<xref rid="B13-sensors-25-01143" ref-type="bibr">13</xref>] and Aittala et al. [<xref rid="B14-sensors-25-01143" ref-type="bibr">14</xref>] proposed similar systems based on the use of LCD and an SLR camera. While the recovery process of Francken et al. is limited to the normal map, Aittala et al. can provide SVBRDFs of isotropic surfaces through Bayesian inference. Similarly, Wang et al. [<xref rid="B15-sensors-25-01143" ref-type="bibr">15</xref>] used a camera and an LCD screen as an area light source to measure specular and diffuse albedos, two surface roughness parameters, and a 1D power spectrum over frequencies for visible surface bumps. Riviere et al. [<xref rid="B16-sensors-25-01143" ref-type="bibr">16</xref>] proposed a mobile reflectometry solution using the LCD of a mobile device as an illumination source in a dimly lit room. Diffuse and specular components were separated by taking two pictures of the same sample with a differently oriented linear polarizer in front of the camera. Albedo, normal, and specular roughness were estimated using the same light patterns described in [<xref rid="B12-sensors-25-01143" ref-type="bibr">12</xref>].</p><p>The final category of flash illumination setups includes the work of Aittala et al. [<xref rid="B17-sensors-25-01143" ref-type="bibr">17</xref>]. A single mobile device is used with its onboard flash to acquire a flash + no-flash image pair of textured material. A multi-stage reconstruction pipeline allows capturing the full anisotropic SVBRDF of surfaces with repetitive patterns. This allows us to assume that multiple points on the surface share the same reflectance properties. The input images are registered by homography, and the image is divided into sub-tiles that roughly match the size of the repeating texture pattern. A master tile is used to compute initial geometric and photometric data of the repeating pattern. This data is then augmented by transferring high-frequency detail from similarly illuminated tiles.</p><p>In recent years, deep-learning based methods have emerged to solve the task of material appearance acquisition; these systems usually employ flashlight setup or exploit global lighting. Deschaintre et al. [<xref rid="B18-sensors-25-01143" ref-type="bibr">18</xref>] proposed the use of a deep encoder&#x02013;decoder convolutional neural network to recover per-pixel normal, diffuse albedo, specular albedo, and specular roughness from a single flash-lit picture of a flat surface. They also introduced the rendering loss to support the training process and evaluated the quality of the recovered SVBRDF by comparing its appearance to the ground truth by rendering both under the same lighting configuration. This takes advantage of an in-network render engine that supports inverse-rendering and backpropagation. Similarly, other works [<xref rid="B19-sensors-25-01143" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01143" ref-type="bibr">20</xref>] used encoder&#x02013;decoder neural networks to estimate the SVBRDF from a single image of a planar surface without needing specular highlights generated by the flashlight. Li et al. [<xref rid="B21-sensors-25-01143" ref-type="bibr">21</xref>] use a cascade of encoder&#x02013;decoder networks to estimate the shape and material appearance of a single image. Gao et al. [<xref rid="B22-sensors-25-01143" ref-type="bibr">22</xref>] proposed a deep inverse rendering framework that used [<xref rid="B18-sensors-25-01143" ref-type="bibr">18</xref>] and an arbitrary number of images to bootstrap the SVBRDF estimation, which was then refined by a subsequent network to reintroduce fine details lost in the estimation. Deschaintre et al. [<xref rid="B23-sensors-25-01143" ref-type="bibr">23</xref>] proposed to use multiple copies of a single image SVBRDF estimation neural network to work with multiple images. The output of each copy of the network was then processed through max pooling and a few convolutional layers to produce the final texture maps.</p><p>One of the main challenges and limitations of using deep learning for this task is the necessity of a large amount of training data. While several BRDF datasets are available [<xref rid="B24-sensors-25-01143" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-01143" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-01143" ref-type="bibr">26</xref>], SVBRDF datasets with a large number of samples are not available, and these are crucial for the supervised training of deep-learning models. Generating such data manually is a complex and time-consuming task. To solve this problem, Deschaintre et al. [<xref rid="B18-sensors-25-01143" ref-type="bibr">18</xref>] used data augmentation over a set of procedural SVBRDFs that were sampled and rendered under multiple lighting directions. Another approach [<xref rid="B19-sensors-25-01143" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01143" ref-type="bibr">20</xref>] used a small training set with known ground truth and a process of self-augmentation to generate additional data for training. Deschaintre et al. [<xref rid="B23-sensors-25-01143" ref-type="bibr">23</xref>] further extended their previous approach using an online renderer for data augmentation during training.</p></sec><sec id="sec2dot2-sensors-25-01143"><title>2.2. Photometric Stereo</title><p>One of the techniques that can be used to recover a surface&#x02019;s properties is photometric stereo (PS). It is a method that uses a similar (but simplified) hardware setup to the hemispherical gantries. The method allows for the recovery of local surface properties from a set of images taken from a single viewpoint illuminated by lights from different directions.</p><p>Woodham [<xref rid="B4-sensors-25-01143" ref-type="bibr">4</xref>] was the first to introduce PS, proposing an efficient method that exploited the intensity of the captured pixels. In fact, the intensity of each pixel in an image depends on the orientation of the corresponding surface patch (its normal), the reflectance of the material from which the surface is made of, and the direction and spectrum of the lighting. Although the reflectance properties are intrinsic to a surface, its relief produces shades that depend on the direction of the incident illumination. Changes in the illumination direction directly translate into changes in the appearance of a 3D surface. PS exploits this knowledge by using several identical light sources with different placements.</p><p>After the initial work of Woodham, several works were presented in the literature to exploit, extend, and adapt the concept of PS for specific applications. Most of the works took advantage of the PS technique to recover the 3D surface variations by using the estimated normal map to generate a depth map. Barsky and Petrou [<xref rid="B27-sensors-25-01143" ref-type="bibr">27</xref>], and later Plata et al. [<xref rid="B28-sensors-25-01143" ref-type="bibr">28</xref>], proposed the extension of the PS approach to use four light sources instead of three and introduced the use of color images (color PS). Previous works mainly addressed the problem of surface orientation recovery using grayscale images and were not interested in the recovery of color albedo maps. Plata et al. instead used an RGB camera, a single light source, and a turntable to acquire RGB images and recover both the surface orientations and the RGB albedo. Xie et al. [<xref rid="B29-sensors-25-01143" ref-type="bibr">29</xref>] proposed to use a PS setup with near point light sources to acquire normal maps of 3D objects with a strong difference in surface orientations. The use of near point lighting creates a nonlinear problem, as the local surface normals are coupled with their distance from the camera as well as from the light sources. They proposed a local/global mesh deformation approach to simultaneously determine the position and the orientation of a facet simultaneously, with each facet corresponding to a pixel in the image. Logothetis et al. [<xref rid="B30-sensors-25-01143" ref-type="bibr">30</xref>] used a semi-calibrated near-field PS approach to perform 3D reconstruction. They relaxed the point light source assumption by requiring only known positions rather than intensities; light attenuation maps were computed explicitly. The method can jointly estimate depth, light source brightness, (scaled) albedo, light attenuation maps, and reflectance coefficients. Liu et al. [<xref rid="B31-sensors-25-01143" ref-type="bibr">31</xref>] presented a near-light PS algorithm with circularly placed point light sources and a perspective camera. In their work, they modeled the captured scene as a 3D triangulated mesh whose vertices corresponded to the observed pixels. They used a two-stage process to first solve PS using the differential images captured by changing the light source position by a small amount along a circular path, and later refined the vertex positions using the original image formation model applied to the raw captured images. Their algorithm is sensitive to calibration errors, so they proposed an accurate light source position estimation approach using a flat panel display. Li et al. [<xref rid="B32-sensors-25-01143" ref-type="bibr">32</xref>] presented a method to capture both the 3D shape and spatially varying reflectance of isotropic materials using a multi-view PS. They combined the structure from motion technique [<xref rid="B33-sensors-25-01143" ref-type="bibr">33</xref>] to obtain precise 3D reconstruction and capture the SVBRDF by simultaneously inferring a set of basis BRDFs and their mixing weights at each surface point.</p><p>All of the above-mentioned works use complex hardware setups that require expensive specialized hardware, such as industrial equipment or laboratory prototypes. However, a few portable PS-based devices have been proposed in the literature. Gorpas et al. [<xref rid="B34-sensors-25-01143" ref-type="bibr">34</xref>] proposed a miniature PS system aimed at three-dimensional structural reconstruction of various types of fabrics. Their goal was to develop a robotic system that could navigate in unstructured environments, identify and retrieve textiles from piles or containers, and untangle and spread the textiles for some industrial process or folding. Their system consisted of a low-cost, off-the-shelf camera operating in macro mode and eight light-emitting diodes (LEDs). The device was a cylinder of about 30 mm in diameter and height. It was able to acquire a 10 &#x000d7; 10 mm textile patch with a resolution of 400 &#x000d7; 400 pixels. Only grayscale images were used to acquire 3D geometry and albedo maps. Kampouris et al. [<xref rid="B35-sensors-25-01143" ref-type="bibr">35</xref>] built a small portable device to acquire a 10 &#x000d7; 10 mm patch of textiles to capture the microstructure of the fabrics. Their system used an RGB camera and four LED light sources to acquire a normal map and grayscale albedo of the textiles. They used this recovered information to address the problem of textile classification using both handcrafted and deep-learning features using normal maps and albedo instead of plain images. Finally, Schmitt et al. [<xref rid="B36-sensors-25-01143" ref-type="bibr">36</xref>] proposed a handheld device for the joint estimation of the pose, geometry, and SVBRDF. While the device was portable, it used a complex hardware design and software pipeline. The device used a Kinect-like active depth sensor, a global shutter RGB camera, and 12 point light sources (high-power LEDs) surrounding the camera in two circles (with radii of 10 cm and 25 cm). They estimated the pose using structure from motion and created a volumetric representation of the object by fusing the acquired depth maps. Normals and albedo were initialized assuming Lambertian reflectance. Specular BRDF parameters were initialized as a uniform mixture of base materials and were later refined by optimizing (gradient-based optimization) their combination, minimizing the photometric error.</p><p><xref rid="sensors-25-01143-t001" ref-type="table">Table 1</xref> compares the main characteristics of the most relevant approaches and devices for material appearance acquisition.</p></sec></sec><sec id="sec3-sensors-25-01143"><title>3. The Proposed SVBRDF Acquisition Device</title><p>This section describes the proposed SVBRDF acquisition device to solve the problem of material appearance acquisition. Due to the wide variety of real-world materials, the scope of the device is restricted, and some assumptions are made before describing the hardware and the software pipeline for the acquisition.</p><sec id="sec3dot1-sensors-25-01143"><title>3.1. Scope and Design Choices</title><p>The goal of the proposed device is to be able to acquire the SVBRDF of a patch of a surface. Some constraints of the surface and its material are applied.</p><p>The device must be portable and usable by people who are not experts in the field. Aiming for widespread use of the device, its cost should also be limited. The materials acquired must be of a planar surface with micro surface height variations in the order of a few millimeters. While building a device for capturing the SVBRDF of generic materials and shapes is possible, it complicates the design of the hardware and makes it difficult to build a portable device. In addition to this, huge variations in the micro surface also need 3D geometry to be considered. Some restriction on the kind of materials also applies. At present, only opaque materials are taken into consideration. Transparent, translucent, as well as highly specular materials (e.g., mirrors, metals) are excluded. The patch to be taken into consideration should be of at least of size <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> cm; this allows for meaningful variations in the characteristics of the material to be included while keeping the device size limited, thus making it a portable device. This size also allows for the reproduction of the materials on displays (e.g., smartphones) matching the screen size to the real size of the acquired material patch.</p><p>Out of all of the techniques used for material appearance acquisition, our device is built upon photometric stereo, which was chosen for several reasons. It allows for building a device that is compact and made of inexpensive consumer components. It is also a static device with no moving parts that could be damaged during the transport of the device. For example, a gonioreflectometer-like or gantry setup has high hardware costs and is not suitable for a mobile device. LCD-based devices are often cheaper, but they are still not suitable for building turnkey portable devices. Statistical-based methods rely heavily on dense pattern repetition to provide accurate acquisitions, and this may not be true when capturing a small patch of the surface. Deep learning-based methods are interesting because they can provide materials&#x02019; BRDF using RGB images and do not require specialized hardware; however, they require huge computing capabilities for both training and inference. They also require a large amount of data for the training process, and such data are not easily collected or publicly available in large quantities. Like many of the other techniques, photometric stereo requires some calibration, but since the device is self-contained, this can be achieved during production rather than by the user.</p><p>The developed device uses the SVBRDF extension of the following BRDF. The Cook&#x02013;Torrance BRDF [<xref rid="B38-sensors-25-01143" ref-type="bibr">38</xref>] is used as the base reflectance representation, but a few changes are made. First, Shlick&#x02019;s approximation of the Fresnel term is adopted. Both the microfacet distribution and the shadowing and masking terms of the Cook&#x02013;Torrance model are replaced by the GGX formulation proposed by [<xref rid="B39-sensors-25-01143" ref-type="bibr">39</xref>]. This BRDF was chosen among the others since it fits the classes of materials that the device aims to capture; it is also a common formulation of the BRDF adopted by many rendering engines. By providing the material appearance in the same format, it is possible to render it in the engines without having to convert between different representations or define new shaders in the engines.</p></sec><sec id="sec3dot2-sensors-25-01143"><title>3.2. Hardware</title><p>For the photometric stereo approach, some essential components are needed: a digital camera, a variable number of light sources, and a setup that allows acquisition without the interference of external light sources.</p><p>We aim at an affordable hardware; thus, the device builds on the most suitable cheap electronics available on the consumer market and uses a custom case to house all of the components. Specifically, the device uses the Raspberry Pi Camera Module V2, SK6812 LEDs and the Raspberry Pi Zero W. (Raspberry Pi Ltd., Cambridge, UK). The complete bill for the materials and the price of each component are reported in <xref rid="sensors-25-01143-t002" ref-type="table">Table 2</xref>.</p><sec id="sec3dot2dot1-sensors-25-01143"><title>3.2.1. Digital Camera Module</title><p>Out of all the available camera modules, the Raspberry Pi Camera Module V2, <uri xlink:href="https://www.raspberrypi.com/documentation/accessories/camera.html">https://www.raspberrypi.com/documentation/accessories/camera.html</uri>, (accessed on 10 February 2025) was chosen. It is a camera sensor module that comes in the form of a small (<inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>25</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>24</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> mm) breakout board mounted with a Sony IMX219 (Raspberry Pi Ltd., Cambridge, UK) [<xref rid="B40-sensors-25-01143" ref-type="bibr">40</xref>] CMOS image sensor. This mobile-format sensor (<inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3.68</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2.76</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> mm, pixel size <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.12</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> &#x000b5;m) provides 8 MP resolution color images (<inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3280</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2464</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> px) and is equipped with a small adjustable lens with a focal length of <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3.04</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> mm (<inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>62.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000b0; horizontal field of view). The choice of this specific module was based on three different factors: First, the camera satisfies the image acquisition requirements; it can acquire an area of size <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>82</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>61</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> mm with a spatial resolution of 40 px/mm when placed 68 mm above the target surface. Second, the camera module is provided with libraries that allow for the acquisition of images using a pre-built camera pipeline as well as RAW-10 images. The ability to acquire RAW images is essential for building a custom camera pipeline for the device. Finally, the module is inexpensive (market price of USD 25) and easily interfaces with the Raspberry Pi Single Board Computers (SBCs) as well as third-party SBCs thanks to its camera serial interface (CSI).</p></sec><sec id="sec3dot2dot2-sensors-25-01143"><title>3.2.2. Light Sources</title><p>In order for the device to work properly, it is essential to be able to illuminate the surface with different light source placements. While several types of light sources are available on the market, the light emitting diode (LED) is known for its efficiency and the large number of variations (shape, power, and wavelength). The most important factors in selecting LEDs are the wavelength of the emitted light, ease of wiring, mechanical assembly, power consumption, and emission.</p><p>The selected light sources are thus pre-built rings mounting SK6812 RGBW LEDs [<xref rid="B41-sensors-25-01143" ref-type="bibr">41</xref>]. The SK6812 LED is a 5050 LED chip (<inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> mm) that comes in a few different variations; the most suitable for this application is the RGB + Cold White. The cold white variation has been chosen since it provides a white color temperature of approximately 6500 K, which is equal to the CIE standard illuminant D65 which resembles the daylight illuminant. The white channel emits <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>6</mml:mn><mml:mspace width="3.33333pt"/><mml:mo>&#x000b1;</mml:mo><mml:mspace width="3.33333pt"/><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> lumens, and the light beam angle is 120&#x000b0;. This LED also offers some advantages over other LEDs in terms of light control and wiring. The wiring diagram consists of a 5 V power supply to be provided to each LED and a single wire control bus to daisy-chain the LEDs. Since the LEDs are already provided and assembled on PCB rings, it is only necessary to wire the power supply and the control bus to the first LED of each ring. Furthermore, inexpensive pre-assembled LED rings are available in diameters ranging from 32 mm (8 LEDs) up to 112 mm (32 LEDs).</p><p>Two rings were used in the device, with 8 and 16 LEDs, respectively, for a total of 24 light sources. The choice was based on mechanical constraints; the rings should not interfere with the camera&#x02019;s field of view, and each LED must provide enough energy to each point in the area portrayed by the camera. Moreover, the use of a large number of light sources helps deal with noise and shadows in the acquired images. Exact LEDs placement is described later in this section.</p></sec><sec id="sec3dot2dot3-sensors-25-01143"><title>3.2.3. Controller Board</title><p>The final electronic component required is a controller board, which is responsible for controlling the image acquisition process. Two different approaches can be employed. The first one is to use this controller board only for the acquisition and delegate the processing to software running on a computer. The second approach involves using the controller not only for acquisition but also for image processing. The second case requires more computational capabilities. Since a computer is, in any case, needed to recover the texture maps, the device employs the first solution to also keep the hardware of the device limited in size, power consumption, and cost. Based on this reasoning and the hardware already selected for the camera module and the light sources, the Raspberry Pi Zero W, <uri xlink:href="https://www.raspberrypi.com/products/raspberry-pi-zero-w/">https://www.raspberrypi.com/products/raspberry-pi-zero-w/</uri>, (accessed on 10 February 2025) is the designated SBC to control the images acquisition process. It has a 1 GHz single-core CPU, 512 MB of RAM, a CSI camera interface, a wireless network adapter, and a general-purpose input/output (GPIO) interface to control the LEDs.</p><p>Power is supplied via the Raspberry&#x02019;s micro-USB power connector. Since the power consumption of the LEDs does not exceed the recommended maximum of 1 A for the 5 V rail, no external voltage regulator is required. A single GPIO pin is used to drive all of the LEDs; refer to the SK6812 datasheet [<xref rid="B41-sensors-25-01143" ref-type="bibr">41</xref>] for details on the control protocol. A Wi-Fi connection is used for communication and data transfer between the device and the computer.</p></sec><sec id="sec3dot2dot4-sensors-25-01143"><title>3.2.4. Device Case</title><p>A 3D-printed plastic case was designed to hold the hardware in place and block the light coming from external sources, thus providing a dark room for the material appearance acquisition. <xref rid="sensors-25-01143-f001" ref-type="fig">Figure 1</xref> shows computer-aided design (CAD) sketches and the final device. Since the housing was 3D printed using PLA, which is highly specular, the inner surface of the device was painted with a matte black color to limit the effects of reflections from the case walls in the acquired images.</p><p>The exact placement of the camera and LED rings has been determined to guarantee that a patch of material (<inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> cm in size) is acquired, with each point on the surface receiving enough light for each LED. Therefore, the camera is placed 68 mm above the surface. The two LED rings are instead placed as follows: the smaller one, which has a radius of 13 mm (center of the ring to center of the LED), is located 45 mm above the surface, forming a 75&#x000b0; angle with the surface. The larger one (33 mm radius) is instead positioned 40 mm above the surface, producing a 51&#x000b0; incident light angle. Both rings are aligned so that their center corresponds to the main view axis of the camera.</p></sec></sec><sec id="sec3dot3-sensors-25-01143"><title>3.3. Software</title><p>The material appearance acquisition software consists of two main modules: a firmware that runs on the Raspberry Pi Zero to control the LEDs and image acquisition and a processing pipeline that runs on a personal computer.</p><sec id="sec3dot3dot1-sensors-25-01143"><title>3.3.1. Onboard Firmware</title><p>Due to the limited computational power and RAM, the goal of the onboard firmware is to coordinate the image acquisition process and transfer the data to the personal computer where the processing takes place. The onboard firmware accepts commands from the main elaboration pipeline and mainly controls the device&#x02019;s LEDs and camera. This software runs on the Raspberry Pi Zero, and the commands and data are exchanged through an HTTP API using the Flask framework. The Raspberry Camera module is controlled through a customized version of the Picamera library, <uri xlink:href="https://github.com/waveform80/picamera">https://github.com/waveform80/picamera</uri>, (accessed on 10 February 2025). The LEDs are driven using the Adafruit CircuitPython NeoPixel library, <uri xlink:href="https://github.com/adafruit/Adafruit_CircuitPython_NeoPixel">https://github.com/adafruit/Adafruit_CircuitPython_NeoPixel</uri>, (accessed on 10 February 2025). The image transmission uses binary streams, while supplementary data are transferred using JavaScript object notation (JSON) documents.</p></sec><sec id="sec3dot3dot2-sensors-25-01143"><title>3.3.2. Main Software and Processing Pipeline</title><p>The software pipeline consists of three main blocks: the camera pipeline, the preprocessing, and the photometric stereo processing. The camera pipeline takes as input the RAW Bayer pattern data from the camera sensor and converts this information into an RGB image. A total of 24 images are acquired for each sample, one image for each LED. The preprocessing applies a series of operations (see <xref rid="sensors-25-01143-f002" ref-type="fig">Figure 2</xref>) to ensure consistency between the acquired images; LED shading correction and global brightness adjustment are the most important. Finally, photometric stereo estimates the material appearance from the images and produces the normal map, albedo map, and roughness map.</p><p>In more detail, the camera pipeline performs the following steps: black level adjustment, lens shading correction, demosaicing, color correction, and lens undistortion. Some of these steps require calibration; see <xref rid="sec3dot4-sensors-25-01143" ref-type="sec">Section 3.4</xref> for details. Black level adjustment is performed according to the camera sensor datasheet. Lens shading correction is applied using a pixel-wise multiplicative calibrated correction matrix. The demosaicing step uses the weighted average demosaicing approach to convert the RAW image to an RGB image. Color correction is then applied to account for sensor nonlinearity (<inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>R</mml:mi><mml:mi>G</mml:mi><mml:msup><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>R</mml:mi><mml:mi>G</mml:mi><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), RGB correction (<inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>R</mml:mi><mml:mi>G</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where M is a <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> linear color correction matrix), and gamma correction (<inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>G</mml:mi><mml:msup><mml:mi>B</mml:mi><mml:mi>&#x003b3;</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). Next, the lens radial distortion is corrected. Finally, the image is cropped to a square matching the 5 &#x000d7; 5 cm patch of the acquired tile (approximately <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2048</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2048</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels). The camera pipeline is used to acquire 24 images, using the same camera acquisition setup but a different LED for each of image.</p><p>Some image pre-processing is then applied to ensure consistency between the 24 images. LED shading correction takes into account the uneven light energy across the surface. Since the distance from the LED differs for each point on the surface and the emission lobe of the LED is not a perfect hemisphere, the more distant part of the surface is usually less illuminated than the part directly under the light source. This step corrects the image so that the entire surface has the same brightness. The correction is conducted using a multiplicative matrix, similar to the lens vignetting correction. Then, the global brightness of the remaining 23 images is adjusted to match the average global brightness of the first image. This takes into account tolerances in the power emitted by the different LEDs. The final pre-processing step creates the grayscale copies of the RGB images that will be used in the PS pipeline.</p><p>After pre-processing PS is applied to recover the albedo and normal maps. The PS equation assumes that the incoming light has the same incident direction for each point on the surface. This is usually true if the light source is a point light placed far from the surface. In this device, light sources are LEDs placed close to the acquired surface and can be approximated to quasi-point light sources. As such, the incident light direction is different for each point. To account for this, the shading function of the PS equation is rewritten as in Equation (<xref rid="FD1-sensors-25-01143" ref-type="disp-formula">1</xref>), which defines <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the intensity of each pixel of coordinates <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in the image.<disp-formula id="FD1-sensors-25-01143"><label>(1)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
<inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c1;</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> defines the albedo, and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the intensity of the incident light. The term <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> defines the incident light direction and uses a different direction vector for each pixel in the image. Finally, <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the surface normal vector. Thanks to the LED shading correction and global brightness adjustment steps, it can be assumed that the incident light intensity is equal for each pixel and for each LED, so <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Equation (<xref rid="FD1-sensors-25-01143" ref-type="disp-formula">1</xref>) can be rewritten for each pixel as Equation (<xref rid="FD2-sensors-25-01143" ref-type="disp-formula">2</xref>).<disp-formula id="FD2-sensors-25-01143"><label>(2)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>I</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:mi>L</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>L</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>I</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>G</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mspace width="2.em"/><mml:mi>where</mml:mi><mml:mspace width="4.pt"/><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
Considering all of the 24 equations (one per image) it is possible to solve the system of Equation (<xref rid="FD3-sensors-25-01143" ref-type="disp-formula">3</xref>) for G and obtain a vector for each pixel that encodes the normal direction <italic toggle="yes">N</italic> and the grayscale albedo <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:math></inline-formula> (see Equation (<xref rid="FD4-sensors-25-01143" ref-type="disp-formula">4</xref>)).<disp-formula id="FD3-sensors-25-01143"><label>(3)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mi>G</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mi>I</mml:mi><mml:mn>24</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mn>24</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mi>G</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-01143"><label>(4)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>G</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="2.em"/><mml:mi>&#x003c1;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
To obtain the color albedo, Equation (<xref rid="FD5-sensors-25-01143" ref-type="disp-formula">5</xref>) is minimized for each RGB channel separately. This is achieved by computing the value of Equation (<xref rid="FD6-sensors-25-01143" ref-type="disp-formula">6</xref>).<disp-formula id="FD5-sensors-25-01143"><label>(5)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>24</mml:mn></mml:munderover><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:msubsup><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi>N</mml:mi></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-01143"><label>(6)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>24</mml:mn></mml:munderover><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:msubsup><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi>N</mml:mi></mml:mfenced><mml:msubsup><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi>N</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>&#x021d2;</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>24</mml:mn></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi>N</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>24</mml:mn></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi>N</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>After obtaining the normal and albedo maps, it is possible to recover the roughness map. According to [<xref rid="B42-sensors-25-01143" ref-type="bibr">42</xref>], the variance of the normal map can be used as an indicator of the surface roughness. Following their intuition, here, a pipeline is defined that can provide an estimation of the roughness map from the normal map recovered through the PS. The steps of the pipelines are visible in <xref rid="sensors-25-01143-f003" ref-type="fig">Figure 3</xref>. Consider <italic toggle="yes">n</italic> as the estimated normal map where for each pixel the surface normal is defined as a vector with three components <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The first operation is the slope exaggeration (Equation (<xref rid="FD7-sensors-25-01143" ref-type="disp-formula">7</xref>)) which allows for the small surface normal perturbations to be mapped into larger changes by exaggerating the component of the normal that points away from the vertical axis. Hereafter, normals are amplified using a high-frequency emphasis filter for surface normal (Equation (<xref rid="FD8-sensors-25-01143" ref-type="disp-formula">8</xref>)) that considers a window around each pixel of the image. Those two steps are meant to boost the variability of the normal directions in preparation for the variance computation. Constant values <italic toggle="yes">s</italic>, <italic toggle="yes">m</italic>, and <italic toggle="yes">k</italic> where empirically found through experimentation with different materials and are constrained to the hardware setup of the device. The variance <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is later computed through Equation (<xref rid="FD9-sensors-25-01143" ref-type="disp-formula">9</xref>) which considers a window around each pixel.<disp-formula id="FD7-sensors-25-01143"><label>(7)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="2.em"/><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-25-01143"><label>(8)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>n</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:msubsup><mml:mi>n</mml:mi><mml:mi>j</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="2.em"/><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>23</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>23</mml:mn><mml:mspace width="3.33333pt"/><mml:mi>px</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2.5</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-01143"><label>(9)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover></mml:mstyle><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mover><mml:msup><mml:mi>n</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mover><mml:msup><mml:mi>n</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mover><mml:msup><mml:mi>n</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>z</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfenced></mml:mrow><mml:mi>m</mml:mi></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>23</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>23</mml:mn><mml:mspace width="3.33333pt"/><mml:mi>px</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
The linear roughness value <italic toggle="yes">r</italic> is then computed using Equation (<xref rid="FD10-sensors-25-01143" ref-type="disp-formula">10</xref>) to better fit the 0&#x02013;1 range.<disp-formula id="FD10-sensors-25-01143"><label>(10)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="2.em"/><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>150</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
Finally, the perceptual roughness as leveraged by rendering engines is obtained by computing the square root of the linear roughness (<inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mi>r</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p>The average execution time for material acquisition is 6 min 00 s on an Intel Core i7 6700K processor. Specifically, image acquisition and data transfer from the device to the PC takes 2 min 35 s, and pipeline execution takes the remaining 3 min 25 s. It should be noted that the current code does not exploit any optimization or high performance computing capabilities.</p></sec></sec><sec id="sec3dot4-sensors-25-01143"><title>3.4. Device Calibration</title><p>A few calibration procedures are required to build the calibration data used for some of the main pipeline steps.</p><sec id="sec3dot4dot1-sensors-25-01143"><title>3.4.1. Lens Shading Correction Calibration</title><p>In this step, we estimate a set of parameters to correct for the vignetting introduced into the image by the lens. The correction factors were computed by using a RAW Bayer image of a completely white tablet screen. To obtain such an image, a translucent white sheet of paper was placed directly over the screen since the ground sample distance (GSD) of the camera was small enough to distinguish the color filters of the LCD screen. <xref rid="sensors-25-01143-f004" ref-type="fig">Figure 4</xref> shows an example of a RAW image before and after lens shading correction. The channels of the captured RAW image are separated based on the <italic toggle="yes">rgGb</italic> Bayer pattern. For each channel, the center <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>64</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> px window is used to define the desired average brightness level <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>; average blur is then used to clear noise. The correction factors are computed as <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec3dot4dot2-sensors-25-01143"><title>3.4.2. Color Correction Calibration</title><p>This calibration estimates the parameters <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></inline-formula> used to correct the camera&#x02019;s sensor nonlinearity, RGB color, and gamma encoding. The parameters are optimized using the Nelder&#x02013;Mead simplex [<xref rid="B43-sensors-25-01143" ref-type="bibr">43</xref>] method over color data acquired from an X-Rite ColorChecker Classic Mini. Due to the size of the color checker, each square was captured separately but using the same camera parameters.</p></sec><sec id="sec3dot4dot3-sensors-25-01143"><title>3.4.3. Lens Undistortion Calibration</title><p>Due to the shallow depth of field and mechanical mount of the camera, the lens radial distortion parameters must be manually calibrated. Parameters for the Brown distortion model [<xref rid="B44-sensors-25-01143" ref-type="bibr">44</xref>] were manually estimated using a black-and-white checker board pattern.</p></sec><sec id="sec3dot4dot4-sensors-25-01143"><title>3.4.4. LED Shading Correction Calibration</title><p>Similar to lens shading correction, this is the process of estimating the correction of the shading introduced in the image by the LEDs. One of the core assumptions of PS is that the energy that lights up the material is equal for each point on the surface. When using light sources close to the surface, this is not true, and light fall-off can be observed across the images. To compensate for the light fall-off, a correction matrix was computed for each of the 24 LEDs by taking an image of a uniform white paper sheet. The acquired image is then cleared of noise using average blur. Correction factors are then computed for each RGB channel separately.</p></sec><sec id="sec3dot4dot5-sensors-25-01143"><title>3.4.5. Incident Light Direction Calibration</title><p>Given that the light sources are not point light placed at infinity, but quasi-point light placed near the surface to be acquired, the incident light direction depends on the LED placement, and it also varies along the surface. To provide the correct light direction vector for each pixel acquired by the camera, a physical calibration target with 71 mirror-like spheres was designed (see <xref rid="sensors-25-01143-f005" ref-type="fig">Figure 5</xref>). The spheres are arranged in a hexagonal pattern to allow for later interpolation. The target structure was printed using a resin-based 3D printer with an XY resolution of 47 &#x003bc;m and Z (layer height) of 10 &#x003bc;m.</p><p>The calibration pipeline executes the steps visible in <xref rid="sensors-25-01143-f006" ref-type="fig">Figure 6</xref>. An image is acquired using all the LEDs to detect positions and contours of the spheres. The spheres&#x02019; centers are also used to determine the Delaunay tessellation later used for interpolation of the light directions, the boundary of the usable region-of-interest, and the length in pixels of the diagonals of the calibration target. Knowing the real length of the target&#x02019;s diagonals and the camera&#x02019;s intrinsic parameters, it is now possible to compute the exact height of the camera from the surface.</p><p>An image for each LED is acquired and then processed through various steps. First, the reflections over the spheres are detected by finding the brightest spots. Then, the light direction is computed using the equation <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where N is the normal direction at the brightest spot on the sphere, and R is the vector of the direction reaching the camera through the focal plane. The 71 known light direction vectors are then used to build the full incident light vector map by using barycentric interpolation.</p></sec><sec id="sec3dot4dot6-sensors-25-01143"><title>3.4.6. Albedo Correction Calibration</title><p>This calibration uses the Finlayson 2015 color correction method [<xref rid="B45-sensors-25-01143" ref-type="bibr">45</xref>] to estimate the color correction to be applied to the estimated color albedo map. The correction parameters are estimated using a GretagMacbeth ColorChecker DC color chart. This chart consists of 237 color patches. The eight glossy patches are excluded, and repeated patches are considered only once. Since the chart exceeds the acquisition size of the device, the tiles are acquired and their albedo is estimated in groups of 4 (2 by 2 patches) to speed up the process. The same camera parameters are used for all of the acquisitions.</p></sec></sec></sec><sec id="sec4-sensors-25-01143"><title>4. Device Accuracy Evaluation</title><p>A common approach to evaluate PS-based methods is to use synthetic images for which the ground truth of the SVBRDF maps is known. This approach works well for assessing modifications to the PS pipeline, but it is not sufficient in our case. In addition to PS, it is necessary to evaluate the hardware assembly and its calibration as well as the camera pipeline and the pre-/post-processing steps. Therefore, a procedure is defined for evaluating the device as a whole, enabling evaluation of the various aspects involved in the material appearance acquisition.</p><sec id="sec4dot1-sensors-25-01143"><title>4.1. Evaluation Target</title><p>To evaluate the quality of the computed normal and albedo maps of the surface, a physical target with known variations in normal direction and uniform color was designed.</p><p>This tool is a square of size <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>30</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>30</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> mm and presents four different shapes with a height variation of <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.25</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> mm (see <xref rid="sensors-25-01143-f007" ref-type="fig">Figure 7</xref>a). In detail, following the clockwise rotation from the top left, we have a torus to stress the normal map at high slant angles, a flattened hemisphere to test the normal estimation at lower slant angles, a truncated cone to test constant normal angles at different heights, and a grid pattern to simulate the variations of a textile. The first three shapes are also useful to test the variations of the normals for all of the possible rotations around the vertical axis. Finally, the four shapes are placed in an area lowered by <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.25</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> mm to have their highest portion at the same height as the edges.</p><p>The evaluation target was printed using the same resin-based 3D printer already used for the light direction calibration target. The 3D-printed tool was airbrushed with a uniform light gray matte coating. The final result is visible in <xref rid="sensors-25-01143-f007" ref-type="fig">Figure 7</xref>b.</p></sec><sec id="sec4dot2-sensors-25-01143"><title>4.2. Albedo Map Evaluation</title><p>The estimated albedo map of the material is evaluated in terms of both color accuracy and color uniformity. The former evaluates the ability of the device to provide perceptually correct albedo maps, while the latter evaluates its ability to produce correct albedo maps for uniform materials but with varying normal directions.</p><sec id="sec4dot2dot1-sensors-25-01143"><title>4.2.1. Color Accuracy</title><p>Evaluation of the color accuracy of the estimated albedo map is performed using the GretagMacbeth ColorChecker DC color chart. This uses the same chart and acquisition procedure previously used for albedo correction calibration (see <xref rid="sec3dot4dot6-sensors-25-01143" ref-type="sec">Section 3.4.6</xref>).</p><p>As pointed out by [<xref rid="B46-sensors-25-01143" ref-type="bibr">46</xref>], the estimation and evaluation of the albedo map presents the challenge of collimating the absolute brightness of the estimate with that of the ground truth. The difference in global brightness is due to the acquisition setup and can be easily dealt with when performing later re-renderings; for this reason, the evaluation should not consider mere differences in global brightness. In that work, a scale-invariant MSE metric is defined to evaluate the albedo. The scale invariance is enforced by a scalar <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula>, which is optimized to minimize the error, thus taking into account the ambiguity in the absolute brightness of the scene or absolute intensity of the albedo.</p><p>While their proposal works for grayscale albedo, extending this concept to color albedo is not a trivial process. Following their reasoning, a scale-invariant version of the <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> metric is proposed. The <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B47-sensors-25-01143" ref-type="bibr">47</xref>] was defined by the International Commission on Illumination (CIE) as a way to measure the perceived visual difference between two colors in the CIELAB color space. Delta E is a metric for understanding how the human eye perceives color differences. The presented modification uses a scalar <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula> multiplied by the lightness to account for different global brightness between the estimated albedo values and the reference ones. The complete metric is reported in Equation (<xref rid="FD11-sensors-25-01143" ref-type="disp-formula">11</xref>).</p><p>Since the albedo texture encodes only the base color of the surface with no information about light temperature or intensity, evaluation is performed against the ground truth colors of the ColorChecker as acquired under an ideal E illuminant, which is an equal energy generator that provides a constant SPD in the visible spectrum.<disp-formula id="FD11-sensors-25-01143"><label>(11)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>-</mml:mi><mml:mo>&#x00394;</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:munder><mml:mo>&#x02211;</mml:mo><mml:mfenced open="(" close=")"><mml:msqrt><mml:mrow><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msqrt></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mi>&#x003b1;</mml:mi><mml:mspace width="2.em"/><mml:msub><mml:mi>k</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>k</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>k</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mspace width="1.em"/><mml:mi>default</mml:mi><mml:mspace width="4.pt"/><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mover><mml:mi>L</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mi>&#x003b1;</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mspace width="2.em"/><mml:mover><mml:mi>C</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mspace width="2.em"/><mml:msubsup><mml:mi>C</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mspace width="2.em"/><mml:msubsup><mml:mi>C</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msubsup><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msqrt><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msup><mml:mover><mml:mi>C</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mn>7</mml:mn></mml:msup><mml:mrow><mml:msup><mml:mover><mml:mi>C</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mn>7</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mn>25</mml:mn><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:msqrt></mml:mfenced><mml:mspace width="2.em"/><mml:msubsup><mml:mi>a</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msubsup><mml:mi>a</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msqrt><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msup><mml:mover><mml:mi>C</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mn>7</mml:mn></mml:msup><mml:mrow><mml:msup><mml:mover><mml:mi>C</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mn>7</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mn>25</mml:mn><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:msqrt></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi>C</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mspace width="2.em"/><mml:msubsup><mml:mi>C</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mover><mml:mi>C</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">atan2</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="4pt"/><mml:mspace width="0.277778em"/><mml:mo form="prefix">mod</mml:mo><mml:mspace width="0.277778em"/><mml:mspace width="4pt"/><mml:mn>360</mml:mn><mml:mo>&#x000b0;</mml:mo><mml:mspace width="2.em"/><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mo form="prefix">atan2</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="4pt"/><mml:mspace width="0.277778em"/><mml:mo form="prefix">mod</mml:mo><mml:mspace width="0.277778em"/><mml:mspace width="4pt"/><mml:mn>360</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="4pt"/><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x02264;</mml:mo><mml:mn>180</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:mn>360</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="4pt"/><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x0003e;</mml:mo><mml:mn>180</mml:mn><mml:mo>&#x000b0;</mml:mo><mml:mo>,</mml:mo></mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02264;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mn>360</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="4pt"/><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x0003e;</mml:mo><mml:mn>180</mml:mn><mml:mo>&#x000b0;</mml:mo><mml:mo>,</mml:mo></mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x0003e;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:msubsup><mml:mi>C</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:msqrt><mml:mo form="prefix">sin</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mover><mml:mi>H</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:mn>360</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mfenced><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="4pt"/><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x0003e;</mml:mo><mml:mn>180</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mfenced><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="4pt"/><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x02264;</mml:mo><mml:mn>180</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>T</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mn>0.17</mml:mn><mml:mo form="prefix">cos</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mover><mml:mi>H</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mn>30</mml:mn><mml:mo>&#x000b0;</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.24</mml:mn><mml:mo form="prefix">cos</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mover><mml:mi>H</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.32</mml:mn><mml:mo form="prefix">cos</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:msup><mml:mover><mml:mi>H</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mn>6</mml:mn><mml:mo>&#x000b0;</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>0.2</mml:mn><mml:mo form="prefix">cos</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:msup><mml:mover><mml:mi>H</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mn>63</mml:mn><mml:mo>&#x000b0;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>S</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>0.015</mml:mn><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mover><mml:mi>L</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mn>50</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msqrt><mml:mrow><mml:mn>20</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mover><mml:mi>L</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mn>50</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle><mml:mspace width="2.em"/><mml:msub><mml:mi>S</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.045</mml:mn><mml:msup><mml:mover><mml:mi>C</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mspace width="2.em"/><mml:msub><mml:mi>S</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.015</mml:mn><mml:msup><mml:mover><mml:mi>C</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>T</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>R</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msup><mml:mrow><mml:msup><mml:mover><mml:mi>C</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mn>7</mml:mn></mml:msup><mml:mrow><mml:msup><mml:mrow><mml:msup><mml:mover><mml:mi>C</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mn>7</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mn>25</mml:mn><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:msqrt><mml:mo form="prefix">sin</mml:mo><mml:mfenced separators="" open="{" close="}"><mml:mn>60</mml:mn><mml:mo>&#x000b0;</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mi>exp</mml:mi><mml:mfenced separators="" open="[" close="]"><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mover><mml:mi>H</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mn>275</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow><mml:mn>25</mml:mn></mml:mfrac></mml:mstyle></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Albedo color evaluation is performed using the <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> following the leave-one-out cross-validation procedure on the acquired color chart data. The obtained average <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is equal to 2.89, indicating a perceptual error that is limited but still observable by the human eye. When estimating the correction parameters using all the available patches simultaneously, the error is about 2.85. Before the albedo color correction, the average <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is equal to 3.62. <xref rid="sensors-25-01143-f008" ref-type="fig">Figure 8</xref> shows the comparison between the desired ground truth albedo colors and the obtained albedo after correction. Note that the larger errors are evenly distributed across the different shades.</p></sec><sec id="sec4dot2dot2-sensors-25-01143"><title>4.2.2. Color Uniformity</title><p>The color uniformity evaluation aims to test the ability of the device to correctly estimate the albedo as the normal direction varies. This evaluation uses the tool introduced in <xref rid="sec4dot1-sensors-25-01143" ref-type="sec">Section 4.1</xref>. To perform this evaluation under ideal conditions, the tool used for evaluation should present a Lambertian surface. Since Lambertian surfaces cannot be easily created in the real world, the target uses a uniform matte gray paint that approximates their characteristics.</p><p>The evaluation of the color uniformity of the albedo uses the <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> metric. This metric is based on <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> but does not consider the lightness channel in the evaluation. Furthermore, since it was not possible to characterize the reflective properties of the painting used, and since this evaluation aims only to measure the uniformity of the albedo, not its colorimetric accuracy (which is evaluated instead in the previous paragraph), the evaluation is made against the average color of the acquired albedo map. The evaluation presents a <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1.89</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, which indicates that the device can correctly acquire the albedo of a material with a limited error even with changes in the normal directions.</p></sec></sec><sec id="sec4dot3-sensors-25-01143"><title>4.3. Normal Map Evaluation</title><p>The quality of the computed normal map is evaluated using the tool described in <xref rid="sec4dot1-sensors-25-01143" ref-type="sec">Section 4.1</xref>. The SVBRDF of the target is acquired by running the software pipeline. The obtained normal map is evaluated following the pipeline defined in <xref rid="sensors-25-01143-f009" ref-type="fig">Figure 9</xref>. First, the location of the target is detected by creating a mask using thresholding, flood fill, erode, and dilate operations. Edges of the target are then detected using the Canny edge detector, and a square is fitted on the edges using the algorithm defined by [<xref rid="B48-sensors-25-01143" ref-type="bibr">48</xref>]. Once target detection is complete, an affine 2D transformation is estimated and applied to account for the rotation of the target around the vertical axis with respect to the camera. The corrected image is then cropped to contain only pixels belonging to the target. Since the contents of the image are 3D vectors, the same rotation correction applied in the image space is also applied to the vectors with respect to the vertical axis Z. Finally, the angular error in degrees, defined in Equation (<xref rid="FD12-sensors-25-01143" ref-type="disp-formula">12</xref>), is computed against the ground truth normal map (<inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) of the target.<disp-formula id="FD12-sensors-25-01143"><label>(12)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>E</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix">arccos</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>&#x022ba;</mml:mo></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The result of the normal map evaluation shows a mean angular error of <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>9.5</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and a median of <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>7.42</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (min = 0.0&#x000b0;, max = 96.98&#x000b0;, 1st quartile = 7.27&#x000b0;, 3rd quartile = 10.40&#x000b0;). By taking a closer look at the error heatmap in <xref rid="sensors-25-01143-f009" ref-type="fig">Figure 9</xref>, it is clear that the error is higher around the edges of the torus and the grid pattern where the slant is greater.</p></sec><sec id="sec4dot4-sensors-25-01143"><title>4.4. Roughness Map Evaluation</title><p>To the best of our knowledge, no method for quantitative evaluation of the quality of roughness texture maps of the real surface has been defined in the literature. Existing evaluation methods are thought to quantize the roughness of real surfaces for mechanical analysis purposes. While this is useful for some tasks, it is not sufficient to evaluate a roughness map since it encodes different information.</p><p>Metrics such as <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> [<xref rid="B49-sensors-25-01143" ref-type="bibr">49</xref>] measure physical properties such as the arithmetic mean roughness value of the profile deviations from the mean line of the roughness as a single real number for the entire surface. Instead, the roughness texture map describes the micro-facet distribution of each pixel separately.</p><p>The main limitation of the current process of roughness map estimation is that it uses various experimentally found values (i.e., window size and scaling factors). While fixed values provide acceptable results, adjustments to those values may be needed for some kinds of materials. In general, the pipeline tends to provide too glossy roughness, but the problem is partly mitigated by the presence of the normal map. This is partly due to the map being derived for each pixel using information from an area around it. Since the roughness map encodes information about sub-pixel micro-surface geometry, additional spatial resolution could be used to improve the roughness estimation. Examples of roughness textures generated by this method are shown in <xref rid="sensors-25-01143-f010" ref-type="fig">Figure 10</xref>.</p></sec><sec id="sec4dot5-sensors-25-01143"><title>4.5. Visual Results</title><p>In <xref rid="sensors-25-01143-f011" ref-type="fig">Figure 11</xref>, we provide some examples of the textures acquired by the proposed device as well as re-renderings where samples are illuminated by a single strong point light. As is visible in the figure, the device captures the fine details of the three texture maps (albedo, normals, roughness). The best results are achieved on the acquisition of surfaces that present limited specular reflections such as the samples of textile, cardboard, stone, and cork. Enough details are still captured in the plastic and wood samples, which present a reflective surface. However, the printed metal sample presents some artifacts in the texture maps. In particular, it is visible how the specular reflections generated by each LED generate a normal map that represents the flat surface as a hemisphere (normals closer to the edges are slightly pointing in the direction of the edge). Given that the roughness map is generated from the normals, it highlights the presence of noise in the acquisition, and it makes visible the artifacts generated by the LEDs of the device. It is also visible in the re-rendering that the incorrect roughness does not induce specular reflections on the metal sample.</p><p>It is also worth noting that the device can correctly acquire and generate texture maps for surfaces that present strong height and normal direction variations. This is most evident in the cork and stone samples, where the former has height variations of about 1 mm and the latter of about 4 mm.</p></sec></sec><sec id="sec5-sensors-25-01143"><title>5. Application to Textiles Virtual Catalogs</title><p>This section discusses the use of the acquisition device in the context of a virtual catalog of textiles. The goal is to provide the user with an intuitive way to check the appearance of textiles under different lighting conditions while also being user-friendly and easy to use. To achieve this, a mobile application that uses a render engine to simulate light conditions is provided. These solutions build upon the SVBRDF maps estimated by our device.</p><sec id="sec5dot1-sensors-25-01143"><title>5.1. Application Workflow</title><p><xref rid="sensors-25-01143-f012" ref-type="fig">Figure 12</xref> shows the workflow of the virtual catalog application. The front-end is a mobile application, while its back-end is implemented as a cloud-based web service. The following subsections provide further details about the various components.</p><sec id="sec5dot1dot1-sensors-25-01143"><title>5.1.1. Simulating the Light Interaction</title><p>The core functionality on which the virtual catalog builds is the ability to show plausible images of textiles under different light conditions. This can be achieved using a render engine. In a render engine, it is possible to set up a virtual 3D scene with the textile, a camera, and a light source, as shown in <xref rid="sensors-25-01143-f013" ref-type="fig">Figure 13</xref>a. In the real world, we usually check the appearance of a textile changing its orientation with respect to a light source, be it artificial or natural sunlight. Moreover, to replicate the behavior the user is used to, we should think of the screen of the device used to show the catalog as the fabric itself. Thus, by rotating the fabric (i.e., the device), the user should be able to change the angle of incident light rays (<xref rid="sensors-25-01143-f013" ref-type="fig">Figure 13</xref>b). This behavior can be achieved thanks to the gyroscope sensor available onboard modern smartphones and tablets.</p><p>In the 3D virtual scene, it makes more sense to change the position and orientation of the light source instead of moving the mesh used for rendering and the camera. Using a sunlight type of light source that provides uniform illumination across the whole scene, it is not necessary to change its position but only the orientation of the light rays. This results in the setup shown in <xref rid="sensors-25-01143-f013" ref-type="fig">Figure 13</xref>c, where the light is rotated at the same angle as the surface in <xref rid="sensors-25-01143-f013" ref-type="fig">Figure 13</xref>b but in the opposite direction.</p><p>The rendering of the textiles uses the Cook&#x02013;Torrance-based BRDF shader model using the albedo, normal, and roughness maps provided by the acquisition device. Since the current device does not generate the specular map, the rendering uses the value 0.5, which provides a good balance between different kinds of textiles.</p></sec><sec id="sec5dot1dot2-sensors-25-01143"><title>5.1.2. User Interface</title><p>The user interface for the virtual catalog of textiles has been built as a mobile application for smartphones and tablets. The application was created using the Unity game engine [<xref rid="B50-sensors-25-01143" ref-type="bibr">50</xref>]. Although this application is not a videogame, Unity presents features that make it possible to handle real-time rendering, data loading, and user interaction on a mobile device in a single framework. It also makes it easy to deploy the application on different mobile platforms (i.e., Android, iOS).</p><p>As is visible in <xref rid="sensors-25-01143-f014" ref-type="fig">Figure 14</xref>, the application presents a tile of the textile as the main content. The size of the tile on the screen is set to match the real size of the acquisition.</p><p>A bottom bar with sub-panels allows the user to (from left to right) reset the tile position, customize the light&#x02019;s intensity and temperature, and change the textile by choosing between previews of available textiles. Using a color picker for the light color was also investigated in the preliminary designs of the application. While it allows more freedom in the choice of the illuminant, it is also difficult for the user to make slight changes to it. On the other hand, the color temperature is a familiar concept to users and allows them to adjust the illuminant color covering the majority of real-world scenarios. By default, the temperature is set to a D65 to mimic the daylight illuminant.</p><p>The application supports touch navigation. Specifically, it is possible to resize the textile by using a pinch and translating it by using a single-finger swipe. The incident light direction is instead computed in real-time and adjusted based on the procedure described in <xref rid="sec5dot1dot1-sensors-25-01143" ref-type="sec">Section 5.1.1</xref>. By changing the rotation of the device, the user can adjust the position of the sunlight providing light to the virtual scene and, thus, the final rendering of the textile. To achieve this behavior, the application uses gyroscope data to rotate the sunlight in the 3D scene. By default, the light direction is vertical above the textile, disregarding the device rotation. This initial positioning acts as a 0&#x000b0; rotation position to account for later changes in the device rotation. A 3D arrow pointer in the lower-right corner of the screen shows the current incident light direction. The user can force a new 0 position by touching such an arrow.</p></sec></sec><sec id="sec5dot2-sensors-25-01143"><title>5.2. User Evaluation</title><p>A standard usability test [<xref rid="B51-sensors-25-01143" ref-type="bibr">51</xref>] with a panel of users was used to evaluate the user experience of the application. A total of 15 subjects of different ages (25&#x02013;85), expertise, and educational backgrounds were selected. The experiment was conducted using a 5.5-inch Android smartphone. The same room with controlled lighting was used for all the participants. Before starting the usability test, the application and its scope were briefly described to the users. They were then observed using the application to conduct a session by consulting different textiles and their appearance changes under different light orientations, intensities, and temperatures. The actual textile for each of the samples available in the app was also provided to them to allow for direct comparison between the real and the virtual. No time limit was imposed. At the end of the test session, each user evaluated their experience by filling in a questionnaire based on the standard System Usability Scale (SUS) developed by John Brooke [<xref rid="B52-sensors-25-01143" ref-type="bibr">52</xref>]. In order to gain more insights into the application, users were also asked to rate (from 1 to 5) the quality of the rendered textiles, the usefulness of the light interaction simulation, the usefulness of the light intensity control, the usefulness of the light temperature control, and their interest in using this application if it were made publicly available. In addition, users ranked various textiles according to the fidelity of the rendering with respect to the real textile; the same rank for multiple textiles was not allowed. The previews of samples used are visible in <xref rid="sensors-25-01143-f015" ref-type="fig">Figure 15</xref>. Furthermore, users were asked to explain their rationale behind the ordering chosen for the textiles. Finally, some free comments from the users were also collected.</p><p>Results of the SUS questionnaire are summarized in <xref rid="sensors-25-01143-t003" ref-type="table">Table 3</xref>. Users rated the application very positively. It was considered easy and simple to use without any prior knowledge or experience. By applying the standard procedure [<xref rid="B52-sensors-25-01143" ref-type="bibr">52</xref>], the application obtained an overall SUS score of 78 out of 100, which is considered above average. The average score is set to 68 from a study on 500 systems, as described by [<xref rid="B53-sensors-25-01143" ref-type="bibr">53</xref>]. <xref rid="sensors-25-01143-t004" ref-type="table">Table 4</xref> shows the scores of the five additional questions specific to the textiles application. The quality of rendered textiles was highly appreciated, with a score of 4.0 out of 5. Users found the main feature of the application useful, rating its ability to simulate light interactions with a score of 4.6.</p><p>In general, users&#x02019; opinions are positive. The majority of them appreciated the features of the application. Users also commented on the usefulness of light interaction simulation on the textiles and how this helps to provide a more complete look and feel on a display as opposed to static images. Finally, the light controls (intensity and temperature) were appreciated. However, some users found the light intensity control less necessary than the temperature control. The ranking of the different textiles (<xref rid="sensors-25-01143-f016" ref-type="fig">Figure 16</xref>) shows that there is a clear opinion on which textiles are perceived as the worst and the best. However, there is no strong preference for textiles that place in the mid-field. In detail, the lowest-ranked textile samples are brown alcantara (7.3) and sparkling cotton (8.2). Their poor placement is due to missing or unsatisfactory specular reflections. The best samples are the green nylon and the yellow damask cotton (2.6) closely followed by the beige nylon (3.0). The two nylon samples (green and beige) present the same texture and material, yet the green one was consistently evaluated slightly better than the beige sample. The other samples rank close together with ranks of 5.0 for the white jute, 5.1 for the printed nylon, 5.5 for the blue jeans, and 5.7 for the green cotton. The jute sample was judged by some users to provide a plastic-looking feel. The majority of the users noticed and lamented the absence of specular reflections on the sparkling threads in the cyan textile.</p><p>Users also pointed out some problems with the application. The main concern was the presence of reflections on the screen of the smartphone when placed under a strong light source. In this scenario, the reflexes have a huge impact on the usefulness of the application since they prevent the correct perception of the rendered textile. To solve this problem, one user suggested changing the way he interacts with the application to change the incident light direction.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-01143"><title>6. Conclusions</title><p>In this paper, we demonstrated the feasibility of building a low-cost, portable device for material appearance acquisition of various surfaces. Thanks to the photometric stereo technique, it has been possible to design and develop a compact device able to acquire the material properties of a limited surface area. New hardware design, software pipeline, calibration, and evaluation procedures have been defined. The device is based on consumer-grade electronics making its overall cost close to EUR 80. Quantitative and qualitative evaluations on acquired texture maps validated the system&#x02019;s ability to build a correct spatially varying representation of materials with Lambertian-like reflectivity. In addition, visual results showed the capacity of the device to acquire material representations of surfaces that present non-Lambertian reflectivities, such as wood and plastics. Our evaluation of the device capabilities demonstrates that the device can be effectively exploited in Industry 4.0 applications. We also assessed this in a mobile application for the virtual catalog of textiles. The application proved to be useful in allowing people to remotely check the appearance of textiles without having the real fabric in their hands. The usability study confirmed this outcome, with users providing generally positive comments.</p><p>The current SVBRDF capture device presents some known limitations. The device has problems in the acquisition of very dark-shaded surfaces. This is due to the hardware lighting and camera setup. On such surfaces, the signal-to-noise ratio (SNR) is not sufficient to clear the noise and estimate correct normal directions using PS. This problem could be mitigated by improving the lighting of the surface with more powerful LEDs. The size of the rendered tile is limited to the size of the acquired material patch. Future work could improve this by designing a device that is able to acquire a larger patch of textiles or by generating tileable textures. At the moment, our device captures 24 images, taking about two minutes to transfer them to the server. The transfer and computational time can be reduced by optimizing the number of lights used with respect to the accuracy of the acquisition. This will also reduce the power consumption of the device. Moreover, the lights used are all identical. The device can be improved by incorporating different lights, effectively performing a multispectral acquisition of the surfaces.</p><p>Moreover, the estimation of the roughness map could be improved. Since the roughness map encodes information that regards sub-pixel micro-surface geometry, additional spatial resolution could be used to garner more insights into the micro-surface geometry and improve the estimation. Since no method for quantitative evaluation of the quality of roughness maps of real surfaces exists, defining one could be useful for the community. Providing a mapping between the physical roughness metrics and the roughness texture map could be useful to enable the evaluation. Finally, the performance and usability of the device may be further assessed in different use cases and various environmental conditions.</p></sec></body><back><ack><title>Acknowledgments</title><p>We sincerely thank Alain Tr&#x000e8;meau for his valuable and constructive suggestions during the planning and development of part of this research work.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, D.M., S.B. and G.C.; methodology, D.M., S.B. and G.C.; software, D.M.; validation, D.M.; investigation, D.M., S.B. and G.C.; writing&#x02014;original draft preparation, D.M.; writing&#x02014;review and editing, D.M., S.B. and G.C.; visualization, D.M., S.B. and G.C.; supervision, S.B. and G.C.; funding acquisition, S.B. and G.C. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Samples of acquired data shown in this article will be made available by the authors on request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">BRDF</td><td align="left" valign="middle" rowspan="1" colspan="1">Bidirectional reflectance distribution function</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPIO</td><td align="left" valign="middle" rowspan="1" colspan="1">General-purpose input/output</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LED</td><td align="left" valign="middle" rowspan="1" colspan="1">Light-emitting diode</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PS</td><td align="left" valign="middle" rowspan="1" colspan="1">Photometric stereo</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SVBRDF</td><td align="left" valign="middle" rowspan="1" colspan="1">Spatially varying bidirectional reflectance distribution function</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-01143"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nicodemus</surname><given-names>F.E.</given-names></name>
</person-group><article-title>Directional reflectance and emissivity of an opaque surface</article-title><source>Appl. Opt.</source><year>1965</year><volume>4</volume><fpage>767</fpage><lpage>775</lpage><pub-id pub-id-type="doi">10.1364/AO.4.000767</pub-id></element-citation></ref><ref id="B2-sensors-25-01143"><label>2.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Nicodemus</surname><given-names>F.E.</given-names></name>
</person-group><source>Geometrical Considerations and Nomenclature for Reflectance</source><publisher-name>US Department of Commerce, National Bureau of Standards</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>1977</year><volume>Volume 160</volume></element-citation></ref><ref id="B3-sensors-25-01143"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Marschner</surname><given-names>S.R.</given-names></name>
<name><surname>Westin</surname><given-names>S.H.</given-names></name>
<name><surname>Lafortune</surname><given-names>E.P.</given-names></name>
<name><surname>Torrance</surname><given-names>K.E.</given-names></name>
<name><surname>Greenberg</surname><given-names>D.P.</given-names></name>
</person-group><article-title>Image-based BRDF measurement including human skin</article-title><source>Proceedings of the Eurographics Workshop on Rendering Techniques</source><conf-loc>Granada, Spain</conf-loc><conf-date>21&#x02013;23 June 1999</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Vienna, Austria</publisher-loc><year>1999</year><fpage>131</fpage><lpage>144</lpage></element-citation></ref><ref id="B4-sensors-25-01143"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Woodham</surname><given-names>R.J.</given-names></name>
</person-group><article-title>Photometric method for determining surface orientation from multiple images</article-title><source>Opt. Eng.</source><year>1980</year><volume>19</volume><fpage>139</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1117/12.7972479</pub-id></element-citation></ref><ref id="B5-sensors-25-01143"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ward</surname><given-names>G.J.</given-names></name>
</person-group><article-title>Measuring and modeling anisotropic reflection</article-title><source>Proceedings of the 19th Annual Conference on Computer Graphics and Interactive Techniques</source><conf-loc>Chicao, IL, USA</conf-loc><conf-date>26&#x02013;31 July 1992</conf-date><fpage>265</fpage><lpage>272</lpage></element-citation></ref><ref id="B6-sensors-25-01143"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Torrance</surname><given-names>K.E.</given-names></name>
<name><surname>Sparrow</surname><given-names>E.M.</given-names></name>
</person-group><article-title>Off-specular peaks in the directional distribution of reflected thermal radiation</article-title><source>J. Heat Transf.</source><year>1966</year><volume>88</volume><fpage>223</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1115/1.3691519</pub-id></element-citation></ref><ref id="B7-sensors-25-01143"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ngan</surname><given-names>A.</given-names></name>
<name><surname>Durand</surname><given-names>F.</given-names></name>
<name><surname>Matusik</surname><given-names>W.</given-names></name>
</person-group><article-title>Experimental Analysis of BRDF Models</article-title><source>Render. Tech.</source><year>2005</year><volume>2005</volume><fpage>2</fpage></element-citation></ref><ref id="B8-sensors-25-01143"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Naik</surname><given-names>N.</given-names></name>
<name><surname>Zhao</surname><given-names>S.</given-names></name>
<name><surname>Velten</surname><given-names>A.</given-names></name>
<name><surname>Raskar</surname><given-names>R.</given-names></name>
<name><surname>Bala</surname><given-names>K.</given-names></name>
</person-group><article-title>Single view reflectance capture using multiplexed scattering and time-of-flight imaging</article-title><source>Proceedings of the 2011 SIGGRAPH Asia Conference</source><conf-loc>New York, NY, USA</conf-loc><conf-date>12&#x02013;15 December 2011</conf-date><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="B9-sensors-25-01143"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mukaigawa</surname><given-names>Y.</given-names></name>
<name><surname>Sumino</surname><given-names>K.</given-names></name>
<name><surname>Yagi</surname><given-names>Y.</given-names></name>
</person-group><article-title>Multiplexed illumination for measuring BRDF using an ellipsoidal mirror and a projector</article-title><source>Proceedings of the Asian Conference on Computer Vision</source><conf-loc>Tokyo, Japan</conf-loc><conf-date>18&#x02013;22 November 2007</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2007</year><fpage>246</fpage><lpage>257</lpage></element-citation></ref><ref id="B10-sensors-25-01143"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ghosh</surname><given-names>A.</given-names></name>
<name><surname>Heidrich</surname><given-names>W.</given-names></name>
<name><surname>Achutha</surname><given-names>S.</given-names></name>
<name><surname>O&#x02019;Toole</surname><given-names>M.</given-names></name>
</person-group><article-title>A basis illumination approach to BRDF measurement</article-title><source>Int. J. Comput. Vis.</source><year>2010</year><volume>90</volume><fpage>183</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1007/s11263-008-0151-7</pub-id></element-citation></ref><ref id="B11-sensors-25-01143"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rump</surname><given-names>M.</given-names></name>
<name><surname>M&#x000fc;ller</surname><given-names>G.</given-names></name>
<name><surname>Sarlette</surname><given-names>R.</given-names></name>
<name><surname>Koch</surname><given-names>D.</given-names></name>
<name><surname>Klein</surname><given-names>R.</given-names></name>
</person-group><article-title>Photo-realistic rendering of metallic car paint from image-based measurements</article-title><source>Comput. Graph. Forum</source><year>2008</year><volume>27</volume><fpage>527</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8659.2008.01150.x</pub-id></element-citation></ref><ref id="B12-sensors-25-01143"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ghosh</surname><given-names>A.</given-names></name>
<name><surname>Chen</surname><given-names>T.</given-names></name>
<name><surname>Peers</surname><given-names>P.</given-names></name>
<name><surname>Wilson</surname><given-names>C.A.</given-names></name>
<name><surname>Debevec</surname><given-names>P.</given-names></name>
</person-group><article-title>Estimating specular roughness and anisotropy from second order spherical gradient illumination</article-title><source>Comput. Graph. Forum</source><year>2009</year><volume>28</volume><fpage>1161</fpage><lpage>1170</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8659.2009.01493.x</pub-id></element-citation></ref><ref id="B13-sensors-25-01143"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Francken</surname><given-names>Y.</given-names></name>
<name><surname>Cuypers</surname><given-names>T.</given-names></name>
<name><surname>Mertens</surname><given-names>T.</given-names></name>
<name><surname>Gielis</surname><given-names>J.</given-names></name>
<name><surname>Bekaert</surname><given-names>P.</given-names></name>
</person-group><article-title>High quality mesostructure acquisition using specularities</article-title><source>Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>23&#x02013;28 June 2008</conf-date><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="B14-sensors-25-01143"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aittala</surname><given-names>M.</given-names></name>
<name><surname>Weyrich</surname><given-names>T.</given-names></name>
<name><surname>Lehtinen</surname><given-names>J.</given-names></name>
</person-group><article-title>Practical SVBRDF capture in the frequency domain</article-title><source>ACM Trans. Graph.</source><year>2013</year><volume>32</volume><fpage>110</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1145/2461912.2461978</pub-id></element-citation></ref><ref id="B15-sensors-25-01143"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>C.P.</given-names></name>
<name><surname>Snavely</surname><given-names>N.</given-names></name>
<name><surname>Marschner</surname><given-names>S.</given-names></name>
</person-group><article-title>Estimating dual-scale properties of glossy surfaces from step-edge lighting</article-title><source>Proceedings of the 2011 SIGGRAPH Asia Conference</source><conf-loc>New York, NY, USA</conf-loc><conf-date>12&#x02013;15 December 2011</conf-date><fpage>1</fpage><lpage>12</lpage></element-citation></ref><ref id="B16-sensors-25-01143"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Riviere</surname><given-names>J.</given-names></name>
<name><surname>Peers</surname><given-names>P.</given-names></name>
<name><surname>Ghosh</surname><given-names>A.</given-names></name>
</person-group><article-title>Mobile surface reflectometry</article-title><source>Proceedings of the ACM SIGGRAPH 2014 Posters</source><conf-loc>Shenzhen, China</conf-loc><conf-date>3&#x02013;6 December 2014</conf-date><publisher-name>Association for Computing Machinery</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2014</year><fpage>1</fpage></element-citation></ref><ref id="B17-sensors-25-01143"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aittala</surname><given-names>M.</given-names></name>
<name><surname>Weyrich</surname><given-names>T.</given-names></name>
<name><surname>Lehtinen</surname><given-names>J.</given-names></name>
</person-group><article-title>Two-shot SVBRDF capture for stationary materials</article-title><source>ACM Trans. Graph.</source><year>2015</year><volume>34</volume><fpage>110</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1145/2766967</pub-id></element-citation></ref><ref id="B18-sensors-25-01143"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Deschaintre</surname><given-names>V.</given-names></name>
<name><surname>Aittala</surname><given-names>M.</given-names></name>
<name><surname>Durand</surname><given-names>F.</given-names></name>
<name><surname>Drettakis</surname><given-names>G.</given-names></name>
<name><surname>Bousseau</surname><given-names>A.</given-names></name>
</person-group><article-title>Single-image svbrdf capture with a rendering-aware deep network</article-title><source>ACM Trans. Graph. (ToG)</source><year>2018</year><volume>37</volume><fpage>128</fpage><pub-id pub-id-type="doi">10.1145/3197517.3201378</pub-id></element-citation></ref><ref id="B19-sensors-25-01143"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Dong</surname><given-names>Y.</given-names></name>
<name><surname>Peers</surname><given-names>P.</given-names></name>
<name><surname>Tong</surname><given-names>X.</given-names></name>
</person-group><article-title>Modeling surface appearance from a single photograph using self-augmented convolutional neural networks</article-title><source>ACM Trans. Graph. (ToG)</source><year>2017</year><volume>36</volume><fpage>45</fpage><pub-id pub-id-type="doi">10.1145/3072959.3073641</pub-id></element-citation></ref><ref id="B20-sensors-25-01143"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ye</surname><given-names>W.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Dong</surname><given-names>Y.</given-names></name>
<name><surname>Peers</surname><given-names>P.</given-names></name>
<name><surname>Tong</surname><given-names>X.</given-names></name>
</person-group><article-title>Single image surface appearance modeling with self-augmented cnns and inexact supervision</article-title><source>Comput. Graph. Forum</source><year>2018</year><volume>37</volume><fpage>201</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1111/cgf.13560</pub-id></element-citation></ref><ref id="B21-sensors-25-01143"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Xu</surname><given-names>Z.</given-names></name>
<name><surname>Ramamoorthi</surname><given-names>R.</given-names></name>
<name><surname>Sunkavalli</surname><given-names>K.</given-names></name>
<name><surname>Chandraker</surname><given-names>M.</given-names></name>
</person-group><article-title>Learning to reconstruct shape and spatially-varying reflectance from a single image</article-title><source>ACM Trans. Graph. (ToG)</source><year>2018</year><volume>37</volume><fpage>269</fpage><pub-id pub-id-type="doi">10.1145/3272127.3275055</pub-id></element-citation></ref><ref id="B22-sensors-25-01143"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gao</surname><given-names>D.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Dong</surname><given-names>Y.</given-names></name>
<name><surname>Peers</surname><given-names>P.</given-names></name>
<name><surname>Xu</surname><given-names>K.</given-names></name>
<name><surname>Tong</surname><given-names>X.</given-names></name>
</person-group><article-title>Deep inverse rendering for high-resolution SVBRDF estimation from an arbitrary number of images</article-title><source>ACM Trans. Graph.</source><year>2019</year><volume>38</volume><fpage>134</fpage><pub-id pub-id-type="doi">10.1145/3306346.3323042</pub-id></element-citation></ref><ref id="B23-sensors-25-01143"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Deschaintre</surname><given-names>V.</given-names></name>
<name><surname>Aittala</surname><given-names>M.</given-names></name>
<name><surname>Durand</surname><given-names>F.</given-names></name>
<name><surname>Drettakis</surname><given-names>G.</given-names></name>
<name><surname>Bousseau</surname><given-names>A.</given-names></name>
</person-group><article-title>Flexible svbrdf capture with a multi-image deep network</article-title><source>Comput. Graph. Forum</source><year>2019</year><volume>38</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1111/cgf.13765</pub-id></element-citation></ref><ref id="B24-sensors-25-01143"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Matusik</surname><given-names>W.</given-names></name>
</person-group><article-title>A Data-Driven Reflectance Model</article-title><source>Ph.D. Thesis</source><publisher-name>Massachusetts Institute of Technology</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2003</year></element-citation></ref><ref id="B25-sensors-25-01143"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Filip</surname><given-names>J.</given-names></name>
<name><surname>V&#x000e1;vra</surname><given-names>R.</given-names></name>
</person-group><article-title>Template-based sampling of anisotropic BRDFs</article-title><source>Comput. Graph. Forum</source><year>2014</year><volume>33</volume><fpage>91</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1111/cgf.12477</pub-id></element-citation></ref><ref id="B26-sensors-25-01143"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dupuy</surname><given-names>J.</given-names></name>
<name><surname>Jakob</surname><given-names>W.</given-names></name>
</person-group><article-title>An adaptive parameterization for efficient material acquisition and rendering</article-title><source>ACM Trans. Graph. (ToG)</source><year>2018</year><volume>37</volume><fpage>274</fpage><pub-id pub-id-type="doi">10.1145/3272127.3275059</pub-id></element-citation></ref><ref id="B27-sensors-25-01143"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Barsky</surname><given-names>S.</given-names></name>
<name><surname>Petrou</surname><given-names>M.</given-names></name>
</person-group><article-title>The 4-source photometric stereo technique for three-dimensional surfaces in the presence of highlights and shadows</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2003</year><volume>25</volume><fpage>1239</fpage><lpage>1252</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2003.1233898</pub-id></element-citation></ref><ref id="B28-sensors-25-01143"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Plata</surname><given-names>C.</given-names></name>
<name><surname>Nieves</surname><given-names>J.L.</given-names></name>
<name><surname>Valero</surname><given-names>E.M.</given-names></name>
<name><surname>Romero</surname><given-names>J.</given-names></name>
</person-group><article-title>Trichromatic red-green-blue camera used for the recovery of albedo and reflectance of rough-textured surfaces under different illumination conditions</article-title><source>Appl. Opt.</source><year>2009</year><volume>48</volume><fpage>3643</fpage><lpage>3653</lpage><pub-id pub-id-type="doi">10.1364/AO.48.003643</pub-id><pub-id pub-id-type="pmid">19571919</pub-id>
</element-citation></ref><ref id="B29-sensors-25-01143"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xie</surname><given-names>W.</given-names></name>
<name><surname>Dai</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>C.C.</given-names></name>
</person-group><article-title>Photometric stereo with near point lighting: A solution by mesh deformation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#x02013;12 June 2015</conf-date><fpage>4585</fpage><lpage>4593</lpage></element-citation></ref><ref id="B30-sensors-25-01143"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Logothetis</surname><given-names>F.</given-names></name>
<name><surname>Mecca</surname><given-names>R.</given-names></name>
<name><surname>Cipolla</surname><given-names>R.</given-names></name>
</person-group><article-title>Semi-calibrated near field photometric stereo</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;26 July 2017</conf-date><fpage>941</fpage><lpage>950</lpage></element-citation></ref><ref id="B31-sensors-25-01143"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>C.</given-names></name>
<name><surname>Narasimhan</surname><given-names>S.G.</given-names></name>
<name><surname>Dubrawski</surname><given-names>A.W.</given-names></name>
</person-group><article-title>Near-light photometric stereo using circularly placed point light sources</article-title><source>Proceedings of the 2018 IEEE International Conference on Computational Photography (ICCP)</source><conf-loc>Pittsburgh, PA, USA</conf-loc><conf-date>4&#x02013;6 May 2018</conf-date><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="B32-sensors-25-01143"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>M.</given-names></name>
<name><surname>Zhou</surname><given-names>Z.</given-names></name>
<name><surname>Wu</surname><given-names>Z.</given-names></name>
<name><surname>Shi</surname><given-names>B.</given-names></name>
<name><surname>Diao</surname><given-names>C.</given-names></name>
<name><surname>Tan</surname><given-names>P.</given-names></name>
</person-group><article-title>Multi-view photometric stereo: A robust solution and benchmark dataset for spatially varying isotropic materials</article-title><source>IEEE Trans. Image Process.</source><year>2020</year><volume>29</volume><fpage>4159</fpage><lpage>4173</lpage><pub-id pub-id-type="doi">10.1109/TIP.2020.2968818</pub-id></element-citation></ref><ref id="B33-sensors-25-01143"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bianco</surname><given-names>S.</given-names></name>
<name><surname>Ciocca</surname><given-names>G.</given-names></name>
<name><surname>Marelli</surname><given-names>D.</given-names></name>
</person-group><article-title>Evaluating the performance of structure from motion pipelines</article-title><source>J. Imaging</source><year>2018</year><volume>4</volume><elocation-id>98</elocation-id><pub-id pub-id-type="doi">10.3390/jimaging4080098</pub-id></element-citation></ref><ref id="B34-sensors-25-01143"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gorpas</surname><given-names>D.</given-names></name>
<name><surname>Kampouris</surname><given-names>C.</given-names></name>
<name><surname>Malassiotis</surname><given-names>S.</given-names></name>
</person-group><article-title>Miniature photometric stereo system for textile surface structure reconstruction</article-title><source>Proceedings of the Videometrics, Range Imaging, and Applications XII; and Automated Visual Inspection</source><conf-loc>Munich, Germany</conf-loc><conf-date>13&#x02013;16 May 2013</conf-date><publisher-name>International Society for Optics and Photonics, SPIE</publisher-name><publisher-loc>Bellingham, WA, USA</publisher-loc><year>2013</year><volume>Volume 8791</volume><fpage>271</fpage><lpage>282</lpage></element-citation></ref><ref id="B35-sensors-25-01143"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kampouris</surname><given-names>C.</given-names></name>
<name><surname>Zafeiriou</surname><given-names>S.</given-names></name>
<name><surname>Ghosh</surname><given-names>A.</given-names></name>
<name><surname>Malassiotis</surname><given-names>S.</given-names></name>
</person-group><article-title>Fine-grained material classification using micro-geometry and reflectance</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>11&#x02013;14 October 2016</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2016</year><fpage>778</fpage><lpage>792</lpage></element-citation></ref><ref id="B36-sensors-25-01143"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Schmitt</surname><given-names>C.</given-names></name>
<name><surname>Donne</surname><given-names>S.</given-names></name>
<name><surname>Riegler</surname><given-names>G.</given-names></name>
<name><surname>Koltun</surname><given-names>V.</given-names></name>
<name><surname>Geiger</surname><given-names>A.</given-names></name>
</person-group><article-title>On joint estimation of pose, geometry and svBRDF from a handheld scanner</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;19 June 2020</conf-date><fpage>3493</fpage><lpage>3503</lpage></element-citation></ref><ref id="B37-sensors-25-01143"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lanz</surname><given-names>O.</given-names></name>
<name><surname>Sottsas</surname><given-names>F.</given-names></name>
<name><surname>Conni</surname><given-names>M.</given-names></name>
<name><surname>Boschetti</surname><given-names>M.</given-names></name>
<name><surname>Nocerino</surname><given-names>E.</given-names></name>
<name><surname>Menna</surname><given-names>F.</given-names></name>
<name><surname>Remondino</surname><given-names>F.</given-names></name>
</person-group><article-title>A versatile multi-camera system for 3D acquisition and modeling</article-title><source>Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.</source><year>2020</year><volume>43</volume><fpage>785</fpage><lpage>790</lpage><pub-id pub-id-type="doi">10.5194/isprs-archives-XLIII-B2-2020-785-2020</pub-id></element-citation></ref><ref id="B38-sensors-25-01143"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cook</surname><given-names>R.L.</given-names></name>
<name><surname>Torrance</surname><given-names>K.E.</given-names></name>
</person-group><article-title>A reflectance model for computer graphics</article-title><source>ACM Trans. Graph. (ToG)</source><year>1982</year><volume>1</volume><fpage>7</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1145/357290.357293</pub-id></element-citation></ref><ref id="B39-sensors-25-01143"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Walter</surname><given-names>B.</given-names></name>
<name><surname>Marschner</surname><given-names>S.R.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Torrance</surname><given-names>K.E.</given-names></name>
</person-group><article-title>Microfacet Models for Refraction through Rough Surfaces</article-title><source>Proceedings of the 18th Eurographics Symposium on Rendering (2007)</source><conf-loc>Grenoble, France</conf-loc><conf-date>25&#x02013;27 June 2007</conf-date></element-citation></ref><ref id="B40-sensors-25-01143"><label>40.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Sony Corporation</collab>
</person-group><article-title>Sony IMX219PQH5-C Datasheet</article-title><year>2014</year><comment>Available online: <ext-link xlink:href="https://github.com/rellimmot/Sony-IMX219-Raspberry-Pi-V2-CMOS/blob/bb4a45eaad8b433c2f29aaa9c06592b4efd7552f/RASPBERRY%20PI%20CAMERA%20V2%20DATASHEET%20IMX219PQH5_7.0.0_Datasheet_XXX.PDF" ext-link-type="uri">https://github.com/rellimmot/Sony-IMX219-Raspberry-Pi-V2-CMOS/blob/bb4a45eaad8b433c2f29aaa9c06592b4efd7552f/RASPBERRY%20PI%20CAMERA%20V2%20DATASHEET%20IMX219PQH5_7.0.0_Datasheet_XXX.PDF</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-02-28">(accessed on 28 February 2024)</date-in-citation></element-citation></ref><ref id="B41-sensors-25-01143"><label>41.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Shenzhen Normand Electronic Co., Ltd</collab>
</person-group><article-title>SK6812RGBW Datasheet</article-title><year>2019</year><comment>Available online: <ext-link xlink:href="http://www.normandled.com/upload/201805/SK6812RGBX-XX%20Datasheet.pdf" ext-link-type="uri">http://www.normandled.com/upload/201805/SK6812RGBX-XX%20Datasheet.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-02-28">(accessed on 28 February 2024)</date-in-citation></element-citation></ref><ref id="B42-sensors-25-01143"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Malzbender</surname><given-names>T.</given-names></name>
<name><surname>Wilburn</surname><given-names>B.</given-names></name>
<name><surname>Gelb</surname><given-names>D.</given-names></name>
<name><surname>Ambrisco</surname><given-names>B.</given-names></name>
</person-group><article-title>Surface Enhancement Using Real-time Photometric Stereo and Reflectance Transformation</article-title><source>Proceedings of the Eurographics Symposium on Rendering Techniques</source><conf-loc>Nicosia, Cyprus</conf-loc><conf-date>26&#x02013;28 June 2006</conf-date><volume>Volume 2006</volume><fpage>245</fpage><lpage>250</lpage></element-citation></ref><ref id="B43-sensors-25-01143"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nelder</surname><given-names>J.A.</given-names></name>
<name><surname>Mead</surname><given-names>R.</given-names></name>
</person-group><article-title>A simplex method for function minimization</article-title><source>Comput. J.</source><year>1965</year><volume>7</volume><fpage>308</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1093/comjnl/7.4.308</pub-id></element-citation></ref><ref id="B44-sensors-25-01143"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Brown</surname><given-names>D.C.</given-names></name>
</person-group><article-title>Decentering distortion of lenses</article-title><source>Photogramm. Eng. Remote Sens.</source><year>1966</year><volume>32</volume><fpage>444</fpage><lpage>462</lpage></element-citation></ref><ref id="B45-sensors-25-01143"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Finlayson</surname><given-names>G.D.</given-names></name>
<name><surname>Mackiewicz</surname><given-names>M.</given-names></name>
<name><surname>Hurlbert</surname><given-names>A.</given-names></name>
</person-group><article-title>Color correction using root-polynomial regression</article-title><source>IEEE Trans. Image Process.</source><year>2015</year><volume>24</volume><fpage>1460</fpage><lpage>1470</lpage><pub-id pub-id-type="doi">10.1109/TIP.2015.2405336</pub-id><pub-id pub-id-type="pmid">25769139</pub-id>
</element-citation></ref><ref id="B46-sensors-25-01143"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Barron</surname><given-names>J.T.</given-names></name>
<name><surname>Malik</surname><given-names>J.</given-names></name>
</person-group><article-title>Shape, albedo, and illumination from a single image of an unknown object</article-title><source>Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Providence, RI, USA</conf-loc><conf-date>16&#x02013;21 June 2012</conf-date><fpage>334</fpage><lpage>341</lpage></element-citation></ref><ref id="B47-sensors-25-01143"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sharma</surname><given-names>G.</given-names></name>
<name><surname>Wu</surname><given-names>W.</given-names></name>
<name><surname>Dalal</surname><given-names>E.N.</given-names></name>
</person-group><article-title>The CIEDE2000 color-difference formula: Implementation notes, supplementary test data, and mathematical observations</article-title><source>Color Res. Appl.</source><year>2005</year><volume>30</volume><fpage>21</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1002/col.20070</pub-id></element-citation></ref><ref id="B48-sensors-25-01143"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Suzuki</surname><given-names>S.</given-names></name>
<name><surname>Abe</surname><given-names>K.</given-names></name>
</person-group><article-title>Topological structural analysis of digitized binary images by border following</article-title><source>Comput. Vision Graph. Image Process.</source><year>1985</year><volume>30</volume><fpage>32</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/0734-189X(85)90016-7</pub-id></element-citation></ref><ref id="B49-sensors-25-01143"><label>49.</label><element-citation publication-type="book"><std>ISO 4287</std><source>Geometrical Product Specifications (GPS)&#x02014;Surface Texture: Profile Method&#x02014;Terms, Definitions and Surface Texture Parameters</source><publisher-name>International Organization for Standardization</publisher-name><publisher-loc>Geneve, Switzerland</publisher-loc><year>1997</year><volume>Volume 32</volume><fpage>313</fpage><lpage>320</lpage></element-citation></ref><ref id="B50-sensors-25-01143"><label>50.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Unity Technologies</collab>
</person-group><article-title>Unity Real-Time Development Platform</article-title><year>2005</year><comment>Available online: <ext-link xlink:href="https://unity.com" ext-link-type="uri">https://unity.com</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-02-28">(accessed on 28 February 2024)</date-in-citation></element-citation></ref><ref id="B51-sensors-25-01143"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Nielsen</surname><given-names>J.</given-names></name>
<name><surname>Landauer</surname><given-names>T.K.</given-names></name>
</person-group><article-title>A mathematical model of the finding of usability problems</article-title><source>Proceedings of the INTERACT&#x02019;93 and CHI&#x02019;93 Conference on Human Factors in Computing Systems</source><conf-loc>New York, NY, USA</conf-loc><conf-date>24&#x02013;29 April 1993</conf-date><fpage>206</fpage><lpage>213</lpage></element-citation></ref><ref id="B52-sensors-25-01143"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Brooke</surname><given-names>J.</given-names></name>
</person-group><article-title>SUS-A quick and dirty usability scale</article-title><source>Usability Eval. Ind.</source><year>1996</year><volume>189</volume><fpage>4</fpage><lpage>7</lpage></element-citation></ref><ref id="B53-sensors-25-01143"><label>53.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Sauro</surname><given-names>J.</given-names></name>
</person-group><source>Measuring Usability with the System Usability Scale (SUS)</source><publisher-name>Measuring U</publisher-name><publisher-loc>Denver, CO, USA</publisher-loc><year>2011</year></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01143-f001"><label>Figure 1</label><caption><p>CAD drawings of the device (<bold>a</bold>&#x02013;<bold>c</bold>) and the assembled device (<bold>d</bold>,<bold>e</bold>).</p></caption><graphic xlink:href="sensors-25-01143-g001" position="float"/></fig><fig position="float" id="sensors-25-01143-f002"><label>Figure 2</label><caption><p>Main software pipeline.</p></caption><graphic xlink:href="sensors-25-01143-g002" position="float"/></fig><fig position="float" id="sensors-25-01143-f003"><label>Figure 3</label><caption><p>Roughness estimation pipeline.</p></caption><graphic xlink:href="sensors-25-01143-g003" position="float"/></fig><fig position="float" id="sensors-25-01143-f004"><label>Figure 4</label><caption><p>RAW image before (<bold>a</bold>) and after (<bold>b</bold>) lens shading correction.</p></caption><graphic xlink:href="sensors-25-01143-g004" position="float"/></fig><fig position="float" id="sensors-25-01143-f005"><label>Figure 5</label><caption><p>Incident light direction calibration target. (<bold>a</bold>) CAD drawing, in orange the camera field of view. (<bold>b</bold>) 3D-printed target.</p></caption><graphic xlink:href="sensors-25-01143-g005" position="float"/></fig><fig position="float" id="sensors-25-01143-f006"><label>Figure 6</label><caption><p>Incident light direction calibration pipeline.</p></caption><graphic xlink:href="sensors-25-01143-g006" position="float"/></fig><fig position="float" id="sensors-25-01143-f007"><label>Figure 7</label><caption><p>Evaluation target. (<bold>a</bold>) CAD drawing. (<bold>b</bold>) 3D-printed target.</p></caption><graphic xlink:href="sensors-25-01143-g007" position="float"/></fig><fig position="float" id="sensors-25-01143-f008"><label>Figure 8</label><caption><p>DC ColorChecker comparison between ground truth and estimated albedo. In each square, the inner part is the estimated albedo, while the outer part is the ground truth. (Best viewed in digital format and zoomed in).</p></caption><graphic xlink:href="sensors-25-01143-g008" position="float"/></fig><fig position="float" id="sensors-25-01143-f009"><label>Figure 9</label><caption><p>Normal map evaluation pipeline and heatmap.</p></caption><graphic xlink:href="sensors-25-01143-g009" position="float"/></fig><fig position="float" id="sensors-25-01143-f010"><label>Figure 10</label><caption><p>Samples of acquired roughness maps.</p></caption><graphic xlink:href="sensors-25-01143-g010" position="float"/></fig><fig position="float" id="sensors-25-01143-f011"><label>Figure 11</label><caption><p>Samples of acquired roughness maps. (<bold>a</bold>) Picture of the surface. (<bold>b</bold>) Albedo texture map. (<bold>c</bold>) Normal texture map. (<bold>d</bold>) Roughness texture map. (<bold>e</bold>) Material re-rendering.</p></caption><graphic xlink:href="sensors-25-01143-g011" position="float"/></fig><fig position="float" id="sensors-25-01143-f012"><label>Figure 12</label><caption><p>Workflow of the virtual catalog of textiles system.</p></caption><graphic xlink:href="sensors-25-01143-g012" position="float"/></fig><fig position="float" id="sensors-25-01143-f013"><label>Figure 13</label><caption><p>Scene setup for light interaction simulation. (<bold>a</bold>) Ideal conditions with view direction and incident light direction perpendicular to the sample; (<bold>b</bold>) real-world conditions, with the viewpoint and the sample rotating while the light source remains fixed; (<bold>c</bold>) a virtual setup that replicates the real-world, with the sample and viewpoint remaining fixed while the incident light direction is rotated.</p></caption><graphic xlink:href="sensors-25-01143-g013" position="float"/></fig><fig position="float" id="sensors-25-01143-f014"><label>Figure 14</label><caption><p>Screen captures of the virtual catalog of textiles. (<bold>a</bold>) Textile with perpendicular lighting; (<bold>b</bold>) textile with almost tangent lighting; (<bold>c</bold>) light setup panel; (<bold>d</bold>) textiles list panel.</p></caption><graphic xlink:href="sensors-25-01143-g014" position="float"/></fig><fig position="float" id="sensors-25-01143-f015"><label>Figure 15</label><caption><p>Previews of textile samples used for evaluation of the virtual catalog of textiles. (<bold>a</bold>) Green nylon; (<bold>b</bold>) blue jeans; (<bold>c</bold>) green cotton; (<bold>d</bold>) beige nylon; (<bold>e</bold>) printed nylon; (<bold>f</bold>) sparkling cotton; (<bold>g</bold>) white jute; (<bold>h</bold>) brown alcantara; (<bold>i</bold>) yellow damask cotton.</p></caption><graphic xlink:href="sensors-25-01143-g015" position="float"/></fig><fig position="float" id="sensors-25-01143-f016"><label>Figure 16</label><caption><p>User ranking of the in-app textiles with respect to the real samples.</p></caption><graphic xlink:href="sensors-25-01143-g016" position="float"/></fig><table-wrap position="float" id="sensors-25-01143-t001"><object-id pub-id-type="pii">sensors-25-01143-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison of the proposed device with existing approaches and devices for material acquisition.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Metric</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Proposed Device</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Gonioreflectometer</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Multi-Camera Systems</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Traditional Photometric Stereo</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy <break/>
(Albedo/Normals)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High (validated in tests)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Very High (lab-grade)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Medium to High</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cost</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">~&#x020ac;80</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x020ac;10,000&#x02013;&#x020ac;100,000+</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x020ac;5000&#x02013;&#x020ac;50,000</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x020ac;1000&#x02013;&#x020ac;5000</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Portability</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Highly portable</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-portable</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Limited portability</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate portability</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ease of Use</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">User-friendly</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Requires expertise</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Requires expertise</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Setup Complexity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Simple, compact</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Complex</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Complex</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">References</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">This work</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B2-sensors-25-01143" ref-type="bibr">2</xref>,<xref rid="B5-sensors-25-01143" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01143" ref-type="bibr">6</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B11-sensors-25-01143" ref-type="bibr">11</xref>,<xref rid="B37-sensors-25-01143" ref-type="bibr">37</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B4-sensors-25-01143" ref-type="bibr">4</xref>,<xref rid="B34-sensors-25-01143" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-01143" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-01143" ref-type="bibr">36</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01143-t002"><object-id pub-id-type="pii">sensors-25-01143-t002_Table 2</object-id><label>Table 2</label><caption><p>Bill for the materials.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Component</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Price [EUR]</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Raspberry Pi Zero W</td><td align="center" valign="middle" rowspan="1" colspan="1">10.61</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Raspberry Pi Camera Module V2</td><td align="center" valign="middle" rowspan="1" colspan="1">28.37</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LED ring 8&#x000d7; SK6812 CW</td><td align="center" valign="middle" rowspan="1" colspan="1">1.52</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LED ring 16&#x000d7; SK6812 CW</td><td align="center" valign="middle" rowspan="1" colspan="1">3.04</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Camera cable</td><td align="center" valign="middle" rowspan="1" colspan="1">4.98</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LEDs connector</td><td align="center" valign="middle" rowspan="1" colspan="1">0.32</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LEDs flat cable</td><td align="center" valign="middle" rowspan="1" colspan="1">0.96</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SD card 8 GB</td><td align="center" valign="middle" rowspan="1" colspan="1">5.03</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D printed case</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.00</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">Total:</italic>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">80.83</italic>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01143-t003"><object-id pub-id-type="pii">sensors-25-01143-t003_Table 3</object-id><label>Table 3</label><caption><p>System Usability Scale (SUS) results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Statement</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Strongly<break/>Disagree</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Strongly<break/>Agree</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Avg.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SUS<break/>Score</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">1</td><td align="left" valign="middle" rowspan="1" colspan="1">I think that I would like to use this application frequently.</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">4.0</td><td align="center" valign="middle" rowspan="1" colspan="1">2.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2</td><td align="left" valign="middle" rowspan="1" colspan="1">I found this application unnecessarily complex.</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.7</td><td align="center" valign="middle" rowspan="1" colspan="1">3.7</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">3</td><td align="left" valign="middle" rowspan="1" colspan="1">I thought this application was easy to use.</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">9</td><td align="center" valign="middle" rowspan="1" colspan="1">4.4</td><td align="center" valign="middle" rowspan="1" colspan="1">3.9</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">4</td><td align="left" valign="middle" rowspan="1" colspan="1">I think that I would need assistance to be able to use this application.</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.7</td><td align="center" valign="middle" rowspan="1" colspan="1">3.9</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">5</td><td align="left" valign="middle" rowspan="1" colspan="1">I found the various functions in this application were well integrated.</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">4.3</td><td align="center" valign="middle" rowspan="1" colspan="1">3.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">6</td><td align="left" valign="middle" rowspan="1" colspan="1">I thought there was too much inconsistency in this application.</td><td align="center" valign="middle" rowspan="1" colspan="1">12</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2</td><td align="center" valign="middle" rowspan="1" colspan="1">3.9</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">7</td><td align="left" valign="middle" rowspan="1" colspan="1">I would imagine that most people would learn to use this application very quickly.</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">4.5</td><td align="center" valign="middle" rowspan="1" colspan="1">3.8</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">8</td><td align="left" valign="middle" rowspan="1" colspan="1">I found this application very cumbersome or awkward to use.</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.4</td><td align="center" valign="middle" rowspan="1" colspan="1">3.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">9</td><td align="left" valign="middle" rowspan="1" colspan="1">I felt very confident using this application.</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">4.3</td><td align="center" valign="middle" rowspan="1" colspan="1">3.5</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">I needed to learn a lot of things before I could get going with this application.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.0</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01143-t004"><object-id pub-id-type="pii">sensors-25-01143-t004_Table 4</object-id><label>Table 4</label><caption><p>Application-specific question results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Question</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average Rating</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Quality of rendered textiles</td><td align="center" valign="middle" rowspan="1" colspan="1">4.0</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Usefulness of light interaction simulation</td><td align="center" valign="middle" rowspan="1" colspan="1">4.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Usefulness of light temperature control</td><td align="center" valign="middle" rowspan="1" colspan="1">4.5</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Usefulness of light intensity control</td><td align="center" valign="middle" rowspan="1" colspan="1">4.4</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Would use the application</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.2</td></tr></tbody></table></table-wrap></floats-group></article>