<!--

File produced by pipelineRunner package (for JATS 2 SCJATS with pipeline SCJATS)
At: 2025-05-22T15:36:10.92Z

Version        : 1.16.1
Last update    : 2024-08-27
Modified by    : dunnm

-->
<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Soc Cogn Affect Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Soc Cogn Affect Neurosci</journal-id><journal-id journal-id-type="publisher-id">scan</journal-id><journal-title-group><journal-title>Social Cognitive and Affective Neuroscience</journal-title></journal-title-group><issn pub-type="ppub">1749-5016</issn><issn pub-type="epub">1749-5024</issn><publisher><publisher-name>Oxford University Press</publisher-name><publisher-loc>UK</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39821290</article-id><article-id pub-id-type="pmc">PMC11781275</article-id>
<article-id pub-id-type="doi">10.1093/scan/nsaf001</article-id><article-id pub-id-type="publisher-id">nsaf001</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Research &#x02013; Neuroscience</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI01880</subject></subj-group></article-categories><title-group><article-title>Neural mechanisms underlying the interactive exchange of facial emotional expressions</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-5588-5067</contrib-id><name><surname>Kroczek</surname><given-names>Leon O H</given-names></name><!--leon.kroczek@ur.de--><aff>
<institution content-type="department">Department of Psychology, Clinical Psychology and Psychotherapy, Regensburg University</institution>, Universit&#x000e4;tsstra&#x000df;e 31, Regensburg 93053, <country country="DE">Germany</country></aff><xref rid="COR0001" ref-type="corresp"/></contrib><contrib contrib-type="author"><name><surname>M&#x000fc;hlberger</surname><given-names>Andreas</given-names></name><aff>
<institution content-type="department">Department of Psychology, Clinical Psychology and Psychotherapy, Regensburg University</institution>, Universit&#x000e4;tsstra&#x000df;e 31, Regensburg 93053, <country country="DE">Germany</country></aff></contrib></contrib-group><author-notes><corresp id="COR0001">*Corresponding author. Department of Psychology, Clinical Psychology and Psychotherapy, Regensburg University, Universit&#x000e4;tsstra&#x000df;e 31, Regensburg 93053, Germany. E-mail: <email xlink:href="leon.kroczek@ur.de">leon.kroczek@ur.de</email></corresp></author-notes><pub-date pub-type="collection"><year>2025</year></pub-date><pub-date pub-type="epub" iso-8601-date="2025-01-17"><day>17</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>17</day><month>1</month><year>2025</year></pub-date><volume>20</volume><issue>1</issue><elocation-id>nsaf001</elocation-id><history><date date-type="received"><day>06</day><month>8</month><year>2024</year></date><date date-type="rev-recd"><day>14</day><month>11</month><year>2024</year></date><date date-type="accepted"><day>09</day><month>1</month><year>2025</year></date><date date-type="editorial-decision"><day>14</day><month>12</month><year>2024</year></date><date date-type="corrected-typeset"><day>30</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025. Published by Oxford University Press.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="nsaf001.pdf"/><abstract><title>Abstract</title><p>Facial emotional expressions are crucial in face-to-face social interactions, and recent findings have highlighted their interactive nature. However, the underlying neural mechanisms remain unclear. This electroencephalography study investigated whether the interactive exchange of facial expressions modulates socio-emotional processing. Participants (<italic toggle="yes">N</italic>&#x02009;=&#x02009;41) displayed a facial emotional expression (angry, neutral, or happy) toward a virtual agent, and the agent then responded with a further emotional expression (angry or happy) or remained neutral (control condition). We assessed subjective experience (valence, arousal), facial EMG (Zygomaticus, Corrugator), and event-related potentials (EPN, LPP) elicited by the agent&#x02019;s response. Replicating previous findings, we found that an agent&#x02019;s happy facial expression was experienced as more pleasant and elicited increased Zygomaticus activity when participants had initiated the interaction with a happy compared to an angry expression. At the neural level, angry expressions resulted in a greater LPP than happy expressions, but only when participants directed an angry or happy, but not a neutral, expression at the agent. These findings suggest that sending an emotional expression increases salience and enhances the processing of received emotional expressions, indicating that an interactive setting alters brain responses to social stimuli.</p></abstract><kwd-group><kwd>social interaction</kwd><kwd>EEG/ERP</kwd><kwd>virtual agents</kwd><kwd>emotion</kwd><kwd>facial expressions</kwd></kwd-group><counts><page-count count="10"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Facial emotional expressions are critical in social interactions, revealing a person&#x02019;s affective state and intentions, and coordinating exchanges (<xref rid="R16" ref-type="bibr">Frith 2009</xref>, <xref rid="R34" ref-type="bibr">Lange et&#x000a0;al. 2022</xref>). The emotion as social information model (<xref rid="R47" ref-type="bibr">Van Kleef 2009</xref>) suggests that perceiving an emotional expression (e.g. facial, body, or voice) impacts the observer in two ways. First, it can evoke an affective response either through mimicry (<xref rid="R12" ref-type="bibr">Dimberg 1982</xref>, <xref rid="R13" ref-type="bibr">Dimberg et&#x000a0;al. 2000</xref>, <xref rid="R24" ref-type="bibr">Hess and Bourgeois 2010</xref>) or socio-emotional evaluation, such as liking someone who smiles or disliking an angry person (<xref rid="R41" ref-type="bibr">Salazar K&#x000e4;mpf et&#x000a0;al. 2018</xref>). Second, it allows observers to infer the mental state, feelings, and intentions of the person displaying the expression (<xref rid="R48" ref-type="bibr">Van Kleef et&#x000a0;al. 2011</xref>), thereby helping to identify the likely cause of the expression and inform subsequent behavior (<xref rid="R29" ref-type="bibr">Kroczek et&#x000a0;al. 2021</xref>, <xref rid="R30" ref-type="bibr">2024</xref>). For example, a smile from a stranger on a train might indicate affiliative intentions, prompting reciprocal behavior such as smiling back or greeting. Conversely, an angry expression might suggest that the person should be left alone. These examples illustrate how emotional expressions facilitate the coordination of interpersonal behavior in social contexts.</p><p>In real-time face-to-face interactions, emotional expressions emerge from dynamic exchanges between interactive partners (<xref rid="R21" ref-type="bibr">Heerey and Crossley 2013</xref>). During a dyadic exchange, individuals both send and receive emotional signals, interpreting them in the context of preceding expressions. This requires nested representations of others&#x02019; inferences (<xref rid="R35" ref-type="bibr">Lehmann et&#x000a0;al. 2024</xref>), that is, one must consider the other person&#x02019;s interpretations of one&#x02019;s previous expressions. Although interactivity is a hallmark of human social behavior (<xref rid="R4" ref-type="bibr">Bolis et&#x000a0;al. 2023</xref>), most studies have focused on noninteractive paradigms in which participants passively observe emotional expressions. Recent arguments suggest that studying social processes without real interaction may lead to reduced or different mechanisms compared to interactive situations (<xref rid="R14" ref-type="bibr">Fischer and van Kleef 2010</xref>, <xref rid="R40" ref-type="bibr">Risko et&#x000a0;al. 2012</xref>, <xref rid="R39" ref-type="bibr">Redcay and Schilbach 2019</xref>). Real-time interactions, however, are challenging to investigate because of uncontrollable variables that may confound the processes of interest (<xref rid="R19" ref-type="bibr">Hadley et&#x000a0;al. 2022</xref>). Therefore, virtual interactions with agents have been proposed as a solution that offers fine-grained control over verbal and nonverbal cues (<xref rid="R5" ref-type="bibr">Bombari et&#x000a0;al. 2015</xref>). Overall, the study of social behavior should include genuinely interactive designs where a person&#x02019;s actions elicit responses from others, and virtual social interactions can be beneficial for this purpose.</p><p>The interpersonal effects of facial emotional expressions are often studied through emotional mimicry, a mechanism in which observing a facial expression activates the corresponding facial muscles in the observer (<xref rid="R12" ref-type="bibr">Dimberg 1982</xref>). Emotional mimicry has been explained by the automatic matched-motor hypothesis, which posits a direct perception-action link (<xref rid="R9" ref-type="bibr">Chartrand Tanya and Bargh John 1999</xref>). Recent models, however, highlight the influence of communicative intentions and affiliative goals (<xref rid="R23" ref-type="bibr">Hess 2021</xref>), suggesting that social context modulates mimicry responses. Mimicry, for instance, is more consistent for positive emotional expressions in social interactions (<xref rid="R24" ref-type="bibr">Hess and Bourgeois 2010</xref>) and is enhanced by affiliative intentions (<xref rid="R41" ref-type="bibr">Salazar K&#x000e4;mpf et&#x000a0;al. 2018</xref>). A recent study examined mimicry in an interactive context. Participants directed facial expressions (smile, neutral, and frown) at a virtual agent, which then responded with a smile or frown (<xref rid="R31" ref-type="bibr">Kroczek and M&#x000fc;hlberger 2022</xref>). Consistent with passive paradigms, observing a smile compared to a frown resulted in increased activation of M. zygomaticus. Crucially, this effect was most pronounced when participants initially smiled at the agent and disappeared when they first frowned. Similarly, initiating an interaction with a smile compared to a frown increased the pleasantness of an agent&#x02019;s smile. This study indicated that the interactive context in face-to-face social interactions influences the evaluation of facial emotional expressions. However, the underlying neural mechanisms remain unknown.</p><p>Electroencephalography (EEG) enables the study of temporal dynamics in face processing, with event-related potentials (ERPs) linked to early (P1, N170), mid-latency (early posterior negativity, EPN), and late (late positive potential, LPP) processing stages (<xref rid="R50" ref-type="bibr">Wieser and Brosch 2012</xref>, <xref rid="R25" ref-type="bibr">Hinojosa et&#x000a0;al. 2015</xref>, <xref rid="R45" ref-type="bibr">Schindler and Bublatzky 2020</xref>, <xref rid="R44" ref-type="bibr">Schindler et&#x000a0;al. 2023</xref>). While emotional processing has been already linked to the N170 component (<xref rid="R25" ref-type="bibr">Hinojosa et&#x000a0;al. 2015</xref>, <xref rid="R44" ref-type="bibr">Schindler et&#x000a0;al. 2023</xref>), the EPN stage involves early attentional processing related to prioritized processing of emotional information and has been demonstrated for a range of emotional stimuli (e.g. faces, scenes, and words). Finally, the LPP concerns higher-order evaluation of emotional content, such as controlled attention or appraisal. The LPP is modulated by task relevance, i.e. effects of emotion were only observed when the task required attending to emotional information in the stimulus (<xref rid="R43" ref-type="bibr">Schindler et&#x000a0;al. 2020</xref>, <xref rid="R28" ref-type="bibr">Krasowski et&#x000a0;al. 2021</xref>). Furthermore, the LPP was found to be sensitive to personal relevance, for instance, when faces were introduced as future interaction partners or when an angry face stimulus was directly facing participants (<xref rid="R7" ref-type="bibr">Bublatzky et&#x000a0;al. 2014</xref>, <xref rid="R8" ref-type="bibr">2017</xref>). These findings suggest that emotional information in faces is processed early and adapts to task demands and personal relevance. This has implications for face-to-face social interactions, where a direct reciprocal exchange should increase relevance. An interactive paradigm is essential for testing this notion.</p><p>This study aimed to determine whether social interaction with a virtual agent influences the neural processing of facial emotional expressions following a previously established paradigm (<xref rid="R31" ref-type="bibr">Kroczek and M&#x000fc;hlberger 2022</xref>, <xref rid="R32" ref-type="bibr">2023</xref>). Participants were instructed to display a facial emotional expression (happy, neutral, or angry) toward a virtual agent on a screen (hence called &#x0201c;initial expression&#x0201d;). The virtual agent then responded with another facial emotional expression (happy, neutral, or angry; hence called &#x0201c;response expression&#x0201d;). Continuous measurements of facial EMG and EEG were conducted to analyze mimicry and ERP responses to the agent&#x02019;s facial expressions (EPN and LPP components). It was expected that the results would replicate the findings of <xref rid="R31" ref-type="bibr">Kroczek and M&#x000fc;hlberger (2022)</xref>, demonstrating an interaction between the participant&#x02019;s initial expression and the agent&#x02019;s response expression in both valence ratings and Zygomaticus muscle activation. Additionally, it was hypothesized that the participant&#x02019;s initial facial expression would influence the neural processing of the agent&#x02019;s subsequent response expression. We expected this interaction effect for the LPP as an indicator of higher-order evaluative processing, given that sending a facial emotional expression to an interactive partner should directly influence the relevance of a response expression (<xref rid="R7" ref-type="bibr">Bublatzky et&#x000a0;al. 2014</xref>, <xref rid="R8" ref-type="bibr">2017</xref>). However, because relevance effects have been shown for both angry and happy facial expressions, we did not predict the direction of the interaction effect.</p></sec><sec id="s2"><title>Methods</title><sec id="s2-s1"><title>Participants</title><p>Forty-one healthy volunteers (34 females, mean age&#x02009;=&#x02009;22.70&#x02009;years, SD&#x02009;=&#x02009;5.08, range&#x02009;=&#x02009;18&#x02013;40) participated in the study. They reported no neurological or mental disorders and had normal or corrected-to-normal vision. Four participants were excluded from the EMG and EEG analyses due to technical issues. The study adhered to the Declaration of Helsinki and was approved by the University of Regensburg&#x02019;s ethics committee (19-1417-101). All participants provided written informed consent and received course credit for participation.</p></sec><sec id="s2-s2"><title>Stimulus material</title><p>The stimulus material has been used in previous studies (<xref rid="R31" ref-type="bibr">Kroczek and M&#x000fc;hlberger 2022</xref>, <xref rid="R32" ref-type="bibr">2023</xref>). The stimuli comprised video clips of four virtual agents (two females, two males) created with MakeHuman (v 1.1.1, <ext-link xlink:href="http://www.makehumancommunity.org" ext-link-type="uri">http://www.makehumancommunity.org</ext-link>) and animated using Blender (v2.79, Blender Foundation). Agents were visualized as white, young adults (see <xref rid="s6" ref-type="sec">Supplementary Fig. S1</xref>). Each video clip lasted 5500&#x02009;ms, starting with the agents displaying a neutral facial expression. After 4000&#x02009;ms, the facial expression changed to either angry or happy, or remained neutral. Emotional expressions transitioned within 500&#x02009;ms and stayed visible for the remaining 1000&#x02009;ms of the clip. In the neutral condition, the expression stayed the same throughout the clip. All videos featured agents with subtle head and body movements and eye blinks (5 different versions), resulting in 60 video clips (4 Agents &#x000d7; 3 Emotions &#x000d7; 5 Versions). Still frames of agents with happy, angry, or neutral expressions were rated for valence and arousal at the end of the experiment. Happy expressions were rated as most pleasant, followed by neutral and angry expressions. Both happy and angry expressions were rated more arousing than neutral ones, with angry expressions being slightly more arousing than happy ones. The emotional expression effect was consistent across the four agents (<xref rid="s6" ref-type="sec">Supplementary Fig. S2</xref>).</p></sec><sec id="s2-s3"><title>Procedure</title><p>After receiving instructions on the experimental procedures, participants completed questionnaires on demographic data (age, sex) and the Social Phobia Inventory (<xref rid="R10" ref-type="bibr">Connor et&#x000a0;al. 2000</xref>). Subsequently, EEG electrodes were prepared, and participants were seated 60&#x02009;cm away from a 21.5-inch LCD screen (HP EliteDisplay E221c, resolution: 1920&#x02009;&#x000d7;&#x02009;1080 pixels). The Social Phobia Inventory was not analyzed in the current study but data are available in the online repository.</p><p>Experimental procedures were controlled using PsychToolbox (<xref rid="R38" ref-type="bibr">Pelli 1997</xref>) in Matlab (v 8.6, MathWorks). In total, 360 pseudo-randomized trials were presented, with no more than three repetitions of initial expression, response expression, or virtual agent. Three practice trials were presented. All trials adhered to the same structure (<xref rid="F1" ref-type="fig">Fig.&#x000a0;1</xref>), beginning with a fixation cross for 500&#x02009;ms, followed by a 1000&#x02009;ms instruction of the emotion to be directed at the virtual agent (Happiness, Anger, Neutral). After another 500&#x02009;ms fixation cross, a virtual agent with neutral expression was presented. After a random interval between 1000 and 1300&#x02009;ms, a white rectangular frame appeared around the agent, signaling participants to display the instructed emotion at the virtual agent using facial expressions (smile for happiness, frown for anger, no expression for neutral). The cue lasted for 1200&#x02009;ms and participants were instructed to stop the facial emotional expression once the cue disappeared. After another delay between 1500 and 1800&#x02009;ms, i.e. exactly 4000&#x02009;ms after video onset, the agent displayed a facial response expression (Angry, Happy, Neutral) for 1500&#x02009;ms until the clip ended. Subsequently, the next trial began, or, in the case of a catch trial, ratings were obtained. Catch trials comprised 20% of all trials (8 trials per condition). Participants rated arousal and valence regarding their interaction with the virtual agent on a 9-point Likert scale (&#x0201c;How high was your arousal with the previous person?&#x0201d;/&#x0201c;How pleasant did you feel with the previous person?&#x0201d;; 1&#x02009;=&#x02009;very low arousal/very unpleasant, 9&#x02009;=&#x02009;very high arousal/very pleasant). The entire experiment lasted approximately 45&#x02009;min. There were self-determined breaks after every 80 trials.</p><fig position="float" id="F1" fig-type="figure"><label>Figure&#x000a0;1.</label><caption><p>Experimental trial structure. The instruction informed participants about the facial emotional expression which they had to direct toward the virtual agent in the subsequent interaction. After the virtual agent appeared on the screen, a white rectangular frame around the virtual agent cued participants to display the facial emotional expression. After a jittered delay the virtual agent would then display a response facial emotional expression (angry or happy) or remain neutral.</p></caption><graphic xlink:href="nsaf001f1" position="float"/></fig></sec><sec id="s2-s4"><title>Prestudy: detection point of emotional expression</title><p>The study used dynamic video clips, in which agents displayed a neutral expression for 4000&#x02009;ms before transitioning to an emotional expression over 500&#x02009;ms. EEG analysis required precise onset timing for differentiating emotional expressions. Therefore, a pretest with an independent sample (<italic toggle="yes">N</italic>&#x02009;=&#x02009;16) used a gating procedure (<xref rid="R37" ref-type="bibr">Pell et&#x000a0;al. 2011</xref>, <xref rid="R29" ref-type="bibr">Kroczek et&#x000a0;al. 2021</xref>) to determine when the transition from neutral to emotional expression was detectable. Participants viewed video clips with varying degrees of transition completeness, starting with a neutral expression for 3000&#x02009;ms, followed by a transition to angry, happy, or remaining neutral. The transition duration varied from 1 to 20 frames. Participants identified the expressions as &#x0201c;angry,&#x0201d; &#x0201c;neutral,&#x0201d; or &#x0201c;happy&#x0201d; in a forced-choice paradigm after each clip. A total of 240 trials were presented in random order and the percentage of correctly identified expressions was measured for each number of presented transition frames (<xref rid="s6" ref-type="sec">Supplementary Fig. S3</xref>). For each participant, the frame number at which the correct expression was detected&#x02009;&#x02265;&#x02009;50% was identified. Results showed the mean detection point for emotional expressions was 7.41 frames (SD&#x02009;=&#x02009;1.07). The detection point was therefore set to eight frames (133&#x02009;ms at 60&#x02009;Hz) for ERP and EMG analysis.</p></sec><sec id="s2-s5"><title>EEG/EMG recording and preprocessing</title><p>EEG data were collected from 32 Ag/AgCl passive electrodes positioned according to the extended 10&#x02013;20 system using an elastic cap (Easy Cap), with impedances kept below 5k&#x003a9;. Additional bipolar electrodes measured Zygomaticus and Corrugator muscle activity using two 6&#x02009;mm Ag/AgCl electrodes per muscle, on the left side of the face following <xref rid="R15" ref-type="bibr">Fridlund and Cacioppo (1986)</xref> as well as vertical and horizontal eye movements. Data were recorded at 1000&#x02009;Hz (NeurOne Tesla EEG/MRI System, Bittium), online-referenced to FCz, with AFz as ground.</p><p>Data preprocessing was conducted in Matlab (v 9.7.0) using EEGLAB (v2022.0, <xref rid="R36" ref-type="bibr">Oostenveld et&#x000a0;al. 2011</xref>). EMG data preprocessing included applying a bandpass filter (30&#x02013;500&#x02009;Hz) and a 50&#x02009;Hz notch filter (&#x02212;6&#x02009;dB, half amplitude, noncausal, zero-phase, Hamming window). Data were rectified, integrated (moving average of 125&#x02009;ms window), log-transformed to minimize skewness, and segmented into 2000&#x02009;ms epochs around the change detection point, with 500&#x02009;ms prestimulus and 1500&#x02009;ms poststimulus periods. Baseline correction was applied. Trials were excluded if participants did not display the instructed facial emotional expression (M&#x02009;=&#x02009;22.57, SD&#x02009;=&#x02009;21.54). To identify these trials, a pre-established procedure was used (<xref rid="R32" ref-type="bibr">Kroczek and M&#x000fc;hlberger 2023</xref>), comparing EMG activity postcue onset against a muscle-specific threshold. For instructed smiles or frowns, Zygomaticus or Corrugator activity had to exceed a muscle-specific threshold. For a neutral expression, neither Zygomaticus nor Corrugator activity could exceed the thresholds. The threshold was set at 25% of the 90th percentile of activation maxima in all trials with a specific facial emotional instruction.</p><p>EEG data preprocessing included artifact correction with independent component analysis (ICA). Therefore, a two-step procedure was conducted. First, data were preprocessed to optimize ICA decomposition. Here, raw EEG was filtered with a 1&#x02009;Hz high-pass filter (&#x02212;6&#x02009;dB, half amplitude, noncausal zerophase, Hamming window), re-referenced to the average, and ICA was performed using runica with the infomax algorithm (<xref rid="R11" ref-type="bibr">Delorme and Makeig 2004</xref>). In a second step, the ICA components were then projected onto another dataset with different filter settings, which was then used for the actual ERP analysis. The second dataset was created by filtering the raw EEG with a 0.1&#x02013;30&#x02009;Hz bandpass filter (same filter specification as above) and re-referencing to the average. ICA components from the first step were projected onto this new dataset, and components associated with eye-blinks and eye-movements were removed. Data were segmented into epochs [&#x02212;200 to 1000&#x02009;ms] time-locked to the change detection point of the virtual agent&#x02019;s facial emotional expression. For trials with a neutral expression, the same timepoint was defined as the onset. Epochs exceeding&#x02009;&#x000b1;&#x02009;80&#x02009;&#x000b5;V were rejected (mean trials rejected&#x02009;=&#x02009;10.82%, SD&#x02009;=&#x02009;15.69%). A prestimulus interval of 200&#x02009;ms was used for baseline correction, and trials in which participants displayed the incorrect facial emotional expression (see above) were excluded.</p><p>Note that participants&#x02019; display of the initial facial emotional expressions was linked to strong EMG activation (<xref rid="s6" ref-type="sec">Supplementary Fig. S4</xref>), which took several seconds to return to baseline. To control for this residual muscle activation in the EMG and ERP analyses, we calculated difference waves by subtracting the neutral response from both angry and happy responses for each initial expression condition. Using this subtraction allowed us to remove the residual muscle activation from the participant&#x02019;s own expression while preserving the activation which was elicited by the agent&#x02019;s response expression. This procedure was conducted for both the EMG and ERP data. <xref rid="s6" ref-type="sec">Supplementary Figs S6 and S7</xref> show ERP data before subtracting in all three response conditions (angry, happy, neutral).</p></sec><sec id="s2-s6"><title>Statistical analyses</title><p>Repeated measures analyses of variance (ANOVAs) were used to analyze ratings (factors: Initial Expression, Response Expression) and EMG data (factors: Initial Expression, Response Expression, Time Window; 15 consecutive 100&#x02009;ms windows). Significant Time Window interactions were further analyzed by averaging data across windows. The EPN (200&#x02013;350&#x02009;ms; channels PO9, PO10, P7, P8, O1, Oz, O2; <xref rid="R22" ref-type="bibr">Herbert et&#x000a0;al. 2013</xref>, <xref rid="R2" ref-type="bibr">Bagherzadeh&#x02010;Azbari et&#x000a0;al. 2023</xref>) and LPP (400&#x02013;800&#x02009;ms; channels CP1, CP2, Pz, P3, P4) components were analyzed using repeated measures ANOVAs (factors: Initial Expression, Response Expression; <xref rid="R43" ref-type="bibr">Schindler et&#x000a0;al. 2020</xref>, <xref rid="R28" ref-type="bibr">Krasowski et&#x000a0;al. 2021</xref>). Greenhouse&#x02013;Geisser adjustments corrected for sphericity violations, and <italic toggle="yes">post-hoc t</italic>-tests with Holm correction followed significant interactions (<xref rid="R17" ref-type="bibr">Greenhouse and Geisser 1959</xref>; <xref rid="R26" ref-type="bibr">Holm 1979</xref>). Note that for EMG and ERP analyses, the levels angry and happy of the factor Response Expression included the difference between angry-neutral response expression conditions and happy-neutral response expression conditions for each initial expression to control for confounding muscle activation elicited by participants&#x02019; active display of an emotional facial expression (see above). Exploratory correlations examined associations between ratings, EMG, and ERP responses. Rating data were recalculated as emotional-neutral differences for each initial expression level. Zygomaticus (0&#x02013;1500&#x02009;ms) and Corrugator (200&#x02013;500&#x02009;ms) activation were averaged based on main analysis results. Alpha was set at 0.05. Stimulus materials, experimental scripts, analysis scripts, and anonymized rating data are available online (<ext-link xlink:href="https://osf.io/ghdrm/" ext-link-type="uri">https://osf.io/ghdrm/</ext-link>). Raw EEG data are available upon request.</p></sec></sec><sec id="s3"><title>Results</title><sec id="s3-s1"><title>Ratings</title><p>Analysis of valence ratings (<xref rid="F2" ref-type="fig">Fig.&#x000a0;2a</xref>) revealed an interaction between Initial Expression and Response Expression, F(4160)&#x02009;=&#x02009;16.67, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.29 (&#x003b5;&#x02009;=&#x02009;0.60), a main effect of Initial Expression, F(2,80)&#x02009;=&#x02009;16.58, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.29 (&#x003b5;&#x02009;=&#x02009;0.73), and a main effect of Response Expression, F(2,80)&#x02009;=&#x02009;67.74, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.63 (&#x003b5;&#x02009;=&#x02009;0.57). An agent&#x02019;s happy expression was most pleasant following an initial happy, intermediate pleasant following an initial neutral, and least pleasant following an angry expression [happy-neutral: t(40)&#x02009;=&#x02009;4.86, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, d&#x02009;=&#x02009;0.76, neutral-angry: t(40)&#x02009;=&#x02009;4.75, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.001, d&#x02009;=&#x02009;0.74]. Whereas an agent&#x02019;s neutral expression was most pleasant following an initial neutral compared to an initial angry, t(40)&#x02009;=&#x02009;4.70, <italic toggle="yes">P</italic> &#x0003c;.001, d&#x02009;=&#x02009;0.73, or initial happy expression, t(40)&#x02009;=&#x02009;2.19, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.045, d&#x02009;=&#x02009;0.34. Valence of angry response expression did not differ between initial expressions (all <italic toggle="yes">P</italic>s &#x0003e;.10).</p><fig position="float" id="F2" fig-type="figure"><label>Figure&#x000a0;2.</label><caption><p>Valence and arousal ratings as a function of the initial facial emotional expression by the participant (angry, neutral, happy) and the facial emotional response expression returned by the virtual agent (angry, neutral, happy).</p></caption><graphic xlink:href="nsaf001f2" position="float"/></fig><p>Analysis of arousal ratings (<xref rid="F2" ref-type="fig">Fig.&#x000a0;2b</xref>) revealed a main effect of Initial Expression, F(2,80)&#x02009;=&#x02009;15.57, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.28 (&#x003b5;&#x02009;=&#x02009;0.83), and a main effect of Response Expression, F(2,80&#x02009;=&#x02009;12.32, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.24, but no interaction between Initial Expression and Response Expression, F(4, 160)&#x02009;=&#x02009;0.47, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.761, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.01. Specifically smiling and frowning increased arousal compared to displaying a neutral expression [happy-neutral: t(40)&#x02009;=&#x02009;4.55, <italic toggle="yes">P</italic> &#x0003c;.001, d&#x02009;=&#x02009;0.71; angry-neutral: t(40)&#x02009;=&#x02009;3.99, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, d&#x02009;=&#x02009;0.62], but there was no difference between smiling and frowning, t(40)&#x02009;=&#x02009;1.43, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.162, d&#x02009;=&#x02009;0.22. Similarly, arousal ratings were higher for happy and angry compared to neutral response expressions [happy-neutral: t(40)&#x02009;=&#x02009;4.86, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, d&#x02009;=&#x02009;0.75; angry-neutral: t(40)&#x02009;=&#x02009;3.59, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.002, d&#x02009;=&#x02009;0.56] but there was no difference between happy and angry response expressions, t(40)&#x02009;=&#x02009;0.85, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.399, d&#x02009;=&#x02009;0.13.</p><p>Overall, sending a facial emotional expression toward an interactive partner modulates pleasantness of a response smile, while arousal is not influenced by the interplay of sending and receiving an emotional expression.</p></sec><sec id="s3-s2"><title>EMG response</title><sec id="s3-s2-s1"><title>M. Zygomaticus</title><p>Analysis of Zygomaticus activity showed a main effect of Response Expression, F(1,36)&#x02009;=&#x02009;6.77, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.013, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.16, a main effect of Time Window, F(14&#x02009;504)&#x02009;=&#x02009;6.78, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.002, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.16 (&#x003b5;&#x02009;=&#x02009;0.15), and interactions between Response Expression and Time Window, F(14, 504)&#x02009;=&#x02009;4.79, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.017, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.12 (&#x003b5;&#x02009;=&#x02009;0.12), and Initial Expression and Response Expression, F(2,72)&#x02009;=&#x02009;4.90, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.010, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.12. Happy response expressions triggered stronger Zygomaticus activation than angry expressions, particularly within 200 to 1400&#x02009;ms post onset of the agent&#x02019;s response expression. Furthermore, stronger Zygomaticus activation was elicited for happy versus angry response expressions when participants initially displayed a happy expression, t(36)&#x02009;=&#x02009;3.48, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.004, d&#x02009;=&#x02009;0.57, but not for initially neutral, t(36)&#x02009;=&#x02009;1.99, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.109, d&#x02009;=&#x02009;0.33, or angry expressions, t(36)&#x02009;=&#x02009;0.20, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.840, d&#x02009;=&#x02009;0.03 (see <xref rid="F3" ref-type="fig">Fig.&#x000a0;3a</xref>). Results indicate that Zygomaticus mimicry effects are amplified when a smile is directed at an interactive partner.</p><fig position="float" id="F3" fig-type="figure"><label>Figure&#x000a0;3.</label><caption><p>EMG activation in the Zygomaticus (a) and Corrugator muscle (b) following happy or angry response expressions of the virtual agents as a function of the initial emotional expression that was displayed by the participant. Shaded areas represent the standard error of the mean.</p></caption><graphic xlink:href="nsaf001f3" position="float"/></fig></sec><sec id="s3-s2-s2"><title>M. Corrugator</title><p>Analysis of Corrugator activity (<xref rid="F3" ref-type="fig">Fig.&#x000a0;3b</xref>) revealed a main effect of Initial Expression, F(2,72)&#x02009;=&#x02009;6.53, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.008, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.15 (&#x003b5;&#x02009;=&#x02009;0.66), and Time Window, F(14&#x02009;504)&#x02009;=&#x02009;4.93, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.003, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.12 (&#x003b5;&#x02009;=&#x02009;0.21), and an interaction effect between Response Expression and Time Window, F(14&#x02009;504)&#x02009;=&#x02009;4.92, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.004, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.16 (&#x003b5;&#x02009;=&#x02009;0.19). Angry response expression elicited higher Corrugator activation than happy ones in a time window of 200 to 500&#x02009;ms, t(36)&#x02009;=&#x02009;3.62, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, d&#x02009;=&#x02009;0.59. Corrugator activity was also higher when participants had initially displayed an angry compared to a neutral, t(36)&#x02009;=&#x02009;3.12, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.011, d&#x02009;=&#x02009;0.51, or happy expression (marginal significant), t(36)&#x02009;=&#x02009;2.32, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.053, d&#x02009;=&#x02009;0.38. These data demonstrate a mimicry effect for angry expressions in the Corrugator which was not modulated by sending an initial facial emotional expression.</p></sec></sec><sec id="s3-s3"><title>Event-related potentials</title><sec id="s3-s3-s1"><title>EPN</title><p>Analysis of the EPN investigated the effects of Initial Expression and Response Expression on temporo-occipital electrodes between 200 and 350&#x02009;ms post detection point of the emotional response expression (<xref rid="F4" ref-type="fig">Fig.&#x000a0;4</xref>). A repeated-measures ANOVA revealed a main effect of Response Expression, F(1,36)&#x02009;=&#x02009;41.92, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.54. Angry response expressions (M&#x02009;=&#x02009;&#x02212;3.56, SD&#x02009;=&#x02009;1.50) elicited a more negative ERP than happy response expressions (M&#x02009;=&#x02009;&#x02212;2.60, SD&#x02009;=&#x02009;1.04). There was no main effect of Initial expression, F(2,72)&#x02009;=&#x02009;2.18, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.121, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.06, and no interaction between Initial Expression and Response Expression F(2,72)&#x02009;=&#x02009;2.68, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.075, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.07.</p><fig position="float" id="F4" fig-type="figure"><label>Figure&#x000a0;4.</label><caption><p>Top: ERPs elicited by the agent&#x02019;s facial expression as a function of the initial emotional expression of the participant (left&#x02009;=&#x02009;angry, middle&#x02009;=&#x02009;neutral, right&#x02009;=&#x02009;happy) and the response expression of the agent (angry&#x02009;=&#x02009;blue line, happy&#x02009;=&#x02009;red line) averaged across temporo-occipital electrodes (PO7, PO8, P7, P8, O1, Oz, O2). ERPs are time-locked to the detection point of the facial emotional expression. Single ERPs always reflect the difference from the neutral control condition (no agent response) for the same initial expression. The 200&#x02013;350&#x02009;ms window in which EPN was analyzed is highlighted in grey. Shaded areas represent the standard error of the mean. Bottom: Topographical distribution of the emotion effect for each initial expression by subtracting ERPs to angry and happy response expression in a time window of 200-300&#x02009;ms.</p></caption><graphic xlink:href="nsaf001f4" position="float"/></fig></sec><sec id="s3-s3-s2"><title>LPP</title><p>The LPP component was examined in centro-parietal electrodes between 400 and 800&#x02009;ms post detection point of the emotional response expression (<xref rid="F5" ref-type="fig">Fig.&#x000a0;5</xref>). ANOVA results showed a main effect of Response Expression, F(1,36)&#x02009;=&#x02009;26.55, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.42, and an interaction between Initial Expression and Response Expression, F(2,72)&#x02009;=&#x02009;3.83, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.026, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.10. No main effect of Initial Expression was found, F(2,72)&#x02009;=&#x02009;1.42, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.249, &#x003b7;<sub>p</sub><sup>2</sup>&#x02009;=&#x02009;0.04. <italic toggle="yes">Post-hoc t</italic>-tests revealed a more positive LPP for angry compared to happy response expressions following both angry, t(36)&#x02009;=&#x02009;4.13, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, d&#x02009;=&#x02009;0.68, and happy, t(36)&#x02009;=&#x02009;3.70, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.001, d&#x02009;=&#x02009;0.61, but not neutral initial expressions, t(36)&#x02009;=&#x02009;1.90, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.066, d&#x02009;=&#x02009;0.32. The difference between angry and happy response expressions was larger after an angry initial expression compared to a neutral one, t(36)&#x02009;=&#x02009;2.66, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.035, d&#x02009;=&#x02009;0.44, but no difference was found between angry versus happy initial expressions, t(36)&#x02009;=&#x02009;1.82, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.153, d&#x02009;=&#x02009;0.29, or happy versus neutral initial expressions, t(36)&#x02009;=&#x02009;0.92, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.036, d&#x02009;=&#x02009;0.15. Therefore, displaying an emotional facial expression at an interactive partner affected the LPP response to emotional expressions. A greater LPP for angry compared to happy expressions was only observed for initial angry and initial happy but not initial neutral conditions.</p><fig position="float" id="F5" fig-type="figure"><label>Figure&#x000a0;5.</label><caption><p>Top: ERPs elicited by the agent&#x02019;s facial expression as a function of the initial emotional expression of the participant (left&#x02009;=&#x02009;angry, middle&#x02009;=&#x02009;neutral, right&#x02009;=&#x02009;happy) and the response expression of the agent (angry&#x02009;=&#x02009;blue line, happy&#x02009;=&#x02009;red line) averaged across centro-parietal electrode (CP1, CP2, Pz, P3, P4). ERPs are time-locked to the detection point of the facial emotional expressions. Single ERPs always reflect the difference from the neutral control condition (no agent response) for the same initial expression. The 400&#x02013;800&#x02009;ms window in which LPP was analyzed is highlighted in grey. Shaded areas represent the standard error of the mean. Bottom: Topographical distribution of the emotion effect for each initial expression by subtracting ERPs to angry and happy response expression in a time window of 400&#x02013;800&#x02009;ms.</p></caption><graphic xlink:href="nsaf001f5" position="float"/></fig></sec></sec><sec id="s3-s4"><title>Correlations</title><p>Pearson correlations investigated the relation of ratings, EMG, and ERP responses. After multiple comparisons correction (Holm), only two significant correlations remained (<xref rid="s6" ref-type="sec">Supplementary Table S1</xref>): Zygomaticus activity correlated positively with valence ratings, r(220)&#x02009;=&#x02009;0.25, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.005, while Corrugator activity correlated positively with LPP amplitude, r(220)&#x02009;=&#x02009;0.21, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.036. In summary, the more pleasant an interaction was experienced, the more activation was elicited in the Zygomaticus muscle, and the more activation was elicited in the Corrugator muscle, the greater the LPP amplitude.</p></sec></sec><sec id="s4"><title>Discussion</title><p>This study examined whether sending a facial emotional expression affects the processing and evaluation of response facial emotional expressions. Results showed two patterns: Firstly, smiling, as opposed to frowning, at an interactive partner increased pleasantness and mimicry of the partner&#x02019;s smiles, replicating findings by <xref rid="R31" ref-type="bibr">Kroczek and M&#x000fc;hlberger (2022</xref>). Secondly, neurophysiological data revealed enhanced processing of the partner&#x02019;s facial expressions when participants initially frowned or smiled but not when they displayed a neutral expression. Thus, while all measures were sensitive to the interplay of sending and receiving facial expressions, behavioral (ratings and mimicry) and neurophysiological measures appear to reflect different socio-affective processing mechanisms. Mimicry and ratings align with an affiliative mechanism, enhancing the evaluation and mimicry of positive expressions, whereas LPP data suggest a relevance mechanism, where facial expressions gain relevance following one&#x02019;s own emotional expression.</p><p>LPPs elicited by an agent&#x02019;s emotional expressions varied based on preceding facial expressions of the participant. Consistent with prior research, angry expressions induced a greater LPP than happy expressions (<xref rid="R3" ref-type="bibr">Blechert et&#x000a0;al. 2012</xref>), which has been linked to enhanced processing of motivationally salient emotional information (<xref rid="R20" ref-type="bibr">Hajcak et&#x000a0;al. 2009</xref>). This pattern emerged only when participants initially displayed angry or happy expressions, not neutral ones. Thus, the initial facial expression impacted the in-depth analysis of subsequent emotional expressions, likely due to increased (self-) relevance from the reciprocal nature of social interaction. This aligns with findings that LPP modulation is affected by perceived personal relevance (<xref rid="R8" ref-type="bibr">Bublatzky et&#x000a0;al. 2017</xref>) or task demands, such as evaluating an expression&#x02019;s emotionality (<xref rid="R43" ref-type="bibr">Schindler et&#x000a0;al. 2020</xref>). These results suggest an adaptive neural mechanism for processing social signals with heightened personal relevance during face-to-face interactions. Additionally, negative emotional expressions may enhance processing due to their indication of significant social consequences.</p><p>A main effect of emotional expression was observed in the EPN component and an additional cluster analysis found that happy and angry expressions differed from 100&#x02009;ms post detection point (<xref rid="s6" ref-type="sec">Supplementary Material</xref>). This aligns with prior findings on early processing of emotional expressions in components like the N170 and EPN (<xref rid="R25" ref-type="bibr">Hinojosa et&#x000a0;al. 2015</xref>, <xref rid="R45" ref-type="bibr">Schindler and Bublatzky 2020</xref>). Please note, however, that the nature of the present paradigm, i.e. display of the face stimulus preceding the onset of the facial emotional expression, precluded us from identifying clear P1 and N170 components in the ERP waveforms. Thus, the early effect observed here may not represent the initial processing of facial emotional expressions in components such as the P1 or N170 (which were reliably elicited when the virtual agent appeared, see <xref rid="s6" ref-type="sec">Supplementary Fig. S5</xref>). Nonetheless, the increased negativity to angry expressions indicates early-stage processing of emotional information in the presented faces. Importantly, this processing stage did not vary based on the emotional expression participants initially sent to the agent, suggesting that early structural and attentional processing of emotional information is not influenced by preceding social interactive context.</p><p>Contrary to ERP findings, EMG responses and valence ratings show a different interaction pattern between sending and receiving facial emotional expressions. Increased Zygomaticus mimicry and more positive evaluations of happy response expressions following initial smiles compared to frowns suggest an affiliative function of reciprocal facial expressions, where sending a smile enhances the rewarding value of a returned smile, thereby boosting the mimicry response (<xref rid="R46" ref-type="bibr">Sims et&#x000a0;al. 2012</xref>). This aligns with the mimicry in social context model (<xref rid="R23" ref-type="bibr">Hess 2021</xref>), which suggests that mimicry occurs only when sender and receiver have affiliative intentions. Our data support this model; sending a smile, communicating affiliative intent (<xref rid="R27" ref-type="bibr">Jack and Schyns 2015</xref>), increased mimicry responses compared to sending an angry expression, which communicates nonaffiliative intent. This interplay of mimicking and being mimicked may establish social bonds between partners (<xref rid="R49" ref-type="bibr">Wicher et&#x000a0;al. 2023</xref>). Furthermore, although we observed a general mimicry effect for nonaffiliative (angry) expressions, it was not influenced by the initial emotional expression. These findings imply that affiliative intentions enhance mimicry effects for affiliative but not nonaffiliative expressions, supporting a socio-affective mechanism for the reciprocal exchange of facial emotional expressions in social interactions.</p><p>Few studies have simultaneously measured EEG and facial EMG. <xref rid="R1" ref-type="bibr">Achaibou et al. (2008)</xref> found a link between the amplitude of the P1 and N170 components and the strength of the EMG mimicry response. Another study suggested that later face processing stages, such as the P3, also vary with mimicry (<xref rid="R33" ref-type="bibr">Kuang et al. 2021</xref>). Contrary, the present study found no associations between mimicry and early stages of facial emotion processing. Keep in mind, however, that our paradigm did not isolate early components like the P1 or N170. Instead, we found that Corrugator muscle activation correlated with LPP amplitude, possibly indicating that enhanced processing of motivationally salient emotional information is associated with Corrugator activation, consistent with <xref rid="R18" ref-type="bibr">Gr&#x000e8;zes et&#x000a0;al. (2013)</xref>, who found enhanced Corrugator activation for self-relevant processing. Furthermore, while no correlations were observed between Zygomaticus muscle activation and neural processes, Zygomaticus activation correlated with the evaluation of pleasantness, aligning with <xref rid="R42" ref-type="bibr">Sato et&#x000a0;al. (2013)</xref>. Again, this pattern suggests an affiliative mechanism enhancing mimicry and evaluative responses for affiliative signals on the one hand, and enhanced processing of emotionally salient and self-relevant information on the other hand.</p><p>While the present study is the first to investigate neural processes regarding the interactive exchange of facial emotional expressions, there are some limitations that need to be discussed. First, rather than real interactions, a &#x0201c;pseudo-interactive&#x0201d; hard-coded paradigm was used where participants were prompted to direct predetermined facial expressions at a virtual agent, who then responded after a random delay. This setup, while ensuring high experimental control, differs from real-life face-to-face interactions. For example, being instructed to display specific expressions may have reduced communicative intentionality and the feeling of social agency (<xref rid="R6" ref-type="bibr">Brandi et&#x000a0;al. 2020</xref>). Additionally, the 1500&#x02013;2000&#x02009;ms delay is longer than typical face-to-face interaction delays (<xref rid="R21" ref-type="bibr">Heerey and Crossley 2013</xref>). However, these delays were still linked to a relatively high experience of responsiveness in a previous study (<xref rid="R32" ref-type="bibr">Kroczek and M&#x000fc;hlberger 2023</xref>). While these limitations need to be considered, it should be noted that sending a facial expression affected processing of subsequent expressions despite being only &#x0201c;pseudo-interactive&#x0201d;, suggesting that one&#x02019;s actions create a meaningful social communicative context. Another limitation of this study is that early ERP components like the N170 elicited by facial emotional expressions could not be clearly identified, as we did not use static images or dynamic expressions starting at timepoint zero. Instead, we used naturalistic video stimuli where the agents appeared on the screen for several seconds before displaying emotional expressions. This approach was necessary for our experimental design and better reflects real-time social behavior. Note, however that we measured a typical N170 at video onset (see <xref rid="s6" ref-type="sec">Supplementary Fig. S5</xref>). Future studies could therefore isolate specific early face processing components by introducing an empty screen before the agent&#x02019;s response expression, allowing for clear onset measurement.</p><p>In summary, active engagement in face-to-face social interaction influences the neural and behavioral processing of facial emotional expressions. The results suggest that sending an emotional expression enhances higher-order evaluative processing of the response expression, while sending an affiliative expression boosts affiliative mimicry and pleasantness evaluation. These findings highlight that social interactive processes may trigger distinct mechanisms essential for navigating complex social environments.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="sup1" position="float" content-type="local-data"><label>nsaf001_Supp</label><media xlink:href="nsaf001_supp.zip"/></supplementary-material></sec></body><back><ack id="ack1"><title>Acknowledgements</title><p>We are thankful to Nadine Heller, Lisa-Marie Ebentheuer, and Nicole Renschler for their help in data acquisition.</p></ack><sec id="s5"><title>Funding</title><p>None declared.</p></sec><sec id="s6"><title>Supplementary data</title><p>
<xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> is available at <italic toggle="yes">SCAN</italic> online.</p></sec><sec sec-type="COI-statement" id="s7"><title>Conflict of interest:</title><p>The authors declare no conflict of interest.</p></sec><ref-list id="ref1"><title>References</title><ref id="R1"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Achaibou</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Pourtois</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Schwartz</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Vuilleumier</surname> &#x000a0;<given-names>P</given-names></string-name></person-group>. <article-title>Simultaneous recording of EEG and facial muscle reactions during spontaneous emotional mimicry</article-title>. <source><italic toggle="yes">Neuropsychologia</italic></source> &#x000a0;<year>2008</year>;<volume>46</volume>:<fpage>1104</fpage>&#x02013;<lpage>13</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.10.019</pub-id><pub-id pub-id-type="pmid">18068737</pub-id>
</mixed-citation></ref><ref id="R2"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bagherzadeh&#x02010;Azbari</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Lion</surname> &#x000a0;<given-names>CJ</given-names></string-name>, <string-name><surname>Stephani</surname> &#x000a0;<given-names>T</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The impact of emotional facial expressions on reflexive attention depends on the aim of dynamic gaze changes: an <sc>erp</sc> study</article-title>. <source><italic toggle="yes">Psychophysiology</italic></source> &#x000a0;<year>2023</year>;<volume>60</volume>:<page-range>e14202</page-range>. doi: <pub-id pub-id-type="doi">10.1111/psyp.14202</pub-id></mixed-citation></ref><ref id="R3"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Blechert</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Sheppes</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Di Tella</surname> &#x000a0;<given-names>C</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>See what you think: reappraisal modulates behavioral and neural responses to social stimuli</article-title>. <source><italic toggle="yes">Psychol Sci</italic></source> &#x000a0;<year>2012</year>;<volume>23</volume>:<fpage>346</fpage>&#x02013;<lpage>53</lpage>. doi: <pub-id pub-id-type="doi">10.1177/0956797612438559</pub-id><pub-id pub-id-type="pmid">22431908</pub-id>
</mixed-citation></ref><ref id="R4"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bolis</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Dumas</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Schilbach</surname> &#x000a0;<given-names>L</given-names></string-name></person-group>. <article-title>Interpersonal attunement in social interactions: from <italic toggle="yes">collective</italic> psychophysiology to <italic toggle="yes">inter-personalized</italic> psychiatry and beyond</article-title>. <source><italic toggle="yes">Philos Trans R Soc B</italic></source> &#x000a0;<year>2023</year>;<volume>378</volume>:<page-range>20210365</page-range>. doi: <pub-id pub-id-type="doi">10.1098/rstb.2021.0365</pub-id></mixed-citation></ref><ref id="R5"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bombari</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Schmid Mast</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Canadas</surname> &#x000a0;<given-names>E</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Studying social interactions through immersive virtual environment technology: virtues, pitfalls, and future challenges</article-title>. <source><italic toggle="yes">Front Psychol</italic></source> &#x000a0;<year>2015</year>;<volume>6</volume>:<fpage>1</fpage>&#x02013;<lpage>11</lpage>. doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2015.00869</pub-id><pub-id pub-id-type="pmid">25688217</pub-id>
</mixed-citation></ref><ref id="R6"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Brandi</surname> &#x000a0;<given-names>M-L</given-names></string-name>, <string-name><surname>Kaifel</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Lahnakoski</surname> &#x000a0;<given-names>JM</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>A naturalistic paradigm simulating gaze-based social interactions for the investigation of social agency</article-title>. <source><italic toggle="yes">Behav Res Methods</italic></source> &#x000a0;<year>2020</year>;<volume>52</volume>:<fpage>1044</fpage>&#x02013;<lpage>55</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13428-019-01299-x</pub-id><pub-id pub-id-type="pmid">31712998</pub-id>
</mixed-citation></ref><ref id="R7"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bublatzky</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Gerdes</surname> &#x000a0;<given-names>ABM</given-names></string-name>, <string-name><surname>White</surname> &#x000a0;<given-names>AJ</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Social and emotional relevance in face processing: happy faces of future interaction partners enhance the late positive potential</article-title>. <source><italic toggle="yes">Front Human Neurosci</italic></source> &#x000a0;<year>2014</year>;<volume>8</volume>:<fpage>1</fpage>&#x02013;<lpage>10</lpage>. doi: <pub-id pub-id-type="doi">10.3389/fnhum.2014.00493</pub-id></mixed-citation></ref><ref id="R8"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bublatzky</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Pittig</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Schupp</surname> &#x000a0;<given-names>HT</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Face-to-face: perceived personal relevance amplifies face processing</article-title>. <source><italic toggle="yes">Soc Cognit Affect Neurosci</italic></source> &#x000a0;<year>2017</year>;<volume>12</volume>:<fpage>811</fpage>&#x02013;<lpage>22</lpage>. doi: <pub-id pub-id-type="doi">10.1093/scan/nsx001</pub-id><pub-id pub-id-type="pmid">28158672</pub-id>
</mixed-citation></ref><ref id="R9"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chartrand Tanya</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Bargh John</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>The chameleon effect: the perception-behavior link and social interaction</article-title>. <source><italic toggle="yes">J Pers Soc Psychol</italic></source> &#x000a0;<year>1999</year>;<volume>76</volume>:<fpage>893</fpage>&#x02013;<lpage>910</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0022-3514.76.6.893</pub-id><pub-id pub-id-type="pmid">10402679</pub-id>
</mixed-citation></ref><ref id="R10"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Connor</surname> &#x000a0;<given-names>KM</given-names></string-name>, <string-name><surname>Davidson</surname> &#x000a0;<given-names>JRT</given-names></string-name>, <string-name><surname>Erik Churchill</surname> &#x000a0;<given-names>L</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Psychometric properties of the social phobia inventory (SPIN). New self-rating scale</article-title>. <source><italic toggle="yes">Br J Psychiatr</italic></source> &#x000a0;<year>2000</year>;<volume>176</volume>:<fpage>379</fpage>&#x02013;<lpage>86</lpage>. doi: <pub-id pub-id-type="doi">10.1192/bjp.176.4.379</pub-id></mixed-citation></ref><ref id="R11"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Delorme</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Makeig</surname> &#x000a0;<given-names>S</given-names></string-name></person-group>. <article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title>. <source><italic toggle="yes">J Neurosci Methods</italic></source> &#x000a0;<year>2004</year>;<volume>134</volume>:<fpage>9</fpage>&#x02013;<lpage>21</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id><pub-id pub-id-type="pmid">15102499</pub-id>
</mixed-citation></ref><ref id="R12"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Dimberg</surname> &#x000a0;<given-names>U</given-names></string-name>
</person-group>. <article-title>Facial reactions to facial expressions</article-title>. <source><italic toggle="yes">Psychophysiology</italic></source> &#x000a0;<year>1982</year>;<volume>19</volume>:<fpage>643</fpage>&#x02013;<lpage>47</lpage>. doi: <pub-id pub-id-type="doi">10.1111/j.1469-8986.1982.tb02516.x</pub-id><pub-id pub-id-type="pmid">7178381</pub-id>
</mixed-citation></ref><ref id="R13"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Dimberg</surname> &#x000a0;<given-names>U</given-names></string-name>, <string-name><surname>Thunberg</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Elmehed</surname> &#x000a0;<given-names>K</given-names></string-name></person-group>. <article-title>Unconscious facial reactions to emotional facial expressions</article-title>. <source><italic toggle="yes">Psychol Sci</italic></source> &#x000a0;<year>2000</year>;<volume>11</volume>:<fpage>86</fpage>&#x02013;<lpage>89</lpage>. doi: <pub-id pub-id-type="doi">10.1111/1467-9280.00221</pub-id><pub-id pub-id-type="pmid">11228851</pub-id>
</mixed-citation></ref><ref id="R14"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Fischer</surname> &#x000a0;<given-names>AH</given-names></string-name>, <string-name><surname>van Kleef</surname> &#x000a0;<given-names>GA</given-names></string-name></person-group>. <article-title>Where have all the people gone? A plea for including social interaction in emotion research</article-title>. <source><italic toggle="yes">Emotion Rev</italic></source> &#x000a0;<year>2010</year>;<volume>2</volume>:<fpage>208</fpage>&#x02013;<lpage>11</lpage>. doi: <pub-id pub-id-type="doi">10.1177/1754073910361980</pub-id></mixed-citation></ref><ref id="R15"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Fridlund</surname> &#x000a0;<given-names>AJ</given-names></string-name>, <string-name><surname>Cacioppo</surname> &#x000a0;<given-names>JT</given-names></string-name></person-group>. <article-title>Guidelines for human electromyographic research</article-title>. <source><italic toggle="yes">Psychophysiology</italic></source> &#x000a0;<year>1986</year>;<volume>23</volume>:<fpage>567</fpage>&#x02013;<lpage>89</lpage>. doi: <pub-id pub-id-type="doi">10.1111/j.1469-8986.1986.tb00676.x</pub-id><pub-id pub-id-type="pmid">3809364</pub-id>
</mixed-citation></ref><ref id="R16"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Frith</surname> &#x000a0;<given-names>CD</given-names></string-name>
</person-group>. <article-title>Role of facial expressions in social interactions</article-title>. <source><italic toggle="yes">Philos Trans R Soc B</italic></source> &#x000a0;<year>2009</year>;<volume>364</volume>:<fpage>3453</fpage>&#x02013;<lpage>58</lpage>. doi: <pub-id pub-id-type="doi">10.1098/rstb.2009.0142</pub-id></mixed-citation></ref><ref id="R17"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Greenhouse</surname> &#x000a0;<given-names>SW.</given-names></string-name>, <string-name><surname>Geisser</surname> &#x000a0;<given-names>S.</given-names></string-name></person-group> &#x000a0;<article-title>On methods in the analysis of profile data</article-title>. <source><italic toggle="yes">Psychometrika</italic></source> &#x000a0;<year>1959</year>;<volume>24</volume>: <fpage>95</fpage>&#x02013;<lpage>112</lpage>.</mixed-citation></ref><ref id="R18"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gr&#x000e8;zes</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Philip</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Chadwick</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Self-relevance appraisal influences facial reactions to emotional body expressions</article-title>. <source><italic toggle="yes">PLoS ONE</italic></source> &#x000a0;<year>2013</year>;<volume>8</volume>:<page-range>e55885</page-range>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0055885</pub-id></mixed-citation></ref><ref id="R19"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hadley</surname> &#x000a0;<given-names>LV</given-names></string-name>, <string-name><surname>Naylor</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Hamilton</surname> &#x000a0;<given-names>AFDC</given-names></string-name></person-group>. <article-title>A review of theories and methods in the science of face-to-face social interaction</article-title>. <source><italic toggle="yes">Nat Rev Psychol</italic></source> &#x000a0;<year>2022</year>;<volume>1</volume>:<fpage>42</fpage>&#x02013;<lpage>54</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s44159-021-00008-w</pub-id></mixed-citation></ref><ref id="R20"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hajcak</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Dunning</surname> &#x000a0;<given-names>JP</given-names></string-name>, <string-name><surname>Foti</surname> &#x000a0;<given-names>D</given-names></string-name></person-group>. <article-title>Motivated and controlled attention to emotion: time-course of the late positive potential</article-title>. <source><italic toggle="yes">Clin Neurophysiol</italic></source> &#x000a0;<year>2009</year>;<volume>120</volume>:<fpage>505</fpage>&#x02013;<lpage>10</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.clinph.2008.11.028</pub-id><pub-id pub-id-type="pmid">19157974</pub-id>
</mixed-citation></ref><ref id="R21"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Heerey</surname> &#x000a0;<given-names>EA</given-names></string-name>, <string-name><surname>Crossley</surname> &#x000a0;<given-names>HM</given-names></string-name></person-group>. <article-title>Predictive and reactive mechanisms in smile reciprocity</article-title>. <source><italic toggle="yes">Psychol Sci</italic></source> &#x000a0;<year>2013</year>;<volume>24</volume>:<fpage>1446</fpage>&#x02013;<lpage>55</lpage>. doi: <pub-id pub-id-type="doi">10.1177/0956797612472203</pub-id><pub-id pub-id-type="pmid">23744875</pub-id>
</mixed-citation></ref><ref id="R22"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Herbert</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Sf&#x000e4;rlea</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Blumenthal</surname> &#x000a0;<given-names>T</given-names></string-name></person-group>.<article-title>Your emotion or mine: labeling feelings alters emotional face perception&#x02014;an ERP study on automatic and intentional affect labeling</article-title>. <source><italic toggle="yes">Front Human Neurosci</italic></source> &#x000a0;<year>2013</year>;<volume>7</volume>:<page-range>7</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fnhum.2013.00007</pub-id></mixed-citation></ref><ref id="R23"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hess</surname> &#x000a0;<given-names>U</given-names></string-name>
</person-group>. <article-title>Who to whom and why: the social nature of emotional mimicry</article-title>. <source><italic toggle="yes">Psychophysiology</italic></source> &#x000a0;<year>2021</year>;<volume>58</volume>:<fpage>1</fpage>&#x02013;<lpage>11</lpage>. doi: <pub-id pub-id-type="doi">10.1111/psyp.13675</pub-id></mixed-citation></ref><ref id="R24"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hess</surname> &#x000a0;<given-names>U</given-names></string-name>, <string-name><surname>Bourgeois</surname> &#x000a0;<given-names>P</given-names></string-name></person-group>. <article-title>You smile-I smile: emotion expression in social interaction</article-title>. <source><italic toggle="yes">Biological Psychol</italic></source> &#x000a0;<year>2010</year>;<volume>84</volume>:<fpage>514</fpage>&#x02013;<lpage>20</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.biopsycho.2009.11.001</pub-id></mixed-citation></ref><ref id="R25"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hinojosa</surname> &#x000a0;<given-names>JA</given-names></string-name>, <string-name><surname>Mercado</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Carreti&#x000e9;</surname> &#x000a0;<given-names>L</given-names></string-name></person-group>. <article-title>N170 sensitivity to facial expression: a meta-analysis</article-title>. <source><italic toggle="yes">Neurosci Biobehav Rev</italic></source> &#x000a0;<year>2015</year>;<volume>55</volume>:<fpage>498</fpage>&#x02013;<lpage>509</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neubiorev.2015.06.002</pub-id><pub-id pub-id-type="pmid">26067902</pub-id>
</mixed-citation></ref><ref id="R26"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Holm</surname> &#x000a0;<given-names>S.</given-names></string-name>
</person-group> &#x000a0;<article-title>A simple sequentially rejective multiple test procedure</article-title>. <source><italic toggle="yes">Scandinavian Journal of Statistics</italic></source> &#x000a0;<year>1979</year>;<volume>6</volume>:<fpage>65</fpage>&#x02013;<lpage>70</lpage>.</mixed-citation></ref><ref id="R27"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Jack</surname> &#x000a0;<given-names>RE</given-names></string-name>, <string-name><surname>Schyns</surname> &#x000a0;<given-names>PG</given-names></string-name></person-group>. <article-title>The human face as a dynamic tool for social communication</article-title>. <source><italic toggle="yes">Curr Biol</italic></source> &#x000a0;<year>2015</year>;<volume>25</volume>:<fpage>R621</fpage>&#x02013;<lpage>R634</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2015.05.052</pub-id><pub-id pub-id-type="pmid">26196493</pub-id>
</mixed-citation></ref><ref id="R28"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Krasowski</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Schindler</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Bruchmann</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Electrophysiological responses to negative evaluative person-knowledge: effects of individual differences</article-title>. <source><italic toggle="yes">Cognit Affect Behav Neurosci</italic></source> &#x000a0;<year>2021</year>;<volume>21</volume>:<fpage>822</fpage>&#x02013;<lpage>36</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13415-021-00894-w</pub-id><pub-id pub-id-type="pmid">33846952</pub-id>
</mixed-citation></ref><ref id="R29"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kroczek</surname> &#x000a0;<given-names>LOH</given-names></string-name>, <string-name><surname>Lingnau</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Schwind</surname> &#x000a0;<given-names>V</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Angry facial expressions bias towards aversive actions</article-title>. <source><italic toggle="yes">PLOS ONE</italic></source> &#x000a0;<year>2021</year>;<volume>16</volume>:<page-range>e0256912</page-range>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0256912</pub-id></mixed-citation></ref><ref id="R30"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kroczek</surname> &#x000a0;<given-names>LOH</given-names></string-name>, <string-name><surname>Lingnau</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Schwind</surname> &#x000a0;<given-names>V</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Observers predict actions from facial emotional expressions during real-time social interactions</article-title>. <source><italic toggle="yes">Behav Brain Res</italic></source> &#x000a0;<year>2024</year>;<volume>471</volume>:<page-range>115126</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.bbr.2024.115126</pub-id></mixed-citation></ref><ref id="R31"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kroczek</surname> &#x000a0;<given-names>LOH</given-names></string-name>, <string-name><surname>M&#x000fc;hlberger</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>Returning a smile: initiating a social interaction with a facial emotional expression influences the evaluation of the expression received in return</article-title>. <source><italic toggle="yes">Biological Psychol</italic></source> &#x000a0;<year>2022</year>;<volume>175</volume>:<page-range>108453</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.biopsycho.2022.108453</pub-id></mixed-citation></ref><ref id="R32"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kroczek</surname> &#x000a0;<given-names>LOH</given-names></string-name>, <string-name><surname>M&#x000fc;hlberger</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>Time to smile: how onset asynchronies between reciprocal facial expressions influence the experience of responsiveness of a virtual agent</article-title>. <source><italic toggle="yes">J Nonverbal Behav</italic></source> &#x000a0;<year>2023</year>;<volume>47</volume>:<fpage>345</fpage>&#x02013;<lpage>60</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s10919-023-00430-z</pub-id></mixed-citation></ref><ref id="R33"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kuang</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Li</surname> &#x000a0;<given-names>X</given-names></string-name>, <string-name><surname>Li</surname> &#x000a0;<given-names>X</given-names></string-name></person-group> &#x000a0;<etal>et al</etal>. <article-title>The effect of eye gaze direction on emotional mimicry: A multimodal study with electromyography and electroencephalography</article-title>. <source><italic toggle="yes">NeuroImage</italic></source> &#x000a0;<year>2021</year>;<volume>226</volume>:<fpage>117604</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117604</pub-id><pub-id pub-id-type="pmid">33278584</pub-id>
</mixed-citation></ref><ref id="R34"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lange</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Heerdink</surname> &#x000a0;<given-names>MW</given-names></string-name>, <string-name><surname>Van Kleef</surname> &#x000a0;<given-names>GA</given-names></string-name></person-group>. <article-title>Reading emotions, reading people: emotion perception and inferences drawn from perceived emotions</article-title>. <source><italic toggle="yes">Curr Opini Psychol</italic></source> &#x000a0;<year>2022</year>;<volume>43</volume>:<fpage>85</fpage>&#x02013;<lpage>90</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.copsyc.2021.06.008</pub-id></mixed-citation></ref><ref id="R35"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lehmann</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Bolis</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Friston</surname> &#x000a0;<given-names>KJ</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>An active-inference approach to second-person neuroscience</article-title>. <source><italic toggle="yes">Perspect Psychol Sci</italic></source> &#x000a0;<year>2024</year>;<volume>19</volume>:<fpage>931</fpage>&#x02013;<lpage>51</lpage>. doi: <pub-id pub-id-type="doi">10.1177/17456916231188000</pub-id><pub-id pub-id-type="pmid">37565656</pub-id>
</mixed-citation></ref><ref id="R36"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Oostenveld</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Fries</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Maris</surname> &#x000a0;<given-names>E</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source><italic toggle="yes">Comput Intell Neurosci</italic></source> &#x000a0;<year>2011</year>;<volume>2011</volume>:<fpage>1</fpage>&#x02013;<lpage>9</lpage>. doi: <pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21837235</pub-id>
</mixed-citation></ref><ref id="R37"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pell</surname> &#x000a0;<given-names>MD</given-names></string-name>, <string-name><surname>Kotz</surname> &#x000a0;<given-names>SA</given-names></string-name>, <string-name><surname>Rustichini</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>On the time course of vocal emotion recognition</article-title>. <source><italic toggle="yes">PLoS ONE</italic></source> &#x000a0;<year>2011</year>;<volume>6</volume>:<page-range>e27256</page-range>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0027256</pub-id></mixed-citation></ref><ref id="R38"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pelli</surname> &#x000a0;<given-names>DG</given-names></string-name>
</person-group>. <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>. <source><italic toggle="yes">Spat Vis</italic></source> &#x000a0;<year>1997</year>;<volume>10</volume>:<fpage>437</fpage>&#x02013;<lpage>42</lpage>. doi: <pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id><pub-id pub-id-type="pmid">9176953</pub-id>
</mixed-citation></ref><ref id="R39"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Redcay</surname> &#x000a0;<given-names>E</given-names></string-name>, <string-name><surname>Schilbach</surname> &#x000a0;<given-names>L</given-names></string-name></person-group>. <article-title>Using second-person neuroscience to elucidate the mechanisms of social interaction</article-title>. <source><italic toggle="yes">Nat Rev Neurosci</italic></source> &#x000a0;<year>2019</year>;<volume>20</volume>:<fpage>495</fpage>&#x02013;<lpage>505</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41583-019-0179-4</pub-id><pub-id pub-id-type="pmid">31138910</pub-id>
</mixed-citation></ref><ref id="R40"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Risko</surname> &#x000a0;<given-names>EF</given-names></string-name>, <string-name><surname>Laidlaw</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Freeth</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Social attention with real versus reel stimuli: toward an empirical approach to concerns about ecological validity</article-title>. <source><italic toggle="yes">Front Human Neurosci</italic></source> &#x000a0;<year>2012</year>;<volume>6</volume>:<page-range>6</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fnhum.2012.00006</pub-id></mixed-citation></ref><ref id="R41"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Salazar K&#x000e4;mpf</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Liebermann</surname> &#x000a0;<given-names>H</given-names></string-name>, <string-name><surname>Kerschreiter</surname> &#x000a0;<given-names>R</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Disentangling the sources of mimicry: social relations analyses of the link between mimicry and liking</article-title>. <source><italic toggle="yes">Psychol Sci</italic></source> &#x000a0;<year>2018</year>;<volume>29</volume>:<fpage>131</fpage>&#x02013;<lpage>38</lpage>. doi: <pub-id pub-id-type="doi">10.1177/0956797617727121</pub-id><pub-id pub-id-type="pmid">29083989</pub-id>
</mixed-citation></ref><ref id="R42"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sato</surname> &#x000a0;<given-names>W</given-names></string-name>, <string-name><surname>Fujimura</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Kochiyama</surname> &#x000a0;<given-names>T</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Relationships among facial mimicry, emotional experience, and emotion recognition</article-title>. <source><italic toggle="yes">PLoS ONE</italic></source> &#x000a0;<year>2013</year>;<volume>8</volume>:<page-range>e57889</page-range>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0057889</pub-id></mixed-citation></ref><ref id="R43"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schindler</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Bruchmann</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Steinweg</surname> &#x000a0;<given-names>A-L</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Attentional conditions differentially affect early, intermediate and late neural responses to fearful and neutral faces</article-title>. <source><italic toggle="yes">Soc Cognit Affect Neurosci</italic></source> &#x000a0;<year>2020</year>;<volume>15</volume>:<fpage>765</fpage>&#x02013;<lpage>74</lpage>. doi: <pub-id pub-id-type="doi">10.1093/scan/nsaa098</pub-id><pub-id pub-id-type="pmid">32701163</pub-id>
</mixed-citation></ref><ref id="R44"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schindler</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Bruchmann</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Straube</surname> &#x000a0;<given-names>T</given-names></string-name></person-group>. <article-title>Beyond facial expressions: a systematic review on effects of emotional relevance of faces on the N170</article-title>. <source><italic toggle="yes">Neurosci Biobehav Rev</italic></source> &#x000a0;<year>2023</year>;<volume>153</volume>:<page-range>105399</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.neubiorev.2023.105399</pub-id></mixed-citation></ref><ref id="R45"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schindler</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Bublatzky</surname> &#x000a0;<given-names>F</given-names></string-name></person-group>. <article-title>Attention and emotion: an integrative review of emotional face processing as a function of attention</article-title>. <source><italic toggle="yes">Cortex</italic></source> &#x000a0;<year>2020</year>;<volume>130</volume>:<fpage>362</fpage>&#x02013;<lpage>86</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cortex.2020.06.010</pub-id><pub-id pub-id-type="pmid">32745728</pub-id>
</mixed-citation></ref><ref id="R46"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sims</surname> &#x000a0;<given-names>TB</given-names></string-name>, <string-name><surname>Van Reekum</surname> &#x000a0;<given-names>CM</given-names></string-name>, <string-name><surname>Johnstone</surname> &#x000a0;<given-names>T</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>How reward modulates mimicry: EMG evidence of greater facial mimicry of more rewarding happy faces</article-title>. <source><italic toggle="yes">Psychophysiology</italic></source> &#x000a0;<year>2012</year>;<volume>49</volume>:<fpage>998</fpage>&#x02013;<lpage>1004</lpage>. doi: <pub-id pub-id-type="doi">10.1111/j.1469-8986.2012.01377.x</pub-id><pub-id pub-id-type="pmid">22563935</pub-id>
</mixed-citation></ref><ref id="R47"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Van Kleef</surname> &#x000a0;<given-names>GA</given-names></string-name>
</person-group>. <article-title>How emotions regulate social life: the emotions as social information (EASI) model</article-title>. <source><italic toggle="yes">Curr Dir Psychol Sci</italic></source> &#x000a0;<year>2009</year>;<volume>18</volume>:<fpage>184</fpage>&#x02013;<lpage>88</lpage>. doi: <pub-id pub-id-type="doi">10.1111/j.1467-8721.2009.01633.x</pub-id></mixed-citation></ref><ref id="R48"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Van Kleef</surname> &#x000a0;<given-names>GA</given-names></string-name>, <string-name><surname>Van Doorn</surname> &#x000a0;<given-names>EA</given-names></string-name>, <string-name><surname>Heerdink</surname> &#x000a0;<given-names>MW</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Emotion is for influence</article-title>. <source><italic toggle="yes">Eur Rev Social Psychol</italic></source> &#x000a0;<year>2011</year>;<volume>22</volume>:<fpage>114</fpage>&#x02013;<lpage>63</lpage>. doi: <pub-id pub-id-type="doi">10.1080/10463283.2011.627192</pub-id></mixed-citation></ref><ref id="R49"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Wicher</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Farmer</surname> &#x000a0;<given-names>H</given-names></string-name>, <string-name><surname>Hamilton</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <source>Cognitive mechanisms of being imitated</source> [<comment>Preprint</comment>]. <publisher-name>PsyArXiv</publisher-name>, <year>2023</year> doi: <pub-id pub-id-type="doi">10.31234/osf.io/yxzps</pub-id></mixed-citation></ref><ref id="R50"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wieser</surname> &#x000a0;<given-names>MJ</given-names></string-name>, <string-name><surname>Brosch</surname> &#x000a0;<given-names>T</given-names></string-name></person-group>. <article-title>Faces in context: a review and systematization of contextual influences on affective face processing</article-title>. <source><italic toggle="yes">Front Psychol</italic></source> &#x000a0;<year>2012</year>;<volume>3</volume>:<fpage>1</fpage>&#x02013;<lpage>13</lpage>. doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2012.00471</pub-id><pub-id pub-id-type="pmid">22279440</pub-id>
</mixed-citation></ref></ref-list></back></article>