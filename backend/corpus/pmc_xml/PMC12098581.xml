<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC12098581</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2025.1474292</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Comparing factor mixture modeling and conditional Gaussian mixture variational autoencoders for cognitive profile clustering</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Orsoni</surname><given-names>Matteo</given-names></name><xref rid="c001" ref-type="corresp">
<sup>*</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/1178280/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Giovagnoli</surname><given-names>Sara</given-names></name><uri xlink:href="http://loop.frontiersin.org/people/704431/overview"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Garofalo</surname><given-names>Sara</given-names></name><uri xlink:href="http://loop.frontiersin.org/people/228111/overview"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Mazzoni</surname><given-names>Noemi</given-names></name><uri xlink:href="http://loop.frontiersin.org/people/627192/overview"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Spinoso</surname><given-names>Matilde</given-names></name><uri xlink:href="http://loop.frontiersin.org/people/2968294/overview"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Benassi</surname><given-names>Mariagrazia</given-names></name><uri xlink:href="http://loop.frontiersin.org/people/162779/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/funding-acquisition/"/><role content-type="https://credit.niso.org/contributor-roles/resources/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib></contrib-group><aff><institution>Department of Psychology, University of Bologna</institution>, <addr-line>Bologna</addr-line>, <country>Italy</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Ioannis Tsaousis, National and Kapodistrian University of Athens, Greece</p></fn><fn fn-type="edited-by"><p>Reviewed by: Srikanth Thudumu, Deakin University, Australia</p><p>Angelos Markos, Democritus University of Thrace, Greece</p></fn><corresp id="c001">*Correspondence: Matteo Orsoni <email>matteo.orsoni2@unibo.it</email></corresp></author-notes><pub-date pub-type="epub"><day>09</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>16</volume><elocation-id>1474292</elocation-id><history><date date-type="received"><day>01</day><month>8</month><year>2024</year></date><date date-type="accepted"><day>21</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025 Orsoni, Giovagnoli, Garofalo, Mazzoni, Spinoso and Benassi.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Orsoni, Giovagnoli, Garofalo, Mazzoni, Spinoso and Benassi</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><sec><title>Introduction</title><p>Understanding individual cognitive profiles is crucial for developing personalized educational interventions, as cognitive differences can significantly impact how students learn. While traditional methods like factor mixture modeling (FMM) have proven robust for identifying latent cognitive structures, recent advancements in deep learning may offer the potential to capture more intricate and complex cognitive patterns.</p></sec><sec><title>Methods</title><p>This study compares FMM (specifically, FMM-1 and FMM-2 models using age as a covariate) with a Conditional Gaussian Mixture Variational Autoencoder (CGMVAE). The comparison utilizes six cognitive dimensions obtained from the PROFFILO assessment game.</p></sec><sec><title>Results</title><p>The FMM-1 model, identified as the superior FMM solution, yielded two well-separated clusters (Silhouette score = 0.959). These clusters represent distinct average cognitive levels, with age significantly predicting class membership. In contrast, the CGMVAE identified ten more nuanced cognitive profiles, exhibiting clear developmental trajectories across different age groups. Notably, one dominant cluster (Cluster 9) showed an increase in representation from 44 to 54% with advancing age, indicating a normative developmental pattern. Other clusters displayed diverse profiles, ranging from subtle domain-specific strengths to atypical profiles characterized by significant deficits balanced by compensatory abilities.</p></sec><sec><title>Discussion</title><p>These findings highlight a trade-off between the methodologies. FMM provides clear, interpretable groupings suitable for broad classification purposes. Conversely, CGMVAE reveals subtle, non-linear variations in cognitive profiles, potentially reflecting complex developmental pathways. Despite practical challenges associated with CGMVAE's complexity and potential cluster overlap, its capacity to uncover nuanced cognitive patterns demonstrates significant promise for informing the development of highly tailored educational strategies.</p></sec></abstract><kwd-group><kwd>variational autoencoders</kwd><kwd>clustering</kwd><kwd>machine learning</kwd><kwd>cognitive profiles</kwd><kwd>factor mixture modeling</kwd></kwd-group><funding-group><award-group><funding-source id="cn001"><institution-wrap><institution>Ministero dell&#x02019;Universit&#x000e0; e della Ricerca</institution><institution-id institution-id-type="doi">10.13039/501100021856</institution-id></institution-wrap></funding-source><award-id award-type="contract" rid="cn001">20209WKCLL</award-id></award-group><funding-statement>The author(s) declare that financial support was received for the research and/or publication of this article. This research was funded by the Italian Ministry of Research and University, Progetti di Ricerca di Rilevante Interesse Nazionale (PRIN)&#x02014;Bando 2020 (Protocol 20209WKCLL; &#x0201c;Computerized, Adaptive, and Personalized Assessment of Executive Functions and Fluid Intelligence&#x0201d;).</funding-statement></funding-group><counts><fig-count count="5"/><table-count count="2"/><equation-count count="4"/><ref-count count="40"/><page-count count="14"/><word-count count="9002"/></counts><custom-meta-group><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Quantitative Psychology and Measurement</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="s1"><title>1 Introduction</title><p>Individual cognitive profiles, encompassing unique strengths and weaknesses in domains such as memory, attention, processing speed, and reasoning, play a pivotal role in shaping the learning process (Altun, <xref rid="B2" ref-type="bibr">2016</xref>; Nesayan et al., <xref rid="B23" ref-type="bibr">2018</xref>; Webster, <xref rid="B36" ref-type="bibr">2002</xref>). Research indicates that personalized instructional strategies aligned with these cognitive differences can significantly enhance educational outcomes. For instance, adaptive learning strategies that cater to individual modalities, such as employing diagrams for visual learners or discussion-based approaches for verbal learners, have been shown to be effective (Swanson and Hoskyn, <xref rid="B33" ref-type="bibr">2001</xref>). Similarly, learners with robust working memory capacities tend to excel in multitasking and complex problem-solving, whereas those with deficits benefit from structured and sequential methods (Gathercole et al., <xref rid="B9" ref-type="bibr">2008</xref>). In addition, differences in processing speed suggest that traditional time-constrained assessments may disadvantage some students, highlighting the need for flexible pacing and differentiated instruction (Kail and Hall, <xref rid="B15" ref-type="bibr">1994</xref>). Moreover, when learners are aware of their own cognitive strengths and limitations, they can leverage metacognitive strategies to improve self-regulated learning and academic performance (Zimmerman and Schunk, <xref rid="B40" ref-type="bibr">2011</xref>). Finally, recognizing and addressing individual cognitive profiles enables the development of personalized teaching strategies that optimize learning experiences, thereby fostering academic success and increased self-efficacy (Orsoni et al., <xref rid="B26" ref-type="bibr">2023</xref>). Clustering constitutes a pivotal domain within unsupervised pattern recognition and can be a relevant approach in finding suitable students cognitive profile (Orsoni et al., <xref rid="B26" ref-type="bibr">2023</xref>). Its fundamental purpose is to partition a collection of unlabeled samples into distinct subsets according to a predefined objective function. The primary aim is to minimize inter-partition similarity while concurrently enhancing intra-partition similarity (Jayanth Krishnan and Mitra, <xref rid="B13" ref-type="bibr">2022</xref>). However, the effectiveness of clustering solutions depends on multiple factors, including data characteristics and algorithm selection. Recently, confirmatory factor analysis (CFA) combined with latent profile analysis (LPA) (Schuster and Krogh, <xref rid="B32" ref-type="bibr">2021</xref>) and factor mixture modeling (FMM) (Lubke and Muth&#x000e9;n, <xref rid="B21" ref-type="bibr">2005</xref>) has emerged as a robust approach for classifying learners' cognitive profiles by identifying underlying cognitive constructs and grouping individuals into latent classes based on their performance data. This methodology has been applied in various domains, including gifted education (Mammadov et al., <xref rid="B22" ref-type="bibr">2016</xref>), in the analysis of self-regulated learning strategies in university students (Ning and Downing, <xref rid="B24" ref-type="bibr">2014</xref>), and in the identification of preclinical cognitive phenotypes for Alzheimer's disease (Hayden et al., <xref rid="B11" ref-type="bibr">2014</xref>). FMM, by following this line, merges continuous factor structures with categorical latent class distinctions. A work of Gomez and Vance (<xref rid="B10" ref-type="bibr">2014</xref>) presented the application of this method in the classification of cognitive heterogeneity in the co-occurrence of the childhood syndromes. Overall, the integration of CFA, LPA, and FMM facilitates a nuanced understanding of cognitive heterogeneity, thereby informing the development of personalized educational and psychological interventions. However, recent advances in deep unsupervised learning have significantly advanced latent class analysis, with methods such as autoencoders (AEs) and variational autoencoders (VAEs) proving particularly effective at enhancing clustering performance. These techniques encode high-dimensional data into compact, interpretable latent spaces that clarify intrinsic group structures (Lin et al., <xref rid="B20" ref-type="bibr">2020</xref>; Yan et al., <xref rid="B39" ref-type="bibr">2023</xref>). For instance, Eskandarnia et al. (<xref rid="B8" ref-type="bibr">2022</xref>) demonstrated the utility of combining dimensionality reduction with clustering algorithms for smart meter load profiling, while Peng et al. (<xref rid="B29" ref-type="bibr">2018</xref>) employed a structured autoencoder framework to map data into non-linear latent spaces optimized for subspace clustering. Further refinements to these architectures include Jiang et al. (<xref rid="B14" ref-type="bibr">2017</xref>), who introduced variational deep embedding (VaDE), integrating VAE with Gaussian mixture models to improve cluster separability. Subsequent innovations in VAE-based models, such as Kopf et al. (<xref rid="B19" ref-type="bibr">2021</xref>) mixture-of-experts (MoE), and Sarkar et al. (<xref rid="B31" ref-type="bibr">2020</xref>) Gaussian Mixture Variational Autoencoder and hybrid Conditional GMVAE for game design applications, have expanded the methodological toolkit for domain-specific clustering challenges.</p><p>This study compares factor mixture modeling (FMM) and Conditional Gaussian Mixture Variational Autoencoder (CGMVAE) as methods for analyzing cognitive profiles derived from six cognitive dimensions across multiple age groups. Notably, this research presents the first application of the CGMVAE architecture within the domain of cognitive profile analysis. Our core objectives are to (1) evaluate how effectively FMM and CGMVAE identify underlying latent cognitive structures, (2) characterize the distinct cognitive profiles each method reveals, and (3) analyze age-related trends in the distribution of these profiles.</p></sec><sec sec-type="methods" id="s2"><title>2 Methods</title><sec><title>2.1 Participants and instrument</title><p>All procedures adhered to the ethical standards established by national committees on human experimentation and were in accordance with the Helsinki Declaration of 1975, as revised in 2008. Approval for the study was obtained from the University of Bologna Bioethics Committee. Both parents and youths provided written and online informed consent to participate in the study.</p><p>A total of <italic>n</italic> = 2570 participants with an age range between 4 and 16 years old were considered eligible for subsequent analyses. <xref rid="F1" ref-type="fig">Figure 1</xref> displays the distribution of sample ages. Due to the presence of a trimodal pattern in age variable, both the FMM and CGMVAE methodologies employed a binned approach for the conditioned variable. Participant ages were then grouped into three categories: [0&#x02013;8), [8&#x02013;12), and [12&#x02013;100), containing <italic>n</italic> = 670, <italic>n</italic> = 821, and <italic>n</italic> = 1, 079 samples, respectively.</p><fig position="float" id="F1"><label>Figure 1</label><caption><p>Bar chart of the sample age distribution.</p></caption><graphic xlink:href="fpsyg-16-1474292-g0001" position="float"/></fig><p>The cognitive abilities of all paticipants were evaluated using an online digital game called PROFFILO, specifically designed for assessing students' cognitive profiles (Orsoni et al., <xref rid="B26" ref-type="bibr">2023</xref>). The instrument convergent validity and specifications were already included in other published studies (Orsoni et al., <xref rid="B25" ref-type="bibr">2021</xref>, <xref rid="B26" ref-type="bibr">2023</xref>). In brief, the PROFFILO assessment comprised six distinct sub-tests (games), each tailored to evaluate a specific cognitive function, namely, logical reasoning, visuospatial attention, motion perception, phonological awareness, verbal comprehension, and working memory, and lasted 20&#x02013;25 min per participant. A total of 54 items have been administered to all the participants divided as follows:</p><list list-type="bullet"><list-item><p><bold>Logical reasoning:</bold> 15 items. The test consists of a series of visual pattern matrices, each with one missing part. The task for the test taker is to identify the missing piece from multiple choices. The data are binary, representing 0 for incorrect and 1 for correct answers.</p></list-item><list-item><p><bold>Visuospatial attention:</bold> 3 items. The task requires the individual to focus their attention to specific visual elements in space, by responding to specific cues while ignoring distractions. The data is continuous in a range between 0 and 1.</p></list-item><list-item><p><bold>Motion perception:</bold> 5 items. In the current task, participants have to recognize directions of moving stimuli, obtained from white dots moving against a black background. This task allows to assess the subject's motion perception skills. The data are binary, representing 0 for incorrect and 1 for correct answers.</p></list-item><list-item><p><bold>Phonological awareness:</bold> 13 items. In the tasks, the test taker is presented with two auditory stimuli, and the task requires selecting the word that corresponds to a word that actually exist, while disregarding the non-word counterpart. The data are binary, representing 0 for incorrect and 1 for correct answers.</p></list-item><list-item><p><bold>Verbal comprehension:</bold> 17 items. The test involves the presentation of spoken sentences or phrases to the test taker, who is then required to select a corresponding picture that accurately represents the meaning of the presented linguistic content. The data are binary, representing 0 for incorrect and 1 for correct answers.</p></list-item><list-item><p><bold>Working memory:</bold> 1 item. The test involves presenting a participant with a sequence of numbers and then asking them to recall the items in reverse order. The length of the sequence increases proportionally with the participant's performance improvement. The test is interrupted after two consecutive errors. The data is continuous, ranging from a minimum value of 0.</p></list-item></list></sec><sec><title>2.2 Factor mixture modeling</title><p>Following the seminal work by Lubke and Muth&#x000e9;n (<xref rid="B21" ref-type="bibr">2005</xref>), factor mixture modeling (FMM) combines factor analysis and latent class analysis to identify latent cognitive structures and classify individuals into distinct subgroups. In this study, we employed a FMM approach to investigate the underlying structure of the students cogntive profiles. By following the work of Clark et al. (<xref rid="B6" ref-type="bibr">2013</xref>), two FMM specifications were estimated: FMM-1 and FMM-2. In FMM-1, or the latent class factor analytic (LCFA) model, the item thresholds and factor loadings were constrained to be invariant across latent classes, with the only class-specific parameter being the factor mean &#x003b1;. In this specification, the factor covariance matrix &#x003a8; was fixed at zero, thereby assuming no within-class heterogeneity beyond differences in factor location.</p><p>By contrast, FMM-2, or mixture factor analysis, allowed for a more flexible representation of the latent structure. Although it maintained invariant item thresholds and factor loadings as in FMM-1, instead of setting the factor variances and covariances to zero, they are now freely estimated in each of the classes. This modification enabled the model to capture also non-normal distributions of the latent variable, thus accommodating variability both between and within diagnostic categories.</p><p>As discussed in Clark et al. (<xref rid="B6" ref-type="bibr">2013</xref>), we adopted a systematic, multi-step approach to develop the FMM-1 and FFM-2, drawing from latent class analysis (LCA) and factor analysis (FA) as foundational paradigms. Initially, LCA models with an increasing number of classes and FA models with an increasing number of factors were fitted. This dual fitting strategy provided baseline solutions against which the more complex FMM could be compared, thereby informing decisions about model complexity and indicating when further increases in the number of classes or factors might be unnecessary. The next phase involved fitting a simple FMM with two latent classes and one latent factor. Subsequent models were estimated by gradually increasing the number of classes. This step allowed for an initial exploration of how class heterogeneity is captured in the presence of a single factor and served as a starting point for progressively building model complexity. Building on the simple FMM, the number of latent factors was increased from one to two, and the number of classes was again systematically increased in parallel. This iterative process, first varying the number of classes, then the number of factors, aimed to identify the point at which the model best captured the underlying structure of the data. The same pattern of complexity escalation was applied for both FMM-1 and FMM-2 models. To determine when to stop increasing the number of classes and factors was guided by the optimal number of classes from the LCA and the optimal number of factors from the FA, ensuring a parsimonious yet explanatory solution. After exploring different combinations of classes and factors, the best-fitting FMM was selected based on Bayesian information criteria (BIC), and Akaike information criterion (AIC) indices. Finally, the best FMM was compared with the optimal LCA and FA models. This comparison was essential to ensure that the integrated FMM solution provided superior fit and a more parsimonious representation of the data relative to the simpler models.</p><p>Moreover, we incorporated the binned age covariate in the models. This approach, as outlined by Lubke and Muth&#x000e9;n (<xref rid="B21" ref-type="bibr">2005</xref>), allowed us to model the effect on latent class membership via a multinomial logistic regression framework. Specifically, the covariate is used to predict class probabilities through regression parameters, which are updated during the M-step of the Expectation-Maximization (EM) algorithm. This approach enables us to account for observed heterogeneity by directly influencing latent class assignments while retaining the overall structure of the FMM.</p></sec><sec><title>2.3 Conditional gaussian mixture variational autoencoder</title><p>Variational autoencoders (VAEs) have emerged as a principled approach to learning deep latent-variable models, enabling flexible data representations in both generative and semi-supervised contexts (Kingma and Welling, <xref rid="B17" ref-type="bibr">2014</xref>, <xref rid="B18" ref-type="bibr">2019</xref>). Building on this foundation, we employ a Conditional Gaussian Mixture Variational Autoencoder (CGMVAE), which extends the standard VAE framework by integrating (1) a conditional variable (binned age in our case) alongside the primary input and (2) a Gaussian mixture prior to capture inherently multi-modal latent distributions (Jiang et al., <xref rid="B14" ref-type="bibr">2017</xref>; Sarkar et al., <xref rid="B31" ref-type="bibr">2020</xref>).</p><sec><title>2.3.1 Model architecture</title><p><xref rid="F2" ref-type="fig">Figure 2</xref> provides an overview of the CGMVAE architecture. The encoder or recognition model maps the concatenation of the input vector <bold>x</bold> and the conditional variable <bold>c</bold> into hidden representations via a sequence of fully connected layers with non-linear activations. Specifically, the encoder outputs:</p><list list-type="roman-lower"><list-item><p><bold>mixture probabilities</bold>
<italic><bold>q</bold></italic><sub><italic>y</italic></sub>, obtained through a <monospace>softmax</monospace> layer, representing a categorical distribution over <italic>K</italic> Gaussian components, and</p></list-item><list-item><p><bold>component-wise Gaussian parameters</bold>, t~he means <italic><bold>&#x003bc;</bold></italic><sub><italic>k</italic></sub> and log-variances <inline-formula><mml:math id="M1" overflow="scroll"><mml:msubsup><mml:mrow><mml:mstyle mathvariant="bold"><mml:mo class="qopname">log</mml:mo><mml:mi>&#x003c3;</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> for each component <italic>k</italic>&#x02208;{1, &#x02026;, <italic>K</italic>}.</p></list-item></list><fig position="float" id="F2"><label>Figure 2</label><caption><p>Graphical illustration of the Conditional Gaussian Mixture Variational Autoencoder. The input <bold>x</bold> is concatenated with condition <bold>c</bold> and then mapped to mixture probabilities and per-component Gaussian parameters. A latent sample <bold>z</bold> is drawn via the reparameterization trick and combined again with <bold>c</bold> for the reconstruction in the decoder.</p></caption><graphic xlink:href="fpsyg-16-1474292-g0002" position="float"/></fig><p>Hence, each data point <bold>x</bold> is associated with a conditional distribution over <bold>z</bold>:</p><disp-formula id="E1">
<label>(1)</label>
<mml:math id="M2" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mi>q</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mtext>z</mml:mtext></mml:mstyle><mml:mo>|</mml:mo><mml:mstyle mathvariant="bold"><mml:mtext>x</mml:mtext></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:mtext>c</mml:mtext></mml:mstyle></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mi>q</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mstyle mathvariant="bold"><mml:mtext>x</mml:mtext></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:mtext>c</mml:mtext></mml:mstyle></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>&#x003bc;</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mtext>x</mml:mtext></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:mtext>c</mml:mtext></mml:mstyle></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mtext>x</mml:mtext></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:mtext>c</mml:mtext></mml:mstyle></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mstyle mathvariant="bold"><mml:mtext>I</mml:mtext></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula><p>Using the reparameterization trick, we draw latent samples <bold>z</bold> from each component in a differentiable manner (Kingma et al., <xref rid="B16" ref-type="bibr">2015</xref>). The final latent representation is computed as the weighted sum of samples across all <italic>K</italic> components, weighted by <italic>q</italic>(<italic>y</italic> = <italic>k</italic>&#x02223;<bold>x</bold>, <bold>c</bold>).</p><p>The <bold>decoder</bold> or generative model receives the latent sample <bold>z</bold> (combined with the same conditional variable <bold>c</bold>) to reconstruct the original input <bold>x</bold>. In particular, <bold>z</bold> and <bold>c</bold> are concatenated and passed through a symmetric stack of fully connected layers, culminating in a final output layer with either:</p><list list-type="bullet"><list-item><p>a <monospace>Sigmoid</monospace> for binary and normalized data or</p></list-item><list-item><p>a linear output for continuous-valued features.</p></list-item></list><p>The decoder thus models <inline-formula><mml:math id="M3" overflow="scroll"><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mtext>x</mml:mtext></mml:mstyle><mml:mo>&#x02223;</mml:mo><mml:mstyle mathvariant="bold"><mml:mtext>z</mml:mtext></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:mtext>c</mml:mtext></mml:mstyle></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mtext>x</mml:mtext></mml:mstyle></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold"><mml:mtext>I</mml:mtext></mml:mstyle></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for the continuous case or a Bernoulli-based parameterization for the binary/normalized data case (Kingma and Welling, <xref rid="B18" ref-type="bibr">2019</xref>).</p></sec><sec><title>2.3.2 Loss function and training</title><p>The total objective is to minimize the negative evidence lower bound (ELBO), augmented to handle the mixture and conditional terms:</p><disp-formula id="E2">
<label>(2)</label>
<mml:math id="M4" overflow="scroll"><mml:mrow><mml:mi>&#x02112;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>c</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo>=</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:munder><mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>z</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>c</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>log</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>z</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>c</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo stretchy="true">&#x0fe38;</mml:mo></mml:munder><mml:mrow><mml:mtext>Reconstruction&#x000a0;Loss</mml:mtext></mml:mrow></mml:munder><mml:mtext>&#x000a0;</mml:mtext><mml:mo>+</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>&#x003b2;</mml:mi><mml:mtext>&#x000a0;&#x000a0;</mml:mtext><mml:munder><mml:munder><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>z</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>c</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02016;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>z</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>c</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo stretchy="true">&#x0fe38;</mml:mo></mml:munder><mml:mrow><mml:mtext>Regularization&#x000a0;Term</mml:mtext></mml:mrow></mml:munder><mml:mo>,</mml:mo></mml:mrow></mml:math>
</disp-formula><p>where the KL divergence <italic>D</italic><sub>KL</sub> is computed between the encoding distribution <italic>q</italic>(<bold>z</bold>|<bold>x</bold>, <bold>c</bold>) and the mixture prior <italic>p</italic>(<bold>z</bold>|<bold>c</bold>), itself represented by learnable <italic><bold>&#x003bc;</bold></italic><sub>prior</sub>, for each mixture component. The scalar &#x003b2;&#x02208;(0, 1] is a hyperparameter that balances the KL term and the reconstruction accuracy regulating the emphasis placed on disentangled latent representations and subpopulation discovery (Higgins et al., <xref rid="B12" ref-type="bibr">2016</xref>).</p><p>The reconstruction loss is implemented as the minimization of the mean squared error (MSE). Each training iteration thus optimizes:</p><disp-formula id="E3">
<label>(3)</label>
<mml:math id="M5" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mtext class="textrm" mathvariant="normal">Total Loss</mml:mtext><mml:mo>=</mml:mo><mml:mtext class="textrm" mathvariant="normal">Reconstruction Loss</mml:mtext><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext class="textrm" mathvariant="normal">KL</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula><p>where the gradient of the loss is backpropagated through the latent sampling by means of the reparameterization trick (Kingma et al., <xref rid="B16" ref-type="bibr">2015</xref>):</p><disp-formula id="E4">
<label>(4)</label>
<mml:math id="M6" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mstyle mathvariant="bold"><mml:mtext>z</mml:mtext></mml:mstyle><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>&#x003bc;</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>&#x003c3;</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02299;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>&#x003f5;</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mtext>&#x02003;</mml:mtext><mml:mstyle mathvariant="bold"><mml:mi>&#x003f5;</mml:mi></mml:mstyle><mml:mo>~</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mn>0</mml:mn></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:mtext>I</mml:mtext></mml:mstyle></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula></sec><sec><title>2.3.3 Implementation details</title><p>We developed the CGMVAE in PyTorch. The encoder combines the original input (<bold>x</bold>) and condition vector (<bold>c</bold>), passing them through fully connected layers (each comprising linear transformations, ReLU activations, and batch normalization). The final shared layer outputs (i) logit scores for the mixture probabilities <bold>q</bold><sub><italic>y</italic></sub>, (ii) <italic><bold>&#x003bc;</bold></italic>, and (iii) <bold>log&#x003c3;</bold><sup>2</sup> for each of the <italic>K</italic> mixtures, reshaped for convenience as <monospace>(batch_size, n_components, latent_dim)</monospace>. The decoder mirrors this structure by mapping (sampled) latent variables back to input space, producing the reconstructed <inline-formula><mml:math id="M7" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mtext>x</mml:mtext></mml:mstyle></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>.</p><p>A notable extension of our CGMVAE is the <monospace>mu_prior</monospace> and <monospace>logvar_prior</monospace> parameters, which are learned for each mixture component, providing a flexible Gaussian mixture prior that can adapt to complex, multi-modal latent structures in real-world data, a relevant aspect due to the nature of datasets that often have subpopulation heterogeneity.</p></sec><sec><title>2.3.4 Training pipeline and hyperparameter optimization</title><p>We trained the CGMVAE using:</p><list list-type="bullet"><list-item><p><bold>Data loaders:</bold> Mini-batched training and validation sets.</p></list-item><list-item><p><bold>Optimization:</bold> Adam is configured via a learning rate, weight decay, and an optional gradient clipping that prevent the gradients from excessively large values.</p></list-item><list-item><p><bold>Scheduler:</bold> A <monospace>ReduceLROnPlateau</monospace> to automatically adjust the learning rate upon plateauing validation losses.</p></list-item><list-item><p><bold>Checkpointing:</bold> Periodic saving of model weights and optimizer states.</p></list-item></list><p>Validation performance is monitored by computing the reconstruction and KL terms on a held-out set, and the best model weights are saved based on minimal validation loss. In addition, we implemented a hyperparameter optimization routine leveraging <monospace>Optuna</monospace> framework to systematically explore a search space for latent dimensions, number of mixture components, learning rate, and free bits parameter. This latter parameter helps to prevent the model from over-regularizing the latent space by ensuring that each latent variable contributes at least a minimal amount of information. The final model is retrained on the full training set using the best hyperparameters and stored for subsequent analyses.</p></sec></sec><sec><title>2.4 Feature focused interpretation of cluster solution and cluster quality assessment</title><p>The cluster interpretation was performed using a feature-focused analysis approach on the trained CGMVAE model and the winner FMM solution. For each cluster, we performed statistical analyses including univariate feature analysis computing means, standard deviations, along with normalized differences from global statistics using z-scores.</p><p>Cluster quality has been assessed on both methods by computing standard and fuzzy cluster quality indices on the latent space representations. In the CGMVAE solution, we derived the latent features via the model's encoder and determined hard assignments on the fuzzy membership matrix. The Silhouette Score (Rousseeuw, <xref rid="B30" ref-type="bibr">1987</xref>) (which measures the cohesion and separation of clusters), the Calinski-Harabasz Index (Calinski and Harabasz, <xref rid="B5" ref-type="bibr">1974</xref>) (which quantifies the ratio of between-cluster dispersion to within-cluster dispersion), and the Davies-Bouldin Score (Davies and Bouldin, <xref rid="B7" ref-type="bibr">1979</xref>) (which assesses average cluster similarity, with lower values reflecting better separation) were then estimated in both CGMVAE and FMM solution. In addition to these, we computed fuzzy clustering metrics to capture the uncertainty in cluster membership for CGMVAE. The Xie-Beni Index (XB) (Xie and Beni, <xref rid="B38" ref-type="bibr">1991</xref>) provides an assessment of the compactness and separation of fuzzy clusters. The fuzzy partition coefficient (FPC) (Wu and Yang, <xref rid="B37" ref-type="bibr">2005</xref>), which averages the squared membership degrees over all samples, indicates the crispness of the clustering, while the partition entropy (PE) (Bezdek, <xref rid="B3" ref-type="bibr">1981</xref>) quantifies the fuzziness of the assignment by measuring the entropy of the fuzzy membership values.</p></sec><sec><title>2.5 Software and packages</title><p>The analyses were conducted using a system equipped with an NVIDIA GeForce RTX 2060 graphics card and an Intel i7-10750H CPU operating at 2.60GHz. Python v3.9.16 (Van Rossum and Drake, <xref rid="B35" ref-type="bibr">2009</xref>) was used for the analyses. The FMM and CGMVAE were performed on Python by using the <italic>scikit-learn</italic> library (Pedregosa et al., <xref rid="B28" ref-type="bibr">2011</xref>). The <italic>Pytorch</italic> library v.2.0.0 (Paszke et al., <xref rid="B27" ref-type="bibr">2019</xref>) was used for CGMVAE model computation, <italic>Optuna</italic> for hyperparameter optimization (Akiba et al., <xref rid="B1" ref-type="bibr">2019</xref>), and <italic>Weight &#x00026; Biases</italic> for experiment tracking (Biewald, <xref rid="B4" ref-type="bibr">2020</xref>).</p></sec></sec><sec sec-type="results" id="s3"><title>3 Results</title><sec><title>3.1 Factor mixture modeling</title><p>The investigation of the underlying structure of students' cognitive profiles by using factor mixture modeling (FMM), comparing specifications with class-invariant (FMM-1) and class-specific (FMM-2) factor structures, followed the approaches outlined by Lubke and Muth&#x000e9;n (<xref rid="B21" ref-type="bibr">2005</xref>) and Clark et al. (<xref rid="B6" ref-type="bibr">2013</xref>). Prior to fitting FMMs, baseline LCA and FA models were estimated to guide the FMM specification process. The best-fitting FA model yielded a BIC of 110,749.98 with five latent factors, while the best LCA model resulted in a BIC of 117,211.73 with two latent classes. Based on these analyses and theoretical considerations, subsequent FMM analyses explored models with up to two latent classes and five latent factors. As outlined in the Methods section, we systematically fitted FMM-1 and FMM-2 models, incorporating the binned age covariate to predict latent class membership. The complexity of the models was increased incrementally, starting with two classes and one factor for both the models.</p><p>Based on the BIC values showed in <xref rid="T1" ref-type="table">Table 1</xref>, the FMM-1 model with 2 classes and 1 factor provided the most parsimonious fit to the data (BIC = 94,285.72). This model outperformed the best-fitting FMM-2 model (BIC = 94,439.41 for the 2-class, 1-factor solution) and was considerably better than the optimal baseline LCA (BIC = 117,211.73) and FA (BIC = 110,749.98) models. The selected FMM-1 model identified two latent classes with distinct profiles characterized by different means on a single underlying cognitive factor. The estimated class proportions were 29.14% for Class 1 and 70.86% for Class 2.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Model comparison between FMM-1 and FMM-2 models.</p></caption><table frame="box" rules="all"><thead><tr style="background-color:#8f9496;color:#ffffff"><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Model specification</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Factors</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>BIC</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>AIC</bold>
</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="5" colspan="1">FMM-1</td><td valign="top" align="center" rowspan="1" colspan="1">1</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>94,285.72</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>93,519.15</bold>
</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">96,140.41</td><td valign="top" align="center" rowspan="1" colspan="1">95,046.15</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">100,594.34</td><td valign="top" align="center" rowspan="1" colspan="1">99,172.39</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">108,745.80</td><td valign="top" align="center" rowspan="1" colspan="1">106,996.15</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">5</td><td valign="top" align="center" rowspan="1" colspan="1">102,263.78</td><td valign="top" align="center" rowspan="1" colspan="1">100,186.44</td></tr><tr><td valign="top" align="left" rowspan="5" colspan="1">FMM-2</td><td valign="top" align="center" rowspan="1" colspan="1">1</td><td valign="top" align="center" rowspan="1" colspan="1">94,439.41</td><td valign="top" align="center" rowspan="1" colspan="1">93,661.14</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">95,124.53</td><td valign="top" align="center" rowspan="1" colspan="1">93,995.16</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">96,447.74</td><td valign="top" align="center" rowspan="1" colspan="1">94,955.57</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">97,587.32</td><td valign="top" align="center" rowspan="1" colspan="1">95,720.64</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">5</td><td valign="top" align="center" rowspan="1" colspan="1">98,186.11</td><td valign="top" align="center" rowspan="1" colspan="1">95,933.22</td></tr><tr><td valign="top" align="left" colspan="2" rowspan="1">Best factor analysis (FA)</td><td valign="top" align="center" rowspan="1" colspan="1">110,749.98</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td></tr><tr><td valign="top" align="left" colspan="2" rowspan="1">Best latent class analysis (LCA)</td><td valign="top" align="center" rowspan="1" colspan="1">117,211.73</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td></tr></tbody></table><table-wrap-foot><p>BIC, Bayesian information criterion; AIC, Akaike information criterion. Lower values indicate better model fit relative to complexity. The best-fitting model based on BIC is shown in bold. BIC values for the best comparison FA and LCA models are provided.</p></table-wrap-foot></table-wrap><p>The selected best-fitting model posits that the underlying cognitive factor structure (defined by item intercepts and factor loadings) is invariant across classes. Differences between the classes arise solely from variations in the mean of the single latent factor. The estimated factor means for the two classes were Class 1 Mean (&#x003b1;<sub>1</sub>): -1.451 and Class 2 Mean (&#x003b1;<sub>2</sub>): 0.657. These means present a clear separation between the classes on the latent cognitive dimension captured by the factor. Class 1 exhibits a significantly lower mean factor score compared to Class 2. As shown in <xref rid="F3" ref-type="fig">Figure 3</xref>, Class 1 represents a subgroup generally expected to perform lower across the assessed cognitive tasks, while Class 2 represents a subgroup expected to perform higher. The expected response profile for each class on each item can be derived using the estimated item intercepts, the invariant factor loadings, and the class-specific factor means. The magnitude of the difference between the two classes on any given item is proportional to the magnitude of that item's factor loading; items with stronger positive loadings (e.g., Logical_6 [loading=0.73], Logical_11 [0.72], Logical_12 [0.69], Phonological Comprehension_9 [0.65]) show the largest expected performance gap favoring Class 2. Conversely, for items with negative loadings, this pattern is reversed. Based on the invariant loadings, the items 1 and 4 of Phonological task showed loadings equal to &#x02212;0.41, and &#x02212;0.21, respectively, are expected to show higher average responses for Class 1 compared to Class 2.</p><fig position="float" id="F3"><label>Figure 3</label><caption><p>Expected response profiles for the two latent classes derived from the Factor Mixture Model (FMM). The blue line (circles) represents Class 1 (factor mean &#x02248; &#x02212;1.45), and the orange line (squares) represents Class 2 (factor mean &#x02248;0.66). Profiles are calculated using class-specific factor loadings and the respective class factor means, illustrating how expected item responses vary. Items exhibiting the largest absolute differences between classes are highlighted with gray dashed vertical lines and labels. Items with negative factor loadings (based on Class 1 loadings, indicated by red dotted vertical lines) show reversed patterns where Class 1 has higher expected responses.</p></caption><graphic xlink:href="fpsyg-16-1474292-g0003" position="float"/></fig><p>By focusing on the covariance role of binned age variable, the analysis revealed a significant association between age and the probability of belonging to Class 1 relative to Class 2. The estimated regression coefficient for Age_C predicting membership in Class 1 versus Class 2 was equal to &#x02212;0.963. This negative coefficient indicates that as the binned age variable increases, the log-odds of being classified into Class 1 (the lower-performing profile) compared to Class 2 decrease. Exponentiating the coefficient yields an odds ratio (OR) of approximately 0.382. This suggests that for each one-unit increase in the binned age variable, the odds of an individual belonging to Class 1 rather than Class 2 are multiplied by 0.382, representing a decrease of approximately 61.8%. Therefore, higher values on the covariate are strongly associated with a reduced likelihood of membership in the lower-performing Class 1 relative to the higher-performing Class 2. The intercept term for the comparison of Class 1 versus Class 2 was &#x02212;0.048, corresponding to an odds ratio of approximately 0.953. This suggests that when the binned age is at its mean (value of 0), the baseline odds of being in Class 1 vs. Class 2 are nearly the same, although slightly lower for Class 1.</p></sec><sec><title>3.2 CGMVAE</title><p>The CGMVAE architecture achieved an optimal performance after 15 epochs with a validation loss equal to 1.516. The encoder architecture comprised a fully connected layer followed by ReLU activation and batch normalization, branching into separate networks for computing mixture components <italic>q</italic>(<italic>y</italic>|<italic>x</italic>), means (&#x003bc;), and log-variances (log&#x003c3;<sup>2</sup>). The decoder reconstructed the input features through a mirror architecture, maintaining the same hidden dimension while incorporating the conditional information. The winner parameters showed an encoder hidden layer of 201 units feeding into a latent space of 58 dimensions, structured as a mixture of 10 Gaussian components. Training employed the Adam optimizer with a learning rate of 9.68 &#x000d7; 10<sup>&#x02212;4</sup> and a weight decay of 4.00 &#x000d7; 10<sup>&#x02212;5</sup>, with the variational objective tuned via a &#x003b2; of 0.051 and a free bits threshold of 0.048. Gradient clipping at 0.644 further ensured stable parameter updates during training. <xref rid="F4" ref-type="fig">Figures 4a</xref>, <xref rid="F4" ref-type="fig">b</xref> showed graphically the reconstruction error by binned ages and the latent space of the two dimensions by applying the t-SNE visualization analysis (van der Maaten and Hinton, <xref rid="B34" ref-type="bibr">2008</xref>). As depicted in <xref rid="F4" ref-type="fig">Figure 4a</xref>, the reconstruction error analysis demonstrates the model's capability to preserve essential data features across all age bins, with reconstructed samples closely matching their original counterparts. However, subtle variations in reconstruction quality are observed across the age spectrum, suggesting age-specific patterns in data complexity and representational characteristics.</p><fig position="float" id="F4"><label>Figure 4</label><caption><p>Model performance evaluation: <bold>(a)</bold> Reconstruction errors across binned age groups and <bold>(b)</bold> t-SNE projection of latent representations, with colors denoting the 10 Gaussian mixture components.</p></caption><graphic xlink:href="fpsyg-16-1474292-g0004" position="float"/></fig></sec><sec><title>3.3 Cluster characteristics by ages and features</title><p><xref rid="F5" ref-type="fig">Figure 5</xref> illustrates the distribution of binned age groups across the proposed cluster solution, where Cluster 9 emerges as the most representative across all age intervals, with its representativeness progressively increasing from 44% in the [0&#x02013;8) age group to 48% in [8&#x02013;12) and peaking at 54% for individuals older than 12 years.</p><fig position="float" id="F5"><label>Figure 5</label><caption><p>Proportion of samples within each cluster across binned age intervals. The bar plot highlights the representation of age groups [0&#x02013;8), [8&#x02013;12), and 12+ within the cluster solution.</p></caption><graphic xlink:href="fpsyg-16-1474292-g0005" position="float"/></fig><p>In contrast, the proportion of subjects in Cluster 5 decline with age, starting at 27% in the [0&#x02013;8) age group, dropping to 16% in [8&#x02013;12), and further decreasing to 8% in the [12&#x02013;100] age group. Meanwhile, Cluster 0 shows an almost homogeneous distribution across the age groups with proportions of 15%, 13%, and 11%, respectively, and Cluster 4 exhibits an increasing trend with age, from 7% in [0&#x02013;8) to 13% in [8&#x02013;12) and 21% in [12&#x02013;100]. All remaining clusters have a homogeneous representation and are less than 10% across all age ranges.</p><p>We focused our detailed analysis on clusters with statistically meaningful sample sizes (<italic>n</italic> &#x02265; 10 subjects). Following this criterion, Clusters 3, 7, and 8 were excluded from further investigation due to insufficient sample representation. Complete z-scores and detailed cluster characteristics for all features are provided in <xref rid="A1" ref-type="app">Appendix A</xref>.</p><p><bold>Cluster 9</bold>. It is the most representative of the entire sample. It exhibits a balanced cognitive profile with performances classified as average across various domains. Among these, motion perception and logical reasoning demonstrated subtle but consistent strengths, with slight positive deviations in verbal and phonological abilities. By investigating z-scores, motion perception showed minor enhancements across tasks, with the strongest observed in tasks 5 (Z = +0.24) and 3 (Z = +0.23) of the test. Logical reasoning tasks followed a similar trend, with task 1 (Z = +0.21) performing the best. Verbal comprehension (e.g., task 17, Z = +0.20), phonological awareness (task 5, Z = +0.16), and working memory (Z = +0.04) also showed slight improvements, albeit within the average performance range. Overall, the cluster reflects a cognitively stable and homogeneous group, with average performance across tasks and minor variability in specific domains, such as phonological awareness, which included only a slight underperformance in task 8 (Z = &#x02013;0.13).</p><p><bold>Cluster 5</bold>. It demonstrates a distinctive cognitive pattern characterized by pronounced strengths in visuospatial attention (task 3: Z = +0.45), above average logical reasoning abilities (e.g., task 15: Z = +0.34; task 11: Z = +0.29), and working memory processing (Z = +0.18), while exhibiting some weaknesses in phonological processing (task 6: Z = &#x02013;0.52; task 4: Z = &#x02013;0.48). The cluster's profile is further distinguished by average motion perception capabilities and verbal comprehension.</p><p><bold>Cluster 4</bold>. It exhibits a cognitive profile characterized by below-average performance in perceptual-motor domains, with consistent reductions across all motion perception variables (ranging from Z = -0.51 to Z = -0.36) and mild impairments in logical reasoning, accompanied by modest difficulties in visuospatial information integration, and in several verbal comprehension subdimensions (e.g., tasks 8, 15, 2, and 9, with reductions ranging from Z = &#x02013;0.39 to Z = &#x02013;0.19). However, strengths emerge in the phonological domain, particularly evidenced in task 4 (Z = +0.41), task 5 (Z = +0.18), and task 9 (Z = +0.11).</p><p><bold>Cluster 2</bold>. It presents a distinctive cognitive pattern characterized by average global performance with specific vulnerabilities and strengths across different domains. In the domains of phonological awareness and verbal comprehension, the subjects have average performances in mostly all domains with few exceptions of vulnerabilities (e.g., phonological awareness, task 5: Z = &#x02013;0.84; verbal comprehension, task 3: Z = &#x02013;0.72; task 11: Z = &#x02013;0.64). However, these limitations are counterbalanced by strengths in logical reasoning tasks (task 5: Z = +0.39; task 13: Z = +0.29) and motion perception (task 2: Z = +0.34; task 1: Z = +0.27 SD). The working memory ability is slight below average (Z = &#x02013;0.22). This cluster indicates a cognitive profile characterized by local specialization rather than integrated processing. The cluster's performance suggests compensatory mechanisms where visual-perceptual and logical abilities potentially offset weaknesses in phonological and verbal domains, pointing to alternative cognitive processing strategies.</p><p><bold>Cluster 0</bold>. It exhibits a distinctive cognitive profile characterized by slight but systematic reductions across several domains, with the most marked deficits observed in verbal comprehension, logical reasoning, and motion perception. Specifically, verbal comprehension tasks such as task 17 show a significant drop (Z = &#x02212;0.62), while logical reasoning measures and motion perception variables consistently fall below the global average. Visuospatial attention is also modestly affected, with reductions approximately &#x02212;0.28 to &#x02212;0.33 in the z-scores, whereas phonological awareness remains comparatively preserved. Working memory showed a slight below performance (Z = &#x02212;0.12). Overall, the profile of Cluster 0 reflects uniform mild reductions in cognitive performance, particularly in logical and perceptual-motor tasks, with potential localized compensations in verbal and phonological processing.</p><p><bold>Cluster 1</bold>. With a small sample size (<italic>n</italic> = 17), it demonstrates a highly distinctive cognitive profile characterized by severe deficits coupled with isolated compensatory mechanisms. The most pronounced impairments are observed in verbal comprehension (task 3: Z 0 &#x02212;2.05), logical reasoning (task 9: Z = &#x02212;1.09), visuospatial attention (task 1 and 2: Z = &#x02212;0.59 and &#x02212;0.41, respectively), and slight in working memory (Z = &#x02212;0.37). However, the cluster exhibits remarkable compensatory strengths in phonological awareness (task 4: Z = +0.70) and motion perception (task 2 and 3: Z = +0.33 and 0.30). This profile suggests a severe deficits in core verbal and logical domains potentially driving compensatory developments in phonological and perceptual processing. The small sample size warrants careful interpretation, but the consistent pattern of extreme polarization across multiple measures suggests a distinct neurocognitive phenotype.</p><p><bold>Cluster 6</bold>. With a small sample size (<italic>n</italic> = 37) exhibits a highly atypical cognitive profile characterized by severe deficits across multiple domains, particularly in verbal comprehension (task 17: Z = &#x02212;1.42), logical reasoning (task 9: Z = &#x02212;1.06), and phonological awareness (task 13: Z = &#x02212;0.99). Working memory shows moderate impairment (Z = &#x02212;0.46), potentially contributing to broader cognitive difficulties. However, the cluster demonstrates remarkable isolated strengths in motion perception (task 4: Z = +0.55; task 3: Z = +0.37). The profile aligns with potential neurodevelopmental conditions characterized by significant language and executive function impairments alongside isolated areas of preserved or enhanced abilities.</p></sec><sec><title>3.4 Comparing FMM and CGMVAE cluster quality</title><p>The cluster quality showed very different results in the two proposed solutions. By focusing on FMM, the solution in the model's latent factor space demonstrates exceptionally good separation between classes based on the three metrics. First, the Silhouette score of 0.959 is very close to the maximum possible value of 1.0, indicating that observations within each cluster are tightly grouped while remaining well separated from other clusters.</p><p>Next, the Davies-Bouldin Index registers at an extremely low of 0.074, reinforcing the notion that clusters are highly distinct; as this index approaches zero, the average distance between clusters grows relative to their internal dispersion. Finally, the Calinski-Harabasz Index reaches 113,432.648, a notably high value that further confirms well-defined cluster structure. Together, these metrics lend strong support to the conclusion that the Factor Mixture Model has partitioned the data into clear latent classes in the factor space. Conversely, the solution proposed by the CGMVAE algorithm indicates that the clusters are relatively weak overall. A negative silhouette score (&#x02013;0.008) and low Calinski-Harabasz index (7.005), together with high Davies-Bouldin (4.244) and Xie-Beni (32.783) indices, showed that the clusters are not highly distinct and that there is considerable overlap between them. In addition, the fuzzy partition coefficient value (0.109) and the high partition entropy (2.257) suggest that membership probabilities are large, which aligns with the need for cautious interpretation of clusters with small sample sizes (e.g., Clusters 1 and 6). Then, even though the detailed profiles and age distributions offer rich insights into cognitive heterogeneity, the overall clustering solution appears to be less sharply delineated, pointing out that the differences, while present, may reflect subtle gradations rather than clear separations.</p></sec></sec><sec sec-type="discussion" id="s4"><title>4 Discussion</title><p>This study presents a comparative analysis between a factor mixture modeling (FMM) and a deep clustering based approach, a Conditional Gaussian Mixture Variational Autoencoder (CGMVAE) in cognitive profile analysis. These two methods have been evaluated in their effectiveness across six cognitive dimensions assessed through the PROFFILO game (Orsoni et al., <xref rid="B25" ref-type="bibr">2021</xref>) and multiple age groups. Our results indicate that the FMM, particularly the simpler two-class, one-factor solution (FMM-1), produced exceptionally clear separations between classes, while the CGMVAE uncovered more nuanced but less well-delineated cluster structures.</p><p>From a model-fitting perspective, the FMM excelled by identifying two broadly distinct latent classes. The solution's robustness was evidenced by near-ideal clustering metrics. Its Silhouette Score (0.959) approached the upper limit of 1.0, signifying minimal overlap between classes and tight intra-class homogeneity. Similarly, a low Davies-Bouldin Index (0.074) and high Calinski-Harabasz Index provided strong convergent evidence for well-defined class boundaries. This clarity suggests that the single latent factor effectively captured most of the individual differences in our data. Class membership probabilities similarly showed clear differences, implying that each subgroup reflected a meaningful cognitive profile, one with lower overall performance and one with higher overall performance on the six different cognitive tasks. The covariance effect showed an effect of binned age. Higher ages are strongly associated with a reduced likelihood of membership in the lower performing Class 1 relative to the higher-performing Class 2.</p><p>By contrast, the CGMVAE, which is designed to handle complex, non-linear relationships, identified multiple clusters with overlapping features and fuzzy boundaries. While this suggests deeper insight into potentially subtle cognitive differences, cluster quality metrics pointed to weaker overall separability. Negative Silhouette Scores (&#x02013;0.008) underscore difficulty in distinguishing unique cluster identities; a high Davies-Bouldin Index (4.244) and small Calinski-Harabasz Index (7.005) further reveal limited divergence across the identified clusters. In addition, the fuzzy partition coefficient (0.109) and elevated partition entropy (2.257) highlight considerable uncertainty in assigning individuals to specific clusters. These results implies that the CGMVAE captures more complex patterns but may be inappropriate when the research or intervention context demands a clear separation between groups. By investigating the reconstruction error analysis, it showed robust performance across all age bins, suggesting effective preservation of essential cognitive characteristics. The analysis of cluster distributions revealed distinct developmental trajectories, with Cluster 9 emerging as predominant and showing increased representation with age (44% to 54%). Contrasting patterns were observed in Cluster 5 (declining from 27% to 8%) and Cluster 4 (increasing from 7% to 21%), while Cluster 0 remained relatively stable across age groups. By looking at the cluster characteristics, the analysis revealed distinct cognitive profiles that suggest different patterns of strengths, weaknesses, and potential compensatory mechanisms across neurocognitive domains. Cluster 9, representing the normative profile, displayed consistently average performance with subtle strengths in motion perception and logical reasoning (Z-scores approximately +0.20), serving as a baseline for comparison. In contrast, Clusters 1 and 6 exhibited the most atypical profiles, characterized by severe deficits in verbal comprehension (Z = &#x02013;2.05 and &#x02013;1.42, respectively) and logical reasoning, coupled with remarkable compensatory strengths in specific domains such as motion perception and phonological awareness. Three intermediate patterns emerged: Cluster 5 showed enhanced visuospatial attention (Z = +0.45) and logical reasoning abilities despite phonological processing weaknesses; Cluster 4 demonstrated consistent perceptual-motor deficits offset by phonological strengths; and Cluster 2 exhibited a profile of selective vulnerabilities in phonological and verbal domains counterbalanced by logical and motion perception strengths. Cluster 0 presented a unique pattern of mild but systematic reductions across multiple domains, particularly affecting verbal comprehension (Z = &#x02013;0.62) and logical reasoning. These distinct profiles suggest different underlying neurocognitive profiles, potentially reflecting various developmental trajectories or compensatory mechanisms. The presence of both severe deficits and domain-specific strengths within the same clusters (particularly in Clusters 1 and 6) points to the possibility of neural reorganization and the development of alternative processing strategies.</p><p>In general, these results present a key methodological trade-off. FMM excels in parsimony and interpretability, generating a small number of highly distinct groups. This makes FMM particularly valuable for applications requiring clear categorical distinctions. Differently, the CGMVAE's flexible architecture can model richer interactions among variables, potentially unveiling hidden facets of cognitive variation that are not captured within simpler factor solutions. However, the cost is weaker cluster separation and greater membership ambiguity, which may complicate direct intervention or classification efforts.</p></sec><sec sec-type="conclusions" id="s5"><title>5 Conclusion</title><p>This study compared factor mixture modeling (FMM) and Conditional Gaussian Mixture Variational Autoencoders (CGMVAE) for identifying cognitive profiles. The evidence showed that both methods offer unique advantages. The FMM provides clear, readily interpretable clusters, while CGMVAE reveals more complex, non-linear patterns in cognitive abilities.</p><p>The results highlighted how FMM analysis successfully identified two distinct and well-separated cognitive classes, with binned age significantly predicting class membership. This highlights FMM's strength in producing a parsimonious and statistically robust partition based on cognitive dimensions. Conversely, the CGMVAE uncovered multiple, more nuanced cognitive profiles characterized by specific combinations of strengths and weaknesses across domains. These profiles exhibited clear developmental trajectories linked to age. However, this increased granularity came at the cost of cluster quality; the CGMVAE clusters showed considerable overlap and lacked sharp boundaries, indicating weaker separation compared to the FMM solution. However, several limitations warrant consideration. Our FMM analysis did not extend to FMM-3 and FMM-4 models (Clark et al., <xref rid="B6" ref-type="bibr">2013</xref>), which allow for class specific item parameters and could potentially capture more subtle forms of measurement non-invariance. The CGMVAE approach, despite its modeling power, presents practical hurdles. Its complexity (58 latent dimensions, 10 components) and high-dimensional latent space hinder interpretability and hidden the direct link between cognitive features and cluster assignments.</p><p>Furthermore, methodological constraints included binning age into broad categories due to a trimodal distribution, potentially masking finer developmental changes, and a lack of demographic data, limiting the examination of socioeconomic or cultural factors. The generalizability of the CGMVAE findings needs confirmation across different cognitive assessment tools as performance might be tool-specific. Finally, the significant computational resources and the inherent &#x0201c;black-box&#x0201d; nature of the CGMVAE could hinder its adoption and trust among practitioners in educational settings.</p><p>Future research should focus on validating these findings across diverse populations and assessment instruments. Integrating the strengths of both approaches, using FMM for initial broad classification and CGMVAE for subsequent refinement of subtle, non-linear variations within those classes, could offer a powerful hybrid strategy. Moreover, translating these complex analytical findings into actionable educational interventions remains a key objective.</p><p>In conclusion, FMM and CGMVAE serve different but complementary roles in cognitive profiling. FMM excels at providing clear, practical classifications, whereas CGMVAE offers a deeper lens into the subtle complexities and developmental dynamics of cognitive performance.</p></sec></body><back><sec sec-type="data-availability" id="s6"><title>Data availability statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.</p></sec><sec sec-type="ethics-statement" id="s7"><title>Ethics statement</title><p>The studies involving humans were approved by the University of Bologna Bioethics Committee. The studies were conducted in accordance with the local legislation and institutional requirements. Written informed consent for participation in this study was provided by the participants' legal guardians/next of kin. Written informed consent was obtained from the individual(s) for the publication of any potentially identifiable images or data included in this article.</p></sec><sec sec-type="author-contributions" id="s8"><title>Author contributions</title><p>MO: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing. SG: Software, Validation, Visualization, Writing &#x02013; review &#x00026; editing. SG: Validation, Visualization, Writing &#x02013; review &#x00026; editing. NM: Validation, Visualization, Writing &#x02013; review &#x00026; editing. MS: Validation, Visualization, Writing &#x02013; review &#x00026; editing. MB: Conceptualization, Funding acquisition, Resources, Software, Supervision, Writing &#x02013; review &#x00026; editing.</p></sec><sec sec-type="COI-statement" id="conf1"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p><p>The author(s) declared that they were an editorial board member of Frontiers, at the time of submission. This had no impact on the peer review process and the final decision.</p></sec><sec sec-type="disclaimer" id="s10"><title>Publisher's note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akiba</surname><given-names>T.</given-names></name><name><surname>Sano</surname><given-names>S.</given-names></name><name><surname>Yanase</surname><given-names>T.</given-names></name><name><surname>Ohta</surname><given-names>T.</given-names></name><name><surname>Koyama</surname><given-names>M.</given-names></name></person-group> (<year>2019</year>). <article-title>&#x0201c;Optuna: a next-generation hyperparameter optimization framework,&#x0201d;</article-title> in <source>The 25th ACM SIGKDD International Conference on Knowledge Discovery &#x00026;Data Mining</source>, <fpage>2623</fpage>&#x02013;<lpage>2631</lpage>. <pub-id pub-id-type="doi">10.1145/3292500.3330701</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Altun</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>). <source>Understanding Cognitive Profiles in Designing Personalized Learning Environments</source>. <publisher-loc>Berlin, Heidelberg</publisher-loc>: <publisher-name>Springer Berlin Heidelberg</publisher-name>, <fpage>259</fpage>&#x02013;<lpage>271</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-662-47724-3_14</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bezdek</surname><given-names>J. C.</given-names></name></person-group> (<year>1981</year>). <source>Pattern Recognition with Fuzzy Objective Function Algorithms</source>. New York: Plenum Press. <pub-id pub-id-type="doi">10.1007/978-1-4757-0450-1</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Biewald</surname><given-names>L.</given-names></name></person-group> (<year>2020</year>). <source>Experiment tracking with weights and biases</source>. Available online at: <ext-link xlink:href="https://wandb.com" ext-link-type="uri">wandb.com</ext-link> (accessed April 16, 2025).</mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calinski</surname><given-names>T.</given-names></name><name><surname>Harabasz</surname><given-names>J.</given-names></name></person-group> (<year>1974</year>). <article-title>A dendrite method for cluster analysis</article-title>. <source>Commun. Stat</source>. <volume>3</volume>, <fpage>1</fpage>&#x02013;<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1080/03610927408827101</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>S. L.</given-names></name><name><surname>Muth&#x000e9;n</surname><given-names>B.</given-names></name><name><surname>Kaprio</surname><given-names>J.</given-names></name><name><surname>D'Onofrio</surname><given-names>B. M.</given-names></name><name><surname>Viken</surname><given-names>R.</given-names></name><name><surname>Rose</surname><given-names>R. J.</given-names></name></person-group> (<year>2013</year>). <article-title>Models and strategies for factor mixture analysis: an example concerning the structure underlying psychological disorders</article-title>. <source>Struct. Equat. Model</source>. <volume>20</volume>:<fpage>824786</fpage>. <pub-id pub-id-type="doi">10.1080/10705511.2013.824786</pub-id><pub-id pub-id-type="pmid">24302849</pub-id>
</mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davies</surname><given-names>D. L.</given-names></name><name><surname>Bouldin</surname><given-names>D. W.</given-names></name></person-group> (<year>1979</year>). <article-title>A cluster separation measure</article-title>. <source>IEEE Trans. Patt. Anal. Mach. Intell</source>. PAMI-<volume>1</volume>, <fpage>224</fpage>&#x02013;<lpage>227</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.1979.4766909</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eskandarnia</surname><given-names>E.</given-names></name><name><surname>Al-Ammal</surname><given-names>H. M.</given-names></name><name><surname>Ksantini</surname><given-names>R.</given-names></name></person-group> (<year>2022</year>). <article-title>An embedded deep-clustering-based load profiling framework</article-title>. <source>Sustain. Cities Soc</source>. <volume>78</volume>:<fpage>103618</fpage>. <pub-id pub-id-type="doi">10.1016/j.scs.2021.103618</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gathercole</surname><given-names>S. E.</given-names></name><name><surname>Alloway</surname><given-names>T. P.</given-names></name><name><surname>Kirkwood</surname><given-names>H. J.</given-names></name><name><surname>Elliott</surname><given-names>J. G.</given-names></name><name><surname>Holmes</surname><given-names>J.</given-names></name><name><surname>Hilton</surname><given-names>K. A.</given-names></name></person-group> (<year>2008</year>). <article-title>Attentional and executive function behaviours in children with poor working memory</article-title>. <source>Learn. Individ. Differ</source>. <volume>18</volume>, <fpage>214</fpage>&#x02013;<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1016/j.lindif.2007.10.003</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez</surname><given-names>R.</given-names></name><name><surname>Vance</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). <article-title>Confirmatory factor analysis, latent profile analysis, and factor mixture modeling of the syndromes of the child behavior checklist and teacher report form</article-title>. <source>Psychol. Assess</source>. <volume>26</volume>, <fpage>1307</fpage>&#x02013;<lpage>1316</lpage>. <pub-id pub-id-type="doi">10.1037/a0037431</pub-id><pub-id pub-id-type="pmid">25068908</pub-id>
</mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayden</surname><given-names>K. M.</given-names></name><name><surname>Kuchibhatla</surname><given-names>M.</given-names></name><name><surname>Romero</surname><given-names>H. R.</given-names></name><name><surname>Plassman</surname><given-names>B. L.</given-names></name><name><surname>Burke</surname><given-names>J. R.</given-names></name><name><surname>Browndyke</surname><given-names>J. N.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Pre-clinical cognitive phenotypes for Alzheimer disease: a latent profile approach</article-title>. <source>Am. J. Geriatr. Psychiat</source>. <volume>22</volume>, <fpage>1364</fpage>&#x02013;<lpage>1374</lpage>. <pub-id pub-id-type="doi">10.1016/j.jagp.2013.07.008</pub-id><pub-id pub-id-type="pmid">24080384</pub-id>
</mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higgins</surname><given-names>I.</given-names></name><name><surname>Matthey</surname><given-names>L.</given-names></name><name><surname>Pal</surname><given-names>A.</given-names></name><name><surname>Burgess</surname><given-names>C. P.</given-names></name><name><surname>Glorot</surname><given-names>X.</given-names></name><name><surname>Botvinick</surname><given-names>M. M.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>&#x0201c;Beta-vae: Learning basic visual concepts with a constrained variational framework,&#x0201d;</article-title> in <source>International Conference on Learning Representations</source>.</mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jayanth Krishnan</surname><given-names>K.</given-names></name><name><surname>Mitra</surname><given-names>K.</given-names></name></person-group> (<year>2022</year>). <article-title>A modified Kohonen map algorithm for clustering time series data</article-title>. <source>Expert Syst. Appl</source>. <volume>201</volume>:<fpage>117249</fpage>. <pub-id pub-id-type="doi">10.1016/j.eswa.2022.117249</pub-id><pub-id pub-id-type="pmid">19103474</pub-id>
</mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Z.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name><name><surname>Tan</surname><given-names>H.</given-names></name><name><surname>Tang</surname><given-names>B.</given-names></name><name><surname>Zhou</surname><given-names>H.</given-names></name></person-group> (<year>2017</year>). <article-title>&#x0201c;Variational deep embedding: an unsupervised and generative approach to clustering,&#x0201d;</article-title> in <source>IJCAI'17: Proceedings of the 26th International Joint Conference on Artificial Intelligence</source>, <fpage>1965</fpage>&#x02013;<lpage>1972</lpage>. <pub-id pub-id-type="doi">10.24963/ijcai.2017/273</pub-id><pub-id pub-id-type="pmid">33285988</pub-id>
</mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kail</surname><given-names>R.</given-names></name><name><surname>Hall</surname><given-names>L. K.</given-names></name></person-group> (<year>1994</year>). <article-title>Processing speed, naming speed, and reading</article-title>. <source>Dev. Psychol</source>. <volume>30</volume>, <fpage>949</fpage>&#x02013;<lpage>954</lpage>. <pub-id pub-id-type="doi">10.1037/0012-1649.30.6.949</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D. P.</given-names></name><name><surname>Salimans</surname><given-names>T.</given-names></name><name><surname>Welling</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>&#x0201c;Variational dropout and the local reparameterization trick,&#x0201d;</article-title> in <source>NIPS'15: Proceedings of the 29th International Conference on Neural Information Processing Systems</source> - <italic>Volume 2</italic>, <fpage>2575</fpage>&#x02013;<lpage>2583</lpage>.</mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D. P.</given-names></name><name><surname>Welling</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>&#x0201c;Auto-encoding variational bayes,&#x0201d;</article-title> in <source>2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada. Conference Track Proceedings</source>.<pub-id pub-id-type="pmid">32176273</pub-id>
</mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D. P.</given-names></name><name><surname>Welling</surname><given-names>M.</given-names></name></person-group> (<year>2019</year>). <article-title>An introduction to variational autoencoders</article-title>. <source>Found. Trends Mach. Learn</source>. <volume>12</volume>, <fpage>307</fpage>&#x02013;<lpage>392</lpage>. <pub-id pub-id-type="doi">10.1561/2200000056</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kopf</surname><given-names>A.</given-names></name><name><surname>Fortuin</surname><given-names>V.</given-names></name><name><surname>Somnath</surname><given-names>V. R.</given-names></name><name><surname>Claassen</surname><given-names>M.</given-names></name></person-group> (<year>2021</year>). <article-title>Mixture-of-experts variational autoencoder for clustering and generating from similarity-based representations on single cell data</article-title>. <source>PLoS Comput. Biol</source>. <volume>17</volume>:<fpage>e1009086</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1009086</pub-id><pub-id pub-id-type="pmid">34191792</pub-id>
</mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>E.</given-names></name><name><surname>Mukherjee</surname><given-names>S.</given-names></name><name><surname>Kannan</surname><given-names>S.</given-names></name></person-group> (<year>2020</year>). <article-title>A deep adversarial variational autoencoder model for dimensionality reduction in single-cell RNA sequencing analysis</article-title>. <source>BMC Bioinform</source>. <volume>21</volume>:<fpage>64</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-020-3401-5</pub-id><pub-id pub-id-type="pmid">32085701</pub-id>
</mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lubke</surname><given-names>G. H.</given-names></name><name><surname>Muth&#x000e9;n</surname><given-names>B. O.</given-names></name></person-group> (<year>2005</year>). <article-title>Investigating population heterogeneity with factor mixture models</article-title>. <source>Psychol. Methods</source>
<volume>10</volume>, <fpage>21</fpage>&#x02013;<lpage>39</lpage>. <pub-id pub-id-type="doi">10.1037/1082-989X.10.1.21</pub-id><pub-id pub-id-type="pmid">15810867</pub-id>
</mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mammadov</surname><given-names>S.</given-names></name><name><surname>Cross</surname><given-names>J.</given-names></name><name><surname>Cross</surname><given-names>T.</given-names></name></person-group> (<year>2016</year>). <article-title>Use of latent profile analysis in studies of gifted students</article-title>. <source>Roeper Rev</source>. <volume>38</volume>, <fpage>175</fpage>&#x02013;<lpage>184</lpage>. <pub-id pub-id-type="doi">10.1080/02783193.2016.1183739</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nesayan</surname><given-names>A.</given-names></name><name><surname>Amani</surname><given-names>M.</given-names></name><name><surname>Gandomani</surname><given-names>R. A.</given-names></name></person-group> (<year>2018</year>). <article-title>Cognitive profile of children and its relationship with academic performance</article-title>. <source>Basic Clin. Neurosci</source>. <volume>10</volume>, <fpage>165</fpage>&#x02013;<lpage>174</lpage>. <pub-id pub-id-type="doi">10.32598/bcn.9.10.230</pub-id><pub-id pub-id-type="pmid">31031903</pub-id>
</mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ning</surname><given-names>H. K.</given-names></name><name><surname>Downing</surname><given-names>K.</given-names></name></person-group> (<year>2014</year>). <article-title>A latent profile analysis of university students' self-regulated learning strategies</article-title>. <source>Stud. High. Educ</source>. <volume>40</volume>, <fpage>1328</fpage>&#x02013;<lpage>1346</lpage>. <pub-id pub-id-type="doi">10.1080/03075079.2014.880832</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orsoni</surname><given-names>M.</given-names></name><name><surname>Benvenuti</surname><given-names>M.</given-names></name><name><surname>Giovagnoli</surname><given-names>S.</given-names></name><name><surname>Mazzoni</surname><given-names>E.</given-names></name><name><surname>Magri</surname><given-names>S.</given-names></name><name><surname>Bartolini</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>&#x0201c;Proffilo: a new digital assessment tool to evaluate learning difficulties in secondary school,&#x0201d;</article-title> in <source>25th Annual International CyberPsychology, CyberTherapy &#x00026;Social Networking Conference (CYPSY25)</source>, <fpage>49</fpage>&#x02013;<lpage>49</lpage>.</mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orsoni</surname><given-names>M.</given-names></name><name><surname>Giovagnoli</surname><given-names>S.</given-names></name><name><surname>Garofalo</surname><given-names>S.</given-names></name><name><surname>Magri</surname><given-names>S.</given-names></name><name><surname>Benvenuti</surname><given-names>M.</given-names></name><name><surname>Mazzoni</surname><given-names>E.</given-names></name><etal/></person-group>. (<year>2023</year>). <article-title>Preliminary evidence on machine learning approaches for clusterizing students' cognitive profile</article-title>. <source>Heliyon</source>
<volume>9</volume>:<fpage>e14506</fpage>. <pub-id pub-id-type="doi">10.1016/j.heliyon.2023.e14506</pub-id><pub-id pub-id-type="pmid">36967938</pub-id>
</mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A.</given-names></name><name><surname>Gross</surname><given-names>S.</given-names></name><name><surname>Massa</surname><given-names>F.</given-names></name><name><surname>Lerer</surname><given-names>A.</given-names></name><name><surname>Bradbury</surname><given-names>J.</given-names></name><name><surname>Chanan</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>&#x0201c;Pytorch: an imperative style, high-performance deep learning library,&#x0201d;</article-title> in <source>Advances in Neural Information Processing Systems</source>, <fpage>8024</fpage>&#x02013;<lpage>8035</lpage>.</mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F.</given-names></name><name><surname>Varoquaux</surname><given-names>G.</given-names></name><name><surname>Gramfort</surname><given-names>A.</given-names></name><name><surname>Michel</surname><given-names>V.</given-names></name><name><surname>Thirion</surname><given-names>B.</given-names></name><name><surname>Grisel</surname><given-names>O.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>Scikit-learn: machine learning in python</article-title>. <source>J. Mach. Learn. Res</source>. <volume>12</volume>, <fpage>2825</fpage>&#x02013;<lpage>2830</lpage>.</mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>X.</given-names></name><name><surname>Feng</surname><given-names>J.</given-names></name><name><surname>Xiao</surname><given-names>S.</given-names></name><name><surname>Yau</surname><given-names>W.-Y.</given-names></name><name><surname>Zhou</surname><given-names>J. T.</given-names></name><name><surname>Yang</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>). <article-title>Structured autoencoders for subspace clustering</article-title>. <source>IEEE Trans. Image Proc</source>. <volume>27</volume>, <fpage>5076</fpage>&#x02013;<lpage>5086</lpage>. <pub-id pub-id-type="doi">10.1109/TIP.2018.2848470</pub-id><pub-id pub-id-type="pmid">29994115</pub-id>
</mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousseeuw</surname><given-names>P. J.</given-names></name></person-group> (<year>1987</year>). <article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title>. <source>J. Comput. Appl. Math</source>. <volume>20</volume>, <fpage>53</fpage>&#x02013;<lpage>65</lpage>. <pub-id pub-id-type="doi">10.1016/0377-0427(87)90125-7</pub-id><pub-id pub-id-type="pmid">15760469</pub-id>
</mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarkar</surname><given-names>A.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Cooper</surname><given-names>S.</given-names></name></person-group> (<year>2020</year>). <article-title>Conditional level generation and game blending</article-title>. <source>arXiv:2010.07735</source>.</mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuster</surname><given-names>V.</given-names></name><name><surname>Krogh</surname><given-names>A. A.</given-names></name></person-group> (<year>2021</year>). <article-title>Manifold learning perspective on representation learning: learning decoder and representations without an encoder</article-title>. <source>Entropy</source>
<volume>23</volume>:<fpage>1403</fpage>. <pub-id pub-id-type="doi">10.3390/e23111403</pub-id><pub-id pub-id-type="pmid">34828101</pub-id>
</mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swanson</surname><given-names>H. L.</given-names></name><name><surname>Hoskyn</surname><given-names>M.</given-names></name></person-group> (<year>2001</year>). <article-title>Instructing adolescents with learning disabilities: a component and composite analysis</article-title>. <source>Learn. Disab. Res. Pract</source>. <volume>16</volume>, <fpage>109</fpage>&#x02013;<lpage>119</lpage>. <pub-id pub-id-type="doi">10.1111/0938-8982.00012</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Maaten</surname><given-names>L.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group> (<year>2008</year>). <article-title>Visualizing data using t-SNE</article-title>. <source>J. Mach. Learn. Res</source>. <volume>9</volume>, <fpage>2579</fpage>&#x02013;<lpage>2605</lpage>.</mixed-citation></ref><ref id="B35"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Van Rossum</surname><given-names>G.</given-names></name><name><surname>Drake</surname><given-names>F. L.</given-names></name></person-group> (<year>2009</year>). <source>Python 3 Reference Manual</source>. CreateSpace.</mixed-citation></ref><ref id="B36"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Webster</surname><given-names>W.</given-names></name></person-group> (<year>2002</year>). <article-title>&#x0201c;Metacognition and the autonomous learner: student reflections on cognitive profiles and learning environment development,&#x0201d;</article-title> in <source>Proceedings of the 4th World Conference of the International Consortium for Educational Development: Spheres of Influence: Ventures and Visions in Educational Development. Organisation and Staff Development Services, University of Western Australia</source>.</mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>K.-L.</given-names></name><name><surname>Yang</surname><given-names>M.-S.</given-names></name></person-group> (<year>2005</year>). <article-title>A cluster validity index for fuzzy clustering</article-title>. <source>Patt. Recognit. Lett</source>. <volume>26</volume>, <fpage>1275</fpage>&#x02013;<lpage>1291</lpage>. <pub-id pub-id-type="doi">10.1016/j.patrec.2004.11.022</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>X. L.</given-names></name><name><surname>Beni</surname><given-names>G.</given-names></name></person-group> (<year>1991</year>). <article-title>A validity measure for fuzzy clustering</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>13</volume>, <fpage>841</fpage>&#x02013;<lpage>847</lpage>. <pub-id pub-id-type="doi">10.1109/34.85677</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>J.</given-names></name><name><surname>Ma</surname><given-names>M.</given-names></name><name><surname>Yu</surname><given-names>Z.</given-names></name></person-group> (<year>2023</year>). <article-title>BMVAE: a variational autoencoder method for clustering single-cell mutation data</article-title>. <source>Bioinformatics</source>
<volume>39</volume>:<fpage>btac790</fpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btac790</pub-id><pub-id pub-id-type="pmid">36478203</pub-id>
</mixed-citation></ref><ref id="B40"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zimmerman</surname><given-names>B. J.</given-names></name><name><surname>Schunk</surname><given-names>D. H.</given-names></name></person-group> (<year>2011</year>). <source>Handbook of Self-Regulation of Learning and Performance</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Routledge</publisher-name>.</mixed-citation></ref></ref-list><app-group><app id="A1"><title>Appendix</title><sec id="s11"><title>Clustering features characteristics</title><table-wrap position="anchor" id="T2"><label>Table 2</label><caption><p>z-Scores across cognitive domains and clusters.</p></caption><table frame="box" rules="all"><thead><tr style="background-color:#8f9496;color:#ffffff"><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Cognitive domain</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Cluster 0</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Cluster 1</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Cluster 2</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Cluster 4</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Cluster 5</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Cluster 6</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>Cluster 9</bold>
</th></tr></thead><tbody><tr style="background-color:#dee1e1;"><td valign="top" align="left" colspan="8" rowspan="1">
<bold>Motion perception</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 1</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.51</td><td valign="top" align="center" rowspan="1" colspan="1">+0.18</td><td valign="top" align="center" rowspan="1" colspan="1">+0.27</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.46</td><td valign="top" align="center" rowspan="1" colspan="1">+0.21</td><td valign="top" align="center" rowspan="1" colspan="1">+0.13</td><td valign="top" align="center" rowspan="1" colspan="1">+0.17</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 2</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.49</td><td valign="top" align="center" rowspan="1" colspan="1">+0.33</td><td valign="top" align="center" rowspan="1" colspan="1">+0.34</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.51</td><td valign="top" align="center" rowspan="1" colspan="1">+0.10</td><td valign="top" align="center" rowspan="1" colspan="1">+0.13</td><td valign="top" align="center" rowspan="1" colspan="1">+0.20</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 3</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.53</td><td valign="top" align="center" rowspan="1" colspan="1">+0.30</td><td valign="top" align="center" rowspan="1" colspan="1">+0.20</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.49</td><td valign="top" align="center" rowspan="1" colspan="1">+0.05</td><td valign="top" align="center" rowspan="1" colspan="1">+0.37</td><td valign="top" align="center" rowspan="1" colspan="1">+0.23</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 4</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.29</td><td valign="top" align="center" rowspan="1" colspan="1">+0.06</td><td valign="top" align="center" rowspan="1" colspan="1">+0.12</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.38</td><td valign="top" align="center" rowspan="1" colspan="1">+0.02</td><td valign="top" align="center" rowspan="1" colspan="1">+0.55</td><td valign="top" align="center" rowspan="1" colspan="1">+0.16</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 5</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.37</td><td valign="top" align="center" rowspan="1" colspan="1">+0.07</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.01</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.36</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.11</td><td valign="top" align="center" rowspan="1" colspan="1">+0.04</td><td valign="top" align="center" rowspan="1" colspan="1">+0.24</td></tr><tr style="background-color:#dee1e1;"><td valign="top" align="left" colspan="8" rowspan="1">
<bold>Logical reasoning</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 1</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.12</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.28</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.37</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.37</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.01</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.81</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.21</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 2</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.28</td><td valign="top" align="center" rowspan="1" colspan="1">+0.08</td><td valign="top" align="center" rowspan="1" colspan="1">+0.27</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.13</td><td valign="top" align="center" rowspan="1" colspan="1">+0.18</td><td valign="top" align="center" rowspan="1" colspan="1">+0.30</td><td valign="top" align="center" rowspan="1" colspan="1">+0.02</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 3</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.49</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.40</td><td valign="top" align="center" rowspan="1" colspan="1">+0.17</td><td valign="top" align="center" rowspan="1" colspan="1">+0.11</td><td valign="top" align="center" rowspan="1" colspan="1">+0.18</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.34</td><td valign="top" align="center" rowspan="1" colspan="1">+0.10</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 4</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.57</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.10</td><td valign="top" align="center" rowspan="1" colspan="1">+0.17</td><td valign="top" align="center" rowspan="1" colspan="1">+0.00</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.18</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.26</td><td valign="top" align="center" rowspan="1" colspan="1">+0.19</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 5</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.27</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.86</td><td valign="top" align="center" rowspan="1" colspan="1">+0.39</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.43</td><td valign="top" align="center" rowspan="1" colspan="1">+0.04</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.01</td><td valign="top" align="center" rowspan="1" colspan="1">+0.16</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 6</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.47</td><td valign="top" align="center" rowspan="1" colspan="1">+0.27</td><td valign="top" align="center" rowspan="1" colspan="1">+0.24</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.08</td><td valign="top" align="center" rowspan="1" colspan="1">+0.12</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.48</td><td valign="top" align="center" rowspan="1" colspan="1">+0.10</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 7</td><td valign="top" align="center" rowspan="1" colspan="1">+0.04</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.13</td><td valign="top" align="center" rowspan="1" colspan="1">+0.08</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.25</td><td valign="top" align="center" rowspan="1" colspan="1">+0.11</td><td valign="top" align="center" rowspan="1" colspan="1">+0.01</td><td valign="top" align="center" rowspan="1" colspan="1">+0.03</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 8</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.34</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.02</td><td valign="top" align="center" rowspan="1" colspan="1">+0.00</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.34</td><td valign="top" align="center" rowspan="1" colspan="1">+0.20</td><td valign="top" align="center" rowspan="1" colspan="1">+0.01</td><td valign="top" align="center" rowspan="1" colspan="1">+0.13</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 9</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.37</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;1.09</td><td valign="top" align="center" rowspan="1" colspan="1">+0.24</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.10</td><td valign="top" align="center" rowspan="1" colspan="1">+0.08</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;1.06</td><td valign="top" align="center" rowspan="1" colspan="1">+0.12</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 10</td><td valign="top" align="center" rowspan="1" colspan="1">+0.02</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.24</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.45</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.36</td><td valign="top" align="center" rowspan="1" colspan="1">+0.06</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.61</td><td valign="top" align="center" rowspan="1" colspan="1">+0.16</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 11</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.29</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.33</td><td valign="top" align="center" rowspan="1" colspan="1">+0.01</td><td valign="top" align="center" rowspan="1" colspan="1">+0.07</td><td valign="top" align="center" rowspan="1" colspan="1">+0.29</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.42</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.02</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 12</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.32</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.10</td><td valign="top" align="center" rowspan="1" colspan="1">+0.19</td><td valign="top" align="center" rowspan="1" colspan="1">+0.03</td><td valign="top" align="center" rowspan="1" colspan="1">+0.13</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.19</td><td valign="top" align="center" rowspan="1" colspan="1">+0.02</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 13</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.15</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.31</td><td valign="top" align="center" rowspan="1" colspan="1">+0.29</td><td valign="top" align="center" rowspan="1" colspan="1">+0.15</td><td valign="top" align="center" rowspan="1" colspan="1">+0.10</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.25</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.06</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 14</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.28</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.86</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.04</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.07</td><td valign="top" align="center" rowspan="1" colspan="1">+0.07</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.77</td><td valign="top" align="center" rowspan="1" colspan="1">+0.11</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 15</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.26</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.13</td><td valign="top" align="center" rowspan="1" colspan="1">+0.02</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.12</td><td valign="top" align="center" rowspan="1" colspan="1">+0.34</td><td valign="top" align="center" rowspan="1" colspan="1">+0.09</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.01</td></tr><tr style="background-color:#dee1e1;"><td valign="top" align="left" colspan="8" rowspan="1">
<bold>Verbal comprehension</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 1</td><td valign="top" align="center" rowspan="1" colspan="1">+0.11</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;1.01</td><td valign="top" align="center" rowspan="1" colspan="1">+0.09</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.11</td><td valign="top" align="center" rowspan="1" colspan="1">+0.10</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.56</td><td valign="top" align="center" rowspan="1" colspan="1">+0.05</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 2</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.04</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.91</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.19</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.21</td><td valign="top" align="center" rowspan="1" colspan="1">+0.12</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.56</td><td valign="top" align="center" rowspan="1" colspan="1">+0.09</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 3</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.25</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;2.05</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.72</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.05</td><td valign="top" align="center" rowspan="1" colspan="1">+0.16</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.86</td><td valign="top" align="center" rowspan="1" colspan="1">+0.16</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 4</td><td valign="top" align="center" rowspan="1" colspan="1">+0.14</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.60</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.16</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.02</td><td valign="top" align="center" rowspan="1" colspan="1">+0.03</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.12</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.01</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 5</td><td valign="top" align="center" rowspan="1" colspan="1">+0.07</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.81</td><td valign="top" align="center" rowspan="1" colspan="1">+0.11</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.22</td><td valign="top" align="center" rowspan="1" colspan="1">+0.10</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.60</td><td valign="top" align="center" rowspan="1" colspan="1">+0.03</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 6</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.05</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.29</td><td valign="top" align="center" rowspan="1" colspan="1">+0.14</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.02</td><td valign="top" align="center" rowspan="1" colspan="1">+0.15</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.48</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.02</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 7</td><td valign="top" align="center" rowspan="1" colspan="1">+0.03</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.32</td><td valign="top" align="center" rowspan="1" colspan="1">+0.05</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.04</td><td valign="top" align="center" rowspan="1" colspan="1">+0.13</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.40</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.02</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 8</td><td valign="top" align="center" rowspan="1" colspan="1">+0.08</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.50</td><td valign="top" align="center" rowspan="1" colspan="1">+0.09</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.39</td><td valign="top" align="center" rowspan="1" colspan="1">+0.14</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.67</td><td valign="top" align="center" rowspan="1" colspan="1">+0.07</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 9</td><td valign="top" align="center" rowspan="1" colspan="1">+0.08</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.25</td><td valign="top" align="center" rowspan="1" colspan="1">+0.06</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.19</td><td valign="top" align="center" rowspan="1" colspan="1">+0.01</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.39</td><td valign="top" align="center" rowspan="1" colspan="1">+0.04</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 10</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.19</td><td valign="top" align="center" rowspan="1" colspan="1">+0.44</td><td valign="top" align="center" rowspan="1" colspan="1">+0.02</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.06</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.24</td><td valign="top" align="center" rowspan="1" colspan="1">+0.08</td><td valign="top" align="center" rowspan="1" colspan="1">+0.13</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 11</td><td valign="top" align="center" rowspan="1" colspan="1">+0.25</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.33</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.64</td><td valign="top" align="center" rowspan="1" colspan="1">+0.02</td><td valign="top" align="center" rowspan="1" colspan="1">+0.24</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.44</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.06</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 12</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.12</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.11</td><td valign="top" align="center" rowspan="1" colspan="1">+0.18</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.07</td><td valign="top" align="center" rowspan="1" colspan="1">+0.22</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.08</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.03</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 13</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.24</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.29</td><td valign="top" align="center" rowspan="1" colspan="1">+0.14</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.00</td><td valign="top" align="center" rowspan="1" colspan="1">+0.08</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.24</td><td valign="top" align="center" rowspan="1" colspan="1">+0.03</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 14</td><td valign="top" align="center" rowspan="1" colspan="1">+0.01</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.49</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.07</td><td valign="top" align="center" rowspan="1" colspan="1">+0.02</td><td valign="top" align="center" rowspan="1" colspan="1">+0.09</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.34</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.01</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 15</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.05</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.23</td><td valign="top" align="center" rowspan="1" colspan="1">+0.05</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.24</td><td valign="top" align="center" rowspan="1" colspan="1">+0.19</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.55</td><td valign="top" align="center" rowspan="1" colspan="1">+0.04</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 16</td><td valign="top" align="center" rowspan="1" colspan="1">+0.03</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.32</td><td valign="top" align="center" rowspan="1" colspan="1">+0.02</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.12</td><td valign="top" align="center" rowspan="1" colspan="1">+0.06</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.40</td><td valign="top" align="center" rowspan="1" colspan="1">+0.03</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 17</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.62</td><td valign="top" align="center" rowspan="1" colspan="1">+0.01</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.40</td><td valign="top" align="center" rowspan="1" colspan="1">+0.01</td><td valign="top" align="center" rowspan="1" colspan="1">+0.12</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;1.42</td><td valign="top" align="center" rowspan="1" colspan="1">+0.20</td></tr><tr style="background-color:#dee1e1;"><td valign="top" align="left" colspan="8" rowspan="1">
<bold>Phonological awareness</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 1</td><td valign="top" align="center" rowspan="1" colspan="1">+0.09</td><td valign="top" align="center" rowspan="1" colspan="1">+0.35</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.03</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.13</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.16</td><td valign="top" align="center" rowspan="1" colspan="1">+0.46</td><td valign="top" align="center" rowspan="1" colspan="1">+0.05</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 2</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.05</td><td valign="top" align="center" rowspan="1" colspan="1">+0.25</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.03</td><td valign="top" align="center" rowspan="1" colspan="1">+0.06</td><td valign="top" align="center" rowspan="1" colspan="1">+0.06</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.55</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.01</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 3</td><td valign="top" align="center" rowspan="1" colspan="1">+0.03</td><td valign="top" align="center" rowspan="1" colspan="1">+0.09</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.10</td><td valign="top" align="center" rowspan="1" colspan="1">+0.08</td><td valign="top" align="center" rowspan="1" colspan="1">+0.04</td><td valign="top" align="center" rowspan="1" colspan="1">+0.27</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.04</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 4</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.10</td><td valign="top" align="center" rowspan="1" colspan="1">+0.70</td><td valign="top" align="center" rowspan="1" colspan="1">+0.04</td><td valign="top" align="center" rowspan="1" colspan="1">+0.41</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.48</td><td valign="top" align="center" rowspan="1" colspan="1">+0.30</td><td valign="top" align="center" rowspan="1" colspan="1">+0.03</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 5</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.23</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.05</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.84</td><td valign="top" align="center" rowspan="1" colspan="1">+0.18</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.19</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.23</td><td valign="top" align="center" rowspan="1" colspan="1">+0.16</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 6</td><td valign="top" align="center" rowspan="1" colspan="1">+0.12</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.09</td><td valign="top" align="center" rowspan="1" colspan="1">+0.05</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.07</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.52</td><td valign="top" align="center" rowspan="1" colspan="1">+0.23</td><td valign="top" align="center" rowspan="1" colspan="1">+0.14</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 7</td><td valign="top" align="center" rowspan="1" colspan="1">+0.11</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.39</td><td valign="top" align="center" rowspan="1" colspan="1">+0.11</td><td valign="top" align="center" rowspan="1" colspan="1">+0.01</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.02</td><td valign="top" align="center" rowspan="1" colspan="1">+0.18</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.04</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 8</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.01</td><td valign="top" align="center" rowspan="1" colspan="1">+0.40</td><td valign="top" align="center" rowspan="1" colspan="1">+0.35</td><td valign="top" align="center" rowspan="1" colspan="1">+0.03</td><td valign="top" align="center" rowspan="1" colspan="1">+0.25</td><td valign="top" align="center" rowspan="1" colspan="1">+0.16</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.13</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 9</td><td valign="top" align="center" rowspan="1" colspan="1">+0.08</td><td valign="top" align="center" rowspan="1" colspan="1">+0.10</td><td valign="top" align="center" rowspan="1" colspan="1">+0.20</td><td valign="top" align="center" rowspan="1" colspan="1">+0.11</td><td valign="top" align="center" rowspan="1" colspan="1">+0.16</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.27</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.12</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 10</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.00</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.80</td><td valign="top" align="center" rowspan="1" colspan="1">+0.11</td><td valign="top" align="center" rowspan="1" colspan="1">+0.01</td><td valign="top" align="center" rowspan="1" colspan="1">+0.04</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.12</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.01</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 11</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.12</td><td valign="top" align="center" rowspan="1" colspan="1">+0.07</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.15</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.00</td><td valign="top" align="center" rowspan="1" colspan="1">+0.02</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.22</td><td valign="top" align="center" rowspan="1" colspan="1">+0.05</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 12</td><td valign="top" align="center" rowspan="1" colspan="1">+0.11</td><td valign="top" align="center" rowspan="1" colspan="1">+0.04</td><td valign="top" align="center" rowspan="1" colspan="1">+0.06</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.09</td><td valign="top" align="center" rowspan="1" colspan="1">+0.21</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.42</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.06</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 13</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.10</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.05</td><td valign="top" align="center" rowspan="1" colspan="1">+0.00</td><td valign="top" align="center" rowspan="1" colspan="1">+0.08</td><td valign="top" align="center" rowspan="1" colspan="1">+0.13</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.99</td><td valign="top" align="center" rowspan="1" colspan="1">+0.01</td></tr><tr style="background-color:#dee1e1;"><td valign="top" align="left" colspan="8" rowspan="1">
<bold>Visuospatial attention</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 1</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.10</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.59</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.02</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.25</td><td valign="top" align="center" rowspan="1" colspan="1">+0.14</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.36</td><td valign="top" align="center" rowspan="1" colspan="1">+0.08</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 2</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.28</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.41</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.03</td><td valign="top" align="center" rowspan="1" colspan="1">+0.00</td><td valign="top" align="center" rowspan="1" colspan="1">+0.15</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.37</td><td valign="top" align="center" rowspan="1" colspan="1">+0.05</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 3</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.33</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.16</td><td valign="top" align="center" rowspan="1" colspan="1">+0.03</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.18</td><td valign="top" align="center" rowspan="1" colspan="1">+0.45</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.54</td><td valign="top" align="center" rowspan="1" colspan="1">+0.01</td></tr><tr style="background-color:#dee1e1;"><td valign="top" align="left" colspan="8" rowspan="1">
<bold>Working memory</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Task 1</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.12</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.37</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.22</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.06</td><td valign="top" align="center" rowspan="1" colspan="1">+0.18</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;0.46</td><td valign="top" align="center" rowspan="1" colspan="1">+0.04</td></tr></tbody></table></table-wrap></sec></app></app-group></back></article>