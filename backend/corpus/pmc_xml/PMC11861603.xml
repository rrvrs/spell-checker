<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006413</article-id><article-id pub-id-type="pmc">PMC11861603</article-id><article-id pub-id-type="doi">10.3390/s25041184</article-id><article-id pub-id-type="publisher-id">sensors-25-01184</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Data Reconstruction Methods in Multi-Feature Fusion CNN Model for Enhanced Human Activity Recognition <xref rid="fn1-sensors-25-01184" ref-type="author-notes">&#x02020;</xref></article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-6260-3077</contrib-id><name><surname>Ko</surname><given-names>Jae Eun</given-names></name></contrib><contrib contrib-type="author"><name><surname>Kim</surname><given-names>SeungHui</given-names></name></contrib><contrib contrib-type="author"><name><surname>Sul</surname><given-names>Jae Ho</given-names></name></contrib><contrib contrib-type="author"><name><surname>Kim</surname><given-names>Sung Min</given-names></name><xref rid="c1-sensors-25-01184" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Lee</surname><given-names>Chengkuo</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Liu</surname><given-names>Po-Liang</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01184">Department of Regulatory Science for Medical Device, Dongguk University, Seoul 04620, Republic of Korea; <email>2018111721@dgu.ac.kr</email> (J.E.K.); <email>2023126928@dgu.ac.kr</email> (S.K.); <email>2018111719@dongguk.edu</email> (J.H.S.)</aff><author-notes><corresp id="c1-sensors-25-01184"><label>*</label>Correspondence: <email>sungmin2009@gmail.com</email></corresp><fn id="fn1-sensors-25-01184"><label>&#x02020;</label><p>Presented at the Accelerometer Data Reconstruction Methods in Features-Fusion CNN Model for Enhanced Human Activity Recognition, AIS-I3S 2024, Singapore, 1&#x02013;4 August 2024.</p></fn></author-notes><pub-date pub-type="epub"><day>14</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1184</elocation-id><history><date date-type="received"><day>30</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>03</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>12</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Background: Human activity recognition (HAR) plays a pivotal role in digital healthcare, enabling applications such as exercise monitoring and elderly care. However, traditional HAR methods relying on accelerometer data often require complex preprocessing steps, including noise reduction and manual feature extraction. Deep learning-based human activity recognition (HAR) using one-dimensional accelerometer data often suffers from noise and limited feature extraction. Transforming time-series signals into two-dimensional representations has shown potential for enhancing feature extraction and reducing noise. However, existing methods relying on single-feature inputs or extensive preprocessing face limitations in robustness and accuracy. Methods: This study proposes a multi-input, two-dimensional CNN architecture using three distinct data reconstruction methods. By fusing features from reconstructed images, the model enhances feature extraction capabilities. This method was validated on a custom HAR dataset without requiring complex preprocessing steps. Results: The proposed method outperformed models using single-reconstruction methods or raw one-dimensional data. Compared to a one-dimensional baseline, it achieved 16.64%, 13.53%, and 16.3% improvements in accuracy, precision, and recall, respectively. We tested across various levels of noise, and the proposed model consistently demonstrated greater robustness than the time-series-based approach. Fusing features from three inputs effectively captured latent patterns and variations in accelerometer data. Conclusions: This study demonstrates that HAR can be effectively improved using a multi-input CNN approach with reconstructed data. This method offers a practical and efficient solution, streamlining feature extraction and enhancing performance, making it suitable for real-world applications.</p></abstract><kwd-group><kwd>human activity recognition</kwd><kwd>HAR</kwd><kwd>accelerometer</kwd><kwd>data reconstruction</kwd><kwd>CNN</kwd><kwd>spectrogram</kwd><kwd>recurrence plot</kwd><kwd>multi-channel plot</kwd></kwd-group><funding-group><award-group><funding-source>Regulatory Science Talent Development Program (R&#x00026;D) of the Ministry of Food and Drug Safety</funding-source><award-id>22183</award-id></award-group><award-group><funding-source>Ministry of Food and Drug Safety</funding-source><award-id>RS-2024-00331775</award-id></award-group><funding-statement>This study was funded by the Regulatory Science Talent Development Program (R&#x00026;D) of the Ministry of Food and Drug Safety (Grant number: 22183 Regulatory Science 367) in 2025. This research was supported by a grant RS-2024-00331775 from the Ministry of Food and Drug Safety in 2025.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01184"><title>1. Introduction</title><p>As the demand for digital healthcare grows, human activity recognition (HAR) has become imperative for practical applications, such as exercise tracking, fall detection, and elderly monitoring. The activity monitoring plays a crucial role in providing real-time feedback on the behaviors of elderly individuals and those requiring special assistance, supporting both medical rehabilitation and caregiver efforts [<xref rid="B1-sensors-25-01184" ref-type="bibr">1</xref>]. Furthermore, applications such as fall detection and posture analysis can swiftly identify actual fall events, helping to prevent falls and reduce related healthcare costs [<xref rid="B2-sensors-25-01184" ref-type="bibr">2</xref>].</p><p>Numerous HAR systems have been developed using various approaches such as wearable sensors, video cameras, and Kinect sensors. Activity data collected through various sensors are used to recognize both simple and complex activities such as walking, running, sitting, lying, falling, and other daily living activities. In recent years, wearable devices equipped with sensors have gained popularity for monitoring human activities, driven by rapid advancements in sensor [<xref rid="B3-sensors-25-01184" ref-type="bibr">3</xref>]. The most common approach for systems utilizing wearable devices is based on data acquired from a tri-axial accelerometer [<xref rid="B4-sensors-25-01184" ref-type="bibr">4</xref>]. A tri-axial accelerometer measures increasing speed, tilt, inclination, rotation, vibration, and crash along the x, y, and z axes [<xref rid="B5-sensors-25-01184" ref-type="bibr">5</xref>]. These measurements provide valuable data for HAR, where the key lies in feature extraction to identify meaningful patterns and representations related to various movements [<xref rid="B6-sensors-25-01184" ref-type="bibr">6</xref>].</p><p>Traditionally, HAR methodologies have followed two main steps: feature extraction and pattern classification [<xref rid="B7-sensors-25-01184" ref-type="bibr">7</xref>]. The feature extraction process encompasses the selection of features that best represent the characteristics of the accelerometer signals. These features may include measurement such as mean, standard deviations values of the samples in a frame, energy measures, interquartile range, and signal entropy, among others [<xref rid="B8-sensors-25-01184" ref-type="bibr">8</xref>]. However, this process is intricate, as it necessitates the manual selection of hand-crafted features to be employed in classification. Consequently, the efficacy of HAR can significantly vary depending on the chosen features. Moreover, since the features are typically determined by human expertise, users are required to possess specialized knowledge and experience in activity recognition [<xref rid="B3-sensors-25-01184" ref-type="bibr">3</xref>,<xref rid="B9-sensors-25-01184" ref-type="bibr">9</xref>].</p><p>Recently, the integration of deep learning into HAR has been conducted, thereby enabling reducing the dependence on the complex manual selection of representative features. R. Mutegeki et al. [<xref rid="B10-sensors-25-01184" ref-type="bibr">10</xref>] utilized data from IMU (Inertial Measurement Unit) sensors, including accelerometers, gyroscopes, and magnetometers, which were applied to a CNN-LSTM model. However, the results were derived from relatively simple datasets consisting of only four or six classes. C.A. Ronao et al. [<xref rid="B11-sensors-25-01184" ref-type="bibr">11</xref>] applied one-dimensional data obtained from accelerometers and gyroscopes to a CNN through feature engineering. A. Ignatov et al. [<xref rid="B12-sensors-25-01184" ref-type="bibr">12</xref>] and F. Nazar et al. [<xref rid="B13-sensors-25-01184" ref-type="bibr">13</xref>] combined one-dimensional accelerometer data with statistical features and applied them to a CNN model.</p><p>Research on HAR using one-dimensional accelerometer data have exhibited promising results. However, while CNNs perform well on specific types of data, their limitations with one-dimensional data necessitate exploring alternative methods to improve performance [<xref rid="B14-sensors-25-01184" ref-type="bibr">14</xref>]. To address these limitations, this study aims to overcome the challenges of deep learning-based HAR using one-dimensional accelerometer data by leveraging two-dimensional representations. Particularly, converting one-dimensional time-series physiological signals into images and applying them to CNN models has proven effective in mitigating measurement noise [<xref rid="B15-sensors-25-01184" ref-type="bibr">15</xref>]. Furthermore, the graphical characteristics of two-dimensional representations can aid in extracting prominent features that may not be apparent in raw time-series data by revealing hidden patterns and structural changes [<xref rid="B16-sensors-25-01184" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01184" ref-type="bibr">17</xref>].</p><p>In this context, there have been previous studies that convert one-dimensional time-series accelerometer data into two-dimensional image data for use in HAR research. Various studies have demonstrated improvements in HAR performance by transforming time-series data into images using techniques such as Fourier transform [<xref rid="B18-sensors-25-01184" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-01184" ref-type="bibr">19</xref>], recurrence plot [<xref rid="B7-sensors-25-01184" ref-type="bibr">7</xref>,<xref rid="B20-sensors-25-01184" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-01184" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01184" ref-type="bibr">22</xref>], and multi-channel plot [<xref rid="B23-sensors-25-01184" ref-type="bibr">23</xref>], followed by their application in CNN models. Building on these advancements, several research methods such as hybrid, multi-branch, and ensemble HAR were proposed to improve the accuracy of HAR. Several studies have explored sensor fusion using accelerometer, gyroscope, and magnetometer data [<xref rid="B5-sensors-25-01184" ref-type="bibr">5</xref>,<xref rid="B24-sensors-25-01184" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-01184" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-01184" ref-type="bibr">26</xref>], as well as ensemble methods of two models combined [<xref rid="B24-sensors-25-01184" ref-type="bibr">24</xref>,<xref rid="B27-sensors-25-01184" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-01184" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-01184" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-01184" ref-type="bibr">30</xref>]. Additionally, research combining time-series data with short-time Fourier transform (STFT) [<xref rid="B23-sensors-25-01184" ref-type="bibr">23</xref>,<xref rid="B31-sensors-25-01184" ref-type="bibr">31</xref>] has been proposed.</p><p>However, these previous studies typically focus on extracting features from a single image and may not fully capture the implicit characteristics that can be derived from various reconstruction methods. Furthermore, pre-processing steps such as normalization and filtering are often conducted before image transformation. This approach does not fully leverage the noise suppression benefits that images can provide. Moreover, the incorporation of additional sensor data, such gyroscope information, often requires pre-processing, which can complicate the process. These studies also generally evaluate their methods on datasets with fewer and simpler classes, which may not reflect the complexity of real-world scenarios.</p><p>Inspired by this view, this study reconstructs multiple images from a single accelerometer dataset, allowing for the extraction of a broader range of latent features. These reconstructed images are then utilized as inputs in a multi-input feature fusion CNN for HAR. By directly converting the data into images, we are bypassing the need for traditional signal processing steps such as noise reduction and filtering, simplifying the process while taking full advantage of the noise suppression capabilities inherent in image data. Additionally, moving beyond small, simple datasets, we have collected a large-scale dataset that includes 15 distinct activities, enabling a more thorough performance evaluation in real-world contexts. This study builds upon our previous work presented at AIS-I3S 2024 [<xref rid="B31-sensors-25-01184" ref-type="bibr">31</xref>]. The main contributions of this study are as follows:<list list-type="order"><list-item><p>We propose a multi-features fusion CNN architecture with three data reconstruction methods to effectively capture the latent features of time-series data and enhancing feature extraction and classification performance.</p></list-item><list-item><p>This study presents a large-scale HAR dataset with 210 subjects and 15 activities, including daily living and fall-related movements, to support real-life action monitoring.</p></list-item><list-item><p>We validated the proposed method on our own dataset, demonstrating the potential of multi-input fusion CNN in improving HAR performance.</p></list-item></list></p></sec><sec id="sec2-sensors-25-01184"><title>2. Related Works</title><sec><title>HAR with Multi-Feature Fusion Models</title><p>Recent advancements in HAR have explored multi-feature fusion techniques to improve classification accuracy, particularly by integrating multiple sensor modalities or different feature representations. Multi-feature fusion approaches leverage additional sensor data or complementary transformations to enhance feature diversity and robustness.</p><p>Several studies have proposed combining multiple sensor modalities to improve HAR performance. Ravi et al. [<xref rid="B32-sensors-25-01184" ref-type="bibr">32</xref>] transformed accelerometer and gyroscope data into spectrograms and extracted shallow statistical features such as mean, standard deviation, amplitude, skewness, and kurtosis. These spectrogram representations were then fused with the statistical features for classification. Kang et al. [<xref rid="B33-sensors-25-01184" ref-type="bibr">33</xref>] assembled skeleton coordinates and time-series data feature vectors using CNNs and the convolutional block attention module. Webber et al. [<xref rid="B24-sensors-25-01184" ref-type="bibr">24</xref>] fused accelerometer and gyroscope data at the sensor level, feature level, and decision level. Dogan et al. [<xref rid="B5-sensors-25-01184" ref-type="bibr">5</xref>] combined the STFT representations of accelerometer, gyroscope, and magnetometer data. Chai et al. [<xref rid="B34-sensors-25-01184" ref-type="bibr">34</xref>] attached IMU and sEMG sensors to the upper and lower body, then applied SVM, KNN, Decision Tree, Random Forest, and CNN-LSTM model ensembles. However, implementing these methods in real-world scenarios poses challenges due to the necessity of multiple physical sensors, which increases system complexity. Additionally, complex preprocessing is required to synchronize and align different sensor data streams, leading to increased computational resources and processing time.</p><p>Some studies focused on extracting different feature representations from the same dataset using various models. Sahoo et al. [<xref rid="B26-sensors-25-01184" ref-type="bibr">26</xref>] transformed accelerometer data into spectrograms and fused features from MobileNet and EfficientNet. Dua et al. [<xref rid="B27-sensors-25-01184" ref-type="bibr">27</xref>] used the same input but applied CNN-GRU with multiple filter sizes for feature fusion. Jain et al. [<xref rid="B28-sensors-25-01184" ref-type="bibr">28</xref>] applied one-dimensional CNN and LSTM fusion on raw time-series data. Nedorubova et al. [<xref rid="B29-sensors-25-01184" ref-type="bibr">29</xref>] and Almanifi et al. [<xref rid="B30-sensors-25-01184" ref-type="bibr">30</xref>] converted time-series data into Continuous Wavelet Transform (CWT) and applied CNN model ensemble for classification. These studies employed a strategy of generating different representation vectors from a single dataset using multiple models and subsequently fusing them. However, as some models may learn similar features, redundant information from the same data could reduce overall efficiency.</p><p>Several studies have explored feature fusion between handcrafted features and accelerometer data to enhance HAR performance. N. Bento et al. [<xref rid="B35-sensors-25-01184" ref-type="bibr">35</xref>] combined handcrafted features with time-series data using ResNet for feature concatenation and classification. Zheng et al. [<xref rid="B23-sensors-25-01184" ref-type="bibr">23</xref>] compared an accelerometer spectrogram with shallow features and spectrogram-only approaches, concluding that the spectrogram-only model achieved better performance. This suggests that the choice of raw data representation significantly impacts model performance, and the handcrafted features may have drawbacks such as subjectivity and a lack of reproducibility, which can affect model generalizability.</p><p>Another approach explored by G. Sharma et al. [<xref rid="B36-sensors-25-01184" ref-type="bibr">36</xref>] involved contrastive learning by utilizing multi-view attention on spectrograms and time-series accelerometer data. However, since this study relies solely on the original time-series signal and its spectrogram transformation, it may overlook latent features that could be discovered through alternative image transformation methods.</p><p>Most previous studies have focused on converting sensor data into a single-image representation and then applying different models for ensemble learning or integrating multiple sensor modalities. In contrast, we take a different approach by deriving multiple representations from a single accelerometer dataset. By extracting latent features from the same data source, we achieve performance improvement without relying on additional sensors or complex multi-model ensembles. Furthermore, our model is designed with a lightweight architecture, utilizing significantly fewer parameters. This not only enhances computational efficiency but also makes real-world deployment more feasible by reducing resource consumption and inference time.</p></sec></sec><sec id="sec3-sensors-25-01184"><title>3. Materials and Methods</title><sec id="sec3dot1-sensors-25-01184"><title>3.1. Data Collection</title><p>The experiment is performed on our own dataset. Our dataset is acquired by a holter electrocardiography sensor (HiCardi, Mezoo Co., Ltd., Wonju, Republic of Korea) at a 250 Hz sampling rate. The holter electrocardiography patch sensor is attached on the left chest. The Institutional Review Board (IRB) of Yonsei University Health System, Severance Hospital College in South Korea approved the study protocol (No. 1-2023-0006). Informed consent was obtained for all participants enrolled in the data collection. Subjects who voluntarily provided written informed constant prior to participating in the clinical trial, and who are between 20 and 40 years old, without any physical disabilities, and do not experience discomfort in replicating daily activities, exercise postures, or fall scenarios, were included in this study. Exclusion criteria were physical impairments, fear or discomfort in simulating fall postures during fall-related research, and difficulty in wearing the holter electrocardiography sensor. While ECG and accelerometer data were obtained, only accelerometer data were utilized in this study. In total, 210 subjects participated in data collection. The demographic characteristics of the subjects can be found in <xref rid="sensors-25-01184-t001" ref-type="table">Table 1</xref>, with 115 males and 95 females included in the study.</p><p>This dataset is compiled with two types of movements: 6 classes of falls and 9 classes of daily activities. A total of 15 kinds of daily activities were performed, repeated 5 times. The 15 types of activities are detailed in <xref rid="sensors-25-01184-t002" ref-type="table">Table 2</xref> and visualized in <xref rid="sensors-25-01184-f001" ref-type="fig">Figure 1</xref>.</p><p>Compared to publicly available HAR datasets [<xref rid="B37-sensors-25-01184" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-01184" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-01184" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-01184" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-01184" ref-type="bibr">41</xref>], our dataset offers two key advantages. First, it includes data from 210 subjects, which is significantly larger than most existing datasets, where the number of participants typically ranges from 10 to 30. This larger sample size enhances the generalizability of the models, allowing for a more robust performance across diverse individuals. Second, while many HAR datasets primarily focus on basic daily activities, our dataset comprises 15 activities that were carefully selected to reflect real-life scenarios, including both daily movements and fall-related actions. The incorporation of fall-related activities is particularly relevant for action monitoring and safety-critical applications, where the accurate recognition of such movements is essential.</p></sec><sec id="sec3dot2-sensors-25-01184"><title>3.2. Data Preprocessing</title><p>In this study, we focus on converting one-dimensional time-series data into images, a method proven to effectively mitigate measurement noise [<xref rid="B14-sensors-25-01184" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01184" ref-type="bibr">15</xref>]. By leveraging this conversion, we eliminate the need for additional normalization or filtering processes, which are commonly used in data preprocessing. Instead, we directly segment the raw data for further analysis. In HAR, segments typically range from 4 to 10 s [<xref rid="B42-sensors-25-01184" ref-type="bibr">42</xref>]. Using segments instead of single data points is motivated by the fact that the raw inertial measurements fluctuate significantly, making the classification of a single data point impractical [<xref rid="B32-sensors-25-01184" ref-type="bibr">32</xref>]. In this study, the data were segmented into 10 s intervals based on the class to which they belonged.</p><p>A total of 15,750 samples were collected for this study. Of these, 12,600 samples were used to form the training dataset, and 3150 samples were reserved for the test dataset. Additionally, 3150 samples from the training dataset were further separated for use as a validation dataset. In line with standard practices for HAR performance evaluation, subject-independent evaluation was performed by splitting the data at the subject level. Specifically, the training and test datasets were separated to ensure that the same subjects did not appear in both sets. This approach is critical to avoid the model learning subject-specific motion patterns rather than general activity features. Moreover, it better reflects real-world scenarios where the model must classify unseen data during deployment, capturing the challenges of activity recognition in new, unobserved subjects.</p></sec><sec id="sec3dot3-sensors-25-01184"><title>3.3. Data Reconstruction</title><p>The input data for the proposed two-dimensional CNN are required to be of image type. Therefore, accelerometer signals are transformed into images using three different reconstruction methods: spectrogram, modified recurrence plot (RP), and modified multi-channel plot (MP). <xref rid="sensors-25-01184-f002" ref-type="fig">Figure 2</xref> shows the reconstructed images. To reduce training time and computational cost, each method encompasses information from the x, y, and z axes for the comprehensive input image. Each reconstruction method generates one image, resulting in three input images for each time-series dataset.</p><sec id="sec3dot3dot1-sensors-25-01184"><title>3.3.1. Spectrogram</title><p>Spectrograms are generated using short-time Fourier transformation (<italic toggle="yes">STFT</italic>). A spectrogram represents an inertial signal <italic toggle="yes">x</italic> as a function of frequency and time. Specifically, it is computed as the magnitude squared of the <italic toggle="yes">STFT</italic>. The <italic toggle="yes">STFT</italic> calculates the Fourier transform separately for each short segment of the signal. This process is expressed as follows:<disp-formula id="FD1-sensors-25-01184"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo stretchy="false">}</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mi>&#x003c9;</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>j</mml:mi><mml:mi>&#x003c9;</mml:mi><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">x</italic>[<italic toggle="yes">n</italic>] is the signal and <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003c9;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>[<italic toggle="yes">n</italic>] is the window function; <italic toggle="yes">exp</italic>(<italic toggle="yes">&#x02212;j</italic><inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif-italic">&#x003c9;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula><italic toggle="yes">n</italic>) is the exponential function, where <italic toggle="yes">j</italic> is the imaginary unit, and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003c9;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the frequency index. The spectrogram is obtained by squaring the magnitude of the STFT:<disp-formula id="FD2-sensors-25-01184"><label>(2)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="sans-serif-italic">&#x003c9;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="sans-serif-italic">&#x003c9;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>&#x000a0;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The raw accelerometer signal represents the motion acceleration occurring in the time domain. The spectrogram enables the analysis of the differences among the nearest accelerometer data points in the frequency domain, allowing for the examination of the temporal variation in frequency induced by human movement [<xref rid="B42-sensors-25-01184" ref-type="bibr">42</xref>]. In prior studies, window lengths of approximately 1 s [<xref rid="B18-sensors-25-01184" ref-type="bibr">18</xref>,<xref rid="B23-sensors-25-01184" ref-type="bibr">23</xref>], 5 s [<xref rid="B43-sensors-25-01184" ref-type="bibr">43</xref>], 10 s [<xref rid="B44-sensors-25-01184" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-01184" ref-type="bibr">45</xref>], and 12 s [<xref rid="B46-sensors-25-01184" ref-type="bibr">46</xref>] were used to segment raw inertial time-series data. Considering that the cadence of normal walking is approximately 90 steps per minute (1.5 steps per second) [<xref rid="B47-sensors-25-01184" ref-type="bibr">47</xref>], we started our experiments with a 1 s window size and a 50% overlap. For our experiments, spectrograms were generated using window lengths of 1/2/5/10 s, as segments longer than 10 s could not be tested due to the segment length limitation of our dataset. To the best of our knowledge, there has been no prior research that specifically analyzes the optimal window size for effectively converting accelerometer data into spectrograms. Therefore, we conducted an analysis to determine the optimal window size for segmentation. Based on our findings, we used spectrogram images generated with a window size of 10 that produced the best results. Previous studies [<xref rid="B32-sensors-25-01184" ref-type="bibr">32</xref>,<xref rid="B42-sensors-25-01184" ref-type="bibr">42</xref>,<xref rid="B44-sensors-25-01184" ref-type="bibr">44</xref>,<xref rid="B48-sensors-25-01184" ref-type="bibr">48</xref>] converted x, y, and z axes data into separate images for use. However, the data for the tri-axes are horizontally stacked into one image for representation considering computational cost.</p></sec><sec id="sec3dot3dot2-sensors-25-01184"><title>3.3.2. Modified Recurrence Plot (RP)</title><p>A recurrence plot (RP) is a graphical method that shows temporal relationships in time-series data. It refers to the situation where the trajectory approaches a position or state that it has visited before. Such recurrent behavior is not easily observable in the time domain. Therefore, it can be effectively represented using the recurrence plot equation proposed by Eckmann et al. [<xref rid="B49-sensors-25-01184" ref-type="bibr">49</xref>].<disp-formula id="FD3-sensors-25-01184"><label>(3)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="sans-serif-italic">&#x00398;</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003f5;</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mo>&#x02225;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02225;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <italic toggle="yes">x</italic> represents the value of the time series at a given time, and &#x00398; (&#x000b7;) is the Heaviside step function. Traditionally, a recurrence plot uses a Heaviside step function to calculate the distance between two vectors, which is then binarized, with a value of 0 if the distance exceeds a threshold, and 1 if it does not. However, in this study, to examine the changes and distribution of the relative temporal relationships in human activity, the binary process is omitted. Instead, the distance between each vector <italic toggle="yes">x<sub>i</sub></italic> and <italic toggle="yes">x<sub>j</sub></italic> is calculated and used directly as matrix <italic toggle="yes">R</italic>. This method allows for a richer relationship representation and minimized information loss in human activity, rather than just dichotomizing the relationship between the values.</p><p>Recurrence can be a key factor in understanding how signals exhibit predictable patterns or movements. By visualizing recurrence data in the form of a plot, hidden patterns, dependencies, and regularities within the data can be uncovered [<xref rid="B22-sensors-25-01184" ref-type="bibr">22</xref>]. This is particularly useful for repetitive activities such as walking, climbing stairs, and running, where recurrent patterns can reveal underlying regularities.</p><p>While traditional RPs are typically based on a single time-series signal, accelerometer data consist of three axes. To address this, we employed a modified RP approach [<xref rid="B21-sensors-25-01184" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01184" ref-type="bibr">22</xref>]. For each axis of the accelerometer data, we computed an individual recurrence plot and then combined the plots by merging the x, y, and z axes into a single RGB image. In this RGB image, each color channel corresponds to the recurrence plot of one accelerometer axis.</p></sec><sec id="sec3dot3dot3-sensors-25-01184"><title>3.3.3. Modified Multi-Channel Plot (MP)</title><p>In the modified MP method, the raw accelerometer data values were normalized using a min&#x02013;max scaler to scale them within the range of 0 to 255 before being mapped to an RGB image. The data values from the x, y, and z axes were then segmented into different parts: the integer, the first to second decimal places, and the third to fourth decimal places, respectively. These segmented values were assigned to the R, G, and B channels of the resulting image. To facilitate an understanding of this process, refer to <xref rid="sensors-25-01184-f003" ref-type="fig">Figure 3</xref>. By integrating the information from all three axes into a single image, subtle differences between activities can be visually represented. This method follows the approach presented in Hur T. et al. [<xref rid="B50-sensors-25-01184" ref-type="bibr">50</xref>]. This process allows the continuous correlation of the signal to be reflected in the RGB channels by applying a slight distortion to the accelerometer data&#x02019;s real numbers.</p></sec></sec><sec id="sec3dot4-sensors-25-01184"><title>3.4. Multi-Features Fusion CNN Model</title><p>The proposed method follows a CNN architecture designed to efficiently extract spatial features from the transformed accelerometer dataset using multiple data reconstruction methods: spectrogram, modified recurrence plot, and modified multichannel plot. Due to the nature of the transformed input data, we leverage CNNs, which possess an inductive bias that makes them highly effective for processing image-based data. The overall architecture of the model is shown in <xref rid="sensors-25-01184-f004" ref-type="fig">Figure 4</xref>. Each multi-input image fed into three separate CNN branches consists of 5 convolutional blocks and a fully connected layer. Each CNN processes its corresponding 2D image representation independently, generating three distinct feature vectors. These three feature vectors are concatenated into a unified representation, and the concatenated feature vector is then passed through 2 fully connected layers (units: 256, number of classes) for human activity classification. The softmax function is applied to vectors coming out from the model before the loss computation.</p><p>The detailed structures of convolutional blocks and each CNN model are shown in <xref rid="sensors-25-01184-f005" ref-type="fig">Figure 5</xref>. The input to the network is a three-channel image size of 64 &#x000d7; 64 &#x000d7; 3. The feature extraction process consists of a series of convolutional blocks, each performing downsampling while increasing the number of feature channels. At the final convolutional block, the output feature map has a spatial size of 2 &#x000d7; 2 with 64 channels, resulting in a flattened feature vector size of 256. This feature vector is then passed through a fully connected layer with 1024 units to enhance the representation capacity before being fed into the feature fusion stage. The feature vectors can reflect the distinct characteristics of the reconstructed input, and these are concatenated for feature fusion. The choice of maintaining a (2, 2, 64) shape at the last convolutional layer, rather than further increasing the number of channels, is intentional to balance feature extraction efficiency and computational cost. By keeping the final feature map compact, we reduce the number of trainable parameters while preserving essential feature information. This parameter reduction is particularly crucial in HAR, where models often need to be deployed on resource-constrained devices. A smaller number of parameters leads to lower memory requirements and faster inference times, making the model more suitable for real-time HAR applications.</p><p>In HAR, lightweight and fast classification is an important factor, making it essential to select the most efficient model. Therefore, this study utilized a 5-layer simple CNN with a minimal number of parameters. To confirm that this 5-layer simple CNN is advantageous not only in terms of parameter efficiency but also in performance, its effectiveness was compared against backbones such as VGG16 [<xref rid="B51-sensors-25-01184" ref-type="bibr">51</xref>], ResNet50 [<xref rid="B52-sensors-25-01184" ref-type="bibr">52</xref>], MobileNet_v2 [<xref rid="B53-sensors-25-01184" ref-type="bibr">53</xref>], and EfficientNet_b0 [<xref rid="B54-sensors-25-01184" ref-type="bibr">54</xref>], and CNN-LSTM and CNN-GRU hybrid models.</p></sec><sec id="sec3dot5-sensors-25-01184"><title>3.5. Evaluation</title><p>For evaluating our multi-features fusion network, the top 1 accuracy, top 3 accuracy, precision, recall, and ROC AUC are calculated, as these parameters are widely accepted evaluations for classification tasks. Accuracy, precision, and recall are defined as Equations (4)&#x02013;(6), respectively:<disp-formula id="FD4-sensors-25-01184"><label>(4)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD5-sensors-25-01184"><label>(5)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-01184"><label>(6)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">TP</italic> is True Positive, <italic toggle="yes">TN</italic> is True Negative, <italic toggle="yes">FP</italic> is False Positive, and <italic toggle="yes">FN</italic> is False Negative. ROC AUC evaluates the model&#x02019;s ability to distinguish between classes by summarizing the trade-off between the <italic toggle="yes">TP</italic> rate and the <italic toggle="yes">FP</italic> rate across different thresholds. A higher AUC indicates better performance.</p></sec></sec><sec id="sec4-sensors-25-01184"><title>4. Experimental Setup</title><p>The experimental setup involved using TensorFlow 2.4.0 and Python 3.6.13 on a system equipped with Intel<sup>&#x000ae;</sup> Core&#x02122; i7-10700K CPU and NVIDIA RTX 3090 GPU. The models were trained without data augmentation. The training was initialized with the number of epochs at 50. Additionally, the ReduceLROnPlateau function was employed to establish a mechanism where the learning rate diminishes by a factor of 0.7 if the validation loss remains stagnant for over three consecutive epochs. The size of input images is fixed at 64 &#x000d7; 64, and the batch size was set to eight. Categorical cross-entropy was used for the loss function. An Adam optimizer with a learning rate of 0.005 was utilized. All experiments were conducted four times, and the mean and standard deviation of the results were recorded.</p></sec><sec sec-type="results" id="sec5-sensors-25-01184"><title>5. Results and Discussion</title><sec id="sec5dot1-sensors-25-01184"><title>5.1. Comparison Study on Spectrogram Window Sizes</title><p>To investigate the impact of spectrogram window size, HAR performance was compared across different window sizes. The window sizes were set to 1, 2, 5, and 10 s, with an overlap of half the window size for each setting. The experimental results (<xref rid="sensors-25-01184-t003" ref-type="table">Table 3</xref>) showed that using a 10 s window size achieved the highest performance across all evaluation metrics. Particularly, the 10 s window size achieved a top one accuracy of 77.89%, representing an improvement of over 10% compared to the 1 s window. The superior performance demonstrated by the 10 s window spectrogram can be explained by several factors. Longer segments provide a greater number of data points, enabling the model to better capture overall patterns and the distinguishing characteristics of various activities. Spectrograms, which analyze signals in the time&#x02013;frequency domain, benefit from longer segments, as they provide a richer representation of frequency variation patterns. Also, shorter segments are more prone to noise, which can obscure meaningful signals and degrade classification accuracy.</p></sec><sec id="sec5dot2-sensors-25-01184"><title>5.2. Comparison of Image Transformation Techniques</title><p>As shown in <xref rid="sensors-25-01184-t004" ref-type="table">Table 4</xref>, it can be observed that the proposed method outperforms each data reconstruction method individually. To evaluate the performance of one-dimensional time-series data, 2D convolutions were replaced with 1D convolutions, and raw accelerometer data were used as a single input. Particularly, the proposed method exhibited 16.77%, 16.13%, and 19.72% improvement over the raw time-series model in terms of accuracy, precision, and recall. When comparing the performance of models based on different data reconstruction methods, the model that fused features from the three input values demonstrated the best performance. This suggests that combining features derived from each reconstructed graphic effectively captures the latent patterns and variations inherent in accelerometer data.</p><p>Based on the confusion matrix shown in <xref rid="sensors-25-01184-f006" ref-type="fig">Figure 6</xref>, it is evident that across all models, including the proposed model, the most confusing classes areas follows: fall forward and fall forward on one&#x02019;s knees. In the case of &#x02018;fall forward&#x02019; and &#x02018;fall forward on one&#x02019;s knees&#x02019;, both actions share falling forward motion elements. Therefore, the movements can be very close in terms of accelerometer data.</p><p>The spectrogram-based model exhibited notable confusion between &#x02018;lying up on the bed&#x02019; and &#x02018;lying down on the bed&#x02019;, &#x02018;squat and rise&#x02019;, and &#x02018;sitting on the chair&#x02019;, as well as among the fall-related classes. This can be attributed to the model&#x02019;s limited utilization of temporal information, which hindered its ability to effectively capture the subtle differences in brief and rapid movements. The misclassification of &#x02018;squat and rise&#x02019; as &#x02018;sitting on the chair&#x02019; was the most prominent issue, likely due to the similarity in their low-frequency patterns. The &#x02018;squat and rise&#x02019; movement involves gradual changes in low-frequency components as the body lowers and rises, while the &#x02018;sitting on the chair&#x02019; movement also generates strong low-frequency energy during the transition from standing to sitting. Furthermore, the use of a 10 s window size for STFT may have blurred finer movement details. The modified RP-based model focuses on capturing the recurrence and repetition of time-series data. The RP-based method performed poorly in distinguishing activities such as &#x02018;lying up on the bed&#x02019; and &#x02018;lying down on the bed&#x02019;, &#x02018;look left and fall&#x02019;, and &#x02018;look right and fall&#x02019;, where the magnitudes of accelerometer data are similar, but the directional patterns are opposite. This limitation arises because the model interprets these activities as recurring patterns without effectively distinguishing their directional differences. Meanwhile, the modified MP-based model showed reduced performance in fall-related activities, likely due to the random noise and abrupt signal changes inherent to such activities. These challenges suggest that while individual models have strengths, their ability to distinguish similar activity patterns remains constrained by their respective feature representations.</p><p>In contrast, the multi-features fusion model achieved superior accuracy by integrating temporal, spatial, and frequency features, enabling it to capture nuanced movement patterns and effectively differentiate overlapping activity classes, as demonstrated in its clear separation of confusion matrix patterns. In this experiment, HAR is achievable through the 2D CNN model without preprocessing such as noise reduction in the time-series data. One-dimensional data can be reconstructed into a spectrogram, modified RP, and modified MP, allowing the extraction of the frequency domain, temporal relation, and time domain-based features, respectively. The fusion of the extracted features has the potential to markedly enhance performances.</p></sec><sec id="sec5dot3-sensors-25-01184"><title>5.3. Comparison of Noise Robustness</title><p>To evaluate the robustness of our method against noise, we introduced Gaussian noise with different intensity levels to the test dataset. The noise levels were set to 5%, 10%, 20%, 30%, 40%, and 50% of the standard deviation (<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> = 550.94) of the original data, corresponding to a <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> value of [22.5, 55, 110, 165, 220, 275]. This approach allows us to systemically analyze the model&#x02019;s performance across varying degrees of noise contamination. Gaussian noise was added to each value of the dataset. <xref rid="sensors-25-01184-f007" ref-type="fig">Figure 7</xref> illustrates the extent of distortion introduced by the varying degrees of noise added.</p><p>The proposed model achieves higher accuracy than the time-series model in the noise-free condition and consistently maintains better performance as noise levels increase, as shown in <xref rid="sensors-25-01184-f008" ref-type="fig">Figure 8</xref>a. Both models degrade when more noise is introduced, and the proposed model shows a smaller drop in performance. The bar charts shown in <xref rid="sensors-25-01184-f008" ref-type="fig">Figure 8</xref>b quantify how much performance declines compared to the original baseline, demonstrating that the time-series model is more vulnerable to noise. Overall, these results demonstrate that our method maintains higher accuracy and stability under noisy conditions compared to conventional time-series approaches. This suggests that our approach is more robust to real-world data imperfections. These findings highlight the potential of our approach for real-world applications where data contamination is inevitable, such as clinical or daily living settings.</p></sec><sec id="sec5dot4-sensors-25-01184"><title>5.4. Comparison with Different Feature Extractor Backbones</title><p>To compare the performance of the backbone for the multi-input CNN, the proposed 5-layer simple CNN was evaluated alongside ImageNet pretrained VGG16, ResNet50, EfficientNet_b0, and MobileNet. Also, we tested the CNN-LSTM and CNN-GRU models. Traditional models were utilized because efficient signal processing for HAR is best achieved with smaller, faster models. As shown in <xref rid="sensors-25-01184-t005" ref-type="table">Table 5</xref>, the proposed simple CNN model, which has the fewest parameters, demonstrated the highest performance.</p><p>The proposed model&#x02014;built from a very simple 5-layer CNN&#x02014;surpasses more complex backbones such as VGG16 and ResNet50, and even hybrid architectures like CNN-LSTM and CNN-GRU in terms of top 1 and top 3 accuracy, precision, recall, and ROC-AUC, despite using far fewer parameters. Specifically, the proposed model has around 3 million parameters, significantly lower than VGG16 (about 21.8 million), ResNet50 (about 25 million), MobileNet_v2 (about 10.5 million), EfficientNet_b0 (about 20.6 million), and CNN-LSTM/GUR (about 7 million each). Despite its streamlined design, it achieves a top 1 accuracy of 91.87%, with precision and recall both exceeding 91%.</p><p>HAR tasks often do not require highly complex models with excessive parameters [<xref rid="B55-sensors-25-01184" ref-type="bibr">55</xref>]. Models with a large number of parameters are prone to overfitting, especially when trained on datasets that are not sufficiently large. Accelerometer data used in HAR tasks are inherently low-dimensional and represented by three axes with repetitive patterns. They lack high-frequency spatial or contextual complexity unlike natural image or text data. Complex architectures including EfficientNet or ResNet50 are designed to process such rich, high-dimensional data. However, our proposed method is designed to avoid data preprocessing steps such as filtering and normalization. The noise of the image data can inadvertently amplify noise due to their capacity to overfit fine-grained details. By keeping the architecture shallow, our model ensures that critical features remain prominent throughout the network, reducing the risk of losing important information through over-parameterization or excessive downsampling. Also, the key reason for the superior performance of the proposed model is that models like CNN-LSTM and CNN-GRU, which excel at handling sequential data, did not fully exploit their time-series advantages because the data were converted into a single image. As a result, the inherent temporal relationships that LSTM and GRU architectures had were not effectively captured.</p><p>The proposed simple CNN is specifically designed to efficiently learn the core features of the accelerometer data, resulting in a better balance between model complexity and task requirements. Thus, the superior performance of the proposed model highlights the importance of selecting an appropriate model.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-01184"><title>6. Conclusions</title><p>The significance of our research lies in demonstrating that human activity recognition can be effectively achieved using a multi-features deep learning model by reconstructing time-series data without complex signal preprocessing steps. By integrating a spectrogram, modified recurrence plot, and modified multichannel plot, the proposed method successfully captures frequency, temporal, and time domain features, enhancing feature extraction and classification performance. The results highlight the superior performance of the proposed simple CNN architecture, which outperformed larger, parameter-intensive models. Through validation on our own motion dataset, we have showcased the effectiveness and robustness of this approach, offering a streamlined and efficient method for HAR that can be readily implemented in real-world applications. Additionally, this study reveals the importance of using an optimal spectrogram window size, with a 10 s window achieving the highest performance due to its ability to provide richer frequency domain information and reduce noise.</p><p>Our proposed method reduces the number of parameters, enabling faster training and inference while maintaining high performance. With an inference time of approximately 0.205 s for a 10 s data interval, the model is capable of processing data and delivering predictions promptly. This makes the model particularly suitable for real-time applications in resource-constrained environments. The lightweight design is well suited for deployment in embedded systems or mobile devices, where computational resources and energy efficiency are critical.</p><p>Further improvements in this study could lead to broader investigations. Firstly, data collection from a wider age range and the evaluation of its effectiveness in a general context. In this study, we collected data from participants aged 20 to 40 due to practical constraints. Collecting data from individuals over 40, particularly the elderly, presented several challenges. Older adults may have difficulty performing certain movements, such as squats or balance-related tasks, making data collection less feasible. Additionally, given the study timeline and available resources, we had to limit the participant age range. Future research should address this limitation by expanding the dataset to include a broader age range. Furthermore, further research is needed to develop safe methods for collecting movement data from elderly individuals without the risk of injury. Secondly, since the data collected include bioelectric signals (EMG) through the Holter ECG system, further study could explore the integration of this electric signal [<xref rid="B33-sensors-25-01184" ref-type="bibr">33</xref>,<xref rid="B56-sensors-25-01184" ref-type="bibr">56</xref>] to enhance the performance of HAR systems.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, methodology, writing&#x02014;original draft preparation, writing&#x02014;review and editing, visualization, software, J.E.K.; validation, data curation, S.K.; validation, formal analysis, J.H.S.; supervision, project administration, funding acquisition, S.M.K.; All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>The study was conducted in accordance with the Declaration of Helsinki, and approved by the Institutional Review Board of Yonsei University Health System, and Severance Hospital College in South Korea approved the study protocol (No. 1-2023-0006).</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in this study. Written informed consent was obtained from the patients to publish this paper.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available on request from the corresponding author due to privacy.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01184"><label>1.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mendes</surname><given-names>S.</given-names></name>
<name><surname>Queiroz</surname><given-names>J.</given-names></name>
<name><surname>Leit&#x000e3;o</surname><given-names>P.</given-names></name>
</person-group><article-title>Data driven multi-agent m-health system to characterize the daily activities of elderly people</article-title><source>Proceedings of the 2017 12th Iberian Conference on Information Systems and Technologies (CISTI)</source><conf-loc>Lisbon, Portugal</conf-loc><conf-date>21&#x02013;24 June 2017</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B2-sensors-25-01184"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nweke</surname><given-names>H.F.</given-names></name>
<name><surname>Teh</surname><given-names>Y.W.</given-names></name>
<name><surname>Mujtaba</surname><given-names>G.</given-names></name>
<name><surname>Al-garadi</surname><given-names>M.A.</given-names></name>
</person-group><article-title>Data fusion and multiple classifier systems for human activity detection and health monitoring: Review and open research directions</article-title><source>Inf. Fusion</source><year>2019</year><volume>46</volume><fpage>147</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2018.06.002</pub-id></element-citation></ref><ref id="B3-sensors-25-01184"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Hao</surname><given-names>S.</given-names></name>
<name><surname>Peng</surname><given-names>X.</given-names></name>
<name><surname>Hu</surname><given-names>L.</given-names></name>
</person-group><article-title>Deep Learning for Sensor-based Activity Recognition: A Survey</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1707.03502</pub-id><pub-id pub-id-type="doi">10.1016/j.patrec.2018.02.010</pub-id></element-citation></ref><ref id="B4-sensors-25-01184"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aphairaj</surname><given-names>D.</given-names></name>
<name><surname>Kitsonti</surname><given-names>M.</given-names></name>
<name><surname>Thanapornsawan</surname><given-names>T.</given-names></name>
</person-group><article-title>Fall detection system with 3-axis accelerometer</article-title><source>J. Phys. Conf. Ser.</source><year>2019</year><volume>1380</volume><fpage>012060</fpage><pub-id pub-id-type="doi">10.1088/1742-6596/1380/1/012060</pub-id></element-citation></ref><ref id="B5-sensors-25-01184"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Dogan</surname><given-names>G.</given-names></name>
<name><surname>Ertas</surname><given-names>S.S.</given-names></name>
<name><surname>Cay</surname><given-names>&#x00130;.</given-names></name>
</person-group><article-title>Human activity recognition using convolutional neural networks</article-title><source>Proceedings of the 2021 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)</source><conf-loc>Melbourne, Australia</conf-loc><conf-date>13&#x02013;15 October 2021</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B6-sensors-25-01184"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Nguyen</surname><given-names>M.N.</given-names></name>
<name><surname>San</surname><given-names>P.P.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Krishnaswamy</surname><given-names>S.</given-names></name>
</person-group><article-title>Deep convolutional neural networks on multichannel time series for human activity recognition</article-title><source>Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)</source><conf-loc>Buenos Aires, Argentina</conf-loc><conf-date>25&#x02013;31 July 2015</conf-date><fpage>3995</fpage><lpage>4001</lpage></element-citation></ref><ref id="B7-sensors-25-01184"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ngo</surname><given-names>T.</given-names></name>
<name><surname>Champion</surname><given-names>B.T.</given-names></name>
<name><surname>Joordens</surname><given-names>M.A.</given-names></name>
<name><surname>Price</surname><given-names>A.</given-names></name>
<name><surname>Morton</surname><given-names>D.</given-names></name>
<name><surname>Pathirana</surname><given-names>P.N.</given-names></name>
</person-group><article-title>Recurrence Quantification Analysis for Human Activity Recognition</article-title><source>Proceedings of the 2020 42nd Annual International Conference of the IEEE Engineering in Medicine &#x00026; Biology Society (EMBC)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>20&#x02013;24 July 2020</conf-date><fpage>4616</fpage><lpage>4619</lpage></element-citation></ref><ref id="B8-sensors-25-01184"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>J.</given-names></name>
<name><surname>San-Segundo-Hern&#x000e1;ndez</surname><given-names>R.</given-names></name>
<name><surname>Pardo</surname><given-names>J.M.</given-names></name>
</person-group><article-title>Feature extraction for robust physical activity recognition</article-title><source>Hum.-Centric Comput. Inf. Sci.</source><year>2017</year><volume>7</volume><fpage>16</fpage><pub-id pub-id-type="doi">10.1186/s13673-017-0097-2</pub-id></element-citation></ref><ref id="B9-sensors-25-01184"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Raj</surname><given-names>R.</given-names></name>
<name><surname>Kos</surname><given-names>A.</given-names></name>
</person-group><article-title>An improved human activity recognition technique based on convolutional neural network</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><fpage>22581</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-49739-1</pub-id><pub-id pub-id-type="pmid">38114574</pub-id>
</element-citation></ref><ref id="B10-sensors-25-01184"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mutegeki</surname><given-names>R.</given-names></name>
<name><surname>Han</surname><given-names>D.S.</given-names></name>
</person-group><article-title>A CNN-LSTM Approach to Human Activity Recognition</article-title><source>Proceedings of the 2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)</source><conf-loc>Fukuoka, Japan</conf-loc><conf-date>19&#x02013;21 February 2020</conf-date><fpage>362</fpage><lpage>366</lpage></element-citation></ref><ref id="B11-sensors-25-01184"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ronao</surname><given-names>C.A.</given-names></name>
<name><surname>Cho</surname><given-names>S.-B.</given-names></name>
</person-group><article-title>Human activity recognition with smartphone sensors using deep learning neural networks</article-title><source>Expert Syst. Appl.</source><year>2016</year><volume>59</volume><fpage>235</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2016.04.032</pub-id></element-citation></ref><ref id="B12-sensors-25-01184"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ignatov</surname><given-names>A.</given-names></name>
</person-group><article-title>Real-time human activity recognition from accelerometer data using Convolutional Neural Networks</article-title><source>Appl. Soft Comput.</source><year>2018</year><volume>62</volume><fpage>915</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1016/j.asoc.2017.09.027</pub-id></element-citation></ref><ref id="B13-sensors-25-01184"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Nazar</surname><given-names>F.</given-names></name>
<name><surname>Jalal</surname><given-names>A.</given-names></name>
</person-group><article-title>Wearable Sensor-Based Activity Recognition over Statistical Features Selection and MLP Approach</article-title><source>Proceedings of the 2024 3rd International Conference on Emerging Trends in Electrical, Control, and Telecommunication Engineering (ETECTE)</source><conf-loc>Lahore, Pakistan</conf-loc><conf-date>6&#x02013;27 November 2024</conf-date><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="B14-sensors-25-01184"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>B.</given-names></name>
<name><surname>Yao</surname><given-names>B.</given-names></name>
<name><surname>He</surname><given-names>W.</given-names></name>
</person-group><article-title>ECG Arrhythmia Classification Using STFT-Based Spectrogram and Convolutional Neural Network</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>92871</fpage><lpage>92880</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2928017</pub-id></element-citation></ref><ref id="B15-sensors-25-01184"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jun</surname><given-names>T.J.</given-names></name>
<name><surname>Nguyen</surname><given-names>H.M.</given-names></name>
<name><surname>Kang</surname><given-names>D.</given-names></name>
<name><surname>Kim</surname><given-names>D.</given-names></name>
<name><surname>Kim</surname><given-names>D.</given-names></name>
<name><surname>Kim</surname><given-names>Y.-H.</given-names></name>
</person-group><article-title>ECG arrhythmia classification using a 2-D convolutional neural network</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1804.06812</pub-id></element-citation></ref><ref id="B16-sensors-25-01184"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Souza</surname><given-names>V.M.A.</given-names></name>
<name><surname>Silva</surname><given-names>D.F.</given-names></name>
<name><surname>Batista</surname><given-names>G.E.A.P.A.</given-names></name>
</person-group><article-title>Extracting Texture Features for Time Series Classification</article-title><source>Proceedings of the 2014 22nd International Conference on Pattern Recognition</source><conf-loc>Stockholm, Sweden</conf-loc><conf-date>24&#x02013;28 August 2014</conf-date><fpage>1425</fpage><lpage>1430</lpage></element-citation></ref><ref id="B17-sensors-25-01184"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qin</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Meng</surname><given-names>S.</given-names></name>
<name><surname>Qin</surname><given-names>Z.</given-names></name>
<name><surname>Choo</surname><given-names>K.-K.R.</given-names></name>
</person-group><article-title>Imaging and fusing time series for wearable sensor-based human activity recognition</article-title><source>Inf. Fusion</source><year>2020</year><volume>53</volume><fpage>80</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2019.06.014</pub-id></element-citation></ref><ref id="B18-sensors-25-01184"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Alemayoh</surname><given-names>T.T.</given-names></name>
<name><surname>Lee</surname><given-names>J.H.</given-names></name>
<name><surname>Okamoto</surname><given-names>S.</given-names></name>
</person-group><article-title>Deep Learning Based Real-time Daily Human Activity Recognition and Its Implementation in a Smartphone</article-title><source>Proceedings of the 2019 16th International Conference on Ubiquitous Robots (UR)</source><conf-loc>Jeju, Republic of Korea</conf-loc><conf-date>24&#x02013;27 June 2019</conf-date><fpage>179</fpage><lpage>182</lpage></element-citation></ref><ref id="B19-sensors-25-01184"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Jiang</surname><given-names>W.</given-names></name>
<name><surname>Yin</surname><given-names>Z.</given-names></name>
</person-group><article-title>Human Activity Recognition Using Wearable Sensors by Deep Convolutional Neural Networks</article-title><source>Proceedings of the 23rd ACM International Conference on Multimedia</source><conf-loc>Brisbane, Australia</conf-loc><conf-date>26&#x02013;30 October 2015</conf-date><fpage>1307</fpage><lpage>1310</lpage></element-citation></ref><ref id="B20-sensors-25-01184"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hatami</surname><given-names>N.</given-names></name>
<name><surname>Gavet</surname><given-names>Y.</given-names></name>
<name><surname>Debayle</surname><given-names>J.</given-names></name>
</person-group><article-title>Bag of recurrence patterns representation for time-series classification</article-title><source>Pattern Anal. Appl.</source><year>2019</year><volume>22</volume><fpage>877</fpage><lpage>887</lpage><pub-id pub-id-type="doi">10.1007/s10044-018-0703-6</pub-id></element-citation></ref><ref id="B21-sensors-25-01184"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Penatti</surname><given-names>O.A.B.</given-names></name>
<name><surname>Santos</surname><given-names>M.F.S.</given-names></name>
</person-group><article-title>Human activity recognition from mobile inertial sensors using recurrence plots</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1712.01429</pub-id></element-citation></ref><ref id="B22-sensors-25-01184"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lew</surname><given-names>C.H.</given-names></name>
<name><surname>Lim</surname><given-names>K.M.</given-names></name>
<name><surname>Lee</surname><given-names>C.P.</given-names></name>
<name><surname>Lim</surname><given-names>J.Y.</given-names></name>
</person-group><article-title>Human Activity Classification Using Recurrence Plot and Residual Network</article-title><source>Proceedings of the 2023 IEEE 11th Conference on Systems, Process &#x00026; Control (ICSPC)</source><conf-loc>Malacca, Malaysia</conf-loc><conf-date>16 December 2023</conf-date><fpage>78</fpage><lpage>83</lpage></element-citation></ref><ref id="B23-sensors-25-01184"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>M.</given-names></name>
<name><surname>Ordieres-Mer&#x000e9;</surname><given-names>J.</given-names></name>
</person-group><article-title>Comparison of Data Preprocessing Approaches for Applying Deep Learning to Human Activity Recognition in the Context of Industry 4.0</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>2146</elocation-id><pub-id pub-id-type="doi">10.3390/s18072146</pub-id><pub-id pub-id-type="pmid">29970873</pub-id>
</element-citation></ref><ref id="B24-sensors-25-01184"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Webber</surname><given-names>M.</given-names></name>
<name><surname>Rojas</surname><given-names>R.F.</given-names></name>
</person-group><article-title>Human Activity Recognition With Accelerometer and Gyroscope: A Data Fusion Approach</article-title><source>IEEE Sens. J.</source><year>2021</year><volume>21</volume><fpage>16979</fpage><lpage>16989</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2021.3079883</pub-id></element-citation></ref><ref id="B25-sensors-25-01184"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Akter</surname><given-names>M.</given-names></name>
<name><surname>Ansary</surname><given-names>S.</given-names></name>
<name><surname>Khan</surname><given-names>M.A.-M.</given-names></name>
<name><surname>Kim</surname><given-names>D.</given-names></name>
</person-group><article-title>Human Activity Recognition Using Attention-Mechanism-Based Deep Learning Feature Combination</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>5715</elocation-id><pub-id pub-id-type="doi">10.3390/s23125715</pub-id><pub-id pub-id-type="pmid">37420881</pub-id>
</element-citation></ref><ref id="B26-sensors-25-01184"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sahoo</surname><given-names>K.K.</given-names></name>
<name><surname>Ghosh</surname><given-names>R.</given-names></name>
<name><surname>Mallik</surname><given-names>S.</given-names></name>
<name><surname>Roy</surname><given-names>A.</given-names></name>
<name><surname>Singh</surname><given-names>P.K.</given-names></name>
<name><surname>Zhao</surname><given-names>Z.</given-names></name>
</person-group><article-title>Wrapper-based deep feature optimization for activity recognition in the wearable sensor networks of healthcare systems</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><fpage>965</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-27192-w</pub-id><pub-id pub-id-type="pmid">36653370</pub-id>
</element-citation></ref><ref id="B27-sensors-25-01184"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dua</surname><given-names>N.</given-names></name>
<name><surname>Singh</surname><given-names>S.N.</given-names></name>
<name><surname>Semwal</surname><given-names>V.B.</given-names></name>
<name><surname>Challa</surname><given-names>S.K.</given-names></name>
</person-group><article-title>Inception inspired CNN-GRU hybrid network for human activity recognition</article-title><source>Multimed. Tools Appl.</source><year>2023</year><volume>82</volume><fpage>5369</fpage><lpage>5403</lpage><pub-id pub-id-type="doi">10.1007/s11042-021-11885-x</pub-id></element-citation></ref><ref id="B28-sensors-25-01184"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jain</surname><given-names>R.</given-names></name>
<name><surname>Semwal</surname><given-names>V.B.</given-names></name>
<name><surname>Kaushik</surname><given-names>P.</given-names></name>
</person-group><article-title>Deep ensemble learning approach for lower extremity activities recognition using wearable sensors</article-title><source>Expert Syst.</source><year>2022</year><volume>39</volume><fpage>e12743</fpage><pub-id pub-id-type="doi">10.1111/exsy.12743</pub-id></element-citation></ref><ref id="B29-sensors-25-01184"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nedorubova</surname><given-names>A.</given-names></name>
<name><surname>Kadyrova</surname><given-names>A.</given-names></name>
<name><surname>Khlyupin</surname><given-names>A.</given-names></name>
</person-group><article-title>Human Activity Recognition using Continuous Wavelet Transform and Convolutional Neural Networks</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2106.12666</pub-id></element-citation></ref><ref id="B30-sensors-25-01184"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Almanifi</surname><given-names>O.R.A.</given-names></name>
<name><surname>Khairuddin</surname><given-names>I.M.</given-names></name>
<name><surname>Razman</surname><given-names>M.A.M.</given-names></name>
<name><surname>Musa</surname><given-names>R.M.</given-names></name>
<name><surname>Majeed</surname><given-names>A.P.P.A.</given-names></name>
</person-group><article-title>Human activity recognition based on wrist PPG via the ensemble method</article-title><source>ICT Express</source><year>2022</year><volume>8</volume><fpage>513</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1016/j.icte.2022.03.006</pub-id></element-citation></ref><ref id="B31-sensors-25-01184"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ko</surname><given-names>J.</given-names></name>
<name><surname>Sung</surname><given-names>J.</given-names></name>
<name><surname>Kwon</surname><given-names>J.</given-names></name>
<name><surname>Kim</surname><given-names>S.</given-names></name>
</person-group><article-title>Accelerometer Data Reconstruction Methods in Features-Fusion CNN Model for Enhanced Human Activity Recognition</article-title><source>Proceedings of the 1st International Conference on AI Sensors and the 10th International Symposium on Sensor Science</source><conf-loc>Singapore</conf-loc><conf-date>1&#x02013;4 August 2024</conf-date></element-citation></ref><ref id="B32-sensors-25-01184"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rav&#x000ec;</surname><given-names>D.</given-names></name>
<name><surname>Wong</surname><given-names>C.</given-names></name>
<name><surname>Lo</surname><given-names>B.P.L.</given-names></name>
<name><surname>Yang</surname><given-names>G.-Z.</given-names></name>
</person-group><article-title>A Deep Learning Approach to on-Node Sensor Data Analytics for Mobile or Wearable Devices</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2017</year><volume>21</volume><fpage>56</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2016.2633287</pub-id><pub-id pub-id-type="pmid">28026792</pub-id>
</element-citation></ref><ref id="B33-sensors-25-01184"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kang</surname><given-names>J.</given-names></name>
<name><surname>Shin</surname><given-names>J.</given-names></name>
<name><surname>Shin</surname><given-names>J.</given-names></name>
<name><surname>Lee</surname><given-names>D.</given-names></name>
<name><surname>Choi</surname><given-names>A.</given-names></name>
</person-group><article-title>Robust Human Activity Recognition by Integrating Image and Accelerometer Sensor Data Using Deep Fusion Network</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>174</elocation-id><pub-id pub-id-type="doi">10.3390/s22010174</pub-id><pub-id pub-id-type="pmid">35009717</pub-id>
</element-citation></ref><ref id="B34-sensors-25-01184"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chai</surname><given-names>X.</given-names></name>
<name><surname>Lee</surname><given-names>B.G.</given-names></name>
<name><surname>Hu</surname><given-names>C.</given-names></name>
<name><surname>Pike</surname><given-names>M.</given-names></name>
<name><surname>Chieng</surname><given-names>D.</given-names></name>
<name><surname>Wu</surname><given-names>R.</given-names></name>
<name><surname>Chung</surname><given-names>W.-Y.</given-names></name>
</person-group><article-title>IoT-FAR: A multi-sensor fusion approach for IoT-based firefighting activity recognition</article-title><source>Inf. Fusion</source><year>2025</year><volume>113</volume><fpage>102650</fpage><pub-id pub-id-type="doi">10.1016/j.inffus.2024.102650</pub-id></element-citation></ref><ref id="B35-sensors-25-01184"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bento</surname><given-names>N.</given-names></name>
<name><surname>Rebelo</surname><given-names>J.</given-names></name>
<name><surname>Carreiro</surname><given-names>A.V.</given-names></name>
<name><surname>Ravache</surname><given-names>F.</given-names></name>
<name><surname>Barandas</surname><given-names>M.</given-names></name>
</person-group><article-title>Exploring Regularization Methods for Domain Generalization in Accelerometer-Based Human Activity Recognition</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>6511</elocation-id><pub-id pub-id-type="doi">10.3390/s23146511</pub-id><pub-id pub-id-type="pmid">37514805</pub-id>
</element-citation></ref><ref id="B36-sensors-25-01184"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sharma</surname><given-names>G.</given-names></name>
<name><surname>Dhall</surname><given-names>A.</given-names></name>
<name><surname>Subramanian</surname><given-names>R.</given-names></name>
</person-group><article-title>MARS: A Multiview Contrastive Approach to Human Activity Recognition From Accelerometer Sensor</article-title><source>IEEE Sens. Lett.</source><year>2024</year><volume>8</volume><elocation-id>6002004</elocation-id><pub-id pub-id-type="doi">10.1109/LSENS.2024.3357941</pub-id></element-citation></ref><ref id="B37-sensors-25-01184"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sikder</surname><given-names>N.</given-names></name>
<name><surname>Nahid</surname><given-names>A.-A.</given-names></name>
</person-group><article-title>KU-HAR: An open dataset for heterogeneous human activity recognition</article-title><source>Pattern Recognit. Lett.</source><year>2021</year><volume>146</volume><fpage>46</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2021.02.024</pub-id></element-citation></ref><ref id="B38-sensors-25-01184"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kwapisz</surname><given-names>J.R.</given-names></name>
<name><surname>Weiss</surname><given-names>G.M.</given-names></name>
<name><surname>Moore</surname><given-names>S.</given-names></name>
</person-group><article-title>Activity recognition using cell phone accelerometers</article-title><source>SIGKDD Explor.</source><year>2011</year><volume>12</volume><fpage>74</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1145/1964897.1964918</pub-id></element-citation></ref><ref id="B39-sensors-25-01184"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Anguita</surname><given-names>D.</given-names></name>
<name><surname>Ghio</surname><given-names>A.</given-names></name>
<name><surname>Oneto</surname><given-names>L.</given-names></name>
<name><surname>Parra</surname><given-names>X.</given-names></name>
<name><surname>Reyes-Ortiz</surname><given-names>J.L.</given-names></name>
</person-group><article-title>A Public Domain Dataset for Human Activity Recognition Using Smartphones</article-title><source>Proceedings of the 21th European Symposium on Artificial Neural Networks</source><conf-loc>Bruges, Belgium</conf-loc><conf-date>24&#x02013;26 April 2013</conf-date></element-citation></ref><ref id="B40-sensors-25-01184"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Sawchuk</surname><given-names>A.A.</given-names></name>
</person-group><article-title>USC-HAD: A daily activity dataset for ubiquitous activity recognition using wearable sensors</article-title><source>Proceedings of the 2012 ACM Conference on Ubiquitous Computing</source><conf-loc>Pittsburgh, PA, USA</conf-loc><conf-date>5&#x02013;8 September 2012</conf-date><fpage>1036</fpage><lpage>1043</lpage></element-citation></ref><ref id="B41-sensors-25-01184"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Garcia-Gonzalez</surname><given-names>D.</given-names></name>
<name><surname>Rivero</surname><given-names>D.</given-names></name>
<name><surname>Fernandez-Blanco</surname><given-names>E.</given-names></name>
<name><surname>Luaces</surname><given-names>M.R.</given-names></name>
</person-group><article-title>A Public Domain Dataset for Real-Life Human Activity Recognition Using Smartphone Sensors</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>2200</elocation-id><pub-id pub-id-type="doi">10.3390/s20082200</pub-id><pub-id pub-id-type="pmid">32295028</pub-id>
</element-citation></ref><ref id="B42-sensors-25-01184"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alsheikh</surname><given-names>M.A.</given-names></name>
<name><surname>Seleim</surname><given-names>A.A.S.</given-names></name>
<name><surname>Niyato</surname><given-names>D.T.</given-names></name>
<name><surname>Doyle</surname><given-names>L.</given-names></name>
<name><surname>Lin</surname><given-names>S.</given-names></name>
<name><surname>Tan</surname><given-names>H.P.</given-names></name>
</person-group><article-title>Deep Activity Recognition Models with Triaxial Accelerometers</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1511.04664</pub-id></element-citation></ref><ref id="B43-sensors-25-01184"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ito</surname><given-names>C.</given-names></name>
<name><surname>Cao</surname><given-names>X.</given-names></name>
<name><surname>Shuzo</surname><given-names>M.</given-names></name>
<name><surname>Maeda</surname><given-names>E.</given-names></name>
</person-group><article-title>Application of CNN for Human Activity Recognition with FFT Spectrogram of Acceleration and Gyro Sensors</article-title><source>Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers</source><conf-loc>Singapore</conf-loc><conf-date>8&#x02013;12 October 2018</conf-date><fpage>1503</fpage><lpage>1510</lpage></element-citation></ref><ref id="B44-sensors-25-01184"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gilmore</surname><given-names>J.</given-names></name>
<name><surname>Nasseri</surname><given-names>M.</given-names></name>
</person-group><article-title>Human Activity Recognition Algorithm with Physiological and Inertial Signals Fusion: Photoplethysmography, Electrodermal Activity, and Accelerometry</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>3005</elocation-id><pub-id pub-id-type="doi">10.3390/s24103005</pub-id><pub-id pub-id-type="pmid">38793858</pub-id>
</element-citation></ref><ref id="B45-sensors-25-01184"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Logacjov</surname><given-names>A.</given-names></name>
<name><surname>Bach</surname><given-names>K.</given-names></name>
</person-group><article-title>Self-supervised learning with randomized cross-sensor masked reconstruction for human activity recognition</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>128</volume><fpage>107478</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2023.107478</pub-id></element-citation></ref><ref id="B46-sensors-25-01184"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tian</surname><given-names>Y.</given-names></name>
<name><surname>Hettiarachchi</surname><given-names>D.</given-names></name>
<name><surname>Yu</surname><given-names>H.</given-names></name>
<name><surname>Kamijo</surname><given-names>S.</given-names></name>
</person-group><article-title>Complex Human Activity Recognition Based on Spatial LSTM and Deep Residual Convolutional Network Using Wearable Motion Sensors</article-title><source>IEEE Sens. J.</source><year>2024</year><volume>24</volume><fpage>23183</fpage><lpage>23196</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2024.3404777</pub-id></element-citation></ref><ref id="B47-sensors-25-01184"><label>47.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>BenAbdelkader</surname><given-names>C.</given-names></name>
<name><surname>Cutler</surname><given-names>R.</given-names></name>
<name><surname>Davis</surname><given-names>L.</given-names></name>
</person-group><article-title>Stride and cadence as a biometric in automatic person identification and verification</article-title><source>Proceedings of the Fifth IEEE International Conference on Automatic Face Gesture Recognition</source><conf-loc>Washinton, DC, USA</conf-loc><conf-date>21 May 2002</conf-date><fpage>372</fpage><lpage>377</lpage></element-citation></ref><ref id="B48-sensors-25-01184"><label>48.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ravi</surname><given-names>D.</given-names></name>
<name><surname>Wong</surname><given-names>C.</given-names></name>
<name><surname>Lo</surname><given-names>B.</given-names></name>
<name><surname>Yang</surname><given-names>G.Z.</given-names></name>
</person-group><article-title>Deep learning for human activity recognition: A resource efficient implementation on low-power devices</article-title><source>Proceedings of the 2016 IEEE 13th International Conference on Wearable and Implantable Body Sensor Networks (BSN)</source><conf-loc>San Francisco, CA, USA</conf-loc><conf-date>14&#x02013;17 June 2016</conf-date><fpage>71</fpage><lpage>76</lpage></element-citation></ref><ref id="B49-sensors-25-01184"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Eckmann</surname><given-names>J.P.</given-names></name>
<name><surname>Kamphorst</surname><given-names>S.O.</given-names></name>
<name><surname>Ruelle</surname><given-names>D.</given-names></name>
</person-group><article-title>Recurrence Plots of Dynamical Systems</article-title><source>Europhys. Lett.</source><year>1987</year><volume>4</volume><fpage>973</fpage><pub-id pub-id-type="doi">10.1209/0295-5075/4/9/004</pub-id></element-citation></ref><ref id="B50-sensors-25-01184"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hur</surname><given-names>T.H.</given-names></name>
<name><surname>Bang</surname><given-names>J.H.</given-names></name>
<name><surname>Huynh-The</surname><given-names>T.</given-names></name>
<name><surname>Lee</surname><given-names>J.</given-names></name>
<name><surname>Kim</surname><given-names>J.-I.</given-names></name>
<name><surname>Lee</surname><given-names>S.</given-names></name>
</person-group><article-title>Iss2Image: A Novel Signal-Encoding Technique for CNN-Based Human Activity Recognition</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>3910</elocation-id><pub-id pub-id-type="doi">10.3390/s18113910</pub-id><pub-id pub-id-type="pmid">30428600</pub-id>
</element-citation></ref><ref id="B51-sensors-25-01184"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Simonyan</surname><given-names>K.</given-names></name>
<name><surname>Zisserman</surname><given-names>A.</given-names></name>
</person-group><article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1409.1556</pub-id></element-citation></ref><ref id="B52-sensors-25-01184"><label>52.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="B53-sensors-25-01184"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Howard</surname><given-names>A.G.</given-names></name>
<name><surname>Zhu</surname><given-names>M.</given-names></name>
<name><surname>Chen</surname><given-names>B.</given-names></name>
<name><surname>Kalenichenko</surname><given-names>D.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Weyand</surname><given-names>T.</given-names></name>
<name><surname>Andreetto</surname><given-names>M.</given-names></name>
<name><surname>Adam</surname><given-names>H.</given-names></name>
</person-group><article-title>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1704.04861</pub-id></element-citation></ref><ref id="B54-sensors-25-01184"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>M.</given-names></name>
<name><surname>Le</surname><given-names>Q.V.</given-names></name>
</person-group><article-title>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1905.11946</pub-id></element-citation></ref><ref id="B55-sensors-25-01184"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ek</surname><given-names>S.</given-names></name>
<name><surname>Portet</surname><given-names>F.</given-names></name>
<name><surname>Lalanda</surname><given-names>P.</given-names></name>
</person-group><article-title>Lightweight Transformers for Human Activity Recognition on Mobile Devices</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2209.11750</pub-id></element-citation></ref><ref id="B56-sensors-25-01184"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lagan&#x000e0;</surname><given-names>F.</given-names></name>
<name><surname>Prattic&#x000f2;</surname><given-names>D.</given-names></name>
<name><surname>Angiulli</surname><given-names>G.</given-names></name>
<name><surname>Oliva</surname><given-names>G.</given-names></name>
<name><surname>Pullano</surname><given-names>S.A.</given-names></name>
<name><surname>Versaci</surname><given-names>M.</given-names></name>
<name><surname>La Foresta</surname><given-names>F.</given-names></name>
</person-group><article-title>Development of an Integrated System of sEMG Signal Acquisition, Processing, and Analysis with AI Techniques</article-title><source>Signals</source><year>2024</year><volume>5</volume><fpage>476</fpage><lpage>493</lpage><pub-id pub-id-type="doi">10.3390/signals5030025</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01184-f001"><label>Figure 1</label><caption><p>Plots of the 15 activities for a single subject.</p></caption><graphic xlink:href="sensors-25-01184-g001" position="float"/></fig><fig position="float" id="sensors-25-01184-f002"><label>Figure 2</label><caption><p>Images reconstructed by (<bold>a</bold>) spectrogram, (<bold>b</bold>) modified RP, and (<bold>c</bold>) modified MP.</p></caption><graphic xlink:href="sensors-25-01184-g002" position="float"/></fig><fig position="float" id="sensors-25-01184-f003"><label>Figure 3</label><caption><p>Example of process for reconstructing modified MP.</p></caption><graphic xlink:href="sensors-25-01184-g003" position="float"/></fig><fig position="float" id="sensors-25-01184-f004"><label>Figure 4</label><caption><p>Overall architecture of proposed model.</p></caption><graphic xlink:href="sensors-25-01184-g004" position="float"/></fig><fig position="float" id="sensors-25-01184-f005"><label>Figure 5</label><caption><p>Detailed structure of convolutional blocks and CNN model.</p></caption><graphic xlink:href="sensors-25-01184-g005" position="float"/></fig><fig position="float" id="sensors-25-01184-f006"><label>Figure 6</label><caption><p>Illustration of confusion matrices.</p></caption><graphic xlink:href="sensors-25-01184-g006" position="float"/></fig><fig position="float" id="sensors-25-01184-f007"><label>Figure 7</label><caption><p>Illustration of Gaussian noise-added signal. (<bold>a</bold>) Original signal, (<bold>b</bold>) 5% noise, (<bold>c</bold>) 10% noise, (<bold>d</bold>) 20% noise, (<bold>e</bold>) 30% noise, (<bold>f</bold>) 40% noise, and (<bold>g</bold>) 50% noise-added signal.</p></caption><graphic xlink:href="sensors-25-01184-g007" position="float"/></fig><fig position="float" id="sensors-25-01184-f008"><label>Figure 8</label><caption><p>Comparison of time-series-based model and proposed models under various Gaussian noise levels. (<bold>a</bold>) Performance comparison; (<bold>b</bold>) performance degradation comparison.</p></caption><graphic xlink:href="sensors-25-01184-g008" position="float"/></fig><table-wrap position="float" id="sensors-25-01184-t001"><object-id pub-id-type="pii">sensors-25-01184-t001_Table 1</object-id><label>Table 1</label><caption><p>The demographic characteristics of subjects.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Age (years)</td><td align="center" valign="middle" rowspan="1" colspan="1">27.17</td><td align="center" valign="middle" rowspan="1" colspan="1">4.48</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Height (cm)</td><td align="center" valign="middle" rowspan="1" colspan="1">169.97</td><td align="center" valign="middle" rowspan="1" colspan="1">8.23</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Weight (kg) </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.0</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01184-t002"><object-id pub-id-type="pii">sensors-25-01184-t002_Table 2</object-id><label>Table 2</label><caption><p>The 15 daily activities in our dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Activity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Abbreviation</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Samples</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Going upstairs</td><td align="center" valign="middle" rowspan="1" colspan="1">UPS</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Going downstairs</td><td align="center" valign="middle" rowspan="1" colspan="1">DWS</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sitting on a chair</td><td align="center" valign="middle" rowspan="1" colspan="1">SC</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Getting up from a chair</td><td align="center" valign="middle" rowspan="1" colspan="1">GC</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lying up on the bed</td><td align="center" valign="middle" rowspan="1" colspan="1">LUB</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lying down on the bed</td><td align="center" valign="middle" rowspan="1" colspan="1">LDB</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Walking</td><td align="center" valign="middle" rowspan="1" colspan="1">WK</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Squat and rise</td><td align="center" valign="middle" rowspan="1" colspan="1">SQ</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Jump rope</td><td align="center" valign="middle" rowspan="1" colspan="1">JR</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fall forward</td><td align="center" valign="middle" rowspan="1" colspan="1">FF</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fall forward on one&#x02019;s knees</td><td align="center" valign="middle" rowspan="1" colspan="1">FFK</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fall back</td><td align="center" valign="middle" rowspan="1" colspan="1">FB</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Look to the left and fall</td><td align="center" valign="middle" rowspan="1" colspan="1">LF</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Look to the right and fall</td><td align="center" valign="middle" rowspan="1" colspan="1">RF</td><td align="center" valign="middle" rowspan="1" colspan="1">1050</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fall out of the bed</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FOB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1050</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01184-t003"><object-id pub-id-type="pii">sensors-25-01184-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison on spectrogram window size.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Window Size (Seconds)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Top 1 <break/>Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Top 3 <break/>Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ROC AUC</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">67.68 (8.88)</td><td align="center" valign="middle" rowspan="1" colspan="1">91.98 (3.40)</td><td align="center" valign="middle" rowspan="1" colspan="1">69.00 (8.12)</td><td align="center" valign="middle" rowspan="1" colspan="1">67.68 (8.88)</td><td align="center" valign="middle" rowspan="1" colspan="1">96.45 (1.47)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">70.05 (10.29)</td><td align="center" valign="middle" rowspan="1" colspan="1">91.20 (3.60)</td><td align="center" valign="middle" rowspan="1" colspan="1">71.22 (9.85)</td><td align="center" valign="middle" rowspan="1" colspan="1">70.05 (10.29)</td><td align="center" valign="middle" rowspan="1" colspan="1">96.63 (1.75)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">74.77 (5.72)</td><td align="center" valign="middle" rowspan="1" colspan="1">94.58 (1.18)</td><td align="center" valign="middle" rowspan="1" colspan="1">76.13 (5.25)</td><td align="center" valign="middle" rowspan="1" colspan="1">74.77 (5.72)</td><td align="center" valign="middle" rowspan="1" colspan="1">97.68 (0.77)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.89 (5.43)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.89 (0.98)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.66 (5.17)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.86 (5.41)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.03 (0.57)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01184-t004"><object-id pub-id-type="pii">sensors-25-01184-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison with each data reconstruction method and proposed method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Top 1<break/>Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Top 3<break/>Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ROC AUC</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1D time series <sup>1</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">75.23. (0.33)</td><td align="center" valign="middle" rowspan="1" colspan="1">91.65 (0.92)</td><td align="center" valign="middle" rowspan="1" colspan="1">78.44(0.14)</td><td align="center" valign="middle" rowspan="1" colspan="1">75.57 (0.33)</td><td align="center" valign="middle" rowspan="1" colspan="1">100 (0)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Spectrogram</td><td align="center" valign="middle" rowspan="1" colspan="1">77.89 (5.43)</td><td align="center" valign="middle" rowspan="1" colspan="1">94.89 (0.98)</td><td align="center" valign="middle" rowspan="1" colspan="1">78.66 (5.17)</td><td align="center" valign="middle" rowspan="1" colspan="1">77.86 (5.41)</td><td align="center" valign="middle" rowspan="1" colspan="1">98.03 (0.57)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Modified RP</td><td align="center" valign="middle" rowspan="1" colspan="1">76.32 (1.57)</td><td align="center" valign="middle" rowspan="1" colspan="1">95.50 (0.36)</td><td align="center" valign="middle" rowspan="1" colspan="1">77.22 (0.98)</td><td align="center" valign="middle" rowspan="1" colspan="1">76.32 (1.57)</td><td align="center" valign="middle" rowspan="1" colspan="1">97.71 (0.16)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Modified MP</td><td align="center" valign="middle" rowspan="1" colspan="1">83.82 (2.18)</td><td align="center" valign="middle" rowspan="1" colspan="1">96.92 (0.24)</td><td align="center" valign="middle" rowspan="1" colspan="1">84.43 (1.77)</td><td align="center" valign="middle" rowspan="1" colspan="1">83.82 (2.18)</td><td align="center" valign="middle" rowspan="1" colspan="1">98.90 (0.09)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.87 (0.16)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.92 (0.12)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.97 (0.18)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.87 (0.16)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.85 (0.20)</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> Converted the 2D model architecture into a 1D model.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-01184-t005"><object-id pub-id-type="pii">sensors-25-01184-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison using different feature extractor backbones.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Backbone Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">VGG16</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ResNet50</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MobileNet</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">EfficientNet</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CNN-LSTM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CNN-GRU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Proposed Model</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of <break/>parameters</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21,799,759</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25,034,127</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10,521,167</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20,571,826</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7,0192,431</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7,059,919</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3,068,431</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Top 1 accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">76.80 (3.95)</td><td align="center" valign="middle" rowspan="1" colspan="1">80.49 (1.00)</td><td align="center" valign="middle" rowspan="1" colspan="1">65.11 (2.76)</td><td align="center" valign="middle" rowspan="1" colspan="1">75.44 (2.30)</td><td align="center" valign="middle" rowspan="1" colspan="1">76.96 (5.25)</td><td align="center" valign="middle" rowspan="1" colspan="1">73.05 (4.67)</td><td align="center" valign="middle" rowspan="1" colspan="1">91.87 (0.16)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Top 3 accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">95.52 (0.02)</td><td align="center" valign="middle" rowspan="1" colspan="1">96.03 (0.08)</td><td align="center" valign="middle" rowspan="1" colspan="1">90.66 (0.00)</td><td align="center" valign="middle" rowspan="1" colspan="1">95.39 (0.68)</td><td align="center" valign="middle" rowspan="1" colspan="1">97.07 (0.44)</td><td align="center" valign="middle" rowspan="1" colspan="1">96.30 (1.00)</td><td align="center" valign="middle" rowspan="1" colspan="1">97.92 (0.12)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">77.75 (3.92)</td><td align="center" valign="middle" rowspan="1" colspan="1">82.11 (0.58)</td><td align="center" valign="middle" rowspan="1" colspan="1">66.33 (1.93)</td><td align="center" valign="middle" rowspan="1" colspan="1">72.81 (3.02)</td><td align="center" valign="middle" rowspan="1" colspan="1">77.84 (5.73)</td><td align="center" valign="middle" rowspan="1" colspan="1">73.99 (4.87)</td><td align="center" valign="middle" rowspan="1" colspan="1">91.97 (0.18)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">76.80 (3.95)</td><td align="center" valign="middle" rowspan="1" colspan="1">80.49 (1.00)</td><td align="center" valign="middle" rowspan="1" colspan="1">65.11 (2.76)</td><td align="center" valign="middle" rowspan="1" colspan="1">75.44 (2.30)</td><td align="center" valign="middle" rowspan="1" colspan="1">76.96 (5.25)</td><td align="center" valign="middle" rowspan="1" colspan="1">73.05 (4.67)</td><td align="center" valign="middle" rowspan="1" colspan="1">91.87 (0.16)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ROC-AUC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.90 (0.12)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.31 (0.00)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.51 (0.17)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.43 (0.14)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.15 (0.61)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.47 (0.70)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.85 (0.20)</td></tr></tbody></table></table-wrap></floats-group></article>