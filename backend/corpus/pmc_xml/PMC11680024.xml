<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11680024</article-id><article-id pub-id-type="doi">10.3390/s24248190</article-id><article-id pub-id-type="publisher-id">sensors-24-08190</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Binary Transformer Based on the Alignment and Correction of Distribution</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-5209-1175</contrib-id><name><surname>Wang</surname><given-names>Kaili</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-24-08190" ref-type="aff">1</xref><xref rid="af2-sensors-24-08190" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Mingtao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-24-08190" ref-type="aff">1</xref><xref rid="af2-sensors-24-08190" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Wan</surname><given-names>Zixin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-24-08190" ref-type="aff">1</xref><xref rid="af2-sensors-24-08190" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1273-7950</contrib-id><name><surname>Shen</surname><given-names>Tao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-24-08190" ref-type="aff">1</xref><xref rid="af3-sensors-24-08190" ref-type="aff">3</xref><xref rid="c1-sensors-24-08190" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Kodogiannis</surname><given-names>Vassilis S.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-24-08190"><label>1</label>Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China; <email>wangkaili0110@stu.kust.edu.cn</email> (K.W.); <email>wangmingtao@stu.kust.edu.cn</email> (M.W.); <email>wanzixin@stu.kust.edu.cn</email> (Z.W.)</aff><aff id="af2-sensors-24-08190"><label>2</label>Yunnan Key Laboratory of Computer Techologies Application, Kunming 650500, China</aff><aff id="af3-sensors-24-08190"><label>3</label>Yunnan Mechanical and Electrical Vocational and Technical College, Kunming 650500, China</aff><author-notes><corresp id="c1-sensors-24-08190"><label>*</label>Correspondence: <email>shentao@kust.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>22</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2024</year></pub-date><volume>24</volume><issue>24</issue><elocation-id>8190</elocation-id><history><date date-type="received"><day>20</day><month>11</month><year>2024</year></date><date date-type="rev-recd"><day>18</day><month>12</month><year>2024</year></date><date date-type="accepted"><day>20</day><month>12</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 by the authors.</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Transformer is a powerful model widely used in artificial intelligence applications. It contains complex structures and has extremely high computational requirements that are not suitable for embedded intelligent sensors with limited computational resources. The binary quantization technology takes up less memory space and has a faster calculation speed; however, it is seldom studied for the lightweight transformer. Compared with full-precision networks, the key bottleneck lies in the distribution shift problem caused by the existing binary quantization methods. To tackle this problem, the feature distribution alignment operation in binarization is investigated. The median shift and mean restore is designed to ensure consistency between the binary feature distribution and the full-precision transformer. Then, a knowledge distillation architecture for distribution correction is developed, which has a teacher&#x02013;student structure comprising a full-precision and binary transformer, to further rectify the feature distribution of the binary student network to ensure the completeness and accuracy of the data. Experimental results on the CIFAR10, CIFAR100, ImageNet-1k, and TinyImageNet datasets show the effectiveness of the proposed binary optimization model, which outperforms the previous state-of-the-art binarization mechanisms while maintaining the same computational complexity.</p></abstract><kwd-group><kwd>binary transformer</kwd><kwd>distribution alignment</kwd><kwd>knowledge distillation</kwd><kwd>distribution correction</kwd></kwd-group><funding-group><award-group><funding-source>Yunnan Outstanding Young Talent Project</funding-source><award-id>202301AV070003</award-id></award-group><award-group><funding-source>Yunnan Major Science and Technology Project</funding-source><award-id>202302AG050009</award-id><award-id>202202AD080013</award-id></award-group><award-group><funding-source>Special Education language Intelligence Sichuan key Laboratory of philosophy and Social Sciences Open Project</funding-source><award-id>YYZN-2024-1</award-id></award-group><award-group><funding-source>photosynthetic Fund</funding-source><award-id>ghfund202407010460</award-id></award-group><funding-statement>This research was funded by the Yunnan Outstanding Young Talent Project (No. 202301AV070003); the Yunnan Major Science and Technology Project (No. 202302AG050009, No. 202202AD080013); the Special Education language Intelligence Sichuan key Laboratory of philosophy and Social Sciences Open Project (No.YYZN-2024-1); and the photosynthetic Fund (No. ghfund202407010460).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-24-08190"><title>1. Introduction</title><p>Smart sensors have garnered widespread attention from researchers due to their powerful data acquisition and processing capabilities. By embedding deep learning models, sensors have achieved significant results in the field of artificial intelligence, including image classification [<xref rid="B1-sensors-24-08190" ref-type="bibr">1</xref>,<xref rid="B2-sensors-24-08190" ref-type="bibr">2</xref>,<xref rid="B3-sensors-24-08190" ref-type="bibr">3</xref>], object detection [<xref rid="B4-sensors-24-08190" ref-type="bibr">4</xref>,<xref rid="B5-sensors-24-08190" ref-type="bibr">5</xref>], and semantic segmentation [<xref rid="B6-sensors-24-08190" ref-type="bibr">6</xref>,<xref rid="B7-sensors-24-08190" ref-type="bibr">7</xref>]. At present, a popular deep learning model is transformer [<xref rid="B8-sensors-24-08190" ref-type="bibr">8</xref>], which has a powerful global feature representation ability compared with the traditional local convolutional neural networks (CNNs). Therefore, the fine architecture of transformer based on an attention mechanism and multi-layer perceptron has been proposed to solve complex problems, such as large language models (LLMs). However, transformer also introduces deployment constraints due to its large model size and extremely high computational burden, especially for embedded sensors with limited resources and insufficient computational power [<xref rid="B9-sensors-24-08190" ref-type="bibr">9</xref>].</p><p>Quantization technology [<xref rid="B10-sensors-24-08190" ref-type="bibr">10</xref>] has been considered the most effective approach for reducing the amount of calculations in transformer. The existing methods can be categorized into two categories: (1) high-bit quantization (16 bit, 8 bit) [<xref rid="B11-sensors-24-08190" ref-type="bibr">11</xref>,<xref rid="B12-sensors-24-08190" ref-type="bibr">12</xref>] and (2) low-bit quantization (4 bit, 2 bit, 1 bit) [<xref rid="B13-sensors-24-08190" ref-type="bibr">13</xref>,<xref rid="B14-sensors-24-08190" ref-type="bibr">14</xref>,<xref rid="B15-sensors-24-08190" ref-type="bibr">15</xref>]. Evidently, binarization is an extreme form of quantization with weights, and activation values are &#x02212;1 or +1 of 1 bit, which can minimize the model size and computational cost to the greatest extent. Nonetheless, at present, most binary neural networks (BNNs) are based on CNN architectures, with few studies on complex transformer architectures.</p><p>In summary, it is still a challenging task to research the binary transformer so that its performance matches full-precision models to the maximum extent. The main reason for this is that simple binarization operations on attention values will lead to significant information loss, resulting in the inaccurate characterization of feature relationships [<xref rid="B16-sensors-24-08190" ref-type="bibr">16</xref>]. Most existing methods attempt to maximize information entropy by adjusting network parameters [<xref rid="B17-sensors-24-08190" ref-type="bibr">17</xref>], thereby enhancing their feature representation capabilities. Currently, many solutions have been studied under the constraint of maximum information entropy, such as weight reshaping [<xref rid="B18-sensors-24-08190" ref-type="bibr">18</xref>] and minimization of gradient error [<xref rid="B19-sensors-24-08190" ref-type="bibr">19</xref>]. Nevertheless, due to changes in the parameter structure, the feature distribution of binary data is significantly different from that of full-precision networks, as shown in <xref rid="sensors-24-08190-f001" ref-type="fig">Figure 1</xref>. The deviation of these feature distributions will cause larger feature representation errors with the stacking of attention blocks and eventually reduce the inference performance of the model.</p><p>Based on the above observations, maximum information entropy is the fundamental requirement for the binary transformer. On the other hand, how to maintain the feature distribution of the binary transformer as consistently as possible with the full-precision network is an issue that urgently needs to be solved in this field. Therefore, this paper investigated the alignment and correction of distribution for binary transformer (ACD-BiT). The contributions of this paper are summarized as follows.</p><list list-type="order"><list-item><p>A distribution alignment binarization method is proposed that addresses the problem of binary feature distribution deviation by median shift and mean restore. Distribution alignment binarization was performed on attention and full-connection layers, and it was further discussed that the effective binary operator is still under a constraint of maximum information entropy.</p></list-item><list-item><p>This paper introduces a knowledge distillation architecture for distribution correction. The teacher&#x02013;student structure is built according to the full-precision and binary transformers. By performing the counterpoint distillation on the query Q and key K in attention and combining the label output of the full-precision teacher network to calculate the cross-entropy loss of the binary student network, the integrity and accuracy of binary transformer data are ensured.</p></list-item><list-item><p>Combining distribution alignment and distillation correction, we propose a novel transformer framework, ACD-BiT. While ensuring maximum information entropy, the feature distribution is consistent with the full-precision data to the greatest extent. To evaluate the effectiveness of the proposed model, experiments are conducted on various datasets, including CIFAR10, CIFAR100, ImageNet-1k, and TinyImageNet. The results show that the proposed model outperforms most of the existing mechanisms. This proves that both information entropy and data distribution are important factors affecting the performance of the binary transformer.</p></list-item></list></sec><sec id="sec2-sensors-24-08190"><title>2. Related Works</title><p>In this section, advanced lightweight transformers are briefly reviewed. The current binary operators are also discussed to illustrate the key bottlenecks in binarization of deep networks.</p><sec id="sec2dot1-sensors-24-08190"><title>2.1. Lightweight Transformers</title><p>Quantization can effectively reduce the model size and computational costs, replacing expensive floating-point operations with simple fixed-point operations. Therefore, the adoption of quantization techniques for research on lightweight transformers has received increasing attention from researchers. For instance, Yuan et al. [<xref rid="B13-sensors-24-08190" ref-type="bibr">13</xref>] constructed an 8-bit transformer, reducing the quantization error of activation values by using a dual uniform quantization method. Li et al. [<xref rid="B20-sensors-24-08190" ref-type="bibr">20</xref>] investigated a log-int-softmax mechanism for inference to construct a 4-bit quantization model. Wang et al. [<xref rid="B16-sensors-24-08190" ref-type="bibr">16</xref>] proposed a binary transformer with a hierarchical-adaptive architecture, which replaces expensive matrix operations with addition and bit-wise operations by switching between two attention modes.</p><p>Different bit quantizations can lead to various lightweight transformer models, and the application scenarios and performance of these models are also diverse. Binary transformer is a superior extreme compression model, characterized by its smaller storage space and faster computation speed, and has a wider range of applications. However, compared to CNNs, complex transformer architectures experience more severe performance degradation as the degree of quantization increases.</p></sec><sec id="sec2dot2-sensors-24-08190"><title>2.2. Binary Networks</title><p>Binary neural networks (BNNs) quantize both weights and activations to the smallest 1 bit, thereby significantly reducing model size and computational consumption. There are many optimizations for binary operators in deep neural networks. Liu et al. [<xref rid="B21-sensors-24-08190" ref-type="bibr">21</xref>] changed the data distribution of weights and activations by reshaping point-wise convolutions and balancing activations. Tan et al. [<xref rid="B22-sensors-24-08190" ref-type="bibr">22</xref>] designed the gradient approximation function to reduce the gradient error in the back-propagation. Qin et al. [<xref rid="B18-sensors-24-08190" ref-type="bibr">18</xref>] introduced a novel teacher&#x02013;student network structure for knowledge distillation, thereby further optimizing binary neural networks. However, the aforementioned studies all aim to reduce the information loss of binary operators by optimizing the network structure.</p><p>In fact, differentiation in deep models can lead to extreme discrepancy of data distribution between full-precision and binary networks, making it difficult to maximize the performance of 1-bit networks. Although there have been some studies on data distribution bias issues, research on binary transformers is lacking [<xref rid="B10-sensors-24-08190" ref-type="bibr">10</xref>,<xref rid="B14-sensors-24-08190" ref-type="bibr">14</xref>].</p></sec></sec><sec sec-type="methods" id="sec3-sensors-24-08190"><title>3. Methods</title><p>This section first reviews the basic theory of binary neural networks. Then, the distribution alignment and distillation correction of two optimization modules are integrated to give the overall architecture of the novel binary transformer (ACD-BiT). Finally, a detailed study is conducted on the two key technologies: distribution alignment (Da) and distribution correction (Dc).</p><sec id="sec3dot1-sensors-24-08190"><title>3.1. Preliminary</title><p><bold>Binary Neural Networks (BNNs).</bold> As the most extreme case of quantized networks, weights and activations are constrained to be +1 and &#x02212;1 after binary operations. For full-precision inputs, the sign function is used to perform binary processing to generate the corresponding binary output results [<xref rid="B23-sensors-24-08190" ref-type="bibr">23</xref>]. The forward-propagation process of the sign function in the binarized network can be expressed as the following formula: <disp-formula id="FD1-sensors-24-08190"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="4pt"/><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>During the back-propagation process, usually, a straight-through estimator (STE) is used to solve the non-differentiability problem of the sign function [<xref rid="B24-sensors-24-08190" ref-type="bibr">24</xref>]. Specifically, an approximation function is used instead of the sign function to back-propagate the gradient to the full-precision weights or activations for updating. Back-propagation can be represented as
<disp-formula id="FD2-sensors-24-08190"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="left"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="4pt"/><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>|</mml:mo><mml:mo>&#x02264;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:math></inline-formula> is the cost function for the minibatch. To minimize the quantization errors, BNNs introduce a full-precision scaling factor <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula>, as follows: <disp-formula id="FD3-sensors-24-08190"><label>(3)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mfenced open="&#x02225;" close="&#x02225;"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mfenced><mml:msub><mml:mo>&#x02113;</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>&#x02248;</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mfenced open="&#x02225;" close="&#x02225;"><mml:mo>&#x000b7;</mml:mo></mml:mfenced><mml:msub><mml:mo>&#x02113;</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> denotes the <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mo>&#x02113;</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mtext>-</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>norm, and n is the dimension of <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>Then, the sign function is applied in forward propagation, and STE is used to obtain the derivatives in back-propagation. For deep neural networks, let W and A represent the full-precision weight and activation, respectively. <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represent the quantized binary weight and activation. Thus, the conventional convolution operation can be represented as
<disp-formula id="FD4-sensors-24-08190"><label>(4)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>&#x02297;</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#x02295;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02295;</mml:mo><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where &#x02297; denotes the convolution operation, &#x02295; denotes bit-wise XNOR and bit-count calculations, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the mean value, and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the scaling factors for activation and weight.</p><p><bold>Binary Transformer.</bold> The transformer network mainly consists of <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow></mml:math></inline-formula> encoder blocks, each containing multi-head attention (MHA) and feed-forward network (FFN) modules. The input data <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are first passed through the binarized linear layer to obtain the batch-normalization of queries <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:math></inline-formula>, keys <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:math></inline-formula>, and values <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:math></inline-formula>, which serve as the input data for the MHA module. Then, the attention score <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow></mml:math></inline-formula> is calculated as follows: <disp-formula id="FD5-sensors-24-08190"><label>(5)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">BN</mml:mi><mml:mi>Q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02295;</mml:mo><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">BN</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">BN</mml:mi><mml:mi>Q</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">BN</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote different binarized linear layers for <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:math></inline-formula>, respectively. <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi mathvariant="bold-italic">d</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow></mml:math></inline-formula> is the dimension of features and <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the attention heads.</p><p>Then, the binary attention feature matrix performs feature enhancement on the binarized value matrix <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:math></inline-formula> to obtain the feature mapping output matrix <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow></mml:math></inline-formula> of MHA, as follows: <disp-formula id="FD6-sensors-24-08190"><label>(6)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">BN</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">BN</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">BN</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the binarized linear layer of <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the activation function.</p></sec><sec id="sec3dot2-sensors-24-08190"><title>3.2. Overall Framework of Transformer Binarization</title><p>In this paper, we propose a novel binary strategy based on distributed alignment for transformer; the overall architecture is shown in <xref rid="sensors-24-08190-f002" ref-type="fig">Figure 2</xref>. The core of the binary strategy is to study the attention mechanism in transformer and perform binary operations on the feature values <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:math></inline-formula>, thereby effectively reducing the model size and computational burden. First, to overcome the problem of feature distribution deviation caused by classical feature reshaping binary strategies, a novel distribution alignment in binary operation is proposed, as shown in <xref rid="sensors-24-08190-f002" ref-type="fig">Figure 2</xref>a. Then, in order to further correct the binarized feature distribution, a teacher&#x02013;student structure based on a full-precision teacher model and a binary student model is constructed, as shown in <xref rid="sensors-24-08190-f002" ref-type="fig">Figure 2</xref>b. By effectively combining the above two methods, while maximizing information entropy, the binary feature distribution is maximally consistent with the original full-precision features, enhancing the feature representation capability of the binary attention.</p></sec><sec id="sec3dot3-sensors-24-08190"><title>3.3. Distribution Alignment in Binary Operation</title><p>The two core requirements of binary neural networks are to minimize information loss and quantization errors. In this section, the binary strategy of distribution alignment based on median shift and mean restore is comprehensively introduced, and the alignment method for weights and activations that simultaneously satisfy information loss and quantization error minimization is presented. Then, the specific algorithm process in the attention mechanism is given.</p><sec id="sec3dot3dot1-sensors-24-08190"><title>3.3.1. Binary Strategy for Distributed Alignment</title><p>Binary neural networks use 1 bit to represent raw floating-point numbers, and the quantization can be formulated as
<disp-formula id="FD7-sensors-24-08190"><label>(7)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:math></inline-formula> denotes full-precision data, <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula> represents scalars of binary values, and <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are binary values, as shown in formula (<xref rid="FD1-sensors-24-08190" ref-type="disp-formula">1</xref>). Then, in forward propagation, the vector multiplication of weights and activations can be represented as
<disp-formula id="FD8-sensors-24-08190"><label>(8)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>&#x02295;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow></mml:math></inline-formula> denote full-precision and binary, and &#x02295; denotes bit-wise XNOR and bit-count calculations.</p><p><bold>Minimizing information loss.</bold> Due to the binarization process, which compresses the weights and activations extremely to 1 bit, parameter representation capabilities are limited. In order to retain as much of the original full-precision information as possible, the mutual information [<xref rid="B25-sensors-24-08190" ref-type="bibr">25</xref>] between the binary representation and the full-precision representation should be maximized; that is,
<disp-formula id="FD9-sensors-24-08190"><label>(9)</label><mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow></mml:math></inline-formula> denotes the information entropy, and <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the conditional entropy of <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow></mml:math></inline-formula> given <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow></mml:math></inline-formula>. Since the binary function is deterministic, <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. That is, maximizing mutual information is equivalent to maximizing information entropy, and the formula is as follows: <disp-formula id="FD10-sensors-24-08190"><label>(10)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:munder><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo form="prefix">log</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the probability of taking the value <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:math></inline-formula>. Combined with the sign function Formula (<xref rid="FD1-sensors-24-08190" ref-type="disp-formula">1</xref>), <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mfenced separators="" open="{" close="}"><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. Then, the information entropy can be expressed as follows: <disp-formula id="FD11-sensors-24-08190"><label>(11)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Then, by taking the derivative of <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with respect to <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>: <disp-formula id="FD12-sensors-24-08190"><label>(12)</label><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo form="prefix">ln</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo form="prefix">ln</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Let <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; the maximum of <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is obtained, and then <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. This indicates that the binarized data should present a uniform distribution, and their probability density function is shown as follows: <disp-formula id="FD13-sensors-24-08190"><label>(13)</label><mml:math id="mm62" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="4pt"/><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>=</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="4pt"/><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Therefore, the binarization of attention weights and activations needs to satisfy the maximization of information entropy, achieve a uniform distribution of binary values, and thereby enhance the representational ability of attention. The uniform distribution quantization process is shown in <xref rid="sensors-24-08190-f003" ref-type="fig">Figure 3</xref>.</p><p>Before binarization, to ensure that the number of features on both sides of the zero mean is the same, the median value is subtracted from the full-precision, i.e., <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, the data are centered around zero with an equal number of features on each side. Then, through the binarization sign function, the following formula can be obtained: <disp-formula id="FD14-sensors-24-08190"><label>(14)</label><mml:math id="mm64" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="4pt"/><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="4pt"/><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The uniform binary distribution is achieved by the binarization operation after the median shift, resulting in minimal information loss after quantization. However, the binary distribution after translation shows a significant difference from the original full-precision distribution, leading to severe quantization errors.</p><p><bold>Minimizing quantization error.</bold> The minimization of information loss carried by binary neurons improves the representation ability of attention features; however, in order to ensure superior binary neural networks, it is necessary to further minimize the quantization error [<xref rid="B26-sensors-24-08190" ref-type="bibr">26</xref>], and the formula is defined as follows: <disp-formula id="FD15-sensors-24-08190"><label>(15)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mfenced separators="" open="&#x02225;" close="&#x02225;"><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the full-precision feature distribution, <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the binary feature distribution, and quantization error <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is used to measure the feature distribution difference between full-precision and binary.</p><p>To ensure that the distribution of the binary feature <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> remains consistent with that of the full-precision feature <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, first, it is necessary to ensure that the modulus length is the same before and after quantization. For the full-precision vector <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, its modulus length is defined as <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>, while for the binary vector <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mfenced separators="" open="{" close="}"><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mfenced><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, its modulus length is fixed equal to <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msqrt><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. Therefore, a scaling factor <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> needs to be introduced to make the quantized modulus length consistent with the full-precision one by scaling the binary <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, thereby reducing the quantization error. The schematic diagram of the magnitude scaling is shown in <xref rid="sensors-24-08190-f004" ref-type="fig">Figure 4</xref>.</p><p>Subsequently, the translation operation carried out to maximize information entropy leads to the center position of the binary data distribution (<inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) being inconsistent with the full-precision one (<inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>). Therefore, it is necessary to introduce a bias factor <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to restore the binarization <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Finally, the binary feature <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> that minimizes the quantization error is obtained: <disp-formula id="FD16-sensors-24-08190"><label>(16)</label><mml:math id="mm82" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:msqrt><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In summary, by performing median shifting, binary quantization of the sign function, modulus scaling, and mean restoration, the two core requirements of the binary network are achieved: minimizing information loss and minimizing quantization error. After applying the distribution alignment binary strategy, the binary features obtained are optimized for maximize information entropy while the feature distribution matches the full-precision features to the maximum extent, thereby significantly enhancing the feature representation capability of binary attention, as shown in <xref rid="sensors-24-08190-f002" ref-type="fig">Figure 2</xref>a.</p></sec><sec id="sec3dot3dot2-sensors-24-08190"><title>3.3.2. Algorithm Flow in Attention</title><p>The transformer block consists of two major components: multi-head self-attention (MHSA) and multi-layer perceptron (MLP). The input data come from the hidden layer states <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which are <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow></mml:math></inline-formula> tokens of length <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:math></inline-formula>. The query (<inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:math></inline-formula>), key (<inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:math></inline-formula>), and value (<inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:math></inline-formula>) that the attention calculation depends on are obtained through different binarized linear layers. The computation of the input features can be represented as
<disp-formula id="FD17-sensors-24-08190"><label>(17)</label><mml:math id="mm89" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub><mml:msub><mml:mtext mathvariant="italic">-linear</mml:mtext><mml:mi mathvariant="bold-italic">q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub><mml:msub><mml:mtext mathvariant="italic">-linear</mml:mtext><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub><mml:msub><mml:mtext mathvariant="italic">-linear</mml:mtext><mml:mi mathvariant="bold-italic">v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub><mml:msub><mml:mtext mathvariant="italic">-linear</mml:mtext><mml:mi mathvariant="bold-italic">q</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub><mml:msub><mml:mtext mathvariant="italic">-linear</mml:mtext><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub><mml:msub><mml:mtext mathvariant="italic">-linear</mml:mtext><mml:mi mathvariant="bold-italic">v</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote three different binary linear layers based on distribution alignment for <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:math></inline-formula>, respectively. Then, distribution alignment binarization is performed on the linear output features to obtain input features for the calculation of attention feature relations. The attention score <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow></mml:math></inline-formula> is calculated as follows: <disp-formula id="FD18-sensors-24-08190"><label>(18)</label><mml:math id="mm97" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02295;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the distribution alignment binarization operation, and <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the feature dimension of the multi-head attention.</p><p>Then, the softmax function is used for normalization to obtain the feature probability matrix <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The feature enhancement operation is performed by multiplying <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow></mml:math></inline-formula> with the binarized feature matrix <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, obtaining the feature mapping output. To better preserve the original feature information, the <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:math></inline-formula> of linear layer outputs are further added. The final attention output is as follows: <disp-formula id="FD19-sensors-24-08190"><label>(19)</label><mml:math id="mm106" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">BN</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For a specific transformer block, the binary quantization process is summarized in Algorithm 1.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold> Binary quantization in Attention</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><list list-type="simple"><list-item><label>1:</label><p><bold>Input:</bold> Hide states feature matrix <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mspace width="-0.166667em"/><mml:mo>&#x02208;</mml:mo><mml:mspace width="-0.166667em"/><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow></mml:math></inline-formula> tokens of length <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:math></inline-formula>), weight matrix <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mspace width="-0.166667em"/><mml:mo>&#x02208;</mml:mo><mml:mspace width="-0.166667em"/><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>2:</label><p><bold>Output:</bold> Attention feature mapping <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>3:</label><p>Binary linear layer:</p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mi>A</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub><mml:mtext mathvariant="italic">-linear</mml:mtext><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">BN</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;get the feature matrices:</p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub><mml:msub><mml:mtext mathvariant="italic">-linear</mml:mtext><mml:mi mathvariant="bold-italic">q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub><mml:msub><mml:mtext mathvariant="italic">-linear</mml:mtext><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub><mml:msub><mml:mtext mathvariant="italic">-linear</mml:mtext><mml:mi mathvariant="bold-italic">v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>4:</label><p>Compute binary features:</p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>A</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>A</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>A</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>5:</label><p>Feature relation <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#x02295;</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msqrt><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>6:</label><p>Attention feature <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>7:</label><p><bold>return</bold> Feature mapping <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">BN</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list></td></tr></tbody></array></p></sec></sec><sec id="sec3dot4-sensors-24-08190"><title>3.4. Distribution Correction Based on Knowledge Distillation</title><p>Knowledge distillation is an essential supervised method for training quantized networks [<xref rid="B27-sensors-24-08190" ref-type="bibr">27</xref>]. The teacher&#x02013;student network structure, constructed with full-precision and binary models, is well fit regarding the performance difference between binary and full-precision models [<xref rid="B28-sensors-24-08190" ref-type="bibr">28</xref>]. Therefore, in order to maintain the distribution consistency of the attention binary features as much as possible, in this paper, we further introduce a distribution correction method based on knowledge distillation, which performs counterpoint distillation on the query <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:math></inline-formula> and key <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:math></inline-formula> and calculates the cross-entropy loss of the binary student network based on the label output of the full-precision teacher network, thereby further correcting the feature distribution of the binarization.</p><p>The attention feature matrices <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:math></inline-formula> were extracted in a layerwise manner from the full-precision teacher network, and the mean squared error (MSE) was used as the loss function to measure the difference between the binary student network and the teacher network in the corresponding features [<xref rid="B29-sensors-24-08190" ref-type="bibr">29</xref>]. The expression for feature counterpoint distillation is as follows: <disp-formula id="FD20-sensors-24-08190"><label>(20)</label><mml:math id="mm127" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>l</mml:mi></mml:munderover><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mi mathvariant="bold-italic">K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>l</mml:mi></mml:munderover><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Let <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> be the output prediction of the teacher network, which is used for student network learning to generate soft labels, and let <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> be the output prediction of the student network. Then, based on the cross-entropy loss function, the following distillation formula can be obtained: <disp-formula id="FD21-sensors-24-08190"><label>(21)</label><mml:math id="mm130" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mtext mathvariant="italic">hd-dist</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x003a8;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x003a8;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:math></inline-formula> denotes the true label, <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the label derived from the teacher network, <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x003a8;</mml:mo><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the softmax function, <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the cross-entropy loss function, and <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></inline-formula> denote the weight scaling factors.</p><p>Therefore, the distillation architecture proposed in this paper first corrects the feature matrix of attention by distillation and then minimizes the loss of the student network with respect to the true labels, while also minimizing the label loss of the teacher network. The distillation objective function can be expressed as follows: <disp-formula id="FD22-sensors-24-08190"><label>(22)</label><mml:math id="mm137" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mi mathvariant="bold-italic">K</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Since the performance of the full-precision teacher network is superior to that of the binary network, the proposed distillation network of the teacher&#x02013;student architecture selects <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. By increasing the full-precision teacher correction weights to further enhance the representational ability of the binary neural network, the performance of the student network can be made to approach full-precision to the greatest extent.</p></sec></sec><sec id="sec4-sensors-24-08190"><title>4. Experiments</title><p>To assess the performance of the proposed ACD-BiT, a series of experiments were conducted on four widely used benchmark datasets: CIFAR10, CIFAR100, ImageNet-1k, and TinyImageNet. First, the experimental datasets, evaluation metrics, and experiment setting were introduced. Second, the loss gradient update parameter of the distillation architecture was tuned and verified. Then, compared with some representative binary transformers, the performance of the model was verified. Then, ablation experiments were carried out on distribution alignment and distillation correction, respectively, to evaluate the effectiveness of the proposed model. Lastly, a series of visualization experiments were carried out to further verify the representation ability of the model. BinaryViT [<xref rid="B30-sensors-24-08190" ref-type="bibr">30</xref>], the classical binary network of PVT architecture, was selected as the baseline model, and all parameters were consistent with the original model except the transformer block.</p><sec id="sec4dot1-sensors-24-08190"><title>4.1. Implementation Details</title><p>In the experiments, ACD-BiT was based on a binary transformer implemented in the PyTorch framework using novel distributed reduction and distillation correction methods. The experiments were conducted on a server equipped with two Tesla A100 GPUs, and the operating system was running on Ubuntu 20.04.</p><p><bold>Benchmark Datasets.</bold> To evaluate the effectiveness of the proposed model, the experiments were verified on four datasets.</p><list list-type="bullet"><list-item><p>CIFAR10 [<xref rid="B31-sensors-24-08190" ref-type="bibr">31</xref>] consists of 60,000 color images of size 32 &#x000d7; 32 divided into 10 categories with 6000 images each. It contains 50,000 training images and 10,000 test images, divided into five training batches and one test batch (each batch contains 10,000 images).</p></list-item><list-item><p>CIFAR100 [<xref rid="B31-sensors-24-08190" ref-type="bibr">31</xref>] contains 100 categories with 600 images per category. Each category is divided into 500 training images and 100 test images. The 100 categories are organized into 20 superclasses such that each image has a &#x0201c;fine&#x0201d; label and a &#x0201c;coarse&#x0201d; label.</p></list-item><list-item><p>ImageNet-1k [<xref rid="B32-sensors-24-08190" ref-type="bibr">32</xref>] consists of 1,281,167 training images and 50,000 validation images covering 1000 categories. Each image is carefully labeled and contains diverse and refined categories, making this one of the most complex large-scale image classification datasets.</p></list-item><list-item><p>Tiny-ImageNet [<xref rid="B32-sensors-24-08190" ref-type="bibr">32</xref>] is extracted from the ImageNet dataset and contains 100,000 images of 200 categories (500 for each class) downsized to 64 &#x000d7; 64 colored images. There are 500 training images and 50 test images for each class.</p></list-item></list><p><bold>Evaluation Metrics.</bold> The main evaluation metrics of the experiment are the image classification accuracy of Top-1 and Top-5, which characterize the accuracy of the first category and the first five categories predicted by the model classification, respectively.</p><p><bold>Experiment Setting.</bold> The weights and activations, including <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:math></inline-formula> and projection layers of the attention module, were binarized by distribution alignment to realize binary attention. In all experiments, the images were uniformly scaled to 224 &#x000d7; 224, the batch size was set to 128, and the model was trained using the AdamW optimizer with an initial learning rate of <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and a weight decay of 0.05. For fair comparison, all experiments were trained for 300 epochs. In the training process, the ViTForImageClassification model was first used to train the teacher model with full precision and then load the optimal weight of the teacher model into the feature values in intermediate layers and prediction outputs of the student network for distillation correction.</p></sec><sec id="sec4dot2-sensors-24-08190"><title>4.2. Hyperparameter Tuning Validation</title><p>In the proposed ACD-BiT, the distillation correction method adopted sets the loss gradient update scaling factor for the full-precision teacher network and the binary student network, which significantly affects the overall performance of the model. Specific hyperparameter tuning and validation experiments were conducted on CIFAR10 and CIFAR100, as shown in <xref rid="sensors-24-08190-t001" ref-type="table">Table 1</xref>.</p><p>The results obtained from the experiments show that as the loss calculation ratio of the full-precision teacher network continues to increase, the overall performance of the model continuously improves. Moreover, the experimental results on CIFAR10 and CIAFAR100 datasets are consistent. It is evident that by increasing the full-precision teacher correction weights, the representational ability of the binary neural network is further enhanced, thereby making the performance of the student network as close to full precision as possible. Therefore, a distillation network with a teacher&#x02013;student architecture is proposed with <inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec4dot3-sensors-24-08190"><title>4.3. Comparative Experiments and Analysis</title><p>To quantitatively evaluate the performance of the proposed binary transformer based on distribution alignment and correction, a series of quantitative methods were compared and analyzed against classic models on CIFAR-10, CIFAR-100, and ImageNet-1k. This includes the overall performance and computational cost of the models.</p><sec id="sec4dot3dot1-sensors-24-08190"><title>4.3.1. Performance Comparison</title><p>As shown in <xref rid="sensors-24-08190-t002" ref-type="table">Table 2</xref>, the performance degradation of the model increased with the continuous increase in the quantization gradient. The proposed method, ACD-BiT, effectively improved the performance of the model while weights and activations both reached the ultimate compression rate of 1 bit. Compared to the baseline model BinaryViT, Top-1 and Top-5 accuracy were improved by 5.37% and 0.63% on CIFAR10 and 1.65% and 2.5% on CIFAR100, respectively. Compared with the state-of-the-art model BinaryFormer, the classification accuracy also improved by 2.65% and 0.37% on CIFAR10 and 1.22% and 2.7% on CIFAR100, respectively. For the large-scale dataset ImageNet-1k, the proposed method had higher inference accuracy. It was 5.43% and 3.39% higher than BinaryViT on Top-1 and Top-5 and 4.30% and 0.79% higher than BinaryFormer, respectively.</p></sec><sec id="sec4dot3dot2-sensors-24-08190"><title>4.3.2. Efficiency Comparison</title><p>To further illustrate the computational cost of the proposed method, <xref rid="sensors-24-08190-t002" ref-type="table">Table 2</xref> also show the model size and computational cost comparison of different methods. The calculation method for model size is the number of model parameters multiplied by four, which is used to evaluate the cost of storage space required for model deployment. The total number of operations (OPs) is calculated using binary operations (BOPs) and floating-point operations (FLOPs), that is, OPs = BOPs/64 + FLOPs, which is used to evaluate the computational cost of the method. It is evident that the memory occupation of ACD-BiT was only 5.8% of the full-precision network, and the computational overhead was reduced by 47.8%. Compared with all the binarization methods, ACD-BiT had the same memory and computational complexity but achieved further improvement in accuracy.</p></sec><sec id="sec4dot3dot3-sensors-24-08190"><title>4.3.3. Robustness Comparison</title><p>In order to further evaluate the robustness of the model, Gaussian blur was used to perform varying degrees of blur operations on the CIFAR10 and CIFAR100 training datasets. The experiment compared the proposed distribution alignment and correction binary transformer with the classic Sign binary model, with the experimental results shown in <xref rid="sensors-24-08190-f005" ref-type="fig">Figure 5</xref>.</p><p>As can be clearly seen from the curve graph, the model performance decreased to a certain extent as the blur gradient increased. The ACD-BiT architecture (indicated by the red curve) proposed in this paper decreased from 89.72% to 88.31% in Top-1 accuracy, and the performance was only reduced by 1.41%. In contrast, the performance of the traditional Sign model decreased by 5.8%, which is a significant drop. It is evident that the overall performance of ACD-BiT was superior to that of the Sign model, and the decline in its performance tends to be gentle with the increase in the blur degree, reflecting good robustness.</p></sec><sec id="sec4dot3dot4-sensors-24-08190"><title>4.3.4. Comparative Experimental Analysis</title><p>Comparative experimental results indicate that the binary quantization ACD-BiT model achieved the best performance on the classic CIFAR image classification datasets and also reached the optimal accuracy on the complex larger-scale dataset ImageNet-1k. Extreme binary quantization significantly reduces accuracy due to the loss of information. This paper combines distribution alignment and distillation correction methods to maximize the restoration of binary feature distributions, improving inference efficiency and further enhancing the accuracy of model inference. Compared to other binarization methods, ACD-BiT adopts an efficient binarization method based on median shift and mean restoration, directly operating on raw full-precision data to avoid the restoration process during inference. This method maximizes information entropy while maintaining a consistent feature data distribution with the original data, thereby improving the accuracy of model inference. In addition, the distillation correction method uses the full-precision teacher network to distill the middle layers features and prediction output of the student network so as to further ensure the consistency of data distribution and enhance the capabilities of the binary network.</p></sec></sec><sec id="sec4dot4-sensors-24-08190"><title>4.4. Ablation Experiments and Analysis</title><p>This section describes the ablation experiments that were conducted on the two core components of the proposed model ACD-BiT: distribution alignment and distillation correction. It fully verifies the effectiveness of the two modules on model performance and then offers a specific analysis according to the experimental results.</p><sec id="sec4dot4dot1-sensors-24-08190"><title>4.4.1. Effectiveness of Two Modules in Binary Strategy</title><p>The ablation experiment results are shown in <xref rid="sensors-24-08190-t003" ref-type="table">Table 3</xref>. The binarization method of ACD-BiT achieved extreme compression to 1 bit in terms of weights and activations, with distribution alignment (Da) and distillation correction (Dc) as the two main modules. First, applying the binary strategy of distribution alignment (Da) alone, the model Top-1 accuracy was improved by 3.26%, 0.78%, and 1.99% on the CIFAR10, CIFAR100, and TinyImageNet datasets, respectively. Meanwhile, the Top-5 accuracy was increased by 0.23%, 1.39%, and 0.78%, respectively. Then, only using distillation-based distribution correction (Dc), the Top-1 accuracy of the model also increased by 1.86%, 0.46%, and 0.69% on the three datasets, and the Top-5 accuracy was improved by 0.02%, 1.33%, and 0.15%, respectively. It is evident that the combination of the above two methods optimized model performance to the greatest extent, achieving the highest precision in binarization.</p></sec><sec id="sec4dot4dot2-sensors-24-08190"><title>4.4.2. Ablation Experimental Analysis</title><p>Through the ablation experiments, the effectiveness and necessity of the distribution alignment (Da) and the distillation correction (Dc) methods in the proposed binary transformer were effectively verified. The binary strategy proposed in this paper effectively solved the problem of feature data distribution deviation caused by information entropy maximization in binary neural networks. The distribution alignment method directly operated on the original full-precision network while maintaining the diversity of information flow data, thereby improving the inference efficiency and reducing information loss caused by quantization. Through median shift and mean restore operation, the original full-precision data distribution was maximally recovered while ensuring the maximum information entropy of the binary data, thereby enhancing the representational capability of the model. The correction method based on knowledge distillation constructed a teacher&#x02013;student structure by training a full-precision teacher network and a binary student network. The counterpoint distillation of the middle layer features of the student network was combined with the prediction output cross-entropy loss calculation to further restore the binary data distribution and improve the reasoning ability of the model. In summary, binarized models combined with distribution alignment and correction can effectively improve inference performance.</p></sec></sec><sec id="sec4dot5-sensors-24-08190"><title>4.5. Visualization Analysis Experiments</title><p>In this section, to further verify the effectiveness of ACD-BiT, the binary feature distribution of feature values, the feature relationship mapping distribution, and the self-attention feature mapping are visualized on the TinyImageNet dataset, respectively.</p><sec id="sec4dot5dot1-sensors-24-08190"><title>4.5.1. Effectiveness of Distribution Alignment</title><p>The binary strategy based on median shift and mean restore can maximize the consistency between the binary feature distribution and the original full-precision feature distribution. <xref rid="sensors-24-08190-f006" ref-type="fig">Figure 6</xref> illustrates the comparison of feature distributions under different binary strategies, using the symmetric axis of the full-precision feature distribution (red dashed line) as the evaluation criterion. It is evident that the Sign binary leads to severe information loss, and there is a large quantization error compared to the full-precision data. The feature reshaping binary aims to make &#x02212;1 and 1 as balanced as possible, maximizing information entropy, but the feature distribution deviates from the symmetric center and has a large deviation from the original full-precision feature. Our distribution alignment and correction binary effectively restores the original full-precision feature distribution while satisfying the maximum information entropy. Its binary feature sets are aligned based on the symmetric axis, thereby comprehensively enhancing the reasoning ability of the binary network.</p></sec><sec id="sec4dot5dot2-sensors-24-08190"><title>4.5.2. Performance of Feature Representation</title><p>To further illustrate that the distribution alignment binary strategy can maximize the inference capability of the model through the median shift and mean restore methods, the feature distribution visualization of the attention feature relationship mapping and its normalized features was performed. The results are shown in <xref rid="sensors-24-08190-f007" ref-type="fig">Figure 7</xref>, and the relationship features of both Sign binarization and feature reshaping binarization show significant deviations from the full-precision feature distribution, indicating a severe decline in the representational capability of the feature relationships. Although feature reshaping maximized information entropy and increased the information capacity of models, the singleness of the information flow data also led to insufficient representation of feature relationships based on binarized features. It is obvious that the distribution alignment method greatly enhanced the representational capability of the feature relationships and that the normalized relationship mapping feature distribution remained consistent with the full-precision. From this, it can be seen that the distribution alignment binary strategy not only maximizes information entropy but also restores the full-precision data distribution while maintaining the diversity of the information flow data, thereby effectively enhancing the inference capability of the models.</p></sec><sec id="sec4dot5dot3-sensors-24-08190"><title>4.5.3. Attention Feature-Guided Visualization</title><p>To analyze the inference ability of the proposed binary quantization model, feature mapping of the self-attention layer was used to generate the feature activation map [<xref rid="B37-sensors-24-08190" ref-type="bibr">37</xref>]. As shown in <xref rid="sensors-24-08190-f008" ref-type="fig">Figure 8</xref>, it highlights the key areas of images that are most relevant to the specific classification prediction, demonstrating the image locations guided by the attention features of model. It is evident that the proposed ACD-BiT could accurately guide the network to focus on the critical areas of the predicted classification objects. Meanwhile, compared to the baseline BinaryViT and the feature reshaping binary quantization model, ACD-BiT paid more attention to areas containing target objects. The visualization results are consistent with the theoretical analysis, which demonstrated that the proposed binary quantization transformer has superior feature representation capability.</p></sec><sec id="sec4dot5dot4-sensors-24-08190"><title>4.5.4. Quantization Error Effect on Attention Matrix</title><p>In order to minimize the computational consumption and deployment pressure, this paper proposes a binary quantization transformer. However, the binary quantization of feature information inevitably brings about representation errors. In order to analyze and compare the performance impact of quantization error on various layers of the model in more detail, the attention feature layer mapping was visualized, as shown in <xref rid="sensors-24-08190-f009" ref-type="fig">Figure 9</xref>. Compared with the full-precision feature map without quantization errors, the distribution of the quantized feature should be as consistent with the full-precision data as possible in diversity and accuracy. It is obvious that the binarized feature distribution of the traditional Sign function was relatively single. As the number of layers increased, the proposed ACD-BiT feature distribution was the closest to the original full-precision and had the strongest representation power.</p><p>In the future, in order to further improve the model performance of the binarized network, it is necessary to further study the feature properties and distribution characteristics of the model so as to make the binary network accuracy as consistent with the full-precision performance as possible.</p></sec><sec id="sec4dot5dot5-sensors-24-08190"><title>4.5.5. Visual Experimental Analysis</title><p>From the four types of visualization experiments, ACD-BiT combined with the distribution alignment based on median shift and mean restore and the distillation correction method based on the teacher&#x02013;student structure achieved superior performance in terms of binary feature distribution, feature relationship mapping distribution, attention feature representation, and quantization error reduction.</p><p>The distribution alignment binary strategy directly performed median shift and mean restore on the full-precision data, ensuring that the quantized binary features maximize the information entropy while maintaining the feature distribution consistent with the full-precision data. Meanwhile, the diversity of information flow data was preserved through modulus scaling and restore, enhancing the feature representation capability. Subsequently, the full-precision teacher network further corrected the feature distribution of the binary student network, minimizing the quantization loss and further ensuring the completeness and accuracy of the data. Through the combination of Da and Dc, the inference ability of the binary transformer model was effectively improved.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-24-08190"><title>5. Conclusions</title><p>Binary quantization transformers, which are more suitable for smart sensors, are seldom investigated. The key bottleneck is the distribution changes compared with full-precision networks, which is caused by existing binarization optimization methods. Motivated by this, the authors of this paper demonstrate improvements from two aspects. (1) The binarization was investigated through a distribution alignment operation. The median shift and mean restore methods were carefully designed to maintain the consistency of the data distribution. Then, the binarization of attention and full-connection were optimized through distribution alignment. It was also discussed whether the effective binary operator is still under the constraint of maximum information entropy. (2) A knowledge distillation architecture for distribution rectification was proposed. The teacher&#x02013;student structure was chosen according to the full-precision and binary transformers. The feature distribution was further corrected by the counterpoint distillation of the attention middle layer features and the cross-entropy loss calculation of the prediction output. This guaranteed that the data distribution of binary transformers was as consistent with the full-precision models as possible. As a result, the experiments showed that our model was superior to most existing mechanisms. This means that information entropy and data distribution are all-important factors in binary transformers. Based on this study, a complex transformer model can be deployed in a variety of smart sensors with poor computation resources.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>K.W.: Conceptualization, methodology, software, data curation, writing&#x02014;original draft, and writing&#x02014;review and editing. M.W.: Conceptualization, software, and writing&#x02014;review and editing. Z.W.: Conceptualization, data, and writing&#x02014;review and editing. T.S.: Conceptualization, project administration, funding acquisition, and writing&#x02014;review and editing. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-24-08190"><label>1.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Alexey</surname><given-names>D.</given-names></name>
<name><surname>Lucas</surname><given-names>D.</given-names></name>
<name><surname>Alexer</surname><given-names>K.</given-names></name>
<name><surname>Dirk</surname><given-names>W.</given-names></name>
<name><surname>Zhai</surname><given-names>X.H.</given-names></name>
<name><surname>Thomas</surname><given-names>U.</given-names></name>
<name><surname>Mostafa</surname><given-names>D.</given-names></name>
<name><surname>Matthias</surname><given-names>M.</given-names></name>
<name><surname>Georg</surname><given-names>H.</given-names></name>
<name><surname>Sylvain</surname><given-names>G.</given-names></name>
<etal/>
</person-group><article-title>An image is worth 16x16 words: Transformers for image recognition at scale</article-title><source>Proceedings of the International Conference on Learning Representations</source><conf-loc>Addis Ababa, Ethiopia</conf-loc><conf-date>26&#x02013;30 April 2020</conf-date><volume>Volume 1</volume><fpage>3</fpage></element-citation></ref><ref id="B2-sensors-24-08190"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>F.</given-names></name>
<name><surname>Song</surname><given-names>Q.</given-names></name>
<name><surname>Jin</surname><given-names>G.</given-names></name>
</person-group><article-title>The classification and denoising of image noise based on deep neural networks</article-title><source>Appl. Intell.</source><year>2020</year><volume>50</volume><fpage>2194</fpage><lpage>2207</lpage><pub-id pub-id-type="doi">10.1007/s10489-019-01623-0</pub-id></element-citation></ref><ref id="B3-sensors-24-08190"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Benjamin</surname><given-names>G.</given-names></name>
<name><surname>Alaaeldin</surname><given-names>E.N.</given-names></name>
<name><surname>Hugo</surname><given-names>T.</given-names></name>
<name><surname>Pierre</surname><given-names>S.</given-names></name>
<name><surname>Arm</surname><given-names>J.</given-names></name>
<name><surname>Herv&#x000e9;</surname><given-names>J.</given-names></name>
<name><surname>Matthijs</surname><given-names>D.</given-names></name>
</person-group><article-title>Levit: A vision transformer in convnet&#x02019;s clothing for faster inference</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#x02013;17 October 2021</conf-date><fpage>12259</fpage><lpage>12269</lpage></element-citation></ref><ref id="B4-sensors-24-08190"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pal</surname><given-names>S.K.</given-names></name>
<name><surname>Pramanik</surname><given-names>A.</given-names></name>
<name><surname>Maiti</surname><given-names>J.</given-names></name>
<name><surname>Mitra</surname><given-names>P.</given-names></name>
</person-group><article-title>Deep learning in multi-object detection and tracking: State of the art</article-title><source>Appl. Intell.</source><year>2021</year><volume>51</volume><fpage>6400</fpage><lpage>6429</lpage><pub-id pub-id-type="doi">10.1007/s10489-021-02293-7</pub-id></element-citation></ref><ref id="B5-sensors-24-08190"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Nicolas</surname><given-names>C.</given-names></name>
<name><surname>Francisco</surname><given-names>M.</given-names></name>
<name><surname>Gabriel</surname><given-names>S.</given-names></name>
<name><surname>Nicolas</surname><given-names>U.</given-names></name>
<name><surname>Alexer</surname><given-names>K.</given-names></name>
<name><surname>Sergey</surname><given-names>Z.</given-names></name>
</person-group><article-title>End-to-end object detection with transformers</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#x02013;28 August, 2020</conf-date><volume>Volume 1</volume><fpage>213</fpage><lpage>229</lpage></element-citation></ref><ref id="B6-sensors-24-08190"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>S.</given-names></name>
<name><surname>Lu</surname><given-names>J.</given-names></name>
<name><surname>Zhao</surname><given-names>H.</given-names></name>
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<name><surname>Luo</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Fu</surname><given-names>Y.</given-names></name>
<name><surname>Feng</surname><given-names>J.</given-names></name>
<name><surname>Xiang</surname><given-names>T.</given-names></name>
<name><surname>Torr</surname><given-names>P.H.</given-names></name>
<etal/>
</person-group><article-title>Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#x02013;25 June 2021</conf-date><fpage>6877</fpage><lpage>6886</lpage><pub-id pub-id-type="doi">10.1109/CVPR46437.2021.00681</pub-id></element-citation></ref><ref id="B7-sensors-24-08190"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Strudel</surname><given-names>R.</given-names></name>
<name><surname>Garcia</surname><given-names>R.</given-names></name>
<name><surname>Laptev</surname><given-names>I.</given-names></name>
<name><surname>Schmid</surname><given-names>C.</given-names></name>
</person-group><article-title>Segmenter: Transformer for semantic segmentation</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#x02013;17 October 2021</conf-date><fpage>7262</fpage><lpage>7272</lpage></element-citation></ref><ref id="B8-sensors-24-08190"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Han</surname><given-names>K.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>H.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Guo</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Tang</surname><given-names>Y.</given-names></name>
<name><surname>Xiao</surname><given-names>A.</given-names></name>
<name><surname>Xu</surname><given-names>C.</given-names></name>
<name><surname>Xu</surname><given-names>Y.</given-names></name>
<etal/>
</person-group><article-title>A survey on vision transformer</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>45</volume><fpage>87</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3152247</pub-id><pub-id pub-id-type="pmid">35180075</pub-id>
</element-citation></ref><ref id="B9-sensors-24-08190"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zeng</surname><given-names>K.</given-names></name>
<name><surname>Wan</surname><given-names>Z.</given-names></name>
<name><surname>Gu</surname><given-names>H.W.</given-names></name>
<name><surname>Shen</surname><given-names>T.</given-names></name>
</person-group><article-title>Self-knowledge distillation enhanced binary neural networks derived from underutilized information</article-title><source>Appl. Intell.</source><year>2024</year><volume>54</volume><fpage>4994</fpage><lpage>5014</lpage><pub-id pub-id-type="doi">10.1007/s10489-024-05444-8</pub-id></element-citation></ref><ref id="B10-sensors-24-08190"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>T.</given-names></name>
<name><surname>Sun</surname><given-names>P.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Zhou</surname><given-names>S.</given-names></name>
</person-group><article-title>FQ-ViT: Post-training quantization for fully quantized vision transformer</article-title><source>Proceedings of the International Joint Conference on Artificial Intelligence</source><conf-loc>Vienna, Austria</conf-loc><conf-date>23&#x02013;29 July 2022</conf-date><fpage>1173</fpage><lpage>1179</lpage></element-citation></ref><ref id="B11-sensors-24-08190"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Narang</surname><given-names>S.</given-names></name>
<name><surname>Diamos</surname><given-names>G.</given-names></name>
<name><surname>Elsen</surname><given-names>E.</given-names></name>
<name><surname>Micikevicius</surname><given-names>P.</given-names></name>
<name><surname>Alben</surname><given-names>J.</given-names></name>
<name><surname>Garcia</surname><given-names>D.</given-names></name>
<name><surname>Ginsburg</surname><given-names>B.</given-names></name>
<name><surname>Houston</surname><given-names>M.</given-names></name>
<name><surname>Kuchaiev</surname><given-names>O.</given-names></name>
<name><surname>Venkatesh</surname><given-names>G.</given-names></name>
<etal/>
</person-group><article-title>Mixed precision training</article-title><source>Proceedings of the Conference on Learning Representation</source><conf-loc>Toulon, France</conf-loc><conf-date>24&#x02013;26 April 2017</conf-date></element-citation></ref><ref id="B12-sensors-24-08190"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Fan</surname><given-names>A.</given-names></name>
<name><surname>Stock</surname><given-names>P.</given-names></name>
<name><surname>Graham</surname><given-names>B.</given-names></name>
<name><surname>Grave</surname><given-names>E.</given-names></name>
<name><surname>Gribonval</surname><given-names>R.</given-names></name>
<name><surname>Jegou</surname><given-names>H.</given-names></name>
<name><surname>Joulin</surname><given-names>A.</given-names></name>
</person-group><article-title>Training with quantization noise for extreme model compression</article-title><source>Proceedings of the Conference on Learning Representation</source><conf-loc>Vienna, Austria</conf-loc><conf-date>4&#x02013;8 May 2021</conf-date><fpage>1</fpage><lpage>13</lpage></element-citation></ref><ref id="B13-sensors-24-08190"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yuan</surname><given-names>Z.</given-names></name>
<name><surname>Xue</surname><given-names>C.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>Q.</given-names></name>
<name><surname>Sun</surname><given-names>G.</given-names></name>
</person-group><article-title>PTQ4ViT: Post-training quantization framework for vision transformers</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2111.12293</pub-id></element-citation></ref><ref id="B14-sensors-24-08190"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Lin</surname><given-names>M.</given-names></name>
<name><surname>Gao</surname><given-names>P.</given-names></name>
<name><surname>Guo</surname><given-names>G.</given-names></name>
<name><surname>L&#x000fc;</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
</person-group><article-title>Q-detr: An efficient low-bit quantized detection transformer</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>18&#x02013;22 June 2023</conf-date><fpage>3842</fpage><lpage>3851</lpage></element-citation></ref><ref id="B15-sensors-24-08190"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gu</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
<name><surname>Han</surname><given-names>J.</given-names></name>
<name><surname>Cao</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Doermann</surname><given-names>D.</given-names></name>
</person-group><article-title>Projection convolutional neural networks for 1-bit CNNs via discrete back propagation</article-title><source>Proceedings of the Association for the Advancement of Artificial Intelligence</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>27 January&#x02013;1 February 2019</conf-date><fpage>8344</fpage><lpage>8351</lpage></element-citation></ref><ref id="B16-sensors-24-08190"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>M.</given-names></name>
<name><surname>Xu</surname><given-names>Z.</given-names></name>
<name><surname>Zheng</surname><given-names>B.</given-names></name>
<name><surname>Xie</surname><given-names>W.</given-names></name>
</person-group><article-title>BinaryFormer: A Hierarchical-Adaptive Binary Vision Transformer (ViT) for Efficient Computing</article-title><source>IEEE Trans. Ind. Inform.</source><year>2024</year><volume>20</volume><fpage>10657</fpage><lpage>10668</lpage><pub-id pub-id-type="doi">10.1109/TII.2024.3396520</pub-id></element-citation></ref><ref id="B17-sensors-24-08190"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yuan</surname><given-names>C.</given-names></name>
<name><surname>AGaian</surname><given-names>S.S.</given-names></name>
</person-group><article-title>A comprehensive review of binary neural network</article-title><source>Artif. Intell. Rev.</source><year>2023</year><volume>56</volume><fpage>12949</fpage><lpage>13013</lpage><pub-id pub-id-type="doi">10.1007/s10462-023-10464-w</pub-id></element-citation></ref><ref id="B18-sensors-24-08190"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qin</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Gong</surname><given-names>R.</given-names></name>
<name><surname>Ding</surname><given-names>Y.</given-names></name>
<name><surname>Xu</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
</person-group><article-title>Distribution-sensitive information retention for accurate binary neural network</article-title><source>Int. J. Comput. Vis.</source><year>2023</year><volume>131</volume><fpage>26</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1007/s11263-022-01687-5</pub-id></element-citation></ref><ref id="B19-sensors-24-08190"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>M.</given-names></name>
<name><surname>Ji</surname><given-names>R.</given-names></name>
<name><surname>Xu</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>Y.</given-names></name>
<name><surname>Huang</surname><given-names>F.</given-names></name>
<name><surname>Lin</surname><given-names>C.W.</given-names></name>
</person-group><article-title>Rotated Binary Neural Network</article-title><source>Adv. Neural Inf. Process. Syst. (Neurips)</source><year>2020</year><volume>33</volume><fpage>7474</fpage><lpage>7485</lpage></element-citation></ref><ref id="B20-sensors-24-08190"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Xu</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
<name><surname>Cao</surname><given-names>X.</given-names></name>
<name><surname>Gao</surname><given-names>P.</given-names></name>
<name><surname>Guo</surname><given-names>G.</given-names></name>
</person-group><article-title>Q-vit: Accurate and fully quantized low-bit vision transformer</article-title><source>Adv. Neural Inf. Process. Syst. (Neurips)</source><year>2022</year><volume>35</volume><fpage>34451</fpage><lpage>34463</lpage></element-citation></ref><ref id="B21-sensors-24-08190"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>C.</given-names></name>
<name><surname>Ding</surname><given-names>W.</given-names></name>
<name><surname>Chen</surname><given-names>P.</given-names></name>
<name><surname>Zhuang</surname><given-names>B.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Zhao</surname><given-names>Y.</given-names></name>
</person-group><article-title>RB-Net: Training highly accurate and efficient binary neural networks with reshaped point-wise convolution and balanced activation</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2022</year><volume>32</volume><fpage>6414</fpage><lpage>6424</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2022.3166803</pub-id></element-citation></ref><ref id="B22-sensors-24-08190"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>M.</given-names></name>
<name><surname>Gao</surname><given-names>W.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Xie</surname><given-names>J.</given-names></name>
<name><surname>Gong</surname><given-names>M.</given-names></name>
</person-group><article-title>Universal Binary Neural Networks Design by Improved Differentiable Neural Architecture Search</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2024</year><volume>34</volume><fpage>9153</fpage><lpage>9165</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2024.3398691</pub-id></element-citation></ref><ref id="B23-sensors-24-08190"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Tu</surname><given-names>Z.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>P.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Adabin: Improving binary neural networks with adaptive binary sets</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#x02013;27 October 2022</conf-date><fpage>379</fpage><lpage>395</lpage></element-citation></ref><ref id="B24-sensors-24-08190"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yoshua</surname><given-names>B.</given-names></name>
<name><surname>Nicholas</surname><given-names>L.</given-names></name>
<name><surname>Aaron</surname><given-names>C.</given-names></name>
</person-group><article-title>Estimating or propagating gradients through stochastic neurons for conditional computation</article-title><source>arXiv</source><year>2013</year><pub-id pub-id-type="arxiv">1308.3432</pub-id></element-citation></ref><ref id="B25-sensors-24-08190"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Messerschmitt</surname><given-names>D.</given-names></name>
</person-group><article-title>Quantizing for maximum output entropy (corresp.)</article-title><source>IEEE Trans. Inf. Theory</source><year>1971</year><volume>17</volume><fpage>612</fpage><pub-id pub-id-type="doi">10.1109/TIT.1971.1054681</pub-id></element-citation></ref><ref id="B26-sensors-24-08190"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Qin</surname><given-names>H.</given-names></name>
<name><surname>Gong</surname><given-names>R.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Shen</surname><given-names>M.</given-names></name>
<name><surname>Wei</surname><given-names>Z.</given-names></name>
<name><surname>Yu</surname><given-names>F.</given-names></name>
<name><surname>Song</surname><given-names>J.</given-names></name>
</person-group><article-title>Forward and backward information retention for accurate binary neural networks</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;19 June 2020</conf-date><fpage>2250</fpage><lpage>2259</lpage></element-citation></ref><ref id="B27-sensors-24-08190"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Touvron</surname><given-names>H.</given-names></name>
<name><surname>Cord</surname><given-names>M.</given-names></name>
<name><surname>Douze</surname><given-names>M.</given-names></name>
<name><surname>Massa</surname><given-names>F.</given-names></name>
<name><surname>Sablayrolles</surname><given-names>A.</given-names></name>
<name><surname>J&#x000e9;gou</surname><given-names>H.</given-names></name>
</person-group><article-title>Training data-efficient image transformers &#x00026; distillation through attention</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Virtual</conf-loc><conf-date>18&#x02013;24 July 2021</conf-date><fpage>10347</fpage><lpage>10357</lpage></element-citation></ref><ref id="B28-sensors-24-08190"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>W.</given-names></name>
<name><surname>Hou</surname><given-names>L.</given-names></name>
<name><surname>Yin</surname><given-names>Y.</given-names></name>
<name><surname>Shang</surname><given-names>L.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Jiang</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>Q.</given-names></name>
</person-group><article-title>Ternarybert: Distillation-aware ultra-low bit bert</article-title><source>Proceedings of the Conference on Empirical Methods in Natural Language Processing</source><conf-loc>Online</conf-loc><conf-date>16&#x02013;20 November 2020</conf-date></element-citation></ref><ref id="B29-sensors-24-08190"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Aguilar</surname><given-names>G.</given-names></name>
<name><surname>Ling</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Yao</surname><given-names>B.</given-names></name>
<name><surname>Fan</surname><given-names>X.</given-names></name>
<name><surname>Guo</surname><given-names>C.</given-names></name>
</person-group><article-title>Knowledge distillation from internal representations</article-title><source>Proceedings of the Conference on Artificial Intelligence (AAAI)</source><conf-loc>New York, NY, USA</conf-loc><conf-date>7&#x02013;12 February 2020</conf-date><fpage>7350</fpage><lpage>7357</lpage><pub-id pub-id-type="doi">10.1609/aaai.v34i05.6229</pub-id></element-citation></ref><ref id="B30-sensors-24-08190"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Le</surname><given-names>P.H.C.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
</person-group><article-title>BinaryViT: Pushing binary vision transformers towards convolutional models</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>18&#x02013;22 June 2023</conf-date><fpage>4664</fpage><lpage>4673</lpage></element-citation></ref><ref id="B31-sensors-24-08190"><label>31.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Krizhevsky</surname><given-names>A.</given-names></name>
<name><surname>Hinton</surname><given-names>G.</given-names></name>
</person-group><source>Learning Multiple Layers of Features from Tiny Images</source><publisher-name>Computer Science University of Toronto</publisher-name><publisher-loc>Toronto, ON, Canada</publisher-loc><year>2009</year><volume>Volume 7</volume></element-citation></ref><ref id="B32-sensors-24-08190"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Deng</surname><given-names>J.</given-names></name>
<name><surname>Dong</surname><given-names>W.</given-names></name>
<name><surname>Socher</surname><given-names>R.</given-names></name>
<name><surname>Li</surname><given-names>L.-J.</given-names></name>
<name><surname>Li</surname><given-names>K.</given-names></name>
<name><surname>Fei-Fei</surname><given-names>L.</given-names></name>
</person-group><article-title>ImageNet: A large-scale hierarchical image database</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Miami, FL, USA</conf-loc><conf-date>20&#x02013;25 June 2009</conf-date><fpage>248</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></element-citation></ref><ref id="B33-sensors-24-08190"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mohammad</surname><given-names>R.</given-names></name>
<name><surname>Vicente</surname><given-names>O.</given-names></name>
<name><surname>Joseph</surname><given-names>R.</given-names></name>
<name><surname>Ali</surname><given-names>F.</given-names></name>
</person-group><article-title>Xnor-net: Imagenet classification using binary convolutional neural networks</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>11&#x02013;14 October 2016</conf-date><fpage>525</fpage><lpage>542</lpage></element-citation></ref><ref id="B34-sensors-24-08190"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Qin</surname><given-names>H.</given-names></name>
<name><surname>Ding</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Yan</surname><given-names>Q.</given-names></name>
<name><surname>Liu</surname><given-names>A.</given-names></name>
<name><surname>Dang</surname><given-names>Q.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
</person-group><article-title>BiBERT: Accurate fully binarized BERT</article-title><source>Proceedings of the Conference on Learning Representation</source><conf-loc>Virtual</conf-loc><conf-date>25&#x02013;29 April 2022</conf-date><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="B35-sensors-24-08190"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Oguz</surname><given-names>B.</given-names></name>
<name><surname>Pappu</surname><given-names>A.</given-names></name>
<name><surname>Xiao</surname><given-names>L.</given-names></name>
<name><surname>Yih</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>M.</given-names></name>
<name><surname>Krishnamoorthi</surname><given-names>R.</given-names></name>
<name><surname>Mehdad</surname><given-names>Y.</given-names></name>
</person-group><article-title>BiT: Robustly binarized multi-distilled transformer</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>14303</fpage><lpage>14316</lpage></element-citation></ref><ref id="B36-sensors-24-08190"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>Y.</given-names></name>
<name><surname>Lou</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>W.</given-names></name>
<name><surname>Zhou</surname><given-names>H.</given-names></name>
<name><surname>Zhuang</surname><given-names>B.</given-names></name>
</person-group><article-title>BiViT: Extremely compressed binary vision transformers</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#x02013;6 October 2023</conf-date><fpage>5651</fpage><lpage>5663</lpage></element-citation></ref><ref id="B37-sensors-24-08190"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Muhammad</surname><given-names>M.B.</given-names></name>
<name><surname>Yeasin</surname><given-names>M.</given-names></name>
</person-group><article-title>Eigen-CAM: Class activation map using principal components</article-title><source>Proceedings of the International Joint Conference on Neural Network</source><conf-loc>Glasgow, UK</conf-loc><conf-date>19&#x02013;24 July 2020</conf-date><fpage>1</fpage><lpage>7</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-24-08190-f001"><label>Figure 1</label><caption><p>Distribution deviation in binarization.</p></caption><graphic xlink:href="sensors-24-08190-g001" position="float"/></fig><fig position="float" id="sensors-24-08190-f002"><label>Figure 2</label><caption><p>Overall framework of binary transformer.</p></caption><graphic xlink:href="sensors-24-08190-g002" position="float"/></fig><fig position="float" id="sensors-24-08190-f003"><label>Figure 3</label><caption><p>Median shift binarization process. <inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent the full-precision and binary distributions; the middle <inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the shifted full-precision distribution; and <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif-bold-italic">&#x003b2;</mml:mi></mml:mrow></mml:math></inline-formula> is the median.</p></caption><graphic xlink:href="sensors-24-08190-g003" position="float"/></fig><fig position="float" id="sensors-24-08190-f004"><label>Figure 4</label><caption><p>Schematic diagram of the modulus length alignment. <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represent the full-precision and binary vectors, and <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif-bold-italic">&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula> is the scaling factor.</p></caption><graphic xlink:href="sensors-24-08190-g004" position="float"/></fig><fig position="float" id="sensors-24-08190-f005"><label>Figure 5</label><caption><p>Performance on different blur degrees.</p></caption><graphic xlink:href="sensors-24-08190-g005" position="float"/></fig><fig position="float" id="sensors-24-08190-f006"><label>Figure 6</label><caption><p>Feature distributions of utilizing diverse binary strategies. (<bold>a</bold>) is the full-precision feature distribution of the input sample, with the red dashed line indicating the mean symmetry axis of the feature distribution. (<bold>b</bold>) represents the traditional binary strategy based on Sign function. (<bold>c</bold>) is the binary strategy of feature reshaping adopted to maximize information entropy. (<bold>d</bold>) is the distribution alignment binary strategy proposed in this paper to achieve feature distribution restoration. The symmetry axis of the input sample is used as the criterion to evaluate the performance of distribution alignment.</p></caption><graphic xlink:href="sensors-24-08190-g006" position="float"/></fig><fig position="float" id="sensors-24-08190-f007"><label>Figure 7</label><caption><p>Distribution comparison of feature relation mapping under different binary strategies. The top part is the feature distribution of the relationship between feature query and feature key, and the bottom part is the normalized results of the feature relationships.</p></caption><graphic xlink:href="sensors-24-08190-g007" position="float"/></fig><fig position="float" id="sensors-24-08190-f008"><label>Figure 8</label><caption><p>Heat map of self-attention feature mapping.</p></caption><graphic xlink:href="sensors-24-08190-g008" position="float"/></fig><fig position="float" id="sensors-24-08190-f009"><label>Figure 9</label><caption><p>Visualization of attention feature maps across different stages of the model.</p></caption><graphic xlink:href="sensors-24-08190-g009" position="float"/></fig><table-wrap position="float" id="sensors-24-08190-t001"><object-id pub-id-type="pii">sensors-24-08190-t001_Table 1</object-id><label>Table 1</label><caption><p>Validation experiment of loss gradient update scale factor for distillation correction method. <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></inline-formula> are the loss calculation ratios of the full-precision teacher network and the binary student network, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1"><inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi mathvariant="bold-italic">&#x003b1;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula>/<inline-formula><mml:math id="mm156" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">CIFAR10 (%)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">CIFAR100 (%)</th></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-5</bold>
</td></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.4/0.6</td><td align="center" valign="middle" rowspan="1" colspan="1">88.84</td><td align="center" valign="middle" rowspan="1" colspan="1">99.48</td><td align="center" valign="middle" rowspan="1" colspan="1">63.56</td><td align="center" valign="middle" rowspan="1" colspan="1">86.78</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.6/0.4</td><td align="center" valign="middle" rowspan="1" colspan="1">89.20</td><td align="center" valign="middle" rowspan="1" colspan="1">99.55</td><td align="center" valign="middle" rowspan="1" colspan="1">63.68</td><td align="center" valign="middle" rowspan="1" colspan="1">86.87</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.8/0.2</td><td align="center" valign="middle" rowspan="1" colspan="1">89.46</td><td align="center" valign="middle" rowspan="1" colspan="1">99.65</td><td align="center" valign="middle" rowspan="1" colspan="1">63.71</td><td align="center" valign="middle" rowspan="1" colspan="1">86.96</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9/0.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>89.72</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>99.67</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>65.49</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>88.92</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>bolded numbers signify optimal results.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-24-08190-t002"><object-id pub-id-type="pii">sensors-24-08190-t002_Table 2</object-id><label>Table 2</label><caption><p>Overall performance and computational consumption comparison of various quantization methods on the classic CIFAR and ImageNet-1k datasets. W and A denote the weight bit width and the activation bit width, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Methods</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">W/A</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Model Size</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">OPs</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">CIFAR10 (%)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">CIFAR100 (%)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">ImageNet-1k (%)</th></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-5</bold>
</td></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ViT (Teacher) [<xref rid="B1-sensors-24-08190" ref-type="bibr">1</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">32/32</td><td align="center" valign="middle" rowspan="1" colspan="1">86.9 M</td><td align="center" valign="middle" rowspan="1" colspan="1">6.27 G</td><td align="center" valign="middle" rowspan="1" colspan="1">91.63</td><td align="center" valign="middle" rowspan="1" colspan="1">99.74</td><td align="center" valign="middle" rowspan="1" colspan="1">79.59</td><td align="center" valign="middle" rowspan="1" colspan="1">92.40</td><td align="center" valign="middle" rowspan="1" colspan="1">80.0</td><td align="center" valign="middle" rowspan="1" colspan="1">94.94</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mixed-Precision [<xref rid="B11-sensors-24-08190" ref-type="bibr">11</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">16/16</td><td align="center" valign="middle" rowspan="1" colspan="1">47.8 M</td><td align="center" valign="middle" rowspan="1" colspan="1">6.27 G</td><td align="center" valign="middle" rowspan="1" colspan="1">94.58</td><td align="center" valign="middle" rowspan="1" colspan="1">99.79</td><td align="center" valign="middle" rowspan="1" colspan="1">83.06</td><td align="center" valign="middle" rowspan="1" colspan="1">97.13</td><td align="center" valign="middle" rowspan="1" colspan="1">76.02</td><td align="center" valign="middle" rowspan="1" colspan="1">92.52</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Quant-Noise [<xref rid="B12-sensors-24-08190" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">8/8</td><td align="center" valign="middle" rowspan="1" colspan="1">24.8 M</td><td align="center" valign="middle" rowspan="1" colspan="1">6.27 G</td><td align="center" valign="middle" rowspan="1" colspan="1">93.66</td><td align="center" valign="middle" rowspan="1" colspan="1">99.74</td><td align="center" valign="middle" rowspan="1" colspan="1">80.61</td><td align="center" valign="middle" rowspan="1" colspan="1">96.70</td><td align="center" valign="middle" rowspan="1" colspan="1">72.12</td><td align="center" valign="middle" rowspan="1" colspan="1">89.75</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FQ-ViT [<xref rid="B10-sensors-24-08190" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">8/4</td><td align="center" valign="middle" rowspan="1" colspan="1">13.4 M</td><td align="center" valign="middle" rowspan="1" colspan="1">3.21 G</td><td align="center" valign="middle" rowspan="1" colspan="1">92.21</td><td align="center" valign="middle" rowspan="1" colspan="1">99.60</td><td align="center" valign="middle" rowspan="1" colspan="1">77.84</td><td align="center" valign="middle" rowspan="1" colspan="1">95.84</td><td align="center" valign="middle" rowspan="1" colspan="1">64.79</td><td align="center" valign="middle" rowspan="1" colspan="1">86.58</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">XNOR-Net [<xref rid="B33-sensors-24-08190" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1/1</td><td align="center" valign="middle" rowspan="1" colspan="1">4.8 M</td><td align="center" valign="middle" rowspan="1" colspan="1">3.21 G</td><td align="center" valign="middle" rowspan="1" colspan="1">75.17</td><td align="center" valign="middle" rowspan="1" colspan="1">98.28</td><td align="center" valign="middle" rowspan="1" colspan="1">57.61</td><td align="center" valign="middle" rowspan="1" colspan="1">82.79</td><td align="center" valign="middle" rowspan="1" colspan="1">48.45</td><td align="center" valign="middle" rowspan="1" colspan="1">73.04</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BiBERT [<xref rid="B34-sensors-24-08190" ref-type="bibr">34</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1/1</td><td align="center" valign="middle" rowspan="1" colspan="1">4.8 M</td><td align="center" valign="middle" rowspan="1" colspan="1">3.21 G</td><td align="center" valign="middle" rowspan="1" colspan="1">80.13</td><td align="center" valign="middle" rowspan="1" colspan="1">98.77</td><td align="center" valign="middle" rowspan="1" colspan="1">58.45</td><td align="center" valign="middle" rowspan="1" colspan="1">83.04</td><td align="center" valign="middle" rowspan="1" colspan="1">50.32</td><td align="center" valign="middle" rowspan="1" colspan="1">77.73</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BiT [<xref rid="B35-sensors-24-08190" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1/1</td><td align="center" valign="middle" rowspan="1" colspan="1">4.8 M</td><td align="center" valign="middle" rowspan="1" colspan="1">3.21 G</td><td align="center" valign="middle" rowspan="1" colspan="1">80.40</td><td align="center" valign="middle" rowspan="1" colspan="1">98.78</td><td align="center" valign="middle" rowspan="1" colspan="1">60.31</td><td align="center" valign="middle" rowspan="1" colspan="1">85.98</td><td align="center" valign="middle" rowspan="1" colspan="1">51.01</td><td align="center" valign="middle" rowspan="1" colspan="1">77.99</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BiViT [<xref rid="B36-sensors-24-08190" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1/1</td><td align="center" valign="middle" rowspan="1" colspan="1">4.8 M</td><td align="center" valign="middle" rowspan="1" colspan="1">3.21 G</td><td align="center" valign="middle" rowspan="1" colspan="1">82.28</td><td align="center" valign="middle" rowspan="1" colspan="1">98.85</td><td align="center" valign="middle" rowspan="1" colspan="1">61.08</td><td align="center" valign="middle" rowspan="1" colspan="1">85.50</td><td align="center" valign="middle" rowspan="1" colspan="1">50.59</td><td align="center" valign="middle" rowspan="1" colspan="1">78.84</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BinaryViT (Baseline) [<xref rid="B30-sensors-24-08190" ref-type="bibr">30</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1/1</td><td align="center" valign="middle" rowspan="1" colspan="1">5.0 M</td><td align="center" valign="middle" rowspan="1" colspan="1">3.27 G</td><td align="center" valign="middle" rowspan="1" colspan="1">84.35</td><td align="center" valign="middle" rowspan="1" colspan="1">99.04</td><td align="center" valign="middle" rowspan="1" colspan="1">63.84</td><td align="center" valign="middle" rowspan="1" colspan="1">86.42</td><td align="center" valign="middle" rowspan="1" colspan="1">53.33</td><td align="center" valign="middle" rowspan="1" colspan="1">78.22</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BinaryFormer [<xref rid="B16-sensors-24-08190" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1/1</td><td align="center" valign="middle" rowspan="1" colspan="1">4.8 M</td><td align="center" valign="middle" rowspan="1" colspan="1">3.19 G</td><td align="center" valign="middle" rowspan="1" colspan="1">87.07</td><td align="center" valign="middle" rowspan="1" colspan="1">99.30</td><td align="center" valign="middle" rowspan="1" colspan="1">64.27</td><td align="center" valign="middle" rowspan="1" colspan="1">86.22</td><td align="center" valign="middle" rowspan="1" colspan="1">54.46</td><td align="center" valign="middle" rowspan="1" colspan="1">80.82</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ACD-BiT (ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1/1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.0 M</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.27 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>89.72</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>99.67</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>65.49</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>88.92</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>58.76</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>81.61</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>bolded numbers signify optimal results.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-24-08190-t003"><object-id pub-id-type="pii">sensors-24-08190-t003_Table 3</object-id><label>Table 3</label><caption><p>Ablation experiments on distributed alignment (Da) and distillation correction (Dc). The evaluation metric is the accuracy of the first category (Top-1 Acc (%)) and the first five categories (Top-5 Acc (%)).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Modules</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">W/A</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">CIFAR10 (%)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">CIFAR100 (%)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">TinyImageNet (%)</th></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Da</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Dc</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Top-5</bold>
</td></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">1/1</td><td align="center" valign="middle" rowspan="1" colspan="1">84.35</td><td align="center" valign="middle" rowspan="1" colspan="1">99.04</td><td align="center" valign="middle" rowspan="1" colspan="1">63.84</td><td align="center" valign="middle" rowspan="1" colspan="1">86.42</td><td align="center" valign="middle" rowspan="1" colspan="1">52.14</td><td align="center" valign="middle" rowspan="1" colspan="1">75.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">1/1</td><td align="center" valign="middle" rowspan="1" colspan="1">87.61</td><td align="center" valign="middle" rowspan="1" colspan="1">99.27</td><td align="center" valign="middle" rowspan="1" colspan="1">64.62</td><td align="center" valign="middle" rowspan="1" colspan="1">87.81</td><td align="center" valign="middle" rowspan="1" colspan="1">54.13</td><td align="center" valign="middle" rowspan="1" colspan="1">76.76</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">1/1</td><td align="center" valign="middle" rowspan="1" colspan="1">86.21</td><td align="center" valign="middle" rowspan="1" colspan="1">99.06</td><td align="center" valign="middle" rowspan="1" colspan="1">64.30</td><td align="center" valign="middle" rowspan="1" colspan="1">87.75</td><td align="center" valign="middle" rowspan="1" colspan="1">52.83</td><td align="center" valign="middle" rowspan="1" colspan="1">76.13</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1/1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>89.72</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>99.67</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>65.49</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>88.92</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>55.24</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>77.57</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>bolded numbers signify optimal results.</p></fn></table-wrap-foot></table-wrap></floats-group></article>