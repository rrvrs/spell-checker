<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006326</article-id><article-id pub-id-type="pmc">PMC11859882</article-id><article-id pub-id-type="doi">10.3390/s25041097</article-id><article-id pub-id-type="publisher-id">sensors-25-01097</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Point-Level Fusion and Channel Attention for 3D Object Detection in Autonomous Driving</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Shen</surname><given-names>Juntao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-01097" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Fang</surname><given-names>Zheng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-25-01097" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Jin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-01097" ref-type="aff">1</xref><xref rid="c1-sensors-25-01097" ref-type="corresp">*</xref></contrib></contrib-group><aff id="af1-sensors-25-01097"><label>1</label>School of Electrical Engineering, Southwest Jiaotong University, Chengdu 611756, China; <email>yunyee@my.swjtu.edu.cn</email></aff><aff id="af2-sensors-25-01097"><label>2</label>Sichuan Institute of Land Science and Technology (Sichuan Center of Satellite Application Technology), Chengdu 610072, China; <email>fangzheng_sclst@163.com</email></aff><author-notes><corresp id="c1-sensors-25-01097"><label>*</label>Correspondence: <email>jhuang@swjtu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1097</elocation-id><history><date date-type="received"><day>18</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>21</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>25</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>As autonomous driving technology progresses, LiDAR-based 3D object detection has emerged as a fundamental element of environmental perception systems. PointPillars transforms point cloud data into a two-dimensional pseudo-image and employs a 2D CNN for efficient and precise detection. Nevertheless, this approach encounters two primary challenges: (1) the sparsity and disorganization of raw point clouds hinder the model&#x02019;s capacity to capture local features, thus impacting detection accuracy; and (2) existing models struggle to detect small objects within complex environments, particularly regarding orientation estimation. To address these issues, we propose two enhancements: (1) point-level fusion of LiDAR point clouds and RGB images, which integrates the semantic information of 2D images with the geometric features of 3D point clouds to improve model performance in intricate scenarios; (2) the incorporation of the Efficient Channel Attention mechanism to concentrate on essential features, particularly for small and sparse objects. Experimental results on the KITTI dataset indicate significant improvements, particularly in small object detection tasks, such as identifying pedestrians and cyclists. The enhanced model also demonstrates substantial gains in the Average Orientation Similarity (AOS) metric. These enhancements enhance the vehicle&#x02019;s ability to track and predict object trajectories in dynamic environments, critical for reliable recognition and decision-making.</p></abstract><kwd-group><kwd>autonomous driving</kwd><kwd>3D object detection</kwd><kwd>multimodal fusion</kwd><kwd>channel attention mechanism</kwd><kwd>ECA</kwd></kwd-group><funding-group><award-group><funding-source>Research on Digital Archives Restoration Technology Based on Deep Neural Networks</funding-source><award-id>2022-X-039</award-id></award-group><award-group><funding-source>National Archives Administration of China</funding-source></award-group><funding-statement>This work was supported by the project &#x0201c;Research on Digital Archives Restoration Technology Based on Deep Neural Networks&#x0201d; (Grant No. 2022-X-039), funded by the National Archives Administration of China.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01097"><title>1. Introduction</title><p>Autonomous driving technology is pivotal in enhancing road safety, efficiency, and convenience [<xref rid="B1-sensors-25-01097" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01097" ref-type="bibr">2</xref>]. Environmental perception systems, fundamental components of autonomous driving, are engineered to perceive and interpret the surrounding environment in real time [<xref rid="B3-sensors-25-01097" ref-type="bibr">3</xref>]. Central to these systems is 3D object detection technology, which facilitates the recognition and localization of objects, such as vehicles, pedestrians, and traffic signs, within a three-dimensional space. This capability is essential for the safe navigation of autonomous vehicles [<xref rid="B4-sensors-25-01097" ref-type="bibr">4</xref>].</p><p>Traditional object detection methods predominantly rely on handcrafted features and rule-based algorithms, such as Haar-like features [<xref rid="B5-sensors-25-01097" ref-type="bibr">5</xref>] and Histogram of Oriented Gradients (HOG) [<xref rid="B6-sensors-25-01097" ref-type="bibr">6</xref>]. While these methods have shown success in controlled environments, their performance is limited in complex and dynamic traffic scenarios. The emergence of deep learning has revolutionized object detection, particularly with the introduction of convolutional neural networks (CNNs) [<xref rid="B7-sensors-25-01097" ref-type="bibr">7</xref>], which automatically learn hierarchical features from large-scale image data. Notable algorithms, including R-CNN [<xref rid="B8-sensors-25-01097" ref-type="bibr">8</xref>], YOLO [<xref rid="B9-sensors-25-01097" ref-type="bibr">9</xref>], and SSD [<xref rid="B10-sensors-25-01097" ref-type="bibr">10</xref>], have significantly advanced 2D object detection. However, these approaches encounter challenges in capturing the 3D spatial information that is critical for autonomous driving applications.</p><p>The integration of LiDAR sensors with monocular cameras has emerged as an effective solution for 3D object detection in autonomous vehicles [<xref rid="B11-sensors-25-01097" ref-type="bibr">11</xref>]. LiDAR delivers precise spatial information through point cloud data, while monocular cameras capture rich color and texture details that LiDAR lacks. The combination of these complementary modalities has the potential to significantly enhance the accuracy and robustness of 3D object detection systems. However, in complex traffic scenarios&#x02014;especially when detecting small or occluded objects&#x02014;effectively fusing these data sources to achieve both real-time performance and high detection accuracy presents a significant challenge.</p><p>In recent years, several deep learning-based methods have been proposed to tackle the 3D object detection problem. PointNet [<xref rid="B12-sensors-25-01097" ref-type="bibr">12</xref>] and PointPillars [<xref rid="B13-sensors-25-01097" ref-type="bibr">13</xref>] are two prominent approaches that process point clouds directly. While these methods demonstrate promising results, they primarily concentrate on local or global feature extraction without fully leveraging attention mechanisms to dynamically emphasize relevant features during detection. Furthermore, existing algorithms often prioritize object detection accuracy at the expense of precision in orientation prediction.</p><p>There remain unresolved questions concerning the optimal strategies for incorporating attention mechanisms into 3D object detection frameworks, particularly within the context of multimodal data fusion [<xref rid="B14-sensors-25-01097" ref-type="bibr">14</xref>]. Some researchers advocate for early fusion strategies, while others favor late fusion [<xref rid="B15-sensors-25-01097" ref-type="bibr">15</xref>], highlighting the necessity for further exploration to optimize multimodal fusion techniques for 3D object detection.</p><p>This study introduces a novel multimodal 3D object detection framework that integrates a channel attention mechanism [<xref rid="B16-sensors-25-01097" ref-type="bibr">16</xref>] with the PointPillars architecture. By combining the PointPainting [<xref rid="B17-sensors-25-01097" ref-type="bibr">17</xref>] algorithm for multimodal data fusion with channel attention, we aim to enhance small object detection and improve the accuracy of orientation prediction. The primary contributions of this work are as follows:</p><p>1. We effectively leverage the complementary strengths of LiDAR and monocular image data as inputs to the multimodal 3D object detection model.</p><p>2. We incorporate a channel attention module [<xref rid="B16-sensors-25-01097" ref-type="bibr">16</xref>] into the PointPillars network to dynamically emphasize relevant features, thereby enhancing feature extraction at both local and global scales.</p><p>3. Comprehensive experiments on the KITTI dataset [<xref rid="B18-sensors-25-01097" ref-type="bibr">18</xref>] demonstrate that the proposed model surpasses existing methods in detecting small and occluded objects, resulting in significant improvements in orientation prediction accuracy</p><p>In conclusion, this study enhances vehicle perception through multimodal data fusion and attention-based feature selection, thereby contributing to the advancement of 3D object detection for autonomous driving. These improvements not only increase detection accuracy but also establish a more reliable foundation for real-time decision-making in autonomous systems.</p></sec><sec id="sec2-sensors-25-01097"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-01097"><title>2.1. 3D Object Detection Methods</title><p>Existing 3D object detection models can be broadly classified into two categories: single-stage [<xref rid="B19-sensors-25-01097" ref-type="bibr">19</xref>] and two-stage frameworks [<xref rid="B20-sensors-25-01097" ref-type="bibr">20</xref>]. Single-stage networks directly predict the final 3D bounding boxes, whereas two-stage networks first generate candidate bounding boxes and subsequently refine them to produce the final 3D boxes. In terms of data processing approaches, contemporary 3D object detection algorithms are typically categorized into those based on raw point clouds [<xref rid="B21-sensors-25-01097" ref-type="bibr">21</xref>], voxel- or pillar-based methods [<xref rid="B22-sensors-25-01097" ref-type="bibr">22</xref>], point cloud and voxel fusion [<xref rid="B23-sensors-25-01097" ref-type="bibr">23</xref>], and multi-view or multimodal approaches [<xref rid="B24-sensors-25-01097" ref-type="bibr">24</xref>].</p><sec id="sec2dot1dot1-sensors-25-01097"><title>2.1.1. Raw Point Clouds Methods</title><p>The raw point cloud-based method entails the direct utilization of point cloud data obtained from LiDAR for three-dimensional object detection and recognition, without converting it into alternative representations such as voxels or projections. This approach is advantageous due to its efficiency and straightforwardness. However, raw LiDAR point cloud data is inherently sparse and unordered, rendering it unsuitable for direct processing by neural networks. To address this challenge, Qi et al. introduced the innovative PointNet [<xref rid="B12-sensors-25-01097" ref-type="bibr">12</xref>] algorithm for object detection in 2017. PointNet extracts features from raw point clouds through the application of T-Net transformation matrices and fully connected layers (multi-layer perceptron, MLP), followed by a max-pooling operation to derive global features. While PointNet effectively manages point cloud disorder, its focus on global feature extraction restricts its ability to capture local details, and the max-pooling operation may lead to the loss of valuable information. To mitigate these limitations, Qi et al. proposed PointNet++ [<xref rid="B25-sensors-25-01097" ref-type="bibr">25</xref>], which employs a hierarchical approach.</p></sec><sec id="sec2dot1dot2-sensors-25-01097"><title>2.1.2. Voxel-Based Methods</title><p>Voxel-based methods partition space into uniform cubes (voxels) and allocate each point in the scene to a specific voxel. By extracting and learning features within each voxel, this approach streamlines the processing of point clouds. The voxelization technique was initially introduced by VoxelNet [<xref rid="B26-sensors-25-01097" ref-type="bibr">26</xref>], which segments point cloud data into uniformly spaced voxels and utilizes a Voxel Feature Encoding (VFE) layer to transform the points within each voxel into feature vectors. This architecture enables the network to stack multiple VFE layers for the learning of complex feature representations. However, due to its reliance on computationally intensive 3D convolution operations, detection speed can be relatively slow. The SECOND [<xref rid="B27-sensors-25-01097" ref-type="bibr">27</xref>] network mitigated this issue by integrating sparse convolutions as a replacement for traditional 3D convolutions.</p></sec><sec id="sec2dot1dot3-sensors-25-01097"><title>2.1.3. Image-Point Cloud Fusion</title><p>RGB-based 2D detection and 3D object detection utilizing point clouds each present distinct advantages while also facing specific limitations. Methods relying solely on images lack spatial depth information and are sensitive to variations in lighting, weather, and viewpoint. In contrast, approaches that depend exclusively on 3D point clouds encounter challenges such as low detection accuracy for small objects, high computational complexity due to large data volumes, and substantial costs associated with LiDAR and depth cameras. Therefore, integrating image and point cloud data for multimodal object detection is crucial for autonomous driving.</p><p>In 2017, Chen et al. proposed the MV3D [<xref rid="B24-sensors-25-01097" ref-type="bibr">24</xref>] object detection network, which combines 2D image features, bird&#x02019;s-eye view (BEV), and front-facing view data within the point cloud scene. In 2020, Sourabh Vora introduced the PointPainting [<xref rid="B17-sensors-25-01097" ref-type="bibr">17</xref>] network, which employs a sequential fusion approach for multimodal object detection and has been widely adopted. In 2021, the 3DVG&#x02013;Transformer [<xref rid="B28-sensors-25-01097" ref-type="bibr">28</xref>] network, based on the Transformer architecture, utilized vision guidance to integrate multimodal data in a global context, effectively enhancing detection accuracy. In 2023, the Multi-Modal Fusion [<xref rid="B29-sensors-25-01097" ref-type="bibr">29</xref>] algorithm processed multimodal data from various sensors during feature extraction through a deep learning model, integrating the multimodal information using a feature fusion layer.</p></sec></sec><sec id="sec2dot2-sensors-25-01097"><title>2.2. PointPillars</title><p>The PointPillars [<xref rid="B13-sensors-25-01097" ref-type="bibr">13</xref>] algorithm efficiently conducts 3D object detection by converting point cloud data into bird&#x02019;s-eye view (BEV) style 2D pseudo-images. It comprises three main components: the Pillar Feature Net (PFN) layer, which transforms point cloud data into pseudo-images; the Backbone network (a 2D CNN), which extracts spatial and semantic information among pillars; and the SSD [<xref rid="B10-sensors-25-01097" ref-type="bibr">10</xref>] detection head, responsible for object classification and bounding box regression. By employing 2D convolutions instead of 3D convolutions, PointPillars [<xref rid="B13-sensors-25-01097" ref-type="bibr">13</xref>] enhances computational efficiency and alleviates the limitations associated with fixed voxel sizes, particularly excelling in the detection of small objects, such as pedestrians and cyclists. However, the conversion of point clouds into pillars results in the loss of height dimension information, which may impair performance in complex scenes and reduce the accuracy of object orientation angle predictions, particularly when compared to algorithms that depend on fine-grained 3D convolutions. The architecture of PointPillars is shown in <xref rid="sensors-25-01097-f001" ref-type="fig">Figure 1</xref>.</p></sec></sec><sec id="sec3-sensors-25-01097"><title>3. Approach</title><p>In this section, we provide a detailed description of the proposed method. First, we present an overview of the 3D object detection architecture, followed by an explanation of the multimodal data fusion strategy. Finally, we discuss the incorporation of attention mechanisms along the channel dimension.</p><sec id="sec3dot1-sensors-25-01097"><title>3.1. Overall Architecture</title><p>The overall structure of the proposed model is depicted in the accompanying <xref rid="sensors-25-01097-f002" ref-type="fig">Figure 2</xref>. Traditional 3D object detection models typically utilize LiDAR&#x02019;s 3D point clouds as input. In contrast, we adopt a point-level fusion strategy known as PointPainting [<xref rid="B17-sensors-25-01097" ref-type="bibr">17</xref>], which employs a 2D semantic segmentation network to extract the target object&#x02019;s pixels from the RGB image. These pixels are subsequently projected onto the LiDAR point cloud to create enhanced point cloud data. The colored points are then input into the PointPillars [<xref rid="B13-sensors-25-01097" ref-type="bibr">13</xref>] network to execute the 3D object detection task. Additionally, we integrate the Efficient Channel Attention (ECA) [<xref rid="B16-sensors-25-01097" ref-type="bibr">16</xref>] net to compute local mutual relationships between channels in the Pillar Feature Net layer and the 2D CNN. The resulting data is applied to the network&#x02019;s feature map, thereby augmenting the model&#x02019;s detection capabilities.</p></sec><sec id="sec3dot2-sensors-25-01097"><title>3.2. Multi-Modal Data Fusion Strategy</title><p>The innovation of the PointPainting [<xref rid="B17-sensors-25-01097" ref-type="bibr">17</xref>] algorithm lies in its capacity to effectively integrate image semantic information with point cloud spatial data, thereby providing more accurate and robust input for subsequent object detection and recognition tasks.</p><p>We adopt PointPainting [<xref rid="B17-sensors-25-01097" ref-type="bibr">17</xref>] as the multimodal data fusion strategy, which utilizes both point clouds and images as inputs to estimate oriented 3D bounding boxes. The algorithm comprises three key stages: (1) Semantic segmentation: Compute pixel-wise segmentation scores using the image semantic segmentation network. (2) Fusion: Project the segmentation scores onto the LiDAR point cloud to effectively color it. (3) 3D object detection: Perform object detection using a LiDAR-based 3D detection network. PointPainting [<xref rid="B17-sensors-25-01097" ref-type="bibr">17</xref>] integrates semantic information into the point cloud by projecting the point cloud onto the image plane, extracting corresponding image features, and subsequently &#x0201c;painting&#x0201d; these features back onto the point cloud data, resulting in an enhanced point cloud.</p><p>We utilize the KITTI dataset [<xref rid="B18-sensors-25-01097" ref-type="bibr">18</xref>] for multimodal data fusion. Each LiDAR point in the KITTI dataset comprises four dimensions: (x, y, z, r), where (x, y, z) represent the 3D spatial coordinates and r denotes the reflection intensity. The semantic segmentation network outputs C class scores (s<sub>0</sub>, s<sub>1</sub>, s<sub>2</sub>, &#x02026;, s<sub>n</sub>), where C = 4 (car, pedestrian, cyclist, background). Once the LiDAR points are projected onto the image plane, the semantic scores of the corresponding pixels (x_img, y_img) are appended to the LiDAR points, resulting in augmented LiDAR points in the form (x, y, z, r, s<sub>0</sub>, s<sub>1</sub>, s<sub>2</sub>, s<sub>3</sub>).</p></sec><sec id="sec3dot3-sensors-25-01097"><title>3.3. Channel Attention Module</title><p>Convolutional neural networks (CNNs) typically assign equal weights to all features during feature extraction, which can result in the inclusion of many irrelevant features and an increase in computational cost. To address this issue and enhance the extraction of relevant features, we optimize the network structure by integrating attention mechanisms, thereby improving the algorithm&#x02019;s performance in detecting small targets. The backbone of the PointPillars [<xref rid="B13-sensors-25-01097" ref-type="bibr">13</xref>] algorithm employs a CNN for feature extraction. We incorporate the Efficient Channel Attention (ECA) mechanism [<xref rid="B16-sensors-25-01097" ref-type="bibr">16</xref>] into the convolutional layers of the backbone, introducing attention computation along the channel dimension. This modification effectively directs the model&#x02019;s focus toward critical features, enhancing detection accuracy and overall performance. Its architecture is shown in <xref rid="sensors-25-01097-f003" ref-type="fig">Figure 3</xref>.</p><p>The principle of the ECA mechanism [<xref rid="B16-sensors-25-01097" ref-type="bibr">16</xref>] in CNN involves the use of a dimension-preserving global average pooling (GAP) to aggregate convolutional features, followed by an adaptive determination of kernel size k, one-dimensional convolution, and learning channel attention via the Sigmoid function to enhance the model&#x02019;s performance in feature processing. The two core steps of ECA are global average pooling and one-dimensional convolution. The ECA module first applies global average pooling to the feature map of each channel, compressing the 2D feature map of each channel into a C &#x000d7; H &#x000d7; W format:<disp-formula id="FD1-sensors-25-01097"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Assume that the input feature map <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> has dimensions <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the number of channels, and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denote the spatial resolution of the feature map. Each channel encodes specific information, such as edge features, color, and depth. The features of small objects are often concentrated in only a few channels, while other channels may contain background or irrelevant information. Global average pooling performs spatial compression on each channel, reducing the <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> feature to a single scalar value, yielding the global feature description <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for each channel. Among them, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the average value of channel <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and finally a channel vector <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is obtained.</p><p>One-dimensional convolution refers to the convolution operation performed along a single dimension. Its primary objective is to capture local interactions between channels, specifically the interdependencies among different channels. Let <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> denote the input channel feature vector, and the one-dimensional convolution operation can be expressed as follows:<disp-formula id="FD2-sensors-25-01097"><label>(2)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mn>1</mml:mn><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The adaptive convolution kernel dynamically adjusts the kernel size, enabling each convolution operation to capture cross-channel interactions based on the number of channels (i.e., the dimensionality of the input features). This can be expressed by the following equation:<disp-formula id="FD3-sensors-25-01097"><label>(3)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where</p><p><inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>: The output channel weight.</p><p><inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c8;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>: The function of the ECA module used to calculate the channel weights.</p><p><inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>: The logarithm to the base 2, used to transform the number of channels into a logarithmic scale, which helps maintain smooth changes in weights when the number of channels varies.</p><p><inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>: A scaling factor that adjusts the weight of the logarithmic scale.</p><p><inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>: A bias term that adjusts the offset of the weights.</p><p><inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>: Represents the operation of taking the odd part, which is a key feature of the ECA module. This ensures that the output weights are odd numbers. This is important because these weights are used to calculate the weights of each channel in subsequent processing. Odd weights can ensure a more uniform distribution of weights after the subsequent Sigmoid activation function, thereby improving the model&#x02019;s expressive power.</p><p>Activate the convolution output weights <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> using the Sigmoid function, where <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the Sigmoid function. The Sigmoid function effectively squashes the output weights to a range between 0 and 1.<disp-formula id="FD4-sensors-25-01097"><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, the feature of each channel is multiplied by its corresponding weight, thereby adjusting the importance of each channel.<disp-formula id="FD5-sensors-25-01097"><label>(4)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x02032;</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec4-sensors-25-01097"><title>4. Experimental Setup</title><p>In this section, we will provide a detailed description of the model framework we designed and the dataset details used.</p><sec id="sec4dot1-sensors-25-01097"><title>4.1. KITTI and Evaluation Metrics</title><p>We used the KITTI dataset [<xref rid="B18-sensors-25-01097" ref-type="bibr">18</xref>]. The KITTI official annotations categorize the dataset into three object classes: cars, pedestrians, and cyclists. It is also evaluated based on four detection metrics: BBOX, BEV, 3D, and AOS. BBOX refers to the accuracy of 2D bounding boxes; BEV indicates the accuracy of bounding boxes in the bird&#x02019;s-eye view; 3D refers to the accuracy of 3D bounding boxes; and AOS measures the accuracy of the object&#x02019;s rotation angle, which evaluates how accurately the system can determine the rotational direction of an object relative to a reference point. The difficulty of detection in the KITTI dataset is categorized based on factors such as whether the ground truth bounding box is occluded, the extent of occlusion, and the height of the bounding box. The specific categorization is shown in <xref rid="sensors-25-01097-t001" ref-type="table">Table 1</xref>.</p></sec><sec id="sec4dot2-sensors-25-01097"><title>4.2. Implementation Details</title><p><bold>Architecture:</bold> To enhance the accuracy of multimodal fusion between point clouds and images in the dataset, this paper employs the PointPainting [<xref rid="B17-sensors-25-01097" ref-type="bibr">17</xref>] method. This approach maps semantic information from RGB images onto the point cloud, resulting in a &#x0201c;painted&#x0201d; point cloud that integrates semantic data. The painted point cloud is subsequently fed into the PointPillars [<xref rid="B13-sensors-25-01097" ref-type="bibr">13</xref>] model for 3D object detection. Additionally, to improve the model&#x02019;s feature extraction capabilities, a channel attention mechanism is introduced into the 2D CNN of PointPillars [<xref rid="B13-sensors-25-01097" ref-type="bibr">13</xref>]. This mechanism adjusts the weights of different channels to enhance the model&#x02019;s ability to capture essential information. Experimental results demonstrate that the PointPillars [<xref rid="B13-sensors-25-01097" ref-type="bibr">13</xref>] model, augmented with channel attention, significantly outperforms competing models in detection accuracy, especially in the case of small object detection, on the KITTI dataset.</p><p><bold>Parameter details:</bold> The KITTI dataset [<xref rid="B18-sensors-25-01097" ref-type="bibr">18</xref>] contains 7481 training samples and 7518 test samples. According to [<xref rid="B24-sensors-25-01097" ref-type="bibr">24</xref>], we split the 7481 training samples into 3712 for the training set and 3769 for the validation set. The voxelization scene range is (0,69.12) &#x000d7; (&#x02212;39.68,39.68) &#x000d7; (&#x02212;3,1), with voxel size (0.16,0.16,4). Each voxel can contain a maximum of 32-point clouds, with a maximum of 16,000 voxels processed during training and 40,000 during testing. The initial learning rate was 0.003. The learning rate increases during the first 40% of the training phase and gradually decreases during the remaining 60%. The classification loss weight was 1.0, the localization loss weight was 2.0, the direction loss weight was 0.2, and the box encoding weight was 1.0. The NMS threshold was set to 0.01, with a maximum of 500 boxes retained. The gradient clipping limit was set to 10 to prevent gradient explosion. The training set was trained for 80 epochs with a batch size of 4. The validation set was evaluated using the AP10 metric, with an IoU threshold of 0.7 for the Car class and 0.5 for both Pedestrian and Cyclist classes.</p><p><bold>Experimental Setup:</bold> As shown in <xref rid="sensors-25-01097-t002" ref-type="table">Table 2</xref>, we used the PointPillars [<xref rid="B13-sensors-25-01097" ref-type="bibr">13</xref>] implementation from the open-source 3D object detection library OpenPCDet as the foundation for the program. The object detection network model was trained on a GPU, specifically an NVIDIA GeForce RTX 3090 with 32 GB of memory and 640 Tensor cores. The GPU uses CUDA version 11.7 The experiment&#x02019;s CPU was an 11th Gen Intel<sup>&#x000ae;</sup> Core&#x02122; i9-11900K @ 3.50 GHz with 16 cores, and the system had 128 GB of memory. The operating system used was Ubuntu 18.04.6, the deep learning environment was Anaconda 3, the framework was PyTorch 1.8.1, and the programming language was Python 3.9.12.</p></sec></sec><sec sec-type="results" id="sec5-sensors-25-01097"><title>5. Results</title><p>We compared the PointPillars [<xref rid="B13-sensors-25-01097" ref-type="bibr">13</xref>] algorithm, which integrates multimodal data, with attention modules of different types: spatial attention using the Squeeze-and-Excitation (SE) module [<xref rid="B30-sensors-25-01097" ref-type="bibr">30</xref>], spatial + channel attention using the Convolutional Block Attention Module (CBAM) [<xref rid="B31-sensors-25-01097" ref-type="bibr">31</xref>], and channel attention using our custom-developed Efficient Channel Attention (ECA) algorithm [<xref rid="B16-sensors-25-01097" ref-type="bibr">16</xref>]. The overall performance of the method is represented using mean Average Precision (mAP) at three different difficulty levels: easy, moderate, and hard. This comparison allows us to assess the impact of each attention mechanism on the model&#x02019;s ability to improve 3D object detection accuracy under varying conditions.</p><p>For easy, moderate, and difficult samples, the IoU threshold for the Car class is set to 0.7 for all levels, while the IoU threshold for other classes is 0.5. The detection categories are Car, Pedestrian, and Cyclist. We used the AP40 and AP10 evaluation metrics, which primarily highlight the advantages of our model in terms of the 3D and AOS metrics. We also tried different fusion methods such as a newer early fusion strategy the Multi-Modal Fusion [<xref rid="B29-sensors-25-01097" ref-type="bibr">29</xref>] and a classic late fusion strategy MV3D (Multi-View 3D) [<xref rid="B24-sensors-25-01097" ref-type="bibr">24</xref>], and show the comparison of AOS Metrics AP10 on the KITTI dataset.</p><p>As shown in the data from <xref rid="sensors-25-01097-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-01097-t004" ref-type="table">Table 4</xref>:</p><p><bold>Pedestrian Category:</bold> Under the AP40 metric, our model outperforms all others in every scenario, achieving scores of 59.67%, 54.67%, and 50.03%, respectively. Similarly, under the AP10 metric, our model also surpasses all others in all scenarios, attaining scores of 59.69%, 55.94%, and 51.68%, respectively.</p><p><bold>Cyclist Category:</bold> Under the AP40 metric, our model outperforms all others in every scenario, achieving scores of 82.49%, 61.43%, and 57.38%, respectively. Similarly, under the AP10 metric, our model also demonstrates superior performance in all scenarios, with results of 80.70%, 61.79%, and 58.14%, respectively.</p><p>As shown in the data from <xref rid="sensors-25-01097-t005" ref-type="table">Table 5</xref> and <xref rid="sensors-25-01097-t006" ref-type="table">Table 6</xref>:</p><p><bold>Pedestrian Category:</bold> Under the AP40 metric, our model outperforms all others in every scenario, achieving scores of 55.55%, 52.87%, and 50.32%, respectively. Similarly, under the AP10 metric, our model also surpasses all others in all scenarios, attaining scores of 54.95%, 52.69%, and 50.43%, respectively.</p><p><bold>Cyclist Category:</bold> Under the AP40 metric, our model outperforms all others in every scenario, achieving scores of 90.92%, 73.70%, and 69.59%, respectively. Similarly, under the AP10 metric, our model also demonstrates superior performance in all scenarios, with results of 88.27%, 72.77%, and 68.93%, respectively.</p><p>The results are shown in the <xref rid="sensors-25-01097-t007" ref-type="table">Table 7</xref>. We observed a significant decrease in prediction accuracy for Cyclist and Pedestrian, with no notable accuracy advantage for Car. Additionally, its higher computational complexity resulted in training times far exceeding those of our model.</p><sec id="sec5dot1-sensors-25-01097"><title>5.1. Performance Analysis</title><p>For the Car category, the performance differences among all models in both AP10 and AP40 evaluations are minimal. This phenomenon can be attributed to the prominent geometric features of cars, which existing methods can effectively capture. Our model primarily enhances small object detection, resulting in a limited impact on the detection accuracy of large objects, such as cars. Conversely, for pedestrian and cyclist detection, the ECA attention mechanism exhibits significant effectiveness, particularly in the AOS metric, for the following reasons.</p><p><bold>Optimized Algorithm Combination:</bold> PointPainting performs early fusion by integrating image features with LiDAR point cloud features prior to their input into the network. This mapping of semantic information from images to each point in the LiDAR point cloud enhances the representation of point cloud features. By supplementing geometric information from LiDAR with semantic information provided by images, it significantly improves the accuracy of 3D object detection, particularly in scenarios where point clouds are sparse or heavily occluded. Our primary network, PointPillars, is an efficient object detection method that segments LiDAR point cloud data into pillars and utilizes convolutional neural networks (CNNs) for feature extraction. Compared to traditional voxel methods, the pillar method retains spatial structure while significantly reducing computational overhead. Thanks to its lightweight pillar structure and convolutional network, PointPillars effectively reduces computation while achieving point-level sequential fusion of point clouds and images through PointPainting, all without increasing computational burden, thereby significantly enhancing detection accuracy.</p><p>Early fusion enables simultaneous consideration of image and point cloud information during feature extraction, which reduces the complexity of computation and information transfer typically associated with late fusion. This approach is well-suited for real-time autonomous driving applications, particularly for vehicles with limited hardware resources. The methods maintain strong performance even in embedded systems or resource-constrained environments.</p><p>Moreover, we contend that the 3D object detection task in autonomous driving inherently contains substantial information redundancy and noise. Thus, an effective mechanism is required to assist the model in selectively fusing information from different modalities (images and point clouds). The advantage of the Efficient Channel Attention (ECA) mechanism lies in its ability to dynamically adjust channel weights, allowing for improved focus on features that contain more informative content, particularly in complex scenarios and sparse point cloud conditions.</p><p><bold>Efficient Attention Mechanism</bold>: Small object detection typically encounters challenges related to sparse features and background interference. In such cases, attention mechanisms enhance the model&#x02019;s feature selectivity, enabling it to automatically focus on the most important aspects of the input data. ECA adapts the convolution kernel size to capture inter-channel dependencies, resulting in a more uniform distribution of attention weights. This adaptation promotes a balanced consideration of the importance of different channels. Such a uniform distribution aids the model in avoiding the neglect of important channels while preventing excessive focus on certain channels, consequently enhancing the recognition accuracy of small targets. In contrast, the attention weight distributions of SE and CBAM are comparatively uneven, with some channels receiving disproportionately high weights while others retain relatively low weights. Furthermore, ECA is computationally efficient, yielding performance improvements for the model without significantly increasing the computational burden. The comparison results are shown in <xref rid="sensors-25-01097-f004" ref-type="fig">Figure 4</xref>.</p></sec><sec id="sec5dot2-sensors-25-01097"><title>5.2. Ablation Study</title><p>In this section, we conduct a comprehensive ablation study to validate the effectiveness of the proposed enhancements in our method. Painted PointPillars [<xref rid="B13-sensors-25-01097" ref-type="bibr">13</xref>] served as our baseline model, being a widely adopted single-frame 3D object detection approach. Our ablation studies utilized four distinct evaluation metrics to assess the model&#x02019;s performance from various perspectives: bounding box (bbox), bird&#x02019;s-eye view (BEV), 3D, and Average Orientation Similarity (AOS). The model was trained for 80 epochs, with automated evaluation on the validation set using the AP40 metric. The Intersection-over-Union (IoU) thresholds for pedestrians and cyclists were both set to 0.5. These metrics offer a holistic evaluation of the model&#x02019;s ability to accurately localize objects, maintain spatial consistency, and address orientation challenges, particularly in the context of small object detection and orientation accuracy.</p><p>As shown in the data from the <xref rid="sensors-25-01097-t008" ref-type="table">Table 8</xref>, the improved model (Ours) significantly outperformed the baseline model (Painted PointPillars) in both pedestrian and cyclist detection tasks.</p><p><bold>Pedestrian:</bold> The bbox metric shows improvements of +5.41, +5.55, and +5.41 for Easy, Moderate, and Hard difficulty levels, respectively, indicating a significant enhancement in boundary box localization. The largest improvement is seen in AOS, with increases of +9.55, +9.53, and +9.17 for Easy, Moderate, and Hard difficulty levels, respectively, demonstrating significant improvements in pedestrian direction prediction.</p><p><bold>Cyclist:</bold> The bbox metric shows improvements of +3.89, +4.35, and +3.90 for Easy, Moderate, and Hard difficulty levels, respectively, also demonstrating significant progress. The improvement in mAPaos is equally notable, with increases of +6.00, +7.35, and +6.73 for Easy, Moderate, and Hard difficulty levels, respectively, indicating substantial improvements in cyclist direction prediction.</p></sec><sec id="sec5dot3-sensors-25-01097"><title>5.3. Qualitative Analysis</title><p>As illustrated in <xref rid="sensors-25-01097-f005" ref-type="fig">Figure 5</xref>, we selected three distinct scenarios for qualitative analysis and comparison. In <xref rid="sensors-25-01097-f005" ref-type="fig">Figure 5</xref>a, the original PointPillars model exhibits suboptimal detection performance for overlapping and occluded targets, failing to detect the occluded white vehicle on the left. In contrast, our enhanced model demonstrates a significant improvement in performance. <xref rid="sensors-25-01097-f005" ref-type="fig">Figure 5</xref>b shows that the original model fails to recognize the small target on the left; however, our model successfully detects and frames the pedestrian with a blue bounding box. Lastly, <xref rid="sensors-25-01097-f005" ref-type="fig">Figure 5</xref>c highlights the substantial improvement achieved by our model in detecting scenes where the target and environment exhibit low distinguishability.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-01097"><title>6. Conclusions and Future Work</title><sec id="sec6dot1-sensors-25-01097"><title>6.1. Conclusions</title><p>In summary, the development of object detection technology for autonomous driving has reached a mature stage. Our team has identified that factors such as computational efficiency, cost, and the accuracy of small target detection play crucial roles in enhancing autonomous driving capabilities. While methods like PointPillars and PointPainting have found applications in multimodal data fusion, research on the integration of efficient attention mechanisms remains relatively sparse. We have innovatively introduced the Efficient Channel Attention (ECA) mechanism, a lightweight approach that minimizes redundant computations by optimizing the weighting of channels.</p><p>While other methods may improve performance by increasing model depth or complexity, we have prioritized low-cost and high-efficiency optimization strategies, emphasizing the practical benefits of our research and significantly enhancing the model&#x02019;s generalization ability. Our focus is not solely on achieving the highest performance but rather on finding a balance between performance and computational efficiency. The ECA mechanism adapts weights dynamically, avoiding excessive computation on non-essential features, thus enabling the model to maintain low computational overhead while ensuring high precision, making it particularly suitable for the real-time demands of autonomous driving scenarios.</p><p>Although PointPillars, PointPainting, and ECA are established methods, we have compared various types of attention mechanisms and explored a range of multimodal fusion strategies through extensive experimental and theoretical validation. Ultimately, we have found this combination to be effective. Our research not only addresses real-time requirements, computational efficiency, and cost-effectiveness in autonomous driving but also enhances the detection accuracy of small targets. This makes our study practically applicable across a wide array of real-world scenarios.</p><p>Experimental results on the KITTI dataset [<xref rid="B18-sensors-25-01097" ref-type="bibr">18</xref>] demonstrate that our method achieves significant improvements in small object detection compared to the baseline, particularly excelling in AOS and 3D AP scores across all difficulty levels. The findings validate that the combination of multimodal fusion and attention mechanisms effectively enhances detection performance, especially for small dynamic objects.</p></sec><sec id="sec6dot2-sensors-25-01097"><title>6.2. Future Work</title><p>Innovation in multimodal fusion technology:<list list-type="order"><list-item><p>Dynamic adaptive fusion strategy: In the future, we can explore dynamic fusion strategies to dynamically adjust the fusion weights of image and point cloud features according to the complexity of the input scene (such as point cloud density or lighting conditions), thereby improving fusion efficiency and detection performance.</p></list-item><list-item><p>End-to-end multimodal fusion: The current method uses an independent preprocessing step of PointPainting. In the future, we can try an end-to-end multimodal learning framework, such as directly jointly modeling images and point clouds through a shared feature extractor to reduce the time overhead of data preprocessing.</p></list-item><list-item><p>Robust fusion method: For the alignment error between point clouds and images in dynamic scenes, learning-based alignment methods can be introduced in the future, such as using image depth estimation or time series-based point cloud prediction methods to improve the robustness of inter-modal alignment.</p></list-item></list></p><p>Improvement of attention mechanism:<list list-type="order"><list-item><p>Joint attention mechanism: Based on ECA, explore the spatial channel joint attention mechanism and design attention mechanisms that can process different modalities simultaneously, such as the Transformer-based cross-modal attention mechanism, which directly captures the global interaction information between images and point clouds. This enables the model to focus on both spatial and channel information in the fused features, especially improving performance in the case of complex backgrounds or small target occlusions.</p></list-item><list-item><p>Lightweight global attention mechanism: In the future, a lightweight global attention mechanism optimized for 3D point clouds can be designed by combining the multi-level attention mechanism, so that the model can capture the global and local relationships between point clouds and image features without significantly increasing the computational cost, and further optimize the recognition and positioning capabilities of small targets.</p></list-item></list></p></sec></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, J.S.; methodology, J.S.; software, J.S.; validation, J.S.; formal analysis, J.S.; investigation, J.S.; resources, J.S.; data curation, J.S.; writing&#x02014;original draft preparation, J.S.; writing&#x02014;review and editing, J.S.; visualization, J.S.; supervision, J.H.; project administration, J.S. and J.S.; funding acquisition, J.H and Z.F. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>KITTI is a project of Karlsruhe Institute of Technology and Toyota Technological Institute at Chicago. It is available at <uri xlink:href="https://www.cvlibs.net/datasets/kitti/">https://www.cvlibs.net/datasets/kitti/</uri> (accessed on 13 May 2024) with permission.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01097"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qian</surname><given-names>R.</given-names></name>
<name><surname>Lai</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
</person-group><article-title>3D Object Detection for Autonomous Driving: A Survey</article-title><source>Pattern Recognit.</source><year>2021</year><volume>130</volume><fpage>108796</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2022.108796</pub-id></element-citation></ref><ref id="B2-sensors-25-01097"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ghasemieh</surname><given-names>A.</given-names></name>
<name><surname>Kashef</surname><given-names>R.</given-names></name>
</person-group><article-title>3D Object Detection for Autonomous Driving: Methods, Models, Sensors, Data, and Challenges</article-title><source>Transp. Eng.</source><year>2022</year><volume>8</volume><fpage>100115</fpage><pub-id pub-id-type="doi">10.1016/j.treng.2022.100115</pub-id></element-citation></ref><ref id="B3-sensors-25-01097"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>C.</given-names></name>
<name><surname>Tian</surname><given-names>D.</given-names></name>
<name><surname>Duan</surname><given-names>X.</given-names></name>
<name><surname>Zhou</surname><given-names>J.</given-names></name>
</person-group><article-title>3D Environmental Perception Modeling in the Simulated Autonomous-Driving Systems</article-title><source>Complex Syst. Model. Simul.</source><year>2021</year><volume>1</volume><fpage>45</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.23919/CSMS.2021.0004</pub-id></element-citation></ref><ref id="B4-sensors-25-01097"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Yang</surname><given-names>S.</given-names></name>
<name><surname>Ye</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>T.</given-names></name>
<name><surname>Du</surname><given-names>Z.</given-names></name>
</person-group><article-title>Research on Environmental 3D Object Detection Technology Fusing LIDAR Dot-Cloud and Vision Image</article-title><source>Proceedings of the 2024 4th International Conference on Intelligent Communications and Computing (ICICC)</source><conf-loc>Zhengzhou, China</conf-loc><conf-date>18&#x02013;20 October 2024</conf-date><fpage>84</fpage><lpage>87</lpage></element-citation></ref><ref id="B5-sensors-25-01097"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lienhart</surname><given-names>R.</given-names></name>
<name><surname>Maydt</surname><given-names>J.</given-names></name>
</person-group><article-title>An extended set of Haar-like features for rapid object detection</article-title><source>Proceedings of the International Conference on Image Processing 2002</source><conf-loc>Bordeaux, France</conf-loc><conf-date>16&#x02013;19 October 2022</conf-date><volume>Volume 1</volume><fpage>I</fpage></element-citation></ref><ref id="B6-sensors-25-01097"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>K.L.</given-names></name>
<name><surname>Mokji</surname><given-names>M.M.</given-names></name>
</person-group><article-title>Automatic target detection in GPR images using Histogram of Oriented Gradients (HOG)</article-title><source>Proceedings of the 2014 2nd International Conference on Electronic Design (ICED)</source><conf-loc>Penang, Malaysia</conf-loc><conf-date>19&#x02013;21 August 2014</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2014</year><fpage>181</fpage><lpage>186</lpage></element-citation></ref><ref id="B7-sensors-25-01097"><label>7.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
</person-group><article-title>Convolutional Neural Network</article-title><source>Intelligent Information Processing with Matlab</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2023</year><fpage>39</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1007/978-981-99-6449-9_2</pub-id></element-citation></ref><ref id="B8-sensors-25-01097"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Raja</surname><given-names>K.</given-names></name>
<name><surname>Pandey</surname><given-names>S.</given-names></name>
<name><surname>Verma</surname><given-names>S.R.</given-names></name>
<name><surname>Sharma</surname><given-names>S.</given-names></name>
<name><surname>Senthilselvi</surname><given-names>A.</given-names></name>
</person-group><article-title>An Intelligent Approach to Object Detection Using R-CNN</article-title><source>Proceedings of the 2024 Second International Conference on Advances in Information Technology (ICAIT)</source><conf-loc>Chikkamagaluru, India</conf-loc><conf-date>24&#x02013;27 July 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><volume>Volume 1</volume><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/icait61638.2024.10690590</pub-id></element-citation></ref><ref id="B9-sensors-25-01097"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liang</surname><given-names>J.</given-names></name>
</person-group><article-title>A review of the development of YOLO object detection algorithm</article-title><source>Appl. Comput. Eng.</source><year>2024</year><volume>71</volume><fpage>39</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.54254/2755-2721/71/20241642</pub-id></element-citation></ref><ref id="B10-sensors-25-01097"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kumar</surname><given-names>V.</given-names></name>
<name><surname>Goel</surname><given-names>V.</given-names></name>
<name><surname>Amoriya</surname><given-names>A.</given-names></name>
<name><surname>Kumar</surname><given-names>A.</given-names></name>
</person-group><article-title>Object Detection Using SSD</article-title><source>Proceedings of the 2023 5th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)</source><conf-loc>Greater Noida, India</conf-loc><conf-date>15&#x02013;16 December 2023</conf-date><pub-id pub-id-type="doi">10.1109/icac3n60023.2023.10541704</pub-id></element-citation></ref><ref id="B11-sensors-25-01097"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qian</surname><given-names>R.</given-names></name>
<name><surname>Lai</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
</person-group><article-title>3D Object Detection for Autonomous Driving: A Survey</article-title><source>Pattern Recognit.</source><year>2022</year><volume>130</volume><fpage>108796</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2022.108796</pub-id></element-citation></ref><ref id="B12-sensors-25-01097"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Qi</surname><given-names>C.R.</given-names></name>
<name><surname>Su</surname><given-names>H.</given-names></name>
<name><surname>Mo</surname><given-names>K.</given-names></name>
<name><surname>Guibas</surname><given-names>L.J.</given-names></name>
</person-group><article-title>PointNet: Deep learning on point sets for 3D classification and segmentation</article-title><source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;26 July 2017</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2017</year></element-citation></ref><ref id="B13-sensors-25-01097"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lang</surname><given-names>A.H.</given-names></name>
<name><surname>Vora</surname><given-names>S.</given-names></name>
<name><surname>Caesar</surname><given-names>H.</given-names></name>
<name><surname>Zhou</surname><given-names>L.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Beijbom</surname><given-names>O.</given-names></name>
</person-group><article-title>Pointpillars: Fast encoders for object detection from point clouds</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#x02013;20 June 2019</conf-date><fpage>12697</fpage><lpage>12705</lpage></element-citation></ref><ref id="B14-sensors-25-01097"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wen</surname><given-names>L.H.</given-names></name>
<name><surname>Jo</surname><given-names>K.H.</given-names></name>
</person-group><article-title>Three-attention mechanisms for one-stage 3-D object detection based on LiDAR and camera</article-title><source>IEEE Trans. Ind. Inform.</source><year>2021</year><volume>17</volume><fpage>6655</fpage><lpage>6663</lpage><pub-id pub-id-type="doi">10.1109/TII.2020.3048719</pub-id></element-citation></ref><ref id="B15-sensors-25-01097"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gadzicki</surname><given-names>K.</given-names></name>
<name><surname>Khamsehashari</surname><given-names>R.</given-names></name>
<name><surname>Zetzsche</surname><given-names>C.</given-names></name>
</person-group><article-title>Early vs late fusion in multimodal convolutional neural networks</article-title><source>Proceedings of the 2020 IEEE 23rd International Conference on Information Fusion (FUSION)</source><conf-loc>Rustenburg, South Africa</conf-loc><conf-date>6&#x02013;9 July 2020</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2020</year></element-citation></ref><ref id="B16-sensors-25-01097"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Q.</given-names></name>
<name><surname>Wu</surname><given-names>B.</given-names></name>
<name><surname>Zhu</surname><given-names>P.</given-names></name>
<name><surname>Li</surname><given-names>P.</given-names></name>
<name><surname>Zuo</surname><given-names>W.</given-names></name>
<name><surname>Hu</surname><given-names>Q.</given-names></name>
</person-group><article-title>ECA-Net: Efficient channel attention for deep convolutional neural networks</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;19 June 2020</conf-date></element-citation></ref><ref id="B17-sensors-25-01097"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Vora</surname><given-names>S.</given-names></name>
<name><surname>Lang</surname><given-names>A.H.</given-names></name>
<name><surname>Helou</surname><given-names>B.</given-names></name>
<name><surname>Beijbom</surname><given-names>O.</given-names></name>
</person-group><article-title>Pointpainting: Sequential fusion for 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;19 June 2020</conf-date></element-citation></ref><ref id="B18-sensors-25-01097"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Geiger</surname><given-names>A.</given-names></name>
<name><surname>Lenz</surname><given-names>P.</given-names></name>
<name><surname>Stiller</surname><given-names>C.</given-names></name>
<name><surname>Urtasun</surname><given-names>R.</given-names></name>
</person-group><article-title>Vision meets robotics: The kitti dataset</article-title><source>Int. J. Robot. Res.</source><year>2013</year><volume>32</volume><fpage>1231</fpage><lpage>1237</lpage><pub-id pub-id-type="doi">10.1177/0278364913491297</pub-id></element-citation></ref><ref id="B19-sensors-25-01097"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>Z.</given-names></name>
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Jia</surname><given-names>J.</given-names></name>
</person-group><article-title>3dssd: Point-based 3d single stage object detector</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;19 June 2020</conf-date></element-citation></ref><ref id="B20-sensors-25-01097"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>W.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Tian</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>F.</given-names></name>
</person-group><article-title>2D and 3D object detection algorithms from images: A Survey</article-title><source>Array</source><year>2023</year><volume>11</volume><fpage>100305</fpage><pub-id pub-id-type="doi">10.1016/j.array.2023.100305</pub-id></element-citation></ref><ref id="B21-sensors-25-01097"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bello</surname><given-names>S.A.</given-names></name>
<name><surname>Yu</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>C.</given-names></name>
<name><surname>Adam</surname><given-names>J.M.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep learning on 3D point clouds</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><elocation-id>1729</elocation-id><pub-id pub-id-type="doi">10.3390/rs12111729</pub-id></element-citation></ref><ref id="B22-sensors-25-01097"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>Y.</given-names></name>
<name><surname>Zhou</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Dong</surname><given-names>J.</given-names></name>
<name><surname>Zheng</surname><given-names>N.</given-names></name>
</person-group><article-title>Voxel or Pillar: Exploring Efficient Point Cloud Representation for 3D Object Detection</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>20&#x02013;27 February 2024</conf-date><volume>Volume 38</volume></element-citation></ref><ref id="B23-sensors-25-01097"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>R.</given-names></name>
<name><surname>Dou</surname><given-names>J.</given-names></name>
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
<name><surname>Pu</surname><given-names>S.</given-names></name>
</person-group><article-title>Rpvnet: A deep and efficient range-point-voxel fusion network for lidar point cloud segmentation</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#x02013;17 October 2021</conf-date></element-citation></ref><ref id="B24-sensors-25-01097"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Ma</surname><given-names>H.</given-names></name>
<name><surname>Wan</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>B.</given-names></name>
<name><surname>Xia</surname><given-names>T.</given-names></name>
</person-group><article-title>Multi-view 3d object detection network for autonomous driving</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;26 July 2017</conf-date></element-citation></ref><ref id="B25-sensors-25-01097"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qi</surname><given-names>C.R.</given-names></name>
<name><surname>Yi</surname><given-names>L.</given-names></name>
<name><surname>Su</surname><given-names>H.</given-names></name>
<name><surname>Guibas</surname><given-names>L.J.</given-names></name>
</person-group><article-title>Pointnet++: Deep hierarchical feature learning on point sets in a metric space</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><volume>30</volume><fpage>5100</fpage><lpage>5109</lpage></element-citation></ref><ref id="B26-sensors-25-01097"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
<name><surname>Tuzel</surname><given-names>O.</given-names></name>
</person-group><article-title>Voxelnet: End-to-end learning for point cloud based 3d object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#x02013;23 June 2018</conf-date></element-citation></ref><ref id="B27-sensors-25-01097"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yan</surname><given-names>Y.</given-names></name>
<name><surname>Mao</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>B.</given-names></name>
</person-group><article-title>Second: Sparsely embedded convolutional detection</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>3337</elocation-id><pub-id pub-id-type="doi">10.3390/s18103337</pub-id><pub-id pub-id-type="pmid">30301196</pub-id>
</element-citation></ref><ref id="B28-sensors-25-01097"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>L.</given-names></name>
<name><surname>Cai</surname><given-names>D.</given-names></name>
<name><surname>Sheng</surname><given-names>L.</given-names></name>
<name><surname>Xu</surname><given-names>D.</given-names></name>
</person-group><article-title>3DVG-Transformer: Relation modeling for visual grounding on point clouds</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#x02013;17 October 2021</conf-date></element-citation></ref><ref id="B29-sensors-25-01097"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>T.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Shi</surname><given-names>Y.</given-names></name>
<name><surname>Jiang</surname><given-names>K.</given-names></name>
<name><surname>Yang</surname><given-names>M.</given-names></name>
<name><surname>Yang</surname><given-names>D.</given-names></name>
</person-group><article-title>Bridging the view disparity between radar and camera features for multi-modal fusion 3d object detection</article-title><source>IEEE Trans. Intell. Veh.</source><year>2023</year><volume>8</volume><fpage>1523</fpage><lpage>1535</lpage><pub-id pub-id-type="doi">10.1109/TIV.2023.3240287</pub-id></element-citation></ref><ref id="B30-sensors-25-01097"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>J.</given-names></name>
<name><surname>Shen</surname><given-names>L.</given-names></name>
<name><surname>Sun</surname><given-names>G.</given-names></name>
</person-group><article-title>Squeeze-and-excitation networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#x02013;23 June 2018</conf-date></element-citation></ref><ref id="B31-sensors-25-01097"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Woo</surname><given-names>S.</given-names></name>
<name><surname>Park</surname><given-names>J.</given-names></name>
<name><surname>Lee</surname><given-names>J.Y.</given-names></name>
<name><surname>Kweon</surname><given-names>I.S.</given-names></name>
</person-group><article-title>Cbam: Convolutional block attention module</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#x02013;14 September 2018</conf-date></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01097-f001"><label>Figure 1</label><caption><p>Architecture of PointPillars.</p></caption><graphic xlink:href="sensors-25-01097-g001" position="float"/></fig><fig position="float" id="sensors-25-01097-f002"><label>Figure 2</label><caption><p>Overall architecture.</p></caption><graphic xlink:href="sensors-25-01097-g002" position="float"/></fig><fig position="float" id="sensors-25-01097-f003"><label>Figure 3</label><caption><p>Efficient Channel Attention.</p></caption><graphic xlink:href="sensors-25-01097-g003" position="float"/></fig><fig position="float" id="sensors-25-01097-f004"><label>Figure 4</label><caption><p>(<bold>a</bold>) ECA vs. CBAM channel attention weights; (<bold>b</bold>) ECA vs. SE channel attention weights.</p></caption><graphic xlink:href="sensors-25-01097-g004" position="float"/></fig><fig position="float" id="sensors-25-01097-f005"><label>Figure 5</label><caption><p>Quantitative analysis of three typical scenarios. (<bold>a</bold>) For occluded vehicles, our model has better performance than the original PointPillars model (<bold>b</bold>) For the small object on the left, our model has better performance than the original PointPillars model (<bold>c</bold>) highlights the improvement achieved by our model in detecting scenes where the target and environment exhibit low distinguishability.</p></caption><graphic xlink:href="sensors-25-01097-g005" position="float"/></fig><table-wrap position="float" id="sensors-25-01097-t001"><object-id pub-id-type="pii">sensors-25-01097-t001_Table 1</object-id><label>Table 1</label><caption><p>KITTI dataset categorization.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Detection Difficulty</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Minimum Bounding Box Height</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Maximum Occlusion Level</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Maximum Truncation Level</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Easy</td><td align="center" valign="middle" rowspan="1" colspan="1">40</td><td align="center" valign="middle" rowspan="1" colspan="1">Fully Visible</td><td align="center" valign="middle" rowspan="1" colspan="1">15%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">25</td><td align="center" valign="middle" rowspan="1" colspan="1">Partially Occluded</td><td align="center" valign="middle" rowspan="1" colspan="1">30%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Difficult to See</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01097-t002"><object-id pub-id-type="pii">sensors-25-01097-t002_Table 2</object-id><label>Table 2</label><caption><p>Experimental configuration.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">System</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Version</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" rowspan="1" colspan="1">11th Gen Intel<sup>&#x000ae;</sup> Core&#x02122; i9-11900K @ 3.50GHz &#x000d7; 16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA Geforce RTX 3090 Founders Edition</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CUDA</td><td align="center" valign="middle" rowspan="1" colspan="1">11.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Operating System </td><td align="center" valign="middle" rowspan="1" colspan="1">Ubuntu 18.04.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Memory</td><td align="center" valign="middle" rowspan="1" colspan="1">64G</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deep Learning Environment </td><td align="center" valign="middle" rowspan="1" colspan="1">Anaconda 3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deep Learning Framework</td><td align="center" valign="middle" rowspan="1" colspan="1">Pytorch 1.8.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Programming Language</td><td align="center" valign="middle" rowspan="1" colspan="1">Python 3.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dataset</td><td align="center" valign="middle" rowspan="1" colspan="1">KITTI</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Code Repository</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OpenPCDet</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01097-t003"><object-id pub-id-type="pii">sensors-25-01097-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of 3D metrics AP40 on the KITTI dataset (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Car</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Pedestrian</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Cyclist</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.39</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">51.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">46.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.83</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars + SE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.62</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.59</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars + CBAM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.62</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">51.75</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>88.34</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>78.08</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>75.38</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>59.67</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>54.67</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>50.03</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>82.49</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>61.43</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>57.38</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01097-t004"><object-id pub-id-type="pii">sensors-25-01097-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison of 3D metrics AP10 on the KITTI dataset (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Car</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Pedestrian</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Cyclist</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.31</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars + SE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.22</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.37</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars + CBAM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.97</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.19</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>86.76</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>76.86</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>75.60</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>59.69</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>55.94</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>51.68</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>80.70</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>61.79</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>58.14</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01097-t005"><object-id pub-id-type="pii">sensors-25-01097-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of AOS metrics AP40 on the KITTI dataset (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Car</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Pedestrian</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Cyclist</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.97</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.88</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars + SE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.68</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">46.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">44.07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.96</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars + CBAM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.52</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.70</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.59</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01097-t006"><object-id pub-id-type="pii">sensors-25-01097-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparison of AOS metrics AP10 on the KITTI dataset (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Car</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Pedestrian</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Cyclist</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.72</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.31</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.00</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars + SE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">46.91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">44.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.36</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars + CBAM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">46.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.17</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.93</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01097-t007"><object-id pub-id-type="pii">sensors-25-01097-t007_Table 7</object-id><label>Table 7</label><caption><p>Comparison of AOS metrics AP10 for different fusion strategies.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Car</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Pedestrian</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Cyclist</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MV3D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.72</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">44.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.17</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-Modal Fusion</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.62</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38.26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.69</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PointPillars</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.72</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.31</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.00</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.93</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01097-t008"><object-id pub-id-type="pii">sensors-25-01097-t008_Table 8</object-id><label>Table 8</label><caption><p>Comparison of effectiveness of different evaluation indicators in KITTI dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">PointPillars</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Ours</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mAP</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mAP</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Delta</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Pedestrian</td><td align="center" valign="middle" rowspan="1" colspan="1">mAP<sub>bbox</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">70.28</td><td align="center" valign="middle" rowspan="1" colspan="1">66.13</td><td align="center" valign="middle" rowspan="1" colspan="1">63.37</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>66.59</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">75.56</td><td align="center" valign="middle" rowspan="1" colspan="1">71.68</td><td align="center" valign="middle" rowspan="1" colspan="1">68.78</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>72.01</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>+5.41</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP<sub>bev</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">63.96</td><td align="center" valign="middle" rowspan="1" colspan="1">57.30</td><td align="center" valign="middle" rowspan="1" colspan="1">53.59</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>58.28</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">64.91</td><td align="center" valign="middle" rowspan="1" colspan="1">59.89</td><td align="center" valign="middle" rowspan="1" colspan="1">55.80</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>60.20</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>+1.92</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP<sub>3d</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">57.55</td><td align="center" valign="middle" rowspan="1" colspan="1">51.25</td><td align="center" valign="middle" rowspan="1" colspan="1">46.77</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>51.86</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">59.67</td><td align="center" valign="middle" rowspan="1" colspan="1">54.67</td><td align="center" valign="middle" rowspan="1" colspan="1">50.03</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>54.79</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>+2.93</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mAP<sub>aos</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>43.36</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>52.91</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>+9.55</bold>
</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Cyclist</td><td align="center" valign="middle" rowspan="1" colspan="1">mAP<sub>bbox</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">88.59</td><td align="center" valign="middle" rowspan="1" colspan="1">71.54</td><td align="center" valign="middle" rowspan="1" colspan="1">67.83</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>75.99</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">92.00</td><td align="center" valign="middle" rowspan="1" colspan="1">75.89</td><td align="center" valign="middle" rowspan="1" colspan="1">71.73</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>79.87</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>+3.89</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP<sub>bev</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">86.06</td><td align="center" valign="middle" rowspan="1" colspan="1">63.15</td><td align="center" valign="middle" rowspan="1" colspan="1">59.08</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>69.43</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">87.35</td><td align="center" valign="middle" rowspan="1" colspan="1">66.95</td><td align="center" valign="middle" rowspan="1" colspan="1">62.51</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>72.27</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>+2.84</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP<sub>3d</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">78.99</td><td align="center" valign="middle" rowspan="1" colspan="1">56.46</td><td align="center" valign="middle" rowspan="1" colspan="1">52.83</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>62.76</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">82.49</td><td align="center" valign="middle" rowspan="1" colspan="1">61.43</td><td align="center" valign="middle" rowspan="1" colspan="1">57.39</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>67.10</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>+4.34</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mAP<sub>aos</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.97</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.07</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.70</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>78.07</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>+6.00</bold>
</td></tr></tbody></table></table-wrap></floats-group></article>