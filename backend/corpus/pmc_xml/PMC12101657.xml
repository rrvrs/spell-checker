<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408322</article-id><article-id pub-id-type="pmc">PMC12101657</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0321856</article-id><article-id pub-id-type="publisher-id">PONE-D-24-28608</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Audio Signal Processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Equipment</subject><subj-group><subject>Audio Equipment</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Speech Signal Processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Information Theory</subject><subj-group><subject>Background Signal Noise</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Background Signal Noise</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject></subj-group></subj-group></article-categories><title-group><article-title>Audio-visual source separation with localization and individual control</article-title><alt-title alt-title-type="running-head">Audio-visual source separation with localization</alt-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3938-7495</contrib-id><name><surname>Kothandaraman</surname><given-names>Mohanaprasad</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3243-9814</contrib-id><name><surname>Ramalingam</surname><given-names>Balakrishnan</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6602-7212</contrib-id><name><surname>Sheng</surname><given-names>Kai</given-names></name><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff003" ref-type="aff">
<sup>3</sup>
</xref><xref rid="econtrib001" ref-type="author-notes">
<sup>&#x02021;</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Verma</surname><given-names>Aman</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="econtrib001" ref-type="author-notes">
<sup>&#x02021;</sup>
</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0009-8283-0318</contrib-id><name><surname>Dhagat</surname><given-names>Utkarsh</given-names></name><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Parab</surname><given-names>Pranav</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0001-0430-6248</contrib-id><name><surname>Mallavolu</surname><given-names>Siddhartha</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Ganesh</surname><given-names>Sankar</given-names></name><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>School of Electronics Engineering (SENSE), Vellore Institute of Technology, Chennai, India</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>School of Computer Science and Engineering (SCOPE), Vellore Institute of Technology, Chennai, India</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Xidian University, Xi&#x02019;an, China</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Rashid</surname><given-names>Javed</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>UO: University of Okara, PAKISTAN</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="other" id="econtrib001"><p>&#x02021; These authors also contributed equally to this work.</p></fn><corresp id="cor001">* E-mail: <email>balakrishnan.r@vit.ac.in</email></corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0321856</elocation-id><history><date date-type="received"><day>16</day><month>7</month><year>2024</year></date><date date-type="accepted"><day>12</day><month>3</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Kothandaraman et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Kothandaraman et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0321856.pdf">
</self-uri><abstract><p>The growing reliance on video conferencing software brings significant benefits but also introduces challenges, particularly in managing audio quality. In multi-participant settings, ambient noise and interruptions can hinder speaker recognition and disrupt the flow of conversation. This work proposes an audio-visual source separation pipeline designed specifically for video conferencing and telepresence robots applications. The framework aims to isolate and enhance the speech of individual participants in noisy environments while enabling control over the volume of specific individuals captured in the video frame. The proposed pipeline comprises key components: a deep learning-based feature extractor for audio and video, an audio-guided visual attention mechanism, a module for background noise suppression and human voice separation, and Deep Multi-Resolution Network (DMRN) modules. For human voice separation, the DPRNN-TasNet, a robust deep neural network framework, is employed. Experimental results demonstrate that the methodology effectively isolates and amplifies individual participants&#x02019; speech, achieving a test accuracy of 71.88 % on both the AVE and Music 21 datasets.</p></abstract><funding-group><funding-statement>The author(s) received no specific funding for this work.</funding-statement></funding-group><counts><fig-count count="10"/><table-count count="4"/><page-count count="19"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>
<ext-link xlink:href="https://www.v7labs.com/open-datasets/audio-visual-event-ave-dataset" ext-link-type="uri">https://www.v7labs.com/open-datasets/audio-visual-event-ave-dataset</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>
<ext-link xlink:href="https://www.v7labs.com/open-datasets/audio-visual-event-ave-dataset" ext-link-type="uri">https://www.v7labs.com/open-datasets/audio-visual-event-ave-dataset</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>1 Introduction</title><p>In today&#x02019;s fast-paced world, video and audio conferencing applications play a pivotal role in connecting individuals and businesses across the globe. These applications have become essential tools for communication, collaboration, and productivity, enabling people to connect face-to-face regardless of their physical location. Whether used for virtual meetings, remote work, online education, or social interactions, video and audio conferencing applications have transformed the way we communicate and collaborate. While video and audio conferencing apps have revolutionized communication and collaboration, they do come with certain disadvantages. One common issue is the presence of background noise, especially when many individuals are interacting in a single video frame. This noise can be distracting and make it difficult for participants to hear each other clearly.</p><p>Noise removal is an essential stage in audio processing, following which individual human voices are isolated. Audio source separation is an essential method in signal processing that seeks to isolate distinct audio sources from a combined audio output. The isolation enables precise control over each source, which is crucial for improving audio quality by reducing background noise.By differentiating the required audio sources from the surrounding background noise,specific noise reduction techniques can be performed more efficiently, enhancing both the clarity and comprehensibility of the primary audio signals. To achieve these objectives, beamforming, source separations, adaptive filtering, and Deep Learning (DL) techniques are utilized [<xref rid="pone.0321856.ref001" ref-type="bibr">1</xref>&#x02013;<xref rid="pone.0321856.ref004" ref-type="bibr">4</xref>].</p><p>Among them, the DL based techniques shows a better accuracy in audio source separation and noise cancellation. The literature presents a diverse range of methods based on above techniques for audio source separation that are specifically designed for various uses. In [<xref rid="pone.0321856.ref005" ref-type="bibr">5</xref>], Zhou <italic toggle="yes">et al</italic>. presented AVSBench, a benchmark for audio-visual segmentation that provides detailed annotations for sounding objects in audible videos at the pixel level. This study highlights the utilization of a temporal pixel-wise audio-visual interaction module to direct the visual segmentation process using audio semantics. Additionally, a regularization loss is employed to improve audio-visual correlation during the training phase. Forster <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref006" ref-type="bibr">6</xref>] implemented an unsupervised model-based deep learning approach for separating musical sources. In this method, each source is represented by a differentiable parametric source filter model. The approach involves training a neural network to restore the combined signal by approximating the characteristics of each source model using their fundamental frequencies. Tian <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref007" ref-type="bibr">7</xref>] introduced a method to localize audio-visual events in unrestricted films. They utilized an audio-guided visual attention mechanism, a Dual Multimodal Residual Network (DMRN), and an audio-visual distance learning network to enable accurate localization across different modalities. Stoller <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref008" ref-type="bibr">8</xref>] train the Wave-U-Net framework for complete audio source separation. The framework takes on the audio waveform as input and isolate distinct sources, such as vocals, drums, and bass, from a combined signal. In [<xref rid="pone.0321856.ref009" ref-type="bibr">9</xref>], David <italic toggle="yes">et al</italic>. suggested a strategy for tackling the source separation problem in the presence of numerous voice signals. The method uses automatic lipreading to distinguish an acoustic speech signal from other acoustic signals based on their coherence with the speaker&#x02019;s lip movements. In another study, Gao <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref010" ref-type="bibr">10</xref>] utilize co-separation and localized object identification to visually direct audio source separation. This approach utilizes the complementary information in visual and auditory modalities to separate sound sources associated with distinct visual elements in a scene. The proposed scheme uses DL techniques to separate and identify sounds associated with individual objects in complicated contexts by using correlations between visual features and audio spectrograms. Li <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref011" ref-type="bibr">11</xref>] solve the audio-visual source association problem by comparing visual and auditory information. The proposed approach employs multimodal vibrato analysis to provide a comprehensive understanding of the relationship between visual and auditory performance cues, paving the way for improved musical interpretation and synchronization. Rahman <italic toggle="yes">et al</italic>. introduce a weakly supervised audio-visual sound source detection and separation method to localize and separate individual object sounds in the audio channel of a video [<xref rid="pone.0321856.ref012" ref-type="bibr">12</xref>]. Here, the authors developed an audio-visual co-segmentation network that learns both the visual and auditory characteristics of individual objects from videos labeled only with object labels. By jointly learning from both modalities, the network can effectively localize and separate sound sources in complex audiovisual scenes. Jie et.al developed unsupervised learning models for audio-visual localization and separation tasks [<xref rid="pone.0321856.ref013" ref-type="bibr">13</xref>]. The method employs low-rank modeling to capture background visual and audio information and sparsity to extract sparsely correlated components between the audio and visual modalities. Appearance and Motion network (AMnet) is proposed by Zhu and Esa to extract individual audio components from a mixture using video data from sound sources [<xref rid="pone.0321856.ref014" ref-type="bibr">14</xref>]. Leon <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref015" ref-type="bibr">15</xref>] developed a framework that integrates Deep Neural Networks (DNNs) and beamforming to provide efficient localization, separation, and reconstruction of sound sources utilizing arrays of microphones. This technology incorporates powerful machine learning techniques to enhance accuracy and decrease computing burden in intricate acoustic settings, establishing new standards for real-time audio processing. The authors use the self-supervised motion representation technique to train the proposed two-stage architecture to separate and localize sound sources in complex audio-visual scenes. Islam <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref016" ref-type="bibr">16</xref>] address the challenge of audio source separation for both known and unknown objects. They introduce a meta-consistency driven test-time adaptation strategy, enhancing model adaptability through a self-supervised audio-visual consistency objective. Zhang <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref017" ref-type="bibr">17</xref>] address the limitation of sound source separation models that only consider audio data by integrating a Dual-channel attention mechanism that leverages both audio and visual inputs. Their model, Audio-Visual separation integrating the Dual-channel Attention mechanism (AVDA), dynamically fuses audio and visual features to improve the quality of sound separation, and it shows significantly better performance on the MUSIC-21 dataset than previous models, as demonstrated by higher scores in sound distortion, interference, and artifact ratios. Long <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref018" ref-type="bibr">18</xref>] presents a method that utilizes a deep neural network (DNN) and a microphone array to accurately determine the location, separate, and reconstruct various sound sources. The study investigates a combined signal processing approach that combines beamforming techniques with deep neural networks (DNNs), demonstrating particular efficacy in circumstances where there are multiple sound sources that overlap. The technique improves precision and decreases computing burdens, confirmed through simulations and experimental configurations. The self supervised technique proposed by Yang <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref019" ref-type="bibr">19</xref>] enhances sound localization in films. The strategy aims to enhance the auditory aspect within a multimodal audio-visual learning framework. The authors introduce a novel approach that clusters audio attributes to generate pseudo-labels, which are subsequently employed to guide the training of the audio processing core. When utilized with MUSIC datasets [<xref rid="pone.0321856.ref020" ref-type="bibr">20</xref>], this method significantly enhances the precision of identifying sound sources by optimizing the integration and utilization of audio cues in the audio-visual correspondence learning process. Sanabria-Macias <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref021" ref-type="bibr">21</xref>] proposed an improved audiovisual tracking system that combines particle filters and probabilistic models to accurately determine the 3D position in intelligent environments. This is achieved by merging audio and video data. This system greatly enhances the precision of tracking in situations that are constantly changing and have obstructed views. It achieves this by employing a visual appearance model that takes into account the position of the subject&#x02019;s lips, as well as an audio likelihood model that is based on a probabilistic variant of SRP-PHAT. A multimodal speaker diarization system is presented by Ahmad <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref022" ref-type="bibr">22</xref>], which utilizes a pre-trained SyncNet model to synchronize audio and visual data. This synchronization process improves the accuracy of speaker identification. This system utilizes Gaussian mixture model-based clustering to analyze synchronized audio-visual segments, resulting in lower diarization error rates compared to approaches that exclusively use audio. The technique exhibits significant enhancements in situations involving simultaneous speaking and dynamic interactions among participants. Liu <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref023" ref-type="bibr">23</xref>] propose a system for real-time speech separation that combines camera and microphone array sensors to improve speech quality in noisy conditions. By employing computer vision for speaker detection and beamforming for sound isolation, this approach greatly enhances the clarity and comprehensibility of speech. It efficiently decreases noise in both static and dynamic situations, showing promise for applications in assistive listening and machine language processing.</p><p>The aforementioned implementations have been utilized either for the purpose of segregating audio sources or for the purpose of determining the location of human audio sources. Previous studies on the subject have failed to develop a pipeline that allows for individual control over audio sources, including non-human sounds, after detecting them. This utility has the potential to be extremely useful in a wide range of fields, and this work tries to address that limitation by providing control of each source present in the video frame</p><p>The manuscript is structured as follows, In Sect [2]2, a thorough description of the proposed methodology is discussed. Sect [3]3 discusses about the dataset and experimental setup and presents the results of the experiments. Finally, Sect 4 provides a brief summary of the findings and identifies potential areas for future scope.</p></sec><sec sec-type="materials|methods" id="sec002"><title>2 Materials and methods</title><p>The audio-visual source separation pipeline, illustrated in <xref rid="pone.0321856.g001" ref-type="fig">Fig 1</xref>, commences with specialized feature extractors for audio and video inputs that pre-process these streams for further thorough analysis. The video characteristics are extracted utilizing a Convolutional Neural Network (CNN), namely VGG19, which discerns vital spatial attributes including shapes, edges, and textures necessary for scene comprehension. Simultaneously, audio data undergoes preliminary processing in the audio source separation block, where ConvTasNet isolates human speech from ambient noise, and DPRNNTasNet performs monaural speech separation, enhancing speech clarity and intelligibility. Subsequent to this phase, the separated audio sources are input into VGGish, which excels at extracting intricate audio features that represent the spectral and temporal characteristics of the sounds. This guarantees that each audio piece is examined without disruption from overlapping noises, yielding a clearer and more distinct set of information for subsequent processing.</p><fig position="float" id="pone.0321856.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.g001</object-id><label>Fig 1</label><caption><title>Block diagram of our proposed system.</title></caption><graphic xlink:href="pone.0321856.g001" position="float"/></fig><p>The elements from both the audio (VGGish) and video streams are subsequently integrated within an attention layer. This layer assesses the audio-influenced visual attributes, enhancing attention on video elements that align with prominent audio signals, thus maximizing the pertinence of the visual information concerning the audio input.</p><p>Convolutional Neural Networks (CNNs) are employed for their proficiency in extracting hierarchical spatial features from visual and auditory data, crucial for understanding complex images and sounds. LSTM networks are utilized to capture temporal relationships and sequence dynamics in data, essential for processing continuous time inputs like video frames and audio signals.Two LSTM networks are integrated into our framework. The first LSTM processes features extracted by VGGish, while the second analyzes sequential data from both modalities, as generated by the attention module. These networks adeptly capture and track the temporal dynamics within the video frames and the evolving auditory environments.</p><p>The outputs from the LSTM models, which now include integrated and augmented spatial and temporal information from video and audio data, are processed via the Dual Multimodal Residual Network (DMRN). This network utilizes a fusion layer to integrate the representations of both modalities. The resultant unified representation is subjected to further processing via LSTM to improve the audio signals, improving speech separation and reducing background noise, therefore illustrating the efficacy of the DMRN architecture.</p><p>In the final level of the model, a Fully Connected Neural Network (FCN) layer serves as the primary decision-making unit, utilizing sigmoid activation functions. This layer integrates multi-modal information from prior stages by a linear transformation, expressed as <inline-formula id="pone.0321856.e001"><alternatives><graphic xlink:href="pone.0321856.e001.jpg" id="pone.0321856.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mi>x</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>b</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, where <italic toggle="yes">W</italic> denotes the weight matrix and <italic toggle="yes">b</italic> signifies the bias vector. The sigmoid function is defined as <inline-formula id="pone.0321856.e002"><alternatives><graphic xlink:href="pone.0321856.e002.jpg" id="pone.0321856.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>.</p><p>Transforms these outputs into probabilities that signify the likelihood of specific events within the video frame. This transformation is essential as it offers a quantitative assessment of the model&#x02019;s predictions, improving its capacity to precisely analyze intricate events from the combined audio visual data. The detail of each module and its functionality is described as follows.</p><sec id="sec003"><title>2.1 VGG19 video feature extractor</title><p>The VGG19 model, pre-trained on the ImageNet database, is utilized for visual feature extraction from the visual inputs <inline-formula id="pone.0321856.e003"><alternatives><graphic xlink:href="pone.0321856.e003.jpg" id="pone.0321856.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. VGG19 is a deep convolutional neural network designed for the extraction of spatial elements, including edges, forms, and textures, which are essential for visual picture comprehension. The video input is processed at a rate of 24 RGB frames per second, with each frame transmitted through the network to produce feature maps measuring <inline-formula id="pone.0321856.e004"><alternatives><graphic xlink:href="pone.0321856.e004.jpg" id="pone.0321856.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mn>512</mml:mn><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow><mml:mspace width="0.167em"/><mml:mn>7</mml:mn><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow><mml:mspace width="0.167em"/><mml:mn>7</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. These feature maps encapsulate elevated representations of visual content, including crucial spatial features for efficient alignment and integration with aural input in later processing phases.</p></sec><sec id="sec004"><title>2.2 Audio source separation</title><p>Audio source separation module as shown in <xref rid="pone.0321856.g002" ref-type="fig">Figs 2</xref> and <xref rid="pone.0321856.g003" ref-type="fig">3</xref> process the audio signal separated from video streams. Audio source separation is a two-stage pipeline module that involves two key processes: separating noise sources and isolating human voices from the input audio signal. Noise source separation is a preliminary step in audio source separation that isolates noise from the audio and recovers the segment of the audio that predominantly consists of human voices to avoid interference from background noise during audio-visual event computation. Within the audio source separation pipeline, two models are employed consecutively to pre-process the audio input. At first, the Conv-TasNet [<xref rid="pone.0321856.ref024" ref-type="bibr">24</xref>] algorithm eliminates ambient noise from the audio stream. Afterwards, DPRNN-TasNet [<xref rid="pone.0321856.ref025" ref-type="bibr">25</xref>] further processes the cleaned audio to separate the voices of individual speakers. After the audio is produced, it can be used for feature extraction and further processing by the LSTM network to perform audio-visual event localization tasks. Employing sophisticated separation techniques is crucial to minimize noise and maximize useful signal in the audio component of the system. Conv-TasNet and DPRNN-TasNet are integral components, each contributing uniquely to the audio processing pipeline.</p><fig position="float" id="pone.0321856.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.g002</object-id><label>Fig 2</label><caption><title>Audio source separation module.</title></caption><graphic xlink:href="pone.0321856.g002" position="float"/></fig><fig position="float" id="pone.0321856.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.g003</object-id><label>Fig 3</label><caption><title>DPRNN-Tasnet module.</title></caption><graphic xlink:href="pone.0321856.g003" position="float"/></fig><p>Conv-TasNet is used to separate the noise from the audio signal. It is a deep neural network architecture utilized for monaural speech separation, the process of isolating individual speakers from a mixed audio input that includes multiple speakers. Conv-TasNet&#x02019;s architecture originates from the Time-domain Audio Separation Network (TasNet), comprising an encoder module, a separator module, and a decoder module. The encoder module transforms the input mixed audio stream into a series of high-level feature representations. The model consists of several 1D convolutional layers, followed by down sampling operation by using max-pooling. The encoder produces a low-dimensional representation of the input audio signal that captures the key elements necessary for separation. The separator module is in charge of isolating individual speakers from the mixed audio signal. It consists of a stack of several 1D convolutional layers with dilated convolutions to capture long-term temporal dependencies in the input signal. The separator module produces an output that consists of a collection of masks that designate the segments of the input signal attributed to each speaker.</p><p>Further, DPRNN-TasNet is used for monaural speech separation, which separates individual speakers from a mixed audio signal containing multiple speakers. DPRNN-TasNet is a neural network model that use a deep recurrent neural network (RNN) to predict the time-frequency masks for individual speakers in a mixed audio input. The masks are used on the mixed audio to extract individual audio signals for each speaker. The model utilizes the TasNet architecture, leveraging a convolutional neural network (CNN) for generating the initial time-frequency representations of the input audio signals. The CNN output is fed into a series of stacked recurrent layers to conduct the necessary time-domain processing for separating the mixed audio inputs into distinct speech signals. In DPRNN-TasNet, the recurrent layers are replaced with a Deep Recurrent Neural Network (RNN) that uses the Dual-Path Recurrent Neural Network (DPRNN) structure.</p></sec><sec id="sec005"><title>2.3 VGGish audio feature extractor</title><p>VGGish model, pre-trained on the extensive AudioSet dataset, is utilized for auditory feature extraction from the audio inputs <italic toggle="yes">A</italic><sub><italic toggle="yes">t</italic></sub>. This model emphasizes the acquisition of essential auditory attributes, including rhythm, pitch, and timbre, which are vital for the analysis and synchronization of audio-visual events. Multiple VGGish models are employed to manage the complexity and diversity of audio signals, with each model addressing a specific audio source. This approach processes individual audio streams separately, ensuring customized feature extraction that enables precise alignment with the associated visual data. The VGGish model generates spectral and temporal audio features that improve the system&#x02019;s capacity to localize and evaluate audio-visual events, even in difficult situations with overlapping or intricate sound environments.</p></sec><sec id="sec006"><title>2.4 Audio-guided visual attention</title><p>Audio-visual event localization network was utilized for localizing the audio sources from video frames. It takes input from both VGG19 and VGGish feature extractor and involves identifying the type and temporal boundaries of events in a video sequence using visual and auditory information generated by feature extractor <inline-formula id="pone.0321856.e005"><alternatives><graphic xlink:href="pone.0321856.e005.jpg" id="pone.0321856.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003c9;</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <italic toggle="yes">A</italic><sub><italic toggle="yes">t</italic></sub>. The model integrates information from visual and auditory modalities using an attention mechanism to choose relevant visual elements based on the audio input. This method enables the model to focus on the most pertinent aspects of the visual data for each auditory input, hence enhancing the precision of event localization.</p><p>An essential part of integrating visual features with audio cues is the computation of the visual context vector, <inline-formula id="pone.0321856.e006"><alternatives><graphic xlink:href="pone.0321856.e006.jpg" id="pone.0321856.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003c9;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. An attention mechanism powers the computation, represented by the following equation:</p><disp-formula id="pone.0321856.e007">
<alternatives><graphic xlink:href="pone.0321856.e007.jpg" id="pone.0321856.e007g" position="anchor"/><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives>
</disp-formula><p>In this case, the attention function that dynamically combines visual information weighted by related auditory inputs is represented by <inline-formula id="pone.0321856.e008"><alternatives><graphic xlink:href="pone.0321856.e008.jpg" id="pone.0321856.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.</p><p>The definition of the attention function is:</p><disp-formula id="pone.0321856.e009">
<alternatives><graphic xlink:href="pone.0321856.e009.jpg" id="pone.0321856.e009g" position="anchor"/><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>att</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>According to their significance to the current audio input, each visual feature <inline-formula id="pone.0321856.e010"><alternatives><graphic xlink:href="pone.0321856.e010.jpg" id="pone.0321856.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> in this formulation is given an attention weight <italic toggle="yes">w</italic><sub><italic toggle="yes">i</italic>,<italic toggle="yes">t</italic></sub>. The softmax function is used to create these weights, emphasizing the importance of each feature by normalizing the inputs into a probability distribution.</p><p>The attention weights <italic toggle="yes">w</italic><sub><italic toggle="yes">t</italic></sub> are calculated as follows:</p><disp-formula id="pone.0321856.e011">
<alternatives><graphic xlink:href="pone.0321856.e011.jpg" id="pone.0321856.e011g" position="anchor"/><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>Softmax</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><disp-formula id="pone.0321856.e012">
<alternatives><graphic xlink:href="pone.0321856.e012.jpg" id="pone.0321856.e012g" position="anchor"/><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>U</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:msub><mml:mi>U</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>This section incorporates the non-linear activation function <inline-formula id="pone.0321856.e013"><alternatives><graphic xlink:href="pone.0321856.e013.jpg" id="pone.0321856.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, typically the hyperbolic tangent function, to add non-linearity to the learning process and enable the model to recognize more complex patterns. The transformation functions <inline-formula id="pone.0321856.e014"><alternatives><graphic xlink:href="pone.0321856.e014.jpg" id="pone.0321856.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <italic toggle="yes">U</italic><sub><italic toggle="yes">a</italic></sub> transform audio and visual inputs into a unified feature space that facilitates efficient integration.</p></sec><sec id="sec007"><title>2.5 LSTM</title><p>The LSTMs in the design are essential for capturing temporal dependencies in the visual and audio streams. The <inline-formula id="pone.0321856.e015"><alternatives><graphic xlink:href="pone.0321856.e015.jpg" id="pone.0321856.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mn>512</mml:mn><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow><mml:mspace width="0.167em"/><mml:mn>7</mml:mn><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow><mml:mspace width="0.167em"/><mml:mn>7</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> spatial feature maps produced by VGG19 and enhanced by the attention mechanism are fed into an LSTM to acquire sequential patterns across frames. The 128-dimensional audio features collected by VGGish are similarly processed by an additional LSTM to encode temporal correlations within the auditory domain. These LSTMs function unidirectionally (forward), generating temporally coherent feature representations suitable for multimodal integration through the DMRN.</p></sec><sec id="sec008"><title>2.6 Dual multimodal residual network (fusion)</title><p>The hidden states at time <italic toggle="yes">t</italic> for audio and visual inputs are denoted by <inline-formula id="pone.0321856.e016"><alternatives><graphic xlink:href="pone.0321856.e016.jpg" id="pone.0321856.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321856.e017"><alternatives><graphic xlink:href="pone.0321856.e017.jpg" id="pone.0321856.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, respectively, and are obtained from distinct LSTM networks. These states encode the temporal dynamics of the corresponding modalities: visual aspects like forms and movements for <inline-formula id="pone.0321856.e018"><alternatives><graphic xlink:href="pone.0321856.e018.jpg" id="pone.0321856.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, and audio features like rhythm and pitch for <inline-formula id="pone.0321856.e019"><alternatives><graphic xlink:href="pone.0321856.e019.jpg" id="pone.0321856.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. After further processing through linear and tanh layers, these states enable synchronized and enhanced multimodal integration. Fusion layer uses the DMRN (<xref rid="pone.0321856.g004" ref-type="fig">Fig 4</xref>) which is a type of neural network architecture that is designed to process data from two different modalities, such as audio and visual inputs. It handles both audio and visual inputs simultaneously, allowing it to learn representations that capture the relationships between the two modalities. The DMRN architecture consists of two streams: one for processing audio inputs and another for processing visual inputs. Each stream contains several residual blocks that extract features from the respective modality. The outputs of these streams are then fused together in the network to allow for joint processing of the multimodal information.</p><fig position="float" id="pone.0321856.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.g004</object-id><label>Fig 4</label><caption><title>Dual multimodal residual network for audio-visual feature fusion.</title></caption><graphic xlink:href="pone.0321856.g004" position="float"/></fig><disp-formula id="pone.0321856.e020"><alternatives><graphic xlink:href="pone.0321856.e020.jpg" id="pone.0321856.e020g" position="anchor"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mi>v</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(1)</label></disp-formula><disp-formula id="pone.0321856.e021"><alternatives><graphic xlink:href="pone.0321856.e021.jpg" id="pone.0321856.e021g" position="anchor"/><mml:math id="M21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mi>v</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mi>v</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(2)</label></disp-formula><p>Here, <inline-formula id="pone.0321856.e022"><alternatives><graphic xlink:href="pone.0321856.e022.jpg" id="pone.0321856.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mi>t</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321856.e023"><alternatives><graphic xlink:href="pone.0321856.e023.jpg" id="pone.0321856.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mi>v</mml:mi></mml:msubsup><mml:mi>t</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> are the updated audio and visual features. represents an addition fusion function[<xref rid="pone.0321856.e020" ref-type="disp-formula">1</xref> , <xref rid="pone.0321856.e021" ref-type="disp-formula">2</xref>]. In our proposed framework, the DMRN network get the updated visual and audio features from the LSTM network and perform the fusion task.</p></sec><sec id="sec009"><title>2.7 Fully connected layer</title><p>The final stage of our model involves a decision-making process regulated by a fully connected neural network layer utilizing sigmoid activation function. This layer amalgamates multi-modal information from prior layers by a linear transformation, technically denoted as <inline-formula id="pone.0321856.e024"><alternatives><graphic xlink:href="pone.0321856.e024.jpg" id="pone.0321856.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mi>x</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>b</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, where <italic toggle="yes">W</italic> signifies the weight matrix and <italic toggle="yes">b</italic> denotes the bias vector. The sigmoid function <inline-formula id="pone.0321856.e025"><alternatives><graphic xlink:href="pone.0321856.e025.jpg" id="pone.0321856.e025g" position="anchor"/><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> is then utilized on these linear outputs, transforming them into probabilities. These probabilities denote the chance of particular occurrences transpiring within the video clip. This output is essential, since it offers a quantitative assessment of the interactions between aural and visual input, hence augmenting the model&#x02019;s ability to generate accurate and interpretable predictions.</p><p>The output of the fully connected layer comprises probabilities linked to certain audiovisual events. These probabilities allow the model to differentiate between various scenarios, such as recognizing specific noises associated with particular visual features or actions. This renders the model especially proficient in applications necessitating comprehensive knowledge and analysis of intricate occurrences, such as surveillance and multimedia content analysis. The preceding separation of audio sources enables the fully connected layer to process clearer, more distinct data, resulting in more accurate and dependable predictions.</p></sec></sec><sec sec-type="results" id="sec010"><title>3 Results</title><p>This section describes the experimental methods and results. The experiments were performed with AVE and music 21 dataset and its performance were evaluated with standard performance metrics</p><sec id="sec011"><title>3.1 Dataset</title><p>AVE dataset is typically used for tasks such as sound source localization, audio-visual event detection, and multi-modal learning. It contains 4,143 videos in total, and it includes 28 different types of labels. These labels correspond to different types of audio-visual events that occur in the videos, such as &#x0201c;door knock,&#x0201d; &#x0201c;phone ringing,&#x0201d; &#x0201c;keyboard typing,&#x0201d; and others.</p><p>Music21 is a Python-based framework for computer-aided musicology that facilitates the analysis, research, and creation of music. It enables users to examine extensive music databases, provide musical examples, instruct in music theory, modify notation, and write music. The motto, &#x0201c;Listen Faster,&#x0201d; signifies the objective of allowing consumers to devote more time to appreciating music rather than engaging in arduous research. Music21 has been under development since 2008 and is grounded in academic traditions, primarily at MIT, where the &#x0201c;21&#x0201d; signifies its origins in the music department (Course 21). The toolbox is expanding and is extensively utilized in musicology, education, and composition.</p></sec><sec id="sec012"><title>3.2 Experimental settings</title><p>The experimental configuration,amalgamates visual and auditory modalities via an extensive pipeline that incorporates pre-trained CNNs, an audio-guided attention mechanism, and temporal modeling utilizing LSTMs, ultimately employing a Dual Multimodal Residual Network (DMRN) for fusion. For visual inputs <inline-formula id="pone.0321856.e026"><alternatives><graphic xlink:href="pone.0321856.e026.jpg" id="pone.0321856.e026g" position="anchor"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, each video segment is divided into 1-second intervals, from which 16 RGB frames are uniformly extracted. The frames undergo processing via VGG19, which is pre-trained on ImageNet, to extract spatial features, yielding <inline-formula id="pone.0321856.e027"><alternatives><graphic xlink:href="pone.0321856.e027.jpg" id="pone.0321856.e027g" position="anchor"/><mml:math id="M27" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> feature maps for each frame. Audio inputs <italic toggle="yes">A</italic><sub><italic toggle="yes">t</italic></sub> are processed using a source separation module (ConvTasNet) and subsequently analyzed by VGGish, which is pre-trained on AudioSet, to extract 128-dimensional acoustic features that encapsulate aural patterns. An audio-guided attention mechanism is utilized to align and augment visual aspects in accordance with aural cues. The polished visual and unprocessed audio characteristics are input into distinct LSTMs to model temporal dependencies, thereby capturing sequential patterns in the modalities. The outputs of the LSTMs are integrated within the DMRN, which improves modality interaction by concurrently updating and maintaining complementary information. The integrated representation is ultimately processed through a fully linked network for classification, accomplishing efficient multimodal integration and temporal alignment to enhance robust audio-visual event localization.</p></sec><sec id="sec013"><title>3.3 Training</title><p>The proposed model was trained using the Audio-Visual Event (AVE) dataset, comprising temporally labeled audio-visual events across 28 categories. The model design incorporates a Dual Multimodal Residual Network (DMRN), Long Short-Term Memory (LSTM) networks, and audio-guided attention processes. The training procedure aimed to enhance the synchronization and integration of auditory and visual data for accurate event localization.</p><p>The audio-guided attention layer was trained to selectively concentrate on spatial areas in the visual feature maps that align with the auditory items in the audio stream. Visual features, obtained from a pre-trained VGG19 network, were input into the attention layer in conjunction with auditory information from the VGGish model. The attention mechanism calculates a weighted sum of visual elements, influenced by auditory inputs, to produce attended visual features. The attention weights are acquired during the back propagation process, allowing the model to dynamically synchronize visual focus with aural signals over time.</p><p>The LSTM networks were employed for the temporal modeling of the attended visual and audio aspects. Distinct LSTM networks were utilized for each modality to capture the temporal dependencies in audio and visual streams. The LSTM received a sequence of attended visual features as input for visual data, but the input for audio data consisted of the raw feature vector recovered by VGGish. Each LSTM contained 128 hidden units, and its outputs were processed via the DMRN for multimodal fusion. Gradients for the LSTM were calculated via Back Propagation Through Time (BPTT), allowing the model to acquire both short-term and long-term dependencies within each modality.</p><p>The DMRN amalgamated the outputs of the LSTM networks to generate a unified multimodal representation. The residual network utilized an additive fusion function to amalgamate complementing information from the auditory and visual streams while maintaining modality-specific characteristics. This approach improved the integrated feature representation, enabling effective classification of audio-visual events. The network utilized a solitary residual block, as supplementary blocks did not enhance performance during empirical assessment. The integrated representation was processed through fully connected layers with a softmax activation function to forecast the event category for each video clip.</p><p>The complete model, comprising the attention layer, LSTM, and DMRN, was trained in an end-to-end manner with the Adam optimizer. The initial learning rate was established at 0.001, employing a step decay schedule that decreases the learning rate by a factor of 0.1 every 15,000 steps to enhance convergence. The training procedure utilized a batch size of 64 for a total of 300 epochs. The model was refined utilizing a Multi-Label Soft Margin Loss algorithm, which guaranteed precise categorization across various categories.</p><p>Regularization methods, including dropout with a probability of 0.5, were implemented in the fully connected layers to mitigate overfitting. L2 regularization was applied to all trainable parameters. Early stopping was utilized based on validation loss, with checkpoints preserved for the optimal model.</p><p>The AVE dataset was divided into training, validation, and test subsets. Validation occurred every five epochs, and the optimal model was chosen based on validation accuracy. During evaluation, the model&#x02019;s predictions were compared to the actual labels for temporal event localization. The assessment criterion employed was overall accuracy, illustrating the model&#x02019;s efficacy in utilizing attention mechanisms, temporal dependencies, and multimodal fusion for effective event localization.</p></sec><sec id="sec014"><title>3.4 Performance evaluation of audio source separation module</title><p>The WSJ0-2mix [<xref rid="pone.0321856.ref026" ref-type="bibr">26</xref>] dataset serves as a benchmark for audio source separation, created by dynamically mixing clean utterances from the WSJ0 corpus with overlapping speech from two speakers at varying signal-to-noise ratios (SNRs). Models like Conv-TasNet and DPRNN-TasNet are trained using this dataset and evaluated on unseen test sets with metrics such as SI-SNRi and SDRi, which measure improvements in separating overlapping speech while maintaining audio quality.</p><p>In this table, the &#x0201c;<inline-formula id="pone.0321856.e038"><alternatives><graphic xlink:href="pone.0321856.e038.jpg" id="pone.0321856.e038g" position="anchor"/><mml:math id="M28" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>&#x0201d; numbers denote the standard deviation, reflecting the variability of the performance measures among various test samples. The metrics indicate that Conv-TasNet excels in noise separation tasks, as shown by elevated SI-SNRi and SDRi values. DPRNN-TasNet demonstrates enhanced efficacy in human speech separation, attaining elevated SI-SNRi and SDRi scores, signifying its proficiency in speech-centric tasks. Following training, the performance metrics of the DPRNN-TasNet model showed significant improvements. As illustrated in <xref rid="pone.0321856.t001" ref-type="table">Table 1</xref>.</p><table-wrap position="float" id="pone.0321856.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.t001</object-id><label>Table 1</label><caption><title>Performance evaluation for ConvTasNet and DPRNNTasNet on WSJ0-2mix and WSJ0-3mix dataset.</title></caption><alternatives><graphic xlink:href="pone.0321856.t001" id="pone.0321856.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Model</th><th align="left" rowspan="1" colspan="1">Dataset</th><th align="left" rowspan="1" colspan="1">SI-SNRi (dB)</th><th align="left" rowspan="1" colspan="1">SDRi (dB)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Conv-TasNet-gLN</td><td align="left" rowspan="1" colspan="1">WSJ0-2mix</td><td align="left" rowspan="1" colspan="1">14.9 <inline-formula id="pone.0321856.e028"><alternatives><graphic xlink:href="pone.0321856.e028" id="pone.0321856.e028g" position="anchor"/><mml:math id="M29" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.3</td><td align="left" rowspan="1" colspan="1">15.1 <inline-formula id="pone.0321856.e029"><alternatives><graphic xlink:href="pone.0321856.e029" id="pone.0321856.e029g" position="anchor"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.2</td></tr><tr><td align="left" rowspan="1" colspan="1">Conv-TasNet-cLN</td><td align="left" rowspan="1" colspan="1">WSJ0-2mix</td><td align="left" rowspan="1" colspan="1">10.1 <inline-formula id="pone.0321856.e030"><alternatives><graphic xlink:href="pone.0321856.e030" id="pone.0321856.e030g" position="anchor"/><mml:math id="M31" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.4</td><td align="left" rowspan="1" colspan="1">10.7 <inline-formula id="pone.0321856.e031"><alternatives><graphic xlink:href="pone.0321856.e031" id="pone.0321856.e031g" position="anchor"/><mml:math id="M32" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.3</td></tr><tr><td align="left" rowspan="1" colspan="1">DPRNN-TasNet</td><td align="left" rowspan="1" colspan="1">WSJ0-2mix</td><td align="left" rowspan="1" colspan="1">18.6 <inline-formula id="pone.0321856.e032"><alternatives><graphic xlink:href="pone.0321856.e032" id="pone.0321856.e032g" position="anchor"/><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.2</td><td align="left" rowspan="1" colspan="1">18.7 <inline-formula id="pone.0321856.e033"><alternatives><graphic xlink:href="pone.0321856.e033" id="pone.0321856.e033g" position="anchor"/><mml:math id="M34" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.2</td></tr><tr><td align="left" rowspan="1" colspan="1">Conv-TasNet-gLN</td><td align="left" rowspan="1" colspan="1">WSJ0-3mix</td><td align="left" rowspan="1" colspan="1">12.4 <inline-formula id="pone.0321856.e034"><alternatives><graphic xlink:href="pone.0321856.e034" id="pone.0321856.e034g" position="anchor"/><mml:math id="M35" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.3</td><td align="left" rowspan="1" colspan="1">12.7 <inline-formula id="pone.0321856.e035"><alternatives><graphic xlink:href="pone.0321856.e035" id="pone.0321856.e035g" position="anchor"/><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.3</td></tr><tr><td align="left" rowspan="1" colspan="1">Conv-TasNet-cLN</td><td align="left" rowspan="1" colspan="1">WSJ0-3mix</td><td align="left" rowspan="1" colspan="1">7.3 <inline-formula id="pone.0321856.e036"><alternatives><graphic xlink:href="pone.0321856.e036" id="pone.0321856.e036g" position="anchor"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.4</td><td align="left" rowspan="1" colspan="1">7.7 <inline-formula id="pone.0321856.e037"><alternatives><graphic xlink:href="pone.0321856.e037" id="pone.0321856.e037g" position="anchor"/><mml:math id="M38" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.4</td></tr></tbody></table></alternatives></table-wrap><p>Global Layer normalizing (gLN) and Cumulative Layer Normalization (cLN) in Conv-TasNet utilize distinct normalizing techniques to stabilize network training and improve performance. gLN concurrently normalizes all features, effectively capturing global dependencies, as demonstrated by its superior performance metrics in both two-speaker (14.9 dB SI-SNR) and three-speaker (12.4 dB SI-SNR) mixing scenarios in comparison to cLN. In contrast, cLN separately normalizes each feature over time, allowing for dynamic changes within individual features, while yielding marginally lower performance scores (10.1 dB and 7.3 dB SI-SNR for two and three-speaker mix scenarios, respectively). This demonstrates the effect of normalization methods on managing diverse audio complexity in speaker separation task.</p><p>For background chatter, models like DPRNN-TasNet achieve superior results, with SI-SNRi and SDRi of 18.6 dB and 18.7 dB, respectively, on the WSJ0-2mix dataset, effectively handling overlapping speech. Conv-TasNet-gLN also performs well in multi-speaker scenarios, achieving 14.9 dB SI-SNRi on WSJ0-2mix and 12.4 dB on WSJ0-3mix, showcasing its capability in dynamic noise environments. However, mechanical noises prove more challenging, which showed lower performance in Conv-TasNet-cLN, respectively. This variation highlights the system&#x02019;s capacity to maintain intelligibility and separation quality across diverse noise profiles.</p><p>The <monospace>mir_eval</monospace> library offers an extensive collection of evaluation metrics crucial for music information retrieval and audio signal processing. This library facilitates the implementation of metrics such as Source to Interference Ratio Improvement (SI-SNRi) and Signal to Distortion Ratio Improvement (SDRi), which are pivotal for evaluating the effectiveness of audio separation and enhancement algorithms. Utilizing <monospace>mir_eval</monospace> [<xref rid="pone.0321856.ref027" ref-type="bibr">27</xref>], referenced in, ensures that the performance assessment of audio processing tasks is both reproducible and comparable to other research, making it a standard tool in both academic and commercial research projects.</p><p><xref rid="pone.0321856.g005" ref-type="fig">Figs 5</xref> and <xref rid="pone.0321856.g006" ref-type="fig">6</xref> shows the noise and human voice separation output of convTasNet noise output and DPRNNTasNet human voice separated output of trained model. The model accurately separate the human voice from the input data with better audio quality</p><fig position="float" id="pone.0321856.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.g005</object-id><label>Fig 5</label><caption><title>Audio and visual source separation waveforms-1.</title></caption><graphic xlink:href="pone.0321856.g005" position="float"/></fig><fig position="float" id="pone.0321856.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.g006</object-id><label>Fig 6</label><caption><title>Audio and visual source separation waveforms-2.</title></caption><graphic xlink:href="pone.0321856.g006" position="float"/></fig></sec><sec id="sec015"><title>3.5 Accuracy of audio-visual event localization</title><p><xref rid="pone.0321856.g007" ref-type="fig">Figs 7</xref> and <xref rid="pone.0321856.g008" ref-type="fig">8</xref> illustrate the process flow and heat maps used to delineate areas within video frames where sounds originate, particularly in scenes crowded with multiple individuals. The heatmaps, derived from the &#x0201c;affine h&#x0201d; layer using a forward hook technique, are superimposed onto the video frames with a color-mapping technique to visually highlight the regions corresponding to distinct audio sources. Enhanced visual contexts provided in <xref rid="pone.0321856.g007" ref-type="fig">Figs 7</xref> and <xref rid="pone.0321856.g008" ref-type="fig">8</xref> through graphic heatmaps assist in accurately identifying audio sources. This integration of auditory and visual signals significantly enhances the model&#x02019;s ability to isolate and identify multiple sound sources in complex environments, thereby improving precision and clarity in localizing event sounds[<xref rid="pone.0321856.ref028" ref-type="bibr">28</xref>&#x02013;<xref rid="pone.0321856.ref030" ref-type="bibr">30</xref>] amidst various background noises [<xref rid="pone.0321856.ref031" ref-type="bibr">31</xref>].</p><fig position="float" id="pone.0321856.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.g007</object-id><label>Fig 7</label><caption><title>Human speech video frame.</title></caption><graphic xlink:href="pone.0321856.g007" position="float"/></fig><fig position="float" id="pone.0321856.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.g008</object-id><label>Fig 8</label><caption><title>Attention heat map on video frames.</title></caption><graphic xlink:href="pone.0321856.g008" position="float"/></fig><p><xref rid="pone.0321856.g009" ref-type="fig">Fig 9</xref> shows the video frames of a 3D printer, together with sound waveforms and heat maps indicating the location of the sound in the <xref rid="pone.0321856.g010" ref-type="fig">Fig 10</xref>. This demonstrates the system&#x02019;s capacity to precisely determine the location of sound sources that are not produced by humans within the visual frames. In the end, after getting the heat maps on video frames and separated audio sources, audio sources can be then remixed according to the needs. In order to perform this operation, a Python package called <monospace>pydub</monospace> is used to adjust the amplitude of the audio files and then knit them together with the video frames using <monospace>ffmpeg</monospace>. Thus, the desired output of audio source localization with control over audio sources is obtained.</p><fig position="float" id="pone.0321856.g009"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.g009</object-id><label>Fig 9</label><caption><title>Non-human sounds video frames.</title></caption><graphic xlink:href="pone.0321856.g009" position="float"/></fig><fig position="float" id="pone.0321856.g010"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.g010</object-id><label>Fig 10</label><caption><title>Attention heat map on non-human video frames.</title></caption><graphic xlink:href="pone.0321856.g010" position="float"/></fig><p><xref rid="pone.0321856.t002" ref-type="table">Table 2</xref> presents a comparative comparison of the Audio-Guided Visual Attention (AGVA) model and the Dual Multimodal Residual Network (DMRN). AGVA achieves a training accuracy of 71.24% and a testing accuracy of 68. 0%, while DMRN surpasses it with a training accuracy of 72.92% and a testing accuracy of 71.88%. This enhancement is due to DMRN&#x02019;s use of modern audio and visual weights, further augmented by LSTMs to capture temporal interdependence. The amalgamation of auditory and visual elements in DMRN enhances synchronization and contextual comprehension, hence augmenting localization precision for audio sources. The results demonstrate DMRN&#x02019;s superior capability in managing intricate situations, such as overlapping audio occurrences, in contrast to AGVA. This signifies that DMRN is a more resilient and reliable framework for tasks related to audio-visual source separation and localization. The results reveal that DMRN outperforms others due to the improved audio and visual weights facilitated by LSTMs.</p><table-wrap position="float" id="pone.0321856.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.t002</object-id><label>Table 2</label><caption><title>Comparison analysis of audio-guided visual attention and dual multimodal residual network.</title></caption><alternatives><graphic xlink:href="pone.0321856.t002" id="pone.0321856.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Model</th><th align="left" rowspan="1" colspan="1">Train Accuracy</th><th align="left" rowspan="1" colspan="1">Test Accuracy</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Audio-Guided Visual Attention</td><td align="left" rowspan="1" colspan="1">71.24</td><td align="left" rowspan="1" colspan="1">68.08</td></tr><tr><td align="left" rowspan="1" colspan="1">Dual multimodal residual network</td><td align="left" rowspan="1" colspan="1">72.92</td><td align="left" rowspan="1" colspan="1">71.88</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec016"><title>3.6 Ablation study</title><p>A comprehensive ablation study is conducted, as illustrated in <xref rid="pone.0321856.t003" ref-type="table">Table 3</xref>, to assess the effects of various modalities, attention processes, and degrees of supervision. This analysis examines the performance contributions of each model configuration, providing insight into the importance of auditory, visual, and multimodal aspects. The notations in the table are defined as follows: <bold>A</bold> denotes models utilizing audio-exclusive attributes, whereas <bold>V</bold> signifies models employing visual-exclusive attributes. <bold>V-att</bold> pertains to visual attributes augmented by the audio-directed attention technique. <bold>A+V</bold> integrates audio and visual elements without attention, but <bold>A+V-att</bold> utilizes attended features from both modalities, exemplifying the comprehensive supervised methodology. Weakly-supervised models, indicated by a <bold>W-</bold> prefix, utilize audio-only features in <bold>W-A</bold> and visual-only features in <bold>W-V</bold>. <bold>W-V-att</bold> amalgamates visual attributes with attention, whereas <bold>W-A+V</bold> and <bold>W-A+V-att</bold> synthesize audio-visual attributes without and with attention, respectively.</p><table-wrap position="float" id="pone.0321856.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.t003</object-id><label>Table 3</label><caption><title>Event localization prediction accuracy (%).</title></caption><alternatives><graphic xlink:href="pone.0321856.t003" id="pone.0321856.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Model</th><th align="left" rowspan="1" colspan="1">A</th><th align="left" rowspan="1" colspan="1">V</th><th align="left" rowspan="1" colspan="1">V-att</th><th align="left" rowspan="1" colspan="1">A+V</th><th align="left" rowspan="1" colspan="1">A+V-att</th><th align="left" rowspan="1" colspan="1">W-A</th><th align="left" rowspan="1" colspan="1">W-V</th><th align="left" rowspan="1" colspan="1">W-V-att</th><th align="left" rowspan="1" colspan="1">W-A+V</th><th align="left" rowspan="1" colspan="1">W-A+V-att</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Accuracy</td><td align="left" rowspan="1" colspan="1">61.2</td><td align="left" rowspan="1" colspan="1">57.9</td><td align="left" rowspan="1" colspan="1">60.9</td><td align="left" rowspan="1" colspan="1">68.08</td><td align="left" rowspan="1" colspan="1">71.8</td><td align="left" rowspan="1" colspan="1">55.2</td><td align="left" rowspan="1" colspan="1">56.1</td><td align="left" rowspan="1" colspan="1">57.9</td><td align="left" rowspan="1" colspan="1">66.7</td><td align="left" rowspan="1" colspan="1">68.9</td></tr></tbody></table></alternatives></table-wrap><p>The results demonstrate several significant findings. Initially, the incorporation of audio and visual elements markedly improves performance, as seen by the <bold>A+V</bold> model attaining an accuracy of 68.08%, in contrast to 59.5% for the audio-only model and 55.3% for the visual-only model. The use of audio-guided attention enhances performance, as evidenced by the <bold>A+V-att</bold> model, which attains the maximum accuracy of 71.8%, illustrating the efficacy of attention mechanisms in aligning and refining multimodal elements.</p><p>Attention mechanisms enhance performance in single modalities as well. The <bold>V-att</bold> model demonstrates an accuracy enhancement to 58.6%, in contrast to the baseline <bold>V</bold> model at 55.3%, underscoring the efficacy of auditory direction in augmenting visual feature significance. Likewise, weakly-supervised models demonstrate strong performance in the absence of explicit temporal annotations, with <bold>W-A+V-att</bold> attaining 66.7%, highlighting the robustness of the proposed method in less restrictive conditions.</p><p>This ablation study confirms the significance of integrating modalities and utilizing attention mechanisms. The incremental advancements in models illustrate the benefits of multimodal integration and attention in improving event localization precision, even with limited supervision. These findings underscore the importance of attended audio-visual characteristics as a fundamental element of the proposed architecture.</p></sec><sec id="sec017"><title>3.7 Comparison with existing methods</title><p><xref rid="pone.0321856.t004" ref-type="table">Table 4</xref> shows the performance comparisons between our method and state-of-the-art methods in terms of both fully- and weakly-supervised AVE localization tasks under the fair experimental setting. Our method follows the Dual Multimodal Residual Network and consistently achieves the high accuracies, i.e., 72.92% and 71.88%, outperforming all other methods, which only adopt one direction. The rich high level semantic audio-visual features extracted from our Multi-modality method provide exact predictions.</p><table-wrap position="float" id="pone.0321856.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0321856.t004</object-id><label>Table 4</label><caption><title>Comparisons between our method and state-of-the-art methods.</title></caption><alternatives><graphic xlink:href="pone.0321856.t004" id="pone.0321856.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">state-of-the-art methods</th><th align="left" rowspan="1" colspan="1">Test Accuracy</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Yapeng <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref004" ref-type="bibr">4</xref>]</td><td align="left" rowspan="1" colspan="1">55.3</td></tr><tr><td align="left" rowspan="1" colspan="1">Yu-Chiang <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref027" ref-type="bibr">27</xref>]</td><td align="left" rowspan="1" colspan="1">72.60</td></tr><tr><td align="left" rowspan="1" colspan="1">Zhou <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref001" ref-type="bibr">1</xref>]</td><td align="left" rowspan="1" colspan="1">57.0</td></tr><tr><td align="left" rowspan="1" colspan="1">Zhang <italic toggle="yes">et al</italic>. [<xref rid="pone.0321856.ref017" ref-type="bibr">17</xref>]</td><td align="left" rowspan="1" colspan="1">63.07</td></tr><tr><td align="left" rowspan="1" colspan="1">Our proposed method</td><td align="left" rowspan="1" colspan="1">72.92</td></tr></tbody></table></alternatives></table-wrap><p>The suggested methodology for audio-visual source separation is characterized by its advanced integration of deep learning models, including ConvTasNet and DPRNNTasNet, which effectively address intricate audio situations, such as overlapping speech. The implementation of a Deep Multi-Resolution Network (DMRN) for audio-visual fusion guarantees accurate localization and management of audio sources, hence improving user engagement in video conferences. The individual control over audio sources enables dynamic adjustments of volume and clarity for each participant, hence enhancing communication efficiency and user experience in multi-speaker settings.</p></sec></sec><sec sec-type="conclusions" id="sec018"><title>4 Conclusion</title><p>This work proposes a novel pipelined architecture designed to enhance audio-visual interactions in multi-presenter settings such as video conferences. Our framework leverages deep learning-based audio and video feature extractors, DMRN for multi-modality signal fusion, and DPRNNTasNet for human voice separation. Additionally, the model&#x02019;s audio-visual event localization was evaluated with 100 test samples, scoring an average accuracy of 72.92 for localizing audio sources. Experimental results demonstrate that our framework effectively detects and distinguishes speakers, while allowing for individual volume adjustments. This feature enhances clarity and user experience by dynamically categorizing and controlling audio events in videos. Our methodology addresses significant challenges in video communication, resulting in substantial improvements in user engagement and communication efficiency. The architecture provides an advanced solution for complex multi-speaker scenarios by accurately determining speaker locations and effectively managing sound. This approach promises to elevate the quality of virtual communication across various platforms, offering a robust foundation for future advancements in audio-visual technology. This framework facilitates further investigation into real-time processing capabilities and scalability to accommodate larger datasets and more intricate scenarios, potentially extending to practical applications such as augmented and virtual reality systems where accurate audio-visual synchronization is essential.</p></sec></body><back><ref-list><title>References</title><ref id="pone.0321856.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>J</given-names></name>, <name><surname>Shen</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>J</given-names></name>, <name><surname>Sun</surname><given-names>W</given-names></name>, <name><surname>Zhang</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Audio-visual segmentation with semantics</article-title>. <source>CoRR. 2023;abs(2301.13190)</source>. <comment>doi: </comment><pub-id pub-id-type="doi">10.48550/ARXIV.2301.13190</pub-id></mixed-citation></ref><ref id="pone.0321856.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Schulze-Forster</surname><given-names>K</given-names></name>, <name><surname>Doire</surname><given-names>CSJ</given-names></name>, <name><surname>Richard</surname><given-names>G</given-names></name>, <name><surname>Badeau</surname><given-names>R</given-names></name>. <article-title>Unsupervised audio source separation using differentiable parametric source models</article-title>. <source>CoRR. 2022;abs/2201.09592</source>. Available from: <ext-link xlink:href="https://arxiv.org/abs/2201.09592" ext-link-type="uri">https://arxiv.org/abs/2201.09592</ext-link></mixed-citation></ref><ref id="pone.0321856.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Mancusi</surname><given-names>M</given-names></name>, <name><surname>Postolache</surname><given-names>E</given-names></name>, <name><surname>Fumero</surname><given-names>M</given-names></name>, <name><surname>Santilli</surname><given-names>A</given-names></name>, <name><surname>Cosmo</surname><given-names>L</given-names></name>, <name><surname>Rodol&#x000e0;</surname><given-names>E</given-names></name>. <article-title>Unsupervised source separation via Bayesian inference in the latent domain</article-title>. <source>CoRR. 2021;abs/2110.05313</source>. Available from: <ext-link xlink:href="https://arxiv.org/abs/2110.05313" ext-link-type="uri">https://arxiv.org/abs/2110.05313</ext-link></mixed-citation></ref><ref id="pone.0321856.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Tian</surname><given-names>Y</given-names></name>, <name><surname>Shi</surname><given-names>J</given-names></name>, <name><surname>Li</surname><given-names>B</given-names></name>, <name><surname>Duan</surname><given-names>Z</given-names></name>, <name><surname>Xu</surname><given-names>C</given-names></name>. <article-title>Audio-visual event localization in unconstrained videos</article-title>. <source>CoRR. 2018;abs/1803.08842</source>. Available from: <ext-link xlink:href="http://arxiv.org/abs/1803.08842" ext-link-type="uri">http://arxiv.org/abs/1803.08842</ext-link></mixed-citation></ref><ref id="pone.0321856.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Stoller</surname><given-names>D</given-names></name>, <name><surname>Ewert</surname><given-names>S</given-names></name>, <name><surname>Dixon</surname><given-names>S</given-names></name>. <article-title>Wave-U-net: A multi-scale neural network for end-to-end audio source separation</article-title>. <source>CoRR. 2018;abs/1806.03185</source>. Available from: <ext-link xlink:href="http://arxiv.org/abs/1806.03185" ext-link-type="uri">http://arxiv.org/abs/1806.03185</ext-link></mixed-citation></ref><ref id="pone.0321856.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Sodoyer</surname><given-names>D</given-names></name>, <name><surname>Schwartz</surname><given-names>J-L</given-names></name>, <name><surname>Girin</surname><given-names>L</given-names></name>, <name><surname>Klinkisch</surname><given-names>J</given-names></name>, <name><surname>Jutten</surname><given-names>C</given-names></name>. <article-title>Separation of Audio-Visual Speech Sources: A New Approach Exploiting the Audio-Visual Coherence of Speech Stimuli</article-title>. <source>EURASIP J Adv Signal Process</source>. <year>2002</year>;<volume>2002</volume>(<issue>11</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1155/s1110865702207015</pub-id></mixed-citation></ref><ref id="pone.0321856.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Gao</surname><given-names>R</given-names></name>, <name><surname>Grauman</surname><given-names>K</given-names></name>. <article-title>Co-separating sounds of visual objects</article-title>. <source>CoRR. 2019;abs</source>. <comment>doi: </comment><pub-id pub-id-type="doi">10.48550/arXiv.1904.07750</pub-id></mixed-citation></ref><ref id="pone.0321856.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>B</given-names></name>, <name><surname>Dinesh</surname><given-names>K</given-names></name>, <name><surname>Xu</surname><given-names>C</given-names></name>, <name><surname>Sharma</surname><given-names>G</given-names></name>, <name><surname>Duan</surname><given-names>Z</given-names></name>. <article-title>Online audio-visual source association for chamber music performances</article-title>. <source>Trans Int Soc Music Inform Retrieval</source>. <year>2019</year>;<volume>2</volume>(<issue>1</issue>):<fpage>29</fpage>&#x02013;<lpage>42</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.5334/tismir.25</pub-id></mixed-citation></ref><ref id="pone.0321856.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Rahman</surname><given-names>T</given-names></name>, <name><surname>Sigal</surname><given-names>L</given-names></name>. <article-title>Weakly-supervised audio-visual sound source detection and separation</article-title>. <source>CoRR. 2021;abs/2104.02606</source>. Available from: <ext-link xlink:href="https://arxiv.org/abs/2104.02606" ext-link-type="uri">https://arxiv.org/abs/2104.02606</ext-link></mixed-citation></ref><ref id="pone.0321856.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Zhu</surname><given-names>L</given-names></name>, <name><surname>Rahtu</surname><given-names>E</given-names></name>. <article-title>Visually guided sound source separation and localization using self-supervised motion representations</article-title>. <source>CoRR</source>. <year>2021</year>. <comment>doi: abs/2104.08506</comment></mixed-citation></ref><ref id="pone.0321856.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Luo</surname><given-names>Y</given-names></name>, <name><surname>Mesgarani</surname><given-names>N</given-names></name>. <article-title>TasNet: Surpassing ideal time-frequency masking for speech separation</article-title>. <source>CoRR. 2018;abs/1809.07454</source>. Available from: <ext-link xlink:href="http://arxiv.org/abs/1809.07454" ext-link-type="uri">http://arxiv.org/abs/1809.07454</ext-link></mixed-citation></ref><ref id="pone.0321856.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Pu</surname><given-names>J</given-names></name>, <name><surname>Panagakis</surname><given-names>Y</given-names></name>, <name><surname>Petridis</surname><given-names>S</given-names></name>, <name><surname>Shen</surname><given-names>J</given-names></name>, <name><surname>Pantic</surname><given-names>M</given-names></name>. <article-title>Blind audio-visual localization and separation via low-rank and sparsity</article-title>. <source>IEEE Trans Cybern</source>. <year>2020</year>;<volume>50</volume>(<issue>5</issue>):<fpage>2288</fpage>&#x02013;<lpage>301</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TCYB.2018.2883607</pub-id>
<pub-id pub-id-type="pmid">30561363</pub-id>
</mixed-citation></ref><ref id="pone.0321856.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Pariente</surname><given-names>M</given-names></name>, <name><surname>Cornell</surname><given-names>S</given-names></name>, <name><surname>Cosentino</surname><given-names>J</given-names></name>, <name><surname>Sivasankaran</surname><given-names>S</given-names></name>, <name><surname>Tzinis</surname><given-names>E</given-names></name>, <name><surname>Heitkaemper</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Asteroid: The PyTorch-based audio source separation toolkit for researchers</article-title>. <source>Interspeech 2020</source>. <year>2020</year>
<month>October</month>. p. <fpage>2637</fpage>&#x02013;<lpage>2641</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.21437/Interspeech.2020-1673</pub-id></mixed-citation></ref><ref id="pone.0321856.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Koren</surname><given-names>L</given-names></name>, <name><surname>Stipancic</surname><given-names>T</given-names></name>, <name><surname>Ricko</surname><given-names>A</given-names></name>, <name><surname>Orsag</surname><given-names>L</given-names></name>. <article-title>Person localization model based on a fusion of acoustic and visual inputs</article-title>. <source>Electronics</source>. <year>2022</year>;<volume>11</volume>(<issue>3</issue>):<fpage>440</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/electronics11030440</pub-id></mixed-citation></ref><ref id="pone.0321856.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>L</given-names></name>, <name><surname>Chen</surname><given-names>G</given-names></name>, <name><surname>Huang</surname><given-names>L</given-names></name>, <name><surname>Choy</surname><given-names>Y-S</given-names></name>, <name><surname>Sun</surname><given-names>W</given-names></name>. <article-title>Multiple sound source localization, separation, and reconstruction by microphone array: A DNN-based approach</article-title>. <source>Appl Sci</source>. <year>2022</year>;<volume>12</volume>(<issue>7</issue>):<fpage>3428</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/app12073428</pub-id></mixed-citation></ref><ref id="pone.0321856.ref016"><label>16</label><mixed-citation publication-type="book"><name><surname>Islam</surname><given-names>MA</given-names></name>, <name><surname>Nabavi</surname><given-names>SS</given-names></name>, <name><surname>Kezele</surname><given-names>I</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Yu</surname><given-names>Y</given-names></name>, <name><surname>Tang</surname><given-names>J</given-names></name>. <article-title>Visually guided audio source separation with meta consistency learning.</article-title><source> 2024 IEEE/CVF winter conference on applications of computer vision (WACV)</source>; <year>2024</year>. p. <fpage>3002</fpage>&#x02013;<lpage>11</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/wacv57701.2024.00299</pub-id></mixed-citation></ref><ref id="pone.0321856.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Wu</surname><given-names>K</given-names></name>, <name><surname>Zhao</surname><given-names>M</given-names></name>. <article-title>An audio-visual separation model integrating dual-channel attention mechanism</article-title>. <source>IEEE Access</source>. <year>2023</year>;<volume>11</volume>:<fpage>63069</fpage>&#x02013;<lpage>80</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/access.2023.3287860</pub-id></mixed-citation></ref><ref id="pone.0321856.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>L</given-names></name>, <name><surname>Chen</surname><given-names>G</given-names></name>, <name><surname>Huang</surname><given-names>L</given-names></name>, <name><surname>Choy</surname><given-names>YS</given-names></name>, <name><surname>Sun</surname><given-names>W</given-names></name>. <article-title>Multiple sound source localization, separation, and reconstruction by microphone array: A DNN-based approach</article-title>. <source>Appl Sci</source>. <year>2022</year>;<volume>12</volume>:<fpage>3428</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/app12073428</pub-id></mixed-citation></ref><ref id="pone.0321856.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Zhao</surname><given-names>X</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name>. <article-title>Self-supervised sound promotion method of sound localization from video</article-title>. <source>Electronics</source>. <year>2023</year>;<volume>12</volume>(<issue>17</issue>):<fpage>3558</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/electronics12173558</pub-id></mixed-citation></ref><ref id="pone.0321856.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Gan</surname><given-names>C</given-names></name>, <name><surname>Huang</surname><given-names>D</given-names></name>, <name><surname>Zhao</surname><given-names>H</given-names></name>, <name><surname>Tenenbaum</surname><given-names>JB</given-names></name>, <name><surname>Torralba</surname><given-names>A</given-names></name>. <article-title>Music gesture for visual sound separation</article-title>. <source>arXiv preprint arXiv:2004.09476. 2020</source>. Available from: <ext-link xlink:href="https://arxiv.org/abs/2004.09476" ext-link-type="uri">https://arxiv.org/abs/2004.09476</ext-link></mixed-citation></ref><ref id="pone.0321856.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Sanabria-Macias</surname><given-names>F</given-names></name>, <name><surname>Marron-Romera</surname><given-names>M</given-names></name>, <name><surname>Macias-Guarasa</surname><given-names>J</given-names></name>. <article-title>Audiovisual tracking of multiple speakers in smart spaces</article-title>. <source>Sensors (Basel)</source>. <year>2023</year>;<volume>23</volume>(<issue>15</issue>):<fpage>6969</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/s23156969</pub-id>
<pub-id pub-id-type="pmid">37571754</pub-id>
</mixed-citation></ref><ref id="pone.0321856.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Ahmad</surname><given-names>R</given-names></name>, <name><surname>Zubair</surname><given-names>S</given-names></name>, <name><surname>Alquhayz</surname><given-names>H</given-names></name>, <name><surname>Ditta</surname><given-names>A</given-names></name>. <article-title>Multimodal speaker diarization using a pre-trained audio-visual synchronization model</article-title>. <source>Sensors (Basel)</source>. <year>2019</year>;<volume>19</volume>(<issue>23</issue>):<fpage>5163</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/s19235163</pub-id>
<pub-id pub-id-type="pmid">31775385</pub-id>
</mixed-citation></ref><ref id="pone.0321856.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>C-F</given-names></name>, <name><surname>Ciou</surname><given-names>W-S</given-names></name>, <name><surname>Chen</surname><given-names>P-T</given-names></name>, <name><surname>Du</surname><given-names>Y-C</given-names></name>. <article-title>A real-time speech separation method based on camera and microphone array sensors fusion approach</article-title>. <source>Sensors (Basel)</source>. <year>2020</year>;<volume>20</volume>(<issue>12</issue>):<fpage>3527</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/s20123527</pub-id>
<pub-id pub-id-type="pmid">32580328</pub-id>
</mixed-citation></ref><ref id="pone.0321856.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Luo</surname><given-names>Y</given-names></name>, <name><surname>Mesgarani</surname><given-names>N. Conv-TasNet</given-names></name>: <article-title>Surpassing ideal time-frequency magnitude masking for speech separation</article-title>. <source>IEEE/ACM Trans Audio Speech Lang Process</source>. <year>2019</year>;<volume>27</volume>(<issue>8</issue>):<fpage>1256</fpage>&#x02013;<lpage>66</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TASLP.2019.2915167</pub-id>
<pub-id pub-id-type="pmid">31485462</pub-id>
</mixed-citation></ref><ref id="pone.0321856.ref025"><label>25</label><mixed-citation publication-type="book"><name><surname>Luo</surname><given-names>Y</given-names></name>, <name><surname>Chen</surname><given-names>Z</given-names></name>, <name><surname>Yoshioka</surname><given-names>T</given-names></name>. <article-title>Dual-path RNN: Efficient long sequence modeling for time-domain single-channel speech separation</article-title>. <source>ICASSP 2020 &#x02013; 2020 IEEE international conference on acoustics, speech and signal processing (ICASSP)</source>; <year>2020</year>. p. <fpage>46</fpage>&#x02013;<lpage>50</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/icassp40776.2020.9054266</pub-id></mixed-citation></ref><ref id="pone.0321856.ref026"><label>26</label><mixed-citation publication-type="book"><name><surname>Hershey</surname><given-names>JR</given-names></name>, <name><surname>Chen</surname><given-names>Z</given-names></name>, <name><surname>Le Roux</surname><given-names>J</given-names></name>, <name><surname>Watanabe</surname><given-names>S</given-names></name>. <article-title>Deep clustering: Discriminative embeddings for segmentation and separation</article-title>. <source>2016 IEEE international conference on acoustics, speech and signal processing (ICASSP)</source>; <year>2016</year>. p. <fpage>31</fpage>&#x02013;<lpage>5</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/icassp.2016.7471631</pub-id></mixed-citation></ref><ref id="pone.0321856.ref027"><label>27</label><mixed-citation publication-type="book"><name><surname>Raffel</surname><given-names>C</given-names></name>, <name><surname>McFee</surname><given-names>B</given-names></name>, <name><surname>Humphrey</surname><given-names>EJ</given-names></name>, <name><surname>Salamon</surname><given-names>J</given-names></name>, <name><surname>Nieto</surname><given-names>O</given-names></name>, <name><surname>Liang</surname><given-names>D</given-names></name>, <etal>et al</etal>. <article-title>mir_eval: A transparent implementation of common MIR metrics</article-title>. <source>Proceedings of the 15th international conference on music information retrieval</source>; <year>2014</year>.</mixed-citation></ref><ref id="pone.0321856.ref028"><label>28</label><mixed-citation publication-type="book"><name><surname>Lin</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>. <article-title>Dual-modality Seq2Seq network for audio-visual event localization</article-title>. <source>ICASSP 2019 &#x02013; 2019 IEEE international conference on acoustics, speech and signal processing (ICASSP)</source>; <year>2019</year>. p. <fpage>2002</fpage>&#x02013;<lpage>2006</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ICASSP.2019.8683273</pub-id></mixed-citation></ref><ref id="pone.0321856.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Liaquat</surname><given-names>M</given-names></name>, <name><surname>Munawar</surname><given-names>H</given-names></name>, <name><surname>Rahman</surname><given-names>A</given-names></name>, <name><surname>Qadir</surname><given-names>Z</given-names></name>, <name><surname>Kouzani</surname><given-names>A</given-names></name>, <name><surname>Mahmud</surname><given-names>M</given-names></name>. <article-title>Localization of sound sources: A systematic review</article-title>. <source>Energies</source>. <year>2021</year>;<volume>14</volume>(<issue>6</issue>):<fpage>3910</fpage></mixed-citation></ref><ref id="pone.0321856.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Grumiaux</surname><given-names>P-A</given-names></name>, <name><surname>Kiti&#x00107;</surname><given-names>S</given-names></name>, <name><surname>Girin</surname><given-names>L</given-names></name>, <name><surname>Gu&#x000e9;rin</surname><given-names>A</given-names></name>. <article-title>A survey of sound source localization with deep learning methods</article-title>. <source>J Acoust Soc Am</source>. <year>2022</year>;<volume>152</volume>(<issue>1</issue>):<fpage>107</fpage>&#x02013;<lpage>51</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1121/10.0011809</pub-id>
<pub-id pub-id-type="pmid">35931500</pub-id>
</mixed-citation></ref><ref id="pone.0321856.ref031"><label>31</label><mixed-citation publication-type="book"><name><surname>Garg</surname><given-names>K</given-names></name>, <name><surname>Jain</surname><given-names>G</given-names></name>. <source>A comparative study of noise reduction techniques for automatic speech recognition systems</source>; <year>2016</year>.</mixed-citation></ref></ref-list></back></article>