<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">J Eye Mov Res</journal-id><journal-id journal-id-type="iso-abbrev">J Eye Mov Res</journal-id><journal-id journal-id-type="publisher-id">jemr</journal-id><journal-title-group><journal-title>Journal of Eye Movement Research</journal-title></journal-title-group><issn pub-type="epub">1995-8692</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC12101172</article-id><article-id pub-id-type="doi">10.3390/jemr18030017</article-id><article-id pub-id-type="publisher-id">jemr-18-00017</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>On the Validity and Benefit of Manual and Automated Drift Correction in Reading Tasks</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4551-3080</contrib-id><name><surname>Al Madi</surname><given-names>Naser</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Vitu</surname><given-names>Fran&#x000e7;oise</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-jemr-18-00017">Department of Computer Science, Colby College, Waterville, ME 04901, USA; <email>nsalmadi@colby.edu</email></aff><pub-date pub-type="epub"><day>09</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>6</month><year>2025</year></pub-date><volume>18</volume><issue>3</issue><elocation-id>17</elocation-id><history><date date-type="received"><day>09</day><month>2</month><year>2025</year></date><date date-type="rev-recd"><day>22</day><month>4</month><year>2025</year></date><date date-type="accepted"><day>29</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the author.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Drift represents a common distortion that affects the position of fixations in eye tracking data. While manual correction is considered very accurate, it is considered subjective and time-consuming. On the other hand, automated correction is fast, objective, and considered less accurate. An objective comparison of the accuracy of manual and automated correction has not been conducted before, and the extent of subjectivity in manual correction is not entirely quantified. In this paper, we compare the accuracy of manual and automated correction of eye tracking data in reading tasks through a novel approach that relies on synthetic data with known ground truth. Moreover, we quantify the subjectivity in manual human correction with real eye tracking data. Our results show that expert human correction is significantly more accurate than automated algorithms, yet novice human correctors are on par with the best automated algorithms. In addition, we found that human correctors show excellent agreement in their correction, challenging the notion that manual correction is &#x0201c;highly subjective&#x0201d;. Our findings provide unique insights, quantifying the benefits of manual and automated correction.</p></abstract><kwd-group><kwd>eye tracking</kwd><kwd>reading</kwd><kwd>correction</kwd><kwd>drift</kwd><kwd>automated-algorithms</kwd><kwd>distortion</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-jemr-18-00017"><title>1. Introduction</title><p>Eye tracking has been an important methodology in the study of reading natural language text [<xref rid="B1-jemr-18-00017" ref-type="bibr">1</xref>], source code [<xref rid="B2-jemr-18-00017" ref-type="bibr">2</xref>], and music [<xref rid="B3-jemr-18-00017" ref-type="bibr">3</xref>]. In reading research, eye movement is often studied on the word level, requiring higher temporal and spatial accuracy. Temporal accuracy is associated with the sampling frequency of the eye tracker, and it indicates the amount of eye movement data recorded relative to the data lost during recording [<xref rid="B4-jemr-18-00017" ref-type="bibr">4</xref>]. Spatial accuracy, on the other hand, is measured in degrees of visual angle between the real fixation position and the detected fixation position. The variation between the detected gaze location and the actual gaze location is called <italic toggle="yes"><bold>drift</bold></italic> [<xref rid="B5-jemr-18-00017" ref-type="bibr">5</xref>]. Many factors could influence data quality in eye tracking studies, including the following:<list list-type="order"><list-item><p><bold>Eye-tracking equipment:</bold> The type resolution and frequency of the eye tracker can influence drift [<xref rid="B4-jemr-18-00017" ref-type="bibr">4</xref>,<xref rid="B6-jemr-18-00017" ref-type="bibr">6</xref>]. For example, remote eye trackers are more susceptible to error compared to tower-mounted eye trackers. In addition, the resolution of the eye tracker camera could limit the detection of pupil movement. Eye tracker frequency determines the smallest eye movement event that can be detected, and if the frequency is low, some samples could be lost, resulting in inaccuracy in the detected fixation position.</p></list-item><list-item><p><bold>Pupil-size changes:</bold> Most camera-based eye-trackers rely on pupil corneal reflection in detecting the fixation position on the screen. Recently, Hooge et al. [<xref rid="B7-jemr-18-00017" ref-type="bibr">7</xref>] found that changes in the pupil size significantly affect the accuracy of the detected fixation position.</p></list-item><list-item><p><bold>Participant differences:</bold> Holmqvist et al. [<xref rid="B8-jemr-18-00017" ref-type="bibr">8</xref>] reported that differences and variations in physiology, neurology, and psychology between participants may influence data quality depending on how the eye model used by the system accounts for these variations. Typically, eye tracking systems rely on models that interpret eye position and movement based on IR-light reflection. These models often assume certain standard eye characteristics, but differences in the physical structure of the eye and surrounding anatomy such as eye color, wearing glasses, contact lenses, eyelid shape, eyelash interference can deviate from model assumptions. Similarly, children and adults may show different patterns of oculomotor control, reflecting neurological development [<xref rid="B9-jemr-18-00017" ref-type="bibr">9</xref>]. In addition, differences in attention, motivation, or mental state can translate to variations in eye tracking data quality. For example, a fatigued participant may blink more, have decreased saccadic velocity, or have longer reaction times [<xref rid="B10-jemr-18-00017" ref-type="bibr">10</xref>,<xref rid="B11-jemr-18-00017" ref-type="bibr">11</xref>].</p></list-item><list-item><p><bold>Operator skill:</bold> More experienced eye tracker operators are often able to record data with higher accuracy [<xref rid="B8-jemr-18-00017" ref-type="bibr">8</xref>].</p></list-item><list-item><p><bold>Eye and eye-tracker positioning:</bold> Adjusting the distance from the eye tracker, infra-red intensity, and the positioning of the monitor and eye tracker can play a role in the quality of the data [<xref rid="B8-jemr-18-00017" ref-type="bibr">8</xref>]. Generally, eye trackers measure the angle of gaze to estimate the gaze position on the screen, and this angle is dependent on the distance between the participant and screen and the dimensions of the screen. Most eye trackers have a limit to the range of eye movement angles they can accurately track. For example, many commercial eye trackers can reliably track movements within &#x000b1;20&#x02013;30 degrees of the center (gaze straight ahead).</p></list-item><list-item><p><bold>Calibration:</bold> Difficulties in calibration [<xref rid="B12-jemr-18-00017" ref-type="bibr">12</xref>], in addition to calibration degradation over time, result in lower data quality.</p></list-item><list-item><p><bold>Head movement:</bold> Free head movement during the experiment can negatively influence calibration accuracy, and subsequently, data quality [<xref rid="B12-jemr-18-00017" ref-type="bibr">12</xref>].</p></list-item><list-item><p><bold>Lighting changes:</bold> Changes in room lighting could influence pupil size and the detection of corneal reflection resulting in lowered accuracy [<xref rid="B4-jemr-18-00017" ref-type="bibr">4</xref>,<xref rid="B12-jemr-18-00017" ref-type="bibr">12</xref>].</p></list-item></list></p><p>During reading, drift can move fixations from one word to another or one line to another, influencing research findings. <xref rid="jemr-18-00017-f001" ref-type="fig">Figure 1</xref> shows a real eye tracking trial from Carr et al. [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>], where the position of fixations drifts from the intended words, illustrating how drift can be detrimental to recording quality. Poor data quality can influence key experimental measures, leading to the misidentification of group differences and eye-movement patterns [<xref rid="B14-jemr-18-00017" ref-type="bibr">14</xref>,<xref rid="B15-jemr-18-00017" ref-type="bibr">15</xref>]. In reading research in particular, it was found that poor quality in fixation positions can falsely produce nonexistent eye-movement effects [<xref rid="B16-jemr-18-00017" ref-type="bibr">16</xref>].</p><p>Drift can be identified and manually corrected, which is often considered the most accurate approach to correcting eye tracking data in reading [<xref rid="B5-jemr-18-00017" ref-type="bibr">5</xref>,<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>,<xref rid="B17-jemr-18-00017" ref-type="bibr">17</xref>,<xref rid="B18-jemr-18-00017" ref-type="bibr">18</xref>]. Yet manual correction is often considered &#x0201c;subjective&#x0201d; and &#x0201c;not necessarily reproducible&#x0201d; [<xref rid="B19-jemr-18-00017" ref-type="bibr">19</xref>,<xref rid="B20-jemr-18-00017" ref-type="bibr">20</xref>]. Therefore, one or two human correctors often correct the data collaboratively, relying on a visualization similar to <xref rid="jemr-18-00017-f001" ref-type="fig">Figure 1</xref> to move fixations to the intended word. This process relies on the experience of the corrector, and two correctors share this responsibility to account for subjective differences between correctors (subjectivity).</p><p>Manual correction is time-consuming and is considered a laborious task, even for a relatively small number of eye tracking trials. Therefore, many automated algorithms have been proposed over the years [<xref rid="B12-jemr-18-00017" ref-type="bibr">12</xref>,<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>,<xref rid="B18-jemr-18-00017" ref-type="bibr">18</xref>,<xref rid="B21-jemr-18-00017" ref-type="bibr">21</xref>,<xref rid="B22-jemr-18-00017" ref-type="bibr">22</xref>,<xref rid="B23-jemr-18-00017" ref-type="bibr">23</xref>,<xref rid="B24-jemr-18-00017" ref-type="bibr">24</xref>,<xref rid="B25-jemr-18-00017" ref-type="bibr">25</xref>,<xref rid="B26-jemr-18-00017" ref-type="bibr">26</xref>,<xref rid="B27-jemr-18-00017" ref-type="bibr">27</xref>,<xref rid="B28-jemr-18-00017" ref-type="bibr">28</xref>]. Each automated algorithm is a heuristic based on specific assumptions about eye tracking data and distortion patterns, and algorithms are assessed against a gold-standard dataset corrected manually by human experts. By comparing the correction of the algorithm to the manual correction of human experts, the accuracy of the algorithm is calculated. While automated algorithms continue to improve in accuracy, manually corrected data is still considered the gold standard. An objective comparison of the accuracy of manual and automated correction has not been conducted before, and the degree of subjectivity in manual correction is not entirely quantified and often accepted anecdotally. Understandably, measuring the accuracy of manual correction objectively is challenging considering that research often relies on manual correction as ground truth.</p><p>In this paper, we compare the accuracy of the manual and automated correction of eye tracking data in reading tasks through a novel approach that relies on synthetic data with known ground truth. Moreover, we measure subjectivity and agreement in manual human correction with real eye tracking data. Our research questions are as follows:<list list-type="bullet"><list-item><p>RQ1: How much of an accuracy benefit is gained by manual correction over automated&#x000a0;correction?</p></list-item><list-item><p>RQ2: How often do manual correctors disagree on correcting eye tracking data in reading tasks?</p></list-item></list></p><p>Our novel approach to assess the accuracy of manual correction consists of generating a set of synthetic eye tracking trials, subsequently introducing distortion to them before they are given to novice and experienced human correctors. By comparing the human-corrected trials to the original trials before distortion was introduced, we can objectively measure the accuracy of human correction. Similarly, we can correct the same distorted trials with the automated correction algorithm and measure the accuracy of automated correction&#x000a0;algorithms.</p><p>Subjectivity in manual correction can be measured by examining the agreement between different correctors or how often different correctors apply the same correction to a given fixation. Also, we measure the Intra-Class Correlation Coefficient (ICC3) [<xref rid="B29-jemr-18-00017" ref-type="bibr">29</xref>] to assess the consistency or agreement of measurements made by multiple raters.</p><p>Relying on data from four real eye tracking datasets to answer these research questions, we highlight the trade-offs between manual and automated correction methods, quantifying the benefits and disadvantages of each correction approach.</p></sec><sec id="sec2-jemr-18-00017"><title>2. Materials and Methods</title><p>This section begins with an overview of the manual correction data from Al Madi et al. [<xref rid="B30-jemr-18-00017" ref-type="bibr">30</xref>] on which we rely on in this study. Then, we present our methodology for collecting data from automated correction algorithms, and finally, we present the details of our analyses.</p><p>The manual correction data were drawn from Al Madi et al. [<xref rid="B30-jemr-18-00017" ref-type="bibr">30</xref>]. The previous study compared manual correction to an assisted (semi-automated) approach to correcting eye tracking data through a tool with a graphical user-interface. While the previous study focused on presenting and validating the assisted-correction approach, in this study, we focus on comparing manual correction to automated correction to understand and quantify the trade-offs between the two correction approaches. This study was approved by the Institutional Review Board (IRB) of Colby College (#2023-064).</p><sec id="sec2dot1-jemr-18-00017"><title>2.1. Manual Correction</title><p>This study utilizes manual correction data from Al Madi et al. [<xref rid="B30-jemr-18-00017" ref-type="bibr">30</xref>], and we review the relevant details about the data in this section. Refer to the original study for a more comprehensive presentation of the experiment and its details.</p><p>The data from Al Madi et al. [<xref rid="B30-jemr-18-00017" ref-type="bibr">30</xref>] comes from 14 participants who were adult college students. Five are female and nine are male, and they participated voluntarily and were awarded a $20 gift card after completing the experiment. Participants were given real and synthetic eye tracking trials to correct. Eighteen synthetic trials included varying degrees of noise, slope, shift, and offset distortions, along with trials containing within-line and between-line regressions. For consistency, we use the same names and implementation presented by [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>] in this paper. <xref rid="jemr-18-00017-f002" ref-type="fig">Figure 2</xref> shows four sample trials.</p><p>According to [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>,<xref rid="B18-jemr-18-00017" ref-type="bibr">18</xref>], noise describes a situation where a subset of fixations experience a distortion on the y-axis, resulting in some fixations drifting above the text and some below the text. Slope describes a situation where fixations experience drift on one side of the screen due to calibration error, leaving fixations on one side intact while fixations on the other side drift below (or above) the text. Shift describes the same phenomenon on the vertical axis, where the first line might be intact and other lines progressively experience drift with the maximum drift at the last line. Offset describes a consistent drift affecting all fixations in the same proportion. Within-line regressions describes a reading behavior, where the eyes make a jump to a previous word on the same line, and similarly, between-line regressions describe jumps to a previous line.</p><p>Initially, the synthetic trials were generated without distortions, then distortions and regressions were introduced. This allows us to compare the human-corrected trial to the original trial (before distortions) to assess correction accuracy. Using these synthetic data allows for a unique and accurate way of objectively assessing the accuracy of manual&#x000a0;correction.</p><p>As described by Al Madi et al. [<xref rid="B30-jemr-18-00017" ref-type="bibr">30</xref>], the synthetic data are generated with a level of realism in terms of fixation durations and typical reading phenomena such as word skipping. Fixation duration is proportional to word length in terms of character spaces, and short words are more likely to be skipped. Each distortion and regression type is captured at varying magnitudes, resulting in trials with low, medium, and high levels of distortion.</p><p>The <sc>ErrorNoise</sc> generator in Algorithm 1 introduces vertical noise to a list of eye-tracking fixation points to simulate measurement inaccuracies or distortions. Each fixation is represented as a triplet containing the <italic toggle="yes">x</italic>-coordinate, <italic toggle="yes">y</italic>-coordinate, and fixation duration. The algorithm iterates over each fixation and perturbs the <italic toggle="yes">y</italic>-coordinate by adding a random value (positive or negative) drawn from a Gaussian distribution with a mean of 0 and standard deviation equal to a user-specified parameter <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The <italic toggle="yes">x</italic>-coordinate and duration remain unchanged. The modified fixation is then stored in a result list, which is returned at the&#x000a0;end.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold>&#x000a0;<sc>ErrorNoise</sc>: Add vertical Gaussian noise to fixations</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>: standard deviation of vertical noise<break/><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>: list of fixation triplets <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><break/><bold>Ensure:</bold>&#x000a0;<inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>: list of distorted fixations
<list list-type="simple"><list-item><label>1:</label><p><inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> empty list</p></list-item><list-item><label>2:</label><p><bold>for</bold>&#x000a0;
<inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>to</bold>&#x000a0;
<inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>do</bold></p></list-item><list-item><label>3:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>4:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>5:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>2</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>6:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>7:</label><p>&#x000a0;&#x000a0;&#x000a0;Append <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>8:</label><p><bold>end for</bold></p></list-item><list-item><label>9:</label><p><bold>return</bold>&#x000a0;
<inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list></td></tr></tbody></array></p><p>The <sc>ErrorSlope</sc> generator in Algorithm 2 simulates a distortion in eye-tracking data by altering the vertical (<italic toggle="yes">y</italic>) coordinate of each fixation according to its horizontal (<italic toggle="yes">x</italic>) displacement from the first fixation. It takes as input a list of fixation triplets <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and a slope factor that controls the degree of vertical drift across the horizontal axis. The algorithm begins by storing the <italic toggle="yes">x</italic>-position of the first fixation, which serves as a reference. For each fixation, it calculates the horizontal offset from this reference and uses it to compute a vertical adjustment proportional to the given slope factor. This modified <italic toggle="yes">y</italic>-coordinate is combined with the original <italic toggle="yes">x</italic>-coordinate and duration, and the adjusted fixation is stored. The result is a list of fixations with a consistent vertical trend applied across their <italic toggle="yes">x</italic>-positions.</p><p>The <sc>ErrorShift</sc> generator in Algorithm 3 introduces a vertical distortion to the eye-tracking fixation coordinates by shifting each fixation&#x02019;s <italic toggle="yes">y</italic>-value based on its vertical distance from a reference line (typically the top line). It accepts as input a list of fixation triplets <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, a list of line y-coordinates, and a shift factor that determines the magnitude of distortion. The algorithm begins by computing the line height as the vertical distance between the first two lines. Then, for each fixation, it calculates how far the fixation&#x02019;s <italic toggle="yes">y</italic>-value is from the first line and applies a scaled distortion proportional to that distance and the shift factor. This adjusted <italic toggle="yes">y</italic>-value is combined with the original <italic toggle="yes">x</italic>-value and duration, and the distorted fixation is stored. The resulting output is a list of fixations with a smooth vertical shift that increases with distance from the top of the screen.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 2</bold>&#x000a0;<sc>ErrorSlope</sc>: Apply linear vertical distortion based on the x-position</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>: vertical slope distortion factor<break/><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>: list of fixation triplets <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><break/><bold>Ensure:</bold>&#x000a0;<inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>: list of slope-distorted fixations
<list list-type="simple"><list-item><label>&#x000a0;&#x000a0;1:</label><p><inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> empty list</p></list-item><list-item><label>&#x000a0;&#x000a0;2:</label><p><inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;3:</label><p><bold>for</bold>&#x000a0;
<inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>to</bold>&#x000a0;
<inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>do</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;4:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;5:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;6:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>2</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;7:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mn>100</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#x000b7;</mml:mo><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;8:</label><p>&#x000a0;&#x000a0;&#x000a0;Append <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;9:</label><p><bold>end for</bold></p></list-item><list-item><label>10:</label><p><bold>return</bold>&#x000a0;
<inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list></td></tr></tbody></array></p><array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 3</bold>&#x000a0;<sc>ErrorShift</sc>: Apply vertical shift distortion relative to the line positions</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>: vertical shift distortion factor<break/><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>: list of line y-coordinates<break/><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>: list of fixation triplets <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><break/><bold>Ensure:</bold>&#x000a0;<inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>: list of vertically shifted fixations
<list list-type="simple"><list-item><label>&#x000a0;&#x000a0;1:</label><p><inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> empty list</p></list-item><list-item><label>&#x000a0;&#x000a0;2:</label><p><inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;3:</label><p><bold>for</bold>&#x000a0;
<inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>to</bold>&#x000a0;
<inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>do</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;4:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;5:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;6:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>2</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;7:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mo>|</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;8:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x000b7;</mml:mo><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;9:</label><p>&#x000a0;&#x000a0;&#x000a0;Append <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>10:</label><p><bold>end for</bold></p></list-item><list-item><label>11:</label><p><bold>return</bold>&#x000a0;
<inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list></td></tr></tbody></array><p>The <sc>ErrorOffset</sc> generator in Algorithm 4 introduces a uniform distortion to the eye-tracking fixation coordinates by applying fixed horizontal and vertical offsets to each fixation. It accepts a list of fixations represented as triplets <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, along with scalar values <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, indicating how much to shift each fixation in the x- and y-directions, respectively. The algorithm iterates through each fixation, adds the offsets to the x- and y-values, and preserves the original duration. The result is a list of systematically displaced fixations that simulate a consistent calibration error or systematic drift in recorded eye-tracking data.</p><p>The <sc>BetweenLineRegression</sc> generator in Algorithm 5 generates synthetic fixations based on the layout of words (AOIs) in a reading stimulus. For each word, it computes a fixation slightly to the left of the word&#x02019;s center (at roughly one-third width) and vertically centered on the line, with added jitter to simulate natural variability around the optimal viewing position. Fixation durations are scaled proportionally to the word width. Additionally, the algorithm introduces between-line regressive fixations that jump backward to earlier words on previous lines based on a user-defined probability. Candidate indexes for regressions are randomly selected, and the algorithm ensures that at least one regression occurs if the regression probability is non-zero. When a regression is triggered, the algorithm attempts to select a valid target on a different line by retrying up to ten times. This simulates more realistic regressions in reading patterns while maintaining the spatial and temporal plausibility of the fixation data.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 4</bold>&#x000a0;<sc>ErrorOffset</sc>: Apply fixed x/y offset to the fixation coordinates</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>: horizontal offset<break/><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>: vertical offset<break/><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>: list of fixation triplets <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><break/><bold>Ensure:</bold>&#x000a0;<inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>: list of offset fixations
<list list-type="simple"><list-item><label>&#x000a0;&#x000a0;1:</label><p><inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> empty list</p></list-item><list-item><label>&#x000a0;&#x000a0;2:</label><p><bold>for</bold>&#x000a0;
<inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>to</bold>&#x000a0;
<inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>do</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;3:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;4:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;5:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mn>2</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;6:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;7:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;8:</label><p>&#x000a0;&#x000a0;&#x000a0;Append <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;9:</label><p><bold>end for</bold></p></list-item><list-item><label>10:</label><p><bold>return</bold>&#x000a0;
<inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list></td></tr></tbody></array></p><array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 5</bold>&#x000a0;<sc>BetweenLineRegression</sc>: Generate fixations with between-line regressions</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mo>_</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>: list of Areas-Of-Interest (AOIs)<break/><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><break/><bold>Ensure:</bold>&#x000a0;<inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>: list of synthetic fixations
<list list-type="simple"><list-item><label>&#x000a0;&#x000a0;1:</label><p>Select <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> probabilistically from indices <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x0003e;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;2:</label><p><bold>if</bold>&#x000a0;no <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>then</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;3:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Add one random regression index from valid range</p></list-item><list-item><label>&#x000a0;&#x000a0;4:</label><p><bold>end if</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;5:</label><p><inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;6:</label><p><bold>while</bold>&#x000a0;
<inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>len</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mo>_</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>do</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;7:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Extract <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mo>_</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;8:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mi>jitter</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mi>jitter</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;9:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mn>100</mml:mn><mml:mo>+</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mn>15</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#x000d7;</mml:mo><mml:mn>40</mml:mn></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>10:</label><p>&#x000a0;&#x000a0;&#x000a0;Append <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>11:</label><p>&#x000a0;&#x000a0;&#x000a0;<bold>if</bold>&#x000a0;<inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>then</bold></p></list-item><list-item><label>12:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Try up to 10 times to select a prior index on a different line</p></list-item><list-item><label>13:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;If successful, set <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>prior</mml:mi><mml:mspace width="4.pt"/><mml:mi>index</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <bold>continue</bold></p></list-item><list-item><label>14:</label><p>&#x000a0;&#x000a0;&#x000a0;<bold>end if</bold></p></list-item><list-item><label>15:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>16:</label><p><bold>end while</bold></p></list-item><list-item><label>17:</label><p><bold>return</bold>&#x000a0;
<inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list></td></tr></tbody></array><p>The <sc>WithinLineRegression</sc> generator in Algorithm 6 simulates eye fixations along lines of text, introducing within-line regressions with a specified probability. This generator is mostly identical to the <sc>BetweenLineRegression</sc> generator; the only difference is that it guarantees a regression within the same line of text instead of a previous line. When a regression index is reached, the algorithm attempts to jump back to a previous word on the same line, simulating a within-line regression. This process continues returning a list of generated fixations that emulate realistic reading behavior with occasional regressive eye movements confined to the same line.</p><p>In addition to the synthetic data, 18 real-data trials come from four datasets, five trials from&#x000a0;Al Madi and Khan [<xref rid="B31-jemr-18-00017" ref-type="bibr">31</xref>], five from the Multilingual Eye Tracking (MET) dataset [<xref rid="B32-jemr-18-00017" ref-type="bibr">32</xref>], four from GazeBase [<xref rid="B33-jemr-18-00017" ref-type="bibr">33</xref>], and four from the Eye Movement In Programming (EMIP) dataset [<xref rid="B34-jemr-18-00017" ref-type="bibr">34</xref>]. <xref rid="jemr-18-00017-f003" ref-type="fig">Figure 3</xref> shows sample trials from each dataset, which represent diverse reading experiments reflecting differences in language, number of lines of text, font height, spacing, and eye tracking frequency. The set of real-data trials is used in measuring subjectivity with highly realistic and representative data.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 6</bold>&#x000a0;<sc>WithinLineRegression</sc>: Generate fixations with within-line regressions</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mo>_</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>: list of Areas-Of-Interest (AOIs)<break/><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><break/><bold>Ensure:</bold>&#x000a0;<inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>: list of generated fixations
<list list-type="simple"><list-item><label>&#x000a0;&#x000a0;1:</label><p>Select <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> probabilistically from indices <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x0003e;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;2:</label><p><bold>if</bold>&#x000a0;<inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is empty and <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>then</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;3:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Add one random regression index to ensure coverage</p></list-item><list-item><label>&#x000a0;&#x000a0;4:</label><p><bold>end if</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;5:</label><p><inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;6:</label><p><bold>while</bold>&#x000a0;
<inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>len</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mo>_</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>do</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;7:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Extract <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mo>_</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;8:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Compute fixation location with small horizontal/vertical jitter</p></list-item><list-item><label>&#x000a0;&#x000a0;9:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mn>100</mml:mn><mml:mo>+</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mn>15</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#x000d7;</mml:mo><mml:mn>40</mml:mn></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>10:</label><p>&#x000a0;&#x000a0;&#x000a0;Append <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>11:</label><p>&#x000a0;&#x000a0;&#x000a0;<bold>if</bold>&#x000a0;<inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>then</bold></p></list-item><list-item><label>12:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Try up to 10 times to pick prior index on the same line</p></list-item><list-item><label>13:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>if</bold> successful <bold>then</bold></p></list-item><list-item><label>14:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> chosen prior index</p></list-item><list-item><label>15:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>continue</bold></p></list-item><list-item><label>16:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>end if</bold></p></list-item><list-item><label>17:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>end if</bold></p></list-item><list-item><label>18:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>19:</label><p><bold>end while</bold></p></list-item><list-item><label>20:</label><p><bold>return</bold>&#x000a0;
<inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list></td></tr></tbody></array></p><p>In the original study by Al Madi et al. [<xref rid="B30-jemr-18-00017" ref-type="bibr">30</xref>], participants corrected half of the trials manually and half using the assisted approach presented in the article. For the purpose of this study, we focus only on trials that were corrected manually, excluding trials that were corrected using the assisted technique. In addition, the original study relied on self-reporting for experience, while here, we rely on the overall performance of each participant. There was a clear gap in the performance of participants, resulting in labeling ten participants as novices and four as experts. More specifically, we took the mean accuracy of each participant in addition to their self-reported experience to label participants as either experts or novices. When the overall accuracy of a participant exceeded 90%, they were entered into the advanced group. All but one person in the experienced group reported being experienced in eye tracking data correction. In other words, participant performance matched self-reported experience with 12 out of 14 participants (one novice and one expert).</p></sec><sec id="sec2dot2-jemr-18-00017"><title>2.2. Automated Correction</title><p>For comparison with manual correction, the same synthetic trials containing distortion are given to nine automated correction algorithms. The accuracy of each algorithm is assessed by comparing the corrected trial to the original synthetic trial before distortion was introduced.</p><p>The automated correction algorithms fall into various categories based on the heuristic strategy that guides their correction. As described by Carr et al. [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>], positional algorithms include the <italic toggle="yes">Attach</italic> algorithm, which assigns each fixation to its closest text line, and the <italic toggle="yes">Chain</italic> algorithm, which groups consecutive fixations and assigns them to the line closest to the mean of their y-values [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>,<xref rid="B35-jemr-18-00017" ref-type="bibr">35</xref>]. Relative positional approaches such as <italic toggle="yes">Cluster</italic> use k-means clustering to group fixations with similar y-values, while <italic toggle="yes">Merge</italic> progressively combines fixation sequences until they match the number of text lines [<xref rid="B35-jemr-18-00017" ref-type="bibr">35</xref>,<xref rid="B36-jemr-18-00017" ref-type="bibr">36</xref>]. The <italic toggle="yes">Regress</italic> algorithm treats fixations as a cloud of points and fits regression lines [<xref rid="B37-jemr-18-00017" ref-type="bibr">37</xref>], whereas <italic toggle="yes">Stretch</italic> minimizes the alignment error between fixation points and text lines through adjustments in offsets and scaling factors [<xref rid="B20-jemr-18-00017" ref-type="bibr">20</xref>]. Sequential algorithms like <italic toggle="yes">Segment</italic> divide fixation sequences into discrete subsequences mapped linearly from top to bottom [<xref rid="B21-jemr-18-00017" ref-type="bibr">21</xref>], while <italic toggle="yes">Warp</italic> employs Dynamic-Time Warp to minimize Euclidean distances between fixation positions and word centers [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>].</p><p>The Attach algorithm, as implemented by Carr et al. [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>], takes as input a list of 2D fixation coordinates (fixation_XY) and a list of y-values corresponding to horizontal text lines (line_Y). Its purpose is to align each fixation vertically with the nearest line, effectively &#x0201c;snapping&#x0201d; the eye movement data to the most probable reading line. For each fixation point, it computes the vertical (y-axis) distance to all lines in line_Y, identifies the line with the minimum absolute difference, and it replaces the fixation&#x02019;s y-coordinate with that of the closest line. This process ensures that all fixations are vertically aligned to the most likely text line.</p><p>Here, we present a more optimized version of the Attach algorithm (Algorithm 7), which improves efficiency by using binary search to find the nearest text line for each fixation. Assuming the array of line positions (<italic toggle="yes">line_Y</italic>) is sorted, the algorithm iterates through each fixation in <italic toggle="yes">fixation_XY</italic>, retrieves its y-coordinate, and uses binary search to find the index of the closest value in <italic toggle="yes">line_Y</italic>. It then replaces the fixation&#x02019;s y-value with that of the nearest line. Because binary search reduces the search from linear time <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to logarithmic time <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the overall time complexity improves from <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">n</italic> is the number of fixations and <italic toggle="yes">m</italic> is the number of lines of text. This optimization improves runtime, especially for large datasets.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 7</bold>&#x000a0;Optimized Attach fixations to the closest line (optimized with binary search)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mi>Y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, array of fixation coordinates<break/><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, sorted array of y-coordinates of lines<break/><bold>Ensure:</bold>&#x000a0;Modified <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> with updated y-values
<list list-type="simple"><list-item><label>1:</label><p><inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> length of <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>2:</label><p><bold>for</bold>&#x000a0;
<inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>to</bold>&#x000a0;
<inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>do</bold></p></list-item><list-item><label>3:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mi>Y</mml:mi><mml:mo>[</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>4:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> Index of value in <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> closest to <italic toggle="yes">y</italic> (using binary search)</p></list-item><list-item><label>5:</label><p>&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mi>Y</mml:mi><mml:mo>[</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo><mml:mo>&#x02190;</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi><mml:mo>[</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>6:</label><p><bold>end for</bold></p></list-item><list-item><label>7:</label><p><bold>return</bold>&#x000a0;
<inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list>
</td></tr></tbody></array></p><p>The <italic toggle="yes">Chain</italic> algorithm [<xref rid="B35-jemr-18-00017" ref-type="bibr">35</xref>], as implemented by Carr et al. [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>],&#x000a0;groups sequences of temporally and spatially close eye fixations into chains and aligns them to the nearest text line. It begins by computing the horizontal and vertical distances between consecutive fixations. A new chain is initiated whenever the distance between fixations exceeds either a horizontal threshold (<inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) or a vertical threshold (<inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), which indicates a transition between reading units such as words or lines. For each chain of fixations, the algorithm calculates the average y-coordinate and finds the closest line in <italic toggle="yes">line_Y</italic> to that average. It then aligns all y-values within the chain to the y-coordinate of that line, effectively snapping entire chains rather than individual fixations to their most likely text line.</p><p>The optimized version of the <italic toggle="yes">Chain</italic> algorithm, presented here in Algorithm 8, improves performance by using binary search to find the closest line for each chain of fixations. First, the algorithm calculates the pairwise distances between consecutive fixations in both x- and y-dimensions. It identifies the boundaries of fixation chains based on whether these distances exceed predefined thresholds (<inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), which typically signal line or word transitions. For each resulting chain, it computes the mean y-coordinate and uses binary search on the sorted array of line positions (<inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) to efficiently find the nearest line. All fixations in the chain are then aligned to that line&#x02019;s y-coordinate. By replacing a linear search with binary search, the algorithm reduces the complexity of line assignment from <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">k</italic> is the number of chains. <italic toggle="yes">n</italic> is the number of fixations and <italic toggle="yes">m</italic> is the number of lines of text.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 8</bold>&#x000a0;Optimized Chain fixations to the nearest line</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mi>Y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, array of fixation coordinates<break/><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, sorted array of line y-positions<break/><bold>Require:</bold>&#x000a0;<inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> &#x02013; spatial thresholds for chaining<break/><bold>Ensure:</bold>&#x000a0;Modified <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> with grouped y-values aligned to lines
<list list-type="simple"><list-item><label>&#x000a0;&#x000a0;1:</label><p><inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> length of <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;2:</label><p><inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mo>|</mml:mo><mml:mi>distances</mml:mi><mml:mspace width="4.pt"/><mml:mi>between</mml:mi><mml:mspace width="4.pt"/><mml:mi>consecutive</mml:mi><mml:mspace width="4.pt"/><mml:mi>fixation</mml:mi><mml:mspace width="4.pt"/><mml:mi>coordinates</mml:mi><mml:mspace width="4.pt"/><mml:mi>in</mml:mi><mml:mspace width="4.pt"/><mml:mi>the</mml:mi><mml:mspace width="4.pt"/><mml:mi mathvariant="normal">x</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;3:</label><p><inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mo>|</mml:mo><mml:mi>distances</mml:mi><mml:mspace width="4.pt"/><mml:mi>between</mml:mi><mml:mspace width="4.pt"/><mml:mi>consecutive</mml:mi><mml:mspace width="4.pt"/><mml:mi>fixation</mml:mi><mml:mspace width="4.pt"/><mml:mi>coordinates</mml:mi><mml:mspace width="4.pt"/><mml:mi>in</mml:mi><mml:mspace width="4.pt"/><mml:mi>the</mml:mi><mml:mspace width="4.pt"/><mml:mi mathvariant="normal">y</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;4:</label><p><inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indices where <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;5:</label><p>Increment all <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> by 1 and append <italic toggle="yes">n</italic></p></list-item><list-item><label>&#x000a0;&#x000a0;6:</label><p><inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;7:</label><p><bold>for all</bold>&#x000a0;<inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>do</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;8:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>mean</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mi>Y</mml:mi><mml:mo>[</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>:</mml:mo><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;9:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>BinarySearchClosest</mml:mi><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>10:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mi>Y</mml:mi><mml:mo>[</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>:</mml:mo><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo><mml:mo>&#x02190;</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi><mml:mo>[</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>11:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>12:</label><p><bold>end for</bold></p></list-item><list-item><label>13:</label><p><bold>return</bold>&#x000a0;
<inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list>
</td></tr></tbody></array></p><p>The <italic toggle="yes">Cluster</italic> algorithm, which is based on Ref. [<xref rid="B35-jemr-18-00017" ref-type="bibr">35</xref>] and implemented by the authors in [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>], groups fixations into <italic toggle="yes">m</italic> vertical clusters, where <italic toggle="yes">m</italic> is the number of expected text lines (i.e., the length of <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). The algorithm applies KMeans clustering to the y-values of the fixations, resulting in <italic toggle="yes">m</italic> clusters corresponding to groups of fixations that likely fall on the same line of text. Each cluster&#x02019;s mean y-coordinate is then computed, and the clusters are sorted by vertical position from top to bottom. After sorting, the algorithm matches the topmost cluster to the topmost line in <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the second-highest cluster to the second line, and so on. Each fixation is then reassigned the y-coordinate of the matched line. This approach depends on the quality of clustering and the assumption that the number of lines is known in advance. The algorithm runs in approximately <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> time, where <italic toggle="yes">t</italic> is the number of KMeans iterations.</p><p>The <italic toggle="yes">Merge</italic> algorithm, based on [<xref rid="B36-jemr-18-00017" ref-type="bibr">36</xref>] and implemented by [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>], is a multi-phase heuristic designed to align eye fixation sequences to specific text lines. The algorithm begins by identifying natural breaks between fixations, using horizontal (X) backtracking regressions or large vertical (Y) jumps to split the sequence into initial candidate groups. These groups, called &#x0201c;sequences&#x0201d;, are then iteratively merged across four phases with increasingly relaxed constraints. In each phase, pairs of sequences are evaluated for potential merging based on their linear fit&#x02014;the slope (gradient) and residual error of a regression line through their combined fixation coordinates. A merge is accepted if the pair satisfies the gradient and error thresholds, except in the final phase where these constraints are ignored. The goal of merging is to reduce the number of fixation sequences to match the number of text lines. Once enough merges have occurred, the algorithm computes the mean y-position of each sequence, sorts them, and aligns each group to the closest corresponding line. In practice, the algorithm runs in approximately <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> time (average time complexity).</p><p>The <italic toggle="yes">Regress</italic> algorithm is a probabilistic approach to aligning eye fixations to lines of text by estimating the most likely mapping using a fitted regression model [<xref rid="B37-jemr-18-00017" ref-type="bibr">37</xref>]. The core idea is that fixations may be misaligned vertically due to systematic distortions, such as screen-viewing angles or calibration errors. To correct this, the algorithm searches for the optimal parameters of a simple linear transformation&#x02014;a slope, an offset, and a noise spread&#x02014;which are constrained to fall within user-specified bounds. These parameters are searched using a gradient-based optimization routine (<italic toggle="yes">minimize</italic>) that aims to maximize the likelihood (or minimize the negative log-likelihood) that the observed vertical fixation positions were drawn from Gaussian distributions centered on the predicted positions for each line. Each fixation&#x02019;s predicted y-coordinate is calculated from its x-position and the current slope estimate, plus an offset relative to each candidate line. For every line, the algorithm evaluates the log-probability of each fixation coming from that line and assigns each fixation to the line with the highest probability. Once the best-fit parameters are found, the y-positions of fixations are snapped to their most likely lines, producing an alignment that accounts for both linear distortion and reading noise. The algorithm runs in approximately <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> time, where <italic toggle="yes">t</italic> is the number of iterations or function evaluations performed by the optimizer.</p><p>The <italic toggle="yes">Segment</italic> algorithm is a heuristic method designed to assign eye fixation coordinates to specific lines of text based on patterns of horizontal eye movement, which are characteristic during reading [<xref rid="B21-jemr-18-00017" ref-type="bibr">21</xref>]. It assumes that the most significant horizontal saccades typically correspond to transitions between lines&#x02014;for example, when a reader reaches the end of one line and moves to the start of the next. The algorithm begins by measuring the horizontal distances between consecutive fixations and ranks the saccades by length. From this ranking, it identifies the <inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> smallest saccades as probable indicators of line changes, where <italic toggle="yes">m</italic> is the total number of text lines. The algorithm then iterates through the sequence of fixations, assigning each one to a line by adjusting its vertical coordinate to match the corresponding line position. When a fixation index corresponds to one of the identified line change positions, the algorithm increments the current line index to move to the next line. This effectively segments the sequence of fixations across the text lines, assuming that line transitions are marked by smaller horizontal movements relative to within-line reading behavior. The algorithm runs in approximately <inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo form="prefix">log</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> time.</p><p>The <italic toggle="yes">split</italic> algorithm is designed to correct vertical drift in eye-tracking data by segmenting sequences of fixations into individual reading lines using horizontal movement patterns [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>]. It begins by calculating the horizontal distances (saccades) between consecutive fixations. These saccades are clustered into two groups using KMeans clustering, which assumes that one group represents short intra-line fixations and the other represents long sweep saccades that typically occur at line boundaries. By identifying which cluster corresponds to these longer sweep saccades (determined by the cluster with the lower mean), the algorithm determines the indices at which a line transition occurs. Using these indices, the algorithm splits the fixation sequence into line segments. For each segment, it computes the mean vertical position of fixations and aligns all fixations within that segment to the closest line in the provided set of reference line positions (<italic toggle="yes">line_Y</italic>). This approach enables the adaptive correction of drift by leveraging natural horizontal eye movement patterns during reading. The most computationally intensive operation is KMeans clustering on <inline-formula><mml:math id="mm156" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> saccades, leading to an overall time complexity of <inline-formula><mml:math id="mm157" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for preprocessing and segmentation, assuming KMeans converges in constant time due to the fixed number of clusters.</p><p>The <italic toggle="yes">stretch</italic> algorithm corrects vertical drift in eye-tracking data by globally adjusting the vertical positions of fixations through a linear transformation involving both scaling and offset. It assumes that vertical drift can be approximated by a uniform stretch and shift of fixation y-coordinates [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>]. To achieve this, the algorithm defines an objective function that computes transformed y-values using a given scale and offset, and ot aligns each transformed fixation to the nearest line in the reference array <italic toggle="yes">line_Y</italic>. The cost is calculated as the sum of absolute differences between the transformed y-values and their corresponding aligned lines. Using bounded optimization, the algorithm finds the parameters that minimize this cost, constrained within user-defined bounds. Once the optimal scale and offset are determined, the final corrected y-positions are generated by aligning the transformed coordinates to the closest lines. Since each evaluation of the objective function involves all <italic toggle="yes">n</italic> fixations and the number of parameters is constant, the time complexity of the algorithm is <inline-formula><mml:math id="mm158" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">t</italic> is the number of optimizer iterations.</p><p>The <italic toggle="yes">warp</italic> algorithm corrects vertical drift in eye-tracking data by aligning each fixation to the most likely line of text based on its proximity to corresponding words [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>]. It employs Dynamic Time Warping (DTW) to establish an optimal alignment between the sequence of fixations and the sequence of word positions (<italic toggle="yes">word_XY</italic>). DTW constructs a cost matrix using pairwise Euclidean distances between fixations and words and finds a minimum-cost path that allows for non-linear alignment to accommodate variations in reading pace. Once this path is obtained, each fixation is associated with one or more words, and its y-coordinate is updated to the mode of the y-values of those associated words, effectively snapping it to the most probable reading line. This approach ensures a robust correction that is tolerant to noise while maintaining alignment with the structure of the text. The overall time complexity of the algorithm is <inline-formula><mml:math id="mm159" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d4aa;</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">n</italic> is the number of fixations and <italic toggle="yes">m</italic> is the number of words.</p></sec><sec id="sec2dot3-jemr-18-00017"><title>2.3. Analyses</title><p>Our first research question compares the accuracy of manual correction with automated algorithms on the synthetic data. Here, we focus on the synthetic data exclusively, to avoid relying on human correction as the ground truth leading to circular bias. We compare manual and automated corrections to the original trials before distortion was introduced. This way, the comparison is objective and the ground truth data are completely reliable.</p><p>Accuracy in the context of correcting eye tracking data in reading tasks is measured as the percentage of fixations that are returned to the same line they were on before distortion was introduced. In other words, if fixation_n was on line 1, then after distortion was introduced, it was moved to another line or an empty space between the line, and a valid correction should return that fixation to line 1. The correction accuracy for a given trial is calculated by counting the number of fixations that were returned to their original lines over the total number of fixations in the trial.</p><p>The second research question is concerned with subjectivity in manual correction. Subjectivity between human correctors would be reflected in applying different corrections to the same fixation, and therefore, it is the opposite of agreement. In other words, high subjectivity would result in low agreement between correctors and vice versa. Therefore, we measure agreement between correctors as the percentage of fixations where correctors assigned a given fixation to the same line.</p><p>Considering that human correctors corrected the same set of trials, we can give each fixation in each trial a unique ID. Then, we can examine the corrected trials from each participant, and for each fixation, we can record the line number it was assigned to in that trial. When all corrected trials from all correctors are processed, we can go over fixations one at a time and measure how often correctors assigned the fixation to the same line. For example, if three out of four correctors assigned a given fixation to the same line, then the agreement for that fixation is 75%. The overall agreement between correctors is the mean of the agreements on all fixations.</p><p>For the agreement, we focus on the corrected trials from human correctors without being concerned with accuracy. The goal is to measure how often correctors agree, not if correctors made the right correction or not. Therefore, we measure agreement over the real data for generalization.</p><p>Furthermore, we approach the agreement question statistically. Studies often use Cohen&#x02019;s Kappa [<xref rid="B38-jemr-18-00017" ref-type="bibr">38</xref>] to assess the agreement between two raters, while in our study, we had 14 raters and therefore we rely on intra-class correlation to assess agreement. The Intra-Class Correlation Coefficient (ICC3) is a statistical measure used to assess the consistency or agreement of measurements made by multiple raters or instruments on the same subjects when a fixed set of raters is used for all observations [<xref rid="B29-jemr-18-00017" ref-type="bibr">29</xref>]. ICC3 is particularly sensitive to both systematic and random errors, as it treats differences between raters as part of the measurement error rather than a random factor. Its value ranges from &#x02212;1 to 1, where higher values indicate stronger agreement. Typically, ICC3 is interpreted as follows: values above 0.75 indicate excellent reliability, values between 0.50 and 0.75 suggest moderate reliability, and values below 0.50 reflect poor reliability.</p></sec></sec><sec sec-type="results" id="sec3-jemr-18-00017"><title>3. Results</title><p>In this section, we start by comparing the accuracy of manual correction to automated correction over synthetic data. Then, we assess subjectivity by examining the agreement among correctors in correcting real eye tracking data. In both accuracy and subjectivity, we explore the differences between experienced and novice correctors.</p><sec id="sec3dot1-jemr-18-00017"><title>3.1. Accuracy</title><p>Here, we compare the accuracy of manual correction to automated algorithms in correcting synthetic trials with known ground truth. <xref rid="jemr-18-00017-f004" ref-type="fig">Figure 4</xref> shows the mean accuracy (green dot), median accuracy (vertical line), and standard deviation (error bars) for human novices, experts, and algorithms. As seen in the figure, the accuracy of automated algorithms ranged from 67% (Attach) to 84% (Cluster), while the mean manual correction of novices was 78% and of experts was 92%. Moreover, the best-performing expert had an accuracy of 95%, which is 11% better than the best automated algorithm.</p><p>A Mann-Whitney U test was conducted to compare the correction accuracy between novices and experts. The results indicated a statistically significant difference, U = 815.0, <italic toggle="yes">p</italic> = 0.0018. The negative effect size, Cohen&#x02019;s d = &#x02212;0.63, suggests a medium effect, indicating that experts performed substantially better than novices in correction accuracy. These findings highlight a notable distinction in performance between the two groups.</p></sec><sec id="sec3dot2-jemr-18-00017"><title>3.2. Subjectivity</title><p>Here, we assess the subjectivity of novices and experts in correcting real data. We start by measuring the agreement between correctors as the percentage of fixations in which correctors assign a given fixation to the same line. We found that, on average, novices agree on the position of 88.5% of fixations, while experts agree on 98.8% fixations. This suggests that experts show very high agreement, indicating low subjectivity in their correction.</p><p>Moreover, we approach the question of subjectivity from the inter-rater agreement direction. Studies often use Cohen&#x02019;s Kappa [<xref rid="B38-jemr-18-00017" ref-type="bibr">38</xref>] to assess the agreement between two raters, while in our study we had 14 raters; therefore, we rely on the Intra-Class Correlation Coefficient (ICC3) to assess agreement. The ICC3 is particularly sensitive to both systematic and random errors, as it treats differences between raters as part of the measurement error rather than a random factor [<xref rid="B29-jemr-18-00017" ref-type="bibr">29</xref>]. Its value ranges from &#x02212;1 to 1, where higher values indicate stronger agreement. Typically, the ICC3 is interpreted as follows: values above 0.75 indicate excellent reliability, values between 0.50 and 0.75 suggest moderate reliability, and values below 0.50 reflect poor reliability.</p><p>Calculating human corrector agreement in multi-line trials in <xref rid="jemr-18-00017-t001" ref-type="table">Table 1</xref>, we generally find that correctors agree substantially (mean ICC = 0.71), with a few exceptions that seem to reflect challenging trials. These low agreement real-data trials contained many fixations and high distortion. Moreover, the table shows a very small probability that agreement was due to chance, as reflected by the <italic toggle="yes">p</italic>-value column. These results suggest that human correctors have a high agreement in correcting eye tracking data, indicating low subjectivity.</p></sec><sec id="sec3dot3-jemr-18-00017"><title>3.3. Summary</title><p>In our investigation of manual and automated correction accuracy, we found that automated algorithms ranged in accuracy from 67% to 84%, while novice manual correctors on average achieved 78% accuracy and expert correctors 92%. The best-performing expert had an average accuracy of 95%, surpassing the best automated algorithm by 11%. Statistical analysis confirmed that experts performed significantly better than novices in correction accuracy (U = 815.0, <italic toggle="yes">p</italic> = 0.0018), with a medium effect size (Cohen&#x02019;s d = &#x02212;0.63).</p><p>Regarding subjectivity, experts demonstrated a very high agreement rate in correcting fixations (98.8%), and agreement among novices was high too (88.5%). An analysis of inter-rater agreement (ICC) showed strong consistency among human correctors, with an average ICC of 0.71, suggesting near-excellent agreement in eye-tracking correction.</p></sec></sec><sec sec-type="discussion" id="sec4-jemr-18-00017"><title>4. Discussion</title><p>Our research investigates the validity of certain widely accepted notions about manual and automated correction in reading experiments. To date, there has been no objective comparison of the accuracy of manual corrections versus automated algorithms. Additionally, prior studies such as [<xref rid="B19-jemr-18-00017" ref-type="bibr">19</xref>,<xref rid="B20-jemr-18-00017" ref-type="bibr">20</xref>,<xref rid="B39-jemr-18-00017" ref-type="bibr">39</xref>] describe manual correction as &#x0201c;subjective&#x0201d; and challenging to reproduce, but the level of subjectivity has never been studied objectively. In addition, researchers often report the frequency and maximum accuracy of the eye tracking equipment as an indicator of the quality of the data, but the correlation between eye-tracker frequency and drift is not fully understood.</p><p>Therefore, comparing the accuracy of manual and automated correction can help researchers understand and balance the trade-offs between correction time and accuracy. Moreover, knowing how subjective human correctors are, we can quantify a potential threat to the validity of manual correction, if there is one.</p><p>We start with asnwering our first research question, <bold>RQ1: How much of an accuracy benefit is gained by manual correction over automated correction?</bold> Our results show that manual correction by experts is substantially better than the best automated algorithm. The best human corrector had an average accuracy of 95%, exceeding the best automated algorithm by 11%. Therefore, manual correction, for the time being, remains the most accurate method of correcting eye tracking data in reading tasks. Until automated algorithms take the lead, expert human correction is the gold standard.</p><p>Additionally, we found that the best automated algorithms approximate and sometimes surpass the manual correction of novices. While this result seems to suggest that the accuracy of automated algorithms is somewhat better than expected, it is important to notice the huge variation in the accuracy of automated algorithms. While the variance in human assessment seems to be influenced by experience (i.e., novices and experts), the variance in automated algorithms appears to be influenced by the alignment between the assumptions made by the algorithm and the conditions of the given trial. For example, some algorithms make assumptions about reading linearity (reading from left to right and top to bottom without jumps back) and their performance is governed by how linear reading is in a given trial. In reading source code, for example, reading patterns are known to be non-linear with significant regressions and progressions. Therefore, a linear algorithm, such as Warp, might not be the most suitable. The results suggest that the choice of the correction algorithm should match the characteristics of the trial being corrected.</p><p>Finally, the results seem to adjust our preconceived conceptions on manual and automated correction. In summary, although expert manual correction remains the most accurate, automated algorithms can be a close second as a reliable option for many research experiments. There still remains some overall difficulty in drift correction, that is, in distinguishing systematic drift from the real properties of eye-movement behavior.</p><p>We now address our second research question, <bold>RQ2: How often do manual correctors disagree on correcting eye tracking data in reading tasks?</bold> Our results appear to challenge the often-held conception that human correction is substantially subjective and &#x0201c;not necessarily reproducible&#x0201d; [<xref rid="B19-jemr-18-00017" ref-type="bibr">19</xref>], especially in the case of expert correctors. Our results show that experts disagree in only 1.2% of the corrected fixations, and even when taking all human correctors as a single group, ICC scores show high agreement. Claims of high subjectivity appear to be inconsistent with our findings. Therefore, our results call for the further exploration of subjectivity in correcting eye tracking data, possibly with more data.</p><p>It is worth noting that agreement between human correctors does not necessarily indicate that the applied correction is valid, as the correction may still be biased overall. However, the agreement suggests that human correctors follow similar strategies that lead to applying the same correction.</p></sec><sec sec-type="conclusions" id="sec5-jemr-18-00017"><title>5. Conclusions</title><p>In this paper, we compared the accuracy of manual and automated correction of eye tracking data in reading tasks. We present a novel approach to make this comparison possible by relying on synthetic data with known ground truth. Our results represent the first assessment of manual correction with objective ground truth data, granting unique insights and quantifying the benefits of manual and automated correction. Moreover, we present an exploration of subjectivity in manual correction, where previous research claimed that manual correction is highly subjective. Yet, our results show that human correctors, especially experts, had high agreement in their corrections, challenging some of the preconceived opinions on manual correction.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Institutional Review Board Statement</title><p>The study was conducted in accordance with the Declaration of Helsinki and approved by the Institutional Review Board (IRB) of Colby College (#2023-064) on 18 April 2023.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data and code are available at the following OSF replication package: <uri xlink:href="https://osf.io/2apnm/">https://osf.io/2apnm/</uri> (accessed on 23 April 2025).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The author does not have any financial or non-financial conflicts of interest to report.</p></notes><ref-list><title>References</title><ref id="B1-jemr-18-00017"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rayner</surname><given-names>K.</given-names></name>
</person-group><article-title>Eye movements in reading and information processing: 20 years of research</article-title><source>Psychol. Bull.</source><year>1998</year><volume>124</volume><fpage>372</fpage><pub-id pub-id-type="doi">10.1037/0033-2909.124.3.372</pub-id><pub-id pub-id-type="pmid">9849112</pub-id>
</element-citation></ref><ref id="B2-jemr-18-00017"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Obaidellah</surname><given-names>U.</given-names></name>
<name><surname>Al Haek</surname><given-names>M.</given-names></name>
<name><surname>Cheng</surname><given-names>P.C.H.</given-names></name>
</person-group><article-title>A survey on the usage of eye-tracking in computer programming</article-title><source>ACM Comput. Surv. (CSUR)</source><year>2018</year><volume>51</volume><fpage>5</fpage><pub-id pub-id-type="doi">10.1145/3145904</pub-id></element-citation></ref><ref id="B3-jemr-18-00017"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Perra</surname><given-names>J.</given-names></name>
<name><surname>Latimier</surname><given-names>A.</given-names></name>
<name><surname>Poulin-Charronnat</surname><given-names>B.</given-names></name>
<name><surname>Baccino</surname><given-names>T.</given-names></name>
<name><surname>Drai-Zerbib</surname><given-names>V.</given-names></name>
</person-group><article-title>A meta-analysis on the effect of expertise on eye movements during music reading</article-title><source>J. Eye Mov. Res.</source><year>2022</year><volume>15</volume><fpage>1</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.16910/jemr.15.4.1</pub-id><pub-id pub-id-type="pmid">37323997</pub-id>
</element-citation></ref><ref id="B4-jemr-18-00017"><label>4.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Holmqvist</surname><given-names>K.</given-names></name>
<name><surname>Nystr&#x000f6;m</surname><given-names>M.</given-names></name>
<name><surname>Andersson</surname><given-names>R.</given-names></name>
<name><surname>Dewhurst</surname><given-names>R.</given-names></name>
<name><surname>Jarodzka</surname><given-names>H.</given-names></name>
<name><surname>Van de Weijer</surname><given-names>J.</given-names></name>
</person-group><source>Eye Tracking: A Comprehensive Guide to Methods and Measures</source><publisher-name>Oxford University Press</publisher-name><publisher-loc>Oxford, UK</publisher-loc><year>2011</year></element-citation></ref><ref id="B5-jemr-18-00017"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hornof</surname><given-names>A.J.</given-names></name>
<name><surname>Halverson</surname><given-names>T.</given-names></name>
</person-group><article-title>Cleaning up systematic error in eye-tracking data by using required fixation locations</article-title><source>Behav. Res. Methods Instruments Comput.</source><year>2002</year><volume>34</volume><fpage>592</fpage><lpage>604</lpage><pub-id pub-id-type="doi">10.3758/BF03195487</pub-id></element-citation></ref><ref id="B6-jemr-18-00017"><label>6.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Yamaya</surname><given-names>A.</given-names></name>
<name><surname>Topi&#x00107;</surname><given-names>G.</given-names></name>
<name><surname>Mart&#x000ed;nez-G&#x000f3;mez</surname><given-names>P.</given-names></name>
<name><surname>Aizawa</surname><given-names>A.</given-names></name>
</person-group><article-title>Dynamic-Programming&#x02013;Based Method for Fixation-to-Word Mapping</article-title><source>Proceedings of the Intelligent Decision Technologies</source><person-group person-group-type="editor">
<name><surname>Neves-Silva</surname><given-names>R.</given-names></name>
<name><surname>Jain</surname><given-names>L.C.</given-names></name>
<name><surname>Howlett</surname><given-names>R.J.</given-names></name>
</person-group><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2015</year><fpage>649</fpage><lpage>659</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-19857-6_55</pub-id></element-citation></ref><ref id="B7-jemr-18-00017"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hooge</surname><given-names>I.T.</given-names></name>
<name><surname>Hessels</surname><given-names>R.S.</given-names></name>
<name><surname>Niehorster</surname><given-names>D.C.</given-names></name>
<name><surname>Andersson</surname><given-names>R.</given-names></name>
<name><surname>Skrok</surname><given-names>M.K.</given-names></name>
<name><surname>Konklewski</surname><given-names>R.</given-names></name>
<name><surname>Stremplewski</surname><given-names>P.</given-names></name>
<name><surname>Nowakowski</surname><given-names>M.</given-names></name>
<name><surname>Tamborski</surname><given-names>S.</given-names></name>
<name><surname>Szkulmowska</surname><given-names>A.</given-names></name>
<etal/>
</person-group><article-title>Eye tracker calibration: How well can humans refixate a target?</article-title><source>Behav. Res. Methods</source><year>2025</year><volume>57</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.3758/s13428-024-02564-4</pub-id></element-citation></ref><ref id="B8-jemr-18-00017"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Holmqvist</surname><given-names>K.</given-names></name>
<name><surname>Nystr&#x000f6;m</surname><given-names>M.</given-names></name>
<name><surname>Mulvey</surname><given-names>F.</given-names></name>
</person-group><article-title>Eye tracker data quality: What it is and how to measure it</article-title><source>Proceedings of the Symposium on Eye Tracking Research and Applications</source><conf-loc>Santa Barbara, CA, USA</conf-loc><conf-date>28&#x02013;30 March 2012</conf-date><fpage>45</fpage><lpage>52</lpage></element-citation></ref><ref id="B9-jemr-18-00017"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Helo</surname><given-names>A.</given-names></name>
<name><surname>Pannasch</surname><given-names>S.</given-names></name>
<name><surname>Sirri</surname><given-names>L.</given-names></name>
<name><surname>R&#x000e4;m&#x000e4;</surname><given-names>P.</given-names></name>
</person-group><article-title>The maturation of eye movement behavior: Scene viewing characteristics in children and adults</article-title><source>Vis. Res.</source><year>2014</year><volume>103</volume><fpage>83</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2014.08.006</pub-id><pub-id pub-id-type="pmid">25152319</pub-id>
</element-citation></ref><ref id="B10-jemr-18-00017"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Di Stasi</surname><given-names>L.L.</given-names></name>
<name><surname>McCamy</surname><given-names>M.B.</given-names></name>
<name><surname>Macknik</surname><given-names>S.L.</given-names></name>
<name><surname>Mankin</surname><given-names>J.A.</given-names></name>
<name><surname>Hooft</surname><given-names>N.</given-names></name>
<name><surname>Catena</surname><given-names>A.</given-names></name>
<name><surname>Martinez-Conde</surname><given-names>S.</given-names></name>
</person-group><article-title>Saccadic eye movement metrics reflect surgical residents&#x02019; fatigue</article-title><source>Ann. Surg.</source><year>2014</year><volume>259</volume><fpage>824</fpage><lpage>829</lpage><pub-id pub-id-type="doi">10.1097/SLA.0000000000000260</pub-id><pub-id pub-id-type="pmid">24169184</pub-id>
</element-citation></ref><ref id="B11-jemr-18-00017"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Abdulin</surname><given-names>E.</given-names></name>
<name><surname>Komogortsev</surname><given-names>O.</given-names></name>
</person-group><article-title>User eye fatigue detection via eye movement behavior</article-title><source>Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>18&#x02013;23 April 2015</conf-date><fpage>1265</fpage><lpage>1270</lpage></element-citation></ref><ref id="B12-jemr-18-00017"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Carl</surname><given-names>M.</given-names></name>
</person-group><article-title>Dynamic programming for re-mapping noisy fixations in translation tasks</article-title><source>J. Eye Mov. Res.</source><year>2013</year><volume>6</volume><fpage>2</fpage><pub-id pub-id-type="doi">10.16910/jemr.6.2.5</pub-id></element-citation></ref><ref id="B13-jemr-18-00017"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Carr</surname><given-names>J.W.</given-names></name>
<name><surname>Pescuma</surname><given-names>V.N.</given-names></name>
<name><surname>Furlan</surname><given-names>M.</given-names></name>
<name><surname>Ktori</surname><given-names>M.</given-names></name>
<name><surname>Crepaldi</surname><given-names>D.</given-names></name>
</person-group><article-title>Algorithms for the automated correction of vertical drift in eye-tracking data</article-title><source>Behav. Res. Methods</source><year>2022</year><volume>54</volume><fpage>287</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.3758/s13428-021-01554-0</pub-id><pub-id pub-id-type="pmid">34159510</pub-id>
</element-citation></ref><ref id="B14-jemr-18-00017"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wass</surname><given-names>S.V.</given-names></name>
<name><surname>Forssman</surname><given-names>L.</given-names></name>
<name><surname>Lepp&#x000e4;nen</surname><given-names>J.</given-names></name>
</person-group><article-title>Robustness and precision: How data quality may influence key dependent variables in infant eye-tracker analyses</article-title><source>Infancy</source><year>2014</year><volume>19</volume><fpage>427</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1111/infa.12055</pub-id></element-citation></ref><ref id="B15-jemr-18-00017"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dalrymple</surname><given-names>K.A.</given-names></name>
<name><surname>Manner</surname><given-names>M.D.</given-names></name>
<name><surname>Harmelink</surname><given-names>K.A.</given-names></name>
<name><surname>Teska</surname><given-names>E.P.</given-names></name>
<name><surname>Elison</surname><given-names>J.T.</given-names></name>
</person-group><article-title>An examination of recording accuracy and precision from eye tracking data from toddlerhood to adulthood</article-title><source>Front. Psychol.</source><year>2018</year><volume>9</volume><elocation-id>803</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2018.00803</pub-id><pub-id pub-id-type="pmid">29875727</pub-id>
</element-citation></ref><ref id="B16-jemr-18-00017"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Reichle</surname><given-names>E.D.</given-names></name>
<name><surname>Drieghe</surname><given-names>D.</given-names></name>
</person-group><article-title>Using EZ Reader to examine the consequences of fixation-location measurement error</article-title><source>J. Exp. Psychol. Learn. Mem. Cogn.</source><year>2015</year><volume>41</volume><fpage>262</fpage><pub-id pub-id-type="doi">10.1037/a0037090</pub-id><pub-id pub-id-type="pmid">24933699</pub-id>
</element-citation></ref><ref id="B17-jemr-18-00017"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Palmer</surname><given-names>C.</given-names></name>
<name><surname>Sharif</surname><given-names>B.</given-names></name>
</person-group><article-title>Towards automating fixation correction for source code</article-title><source>Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research &#x00026; Applications</source><conf-loc>Charleston, CA, USA</conf-loc><conf-date>14&#x02013;17 March 2016</conf-date><fpage>65</fpage><lpage>68</lpage></element-citation></ref><ref id="B18-jemr-18-00017"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Al Madi</surname><given-names>N.</given-names></name>
</person-group><article-title>Advancing Dynamic-Time Warp Techniques for Correcting Eye Tracking Data in Reading Source Code</article-title><source>J. Eye Mov. Res.</source><year>2024</year><volume>17</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.16910/jemr.17.1.4</pub-id></element-citation></ref><ref id="B19-jemr-18-00017"><label>19.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Busjahn</surname><given-names>T.</given-names></name>
</person-group><article-title>Empirical Analysis of Eye Movements During Code Reading: Evaluation and Development of Methods</article-title><source>Ph.D. Thesis</source><publisher-name>University of Paderborn</publisher-name><publisher-loc>Paderborn, Germany</publisher-loc><year>2021</year></element-citation></ref><ref id="B20-jemr-18-00017"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Lohmeier</surname><given-names>S.</given-names></name>
</person-group><article-title>Experimental Evaluation and Modelling of the Comprehension of Indirect Anaphors in a Programming Language</article-title><source>Master&#x02019;s Thesis</source><publisher-name>Technische Universit&#x000e4;t Berlin</publisher-name><publisher-loc>Berlin, Germany</publisher-loc><year>2015</year></element-citation></ref><ref id="B21-jemr-18-00017"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Abdulin</surname><given-names>E.</given-names></name>
<name><surname>Komogortsev</surname><given-names>O.</given-names></name>
</person-group><article-title>Person verification via eye movement-driven text reading model</article-title><source>Proceedings of the 2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems (BTAS)</source><conf-loc>Arlington, VA, USA</conf-loc><conf-date>8&#x02013;11 September 2015</conf-date><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/BTAS.2015.7358786</pub-id></element-citation></ref><ref id="B22-jemr-18-00017"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Beymer</surname><given-names>D.</given-names></name>
<name><surname>Russell</surname><given-names>D.M.</given-names></name>
<name><surname>Orton</surname><given-names>P.Z.</given-names></name>
</person-group><article-title>Wide vs. narrow paragraphs: An eye tracking analysis</article-title><source>Proceedings of the IFIP Conference on Human-Computer Interaction</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2005</year><fpage>741</fpage><lpage>752</lpage></element-citation></ref><ref id="B23-jemr-18-00017"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lima Sanches</surname><given-names>C.</given-names></name>
<name><surname>Augereau</surname><given-names>O.</given-names></name>
<name><surname>Kise</surname><given-names>K.</given-names></name>
</person-group><article-title>Vertical error correction of eye trackers in nonrestrictive reading condition</article-title><source>IPSJ Trans. Comput. Vis. Appl.</source><year>2016</year><volume>8</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1186/s41074-016-0008-x</pub-id></element-citation></ref><ref id="B24-jemr-18-00017"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Martinez-Gomez</surname><given-names>P.</given-names></name>
<name><surname>Chen</surname><given-names>C.</given-names></name>
<name><surname>Hara</surname><given-names>T.</given-names></name>
<name><surname>Kano</surname><given-names>Y.</given-names></name>
<name><surname>Aizawa</surname><given-names>A.</given-names></name>
</person-group><article-title>Image registration for text-gaze alignment</article-title><source>Proceedings of the 2012 ACM international conference on Intelligent User Interfaces</source><conf-loc>Lisbon, Portugal</conf-loc><conf-date>14&#x02013;17 February 2012</conf-date><fpage>257</fpage><lpage>260</lpage></element-citation></ref><ref id="B25-jemr-18-00017"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yamaya</surname><given-names>A.</given-names></name>
<name><surname>Topi&#x00107;</surname><given-names>G.</given-names></name>
<name><surname>Aizawa</surname><given-names>A.</given-names></name>
</person-group><article-title>Fixation-to-Word Mapping with Classification of Saccades</article-title><source>Proceedings of the 21st International Conference on Intelligent User Interfaces</source><conf-loc>Sonoma, CA, USA</conf-loc><conf-date>7&#x02013;10 March 2016</conf-date></element-citation></ref><ref id="B26-jemr-18-00017"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Hornof</surname><given-names>A.J.</given-names></name>
</person-group><article-title>Mode-of-disparities error correction of eye-tracking data</article-title><source>Behav. Res. Methods</source><year>2011</year><volume>43</volume><fpage>834</fpage><lpage>842</lpage><pub-id pub-id-type="doi">10.3758/s13428-011-0073-0</pub-id><pub-id pub-id-type="pmid">21487905</pub-id>
</element-citation></ref><ref id="B27-jemr-18-00017"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>John</surname><given-names>S.</given-names></name>
<name><surname>Weitnauer</surname><given-names>E.</given-names></name>
<name><surname>Koesling</surname><given-names>H.</given-names></name>
</person-group><article-title>Entropy-based correction of eye tracking data for static scenes</article-title><source>Proceedings of the Symposium on Eye Tracking Research and Applications&#x02014;ETRA &#x02019;12</source><conf-loc>Santa Barbara, CA, USA</conf-loc><conf-date>28&#x02013;30 March 2012</conf-date><fpage>297</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1145/2168556.2168620</pub-id></element-citation></ref><ref id="B28-jemr-18-00017"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Culemann</surname><given-names>W.</given-names></name>
<name><surname>Neuber</surname><given-names>L.</given-names></name>
<name><surname>Heine</surname><given-names>A.</given-names></name>
</person-group><article-title>Systematic Drift Correction in Eye Tracking Reading Studies: Integrating Line Assignments with Implicit Recalibration</article-title><source>Procedia Comput. Sci.</source><year>2024</year><volume>246</volume><fpage>2821</fpage><lpage>2830</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2024.09.389</pub-id></element-citation></ref><ref id="B29-jemr-18-00017"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bartko</surname><given-names>J.J.</given-names></name>
</person-group><article-title>The intraclass correlation coefficient as a measure of reliability</article-title><source>Psychol. Rep.</source><year>1966</year><volume>19</volume><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.2466/pr0.1966.19.1.3</pub-id><pub-id pub-id-type="pmid">5942109</pub-id>
</element-citation></ref><ref id="B30-jemr-18-00017"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Al Madi</surname><given-names>N.</given-names></name>
<name><surname>Torra</surname><given-names>B.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Tariq</surname><given-names>N.</given-names></name>
</person-group><article-title>Combining automation and expertise: A semi-automated approach to correcting eye-tracking data in reading tasks</article-title><source>Behav. Res. Methods</source><year>2025</year><volume>57</volume><fpage>72</fpage><pub-id pub-id-type="doi">10.3758/s13428-025-02597-3</pub-id><pub-id pub-id-type="pmid">39856463</pub-id>
</element-citation></ref><ref id="B31-jemr-18-00017"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Al Madi</surname><given-names>N.</given-names></name>
<name><surname>Khan</surname><given-names>J.</given-names></name>
</person-group><article-title>Constructing semantic networks of comprehension from eye-movement during reading</article-title><source>Proceedings of the 2018 IEEE 12th International Conference on Semantic Computing (ICSC)</source><conf-loc>Laguna Hills, CA, USA</conf-loc><conf-date>31 January&#x02013;2 February 2018</conf-date><fpage>49</fpage><lpage>55</lpage></element-citation></ref><ref id="B32-jemr-18-00017"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Raymond</surname><given-names>O.</given-names></name>
<name><surname>Moldagali</surname><given-names>Y.</given-names></name>
<name><surname>Al Madi</surname><given-names>N.</given-names></name>
</person-group><article-title>A Dataset of Underrepresented Languages in Eye Tracking Research</article-title><source>Proceedings of the 2023 Symposium on Eye Tracking Research and Applications</source><conf-loc>T&#x000fc;bingen, Germany</conf-loc><conf-date>30 May&#x02013;2 June 2023</conf-date><fpage>1</fpage><lpage>2</lpage></element-citation></ref><ref id="B33-jemr-18-00017"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Griffith</surname><given-names>H.</given-names></name>
<name><surname>Lohr</surname><given-names>D.</given-names></name>
<name><surname>Abdulin</surname><given-names>E.</given-names></name>
<name><surname>Komogortsev</surname><given-names>O.</given-names></name>
</person-group><article-title>GazeBase, a large-scale, multi-stimulus, longitudinal eye movement dataset</article-title><source>Sci. Data</source><year>2021</year><volume>8</volume><fpage>184</fpage><pub-id pub-id-type="doi">10.1038/s41597-021-00959-y</pub-id><pub-id pub-id-type="pmid">34272404</pub-id>
</element-citation></ref><ref id="B34-jemr-18-00017"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bednarik</surname><given-names>R.</given-names></name>
<name><surname>Busjahn</surname><given-names>T.</given-names></name>
<name><surname>Gibaldi</surname><given-names>A.</given-names></name>
<name><surname>Ahadi</surname><given-names>A.</given-names></name>
<name><surname>Bielikova</surname><given-names>M.</given-names></name>
<name><surname>Crosby</surname><given-names>M.</given-names></name>
<name><surname>Essig</surname><given-names>K.</given-names></name>
<name><surname>Fagerholm</surname><given-names>F.</given-names></name>
<name><surname>Jbara</surname><given-names>A.</given-names></name>
<name><surname>Lister</surname><given-names>R.</given-names></name>
<etal/>
</person-group><article-title>EMIP: The eye movements in programming dataset</article-title><source>Sci. Comput. Program.</source><year>2020</year><volume>198</volume><fpage>102520</fpage><pub-id pub-id-type="doi">10.1016/j.scico.2020.102520</pub-id></element-citation></ref><ref id="B35-jemr-18-00017"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schroeder</surname><given-names>S.</given-names></name>
</person-group><article-title>popEye&#x02014;An R package to analyse eye movement data from reading experiments</article-title><source>J. Eye Mov. Res.</source><year>2019</year><volume>12</volume><fpage>92</fpage></element-citation></ref><ref id="B36-jemr-18-00017"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>&#x00160;pakov</surname><given-names>O.</given-names></name>
<name><surname>Istance</surname><given-names>H.</given-names></name>
<name><surname>Hyrskykari</surname><given-names>A.</given-names></name>
<name><surname>Siirtola</surname><given-names>H.</given-names></name>
<name><surname>R&#x000e4;ih&#x000e4;</surname><given-names>K.J.</given-names></name>
</person-group><article-title>Improving the performance of eye trackers with limited spatial accuracy and low sampling rates for reading analysis by heuristic fixation-to-word mapping</article-title><source>Behav. Res. Methods</source><year>2019</year><volume>51</volume><fpage>2661</fpage><lpage>2687</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-1120-x</pub-id><pub-id pub-id-type="pmid">30225553</pub-id>
</element-citation></ref><ref id="B37-jemr-18-00017"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>A.L.</given-names></name>
</person-group><article-title>Software for the automatic correction of recorded eye fixation locations in reading experiments</article-title><source>Behav. Res. Methods</source><year>2013</year><volume>45</volume><fpage>679</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.3758/s13428-012-0280-3</pub-id><pub-id pub-id-type="pmid">23239069</pub-id>
</element-citation></ref><ref id="B38-jemr-18-00017"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>J.</given-names></name>
</person-group><article-title>A coefficient of agreement for nominal scales</article-title><source>Educ. Psychol. Meas.</source><year>1960</year><volume>20</volume><fpage>37</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1177/001316446002000104</pub-id></element-citation></ref><ref id="B39-jemr-18-00017"><label>39.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>N&#x000fc;ssli</surname><given-names>M.A.</given-names></name>
</person-group><source>Dual Eye-Tracking Methods for the Study of Remote Collaborative Problem Solving</source><comment>Technical report</comment><publisher-name>EPFL</publisher-name><publisher-loc>Lausanne, Switzerland</publisher-loc><year>2011</year></element-citation></ref></ref-list></back><floats-group><fig position="float" id="jemr-18-00017-f001"><label>Figure 1</label><caption><p>Example trial showing fixations drift (data from Carr et al. [<xref rid="B13-jemr-18-00017" ref-type="bibr">13</xref>]). (<bold>a</bold>) Trial before correction (showing drift). (<bold>b</bold>) Trial after correction.</p></caption><graphic xlink:href="jemr-18-00017-g001" position="float"/></fig><fig position="float" id="jemr-18-00017-f002"><label>Figure 2</label><caption><p>Synthetic eye tracking data showing (<bold>a</bold>) noise, (<bold>b</bold>) slope, (<bold>c</bold>) shift, and (<bold>d</bold>) offset distortions in reading (from Al Madi et al. [<xref rid="B30-jemr-18-00017" ref-type="bibr">30</xref>]).</p></caption><graphic xlink:href="jemr-18-00017-g002" position="float"/></fig><fig position="float" id="jemr-18-00017-f003"><label>Figure 3</label><caption><p>Sample trials from the four datasets. (<bold>a</bold>) AlMadi2018; (<bold>b</bold>) MET dataset; (<bold>c</bold>) GazeBase dataset; (<bold>d</bold>) EMIP dataset.</p></caption><graphic xlink:href="jemr-18-00017-g003" position="float"/></fig><fig position="float" id="jemr-18-00017-f004"><label>Figure 4</label><caption><p>Manual and automated correction accuracy. Green dot is the mean.</p></caption><graphic xlink:href="jemr-18-00017-g004" position="float"/></fig><table-wrap position="float" id="jemr-18-00017-t001"><object-id pub-id-type="pii">jemr-18-00017-t001_Table 1</object-id><label>Table 1</label><caption><p>ICC results for real-data trials. Each row represents a real-data trial from a dataset that was corrected by several participants, and the ICC score represents agreement among correctors.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Trial</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ICC</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">df1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">df2</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"><italic toggle="yes">p</italic>-Value</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">024_hindi_single_6_LargeFont</td><td align="center" valign="middle" rowspan="1" colspan="1">MET</td><td align="center" valign="middle" rowspan="1" colspan="1">0.66</td><td align="center" valign="middle" rowspan="1" colspan="1">16.674352</td><td align="center" valign="middle" rowspan="1" colspan="1">135</td><td align="center" valign="middle" rowspan="1" colspan="1">945</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">027_russian_multi_6_LargeFont</td><td align="center" valign="middle" rowspan="1" colspan="1">MET</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40</td><td align="center" valign="middle" rowspan="1" colspan="1">6.353907</td><td align="center" valign="middle" rowspan="1" colspan="1">91</td><td align="center" valign="middle" rowspan="1" colspan="1">637</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">040_urdu_paragraph2_1_LargeFont</td><td align="center" valign="middle" rowspan="1" colspan="1">MET</td><td align="center" valign="middle" rowspan="1" colspan="1">0.36</td><td align="center" valign="middle" rowspan="1" colspan="1">5.675295</td><td align="center" valign="middle" rowspan="1" colspan="1">469</td><td align="center" valign="middle" rowspan="1" colspan="1">3283</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">trial_1_participant_100_TEX_R1S1_bg</td><td align="center" valign="middle" rowspan="1" colspan="1">GazeBase</td><td align="center" valign="middle" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">176.530545</td><td align="center" valign="middle" rowspan="1" colspan="1">342</td><td align="center" valign="middle" rowspan="1" colspan="1">2394</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">trial_1_participant_102_TEX_R1S1_bg</td><td align="center" valign="middle" rowspan="1" colspan="1">GazeBase</td><td align="center" valign="middle" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">176.038158</td><td align="center" valign="middle" rowspan="1" colspan="1">429</td><td align="center" valign="middle" rowspan="1" colspan="1">3003</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">trial_2_participant_101_rectangle_java2</td><td align="center" valign="middle" rowspan="1" colspan="1">EMIP</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73</td><td align="center" valign="middle" rowspan="1" colspan="1">23.288324</td><td align="center" valign="middle" rowspan="1" colspan="1">156</td><td align="center" valign="middle" rowspan="1" colspan="1">1092</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">trial_2_participant_21_rectangle_java2</td><td align="center" valign="middle" rowspan="1" colspan="1">EMIP</td><td align="center" valign="middle" rowspan="1" colspan="1">0.67</td><td align="center" valign="middle" rowspan="1" colspan="1">15.669201</td><td align="center" valign="middle" rowspan="1" colspan="1">315</td><td align="center" valign="middle" rowspan="1" colspan="1">1890</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">034_japanese_multi_5_LargeFont</td><td align="center" valign="middle" rowspan="1" colspan="1">MET</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91</td><td align="center" valign="middle" rowspan="1" colspan="1">42.252772</td><td align="center" valign="middle" rowspan="1" colspan="1">109</td><td align="center" valign="middle" rowspan="1" colspan="1">327</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">016_spanish_multi_5_LargeFont</td><td align="center" valign="middle" rowspan="1" colspan="1">MET</td><td align="center" valign="middle" rowspan="1" colspan="1">0.24</td><td align="center" valign="middle" rowspan="1" colspan="1">1.963989</td><td align="center" valign="middle" rowspan="1" colspan="1">87</td><td align="center" valign="middle" rowspan="1" colspan="1">174</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">trial_1_participant_101_TEX_R1S1_bg</td><td align="center" valign="middle" rowspan="1" colspan="1">GazeBase</td><td align="center" valign="middle" rowspan="1" colspan="1">0.87</td><td align="center" valign="middle" rowspan="1" colspan="1">34.776527</td><td align="center" valign="middle" rowspan="1" colspan="1">343</td><td align="center" valign="middle" rowspan="1" colspan="1">1372</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">trial_1_participant_103_TEX_R1S1_bg</td><td align="center" valign="middle" rowspan="1" colspan="1">GazeBase</td><td align="center" valign="middle" rowspan="1" colspan="1">0.98</td><td align="center" valign="middle" rowspan="1" colspan="1">421.786702</td><td align="center" valign="middle" rowspan="1" colspan="1">343</td><td align="center" valign="middle" rowspan="1" colspan="1">1372</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">trial_2_participant_178_rectangle_java2</td><td align="center" valign="middle" rowspan="1" colspan="1">EMIP</td><td align="center" valign="middle" rowspan="1" colspan="1">0.68</td><td align="center" valign="middle" rowspan="1" colspan="1">11.891540</td><td align="center" valign="middle" rowspan="1" colspan="1">230</td><td align="center" valign="middle" rowspan="1" colspan="1">920</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">trial_5_participant_60_rectangle_java2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EMIP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.535659</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">254</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1016</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Mean ICC</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.71</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>-</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>-</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>-</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>-</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"/></tr></tbody></table></table-wrap></floats-group></article>