<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Nat Hum Behav</journal-id><journal-id journal-id-type="iso-abbrev">Nat Hum Behav</journal-id><journal-title-group><journal-title>Nature Human Behaviour</journal-title></journal-title-group><issn pub-type="epub">2397-3374</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39695250</article-id><article-id pub-id-type="pmc">PMC11860214</article-id>
<article-id pub-id-type="publisher-id">2077</article-id><article-id pub-id-type="doi">10.1038/s41562-024-02077-2</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>How human&#x02013;AI feedback loops alter human perceptual, emotional and social judgements</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3792-1992</contrib-id><name><surname>Glickman</surname><given-names>Moshe</given-names></name><address><email>mosheglickman345@gmail.com</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Sharot</surname><given-names>Tali</given-names></name><address><email>t.sharot@ucl.ac.uk</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02jx3x895</institution-id><institution-id institution-id-type="GRID">grid.83440.3b</institution-id><institution-id institution-id-type="ISNI">0000 0001 2190 1201</institution-id><institution>Affective Brain Lab, Department of Experimental Psychology, </institution><institution>University College London, </institution></institution-wrap>London, UK </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02jx3x895</institution-id><institution-id institution-id-type="GRID">grid.83440.3b</institution-id><institution-id institution-id-type="ISNI">0000 0001 2190 1201</institution-id><institution>Max Planck UCL Centre for Computational Psychiatry and Ageing Research, </institution><institution>University College London, </institution></institution-wrap>London, UK </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/042nb2s44</institution-id><institution-id institution-id-type="GRID">grid.116068.8</institution-id><institution-id institution-id-type="ISNI">0000 0001 2341 2786</institution-id><institution>Department of Brain and Cognitive Sciences, </institution><institution>Massachusetts Institute of Technology, </institution></institution-wrap>Cambridge, MA USA </aff></contrib-group><pub-date pub-type="epub"><day>18</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>18</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="ppub"><year>2025</year></pub-date><volume>9</volume><issue>2</issue><fpage>345</fpage><lpage>359</lpage><history><date date-type="received"><day>24</day><month>3</month><year>2023</year></date><date date-type="accepted"><day>30</day><month>10</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Artificial intelligence (AI) technologies are rapidly advancing, enhancing human capabilities across various fields spanning from finance to medicine. Despite their numerous advantages, AI systems can exhibit biased judgements in domains ranging from perception to emotion. Here, in a series of experiments (<italic>n</italic>&#x02009;=&#x02009;1,401 participants), we reveal a feedback loop where human&#x02013;AI interactions alter processes underlying human perceptual, emotional and social judgements, subsequently amplifying biases in humans. This amplification is significantly greater than that observed in interactions between humans, due to both the tendency of AI systems to amplify biases and the way humans perceive AI systems. Participants are often unaware of the extent of the AI&#x02019;s influence, rendering them more susceptible to it. These findings uncover a mechanism wherein AI systems amplify biases, which are further internalized by humans, triggering a snowball effect where small errors in judgement escalate into much larger ones.</p></abstract><abstract id="Abs2" abstract-type="web-summary"><p id="Par2">Glickman and Sharot reveal a human&#x02013;AI feedback loop, where AI amplifies subtle human biases, which are then further internalized by humans. This cycle, observed across various domains, leads to substantial increases in human bias over time.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Human behaviour</kwd><kwd>Decision making</kwd><kwd>Psychology</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust (Wellcome)</institution></institution-wrap></funding-source><award-id>214268/Z/18/Z</award-id><principal-award-recipient><name><surname>Sharot</surname><given-names>Tali</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Main</title><p id="Par3">Interactions between humans and artificial intelligence (AI) systems have become prevalent, transforming modern society at an unprecedented pace. A vital research challenge is to establish how these interactions alter human beliefs. While decades of research have characterized how humans influence each other<sup><xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR3">3</xref></sup>, the influence of AI on humans may be qualitatively and quantitatively different. This is partially because AI judgements are distinct from human judgements in several ways (for example, they tend to be less noisy<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>) and because humans may perceive AI judgements differently from those of other humans<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. In this Article, we show how human<bold>&#x02013;</bold>AI interactions impact human cognition. In particular, we reveal that when humans repeatedly interact with biased AI systems, they learn to be more biased themselves. We show this in a range of domains and algorithms, including a widely used real-world text-to-image AI system.</p><p id="Par4">Modern AI systems rely on machine learning algorithms, such as convolutional neural networks<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> (CNNs) and transformers<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, to identify complex patterns in vast datasets, without requiring extensive explicit programming. These systems clearly augment human natural capabilities in a variety of domains, such as health care<sup><xref ref-type="bibr" rid="CR9">9</xref>&#x02013;<xref ref-type="bibr" rid="CR11">11</xref></sup>, education<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, marketing<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and finance<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. However, it is well documented that AI systems can automate and perpetuate existing human biases in areas ranging from medical diagnoses to hiring decisions<sup><xref ref-type="bibr" rid="CR15">15</xref>&#x02013;<xref ref-type="bibr" rid="CR17">17</xref></sup>, and may even amplify those biases<sup><xref ref-type="bibr" rid="CR18">18</xref>&#x02013;<xref ref-type="bibr" rid="CR20">20</xref></sup>. While this problem has been established, a potentially more profound and complex concern has been largely overlooked until now. As critical decisions increasingly involve collaboration between AI and humans (for example, AI systems assisting physicians in diagnosis and offering humans advice on various topics<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>), these interactions provide a mechanism through which not only biased humans generate biased AI systems, but biased AI systems can alter human beliefs, leaving them more biased than they initially were. This possibility, predicted from a synthesis of bias amplification and human feedback learning, holds substantial implications for our modern society, but has not yet been empirically tested.</p><p id="Par5">Bias, defined as a systematic error in judgements, can emerge in AI systems primarily due to inherent human biases embedded in the datasets the algorithm was trained on (&#x02018;bias in bias out&#x02019;<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>; see also ref. <sup><xref ref-type="bibr" rid="CR24">24</xref></sup>) and/or when the data are more representative of one class than the other<sup><xref ref-type="bibr" rid="CR25">25</xref>&#x02013;<xref ref-type="bibr" rid="CR27">27</xref></sup>. For example, generative AI systems such as text-to-image technologies and large language models learn from available data on the Internet, which being generated by humans contains inaccuracies and biases, even in cases where the ground truth exists. As a result, these AI systems end up reflecting a host of human biases (such as cognitive biases<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>, as well as racial and gender biases<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>). When humans subsequently interact with these systems (for example, by generating images or text), they may learn from them in turn. Interaction with other AI technologies that exhibit bias (including social bias), such as CNN-based facial recognition algorithms<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, recommendation systems<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, hiring tools<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> and credit allocation tools<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, may also induce similar circularity. Moreover, human biases can be amplified even when individuals are not directly interacting with an AI system, but merely observing its output. Indeed, an estimated 15&#x02009;billion AI-generated images circulate online<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, which users routinely consume passively on social media, news websites and other digital platforms. As a result, the impact of AI-generated content on human biases may extend beyond the immediate users of these systems.</p><p id="Par6">Here, over a series of studies, we demonstrate that when humans and AI interact, even minute perceptual, emotional and social biases originating either from AI systems or humans leave human beliefs more biased, potentially forming a feedback loop. The impact of AI on humans&#x02019; beliefs is gradually observed over time, as humans slowly learn from the AI systems. We uncover that the amplification effect is greater in human<bold>&#x02013;</bold>AI interactions than in human<bold>&#x02013;</bold>human interactions, due both to human perception of AI and the unique characteristics of AI judgements. In particular, AI systems may be more sensitive to minor biases in the data than humans due to their expansive computational resources<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> and may therefore be more likely to leverage them to improve prediction accuracy, especially when the data are noisy<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. Moreover, once trained, AI systems&#x02019; judgements tend to be less noisy than those of humans<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Thus, AI systems provide a higher signal-to-noise ratio than humans, which enables rapid learning by humans, even if the signal is biased. In fact, if the AI is perceived as being superior to humans<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR38">38</xref>,<xref ref-type="bibr" rid="CR39">39</xref></sup> (but see ref. <sup><xref ref-type="bibr" rid="CR40">40</xref></sup>), learning its bias can be considered perfectly rational. Amplification of bias only occurs if the bias already exists in the system: when humans interact with an accurate AI system, their judgements are improved.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Human&#x02013;AI feedback loops can amplify human&#x02019;s biases</title><p id="Par7">We begin by collecting human data in an emotion aggregation task in which human judgement is slightly biased. We then demonstrate that training an AI algorithm on this slightly biased dataset results in the algorithm not only adopting the bias but further amplifying it. Next, we show that when humans interact with the biased AI, their initial bias increases (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>; human<bold>&#x02013;</bold>AI interaction). This bias amplification does not occur in an interaction including only human participants (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>; human&#x02013;human interaction).<fig id="Fig1"><label>Fig. 1</label><caption><title>Human<bold>&#x02013;</bold>AI interaction creates a feedback loop that makes humans more biased (experiment 1).</title><p><bold>a,</bold> Human<bold>&#x02013;</bold>AI interaction. Human classifications in an emotion aggregation task are collected (level 1) and fed to an AI algorithm (CNN; level 2). A new pool of human participants (level 3) then interact with the AI. During level 1 (emotion aggregation), participants are presented with an array of 12 faces and asked to classify the mean emotion expressed by the faces as more sad or more happy. During level 2 (CNN), the CNN is trained on human data from level 1. During level 3 (human<bold>&#x02013;</bold>AI interaction), a new group of participants provide their emotion aggregation response and are then presented with the response of an AI before being asked whether they would like to change their initial response. <bold>b</bold>, Human&#x02013;human interaction. This is conceptually similar to the human&#x02013;AI interaction, except the AI (level 2) is replaced with human participants. The participants in level 2 are presented with the arrays and responses of the participants in level 1 (training phase) and then judge new arrays on their own as either more sad or more happy (test phase). The participants in level 3 are then presented with the responses of the human participants from level 2 and asked whether they would like to change their initial response. <bold>c</bold>, Human<bold>&#x02013;</bold>AI-perceived-as-human interaction. This condition is also conceptually similar to the human<bold>&#x02013;</bold>AI interaction condition, except participants in level 3 are told they are interacting with another human when in fact they are interacting with an AI system (input: AI; label: human). <bold>d</bold>, Human&#x02013;human-perceived-as-AI interaction. This condition is similar to the human&#x02013;human interaction condition, except that participants in level 3 are told they are interacting with AI when in fact they are interacting with other humans (input: human; label: AI). <bold>e</bold>, Level 1 and 2 results. Participants in level 1 (green circle; <italic>n</italic>&#x02009;=&#x02009;50) showed a slight bias towards the response more sad. This bias was amplified by AI in level 2 (blue circle), but not by human participants in level 2 (orange circle; <italic>n</italic>&#x02009;=&#x02009;50). The <italic>P</italic>&#x02009;values were derived using permutation tests. All significant <italic>P</italic>&#x02009;values remained significant after applying Benjamini&#x02013;Hochberg false discovery rate correction at <italic>&#x003b1;</italic>&#x02009;=&#x02009;0.05. <bold>f</bold>, Level 3 results. When interacting with the biased AI, participants became more biased over time (human&#x02013;AI interaction; blue line). In contrast, no bias amplification was observed when interacting with humans (human&#x02013;human interaction; orange line). When interacting with an AI labelled as human (human&#x02013;AI-perceived-as-human interaction; grey line) or humans labelled as AI (human&#x02013;AI-perceived-as-human interaction; pink line), participants&#x02019; bias increased but less than for the human&#x02013;AI interaction (<italic>n</italic>&#x02009;=&#x02009;200 participants). The shaded areas and error bars represent s.e.m.</p></caption><graphic xlink:href="41562_2024_2077_Fig1_HTML" id="d33e436"/></fig></p><sec id="Sec4"><title>Humans exhibit a small judgement bias</title><p id="Par8">Fifty participants performed an emotion aggregation task (adapted from refs. <sup><xref ref-type="bibr" rid="CR41">41</xref>&#x02013;<xref ref-type="bibr" rid="CR44">44</xref></sup>). On each of 100 trials, participants were presented briefly (500&#x02009;ms) with an array of 12 faces and were asked to report whether the mean emotion expressed by the faces in the array was more sad or more happy (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>; level 1). The faces were sampled from a dataset of 50 morphed faces, created by linearly interpolating between sad and happy expressions (<xref rid="Sec16" ref-type="sec">Methods</xref>). Based on the morphing ratio, each face was ranked from 1 (100% sad face) to 50 (100% happy face). These rankings were closely associated with participants&#x02019; own rankings of each face when observed one by one (<italic>b</italic>&#x02009;=&#x02009;0.8; <italic>t</italic>(50)&#x02009;=&#x02009;26.25; <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; see <xref rid="MOESM1" ref-type="media">Supplementary Results</xref>). We created 100 unique arrays of 12 faces for each participant. The average ranking of the 12 faces in half of the arrays was smaller than 25.5 (thus, the array was more sad) and greater than 25.5 in the other half (thus the array was more happy).</p><p id="Par9">Bias in this task was defined as the difference between the average responses of a participant across all trials and the actual average. The actual average in the task was 0.5, as responses were coded as either 1 (more sad) or 0 (more happy), and exactly half of the trials were more sad and half were more happy. Mathematically, the bias is expressed as:<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\rm{Bias}}}=\frac{1}{n}\mathop{\sum }\limits_{i=1}^{n}{C}_{i}-0.5$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi mathvariant="normal">Bias</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><graphic xlink:href="41562_2024_2077_Article_Equa.gif" position="anchor"/></alternatives></disp-formula>Where <italic>n</italic> denotes the total number of data points and <italic>C</italic><sub><italic>i</italic></sub> denotes the classification assigned to each data point (<italic>C</italic><sub><italic>i</italic></sub>&#x02009;=&#x02009;1 for a more sad classification and <italic>C</italic><sub><italic>i</italic></sub>&#x02009;=&#x02009;0 for a more happy classification). A positive bias indicates a tendency towards classifying responses as more sad, whereas a negative bias suggests a leaning towards classifying responses as more happy. For example, if a participant were to classify 0.7 of the arrays as more sad, their bias would be 0.7&#x02009;&#x02212;&#x02009;0.5&#x02009;=&#x02009;0.2, whereas if they were to classify 0.3 of the arrays as more sad, their bias would be 0.3&#x02009;&#x02212;&#x02009;0.5&#x02009;=&#x02009;&#x02212;0.2.</p><p id="Par10">Consistent with previous studies showing that interpretation of an ambiguous valence is more likely to be negative under short encoding times<sup><xref ref-type="bibr" rid="CR45">45</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>, participants showed a slight but significant tendency to report that the faces were more sad. In particular, they categorized 53.08% of the arrays as more sad, which is a greater proportion than would be expected by chance (permutation test against 50%: <italic>P</italic>&#x02009;=&#x02009;0.017; <italic>d</italic>&#x02009;=&#x02009;0.34; 95% confidence interval (CI)<sub>more sad</sub>&#x02009;=&#x02009;0.51 to 0.56; green circle in Fig. <xref rid="Fig1" ref-type="fig">1e</xref>; see also <xref rid="MOESM1" ref-type="media">Supplementary Results</xref> for estimation of the bias by psychometric function analysis). The bias was much larger in the first block than subsequent blocks (<italic>M</italic><sub>block 1</sub>&#x02009;=&#x02009;56.72%; <italic>M</italic><sub>blocks 2&#x02013;4</sub>&#x02009;=&#x02009;51.87%; permutation test comparing the first block with the rest: <italic>P</italic>&#x02009;=&#x02009;0.002; <italic>d</italic>&#x02009;=&#x02009;0.46; 95% CI&#x02009;=&#x02009;0.02 to 0.08), suggesting that the participants corrected their bias over time.</p></sec><sec id="Sec5"><title>AI trained on biased human data amplifies the bias</title><p id="Par11">Next, we used a CNN<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> to classify each array of faces into more happy or more sad. As detailed below, the CNN amplified the classification bias observed in the human participants (see <xref rid="Sec16" ref-type="sec">Methods</xref> for further details of the model).</p><p id="Par12">First, to test the accuracy of the model, we trained it on the 5,000 arrays that were presented to the participants in level 1 (5,000 arrays&#x02009;=&#x02009;50 participants&#x02009;&#x000d7;&#x02009;100 arrays), with class labels based on the objective ranking scores of the arrays (that is, not the human labels). The model was then evaluated on a 300 out-of-sample test set and showed a classification accuracy of 96%, suggesting that it was highly accurate and did not show a bias if trained on non-biased data (see Table <xref rid="Tab1" ref-type="table">1</xref>). Next, we trained the model on class labels defined based on the human classification (5,000 samples of arrays; Fig. <xref rid="Fig1" ref-type="fig">1a</xref>) and evaluated it on 300 arrays in an out-of-sample test set. The model classified the average emotion as more sad in 65.33% of the cases, despite only 50% of the arrays being more sad. This number was significantly greater than would be expected by chance (permutation test against 50%: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; 95% CI<sub>more sad</sub>&#x02009;=&#x02009;0.60 to 0.71; blue circle in Fig. <xref rid="Fig1" ref-type="fig">1e</xref>) and significantly greater than the bias observed in the human data (level 1), which was only 53% (permutation test: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;1.33; 95% CI&#x02009;=&#x02009;0.09 to 0.14; Fig. <xref rid="Fig1" ref-type="fig">1e</xref>). In other words, the AI algorithm greatly amplified the human bias embedded in the data it was trained on. Similar results were obtained for CNNs with different architectures, including ResNet50 (ref. <sup><xref ref-type="bibr" rid="CR47">47</xref></sup>; see <xref rid="MOESM1" ref-type="media">Supplementary Results</xref>).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Accuracy and bias in the training data and CNN classifications</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Labels</th><th>Objective ranking<break/>(accuracy&#x02009;=&#x02009;100%;<break/>bias&#x02009;=&#x02009;0%)</th><th>Objective ranking + minor bias<break/>(accuracy&#x02009;=&#x02009;97%;<break/>bias&#x02009;=&#x02009;3%)</th><th>Participant classifications<break/>(accuracy&#x02009;=&#x02009;63%;<break/>bias&#x02009;=&#x02009;3%)</th><th>Random labels + minor bias<break/>(accuracy&#x02009;=&#x02009;50%;<break/>bias&#x02009;=&#x02009;3%)</th></tr></thead><tbody><tr><td>Accuracy&#x02009;&#x02212;&#x02009;objective labels</td><td>96%</td><td>94%</td><td>66%</td><td>50%</td></tr><tr><td>Accuracy&#x02009;&#x02013;&#x02009;training labels</td><td>96%</td><td>92%</td><td>69%</td><td>53%</td></tr><tr><td>Bias</td><td>1%</td><td>3%</td><td>15%</td><td>50%</td></tr></tbody></table><table-wrap-foot><p>Training was conducted using four different label sets: (1) objective (based on morphing ranking scores); (2) objective with a 3% bias; (3) participant classifications; and (4) random labels with a 3% bias. The predictions of the model were assessed on an out-of-sample test set of 300 arrays. Accuracy and bias were evaluated with respect to the objective labels and with respect to the labels the models were trained on (training labels).</p></table-wrap-foot></table-wrap></p><p id="Par13">A possible reason for the bias amplification of the AI is that it exploits biases in the data to improve its prediction accuracy. This should happen more when the data are noisy or inconsistent. To test this hypothesis, we retrained the model with two new sets of labels. First, we used non-noisy labels (that is, based on the objective ranking scores of the arrays), but induced a minor bias by switching 3% of the labels. Thus, 53% of the labels were classified as more sad. Second, we used very noisy labels (random labels), in which we also induced a 3% bias. If the bias amplification were due to noise, the bias of the latter model should be higher than that of the former. The results confirmed this hypothesis (Table <xref rid="Tab1" ref-type="table">1</xref>): the average bias of the model trained on the accurate labels with a minor bias was exactly 3%, whereas the average bias of the model trained on the random labels with a bias of 3% was 50% (that is, the model classified 100% of arrays as more sad). These results indicate that the bias amplification of the CNN model is related to the noise in the data.</p></sec><sec id="Sec6"><title>Interaction with biased AI increases human bias</title><p id="Par14">Next, we set out to examine whether interacting with the biased AI algorithm would alter human judgements (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>; level 3). To this end, we first measured participants&#x02019; baseline performance on the emotion aggregation task for 150 trials, so that we could compare their judgements after interacting with the AI versus before. As in level 1, we found that participants had a small bias at first (<italic>M</italic><sub>block 1</sub>&#x02009;=&#x02009;52.23%), which decreased in subsequent blocks, (<italic>M</italic><sub>blocks 2&#x02013;5</sub>&#x02009;=&#x02009;49.23%; permutation test testing the first block against the rest of the blocks: <italic>P</italic>&#x02009;=&#x02009;0.03; <italic>d</italic>&#x02009;=&#x02009;0.31; 95% CI&#x02009;=&#x02009;0.01 to 0.06). The next question was whether interacting with AI would cause the bias to reappear in humans and perhaps even increase.</p><p id="Par15">To test this hypothesis, on each of 300 trials, participants first indicated whether the array of 12 faces was more sad or more happy. They were then presented with the response of the AI to the same array (participants were told that they &#x0201c;will be presented with the response of an AI algorithm that was trained to perform the task&#x0201d;). They were then asked whether they would like to change their initial response or not (that is, from more sad to more happy or vice versa). The participants changed their response on 32.72% (&#x000b1;2.3%&#x02009;s.e.) of the trials in which the AI provided a different response and on 0.3% (&#x000b1;0.1%&#x02009;s.e.) of the trials in which the AI provided the same response as they did (these proportions are significantly different: permutation test: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;1.97; 95% CI&#x02009;=&#x02009;0.28 to 0.37). Further study (Supplementary Experiment <xref rid="MOESM1" ref-type="media">1</xref>) showed that when not interacting with any associate, participants changed their decisions only on 3.97% of trials, which was less than when interacting with a disagreeing AI (permutation test: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;&#x02212;2.53; 95% CI&#x02009;=&#x02009;&#x02212;0.57 to &#x02212;0.42) and more than when interacting with an agreeing AI (permutation test: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;0.98; 95% CI&#x02009;=&#x02009;0.02 to 0.05).</p><p id="Par16">The primary question of interest, however, was not whether participants changed their response after observing the AI&#x02019;s response. Rather, it was whether over time their own response regarding an array (before observing the AI&#x02019;s response to that specific array) became more and more biased due to previous interactions with the AI. That is, did participants learn to become more biased over time?</p><p id="Par17">Indeed, whereas in the baseline blocks participants classified on average only 49.9% (&#x000b1;1.1%&#x02009;s.e.) of the arrays as more sad, when interacting with the AI this rate increased significantly to 56.3% (&#x000b1;1.1%&#x02009;s.e.; permutation test for interaction blocks against baseline: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;0.84; 95% CI<sub>more sad</sub>&#x02009;=&#x02009;0.54 to 0.59). The learned bias increased over time: in the first interaction block it was only 50.72%, whereas in the last interaction block it was 61.44%. This increase in bias was confirmed by a linear mixed model predicting a higher rate of more sad classifications as the block number (a fixed factor) increased, with random intercepts and slopes at the participant level (<italic>b</italic>&#x02009;=&#x02009;0.02; <italic>t</italic>(50)&#x02009;=&#x02009;6.23; <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; Fig. <xref rid="Fig1" ref-type="fig">1f</xref>).</p><p id="Par18">These results demonstrate an algorithmic bias feedback loop; training an AI algorithm on a set of slightly biased human data results in the algorithm amplifying it. Subsequent interactions of other humans with this algorithm further increase the humans&#x02019; initial bias levels, creating a feedback loop.</p></sec></sec><sec id="Sec7"><title>Human&#x02013;human interactions did not amplify bias</title><p id="Par19">Next, we investigated whether the same degree of bias contagion occurs in interactions involving only humans. To this end, we used the same interaction structure as above, except the AI system was replaced with human participants (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>).</p><sec id="Sec8"><title>Humans exhibit a small judgement bias</title><p id="Par20">The responses used in the first level of the human&#x02013;human interaction were the same as those used in the human&#x02013;AI interaction described above.</p></sec><sec id="Sec9"><title>Humans trained on human data do not amplify bias</title><p id="Par21">Conceptually similar to AI algorithm training, here we aimed to train humans on human data (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>; level 2). The participants were presented with 100 arrays of 12 faces. They were told they would be presented with the responses of other participants who performed the task before. For each of the 100 arrays, they observed the response of a pseudo-randomly selected participant from level 1 (see <xref rid="Sec16" ref-type="sec">Methods</xref> for further details). Thereafter, they judged ten new arrays on their own (as either more sad or more happy). To verify that the participants attended to the responses of the other level 1 participants, they were asked to report them on 20% of the trials (randomly chosen). Participants who gave an incorrect answer on more than 10% of the trials (and thus were not attending the task; <italic>n</italic>&#x02009;=&#x02009;14), were excluded from the experiment.</p><p id="Par22">Participants characterized the arrays as more sad 54.8% of the time, which is more than would be expected by chance (permutation test against 50%: <italic>P</italic>&#x02009;=&#x02009;0.007; <italic>d</italic>&#x02009;=&#x02009;0.41; 95% CI<sub>more sad</sub>&#x02009;=&#x02009;52 to 58%). Critically, this result did not differ from that of level 1 human participants (permutation test level 1 humans versus level 2 humans: <italic>P</italic>&#x02009;=&#x02009;0.43; <italic>d</italic>&#x02009;=&#x02009;0.11; 95% CI&#x02009;=&#x02009;&#x02212;0.02 to 0.06; Fig. <xref rid="Fig1" ref-type="fig">1e</xref>), but was significantly lower than for the AI algorithm, which characterized 65.13% of the arrays as more sad (permutation test level 2 humans against level 2 AI: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;0.86; 95% CI&#x02009;=&#x02009;&#x02212;0.07 to &#x02212;0.013; Fig. <xref rid="Fig1" ref-type="fig">1e</xref>). This difference was unlikely to have been driven by variations in training sample sizes, as the effect was observed even when AI and human participants were trained on identical datasets (Supplementary Experiment <xref rid="MOESM1" ref-type="media">2</xref>). Furthermore, the results were generalized to a different training method, in which participants were incentivized to actively predict the responses of other participants (Supplementary Experiment <xref rid="MOESM1" ref-type="media">3</xref>).</p><p id="Par23">In conclusion, unlike the AI, human bias was not amplified after being trained on biased human data. This is not surprising, as the level of bias participants in level 2 naturally exhibit is probably the same as the one they were trained on. Moreover, unlike AI systems, humans base their judgements on factors that go beyond the training session, such as previous experiences and expectations.</p></sec><sec id="Sec10"><title>Human&#x02013;human interaction does not increase bias</title><p id="Par24">Next, we exposed a new pool of participants (<italic>n</italic>&#x02009;=&#x02009;50) to the judgements of humans from level 2. The task and analysis were identical to those described for level 3 of the human&#x02013;AI interaction (except, of course, participants were interacting with humans, which they were made aware of; Fig. <xref rid="Fig1" ref-type="fig">1b</xref>).</p><p id="Par25">Before being exposed to the other human&#x02019;s response, participants completed five baseline blocks. As in levels 1 and 3 (human&#x02013;AI interaction), participants showed a significant bias during the first block <italic>(M</italic><sub>block 1</sub>&#x02009;=&#x02009;53.67%) which disappeared over time (<italic>M</italic><sub>blocks 2&#x02013;5</sub>&#x02009;=&#x02009;49.87%; permutation test for the first baseline block against the rest of the baseline blocks: <italic>P</italic>&#x02009;=&#x02009;0.007; <italic>d</italic>&#x02009;=&#x02009;0.40; 95% CI&#x02009;=&#x02009;0.01 to 0.06).</p><p id="Par26">Next, participants interacted with other human participants (human&#x02013;human interaction; level 2). As expected, participants changed their classification more when the other participants disagreed with them (11.27&#x02009;&#x000b1;&#x02009;1.4%&#x02009;s.e.) than when they agreed with them (0.2&#x02009;&#x000b1;&#x02009;0.03%&#x02009;s.e.) (permutation test comparing the two: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;1.11; 95% CI&#x02009;=&#x02009;0.08 to 0.14) and less than when interacting with a disagreeing AI (which was 32.72%; permutation test comparing the response change when interacting with a disagreeing AI compared with interacting with a disagreeing human: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;1.07; 95% CI&#x02009;=&#x02009;0.16 to 0.27).</p><p id="Par27">Importantly, there was no evidence of learned bias in the human&#x02013;human interaction (Fig. <xref rid="Fig1" ref-type="fig">1f</xref>). Classification rates were no different when interacting with other humans (<italic>M</italic><sub>more sad</sub>&#x02009;=&#x02009;51.45&#x02009;&#x000b1;&#x02009;1.3%&#x02009;s.e.) than baseline (50.6&#x02009;&#x000b1;&#x02009;1.3%&#x02009;s.e.) (permutation test for interaction blocks against baseline: <italic>P</italic>&#x02009;=&#x02009;0.48; <italic>d</italic>&#x02009;=&#x02009;0.10; 95% CI<sub>more sad</sub>&#x02009;=&#x02009;&#x02212;0.01 to 0.03) and did not change over time (<italic>b</italic>&#x02009;=&#x02009;0.003; <italic>t</italic>(50)&#x02009;=&#x02009;1.1; <italic>P</italic>&#x02009;=&#x02009;0.27).</p><p id="Par28">Taken together, these results indicate that human bias is significantly amplified in a human&#x02013;AI interaction, more so than in interactions between humans. These findings suggest that the impact of biased AI systems extends beyond their own biased judgement to their ability to bias human judgement. This raises concerns for human interactions with potentially biased algorithms across different domains.</p></sec><sec id="Sec11"><title>AI&#x02019;s output and human perception of AI shape its influence</title><p id="Par29">A question that arises is whether participants became more biased when interacting with the AI system compared with humans because the AI provided more biased judgements, because they perceived the AI system differently than other humans, or both. To address this question, we ran two additional iterations of the experiment. In the first iteration (AI perceived as human), participants interacted with an AI system but were told they were interacting with another human participant (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>). In the second iteration (human perceived as AI), participants interacted with an AI system but were told they were interacting with another human participant (Fig. <xref rid="Fig1" ref-type="fig">1d</xref>).</p><p id="Par30">To this end, new pools of participants (<italic>n</italic>&#x02009;=&#x02009;50 per condition) were recruited. First, they performed the baseline test described above and then they interacted with their associate (level 3). When interacting with the AI (which was believed to be a human) participants&#x02019; bias increased over time: in the first interaction block it was only 50.5%, whereas in the last interaction block it was 55.28% (Fig. <xref rid="Fig1" ref-type="fig">1f</xref>). The increase in bias across blocks was confirmed by a linear mixed model predicting a higher rate of more sad classifications as the block number (a fixed factor) increased, with random intercepts and slopes at the participant level (<italic>b</italic>&#x02009;=&#x02009;0.01; <italic>t</italic>(50)&#x02009;=&#x02009;3.14; <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001). Similar results were obtained for the human&#x02013;human-perceived-as-AI interaction. The bias increased across blocks (from 49.0% in the first block to 54.6% in the last), as was confirmed by a linear mixed model (<italic>b</italic>&#x02009;=&#x02009;0.01; <italic>t</italic>(50)&#x02009;=&#x02009;2.85; <italic>P</italic>&#x02009;=&#x02009;0.004; Fig. <xref rid="Fig1" ref-type="fig">1f</xref>). In both cases, the bias was greater than at baseline (human&#x02013;AI perceived as human: <italic>M</italic><sub>bias</sub>&#x02009;=&#x02009;3.85 (permutation test comparing with baseline: <italic>P</italic>&#x02009;=&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;0.49; 95% CI&#x02009;=&#x02009;0.02 to 0.06); human&#x02013;human perceived as AI: <italic>M</italic><sub>bias</sub>&#x02009;=&#x02009;2.49 (permutation test comparing with baseline: <italic>P</italic>&#x02009;=&#x02009;0.04; <italic>d</italic>&#x02009;=&#x02009;0.29; 95% CI&#x02009;=&#x02009;0.01 to 0.05)).</p><p id="Par31">Was the induced bias a consequence of the type of input (AI versus human) or the perception of that input (perceived as AI versus perceived as human)? To investigate this, we submitted the induced bias scores (the percentage of more sad judgements minus the baseline percentage of more sad judgements) into a 2 (input: AI versus human)&#x02009;&#x000d7;&#x02009;2 (label: AI versus human) analysis of variance (ANOVA) with time (blocks 1&#x02013;6) as a covariate (Fig. <xref rid="Fig1" ref-type="fig">1f</xref>). The results revealed interactions between input and time (<italic>F</italic>(4.55, 892.35)&#x02009;=&#x02009;3.40; <italic>P</italic>&#x02009;=&#x02009;0.006) and between label and time (<italic>F</italic>(4.55, 892.35)&#x02009;=&#x02009;2.65; <italic>P</italic>&#x02009;=&#x02009;0.026). In addition, there were main effects of input (<italic>F</italic>(1, 196)&#x02009;=&#x02009;9.45; <italic>P</italic>&#x02009;=&#x02009;0.002) and time (<italic>F</italic>(4.55, 892.35)&#x02009;=&#x02009;14.80; <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001). No other effects were significant (all <italic>P</italic>&#x02009;values&#x02009;&#x0003e;&#x02009;0.06). Thus, as illustrated in Fig. <xref rid="Fig1" ref-type="fig">1f</xref>, both the AI&#x02019;s input and its label contributed to enhanced bias in humans over time.</p><p id="Par32">Finally, we assessed the rate of decision changes among participants. Participants were more likely to change their classification when their associate disagreed with them. In human&#x02013;AI-perceived-as-human interactions, decision changes occurred at a rate of 16.84% (&#x000b1;1.2%&#x02009;s.e.) when there was a disagreement, compared with a mere 0.2% (&#x000b1;0.05%&#x02009;s.e.) when agreeing (permutation test comparing the two: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;1.22; 95% CI&#x02009;=&#x02009;0.13 to 0.20). Similarly, for the human&#x02013;human-perceived-as-AI condition, decision changes were observed in 31.84% (&#x000b1;2.5%&#x02009;s.e.) when disagreement existed, compared with 0.4% (&#x000b1;0.1%&#x02009;s.e.) in cases of agreement (permutation test comparing the two: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;1.7; 95% CI&#x02009;=&#x02009;0.26 to 0.36).</p><p id="Par33">To quantify the effects of input and label on decision changes in cases of disagreement, we submitted the percentage of decision change into a 2 (input: AI versus human)&#x02009;&#x000d7;&#x02009;2 (label: AI versus human) ANOVA with time (blocks 1&#x02013;6) as a covariate. The results revealed that both the AI&#x02019;s input (<italic>F</italic>(1, 196)&#x02009;=&#x02009;7.05; <italic>P</italic>&#x02009;=&#x02009;0.009) and its label (<italic>F</italic>(1, 196)&#x02009;=&#x02009;76.30; <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001) increased the likelihood of a decision change. These results remained consistent after applying Welch&#x02019;s correction to address violations of the homogeneity of variance assumption: for AI&#x02019;s input <italic>F</italic>(1, 197.92)&#x02009;=&#x02009;5.11 and <italic>P</italic>&#x02009;=&#x02009;0.02 and for AI&#x02019;s label <italic>F</italic>(1, 175.57)&#x02009;=&#x02009;74.21 and <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001. All other main effects and interactions were not significant (all <italic>P</italic>&#x02009;values&#x02009;&#x0003e;&#x02009;0.13).</p></sec></sec><sec id="Sec12"><title>Biased algorithms bias decisions, whereas accurate ones improve them</title><p id="Par34">Next, we sought to generalize the above results to different types of algorithm and domain. In particular, we aimed to mimic a situation in which humans are not a priori biased, but rather AI bias emerges for other reasons (for example, if it was trained on unbalanced data). To this end, we employed a variant of the random dot kinematogram (RDK) task<sup><xref ref-type="bibr" rid="CR48">48</xref>&#x02013;<xref ref-type="bibr" rid="CR51">51</xref></sup>, in which participants were presented with an array of moving dots and asked to estimate the percentage of dots that moved from left to right on a scale ranging from 0% (no dots moved from left to right) to 100% (all dots moved from left to right). To estimate baseline performance, participants first performed the RDK task on their own for 30 trials and reported their confidence on a scale ranging from not confident at all to very confident (Fig. <xref rid="Fig2" ref-type="fig">2a</xref>). Across trials, the actual average percentage of dots that moved rightward was 50.13&#x02009;&#x000b1;&#x02009;20.18% (s.d.), which was not significantly different from 50% (permutation test against 50%: <italic>P</italic>&#x02009;=&#x02009;0.98; <italic>d</italic>&#x02009;=&#x02009;0.01; 95% CI&#x02009;=&#x02009;42.93 to 57.33%), and the average confidence was 0.56&#x02009;&#x000b1;&#x02009;0.17 (s.d.).<fig id="Fig2"><label>Fig. 2</label><caption><title>A biased algorithm produces human bias, whereas an accurate algorithm improves human judgement.</title><p><bold>a</bold>, Baseline block. Participants performed the RDK task, in which an array of moving dots was presented for 1&#x02009;s. They estimated the percentage of dots that moved from left to right and reported their confidence. <bold>b</bold>, Algorithms. Participants interacted with three algorithms: accurate (blue distribution), biased (orange distribution) and noisy (red distribution). <bold>c</bold>, Interaction blocks. Participants provided their independent judgement and confidence (self-paced) and then observed their own response and a question mark where the AI algorithm response would later appear. Participants were asked to assign weights to their response and the response of the algorithm (self-paced). Thereafter, the response of the algorithm was revealed (2&#x02009;s). Note that the AI algorithm&#x02019;s response was revealed only after the participants indicated their weighting. As a result, they had to rely on their global evaluation of the AI based on previous trials. <bold>d</bold>, AI-induced bias. Interacting with a biased AI resulted in significant human bias relative to baseline (<italic>P</italic> values shown in red) and relative to interactions with the other algorithms (<italic>P</italic> values shown in black; <italic>n</italic>&#x02009;=&#x02009;120). <bold>e</bold>, When interacting with a biased algorithm, AI-induced bias increases over time (<italic>n</italic>&#x02009;=&#x02009;50). <bold>f</bold>, AI-induced accuracy change. Interacting with an accurate AI resulted in a significant increase in human accuracy (that is, reduced error) relative to baseline (<italic>P</italic> values shown in red) and relative to interactions with the other algorithms (<italic>P</italic> values shown in black; <italic>n</italic>&#x02009;=&#x02009;120). <bold>g</bold>, When interacting with an accurate algorithm, AI-induced accuracy increases over time (<italic>n</italic>&#x02009;=&#x02009;50). <bold>h</bold>,<bold>i</bold>, Participants perceived the influence of the accurate algorithm on their judgements to be greatest (<bold>h</bold>; <italic>n</italic>&#x02009;=&#x02009;120), even though the actual influence of the accurate and biased algorithms was the same (<bold>i</bold>; <italic>n</italic>&#x02009;=&#x02009;120). The thin grey lines and circles correspond to individual participants. In <bold>d</bold> and <bold>f</bold>, the circles correspond to group means, the central lines represent median values and the bottom and top edges are the 25th and 75th percentiles, respectively. In <bold>e</bold> and <bold>g</bold>, the error bars represent s.e.m. The <italic>P</italic>&#x02009;values were derived using permutation tests. All significant <italic>P</italic>&#x02009;values remained significant after applying Benjamini&#x02013;Hochberg false discovery rate correction at <italic>&#x003b1;</italic>&#x02009;=&#x02009;0.05.</p></caption><graphic xlink:href="41562_2024_2077_Fig2_HTML" id="d33e1137"/></fig></p><p id="Par35">To examine whether and how different algorithmic response patterns affect human decision-making, we used three simple algorithms: accurate, biased and noisy. The accurate algorithm always indicated the correct percentage of dots that moved from left to right (Fig. <xref rid="Fig2" ref-type="fig">2b</xref>; blue distribution). The biased algorithm provided systematically upward biased estimates of dots that moved to the right (Fig. <xref rid="Fig2" ref-type="fig">2b</xref>; orange distribution; <italic>M</italic><sub>bias</sub>&#x02009;=&#x02009;24.96). The noisy algorithm provided responses that were equal to those of the accurate algorithm plus Gaussian noise (s.d.&#x02009;=&#x02009;30; Fig. <xref rid="Fig2" ref-type="fig">2b</xref>; red distribution). The biased and noisy algorithms had the same absolute error (<xref rid="Sec16" ref-type="sec">Methods</xref>). The algorithms used here were hard coded to allow full control over their responses.</p><p id="Par36">On each trial, participants first provided their judgement and confidence and then observed their own response and a question mark where the algorithm response would later appear (Fig. <xref rid="Fig2" ref-type="fig">2c</xref>). They were asked to assign weight to their own response and to that of the algorithm on a scale ranging from 100% you to 100% AI (<xref rid="Sec16" ref-type="sec">Methods</xref>). Thus, if a participant assigned a weight of <italic>w</italic> to their own response, the final joint decision would be:<disp-formula id="Equb"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{l}{{\rm{Final}}\; {\rm{joint}}\; {\rm{decision}}}\\=w \times({{\rm{participant}}}\mbox{'}{{\rm{s}}\; {\rm{response}}})+(1-w)\times\,({{\rm{AI}}}\mbox{'}{{\rm{s}}\; {\rm{response}}})\end{array}$$\end{document}</tex-math><mml:math id="M4"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="normal">Final</mml:mi><mml:mspace width="0.16em"/><mml:mi mathvariant="normal">joint</mml:mi><mml:mspace width="0.16em"/><mml:mi mathvariant="normal">decision</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">participant</mml:mi><mml:mi>&#x02019;</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mspace width="0.16em"/><mml:mi mathvariant="normal">response</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">AI</mml:mi><mml:mi>&#x02019;</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mspace width="0.16em"/><mml:mi mathvariant="normal">response</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="41562_2024_2077_Article_Equb.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par37">This weighting task is analogous to the change decision task in experiment 1; however, here we used a continuous scale instead of a binary choice, allowing us to obtain a finer assessment of participants&#x02019; judgements.</p><p id="Par38">After participants provided their response, the response of the AI algorithm was revealed (Fig. <xref rid="Fig2" ref-type="fig">2c</xref>). Note that the AI algorithm response was exposed only after the participants indicated their weighting. This was done to prevent participants from relying on the concrete response of the algorithm on a specific trial, instead making them rely on their global evaluation of the algorithm. The participants interacted with each algorithm for 30 trials. The order of the algorithms (bias, noisy or accurate) was counterbalanced.</p><p id="Par39">Bias in the RDK task was defined as follows:<disp-formula id="Equc"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{Bias}}=\frac{{\sum }_{i=1}^{n}({\rm{Participant}}\mbox{'}{\rm{s}}\,{\rm{response}}_{i}-{\rm{Evidence}}_{i})}{n}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi mathvariant="normal">Bias</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mo mathsize="big">&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Participant</mml:mi><mml:mi>&#x02019;</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">response</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Evidence</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41562_2024_2077_Article_Equc.gif" position="anchor"/></alternatives></disp-formula>where <italic>i</italic> and <italic>n</italic> correspond to the index of the present trial and the total number of trials, respectively. Evidence corresponds to the percentage of dots that moved rightward in the <italic>i</italic>-th trial. To compute AI-induced bias in participants, we subtracted the participant&#x02019;s bias in the baseline block from the bias in the interaction blocks.<disp-formula id="Equd"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\rm{AI}}{\mbox{-}}{\rm{induced}}\; {\rm{bias}}}={{\rm{Bias}}}_{{{\rm{AI}}\; {\rm{interaction}}\; {\rm{blocks}}}}-{{\rm{Bias}}}_{{{\rm{baseline}}}}$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi mathvariant="normal">AI</mml:mi><mml:mstyle><mml:mtext>-</mml:mtext></mml:mstyle><mml:mi mathvariant="normal">induced</mml:mi><mml:mspace width="0.16em"/><mml:mi mathvariant="normal">bias</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Bias</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">AI</mml:mi><mml:mspace width="0.16em"/><mml:mi mathvariant="normal">interaction</mml:mi><mml:mspace width="0.16em"/><mml:mi mathvariant="normal">blocks</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Bias</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">baseline</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><graphic xlink:href="41562_2024_2077_Article_Equd.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par40">At the group level, no systematic bias in baseline responses was detected (mean response at baseline&#x02009;=&#x02009;0.62; permutation test against 0: <italic>P</italic>&#x02009;=&#x02009;0.28; <italic>d</italic>&#x02009;=&#x02009;0.1; 95% CI&#x02009;=&#x02009;&#x02212;0.48 to 1.76).</p><p id="Par41">To define accuracy, we first computed an error score for each participant:<disp-formula id="Eque"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{Error}}=\frac{{\sum }_{i=1}^{n}|{\rm{Participant}}\mbox{'} s\,{\rm{response}}_{i}-{\rm{Evidence}}_{i}|}{n}$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi mathvariant="normal">Error</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mo mathsize="big">&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02223;</mml:mo><mml:mi mathvariant="normal">Participant</mml:mi><mml:mi>&#x02019;</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">response</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Evidence</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41562_2024_2077_Article_Eque.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par42">Then, this quantity was subtracted from the error score in the baseline block, indicating changes in accuracy.<disp-formula id="Equf"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\rm{AI}}{\mbox{-}}{\rm{induced}}\; {\rm{accuracy}}\; {\rm{change}}}={{\rm{Error}}}_{{{\rm{baseline}}}}-{{\rm{Error}}}_{{{\rm{AI}}\; {\rm{interaction}}\; {\rm{blocks}}}}$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi mathvariant="normal">AI</mml:mi><mml:mstyle><mml:mtext>-</mml:mtext></mml:mstyle><mml:mi mathvariant="normal">induced</mml:mi><mml:mspace width="0.16em"/><mml:mi mathvariant="normal">accuracy</mml:mi><mml:mspace width="0.16em"/><mml:mi mathvariant="normal">change</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Error</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">baseline</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Error</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">AI</mml:mi><mml:mspace width="0.16em"/><mml:mi mathvariant="normal">interaction</mml:mi><mml:mspace width="0.16em"/><mml:mi mathvariant="normal">blocks</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><graphic xlink:href="41562_2024_2077_Article_Equf.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par43">That is, if errors when interacting with the AI (second quantity) were smaller than baseline errors (first quantity), the change would be positive, indicating that participants became more accurate. However, if errors when interacting with the AI (second quantity) were larger than during baseline (first quantity), the change would be negative, indicating that participants became less accurate when interacting with the AI.</p><p id="Par44">The results revealed that participants became more biased (towards the right) when interacting with the biased algorithm relative to baseline performance (<italic>M</italic><sub>bias (biased AI)</sub>&#x02009;=&#x02009;2.66 and <italic>M</italic><sub>bias (baseline)</sub>&#x02009;=&#x02009;0.62; permutation test: <italic>P</italic>&#x02009;=&#x02009;0.002; <italic>d</italic>&#x02009;=&#x02009;0.28; 95% CI&#x02009;=&#x02009;0.76 to 3.35; Fig. <xref rid="Fig2" ref-type="fig">2d</xref>) and relative to when interacting with the accurate algorithm (<italic>M</italic><sub>bias (accurate AI)</sub>&#x02009;=&#x02009;1.26; permutation test: <italic>P</italic>&#x02009;=&#x02009;0.006; <italic>d</italic>&#x02009;=&#x02009;0.25; 95% CI&#x02009;=&#x02009;0.42 to 2.37; Fig. <xref rid="Fig2" ref-type="fig">2d</xref>) and the noisy algorithm (<italic>M</italic><sub>bias (noisy AI)</sub>&#x02009;=&#x02009;1.15; permutation test: <italic>P</italic>&#x02009;=&#x02009;0.006; <italic>d</italic>&#x02009;=&#x02009;0.25; 95% CI&#x02009;=&#x02009;0.44 to 2.56; Fig. <xref rid="Fig2" ref-type="fig">2d</xref>). No differences in bias were found between the accurate and noisy algorithms, nor when interacting with these algorithms relative to baseline performance (all <italic>P</italic>&#x02009;values&#x02009;&#x0003e;&#x02009;0.28). See also <xref rid="MOESM1" ref-type="media">Supplementary Results</xref> for analysis of the AI-induced bias on a trial-by-trial basis.</p><p id="Par45">The AI-induced bias was replicated in a follow-up study (<italic>n</italic>&#x02009;=&#x02009;50; <xref rid="Sec16" ref-type="sec">Methods</xref>) in which participants interacted exclusively with a biased algorithm across five blocks (<italic>M</italic><sub>bias</sub>&#x02009;=&#x02009;5.03; permutation test: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;0.72; 95% CI&#x02009;=&#x02009;3.14 to 6.98; Fig. <xref rid="Fig2" ref-type="fig">2e</xref>). Critically, we found a significant linear relationship over time (<italic>b</italic>&#x02009;=&#x02009;1.0; <italic>t</italic>(50)&#x02009;=&#x02009;2.99; <italic>P</italic>&#x02009;=&#x02009;0.004; Fig. <xref rid="Fig2" ref-type="fig">2e</xref>), indicating that the more participants interacted with the biased algorithm, the more biased their judgements became. The learning of bias induced by the AI was also supported by a computational learning model (<xref rid="MOESM1" ref-type="media">Supplementary Models</xref>).</p><p id="Par46">Interaction with the accurate algorithm increased the accuracy of participants&#x02019; independent judgements compared with baseline performance (<italic>M</italic><sub>errors (accurate AI)</sub>&#x02009;=&#x02009;13.48, <italic>M</italic><sub>errors (baseline)</sub>&#x02009;=&#x02009;15.03 and <italic>M</italic><sub>accuracy change (accurate AI)</sub>&#x02009;=&#x02009;1.55; permutation test: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;0.32; 95% CI&#x02009;=&#x02009;0.69 to 2.42; Fig. <xref rid="Fig2" ref-type="fig">2f</xref>) and compared with when interacting with the biased algorithm (<italic>M</italic><sub>errors (biased AI)</sub>&#x02009;=&#x02009;14.73 and <italic>M</italic><sub>accuracy change (biased AI)</sub>&#x02009;=&#x02009;0.03; permutation test: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;0.33; 95% CI&#x02009;=&#x02009;0.58 to 1.94; Fig. <xref rid="Fig2" ref-type="fig">2f</xref>) and the noisy algorithm (<italic>M</italic><sub>errors (noisy AI)</sub>&#x02009;=&#x02009;14.36 and <italic>M</italic><sub>accuracy change (noisy AI)</sub>&#x02009;=&#x02009;0.67; permutation test: <italic>P</italic>&#x02009;=&#x02009;0.01; <italic>d</italic>&#x02009;=&#x02009;0.22; 95% CI&#x02009;=&#x02009;0.22 to 1.53; Fig. <xref rid="Fig2" ref-type="fig">2f</xref>). No differences in induced accuracy change were found between the biased and noisy algorithms, nor were there differences in errors when interacting with these algorithms relative to baseline performance (all <italic>P</italic>&#x02009;values&#x02009;&#x0003e;&#x02009;0.14; Fig. <xref rid="Fig2" ref-type="fig">2f</xref>).</p><p id="Par47">The AI-induced accuracy change was replicated in a follow-up study (<italic>n</italic>&#x02009;=&#x02009;50; <xref rid="Sec16" ref-type="sec">Methods</xref>) in which participants interacted exclusively with an accurate algorithm across five blocks (<italic>M</italic><sub>accuracy change</sub>&#x02009;=&#x02009;3.55; permutation test: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;0.64; 95% CI&#x02009;=&#x02009;2.14 to 5.16; Fig. <xref rid="Fig2" ref-type="fig">2g</xref>). Critically, we found a significant linear relationship for the AI-induced accuracy change over time (<italic>b</italic>&#x02009;=&#x02009;0.84; <italic>t</italic>(50)&#x02009;=&#x02009;5.65; <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; Fig. <xref rid="Fig2" ref-type="fig">2g</xref>), indicating that the more participants interacted with the accurate algorithm, the more accurate their judgements became. For participants&#x02019; confidence rating and weight assignment decisions, see <xref rid="MOESM1" ref-type="media">Supplementary Results</xref>.</p><p id="Par48">Importantly, the increase in accuracy when interacting with the accurate AI could not be attributed to participants copying the algorithm&#x02019;s accurate response, not could the increased bias when interacting with the biased algorithm be attributed to participants copying the algorithm&#x02019;s biased responses. This is because we purposefully designed the task such that participants would indicate their judgements on each trial before they observed the algorithm&#x02019;s response. Instead, the participants learned to provide more accurate judgements in the former case and learned to provide more biased judgements in the latter case.</p><sec id="Sec13"><title>Participants underestimate the biased algorithm&#x02019;s impact</title><p id="Par49">We sought to explore whether participants were aware of the substantial influence the algorithms had on them. To test this, participants were asked to evaluate to what extent they believed their responses were influenced by the different algorithms they interacted with (<xref rid="Sec16" ref-type="sec">Methods</xref>). As shown in Fig. <xref rid="Fig2" ref-type="fig">2h</xref>, participants reported being more influenced by the accurate algorithm compared with the biased one (permutation test: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;0.57; 95% CI&#x02009;=&#x02009;0.76 to 1.44) and the noisy one (permutation test: <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>d</italic>&#x02009;=&#x02009;0.58; 95% CI&#x02009;=&#x02009;0.98 to 1.67). No significant difference was found between how participants perceived the influence of the biased and noisy algorithms (permutation test: <italic>P</italic>&#x02009;=&#x02009;0.11; <italic>d</italic>&#x02009;=&#x02009;0.15; 95% CI&#x02009;=&#x02009;&#x02212;0.05 to 0.52).</p><p id="Par50">In reality, however, the magnitude by which they became more biased when interacting with a biased algorithm was equal to the magnitude by which they became more accurate when interacting with an accurate algorithm. We quantified influence using two different methods (<xref rid="Sec16" ref-type="sec">Methods</xref>) and both revealed the same result (Fig. <xref rid="Fig2" ref-type="fig">2i</xref>; <italic>z</italic>-scoring across algorithms: permutation test: <italic>P</italic>&#x02009;=&#x02009;0.90; <italic>d</italic>&#x02009;=&#x02009;&#x02212;0.01; 95% CI&#x02009;=&#x02009;&#x02212;0.19 to 0.17; as a percentage difference relative to baseline: permutation test: <italic>P</italic>&#x02009;=&#x02009;0.89; <italic>d</italic>&#x02009;=&#x02009;&#x02212;0.02; 95% CI&#x02009;=&#x02009;&#x02212;1.44 to 1.90).</p><p id="Par51">These results show that in different paradigms, and under different response protocols, interacting with a biased algorithm biases participants&#x02019; independent judgements. Moreover, interacting with an accurate algorithm increased the accuracy of participants&#x02019; independent judgements. Strikingly, the participants were unaware of the strong effect that the biased algorithm had on them.</p></sec></sec><sec id="Sec14"><title>Real-world generative AI-induced bias in social judgements</title><p id="Par52">Thus far, we have demonstrated that interacting with biased algorithms leads to more biased human judgements in perceptual and emotion-based tasks. These tasks allowed for precise measurements and facilitated our ability to dissociate effects. Next, we aimed to generalize these findings to social judgements by using AI systems commonly employed in real-world settings, thereby increasing the ecological validity of our results<sup><xref ref-type="bibr" rid="CR52">52</xref>&#x02013;<xref ref-type="bibr" rid="CR54">54</xref></sup> (see also Supplementary Experiment <xref rid="MOESM1" ref-type="media">5</xref> for a controlled experiment examining a social judgement task). To this end, we examined changes to human judgements following interactions with Stable Diffusion&#x02014;a widely used generative AI system designed to create images based on textual prompts<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>.</p><p id="Par53">Recent studies have reported that Stable Diffusion amplifies existing social imbalances. For example, it over-represents White men in high-power and high-income professions compared with other demographic groups<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR56">56</xref></sup>. Such biases can stem from different sources, including problematic training data and/or flawed content moderation techniques<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. Stable Diffusion outputs are used in diverse applications, such as videos, advertisements and business presentations. Consequently, these outputs have the potential to impact humans&#x02019; belief systems, even when an individual does not directly interact with the AI system but merely observes its output (for example, on social media, in advertisements or during a colleague&#x02019;s presentation). Here, we test whether interacting with Stable Diffusion&#x02019;s outputs increases bias in human judgement.</p><p id="Par54">To test this, we first prompted Stable Diffusion to create: &#x0201c;A color photo of a financial manager, headshot, high-quality&#x0201d; (<xref rid="Sec16" ref-type="sec">Methods</xref>). As expected, the images produced by Stable Diffusion over-represented White men (85% of images) relative to their representation in the population. For example, in the United States only 44.3% of financial managers are men<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>, of whom a fraction are White, and in the United Kingdom only about half are men<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>, of whom a fraction are White. In other Western countries the percentage of financial managers who are White men is also less than 85% and in many non-Western countries the numbers are probably even lower.</p><p id="Par55">Next, we conducted an experiment (<italic>n</italic>&#x02009;=&#x02009;100) to examine how participants&#x02019; judgements about who is most likely to be a financial manager would alter after interactions with Stable Diffusion. To this end, before and after interacting with Stable Diffusion, participants completed 100 trials. On each trial, they were presented with images of six individuals from different race and gender groups: (1) White men; (2) White women; (3) Asian men; (4) Asian women; (5) Black men; and (6) Black women (see Fig. <xref rid="Fig3" ref-type="fig">3a</xref>; stage 1; baseline). The images were taken from the Chicago Face Database<sup><xref ref-type="bibr" rid="CR59">59</xref></sup> and were balanced in terms of age, attractiveness and racial prototypicality (<xref rid="Sec16" ref-type="sec">Methods</xref>). On each trial, participants were asked: &#x0201c;which person is most likely to be a financial manager?&#x0201d;. They responded by clicking on one of the images. Before this, participants were provided with a definition of financial manager (<xref rid="Sec16" ref-type="sec">Methods</xref>). We were interested in whether participants&#x02019; responses would gravitate towards White men after interacting with Stable Diffusion outputs.<fig id="Fig3"><label>Fig. 3</label><caption><title>Interaction with a real-world AI system amplifies human bias (<italic>n</italic>&#x02009;=&#x02009;100).</title><p><bold>a</bold>, Experimental design. The experiment consisted of three stages. In stage 1, participants were presented with images featuring six individuals from different race and gender groups: a White man, a White woman, an Asian man, an Asian woman, a Black man and a Black woman. On each trial, participants selected the person who they thought was most likely to be a financial manager. In stage 2, for each trial, three images of financial managers generated by Stable Diffusion were randomly chosen and presented to the participants. In the control condition, participants were presented with three images of fractals instead. In stage 3, participants repeated the task from stage 1, allowing measurement of the change in participants&#x02019; choices before versus after exposure to the AI-generated images. <bold>b</bold>, The results revealed a significant increase in participants&#x02019; inclination to choose White men as financial managers after being exposed to AI-generated images, but not after being exposed to fractal neutral images (control). The error bars represent s.e.m. Face stimuli in <bold>a</bold> reproduced from ref. <sup><xref ref-type="bibr" rid="CR59">59</xref></sup> under a Creative Commons licence <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</ext-link>.</p></caption><graphic xlink:href="41562_2024_2077_Fig3_HTML" id="d33e1774"/></fig></p><p id="Par56">Before interacting with Stable Diffusion, participants selected White men, White women, Asian men, Asian women, Black men and Black women 32.36, 14.94, 14.40, 20.24, 6.64 and 11.12% of the time, respectively. Although there is no definitive ground truth here, based on demographic data, White men is estimated not to be a normative response (for details, see <xref rid="MOESM1" ref-type="media">Supplementary Results</xref>). Next, participants were exposed to the outputs of Stable Diffusion (see Fig. <xref rid="Fig3" ref-type="fig">3a</xref>; stage 2; exposure). Specifically, participants were told that they would be shown three images of financial managers generated by AI (Stable Diffusion) and received a brief explanation about Stable Diffusion (<xref rid="Sec16" ref-type="sec">Methods</xref>). Then, on each trial, participants viewed three images of financial managers that were randomly chosen from those generated by Stable Diffusion for 1.5&#x02009;s. This brief exposure time mimics common real-world interaction with AI-generated content on platforms such as social media, news websites and advertisements. Such encounters are often brief, with users rapidly scrolling through content. For example, the average viewing time for images on mobile devices is 1.7&#x02009;s (ref. <sup><xref ref-type="bibr" rid="CR60">60</xref></sup>).</p><p id="Par57">In stage 3 (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>; stage 3; post-exposure), participants repeated the task from stage 1. The primary measure of interest was the change in participants&#x02019; judgements. The data were analysed using a mixed model multinomial logistic regression with exposure (before versus after exposure to AI images) as a fixed factor, with random intercepts and slopes at the participant level. This model was chosen because the dependent variable involved a choice from six distinct and unordered categories (see <xref rid="MOESM1" ref-type="media">Supplementary Results</xref> for an alternative analysis).</p><p id="Par58">The findings revealed a significant effect for exposure (<italic>F</italic>(5, 62)&#x02009;=&#x02009;5.89; <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; Fig. <xref rid="Fig3" ref-type="fig">3b</xref>), indicating that exposure to the AI images altered human judgements. In particular, exposure increased the likelihood of choosing White men as financial managers (<italic>M</italic><sub>before exposure</sub>&#x02009;=&#x02009;32.36%; <italic>M</italic><sub>after exposure</sub>&#x02009;=&#x02009;38.20%) compared with White women (<italic>M</italic><sub>before exposure</sub>&#x02009;=&#x02009;14.94%; <italic>M</italic><sub>after exposure</sub>&#x02009;=&#x02009;14.40%; <italic>b</italic>&#x02009;=&#x02009;0.26; <italic>t</italic>&#x02009;=&#x02009;2.08; <italic>P</italic>&#x02009;=&#x02009;0.04; 95% CI&#x02009;=&#x02009;0.01 to 0.50), Asian women (<italic>M</italic><sub>before exposure</sub>&#x02009;=&#x02009;20.24%; <italic>M</italic><sub>after exposure</sub>&#x02009;=&#x02009;17.14%; <italic>b</italic>&#x02009;=&#x02009;0.47; <italic>t</italic>&#x02009;=&#x02009;3.79; <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; 95% CI&#x02009;=&#x02009;0.22 to 0.72), Black men (<italic>M</italic><sub>before exposure</sub>&#x02009;=&#x02009;6.64%; <italic>M</italic><sub>after exposure</sub>&#x02009;=&#x02009;5.62%; <italic>b</italic>&#x02009;=&#x02009;0.65; <italic>t</italic>&#x02009;=&#x02009;3.04; <italic>P</italic>&#x02009;=&#x02009;0.004; 95% CI&#x02009;=&#x02009;0.22 to 1.08) and Black women (<italic>M</italic><sub>before exposure</sub>&#x02009;=&#x02009;11.12%; <italic>M</italic><sub>after exposure</sub>&#x02009;=&#x02009;10.08%; <italic>b</italic>&#x02009;=&#x02009;0.47; <italic>t</italic>&#x02009;=&#x02009;2.46; <italic>P</italic>&#x02009;=&#x02009;0.02; 95% CI&#x02009;=&#x02009;0.09 to 0.87). No significant difference was found between White men and Asian men (<italic>M</italic><sub>before exposure</sub>&#x02009;=&#x02009;14.70%; <italic>M</italic><sub>after exposure</sub>&#x02009;=&#x02009;14.56%<italic>; b</italic>&#x02009;=&#x02009;0.28; <italic>t</italic>&#x02009;=&#x02009;2.01; <italic>P</italic>&#x02009;=&#x02009;0.051; 95% CI&#x02009;=&#x02009;&#x02212;0.001 to 0.57).</p><p id="Par59">We also ran this experiment with another group of participants to control for order effects. The controls were never exposed to the Stable Diffusion images of financial managers; instead, they were exposed to neutral images of fractals (see Fig. <xref rid="Fig3" ref-type="fig">3a</xref>; stage 2; exposure). The same analysis was performed for the control condition as for the treatment condition. As expected, no significant effect of exposure to neutral fractals was found for the control condition (<italic>F</italic>(5, 67)&#x02009;=&#x02009;1.69; <italic>P</italic>&#x02009;=&#x02009;0.15; Fig. <xref rid="Fig3" ref-type="fig">3b</xref>). Additionally, no significant differences were observed when comparing White men (<italic>M</italic><sub>before exposure</sub>&#x02009;=&#x02009;28.42%; <italic>M</italic><sub>after exposure</sub>&#x02009;=&#x02009;27.28%) with each of the demographic groups (all <italic>P</italic>&#x02009;values&#x02009;&#x0003e;&#x02009;0.06): White women (<italic>M</italic><sub>before exposure</sub>&#x02009;=&#x02009;15.64%; <italic>M</italic><sub>after exposure</sub>&#x02009;=&#x02009;15.36%), Asian men (<italic>M</italic><sub>before exposure</sub>&#x02009;=&#x02009;12.00%; <italic>M</italic><sub>after exposure</sub>&#x02009;=&#x02009;11.18%), Asian women (<italic>M</italic><sub>before exposure</sub>&#x02009;=&#x02009;20.52%; <italic>M</italic><sub>after exposure</sub>&#x02009;=&#x02009;19.74%), Black men (<italic>M</italic><sub>before exposure</sub>&#x02009;=&#x02009;8.78%; <italic>M</italic><sub>after exposure</sub>&#x02009;=&#x02009;9.30%) and Black women (<italic>M</italic><sub>before exposure</sub>&#x02009;=&#x02009;14.64%; <italic>M</italic><sub>after exposure</sub>&#x02009;=&#x02009;17.14%). Comparison of the treatment and control groups indicated that the former showed a greater increase than the latter in selecting White men after exposure to the images relative to before (permutation test comparing the change in selecting White men across groups: <italic>P</italic>&#x02009;=&#x02009;0.02; <italic>d</italic>&#x02009;=&#x02009;0.46; 95% CI&#x02009;=&#x02009;0.01 to 0.13).</p><p id="Par60">These results suggest that interactions with a commonly used AI system that amplifies imbalances in real-world representation induce bias in humans. Crucially, the AI system in this experiment is firmly rooted in the real world. Stable Diffusion has an estimated 10&#x02009;million users generating millions of images daily<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>, underscoring the importance of this phenomenon. These findings were replicated in a follow-up experiment with slight changes to the task (see Supplementary Experiment <xref rid="MOESM1" ref-type="media">6</xref>).</p></sec></sec><sec id="Sec15" sec-type="discussion"><title>Discussion</title><p id="Par61">Our findings reveal that human&#x02013;AI interactions create a feedback loop where even small biases emerging from either side increase subsequent human error. First, AI algorithms amplify minute biases embedded in the human data they were trained on. Then, interactions with these biased algorithms increase initial human biases. A similar effect was not observed for human&#x02013;human interactions. Unlike the AI, humans did not amplify the initial small bias present in the data, possibly because humans are less sensitive to minor biases in the data, whereas the AI exploits them to improve its prediction accuracy (see Table <xref rid="Tab1" ref-type="table">1</xref>).</p><p id="Par62">The effect of AI-induced bias was generalized across a range of algorithms (such as CNN and text-to-image generative AI), tasks and response protocols, including motion discrimination, emotion aggregation and social-based biases. Over time, as participants interacted with the biased AI system repeatedly, their judgements became more biased, suggesting that they learned to adopt the AI system&#x02019;s bias. Using computational modelling (<xref rid="MOESM1" ref-type="media">Supplementary Models</xref>), we show that humans learn from interactions with an AI algorithm to become biased, rather than just adopting the AI&#x02019;s judgement per se. Interestingly, participants underestimated the substantial impact of the biased algorithm on their judgement, which could leave them more susceptible to its influence.</p><p id="Par63">We further demonstrated a bias feedback loop in experiments utilizing a popular real-world AI system&#x02014;Stable Diffusion. Stable Diffusion tends to over-represent White men when prompted to generate images of high-power and high-income professionals<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. Here, we show that exposure to such Stable Diffusion images biases human judgement. This probably happens in real-world scenarios when individuals interact with Stable Diffusion directly and/or encounter images created by Stable Diffusion on various digital platforms, such as social media and news websites.</p><p id="Par64">Together, the present series of experiments demonstrates a human&#x02013;AI feedback loop that leaves humans more biased than they initially were, both due to the AI&#x02019;s signal and to the human perception of AI<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>. These findings go beyond previous research on AI bias amplification<sup><xref ref-type="bibr" rid="CR18">18</xref>&#x02013;<xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR63">63</xref>&#x02013;<xref ref-type="bibr" rid="CR66">66</xref></sup>, revealing a problem potentially relevant to various AI systems and decision-making contexts, such as hiring or medical diagnosis.</p><p id="Par65">The current results uncover a fundamental mechanism of bias amplification in human&#x02013;AI interactions. As such, they underscore the heightened responsibility that algorithm developers must confront in designing and deploying AI systems. Not only may AI algorithms exhibit bias themselves, but they also have the potential to amplify the biases of humans interacting with them, creating a profound feedback loop. The implications can be widespread due to the vast scale and rapidly growing prevalence of AI systems. Of particular concern is the potential effect of biased AIs on children<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>, who have more flexible and malleable knowledge representations and thus may adopt AI systems&#x02019; biases more readily.</p><p id="Par66">It is important to clarify that our findings do not suggest that all AI systems are biased, nor that all AI&#x02013;human interactions will create a bias. To the contrary, we demonstrate that when humans interact with an accurate AI, their judgements become more accurate (consistent with studies showing that human&#x02013;AI interaction can improve performance outcomes<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>). Rather, the results suggest that when a bias exists in the system it has the potential to amplify via a feedback loop. Because biases exist in both humans and AI systems, this is a problem that should be taken seriously.</p><p id="Par67">Our results indicate that participants learned the AI system&#x02019;s bias readily, primarily due to the characteristics of the AI&#x02019;s judgements, but also because of participants&#x02019; perception of the AI (see Fig. <xref rid="Fig1" ref-type="fig">1f</xref>; for extensive discussion, see ref. <sup><xref ref-type="bibr" rid="CR62">62</xref></sup>). Specifically, we observed that when participants were told they were interacting with a human when in fact they were interacting with an AI, they learned the AI&#x02019;s bias to a lesser extent than when they believed they were interacting with an AI (although they did still significantly learn the bias). This may be because participants perceived the AI systems as superior to humans on the task<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR38">38</xref></sup>. Thus, participants became more biased, even though they were updating their beliefs in a fashion that may be viewed as perfectly rational.</p><p id="Par68">An intriguing question raised by the current findings is whether the observed amplification of bias endures over time. Further research is required to assess the longevity of this effect. Several factors are likely to influence the persistence of bias, including the duration of exposure to the biased AI, the salience of the bias and individual differences in the perception of AI systems<sup><xref ref-type="bibr" rid="CR69">69</xref></sup>. Nonetheless, even temporary effects could carry substantial consequences, particularly considering the scale at which human&#x02013;AI interactions occur.</p><p id="Par69">In conclusion, AI systems are increasingly integrated into numerous domains, making it crucial to understand how to effectively use them while mitigating their associated risks. The current study reveals that biased algorithms not only produce biased evaluations, but substantially amplify such biases in human judgements, creating a feedback loop. This underscores the pressing need to increase awareness among researchers, policymakers and the public of how AI systems can influence human judgements. It is possible that strategies aimed at increasing awareness of potential biases induced by AI systems may mitigate their impact&#x02014;an option that should be tested. Importantly, our results also suggest that interacting with an accurate AI algorithm increases accuracy. Thus, reducing algorithmic bias may hold the potential to reduce biases in humans, increasing the quality of human judgement in domains ranging from health to law.</p></sec><sec id="Sec16"><title>Methods</title><sec id="Sec17"><title>Ethical statement</title><p id="Par70">This study was conducted in compliance with all of the relevant ethical regulations and received approval from the ethics committee of University College London (3990/003 and EP_2023_013). All of the participants provided informed consent before their involvement in the study.</p></sec><sec id="Sec18"><title>Participants</title><p id="Par71">A total of 1,401 individuals participated in this study. For experiment 1 (level 1), <italic>n</italic>&#x02009;=&#x02009;50 (32 women and 18 men; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;38.74&#x02009;&#x000b1;&#x02009;11.17&#x02009;years (s.d.)). For experiment 1 (human&#x02013;human; level 2), <italic>n</italic>&#x02009;=&#x02009;50 (23 women, 25 men and two not reported; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;34.58&#x02009;&#x000b1;&#x02009;11.87&#x02009;years (s.d.)). For experiment 1 (human&#x02013;AI; level 3), <italic>n</italic>&#x02009;=&#x02009;50 (24 women, 24 men and two not reported; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;39.85&#x02009;&#x000b1;&#x02009;14.29&#x02009;years (s.d.)). For experiment 1 (human&#x02013;human; level 3), <italic>n</italic>&#x02009;=&#x02009;50 (20 women and 30 men; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;40.16&#x02009;&#x000b1;&#x02009;13.45&#x02009;years (s.d.)). For experiment 1 (human&#x02013;AI perceived as human; level 3), <italic>n</italic>&#x02009;=&#x02009;50 (15 women, 30 men, four not reported and one non-binary; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;40.16&#x02009;&#x000b1;&#x02009;13.45&#x02009;years (s.d.)). For experiment 1 (human&#x02013;human perceived as AI; level 3), <italic>n</italic>&#x02009;=&#x02009;50 (18 women, 30 men, one not reported and one non-binary; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;34.79&#x02009;&#x000b1;&#x02009;10.80&#x02009;years (s.d.)). For experiment 2, <italic>n</italic>&#x02009;=&#x02009;120 (57 women, 60 men, one other and two not reported; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;38.67&#x02009;&#x000b1;&#x02009;13.19&#x02009;years (s.d.)). For experiment 2 (accurate algorithm), <italic>n</italic>&#x02009;=&#x02009;50 (23 women and 27 men; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;36.74&#x02009;&#x000b1;&#x02009;13.45&#x02009;years (s.d.)). For experiment 2 (biased algorithm), <italic>n</italic>&#x02009;=&#x02009;50 (26 women, 23 men and one not reported; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;34.91&#x02009;&#x000b1;&#x02009;8.87&#x02009;years (s.d.)). For experiment 3, <italic>n</italic>&#x02009;=&#x02009;100 (40 women, 56 men and four not reported; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;30.71&#x02009;&#x000b1;&#x02009;12.07&#x02009;years (s.d.)). For Supplementary Experiment <xref rid="MOESM1" ref-type="media">1</xref>, <italic>n</italic>&#x02009;=&#x02009;50 (26 women, 17 men and seven not reported; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;39.18&#x02009;&#x000b1;&#x02009;14.01&#x02009;years (s.d.)). For Supplementary Experiment <xref rid="MOESM1" ref-type="media">2</xref>, <italic>n</italic>&#x02009;=&#x02009;50 (24 women, 23 men, one other and two not reported; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;36.45&#x02009;&#x000b1;&#x02009;12.97&#x02009;years (s.d.)). For Supplementary Experiment <xref rid="MOESM1" ref-type="media">3</xref>, <italic>n</italic>&#x02009;=&#x02009;50 (20 women, 29 men and one not reported; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;32.05&#x02009;&#x000b1;&#x02009;10.08&#x02009;years (s.d.)). For Supplementary Experiment <xref rid="MOESM1" ref-type="media">4</xref>, <italic>n</italic>&#x02009;=&#x02009;386 (241 women, 122 men, seven other and 16 not reported; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;28.07&#x02009;&#x000b1;&#x02009;4.65&#x02009;years (s.d.)). For Supplementary Experiment <xref rid="MOESM1" ref-type="media">5</xref>, <italic>n</italic>&#x02009;=&#x02009;45 (19 women, 23 men, one other and two not reported; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;39.50&#x02009;&#x000b1;&#x02009;14.55&#x02009;years (s.d.)). For Supplementary Experiment <xref rid="MOESM1" ref-type="media">6</xref>, <italic>n</italic>&#x02009;=&#x02009;200 (85 women, 98 men, five other and 12 not reported; <italic>M</italic><sub>age</sub>&#x02009;=&#x02009;30.87&#x02009;&#x000b1;&#x02009;10.26&#x02009;years (s.d.)).</p><p id="Par72">Sample sizes were determined based on pilot studies to achieve a power of 0.8 (<italic>&#x003b1;</italic>&#x02009;=&#x02009;0.05) using G*Power<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>. In each experiment, the largest <italic>n</italic> required to detect a key effect was used and rounded up. Participants were recruited via Prolific (<ext-link ext-link-type="uri" xlink:href="https://prolific.com/">https://prolific.com/</ext-link>) and received, in exchange for participation, a payment of &#x000a3;7.50 per hour until April 2022, after which the rate was increased to &#x000a3;9.00 per hour. Additionally, participants in experiments 1 and 2 received a bonus fee ranging from &#x000a3;0.50 to &#x000a3;2,00, which was determined based on performance. All participants had normal or corrected-to-normal vision. The experiments were designed in PsychoPy3 (2022.2.5) and hosted on the Pavlovia platform (<ext-link ext-link-type="uri" xlink:href="https://pavlovia.org/">https://pavlovia.org/</ext-link>).</p></sec><sec id="Sec19"><title>Tasks and analyses</title><sec id="Sec20"><title>Emotional aggregation task</title><sec id="FPar1"><title>AI&#x02013;human interaction</title><p id="Par73">For level 1, participants performed 100 trials of the emotion aggregation task. On each trial, an array of 12 emotional faces, ranging from sad to happy, was presented for 500&#x02009;ms (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>). The participants indicated whether, on average, the faces were more happy or more sad. Each participant was presented with 100 unique arrays of faces, which were generated as described below.</p><p id="Par74">To generate the individual faces used in this task, a total of 50 morphed greyscale faces were adopted from ref. <sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. The faces were created by matching multiple facial features (for example, the corners of the mouth and centres of the eyes) between extreme sad and happy expressions of the same person (taken from the Ekman gallery<sup><xref ref-type="bibr" rid="CR71">71</xref></sup>) and then linearly interpolating between them. The morphed faces ranged from 1 (100% sad face) to 50 (100% happy face), based on the morphing ratio. These objective ranking scores of each face correlated well with participants&#x02019; subjective perception of the emotion expressed by the face. This was determined by showing participants the faces one by one before performing the emotion aggregation task and asking them to rate the faces on a scale from very sad to very happy (self-paced). A linear regression between the objective rankings of the faces and subjective evaluations of the participants indicated that the participants were highly sensitive to the emotional expressions (<italic>b</italic>&#x02009;=&#x02009;0.8; <italic>t</italic>(50)&#x02009;=&#x02009;26.25; <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001; <italic>R</italic><sup>2</sup>&#x02009;=&#x02009;0.84).</p><p id="Par75">The 100 arrays of 12 emotional faces were generated as follows. For 50 of the arrays, the 12 faces were randomly sampled (with repetition) from a uniform distribution in the interval [1,50] with a mean of 25.5. Then, for each of these arrays, a mirror array was created in which the ranking score of each face was equal to 51 minus the ranking scores of the face in the original trial. For example, if the ranking scores of faces in an original array were 21, 44, &#x02026;, 25, the ranking scores of the faces in the mirror array were 51&#x02009;&#x02212;&#x02009;21&#x02009;=&#x02009;30, 51&#x02009;&#x02212;&#x02009;44&#x02009;=&#x02009;7, &#x02026;, 51&#x02009;&#x02212;&#x02009;25&#x02009;=&#x02009;26. This method ensured that for half of the trials the objective mean ranking of the array was higher than the mean of the uniform distribution (mean&#x02009;&#x0003e;&#x02009;25.5; more happy faces) and in the other half it was lower (mean&#x02009;&#x0003c;&#x02009;25.5; more sad faces). If the objective mean ranking of an array was exactly 25.5, the faces were resampled.</p><p id="Par76">Bias in the emotion aggregation task was defined as a percentage of more sad responses beyond 50%. As described in the <xref rid="Sec2" ref-type="sec">Results</xref>, at the group level the participants showed a tendency to classify the arrays of faces as more sad (permutation test against 50%: <italic>P</italic>&#x02009;=&#x02009;0.017; <italic>d</italic>&#x02009;=&#x02009;0.34; 95% CI<sub>more sad</sub>&#x02009;=&#x02009;0.51 to 0.56). Similar results were observed when the bias was quantified using a psychometric function analysis (see <xref rid="MOESM1" ref-type="media">Supplementary Results</xref> for more details).</p><p id="Par77">For level 2, the choices of the participants in level 1 (5,000 choices) were fed into a CNN consisting of five convolutional layers (with filter sizes of 32, 64, 128, 256, 512 and rectified linear unit (ReLU) activation functions) and three fully connected dense layers (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>). A 0.5 dropout rate was used. The predictions of the CNN were calculated on a test set consisting of 300 new arrays of faces (that is, arrays that were not included in the training or validation sets). Half of the arrays in the test set had an objective mean ranking score higher than 25.5 (that is, the more happy classification) and the other half had a score lower than 25.5 (that is, the more sad classification).</p><p id="Par78">For level 3, participants first performed the same procedure described in level 1, except they performed 150 trials instead of 100. These trials were used to measure the baseline performance of participants in the emotion aggregation task. Then, participants performed the emotion aggregation task as in the previous experiment. However, on each trial, after indicating their choice, they were also presented with the response of an AI algorithm for 2&#x02009;s (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>). The participants were then asked whether they would like to change their decision (that is, from more sad to more happy and vice versa) by clicking on the yes or no buttons (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>). Before interacting with the AI, participants were told that they &#x0201c;will be presented with the response of an AI algorithm that was trained to perform the task&#x0201d;. Overall, participants performed 300 trials divided into six blocks.</p></sec><sec id="FPar2"><title>Human&#x02013;human interaction</title><p id="Par79">For level 1, the responses in the first level of the human&#x02013;human interaction were the same as those in the human&#x02013;AI interaction.</p><p id="Par80">For level 2, participants first performed the same procedure as in level 1. Next, they were presented with 100 arrays of 12 faces for 500&#x02009;ms, followed by the response of another participant from level 1 to the same array, which was presented for 2&#x02009;s (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>). On each trial, the total numbers of more sad and more happy classifications of the other participants (up until that trial) were presented at the bottom of the screen. Two trials were pseudo-randomly sampled from each of the 50 participants in level 1. The first trial was sampled randomly and the second was its matched mirror trial. The responses were sampled such that they preserved the bias and accuracy of the full set (with differences in bias and accuracy not exceeding 1%).</p><p id="Par81">To verify that the participants attended to the task, they were asked to report the response of the other player on 20% of the trials, which were randomly selected (that is, they were asked &#x0201c;What was the response of the other player?&#x0201d; and had to choose between more sad and more happy). The data from participants whose accuracy scores were lower than 90% were excluded from further analysis (<italic>n</italic>&#x02009;=&#x02009;14 participants) for lack of engagement with the task.</p><p id="Par82">After completing this part of the experiment, participants performed the emotion aggregation task again on their own for another ten trials.</p><p id="Par83">For level 3, participants performed the same procedure as described for human&#x02013;AI interaction (level 3), except that here they interacted with a human associate instead of an AI associate. The responses of the human associate were pseudo-randomly sampled from the human&#x02013;human network (level 2), such that six responses were pseudo-randomly sampled from each participant (a total of 300 trials). Before interacting with the human associate, participants were told that they &#x0201c;will be presented with the responses of another participant who already performed the task&#x0201d;.</p></sec><sec id="FPar3"><title>Human&#x02013;AI-perceived-as-human interaction</title><p id="Par84">For level 1, the responses in the first level were the same as those for the human&#x02013;AI and human&#x02013;human interactions.</p><p id="Par85">Level 2 was the same as that in the human&#x02013;AI interaction.</p><p id="Par86">For level 3, participants performed the exact same procedure as in the human&#x02013;human interaction. The only difference was that, while they were led to believe that they &#x0201c;will be presented with the responses of another participant who already performed the task&#x0201d;, they were in fact interacting with the AI system trained in level 2.</p></sec><sec id="FPar4"><title>Human&#x02013;human-perceived-as-AI interaction</title><p id="Par87">The responses in the first level were the same as those for the human&#x02013;AI and human&#x02013;human interactions.</p><p id="Par88">The second level was the same as that in the human&#x02013;human interaction.</p><p id="Par89">For level 3, participants performed the exact same procedure as in the human&#x02013;AI interaction. The only difference was that, while they were led to believe that they &#x0201c;will be presented with the response of an AI algorithm that was trained to perform the task&#x0201d;, they were in fact interacting with the human participants from level 2.</p></sec></sec><sec id="Sec21"><title>RDK task</title><sec id="FPar5"><title>Main experiment</title><p id="Par90">For the baseline part of this experiment, participants performed a version of the RDK task<sup><xref ref-type="bibr" rid="CR48">48</xref>&#x02013;<xref ref-type="bibr" rid="CR51">51</xref></sup> across 30 trials. On each trial, participants were presented with an array of 100 white dots moving against a grey background. On each trial, the percentage of dots moving from left to right was one of the following: 6, 16, 22, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50 (presented twice), 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 78, 86 or 96%. The display was presented for 1&#x02009;s and then disappeared. Participants were asked to estimate the percentage of dots that moved from left to right on a scale ranging from 0% left to right to 100% left to right, as well as to indicate their confidence on a scale ranging from not confident at all to very confident (Fig. <xref rid="Fig2" ref-type="fig">2a</xref>, top panel).</p><p id="Par91">Interaction blocks were then introduced. On each trial, participants first performed the RDK task exactly as described above. Then, they were presented with their response (Fig. <xref rid="Fig2" ref-type="fig">2c</xref>) and a question mark where the AI algorithm response would later appear. They were asked to assign a weight to each response on a scale ranging between 100% you to 100% AI (self-paced). The final joint response was calculated according to the following formula:<disp-formula id="Equg"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{l}{{\rm{Final}}\; {\rm{joint}}\; {\rm{response}}}\\=w\times({{\rm{participant}}}\mbox{'}{{\rm{s}}\; {\rm{response}}})+(1-w)\times({{\rm{AI}}}\mbox{'}{{\rm{s}}\; {\rm{response}}})\end{array}$$\end{document}</tex-math><mml:math id="M14"><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow><mml:mspace width="thickmathspace"/><mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mspace width="thickmathspace"/><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>'</mml:mtext></mml:mstyle><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mspace width="thickmathspace"/><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>'</mml:mtext></mml:mstyle><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mspace width="thickmathspace"/><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="41562_2024_2077_Article_Equg.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par92">Where <italic>w</italic> is the weight the participants assigned to their own response. For example, if the response of the participant was 53% of the dots moved rightward and the response of the AI was 73% of the dots moved rightward and the participants assigned a weight of 40% to their response, the final joint response was 0.4 &#x000d7; (53%)&#x02009;+&#x02009;0.6 &#x000d7; (73%)&#x02009;=&#x02009;65% of the dots moved rightward. Note that because the AI response was not revealed until the participants indicated their weighting, participants had to rely on their evaluation of the AI based on past trials and could not rely on the response of the AI on that trial. Thereafter, the AI response was revealed and remained on screen for 2&#x02009;s. Participants completed three blocks each consisting of 30 trials.</p><p id="Par93">The participants interacted with three different algorithms: an accurate algorithm, a biased algorithm and a noisy algorithm (Fig. <xref rid="Fig2" ref-type="fig">2b</xref>). The accurate algorithm provided the correct response on all trials. The biased algorithm provided a response that was higher than the correct response by 0&#x02013;49% (mean bias&#x02009;=&#x02009;24.96%). The noisy algorithm provided responses similar to those of the accurate algorithm, but with the addition of a considerable amount of Gaussian noise (s.d.&#x02009;=&#x02009;28.46). The error (that is, the mean absolute difference from the correct response) of the biased and noisy algorithms was virtually the same (24.96 and 25.33, respectively).</p><p id="Par94">The order of the algorithms was randomized between participants using the Latin square method with the following orders: (1) accurate, biased, noisy; (2) biased, noisy, accurate; and (3) noisy, accurate, biased. Before interacting with the algorithms, participants were told that they &#x0201c;will be presented with the response of an AI algorithm that was trained to perform the task&#x0201d;. Before starting each block, participants were told that they would interact with a new and different algorithm. The algorithms were labelled algorithm A, algorithm B and algorithm C. At the end of the experiment, the participants were asked the following questions: (1) &#x0201c;To what extent were your responses influenced by the responses of algorithm A?&#x0201d;; and (2) &#x0201c;How accurate was algorithm A?&#x0201d;. These questions were repeated for algorithms B and C. The response to the first question was given on a scale ranging from not at all (coded as 1) to very much (coded as 7) and the response to the second question was given on a scale ranging from not accurate at all (coded as 1) to very accurate (coded as 7). To assist participants in distinguishing between the algorithms, each algorithm was consistently represented with the same font colour (A, green; B, blue; C, purple) throughout the whole experiment.</p><p id="Par95">We used three main dependent measures: bias, accuracy (error) and the weight assigned to the AI evaluations. Bias was defined as the mean difference between a participant&#x02019;s responses and the correct percentage of dots that moved from left to right. For each participant, the bias in the baseline block was subtracted from the bias in the interaction blocks. The resulting difference in bias was compared against zero. Positive values indicated that participants reported more rightward movement in the interaction blocks than at baseline, whereas negative values indicated the opposite. Error was defined as the mean absolute difference between a participant&#x02019;s responses and the correct percentage of dots that moved from left to right. In all analyses, for each participant, the error in the interaction blocks was subtracted from the error in the baseline blocks. Thus, positive values of this difference score indicated increased accuracy due to interaction with the AI, whereas negative values indicated reduced accuracy. The weights assigned to the AI evaluations were defined as the average weight participants assigned to the AI response on a scale ranging from &#x02212;1 (weight of 0% to the AI response) to 1 (weight of 100% to the AI response).</p><p id="Par96">The influences of the biased and accurate algorithms were quantified using two different methods: relative changes and <italic>z</italic>-scoring across algorithms. The relative change in bias was computed by dividing the AI-induced bias by the baseline bias, while the relative change in accuracy was computed by dividing the AI-induced accuracy change by the baseline error. A comparison of the relative changes in bias and accuracy yielded no significant difference (permutation test: <italic>P</italic>&#x02009;=&#x02009;0.89; <italic>d</italic>&#x02009;=&#x02009;&#x02212;0.02; 95% CI&#x02009;=&#x02009;&#x02212;1.44 to 1.9). The same result was obtained for <italic>z</italic>-scoring across algorithms. In this method, we <italic>z</italic>-scored the AI-induced bias of each participant when interacting with each algorithm (that is, for each participant, we <italic>z</italic>-scored across algorithms and not across participants). Therefore, three z-scores were obtained for each participant, indicating the relative effect of the biased, accurate and noisy algorithms. The same procedure was repeated for the AI-induced accuracy, resulting in three <italic>z</italic>-scores indicating the relative influences of the different algorithms on the accuracy of each participant. Then, the <italic>z</italic>-scores of the bias algorithm (for the AI-induced bias) and the <italic>z</italic>-scores of the accurate algorithm (for the AI-induced accuracy change) were compared across participants. No significant difference was found between them (permutation test: <italic>P</italic>&#x02009;=&#x02009;0.90; <italic>d</italic>&#x02009;=&#x02009;&#x02212;0.01; 95% CI&#x02009;=&#x02009;&#x02212;0.19 to 0.17).</p></sec><sec id="FPar6"><title>Effects across time</title><p id="Par97">To examine the AI-induced bias and accuracy effects across time, we conducted two additional experiments. In the first one, participants performed the RDK task exactly as described above, except for one difference. Instead of interacting with accurate, biased and noisy algorithms, participants interacted only with a biased algorithm across five blocks. The second experiment was similar to the first, except for participants interacting with an accurate algorithm across five blocks.</p></sec></sec><sec id="Sec22"><title>Experiment 3</title><p id="Par98">This experiment aimed to investigate whether exposure to images generated by the popular AI system Stable Diffusion<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>, which is known to exemplify social imbalances<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, increases judgement bias in humans. To assess this, participants completed a judgement task before and after viewing Stable Diffusion-generated images. Their performance was compared with that of a control group in which participants were presented with fractals images.</p><sec id="FPar7"><title>Procedure</title><p id="Par99">A total of 100 participants were recruited for the experiment. Participants were randomly assigned to either the AI exposure group (<italic>n</italic>&#x02009;=&#x02009;50) or a control fractal exposure group (<italic>n</italic>&#x02009;=&#x02009;50).</p><p id="Par100">The study comprised three stages. In stage 1 (baseline assessment), the participants completed 100 trials in which they were shown an image featuring six individual headshots and were asked: &#x0201c;Who do you think is more likely to be a financial manager?&#x0201d; (see Fig. <xref rid="Fig3" ref-type="fig">3a</xref>; stage 1). Participants made their selection by clicking on the chosen image using their computer mouse. Before this stage, participants were provided with a definition of a financial manager (&#x0201c;a person responsible for the supervision and handling of the financial affairs of an organization&#x0201d;; taken from the Collins Dictionary).</p><p id="Par101">In stage 2 (exposure), participants in the AI condition completed 100 trials in which they were presented with Stable Diffusion-generated images of financial managers (three images per trial). The three images were randomly chosen and presented for 1.5&#x02009;s. Before viewing the images, participants were presented with a brief description of Stable Diffusion. Participants in the control group were shown fractal images instead of financial manager images.</p><p id="Par102">In stage 3 (post-exposure), participants completed 100 trials in which the judgement task from stage 1 was repeated.</p><p id="Par103">The order of the trials was randomized for all stages across participants.</p></sec><sec id="FPar8"><title>Stimuli</title><p id="Par104">The stimuli in each trial consisted of images of six individuals (a White man, a White woman, an Asian man, an Asian woman, a Black man and a Black woman) selected from the Chicago Face Database (see the GitHub repository for the exact images used)<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. From each demographic category, ten images of individuals aged 30&#x02013;40&#x02009;years were chosen. The chosen individuals were balanced in age, attractiveness and racial prototypicality (all <italic>P</italic>&#x02009;values&#x02009;&#x0003e;&#x02009;0.16). Each image was presented against a grey background with a circle framing the face (see Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). The locations of the individuals from each demographic group in the image within each trial were randomly determined.</p><p id="Par105">In the AI exposure condition, Stable Diffusion (version 2.1) was used to generate 100 images of financial managers, using the prompt: &#x0201c;A color photo of a financial manager, headshot, high-quality&#x0201d;. Images that contained multiple people, unclear faces or distortions were replaced with other images of the same race and gender. The control condition featured 100 fractal images of the same size and resolution as the images of the financial managers. Thirty naive observers categorized the faces according to race and gender (Cohen&#x02019;s <italic>&#x003ba;</italic>&#x02009;=&#x02009;0.611). Each image was ultimately classified based on the majority categorization across the 30 participants. Of the Stable Diffusion-generated images, 85% were classified as White men, 11% as White women, 3% as non-White men and 1% as non-White women.</p></sec></sec></sec><sec id="Sec23"><title>Statistical analyses</title><p id="Par106">All of the statistical tests were two sided. Mean comparisons utilized non-parametric permutation tests, with <italic>P</italic>&#x02009;values computed using 10<sup>5</sup> random shuffles. When parametric tests were employed, normality was assumed based on the central limit theorem, as all conditions had sufficiently large sample sizes to justify this assumption. In repeated measures ANOVAs, the assumption of sphericity was tested using Mauchly&#x02019;s test. In case of violation, Greenhouse&#x02013;Geisser correction was applied. The equality-of-variances assumption was tested using Levene&#x02019;s test. In case of violation, Welch correction was applied.</p></sec><sec id="Sec24"><title>Reporting summary</title><p id="Par107">Further information on research design is available in the <xref rid="MOESM2" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p></sec></sec><sec id="Sec25" sec-type="supplementary-material"><title>Supplementary information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41562_2024_2077_MOESM1_ESM.pdf"><label>Supplementary Information</label><caption><p>Supplementary results, Figs. 1&#x02013;4, models and experiments.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="41562_2024_2077_MOESM2_ESM.pdf"><caption><p>Reporting Summary</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s41562-024-02077-2.</p></sec><ack><title>Acknowledgements</title><p>We thank B. Blain, I. Cogliati Dezza, R. Dubey, L. Globig, C. Kelly, R. Koster, T. Nahari, V. Vellani, S. Zheng, I. Pinhorn, H. Haj-Ali, L. Tse, N. Nachman, R. Moran, M. Usher, I. Fradkin and D. Rosenbaum for critical reading of the manuscript and helpful comments. T.S. is funded by a Wellcome Trust Senior Research Fellowship (214268/Z/18/Z). The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>M.G. and T.S. conceived of the study idea, developed the methodology, visualized the results, wrote the original draft and reviewed and edited the manuscript. M.G. performed the investigation. T.S. acquired funding and administered and supervised the project.</p></notes><notes notes-type="peer-review"><title>Peer review</title><sec id="FPar9"><title>Peer review information</title><p id="Par108"><italic>Nature Human Behaviour</italic> thanks Emily Wall and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p></sec></notes><notes notes-type="data-availability"><title>Data availability</title><p>The data that support the findings of this study are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/affective-brain-lab/BiasedHumanAI">https://github.com/affective-brain-lab/BiasedHumanAI</ext-link>.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The code related to this study is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/affective-brain-lab/BiasedHumanAI">https://github.com/affective-brain-lab/BiasedHumanAI</ext-link>.</p></notes><notes id="FPar10" notes-type="COI-statement"><title>Competing interests</title><p id="Par109">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Centola</surname><given-names>D</given-names></name></person-group><article-title>The spread of behavior in an online social network experiment</article-title><source>Science</source><year>2010</year><volume>329</volume><fpage>1194</fpage><lpage>1197</lpage><pub-id pub-id-type="pmid">20813952</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Centola, D. The spread of behavior in an online social network experiment. <italic>Science</italic><bold>329</bold>, 1194&#x02013;1197 (2010).<pub-id pub-id-type="pmid">20813952</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Moussa&#x000ef;d</surname><given-names>M</given-names></name><name><surname>Herzog</surname><given-names>SM</given-names></name><name><surname>K&#x000e4;mmer</surname><given-names>JE</given-names></name><name><surname>Hertwig</surname><given-names>R</given-names></name></person-group><article-title>Reach and speed of judgment propagation in the laboratory</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2017</year><volume>114</volume><fpage>4117</fpage><lpage>4122</lpage><pub-id pub-id-type="pmid">28373540</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Moussa&#x000ef;d, M., Herzog, S. M., K&#x000e4;mmer, J. E. &#x00026; Hertwig, R. Reach and speed of judgment propagation in the laboratory. <italic>Proc. Natl Acad. Sci. USA</italic><bold>114</bold>, 4117&#x02013;4122 (2017).<pub-id pub-id-type="pmid">28373540</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B</given-names></name><etal/></person-group><article-title>Realistic modelling of information spread using peer-to-peer diffusion patterns</article-title><source>Nat. Hum. Behav.</source><year>2020</year><volume>4</volume><fpage>1198</fpage><lpage>1207</lpage><pub-id pub-id-type="pmid">32860013</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Zhou, B. et al. Realistic modelling of information spread using peer-to-peer diffusion patterns. <italic>Nat. Hum. Behav.</italic><bold>4</bold>, 1198&#x02013;1207 (2020).<pub-id pub-id-type="pmid">32860013</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Kahneman, D., Sibony, O. &#x00026; Sunstein, C. R. <italic>Noise: A Flaw in Human Judgment</italic> (Hachette UK, 2021).</mixed-citation></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Araujo</surname><given-names>T</given-names></name><name><surname>Helberger</surname><given-names>N</given-names></name><name><surname>Kruikemeier</surname><given-names>S</given-names></name><name><surname>de Vreese</surname><given-names>CH</given-names></name></person-group><article-title>In AI we trust? Perceptions about automated decision-making by artificial intelligence</article-title><source>AI Soc.</source><year>2020</year><volume>35</volume><fpage>611</fpage><lpage>623</lpage></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Araujo, T., Helberger, N., Kruikemeier, S. &#x00026; de Vreese, C. H.In AI we trust? Perceptions about automated decision-making by artificial intelligence. <italic>AI Soc.</italic><bold>35</bold>, 611&#x02013;623 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Logg</surname><given-names>JM</given-names></name><name><surname>Minson</surname><given-names>JA</given-names></name><name><surname>Moore</surname><given-names>DA</given-names></name></person-group><article-title>Algorithm appreciation: people prefer algorithmic to human judgment</article-title><source>Organ Behav. Hum. Decis. Process.</source><year>2019</year><volume>151</volume><fpage>90</fpage><lpage>103</lpage></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Logg, J. M., Minson, J. A. &#x00026; Moore, D. A. Algorithm appreciation: people prefer algorithmic to human judgment. <italic>Organ Behav. Hum. Decis. Process.</italic><bold>151</bold>, 90&#x02013;103 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="pmid">26017442</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">LeCun, Y., Bengio, Y. &#x00026; Hinton, G. Deep learning. <italic>Nature</italic><bold>521</bold>, 436&#x02013;444 (2015).<pub-id pub-id-type="pmid">26017442</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Vaswani, A. et al. Attention is all you need. In <italic>Proc. Advances in Neural Information Processing Systems</italic><italic>30 (NIPS 2017)</italic>. 5998&#x02013;6008 (Curran Associates, 2017).</mixed-citation></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning-a technology with the potential to transform health care</article-title><source>J. Am. Med. Assoc.</source><year>2018</year><volume>320</volume><fpage>1101</fpage><lpage>1102</lpage></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Hinton, G. Deep learning-a technology with the potential to transform health care. <italic>J. Am. Med. Assoc.</italic><bold>320</bold>, 1101&#x02013;1102 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Loftus</surname><given-names>TJ</given-names></name><etal/></person-group><article-title>Artificial intelligence and surgical decision-making</article-title><source>JAMA Surg.</source><year>2020</year><volume>155</volume><fpage>148</fpage><lpage>158</lpage><pub-id pub-id-type="pmid">31825465</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Loftus, T. J. et al. Artificial intelligence and surgical decision-making. <italic>JAMA Surg.</italic><bold>155</bold>, 148&#x02013;158 (2020).<pub-id pub-id-type="pmid">31825465</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Topol</surname><given-names>EJ</given-names></name></person-group><article-title>High-performance medicine: the convergence of human and artificial intelligence</article-title><source>Nat. Med.</source><year>2019</year><volume>25</volume><fpage>44</fpage><lpage>56</lpage><pub-id pub-id-type="pmid">30617339</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Topol, E. J.High-performance medicine: the convergence of human and artificial intelligence. <italic>Nat. Med.</italic><bold>25</bold>, 44&#x02013;56 (2019).<pub-id pub-id-type="pmid">30617339</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Roll</surname><given-names>I</given-names></name><name><surname>Wylie</surname><given-names>R</given-names></name></person-group><article-title>Evolution and revolution in artificial intelligence in education</article-title><source>Int. J. Artif. Intell. Educ.</source><year>2016</year><volume>26</volume><fpage>582</fpage><lpage>599</lpage></element-citation><mixed-citation id="mc-CR12" publication-type="journal">Roll, I. &#x00026; Wylie, R. Evolution and revolution in artificial intelligence in education. <italic>Int. J. Artif. Intell. Educ.</italic><bold>26</bold>, 582&#x02013;599 (2016).</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>L</given-names></name><name><surname>Sun</surname><given-names>B</given-names></name></person-group><article-title>Machine learning and AI in marketing &#x02013; connecting computing power to human insights</article-title><source>Int. J. Res. Market.</source><year>2020</year><volume>37</volume><fpage>481</fpage><lpage>504</lpage></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Ma, L. &#x00026; Sun, B. Machine learning and AI in marketing &#x02013; connecting computing power to human insights. <italic>Int. J. Res. Market.</italic><bold>37</bold>, 481&#x02013;504 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Emerson, S., Kennedy, R., O&#x02019;Shea, L. &#x00026; O&#x02019;Brien, J. Trends and applications of machine learning in quantitative finance. In <italic>Proc. 8th International Conference on Economics and Finance Research</italic> (SSRN, 2019).</mixed-citation></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Caliskan</surname><given-names>A</given-names></name><name><surname>Bryson</surname><given-names>JJ</given-names></name><name><surname>Narayanan</surname><given-names>A</given-names></name></person-group><article-title>Semantics derived automatically from language corpora contain human-like biases</article-title><source>Science</source><year>2017</year><volume>356</volume><fpage>183</fpage><lpage>186</lpage><pub-id pub-id-type="pmid">28408601</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Caliskan, A., Bryson, J. J. &#x00026; Narayanan, A. Semantics derived automatically from language corpora contain human-like biases. <italic>Science</italic><bold>356</bold>, 183&#x02013;186 (2017).<pub-id pub-id-type="pmid">28408601</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Obermeyer</surname><given-names>Z</given-names></name><name><surname>Powers</surname><given-names>B</given-names></name><name><surname>Vogeli</surname><given-names>C</given-names></name><name><surname>Mullainathan</surname><given-names>S</given-names></name></person-group><article-title>Dissecting racial bias in an algorithm used to manage the health of populations</article-title><source>Science</source><year>2019</year><volume>366</volume><fpage>447</fpage><lpage>453</lpage><pub-id pub-id-type="pmid">31649194</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Obermeyer, Z., Powers, B., Vogeli, C. &#x00026; Mullainathan, S. Dissecting racial bias in an algorithm used to manage the health of populations. <italic>Science</italic><bold>366</bold>, 447&#x02013;453 (2019).<pub-id pub-id-type="pmid">31649194</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Omiye</surname><given-names>JA</given-names></name><name><surname>Lester</surname><given-names>JC</given-names></name><name><surname>Spichak</surname><given-names>S</given-names></name><name><surname>Rotemberg</surname><given-names>V</given-names></name><name><surname>Daneshjou</surname><given-names>R</given-names></name></person-group><article-title>Large language models propagate race-based medicine</article-title><source>NPJ Digit. Med.</source><year>2023</year><volume>6</volume><fpage>195</fpage><pub-id pub-id-type="pmid">37864012</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Omiye, J. A., Lester, J. C., Spichak, S., Rotemberg, V. &#x00026; Daneshjou, R. Large language models propagate race-based medicine. <italic>NPJ Digit. Med.</italic><bold>6</bold>, 195 (2023).<pub-id pub-id-type="pmid">37864012</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Hall, M., van der Maaten, L., Gustafson, L., Jones, M. &#x00026; Adcock, A. A systematic study of bias amplification. Preprint at 10.48550/arXiv.2201.11706 (2022).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Leino, K., Fredrikson, M., Black, E., Sen, S. &#x00026; Datta, A. Feature-wise bias amplification. Preprint at 10.48550/arXiv.1812.08999 (2019).</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Lloyd, K. Bias amplification in artificial intelligence systems. Preprint at 10.48550/arXiv.1809.07842 (2018).</mixed-citation></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Troyanskaya</surname><given-names>O</given-names></name><etal/></person-group><article-title>Artificial intelligence and cancer</article-title><source>Nat Cancer</source><year>2020</year><volume>1</volume><fpage>149</fpage><lpage>152</lpage><pub-id pub-id-type="pmid">35122011</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Troyanskaya, O. et al. Artificial intelligence and cancer. <italic>Nat Cancer</italic><bold>1</bold>, 149&#x02013;152 (2020).<pub-id pub-id-type="pmid">35122011</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Skjuve, M., Brandtzaeg, P. B. &#x00026; F&#x000f8;lstad, A. Why do people use ChatGPT? Exploring user motivations for generative conversational AI. <italic>First Monday</italic><bold>29</bold> (2024); 10.5210/fm.v29i1.13541</mixed-citation></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Mayson</surname><given-names>SG</given-names></name></person-group><article-title>Bias in, bias out</article-title><source>Yale Law J.</source><year>2019</year><volume>128</volume><fpage>2218</fpage><lpage>2300</lpage></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Mayson, S. G. Bias in, bias out. <italic>Yale Law J.</italic><bold>128</bold>, 2218&#x02013;2300 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>JC</given-names></name><name><surname>Uddenberg</surname><given-names>S</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Todorov</surname><given-names>A</given-names></name><name><surname>Suchow</surname><given-names>JW</given-names></name></person-group><article-title>Deep models of superficial face judgments.</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2022</year><volume>119</volume><fpage>e2115228119</fpage><pub-id pub-id-type="pmid">35446619</pub-id>
</element-citation><mixed-citation id="mc-CR24" publication-type="journal">Peterson, J. C., Uddenberg, S., Griffiths, T. L., Todorov, A. &#x00026; Suchow, J. W. Deep models of superficial face judgments. <italic>Proc. Natl Acad. Sci. USA</italic><bold>119</bold>, e2115228119 (2022).<pub-id pub-id-type="pmid">35446619</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Geirhos, R. et al. ImageNET-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. Preprint at 10.48550/arXiv.1811.12231 (2022).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Benjamin, A., Qiu, C., Zhang, L.-Q., Kording, K. &#x00026; Stocker, A. Shared visual illusions between humans and artificial neural networks. In <italic>Proc 2019 Conference on Cognitive Computational Neuroscience</italic> (2019).</mixed-citation></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>M</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><article-title>Biased orientation representations can be explained by experience with nonuniform training set statistics</article-title><source>J. Vis.</source><year>2021</year><volume>21</volume><fpage>1</fpage><lpage>22</lpage><pub-id pub-id-type="pmid">34609475</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Henderson, M. &#x00026; Serences, J. T. Biased orientation representations can be explained by experience with nonuniform training set statistics. <italic>J. Vis.</italic><bold>21</bold>, 1&#x02013;22 (2021).<pub-id pub-id-type="pmid">34609475</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Binz</surname><given-names>M</given-names></name><name><surname>Schulz</surname><given-names>E</given-names></name></person-group><article-title>Using cognitive psychology to understand GPT-3</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2023</year><volume>120</volume><fpage>e2218523120</fpage><pub-id pub-id-type="pmid">36730192</pub-id>
</element-citation><mixed-citation id="mc-CR28" publication-type="journal">Binz, M. &#x00026; Schulz, E. Using cognitive psychology to understand GPT-3. <italic>Proc. Natl Acad. Sci. USA</italic><bold>120</bold>, e2218523120 (2023).<pub-id pub-id-type="pmid">36730192</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Yax</surname><given-names>N</given-names></name><name><surname>Anll&#x000f3;</surname><given-names>H</given-names></name><name><surname>Palminteri</surname><given-names>S</given-names></name></person-group><article-title>Studying and improving reasoning in humans and machines</article-title><source>Commun. Psychol.</source><year>2024</year><volume>2</volume><fpage>51</fpage><pub-id pub-id-type="pmid">39242743</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Yax, N., Anll&#x000f3;, H. &#x00026; Palminteri, S. Studying and improving reasoning in humans and machines. <italic>Commun. Psychol.</italic><bold>2</bold>, 51 (2024).<pub-id pub-id-type="pmid">39242743</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Luccioni, A. S., Akiki, C., Mitchell, M. &#x00026; Jernite, Y. Stable bias: evaluating societal representations in diffusion models. <italic>Adv. Neural Inf. Process. Syst.</italic><bold>36</bold> (2024).</mixed-citation></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Buolamwini</surname><given-names>J</given-names></name><name><surname>Gebru</surname><given-names>T</given-names></name></person-group><article-title>Gender shades: intersectional accuracy disparities in commercial gender classification</article-title><source>Proc. Mach. Learn. Res.</source><year>2018</year><volume>81</volume><fpage>1</fpage><lpage>15</lpage></element-citation><mixed-citation id="mc-CR31" publication-type="journal">Buolamwini, J. &#x00026; Gebru, T. Gender shades: intersectional accuracy disparities in commercial gender classification. <italic>Proc. Mach. Learn. Res.</italic><bold>81</bold>, 1&#x02013;15 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Morewedge</surname><given-names>CK</given-names></name><etal/></person-group><article-title>Human bias in algorithm design</article-title><source>Nat. Hum. Behav.</source><year>2023</year><volume>7</volume><fpage>1822</fpage><lpage>1824</lpage><pub-id pub-id-type="pmid">37985907</pub-id>
</element-citation><mixed-citation id="mc-CR32" publication-type="journal">Morewedge, C. K. et al. Human bias in algorithm design. <italic>Nat. Hum. Behav.</italic><bold>7</bold>, 1822&#x02013;1824 (2023).<pub-id pub-id-type="pmid">37985907</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Dastin, J. Amazon scraps secret AI recruiting tool that showed bias against women. <italic>Reuters</italic><ext-link ext-link-type="uri" xlink:href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G">https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G</ext-link> (2018).</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Nasiripour, S. &#x00026; Natarajan, S. Apple co-founder says Goldman&#x02019;s apple card algorithm discriminates. <italic>Bloomberg</italic> (10 November 2019).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Valyaeva, A. AI has already created as many images as photographers have taken in 150 years. <italic>Everypixel Journal</italic><ext-link ext-link-type="uri" xlink:href="https://journal.everypixel.com/ai-image-statistics">https://journal.everypixel.com/ai-image-statistics</ext-link> (2024).</mixed-citation></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><article-title>Understanding human intelligence through human limitations</article-title><source>Trends Cogn. Sci.</source><year>2020</year><volume>24</volume><fpage>873</fpage><lpage>883</lpage><pub-id pub-id-type="pmid">33041198</pub-id>
</element-citation><mixed-citation id="mc-CR36" publication-type="journal">Griffiths, T. L. Understanding human intelligence through human limitations. <italic>Trends Cogn. Sci.</italic><bold>24</bold>, 873&#x02013;883 (2020).<pub-id pub-id-type="pmid">33041198</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name><surname>Geirhos</surname><given-names>R</given-names></name><etal/></person-group><article-title>Shortcut learning in deep neural networks</article-title><source>Nat. Mach. Intell.</source><year>2020</year><volume>2</volume><fpage>665</fpage><lpage>673</lpage></element-citation><mixed-citation id="mc-CR37" publication-type="journal">Geirhos, R. et al. Shortcut learning in deep neural networks. <italic>Nat. Mach. Intell.</italic><bold>2</bold>, 665&#x02013;673 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Bogert</surname><given-names>E</given-names></name><name><surname>Schecter</surname><given-names>A</given-names></name><name><surname>Watson</surname><given-names>RT</given-names></name></person-group><article-title>Humans rely more on algorithms than social influence as a task becomes more difficult</article-title><source>Sci. Rep.</source><year>2021</year><volume>11</volume><fpage>8028</fpage><pub-id pub-id-type="pmid">33850211</pub-id>
</element-citation><mixed-citation id="mc-CR38" publication-type="journal">Bogert, E., Schecter, A. &#x00026; Watson, R. T. Humans rely more on algorithms than social influence as a task becomes more difficult. <italic>Sci. Rep.</italic><bold>11</bold>, 8028 (2021).<pub-id pub-id-type="pmid">33850211</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name><surname>Hou</surname><given-names>YTY</given-names></name><name><surname>Jung</surname><given-names>MF</given-names></name></person-group><article-title>Who is the expert? Reconciling algorithm aversion and algorithm appreciation in AI-supported decision making</article-title><source>Proc. ACM Hum. Comput. Interact.</source><year>2021</year><volume>5</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="pmid">36644216</pub-id>
</element-citation><mixed-citation id="mc-CR39" publication-type="journal">Hou, Y. T. Y. &#x00026; Jung, M. F. Who is the expert? Reconciling algorithm aversion and algorithm appreciation in AI-supported decision making. <italic>Proc. ACM Hum. Comput. Interact.</italic><bold>5</bold>, 1&#x02013;25 (2021).<pub-id pub-id-type="pmid">36644216</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Dietvorst</surname><given-names>BJ</given-names></name><name><surname>Simmons</surname><given-names>JP</given-names></name><name><surname>Massey</surname><given-names>C</given-names></name></person-group><article-title>Algorithm aversion: people erroneously avoid algorithms after seeing them err.</article-title><source>J. Exp. Psychol. Gen.</source><year>2015</year><volume>144</volume><fpage>114</fpage><lpage>126</lpage><pub-id pub-id-type="pmid">25401381</pub-id>
</element-citation><mixed-citation id="mc-CR40" publication-type="journal">Dietvorst, B. J., Simmons, J. P. &#x00026; Massey, C. Algorithm aversion: people erroneously avoid algorithms after seeing them err. <italic>J. Exp. Psychol. Gen.</italic><bold>144</bold>, 114&#x02013;126 (2015).<pub-id pub-id-type="pmid">25401381</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name><surname>Haberman</surname><given-names>J</given-names></name><name><surname>Harp</surname><given-names>T</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name></person-group><article-title>Averaging facial expression over time</article-title><source>J. Vis.</source><year>2009</year><volume>9</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmid">19761316</pub-id>
</element-citation><mixed-citation id="mc-CR41" publication-type="journal">Haberman, J., Harp, T. &#x00026; Whitney, D. Averaging facial expression over time. <italic>J. Vis.</italic><bold>9</bold>, 1&#x02013;13 (2009).<pub-id pub-id-type="pmid">19761316</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Whitney</surname><given-names>D</given-names></name><name><surname>Yamanashi Leib</surname><given-names>A</given-names></name></person-group><article-title>Ensemble perception</article-title><source>Annu. Rev. Psychol.</source><year>2018</year><volume>69</volume><fpage>105</fpage><lpage>129</lpage><pub-id pub-id-type="pmid">28892638</pub-id>
</element-citation><mixed-citation id="mc-CR42" publication-type="journal">Whitney, D. &#x00026; Yamanashi Leib, A. Ensemble perception. <italic>Annu. Rev. Psychol.</italic><bold>69</bold>, 105&#x02013;129 (2018).<pub-id pub-id-type="pmid">28892638</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Goldenberg</surname><given-names>A</given-names></name><name><surname>Weisz</surname><given-names>E</given-names></name><name><surname>Sweeny</surname><given-names>TD</given-names></name><name><surname>Cikara</surname><given-names>M</given-names></name><name><surname>Gross</surname><given-names>JJ</given-names></name></person-group><article-title>The crowd&#x02013;emotion&#x02013;amplification effect</article-title><source>Psychol. Sci.</source><year>2021</year><volume>32</volume><fpage>437</fpage><lpage>450</lpage><pub-id pub-id-type="pmid">33626289</pub-id>
</element-citation><mixed-citation id="mc-CR43" publication-type="journal">Goldenberg, A., Weisz, E., Sweeny, T. D., Cikara, M. &#x00026; Gross, J. J. The crowd&#x02013;emotion&#x02013;amplification effect. <italic>Psychol. Sci.</italic><bold>32</bold>, 437&#x02013;450 (2021).<pub-id pub-id-type="pmid">33626289</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name><surname>Hadar</surname><given-names>B</given-names></name><name><surname>Glickman</surname><given-names>M</given-names></name><name><surname>Trope</surname><given-names>Y</given-names></name><name><surname>Liberman</surname><given-names>N</given-names></name><name><surname>Usher</surname><given-names>M</given-names></name></person-group><article-title>Abstract thinking facilitates aggregation of information</article-title><source>J. Exp. Psychol. Gen.</source><year>2022</year><volume>151</volume><fpage>1733</fpage><lpage>1743</lpage><pub-id pub-id-type="pmid">34928684</pub-id>
</element-citation><mixed-citation id="mc-CR44" publication-type="journal">Hadar, B., Glickman, M., Trope, Y., Liberman, N. &#x00026; Usher, M. Abstract thinking facilitates aggregation of information. <italic>J. Exp. Psychol. Gen.</italic><bold>151</bold>, 1733&#x02013;1743 (2022).<pub-id pub-id-type="pmid">34928684</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Neta</surname><given-names>M</given-names></name><name><surname>Whalen</surname><given-names>PJ</given-names></name></person-group><article-title>The primacy of negative interpretations when resolving the valence of ambiguous facial expressions</article-title><source>Psychol. Sci.</source><year>2010</year><volume>21</volume><fpage>901</fpage><lpage>907</lpage><pub-id pub-id-type="pmid">20534779</pub-id>
</element-citation><mixed-citation id="mc-CR45" publication-type="journal">Neta, M. &#x00026; Whalen, P. J. The primacy of negative interpretations when resolving the valence of ambiguous facial expressions. <italic>Psychol. Sci.</italic><bold>21</bold>, 901&#x02013;907 (2010).<pub-id pub-id-type="pmid">20534779</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Neta</surname><given-names>M</given-names></name><name><surname>Tong</surname><given-names>TT</given-names></name></person-group><article-title>Don&#x02019;t like what you see? Give it time: longer reaction times associated with increased positive affect</article-title><source>Emotion</source><year>2016</year><volume>16</volume><fpage>730</fpage><lpage>739</lpage><pub-id pub-id-type="pmid">27055094</pub-id>
</element-citation><mixed-citation id="mc-CR46" publication-type="journal">Neta, M. &#x00026; Tong, T. T. Don&#x02019;t like what you see? Give it time: longer reaction times associated with increased positive affect. <italic>Emotion</italic><bold>16</bold>, 730&#x02013;739 (2016).<pub-id pub-id-type="pmid">27055094</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &#x00026; Sun, J. Deep residual learning for image recognition. Preprint at 10.48550/arXiv.1512.03385 (2015).</mixed-citation></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="journal"><person-group person-group-type="author"><name><surname>Bang</surname><given-names>D</given-names></name><name><surname>Moran</surname><given-names>R</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Fleming</surname><given-names>SM</given-names></name></person-group><article-title>Neurocomputational mechanisms of confidence in self and others</article-title><source>Nat. Commun.</source><year>2022</year><volume>13</volume><fpage>4238</fpage><pub-id pub-id-type="pmid">35869044</pub-id>
</element-citation><mixed-citation id="mc-CR48" publication-type="journal">Bang, D., Moran, R., Daw, N. D. &#x00026; Fleming, S. M. Neurocomputational mechanisms of confidence in self and others. <italic>Nat. Commun.</italic><bold>13</bold>, 4238 (2022).<pub-id pub-id-type="pmid">35869044</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR49"><label>49.</label><citation-alternatives><element-citation id="ec-CR49" publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><article-title>Bounded integration in parietal cortex underlies decisions even when viewing duration is dictated by the environment</article-title><source>J. Neurosci.</source><year>2008</year><volume>28</volume><fpage>3017</fpage><lpage>3029</lpage><pub-id pub-id-type="pmid">18354005</pub-id>
</element-citation><mixed-citation id="mc-CR49" publication-type="journal">Kiani, R., Hanks, T. D. &#x00026; Shadlen, M. N. Bounded integration in parietal cortex underlies decisions even when viewing duration is dictated by the environment. <italic>J. Neurosci.</italic><bold>28</bold>, 3017&#x02013;3029 (2008).<pub-id pub-id-type="pmid">18354005</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><article-title>Neuronal correlates of a perceptual decision</article-title><source>Nature</source><year>1989</year><volume>341</volume><fpage>52</fpage><lpage>54</lpage><pub-id pub-id-type="pmid">2770878</pub-id>
</element-citation><mixed-citation id="mc-CR50" publication-type="journal">Newsome, W. T., Britten, K. H. &#x00026; Movshon, J. A. Neuronal correlates of a perceptual decision. <italic>Nature</italic><bold>341</bold>, 52&#x02013;54 (1989).<pub-id pub-id-type="pmid">2770878</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>G</given-names></name><name><surname>Sloane</surname><given-names>JF</given-names></name><name><surname>Donkin</surname><given-names>C</given-names></name><name><surname>Newell</surname><given-names>BR</given-names></name></person-group><article-title>Adapting to the algorithm: how accuracy comparisons promote the use of a decision aid</article-title><source>Cogn. Res. Princ. Implic.</source><year>2022</year><volume>7</volume><fpage>14</fpage><pub-id pub-id-type="pmid">35133521</pub-id>
</element-citation><mixed-citation id="mc-CR51" publication-type="journal">Liang, G., Sloane, J. F., Donkin, C. &#x00026; Newell, B. R. Adapting to the algorithm: how accuracy comparisons promote the use of a decision aid. <italic>Cogn. Res. Princ. Implic.</italic><bold>7</bold>, 14 (2022).<pub-id pub-id-type="pmid">35133521</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><citation-alternatives><element-citation id="ec-CR52" publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>DT</given-names></name></person-group><article-title>Factors relevant to the validity of experiments in social settings</article-title><source>Psychol. Bull.</source><year>1957</year><volume>54</volume><fpage>297</fpage><lpage>312</lpage><pub-id pub-id-type="pmid">13465924</pub-id>
</element-citation><mixed-citation id="mc-CR52" publication-type="journal">Campbell, D. T. Factors relevant to the validity of experiments in social settings. <italic>Psychol. Bull.</italic><bold>54</bold>, 297&#x02013;312 (1957).<pub-id pub-id-type="pmid">13465924</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR53"><label>53.</label><citation-alternatives><element-citation id="ec-CR53" publication-type="journal"><person-group person-group-type="author"><name><surname>Kihlstrom</surname><given-names>JF</given-names></name></person-group><article-title>Ecological validity and &#x0201c;ecological validity&#x0201d;</article-title><source>Perspect. Psychol. Sci.</source><year>2021</year><volume>16</volume><fpage>466</fpage><lpage>471</lpage><pub-id pub-id-type="pmid">33593121</pub-id>
</element-citation><mixed-citation id="mc-CR53" publication-type="journal">Kihlstrom, J. F. Ecological validity and &#x0201c;ecological validity&#x0201d;. <italic>Perspect. Psychol. Sci.</italic><bold>16</bold>, 466&#x02013;471 (2021).<pub-id pub-id-type="pmid">33593121</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR54"><label>54.</label><citation-alternatives><element-citation id="ec-CR54" publication-type="journal"><person-group person-group-type="author"><name><surname>Orne</surname><given-names>M</given-names></name></person-group><article-title>On the social psychology of the psychological experiment</article-title><source>Am. Psychol.</source><year>1962</year><volume>17</volume><fpage>776</fpage><lpage>783</lpage></element-citation><mixed-citation id="mc-CR54" publication-type="journal">Orne, M. On the social psychology of the psychological experiment. <italic>Am. Psychol.</italic><bold>17</bold>, 776&#x02013;783 (1962).</mixed-citation></citation-alternatives></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Rombach, R., Blattmann, A., Lorenz, D., Esser, P. &#x00026; Ommer, B. High-resolution image synthesis with latent diffusion models. Preprint at 10.48550/arXiv.2112.10752 (2022).</mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Bianchi, F. et al. Easily accessible text-to-image generation amplifies demographic stereotypes at large scale. Preprint at 10.48550/arXiv.2211.03759 (2023).</mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other"><italic>Labor Force Statistics from the Current Population Survey</italic> (US Bureau of Labor Statistics, 2022); <ext-link ext-link-type="uri" xlink:href="https://www.bls.gov/cps/aa2022/cpsaat11.htm">https://www.bls.gov/cps/aa2022/cpsaat11.htm</ext-link></mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other"><italic>Women in Finance Director Positions</italic> (Office for National Statistics, 2021); <ext-link ext-link-type="uri" xlink:href="https://www.ons.gov.uk/aboutus/transparencyandgovernance/freedomofinformationfoi/womeninfinancedirectorpositions">https://www.ons.gov.uk/aboutus/transparencyandgovernance/freedomofinformationfoi/womeninfinancedirectorpositions</ext-link></mixed-citation></ref><ref id="CR59"><label>59.</label><citation-alternatives><element-citation id="ec-CR59" publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>DS</given-names></name><name><surname>Correll</surname><given-names>J</given-names></name><name><surname>Wittenbrink</surname><given-names>B</given-names></name></person-group><article-title>The Chicago face database: a free stimulus set of faces and norming data</article-title><source>Behav. Res. Methods</source><year>2015</year><volume>47</volume><fpage>1122</fpage><lpage>1135</lpage><pub-id pub-id-type="pmid">25582810</pub-id>
</element-citation><mixed-citation id="mc-CR59" publication-type="journal">Ma, D. S., Correll, J. &#x00026; Wittenbrink, B. The Chicago face database: a free stimulus set of faces and norming data. <italic>Behav. Res. Methods</italic><bold>47</bold>, 1122&#x02013;1135 (2015).<pub-id pub-id-type="pmid">25582810</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Capturing attention in feed: the science behind effective video creative. <italic>Facebook IQ</italic><ext-link ext-link-type="uri" xlink:href="https://www.facebook.com/business/news/insights/capturing-attention-feed-video-creative">https://www.facebook.com/business/news/insights/capturing-attention-feed-video-creative</ext-link> (2016).</mixed-citation></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="other">Stability AI. Celebrating one year(ish) of Stable Diffusion &#x02026; and what a year it&#x02019;s been! (2024); <ext-link ext-link-type="uri" xlink:href="https://stability.ai/news/celebrating-one-year-of-stable-diffusion">https://stability.ai/news/celebrating-one-year-of-stable-diffusion</ext-link></mixed-citation></ref><ref id="CR62"><label>62.</label><citation-alternatives><element-citation id="ec-CR62" publication-type="journal"><person-group person-group-type="author"><name><surname>Glickman</surname><given-names>M</given-names></name><name><surname>Sharot</surname><given-names>T</given-names></name></person-group><article-title>AI-induced hyper-learning in humans</article-title><source>Curr. Opin. Psychol</source><year>2024</year><volume>60</volume><fpage>101900</fpage><pub-id pub-id-type="pmid">39348730</pub-id>
</element-citation><mixed-citation id="mc-CR62" publication-type="journal">Glickman, M. &#x00026; Sharot, T. AI-induced hyper-learning in humans<italic>Curr. Opin. Psychol.</italic><bold>60</bold>, 101900 (2024).<pub-id pub-id-type="pmid">39348730</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Zhao, J., Wang, T., Yatskar, M., Ordonez, V. &#x00026; Chang, K. W. Men also like shopping: reducing gender bias amplification using corpus-level constraints. In <italic>Proc. 2017 Conference on Empirical Methods in Natural Language Processing</italic> (Association for Computational Linguistics, 2017).</mixed-citation></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="other">Dinan, E. et al. Queens are powerful too: mitigating gender bias in dialogue generation. In <italic>Proc. 2020 Conference on Empirical Methods in Natural Language Processing</italic> (Association for Computational Linguistics, 2020).</mixed-citation></ref><ref id="CR65"><label>65.</label><citation-alternatives><element-citation id="ec-CR65" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>A</given-names></name><name><surname>Russakovsky</surname><given-names>O</given-names></name></person-group><article-title>Directional bias amplification</article-title><source>Proc. Mach. Learn. Res.</source><year>2021</year><volume>139</volume><fpage>2640</fpage><lpage>3498</lpage></element-citation><mixed-citation id="mc-CR65" publication-type="journal">Wang, A. &#x00026; Russakovsky, O. Directional bias amplification. <italic>Proc. Mach. Learn. Res.</italic><bold>139</bold>, 2640&#x02013;3498 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR66"><label>66.</label><mixed-citation publication-type="other">Mansoury, M., Abdollahpouri, H., Pechenizkiy, M., Mobasher, B. &#x00026; Burke, R. Feedback loop and bias amplification in recommender systems. Preprint at 10.48550/arXiv.2007.13019 (2020).</mixed-citation></ref><ref id="CR67"><label>67.</label><citation-alternatives><element-citation id="ec-CR67" publication-type="journal"><person-group person-group-type="author"><name><surname>Kidd</surname><given-names>C</given-names></name><name><surname>Birhane</surname><given-names>A</given-names></name></person-group><article-title>How AI can distort human beliefs</article-title><source>Science</source><year>2023</year><volume>380</volume><fpage>1222</fpage><lpage>1223</lpage><pub-id pub-id-type="pmid">37347992</pub-id>
</element-citation><mixed-citation id="mc-CR67" publication-type="journal">Kidd, C. &#x00026; Birhane, A. How AI can distort human beliefs. <italic>Science</italic><bold>380</bold>, 1222&#x02013;1223 (2023).<pub-id pub-id-type="pmid">37347992</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR68"><label>68.</label><citation-alternatives><element-citation id="ec-CR68" publication-type="journal"><person-group person-group-type="author"><name><surname>Tschandl</surname><given-names>P</given-names></name><etal/></person-group><article-title>Human&#x02013;computer collaboration for skin cancer recognition</article-title><source>Nat. Med.</source><year>2020</year><volume>26</volume><fpage>1229</fpage><lpage>1234</lpage><pub-id pub-id-type="pmid">32572267</pub-id>
</element-citation><mixed-citation id="mc-CR68" publication-type="journal">Tschandl, P. et al. Human&#x02013;computer collaboration for skin cancer recognition. <italic>Nat. Med.</italic><bold>26</bold>, 1229&#x02013;1234 (2020).<pub-id pub-id-type="pmid">32572267</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR69"><label>69.</label><citation-alternatives><element-citation id="ec-CR69" publication-type="journal"><person-group person-group-type="author"><name><surname>Pataranutaporn</surname><given-names>P</given-names></name><name><surname>Liu</surname><given-names>R</given-names></name><name><surname>Finn</surname><given-names>E</given-names></name><name><surname>Maes</surname><given-names>P</given-names></name></person-group><article-title>Influencing human&#x02013;AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness</article-title><source>Nat. Mach. Intell.</source><year>2023</year><volume>5</volume><fpage>1076</fpage><lpage>1086</lpage></element-citation><mixed-citation id="mc-CR69" publication-type="journal">Pataranutaporn, P., Liu, R., Finn, E. &#x00026; Maes, P. Influencing human&#x02013;AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness. <italic>Nat. Mach. Intell.</italic><bold>5</bold>, 1076&#x02013;1086 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR70"><label>70.</label><citation-alternatives><element-citation id="ec-CR70" publication-type="journal"><person-group person-group-type="author"><name><surname>Erdfelder</surname><given-names>E</given-names></name><name><surname>FAul</surname><given-names>F</given-names></name><name><surname>Buchner</surname><given-names>A</given-names></name><name><surname>Lang</surname><given-names>AG</given-names></name></person-group><article-title>Statistical power analyses using G*Power 3.1: tests for correlation and regression analyses</article-title><source>Behav. Res. Methods</source><year>2009</year><volume>41</volume><fpage>1149</fpage><lpage>1160</lpage><pub-id pub-id-type="pmid">19897823</pub-id>
</element-citation><mixed-citation id="mc-CR70" publication-type="journal">Erdfelder, E., FAul, F., Buchner, A. &#x00026; Lang, A. G. Statistical power analyses using G*Power 3.1: tests for correlation and regression analyses. <italic>Behav. Res. Methods</italic><bold>41</bold>, 1149&#x02013;1160 (2009).<pub-id pub-id-type="pmid">19897823</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR71"><label>71.</label><citation-alternatives><element-citation id="ec-CR71" publication-type="journal"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P</given-names></name><name><surname>Friesen</surname><given-names>WV</given-names></name></person-group><article-title>Measuring facial movement</article-title><source>Environ. Psychol. Nonverbal Behav.</source><year>1976</year><volume>1</volume><fpage>56</fpage><lpage>75</lpage></element-citation><mixed-citation id="mc-CR71" publication-type="journal">Ekman, P. &#x00026; Friesen, W. V. Measuring facial movement. <italic>Environ. Psychol. Nonverbal Behav.</italic><bold>1</bold>, 56&#x02013;75 (1976).</mixed-citation></citation-alternatives></ref></ref-list></back></article>