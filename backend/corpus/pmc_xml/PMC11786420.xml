<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Biomed Eng</journal-id><journal-id journal-id-type="iso-abbrev">BMC Biomed Eng</journal-id><journal-title-group><journal-title>BMC Biomedical Engineering</journal-title></journal-title-group><issn pub-type="epub">2524-4426</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39891283</article-id><article-id pub-id-type="pmc">PMC11786420</article-id><article-id pub-id-type="publisher-id">88</article-id><article-id pub-id-type="doi">10.1186/s42490-025-00088-2</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>A novel ViT-BILSTM model for physical activity intensity classification in adults using gravity-based acceleration</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Wang</surname><given-names>Lin</given-names></name><address><email>lw679@exeter.ac.uk</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Luo</surname><given-names>Zizhang</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Tianle</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03yghzc09</institution-id><institution-id institution-id-type="GRID">grid.8391.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8024</institution-id><institution>Faculty of Health and Life Sciences, </institution><institution>University of Exeter, </institution></institution-wrap>Heavitree Road, Exeter, EX1 2LU UK </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05bhmhz54</institution-id><institution-id institution-id-type="GRID">grid.410654.2</institution-id><institution-id institution-id-type="ISNI">0000 0000 8880 6009</institution-id><institution>Engineering &#x00026; Technology College, Yangtze University, </institution></institution-wrap>Jingzhou, 434023 China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04xs57h96</institution-id><institution-id institution-id-type="GRID">grid.10025.36</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8470</institution-id><institution>Department of Computer Science, </institution><institution>University of Liverpool, </institution></institution-wrap>Liverpool, L69 3DR UK </aff></contrib-group><pub-date pub-type="epub"><day>1</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>1</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>7</volume><elocation-id>2</elocation-id><history><date date-type="received"><day>6</day><month>7</month><year>2024</year></date><date date-type="accepted"><day>2</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><sec><title>Aim</title><p id="Par1">The aim of this study is to apply a novel hybrid framework incorporating a Vision Transformer (ViT) and bidirectional long short-term memory (Bi-LSTM) model for classifying physical activity intensity (PAI) in adults using gravity-based acceleration. Additionally, it further investigates how PAI and temporal window (TW) impacts the model&#x02019; s accuracy.</p></sec><sec><title>Method</title><p id="Par2">This research used the Capture-24 dataset, consisting of raw accelerometer data from 151 participants aged 18 to 91. Gravity-based acceleration was utilised to generate images encoding various PAIs. These images were subsequently analysed using the ViT-BiLSTM model, with results presented in confusion matrices and compared with baseline models. The model&#x02019;s robustness was evaluated through temporal stability testing and examination of accuracy and loss curves.</p></sec><sec><title>Result</title><p id="Par3">The ViT-BiLSTM model excelled in PAI classification task, achieving an overall accuracy of 98.5%&#x02009;&#x000b1;&#x02009;1.48% across five TWs-98.7% for 1s, 98.1% for 5s, 98.2% for 10s, 99% for 15s, and 98.65% for 30s of TW. The model consistently exhibited superior accuracy in predicting sedentary (98.9%&#x02009;&#x000b1;&#x02009;1%) compared to light physical activity (98.2%&#x02009;&#x000b1;&#x02009;2%) and moderate-to-vigorous physical activity (98.2%&#x02009;&#x000b1;&#x02009;3%). ANOVA showed no significant accuracy variation across PAIs (F&#x02009;=&#x02009;2.18, <italic>p</italic>&#x02009;=&#x02009;0.13) and TW (F&#x02009;=&#x02009;0.52, <italic>p</italic>&#x02009;=&#x02009;0.72). Accuracy and loss curves show the model consistently improves its performance across epochs, demonstrating its excellent robustness.</p></sec><sec><title>Conclusion</title><p id="Par4">This study demonstrates the ViT-BiLSTM model&#x02019;s efficacy in classifying PAI using gravity-based acceleration, with performance remaining consistent across diverse TWs and intensities. However, PAI and TW could result in slight variations in the model&#x02019;s performance. Future research should concern and investigate the impact of gravity-based acceleration on PAI thresholds, which may influence model&#x02019;s robustness and reliability.</p></sec><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1186/s42490-025-00088-2.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Deep learning</kwd><kwd>Raw accelerometer data</kwd><kwd>Variation</kwd><kwd>Generalisation</kwd><kwd>Physical activity patterns</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; BioMed Central Ltd. part of Springer Nature 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par18">The use of accelerometer-based measurements for physical activity (PA) has become increasingly prevalent, as it reduces biases inherent in self-reported data and provides more accurate and insightful information on PA [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]. However, this method presents challenges, particularly in classifying different intensities of PA [<xref ref-type="bibr" rid="CR3">3</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref>]. Different intensities of PA can have varying effects on health. For instance, prolonged periods of light PA (LPA) and moderate-to-vigorous PA (MVPA) have different impacts on cardiovascular health in adults [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>]. Thus, accurately capturing different intensities is crucial for understanding their health implications [<xref ref-type="bibr" rid="CR8">8</xref>&#x02013;<xref ref-type="bibr" rid="CR10">10</xref>].</p><p id="Par19">Traditionally, accelerometer data processing methods estimate PA intensity using cut-point that based on metabolic equivalents (METs), such as LPA &#x0003c;3 METs; MPA&#x02009;=&#x02009;3&#x02013;5.99 METs; VPA &#x02265;6 METs [<xref ref-type="bibr" rid="CR11">11</xref>]. Alternatively, studies have used counts, which represent the cumulative acceleration signals within a specified time interval (epoch), typically filtered to remove noise and high-frequency vibrations, and expressed as an aggregate value for each epoch [<xref ref-type="bibr" rid="CR12">12</xref>]. Within the adults population, various studies use different step count thresholds to classify LPA, MVPA, and sedentary (SB), such as, for MVPA, thresholds include &#x02265;1952 counts per minute [<xref ref-type="bibr" rid="CR12">12</xref>], and &#x02265;2020 counts/min [<xref ref-type="bibr" rid="CR13">13</xref>]. Using different cut points for the same population and the same intensity complicates comparative analyses. Furthermore, METs-based intensity estimation can be affected by individual differences, environmental factors, and device placement, leading to inaccuracies [<xref ref-type="bibr" rid="CR14">14</xref>].</p><p id="Par20">Recent research has explored machine learning methods to overcome the limitations of cut points in classifying PA intensities [<xref ref-type="bibr" rid="CR15">15</xref>&#x02013;<xref ref-type="bibr" rid="CR18">18</xref>]. Previous work predominantly relied on traditional machine learning algorithms like k-Nearest Neighbours (<italic>k&#x02013;NN</italic>), Support Vector Machine (SVM), Random Forest (RF), hidden semi-Markov models for activity recognition [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>]. These machine learning methods have shown good performance and efficiency in classifying PA intensities. However, they depend on manually designed and selected features, which are time-consuming and may miss important features [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR21">21</xref>]. For example, Chong, Tjurin [<xref ref-type="bibr" rid="CR22">22</xref>] used filter, wrapper, and embedded methods to find suitable feature subsets for activity prediction. While wrappers can find better feature subsets, they are computationally expensive and prone to overfitting. Filter and wrapper methods also struggle to capture complex feature interactions. Consequence, convolutional neural networks (CNNs) have emerged as a powerful alternative due to their ability to automatically learn and extract relevant features from raw data without manual intervention. This characteristic enables CNNs to capture complex patterns and interactions within the data that traditional machine learning methods might miss.</p><p id="Par21">The study by Nawaratne, Alahakoon [<xref ref-type="bibr" rid="CR16">16</xref>] represents an advancement in the field of accelerometer-based PA by leveraging deep learning, the research provides a more accurate, user-friendly approach to predicting energy expenditure and physical activity intensity in free-living conditions. Specifically, the Convolutional Neural Network with custom feature extraction models for Untrained Group results shows that SB achieved correct predictions of 85.4%, LPA achieved correct predictions of 84.2% and MVPA achieved correct predictions of 63.1%. The study by Widianto, Sugiarto [<xref ref-type="bibr" rid="CR21">21</xref>], applied a CNN model to classify PAI in adults wearing five accelerometers. The model achieved accuracies of 97% for MPA, 95% for LPA, and 98% for SD. However, previous studies were conducted in laboratory settings. The CNN model, which demonstrated excellent performance, required participants to wear five accelerometers. This setup reduces the feasibility of capturing physical activity in natural environments. Moreover, these studies rely on unidimensional time-series data, potentially failing to capture the global information during activities.</p><p id="Par22">Recently, Farrahi, Muhammad [<xref ref-type="bibr" rid="CR15">15</xref>] applied AccNet24 framework that has laid the groundwork for analysing 24-h PA behaviours using wrist-worn accelerometer data in free-living conditions, applied recurrent neural networks (RNNs), including BiLSTM (Bidirectional Long Short-Term Memory) networks to classify PAI. Unlike traditional one-dimensional raw accelerometer data processing, this framework uses two-dimensional (2D) images to handle the data. This approach can provide the model with richer information. Moreover, BiLSTM networks excel at processing time-series data by capturing dynamic patterns and temporal dependencies in both forward and backward directions. This bidirectional architecture allows the network to utilize information from both past and future states at each time step, which is particularly valuable for time-series data where the context of both preceding and succeeding data points can influence the interpretation of a given point [<xref ref-type="bibr" rid="CR23">23</xref>]. This is particularly useful when processing continuous activity data or transitions between different activities. Nevertheless, this model is not sensitive to the global spatial information of the activity, meaning that intensity information might be easily overlooked. On the other hand, ViT excels at extracting complex spatial features from images, such as changes in motion position and intensity, as well as, the ViT model leverages a global attention mechanism to identify more meaningful global features within the images, further enhancing classification performance [<xref ref-type="bibr" rid="CR24">24</xref>]. Consequently, this study combines the strengths of ViT and BiLSTM, fully leveraging the spatial features of images and the temporal features of time series to improve the accuracy of activity intensity classification. Furthermore, temporal stability is a crucial factor affecting model performance, especially for images generated from time-series data [<xref ref-type="bibr" rid="CR25">25</xref>]. Farrahi, Muhammad [<xref ref-type="bibr" rid="CR15">15</xref>] study just a 30-s TW to generate images; it may overlook that TW is a crucial factor influencing PA features in adults, consequently affecting the model&#x02019;s accuracy. Meanwhile, the variance in intensity might also influence model&#x02019;s accuracy as SD typically involves longer, stable periods, whereas MVPA requires higher gravity acceleration and is more distinct. But, predicting LPA is more challenging due to its complexity, such as slow walking, fast walking, or occasional movement [<xref ref-type="bibr" rid="CR26">26</xref>]. Additionally, Farrahi, Muhammad [<xref ref-type="bibr" rid="CR15">15</xref>] used METs-based classification from video-labelled datasets, which may limit the model&#x02019;s practical utility. As METs-based activity intensity has been shown to confuse intensity types [<xref ref-type="bibr" rid="CR14">14</xref>], further affecting the connection between intensity and health outcomes [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>]. Recent studies advocate the use of gravity-based acceleration to cumulate PA, as it can reduce errors associated with traditional methods such as cut points and METs in calculating accumulated activity [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR27">27</xref>].</p><p id="Par23">Therefore, this study used the ViT-BiLSTM model to classify images encoded from gravity-based acceleration data to determine PAI, Further, it considers how temporal stability (different TWs for images), and PAI, affect the model&#x02019;s accuracy. The objectives of this study are: (1) to use the ViT-BiLSTM model to predict PA intensities calculated from gravity-based acceleration data in adults; (2) to examine the model&#x02019;s robustness across different TWs and PAIs; and (3) to observation how different PAIs and TWs impact the accuracy of model.</p></sec><sec id="Sec2"><title>Methodology</title><p id="Par24">In this study, we developed a novel framework to classify PAI employing a hybrid Vision Transformer (ViT) and bidirectional long short-term memory (Bi-LSTM) network. We used raw accelerometer data from 151 participants, which were pre-processed into gravitational acceleration using the Euclidean Norm Minus One (ENMO) algorithm and subsequently converted into GAF images. The overall workflow is illustrated in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>. Initially, GAF images were generated from the pre-processed data. These images were processed by the ViT component to extract spatial features. These features were subseqently fed into a BiLSTM network to capture temporal dependencies. Finally, a fully connected layer that classified the PAIs into SD, LPA, and MVPA is added on the top of the network.<fig id="Fig1"><label>Fig. 1</label><caption><p>Overall flowchart of ViT-BiLSTM model for physical activity intensity in adults using gravity-based acceleration</p></caption><graphic xlink:href="42490_2025_88_Fig1_HTML" id="MO1"/></fig></p><sec id="Sec3"><title>Dataset</title><p id="Par25">The present study employed the Capture-24 dataset, which comprises data from Axivity AX3 wrist-worn activity trackers collected from 151 adults aged 18&#x02013;91&#x000a0;years in Oxfordshire between 2014 and 2016. Participants wore these devices continuously over approximately 24&#x000a0;h at a sampling frequency of 100&#x000a0;Hz, resulting in nearly 4000&#x000a0;h of data, with over 2500&#x000a0;h annotated based on validated ground truth activities [<xref ref-type="bibr" rid="CR28">28</xref>].</p></sec><sec id="Sec4"><title>Participants</title><p id="Par26">The Capture-24 dataset includes 131 participants (78 women): 74 young adults (18&#x02013;39&#x000a0;years; 32 men, 42 women), 42 middle-aged adults (40&#x02013;59&#x000a0;years; 7 men, 27 women), and 15 older adults (60+ years; 14 men, 9 women) [<xref ref-type="bibr" rid="CR29">29</xref>].</p></sec><sec id="Sec5"><title>Acceleration signal-to-image</title><sec id="Sec6"><title>Data pre-processing (physical activity intensity labelling)</title><p id="Par27">The Euclidean Norm Minus One (ENMO) algorithm calculates raw gravitational acceleration (g) by subtracting 1&#x000a0;g (1&#x000a0;g&#x02009;=&#x02009;9.81&#x000a0;m/s<sup>2</sup>) from the Euclidean norm of the three-axis acceleration signals. Its simplicity and effectiveness lie in its ability to separate gravitational and movement components without requiring complex frequency filtering, thereby demonstrating robust performance, particularly in free-living conditions [<xref ref-type="bibr" rid="CR30">30</xref>]. Subsequently, images are labelled based on the Hildebrand et al. [<xref ref-type="bibr" rid="CR31">31</xref>] thresholds derived from the raw gravitational acceleration (g).</p><p id="Par28">Equation (<xref rid="Equ1" ref-type="disp-formula">1</xref>) defines the ENMO, which is used to compute the magnitude of acceleration in second-by-second time series and classify the PAI:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ENMO = \sqrt {{x^2} + {y^2} + {z^2}} - 1g$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par29">where x, y, and z represent the components of acceleration in three dimensions. ENMO is calculated by taking the Euclidean norm (<italic>i.e.,</italic> the length of the acceleration vector) in three-dimensional space and then subtracting 1&#x000a0;g (1&#x000a0;g&#x02009;=&#x02009;9.81&#x000a0;m/s<sup>2</sup>) (the acceleration due to gravity), yielding a corrected value for activity intensity [<xref ref-type="bibr" rid="CR30">30</xref>]. The x, y, and z orientations of the Axivity AX3 are explained in Supplemental Material <xref rid="MOESM1" ref-type="media">1</xref>.</p><p id="Par30">The PAI threshold defined by Hildebrand et al. [<xref ref-type="bibr" rid="CR31">31</xref>] for adults (18&#x02013;65&#x000a0;years) uses raw gravity acceleration (g) as follow:<list list-type="bullet"><list-item><p id="Par31">Sedentary: 0&#x02013;10&#x000a0;g/s</p></list-item><list-item><p id="Par32">LPA: 10&#x02013;42&#x000a0;g/s PAI threshold of Hildebrand et al. [<xref ref-type="bibr" rid="CR31">31</xref>]</p></list-item><list-item><p id="Par33">MVPA: &#x0003e;42&#x000a0;g/s</p></list-item></list></p><p id="Par34">for sedentary behaviour, the acceleration is less than 10&#x000a0;g/s; LPA is between 10 and 42&#x000a0;g/s; for MVPA is above 42&#x000a0;g/s.</p><p id="Par35">Additionally, the collection data are based on an acceleration sampling frequency of 100&#x000a0;Hz, meaning that the defined activity intensities correspond to the cumulative acceleration over specific data points within a given time window. For instance, a 1-s window would include 100 data points, while 5-s, 10-s, and 15-s windows would represent 500, 1000, and 1500 data points, respectively.</p></sec><sec id="Sec7"><title>Gramian angular field</title><p id="Par36">To transform raw acceleration signals into GAF images, we followed a comprehensive process involving several steps, as detailed below:</p><p id="Par37">We computed the magnitude of the raw acceleration signal (<italic>x,y,z</italic>) using the ENMO method to remove the effect of gravity and filter out negative values. The result is a signal sequence <italic>v</italic><sub><italic>t</italic></sub> at each time point <italic>t</italic>, where: <italic>v</italic><sub><italic>t</italic></sub> represents the magnitude of acceleration at time <italic>t</italic>.</p><p id="Par38">To ensure consistency across all samples, the signal sequence <inline-formula id="IEq1"><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left( {{v_t}} \right)$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq1.gif"/></alternatives></inline-formula> was normalized to the range [&#x02212;1, 1]. The normalization formula is as follows:<disp-formula id="Equb"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widetilde {{v_t}} = \frac{{{v_t} - \min \left( v \right)}}{{\max \left( v \right) - \min \left( v \right)}} \cdot 2 - 1,\forall t$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equb.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par39">Where: <inline-formula id="IEq2"><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${v_t}{\text{ }}$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq2.gif"/></alternatives></inline-formula>is the original signal value at time <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t;$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq3.gif"/></alternatives></inline-formula>
<inline-formula id="IEq4"><alternatives><tex-math id="M6">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\min \left( v \right)$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq4.gif"/></alternatives></inline-formula> and<inline-formula id="IEq5"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{ max}}\left( v \right)$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq5.gif"/></alternatives></inline-formula> represent the minimum and maximum values of the signal, respectively; <inline-formula id="IEq6"><alternatives><tex-math id="M8">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widetilde {{v_t}}$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq6.gif"/></alternatives></inline-formula> is the normalized signal value.</p><p id="Par40">The normalized signal <inline-formula id="IEq7"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widetilde {{v_t}}$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq7.gif"/></alternatives></inline-formula> is mapped into a polar coordinate system, representing the signal in terms of angles and radii.</p><p id="Par410"><italic>Angle</italic> (<italic>&#x003b8;</italic><sub><italic>t</italic></sub>)</p><p id="Par41">Each normalized signal value is converted into an angular value using the inverse cosine function:<disp-formula id="Equd"><alternatives><tex-math id="M10">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\theta _t} = \arccos \left( {\widetilde {{v_t}}} \right),\quad - 1 \leqslant \widetilde {{v_t}} \leqslant 1$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equd.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par42">The angle encodes the relative amplitude of the signal.</p><p id="Par43">Radius (<italic>r</italic><sub><italic>t</italic></sub>)</p><p id="Par430">
<disp-formula id="Eque"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r_t} = \frac{t}{T},\quad t = 1,2, \ldots ,T$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Eque.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par44">The radius represents the normalized time index, preserving the sequential nature of the time series, where <italic>T</italic> is the total length of the signal sequence.</p><p id="Par45">Using the angular values <inline-formula id="IEq8"><alternatives><tex-math id="M12">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\theta _t}$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq8.gif"/></alternatives></inline-formula> from the polar representation, the GAF matrix is constructed to capture the temporal dependencies of the time series. The matrix elements are defined as:<disp-formula id="Equf"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G\left[ {i,j} \right] = \cos \left( {{\theta _i} + {\theta _j}} \right),\quad i,j = 1,2, \ldots ,T$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equf.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par46">where:</p><p id="Par47">G[<italic>i,j</italic>] represents the cosine of the sum of the angles at time points <italic>i</italic> and <italic>j</italic>; This formula encodes both global and local temporal features of the signal. To facilitate visualization, the GAF matrix values were normalized to the range [0, 1]. And then, the GAF images were generated with a resolution of 224&#x02009;&#x000d7;&#x02009;224 pixels, which is a widely adopted standard resolution in computer vision tasks, particularly for models using ViT architecture.</p></sec></sec><sec id="Sec8"><title>ViT-BiLSTM model</title><p id="Par48">The ViT-BiLSTM model combines the ViT for spatial feature extraction and BiLSTM for capturing temporal dependencies. Below is the detailed algorithmic description:</p><sec id="Sec9"><title>ViT component</title><sec id="Sec10"><title>Patch embedding</title><p id="Par49">The GAF images derived from accelerometer data are divided into fixed-size patches. Each patch is then flattened and mapped to a lower-dimensional space through a linear projection. Given an input image X&#x02208;R<sup><italic>H&#x000d7;W&#x000d7;C</italic></sup> of height <italic>H,</italic> width <italic>W,</italic> and <italic>C</italic> channels, the image is divided into <italic>N</italic> patches, each of size <italic>P</italic>&#x02009;<italic>&#x000d7;</italic>&#x02009;<italic>P</italic>. The resulting patches <italic>x</italic><sub><italic>p</italic></sub> are linearly transformed into embeddings <italic>z</italic><sub><italic>p</italic></sub>:<disp-formula id="Equg"><alternatives><tex-math id="M14">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${z_p} = {E_{xp}} + ep$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equg.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par50">where <italic>E</italic> is a learnable embedding matrix and <italic>e</italic><sub><italic>p</italic></sub> is the positional embedding.</p></sec><sec id="Sec11"><title>Positional encoding</title><p id="Par51">Positional encodings <italic>e</italic><sub><italic>p</italic></sub> are added to the patch embeddings to retain spatial information. These encodings help the model understand the order and position of patches.</p></sec><sec id="Sec12"><title>Transformer encoder</title><p id="Par52">The transformer encoder consists of multiple layers, each containing a multi-head self-attention mechanism and a position-wise feed-forward network. Each encoder layer updates the patch embeddings as follows:</p><p id="Par520">
<disp-formula id="Equz"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z{\prime _p} = {\rm{LayerNorm}}\left( {{z_p} + {\rm{MultiHeadSelfAttention}}\left( {{z_p}} \right)} \right)$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equz.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par5200">
<disp-formula id="Equh"><alternatives><tex-math id="M16">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${z_p}^{(i + 1)} = {\rm{ LayerNorm }}\left( {z{\prime _p} + {\rm{FeedForward }}\left( {z{\prime _p}} \right)} \right)$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equh.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par53">where <italic>z</italic><sup><italic>(i)</italic></sup><sub><italic>p</italic></sub> denotes the patch embeddings after the <italic>i&#x02013;th</italic> encoder layer.</p><p id="Par54">The features extracted by the ViT component are then fed into a BiLSTM model, which captures temporal dependencies through forward and reverse LSTM layers. This bidirectional processing enhances the model&#x02019;s ability to learn temporal patterns in both directions, contributing to improved classification accuracy.</p></sec></sec></sec><sec id="Sec13"><title>BiLSTM component</title><sec id="Sec14"><title>Sequence processing</title><p id="Par55">The feature vectors from the ViT component, representing spatial features of the input image patches, are fed into a BiLSTM network to capture temporal dependencies. The BiLSTM processes the sequence of feature vectors in both forward and backward directions.</p><p id="Par56">Let <italic>h</italic><sub><italic>t</italic></sub>forward and <italic>h</italic><sub><italic>t</italic></sub>backword be the hidden states of the forward and backward LSTM cells at time step <italic>t</italic>, respectively. The BiLSTM outputs are concatenated:<disp-formula id="Equi"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${h_t}{\text{forward}} = {\text{ LSTM }}({h_{t - 1}}{\text{forward}},{\text{ }}{{\text{x}}_{\text{t}}},{\text{ }}{{\text{c}}_{t - 1}}{\text{forward}}),{\text{ t}} \in \left[ {0,{\text{T}}} \right]$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equi.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equj"><alternatives><tex-math id="M18">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${h_t}{\text{backward}} = {\text{ LSTM }}({h_{t - 1}}{\text{backward}},{\text{ }}{{\text{x}}_{\text{t}}},{\text{ }}{{\text{c}}_{t - 1}}{\text{backward}}),{\text{ t}} \in \left[ {0,{\text{T}}} \right]$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equj.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equk"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\text{H}}_t} = [{h_t}{\text{forward}};{h_t}{\text{backward}}]$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equk.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par57">
<italic>Concatenation and Classification:</italic>
<disp-formula id="Equl"><alternatives><tex-math id="M20">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${h_{final}} = [{h_t}\,{\text{forward}};{h_t}\,{\text{backward}}]$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equl.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par58">The final output is computed using a softmax layer:<disp-formula id="Equm"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Output}} = {\text{Softmax}}\left( {{W_o}{h_{final}} + {b_o}} \right)$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equm.gif" position="anchor"/></alternatives></disp-formula></p></sec></sec><sec id="Sec15"><title>Experimental setup</title><p id="Par59">This study encoding images into three different PAI: SD, LPA, and MVPA, using gravity acceleration. For the GAF image generation, data was collected daily from 8 AM to 10 PM, focusing on the time period when SD, LPA, and MVPA activities predominantly occur during typical waking hours. The ViT-BiLSTM model then predicted the PAI. To enhance the credibility of the model, this study compares several models: CNN, ViT, BiLSTM, ViT-BiLSTM, and CNN-BiLSTM. Consequently, the temporal stability testing, boundary value analysis, and accuracy and loss curves assessed robustness of model. ANOVA tests are used to examine the robustness and reliability of the model&#x02019;s accuracy across various TWs (e.g., 1s, 5s, 30s), ensuring consistent performance regardless of these variations. Additionally, accuracy and loss values in the training-validation process were utilised to observe changes in accuracy and loss values, based on the TW yielding the highest accuracy results. This approach aids in understanding the model&#x02019;s learning dynamics and stability over time. Finally, we examined the mean and standard deviation values to understand how PAI and TW affect the model&#x02019;s accuracy.</p></sec><sec id="Sec16"><title>Training details</title><p id="Par60">All experiments are carried out on a workstation with NVIDIA 2080ti GPUs, and the dataset is divided into training (60%), validation (20%) and testing (20%) sets. During the training process, model performance was evaluated on both training and validation sets after each epoch. This allowed us to monitor the model&#x02019;s learning progress and ensure it generalized well to unseen data. The validation performance served as an important indicator for potential overfitting, helping to optimize the model&#x02019;s hyperparameters and determine when to stop training. During the training process, model performance was evaluated on both training and validation sets after each epoch. This allowed us to monitor the model&#x02019;s learning progress and ensure it generalized well to unseen data. The validation performance served as an important indicator for potential overfitting, helping to optimize the model&#x02019;s hyperparameters and determine when to stop training.</p><p id="Par61">As Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> shows, the training process utilised a batch size of 16, with a sequence length of 4, resizing each image resized to 224&#x000a0;&#x000d7;&#x000a0;224 pixels. The model was trained over 10 epochs with a learning rate of 1e-5, and weight decay set to 0.001 to prevent overfitting. The selection of 10 training epochs was determined through extensive preliminary experiments that evaluated the trade-off between model performance and computational efficiency. While the loss curves showed continuing minor decrements beyond 10 epochs, we observed that: 1. The rate of improvement in both training and validation loss decreased substantially after epoch 8, with changes in validation accuracy of less than 0.1% per subsequent epoch; 2. The model achieved 98.5%&#x02009;&#x000b1;&#x02009;1.48% accuracy across all temporal windows by epoch 10; 3. Extended training beyond 10 epochs (tested up to 20 epochs) produced only marginal improvements (&#x0003c;0.2% increase in accuracy) while significantly increasing computational costs; 4. Early stopping criteria monitoring validation loss showed that the risk of overfitting increased after epoch 10, even though training loss continued to decrease.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Model&#x02019;s hyper-parameters</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Stage</th><th align="left">Hyper-parameter</th><th align="left">Value</th></tr></thead><tbody><tr><td align="left" rowspan="2">Image processing</td><td align="left">Image size</td><td align="left">224 &#x000b4; 224</td></tr><tr><td align="left">Sequence length</td><td align="left">4</td></tr><tr><td align="left" rowspan="10">Architecture</td><td align="left">ViT</td><td align="left"/></tr><tr><td align="left">Model</td><td align="left">vit base patch16 224</td></tr><tr><td align="left">Pretrained</td><td align="left">True</td></tr><tr><td align="left">Input channels</td><td align="left">3</td></tr><tr><td align="left">Output features</td><td align="left">768</td></tr><tr><td align="left">BiLSTM</td><td align="left"/></tr><tr><td align="left">Hidden size</td><td align="left">128</td></tr><tr><td align="left">Number of layers</td><td align="left">2</td></tr><tr><td align="left">Bidirectional</td><td align="left">True</td></tr><tr><td align="left">Output dimension</td><td align="left">256</td></tr><tr><td align="left" rowspan="10">Training</td><td align="left">Batch size</td><td align="left">16</td></tr><tr><td align="left">Learning rate</td><td align="left">1e-5</td></tr><tr><td align="left">Weight decay</td><td align="left">0.001</td></tr><tr><td align="left">Optimizer</td><td align="left">Adam</td></tr><tr><td align="left">Loss function</td><td align="left">CrossEntropyLoss</td></tr><tr><td align="left">Scheduler</td><td align="left">StepLR</td></tr><tr><td align="left">Scheduler step size</td><td align="left">1</td></tr><tr><td align="left">Scheduler gamma</td><td align="left">0.8</td></tr><tr><td align="left">Number of epochs</td><td align="left">10</td></tr><tr><td align="left">Dropout</td><td align="left">0.5</td></tr><tr><td align="left" rowspan="4">Data augmentation</td><td align="left">Resize</td><td align="left">224 &#x000b4; 224</td></tr><tr><td align="left">Convert to RGB</td><td align="left">Yes</td></tr><tr><td align="left">Normalize mean</td><td align="left">0.3796, 0.3915, 0.8996</td></tr><tr><td align="left">Normalize std</td><td align="left">0.1860, 0.3054, 0.1428</td></tr></tbody></table></table-wrap></p><p id="Par62">During training, we employed Adaptive Moment Estimation (Adam) optimizer proposed by Kingma (2015) to update the model parameters. Adam adapts the learning rates for each parameter using estimates of first and second moments of the gradients. Specifically, the learning rate was adjusted using a step learning rate scheduler with a gamma of 0.8 and exponential decay rates for the moment estimates &#x003b2;&#x02081;&#x02009;=&#x02009;0.9 and &#x003b2;&#x02082;&#x02009;=&#x02009;0.999. Mixed precision training was enabled through the use of a gradient scaler to enhance computational efficiency. The network architecture, and training parameters are detailed in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.</p></sec><sec id="Sec17"><title>Evaluation metrics</title><p id="Par63">The model is trained using CrossEntropyLoss <inline-formula id="IEq9"><alternatives><tex-math id="M22">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L\left( {y,\hat y} \right) = - {\rm{ }}\sum\nolimits_1^n {[{y_i}*\log \left( {{{\hat y}_i}} \right)]}$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq9.gif"/></alternatives></inline-formula> as the loss function, where <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq10.gif"/></alternatives></inline-formula> is the true label and <inline-formula id="IEq11"><alternatives><tex-math id="M24">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat y$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq11.gif"/></alternatives></inline-formula> is the predicted probability distribution. In order to comprehensively evaluate the model&#x02019;s performance, we employed multiple metrics, based on the following basic evaluation components in classification tasks. True Positives (TP): Cases where the model correctly identified the actual physical activity intensity; True Negatives (TN): Cases where the model correctly identified that the activity was not of a particular intensity; False Positives (FP): Cases where the model incorrectly classified an activity as a particular intensity; False Negatives (FN): Cases where the model failed to identify the actual intensity level.</p><p id="Par64">Using these components, we calculate our four key performance metrics, that is,</p><p id="Par65">Accuracy: Measures the overall correct predictions across all intensity levels</p><p id="Par66">
<disp-formula id="Equn"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Acc. = \frac{{Num.\,of\,correct\,predictions}}{{Total\,num.\,of\,predictions}}.$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equn.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par67">Precision: Indicates the model&#x02019;s ability to correctly identify positive cases for each intensity level</p><p id="Par68">
<disp-formula id="Equo"><alternatives><tex-math id="M26">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Precision = \frac{{TP}}{{TP + FP}}.$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equo.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par69">Recall: Measures the model&#x02019;s ability to detect all actual positive cases for each intensity level</p><p id="Par70">
<disp-formula id="Equp"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Recall = \frac{{TP}}{{TP + FN}}.$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equp.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par71">Area Under the Receiver Operating Characteristic Curve (AUC-ROC): Evaluates the model&#x02019;s ability to distinguish between classes across different classification thresholds<disp-formula id="Equq"><alternatives><tex-math id="M28">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{AUC}} = \int_0^1 {{\rm{TPR}}\left( {{\rm{FP}}{{\rm{R}}^{ - 1}}\left( {\rm{x}} \right)} \right){\rm{dx,}}}$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equq.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par72">where TPR is the True Positive Rate and FPR is the False Positive Rate</p><p id="Par73">F1 Score: Provides a balanced measure of precision and recall</p><p id="Par74">
<disp-formula id="Equr"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F1{\mkern 1mu} Score = 2 \times {\mkern 1mu} {{Precision \times Recall} \over {Precision + Recall}}.$$\end{document}</tex-math><graphic xlink:href="42490_2025_88_Article_Equr.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par75">where <inline-formula id="IEq12"><alternatives><tex-math id="M30">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p_o}$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq12.gif"/></alternatives></inline-formula> is the observed agreement and <inline-formula id="IEq13"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p_e}$$\end{document}</tex-math><inline-graphic xlink:href="42490_2025_88_Article_IEq13.gif"/></alternatives></inline-formula>. is the expected agreement by chance.</p><p id="Par76">To visualise the classification performance, confusion matrix is generated for the different activity intensity levels. In these confusion metrics, diagonal elements represent correct predictions where the model&#x02019;s output matches the true activity intensity and off-diagonal elements indicate misclassifications where the model&#x02019;s prediction differs from the true intensity.</p><p id="Par77">Additionally, we calculated per-class accuracy to provide detailed insights into the model&#x02019;s performance for each activity category. These metrics collectively facilitate a comprehensive evaluation of the model&#x02019;s predictive accuracy and reliability, ensuring robust performance across various levels of physical activity intensity.</p></sec><sec id="Sec18"><title>Statistical testing</title><p id="Par78">To assess differences in model accuracy across time windows and physical activity intensities, we employed a two-way analysis of variance (ANOVA), which examines the influence of two independent factors (TW and PAI) and their interaction on a dependent variable (model accuracy). As we hypothesise that the noise levels associated with different intensities and time windows may have a minimal impact on the model&#x02019;s accuracy, particularly if the model demonstrates high robustness.</p><p id="Par79">The analysis of variance (ANOVA) was conducted to assess differences in model accuracy across physical activity intensities and temporal windows. The following statistical parameters were evaluated: F-value measures the ratio of variance between the groups to the variance within the groups, indicating whether differences between means are significant. Higher F-values suggest greater between-group differences relative to within-group variation. p-value is that statistical significance was set at <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05. This threshold was chosen following standard practice in machine learning and physical activity research.</p></sec></sec><sec id="Sec19"><title>Results</title><p id="Par80">As illustrated in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> overall flowchart shows. First, the process begins with data pre-processing where 3-axis acceleration gravity (time series) data is converted into GAF images using the ENMO method. which effectively captures the temporal correlations of the acceleration data. Second, these GAF images were processed using a ViT to extract meaningful features, that the ViT model segments the GAF images into patches, applies position embeddings, and encodes them using multi-head attention mechanisms to generate robust feature representations.</p><sec id="Sec20"><title>Model performance</title><p id="Par81">Figure <xref rid="Fig2" ref-type="fig">2</xref> compares the confusion matrices for different models: (a) ViT-BiLSTM (Gravity-based), (b) ViT-BiLSTM (METs-based), (c) CNN-BiLSTM, (d) ViT, (e) CNN, and (f) BiLSTM. The proposed ViT-BiLSTM model achieved excellent performance in classifying physical activity intensities compared to others. Specifically, the ViT-BiLSTM model achieved an overall accuracy of 99.63%, with per-class accuracies of 99.5% for LPA, 98.9% for MVPA, and 99.5% for SD. In contrast, the CNN-BiLSTM model reached an accuracy of 92.01%, the ViT model had an accuracy of 80.36%, the CNN model showed an accuracy of 74.57%, and the BiLSTM model attained an accuracy of 80.11%. (The Confusion Matrices 10 epochs for the comparison of different models with 30 TWs as shows in Supplementary material <xref rid="MOESM2" ref-type="media">2</xref>.)<fig id="Fig2"><label>Fig. 2</label><caption><p>Confusion matrices for comparison of accuracy of different models</p></caption><graphic xlink:href="42490_2025_88_Fig2_HTML" id="MO14"/></fig></p><p id="Par82">The receiver operating characteristic (ROC) curves and their corresponding Area Under the Curve (AUC-ROC) values demonstrate the model&#x02019;s strong discriminative ability across all activity intensities (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>). The model achieved excellent discrimination with AUC-ROC values of almost 1.0 for all physical activity intensitives. These high AUC-ROC values indicate the model&#x02019;s robust capability to distinguish between different activity intensities while maintaining low false positive rates. The ROC curves show particularly strong performance in the low false-positive rate region, suggesting the model maintains high precision even at strict classification thresholds.<fig id="Fig3"><label>Fig. 3</label><caption><p>Receiver Operating Characteristic (ROC) curves for different physical activity intensities with TW 15s</p></caption><graphic xlink:href="42490_2025_88_Fig3_HTML" id="MO15"/></fig></p><p id="Par83">To evaluate the model&#x02019;s robustness and reliability, we analysed its performance across different physical activity intensities (PAI) and temporal windows (TW) using both visual and statistical approaches. The distribution and consistency of accuracy scores are visualized through box plots in Figs.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> and <xref rid="Fig5" ref-type="fig">5</xref>, revealing distinct classification patterns across different conditions.<fig id="Fig4"><label>Fig. 4</label><caption><p>Box plot of model&#x02019;s accuracy across different pais</p></caption><graphic xlink:href="42490_2025_88_Fig4_HTML" id="MO16"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><p>Box plot of model&#x02019;s accuracy across different temporal window</p></caption><graphic xlink:href="42490_2025_88_Fig5_HTML" id="MO17"/></fig></p><p id="Par84">Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> demonstrates the model&#x02019;s performance across intensity levels (SD, LPA, and MVPA), while Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> illustrates performance variations across different temporal windows (1s, 5s, 10s, 15s, and 30s). The ANOVA results, presented in Supplementary Material <xref rid="MOESM3" ref-type="media">3</xref>, indicate that the model maintains consistent accuracy across both different temporal windows (F&#x02009;=&#x02009;0.52, <italic>p</italic>&#x02009;=&#x02009;0.72) and physical activity intensities (F&#x02009;=&#x02009;2.18, <italic>p</italic>&#x02009;=&#x02009;0.13), suggesting robust and stable performance regardless of these variations.</p><p id="Par85">Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> shows that the model achieves the highest and most consistent accuracy for SD, with minimal variance and no significant outliers. In contrast, the accuracy for MVPA exhibits a higher variance with notable outliers, indicating less consistency in predictions. LPA demonstrates intermediate performance, with lower variance compared to MVPA but slightly higher than SD.</p><p id="Par86">Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> illustrates the accuracy of the model across different epoch sizes: 1s, 5s, 10s, 15s, and 30s. The whiskers for 5s and 30s are relatively long, indicating greater variability in model predictions for these epoch sizes compared to others. Meanwhile, the average accuracy levels for 15s and 10s are higher, suggesting that the model is most stable when predicting at these epoch sizes. Additionally, the presence of outliers in 1s, 5s, 10s, and 15s may indicate slight inconsistencies in predictions.</p><p id="Par87">Furthermore, the loss curves were conducted to observe the model&#x02019;s performance based on the most optimal 15s TW for model predictions. Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> show the results for accuracy and loss curves analysis. The training accuracy (depicted by the orange line) shows a consistent increase from approximately 95% to approximately 100% as the epochs progress from 1 to 10. This indicates that the model is learning well and improving its performance on the training data. The validation accuracy (depicted by the blue line) also shows a steady increase from approximately 96 to 100%, indicating strong generalisation to unseen data. The training loss (depicted by the blue line) decreases sharply from about 9 to nearly 0 as the epochs progress. This rapid decline suggests that the model is quickly learning and minimizing errors on the training data. The validation loss (depicted by the orange line) shows a gradual decrease from around 1 to near 0, indicating a steady improvement in model performance on the validation data. The relatively small and stable loss values demonstrate that the model is not overfitting. These curves show that the model performs well during training and maintains good generalisation on validation data, as indicated by the close alignment between training and validation accuracy and the decreasing loss values. The model&#x02019;s consistent performance across epochs highlights its robustness and effectiveness in learning the underlying patterns in the data. Supplementary materials <xref rid="MOESM4" ref-type="media">4</xref> shows accuracy and loss Curves for different TWs, which are also stability across epochs.<fig id="Fig6"><label>Fig. 6</label><caption><p>Accuracy and loss curves in the training-validation process</p></caption><graphic xlink:href="42490_2025_88_Fig6_HTML" id="MO18"/></fig></p><p id="Par88">Finally, from the results in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, it can be observed how different intensities and TWs affect the model&#x02019;s accuracy. The model&#x02019;s performance varies with different TWs used for generating images. Specifically, the 15s TW shows the best performance, achieving the highest train (0.99&#x02009;&#x000b1;&#x02009;0.01) and test (0.99&#x02009;&#x000b1;&#x02009;0.01) accuracies. In contrast, the 5s TW exhibits the lowest train (0.981&#x02009;&#x000b1;&#x02009;0.01) and test (0.981&#x02009;&#x000b1;&#x02009;0.01) accuracies. When considering different PAIs across various TWs, the model consistently achieves higher accuracy in predicting SD (0.989&#x02009;&#x000b1;&#x02009;0.01) compared to LPA (0.982&#x02009;&#x000b1;&#x02009;0.02) and MVPA (0.982&#x02009;&#x000b1;&#x02009;0.03). This trend is apparent across all TWs, indicating that SD is easier for the model to predict accurately. In shorter epochs like 1s or 5s, the model shows better prediction accuracy for MVPA (0.992&#x02009;&#x000b1;&#x02009;0.007; 0.985&#x02009;&#x000b1;&#x02009;0.01) compared to other intensities. However, in longer epochs such as 30s, the model achieves the lowest prediction accuracy for MVPA (0.965&#x02009;&#x000b1;&#x02009;0.05), but higher for LPA (0.984&#x02009;&#x000b1;&#x02009;0.01), and highest for SD (0.995&#x02009;&#x000b1;&#x02009;0.007). The 1s and 15s epochs demonstrated markedly higher standard deviations in test loss (SD&#x02009;=&#x02009;0.20 and SD&#x02009;=&#x02009;0.21, respectively) compared to other temporal windows (5s: SD&#x02009;=&#x02009;0.007; 10s: SD&#x02009;=&#x02009;0.006; 30s: SD&#x02009;=&#x02009;0.006). Similarly, training loss values showed substantial variation, with the 1s and 15s epochs exhibiting higher standard deviations (SD&#x02009;=&#x02009;2.8 and SD&#x02009;=&#x02009;2.91) compared to other epochs (5s: SD&#x02009;=&#x02009;0.19; 10s: SD&#x02009;=&#x02009;0.18; 30s: SD&#x02009;=&#x02009;0.17). This pattern suggests that while these temporal windows achieved high accuracy (1s: 0.987&#x02009;&#x000b1;&#x02009;1.5; 15s: 0.99&#x02009;&#x000b1;&#x02009;0.01), they also experienced greater fluctuations in model performance during training. The increased variability might be attributed to two-folds: (1) the challenge of capturing complete activity patterns in very short time segments (1s), leading to more unstable predictions; (2) add the explanation for 15s time window. These findings highlight an important trade-off between prediction accuracy and stability across different temporal windows, suggesting that window selection should consider both performance metrics and consistency requirements for specific applications.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Summary of model performance across different TWs and physical activity intensities</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Epoch (training times)</th><th align="left">Train accuracy</th><th align="left">Test accuracy</th><th align="left">Train loss</th><th align="left">Test loss</th></tr><tr><th align="left">Mean (SD)</th><th align="left">Mean (SD)</th><th align="left">Mean (SD)</th><th align="left">Mean (SD)</th></tr></thead><tbody><tr><td align="left">Total (<italic>n</italic>&#x02009;=&#x02009;50)</td><td align="left">0.984 (0.17)</td><td align="left">0.985 (1.48)</td><td align="left">0.096 (0.2)</td><td align="left">0.088 (0.16)</td></tr><tr><td align="left">LPA</td><td align="left"/><td align="left">0.982 (0.02)</td><td align="left"/><td align="left"/></tr><tr><td align="left">MVPA</td><td align="left"/><td align="left">0.982 (0.02)</td><td align="left"/><td align="left"/></tr><tr><td align="left">SD</td><td align="left"/><td align="left">0.989 (0.01)</td><td align="left"/><td align="left"/></tr><tr><td align="left">1s epoch (<italic>n</italic>&#x02009;=&#x02009;10)</td><td align="left">0.987 (0.01)</td><td align="left">0.987 (1.5)</td><td align="left">0.2 (0.28)</td><td align="left">0.194 (0.20)</td></tr><tr><td align="left">LPA</td><td align="left"/><td align="left">0.981 (0.03)</td><td align="left"/><td align="left"/></tr><tr><td align="left">MVPA</td><td align="left"/><td align="left">0.992 (0.007)</td><td align="left"/><td align="left"/></tr><tr><td align="left">SD</td><td align="left"/><td align="left">0.990 (0.11)</td><td align="left"/><td align="left"/></tr><tr><td align="left">5s epoch (<italic>n</italic>&#x02009;=&#x02009;10)</td><td align="left">0.981 (0.01)</td><td align="left">0.981 (0.01)</td><td align="left">0.014 (0.019)</td><td align="left">0.007 (0.007)</td></tr><tr><td align="left">LPA</td><td align="left"/><td align="left">0.977 (0.02)</td><td align="left"/><td align="left"/></tr><tr><td align="left">MVPA</td><td align="left"/><td align="left">0.985 (0.01)</td><td align="left"/><td align="left"/></tr><tr><td align="left">SD</td><td align="left"/><td align="left">0.982 (0.01)</td><td align="left"/><td align="left"/></tr><tr><td align="left">10s epoch (<italic>n</italic>&#x02009;=&#x02009;10)</td><td align="left">0.982 (0.02)</td><td align="left">0.982 (0.02)</td><td align="left">0.0131 (0.018)</td><td align="left">0.006 (0.006)</td></tr><tr><td align="left">LPA</td><td align="left"/><td align="left">0.978 (0.03)</td><td align="left"/><td align="left"/></tr><tr><td align="left">MVPA</td><td align="left"/><td align="left">0.981 (0.03)</td><td align="left"/><td align="left"/></tr><tr><td align="left">SD</td><td align="left"/><td align="left">0.988 (0.01)</td><td align="left"/><td align="left"/></tr><tr><td align="left">15s epoch (<italic>n</italic>&#x02009;=&#x02009;10)</td><td align="left">0.99 (0.01)</td><td align="left">0.99 (0.01)</td><td align="left">0.23 (0.29)</td><td align="left">0.228 (0.21)</td></tr><tr><td align="left">LPA</td><td align="left"/><td align="left">0.99 (0.006)</td><td align="left"/><td align="left"/></tr><tr><td align="left">MVPA</td><td align="left"/><td align="left">0.987 (0.02)</td><td align="left"/><td align="left"/></tr><tr><td align="left">SD</td><td align="left"/><td align="left">0.992 (0.009)</td><td align="left"/><td align="left"/></tr><tr><td align="left">30s epoch (<italic>n</italic>&#x02009;=&#x02009;10)</td><td align="left">0.981 (0.02)</td><td align="left">0.986 (1.39)</td><td align="left">0.014 (0.017)</td><td align="left">0.007 (0.006)</td></tr><tr><td align="left">LPA</td><td align="left"/><td align="left">0.984 (0.01)</td><td align="left"/><td align="left"/></tr><tr><td align="left">MVPA</td><td align="left"/><td align="left">0.965 (0.05)</td><td align="left"/><td align="left"/></tr><tr><td align="left">SD</td><td align="left"/><td align="left">0.995 (0.007)</td><td align="left"/><td align="left"/></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec21"><title>Discussion</title><p id="Par89">This study innovatively combines ViT and BiLSTM to predict PAI using gravity-based acceleration to generate images. It is also the first to consider the impact of different intensities and TWs on model robustness. Additionally, this study investigates how PAI and TW impact the accuracy of the ViT-BiLSTM model. The present study demonstrates that: (1) the ViT-BiLSTM model exhibits high performance in predicting PAI; (2) the study confirms the feasibility of using gravity-based acceleration for intensity classification tasks. The gravity-based calculation of PAI significantly enhances model accuracy compared to traditional MET-based methods; (3) the model&#x02019;s high performance reveals good robustness and reliability, unaffected by variations in intensity and TWs; and the model consistently improves its performance across epochs, with both training and validation accuracy increasing to near 100%, and training and validation loss decreasing to nearly zero in accuracy and loss curve analyses. (4) the TW and PAI are the potential factors that contribute to the model&#x02019;s accuracy.</p><p id="Par90">The present study applied the hybrid model of ViT-BiLSTM in classifying LPA, MVPA, SD achieving an overall accuracy exceeding 99.6% (gravity-based) and 96.9% (METs-based) using a 30-s TW, which is higher than other previous models [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR15">15</xref>&#x02013;<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR32">32</xref>, <xref ref-type="bibr" rid="CR33">33</xref>]. The present results are directly comparable with those reported in 8 published studies. Three of these studies used traditional machine learning methods on the same dataset (Capture-24) we used, obtaining overall accuracies of 80% [<xref ref-type="bibr" rid="CR20">20</xref>], 88% [<xref ref-type="bibr" rid="CR33">33</xref>] and 87% [<xref ref-type="bibr" rid="CR32">32</xref>]. This may be due to traditional machine learning models relying on linear regression, which have limitations in predicting the complex variations in intensity during PA. The other five studies were based on deep learning methods, with four of them using laboratory-collected accelerometer data. In these five studies, one study applied ANN and k&#x02013;NN models, resulting in 92% and 80% accuracy, respectively [<xref ref-type="bibr" rid="CR19">19</xref>]. Two studies used CNN models: one study with data from five accelerometers showed a range of 92%&#x02013;98% accuracy [<xref ref-type="bibr" rid="CR21">21</xref>]; while another study achieved accuracies of 63%, 84.2%, and 85.4% for MVPA, LPA, and SD, respectively [<xref ref-type="bibr" rid="CR16">16</xref>], Additionally, a study implementing a BiLSTM model with data from three accelerometers achieved 90% accuracy [<xref ref-type="bibr" rid="CR9">9</xref>]. Considering only a single model framework and a single data dimension may limit the model&#x02019;s ability to accurately assess PAI, as ANN, k&#x02013;NN, and CNNs alone may not handle time series data effectively, Moreover, BiLSTM models without a preceding convolutional layer cannot extract spatial features.</p><p id="Par91">Furthermore, compared to a recent study by Farrahi, Muhammad [<xref ref-type="bibr" rid="CR15">15</xref>], which used the AccNet24 model and achieved accuracies of 98.6% for SD, 95.6% for LPA, and 94.7% for MVPA using METs-based PAI on the Capture-24 dataset with a 30-s TW window, our ViT-BiLSTM model, also based on METs for intensity classification and using the same dataset and TW, achieved higher accuracies of 98.2% for SD, 96% for LPA, and 96.3% for MVPA. This may because the ViT-BiLSTM model may provide the global information and utilises attention mechanisms, excelling at capturing the magnitude of intensity under variable visual conditions with the ViT model, while the integration with the BiLSTM model enhances its ability to accurately capture dynamic physical activities. Additionally, when our ViT-BiLSTM model, based on gravity-based acceleration for intensity classification and using the same dataset and TW, achieved higher accuracies of 99.9% for SD, 99.5% for LPA, and 99.6% for MVPA, this may confirm the feasibility of using gravity-based acceleration for intensity classification tasks. The gravity-based calculation of PAI enhances model accuracy compared to traditional MET-based methods. This may be because the present study used the gravity-based acceleration to classify images for model training. Gravity-based acceleration, to some extent, surpasses the METs-based method, reducing the likelihood of misclassification for PAI during image generation.</p><p id="Par92">Meanwhile, the ViT-BiLSTM model demonstrated excellent robustness and generalisation. ANOVA showed no accuracy variation across PAIs (F&#x02009;=&#x02009;2.18, <italic>p</italic>&#x02009;=&#x02009;0.13) and TWs (F&#x02009;=&#x02009;0.52, <italic>p</italic>&#x02009;=&#x02009;0.72). The ViT-BiLSTM model consistently improved its performance across epochs, with both training and validation accuracy increasing to nearly 100%, and training and validation loss decreasing to nearly zero in accuracy and loss curves analysis. Again, the present study observed that different models exhibited similar trends when predicting PAI, SD is relatively easier to predict, likely due to the stable nature of sedentary behaviour. In contrast, the prediction accuracy for LPA and MVPA is more likely lower than for SD, confirming the complexity of LPA and MVPA behaviours [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR21">21</xref>], However, the ViT-BiLSTM model can almost overcome the variations between behaviours, achieving nearly perfect accuracy in predicting different PAIs. The present study tested the variation in the model&#x02019;s accuracy across different PAIs, The ViT-BiLSTM model results for predicting SD, LPA, and MVPA were 99.9%, 99.5%, and 99.6%, respectively.</p><p id="Par93">PA characteristics may be a potential factor affecting the performance of the model and should be taken into consideration. Previous studies have also reported similar findings. Nawaratne, Alahakoon [<xref ref-type="bibr" rid="CR16">16</xref>] found that SB achieved correct predictions of 85.4%, LPA achieved correct predictions of 84.2%, and MVPA achieved correct predictions of 63.1% based on a 60-s TW. In contrast, Widianto, Sugiarto [<xref ref-type="bibr" rid="CR21">21</xref>] showed accuracies of 98%, 95%, and 97% for SD, LPA, and MVPA, respectively, based on a 1-s TW. Also, Recent studies have reported similar findings, showing that a 1-s window performs best in predicting any intensity level. This is mainly because a 1-s window calculates acceleration more accurately, reducing the likelihood of values being averaged out in images. Additionally, for predicting LPA and SD, longer windows may provide higher accuracy, while shorter windows are more effective in predicting MVPA compared to LPA, as MVPA usually involves a amount of activity expenditure over a short period [<xref ref-type="bibr" rid="CR34">34</xref>], Consequently, MVPA often lasts only a few seconds in adults, especially middle-aged or older individuals than LPA and SD [<xref ref-type="bibr" rid="CR35">35</xref>]. However, using longer windows may introduce noise to the model due to greater variations in MVPA magnitude, which can affect the model&#x02019;s accuracy during data preprocessing. Although the impact of this factor on the model in this study was very slight, considering the characteristics of adult PA behaviour may help overcome some noise effects in future model development in this area. Additionally, Outliers mainly refer to the lowest values observed during the first epoch of the model, with a small difference of approximately 2&#x02013;5% compared to the average accuracy, which indirectly demonstrates the good convergence performance of the model.</p><p id="Par94">Strength of the present study demonstrates several notable strengths in its approach to PAI classification. It innovatively combines ViT and BiLSTM models, fully leveraging the advantages of both architectures to enhance classification accuracy. The use of gravity-based acceleration methods for classifying PAI marks a significant improvement over traditional METs-based approaches, potentially reducing classification errors. This study considers different TWs and PAIs, providing a nuanced understanding of how these factors impact model performance. By utilising the real-world Capture-24 dataset, the study enhances the practical applicability of its findings. Furthermore, the rigorous evaluation of model performance and stability through multiple methods, including ANOVA, confusion matrices, and accuracy and loss curves, underscores the study&#x02019;s methodological robustness. These strengths collectively contribute to the study&#x02019;s advancement in the field of PA monitoring and classification using deep learning techniques. In this study, the ViT-BiLSTM model demonstrated high accuracy in classifying physical activity intensities, Future efforts could focus on optimizing the model for such environments by leveraging techniques like model pruning, which reduces unnecessary parameters, and quantization, which decreases the precision of weights to lower memory usage and computational demands. Additionally, lightweight architectures could be explored as alternatives to the current design. These strategies would maintain model performance while enabling its application in real-world scenarios. A limitation of the present study is that it focused solely on adult populations and used a single intensity threshold to calculate activity intensity. Future work should include multiple threshold comparisons to evaluate the model&#x02019;s robustness under diverse conditions.</p></sec><sec id="Sec22"><title>Conclusion</title><p id="Par95">The ViT-BiLSTM model proposed in this study demonstrated exceptional performance in classifying PAI using gravity-based acceleration data, achieving an overall accuracy of 99.63%. The model exhibited excellent robustness and reliability across different TWs and activity intensities. The research found that a 15-s TW yielded the best performance in most cases, and the model&#x02019;s accuracy in predicting sedentary behaviour was slightly higher than for light and moderate-to-vigorous activities. These findings provide important methodological references for future PA monitoring and classification research. Future study should focus on validating these findings across additional datasets using different PA thresholds and exploring the model&#x02019;s performance with a broader range of physical activities to further enhance its practical applications.</p></sec><sec id="Sec29" sec-type="supplementary-material"><title>Electronic supplementary material</title><p>Below is the link to the electronic supplementary material.</p><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="42490_2025_88_MOESM1_ESM.docx"><caption><p>Supplementary Material 1</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="42490_2025_88_MOESM2_ESM.docx"><caption><p>Supplementary Material 2</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="42490_2025_88_MOESM3_ESM.docx"><caption><p>Supplementary Material 3</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM4"><media xlink:href="42490_2025_88_MOESM4_ESM.docx"><caption><p>Supplementary Material 4</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM5"><media xlink:href="42490_2025_88_MOESM5_ESM.docx"><caption><p>Supplementary Material 5</p></caption></media></supplementary-material>
</p></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>ViT</term><def><p id="Par5">Vision Transformer</p></def></def-item><def-item><term>BiLSTM</term><def><p id="Par6">Bidirectional Long Short-Term Memory</p></def></def-item><def-item><term>PAI</term><def><p id="Par7">Physical Activity Intensity</p></def></def-item><def-item><term>TW</term><def><p id="Par8">Temporal Window</p></def></def-item><def-item><term>CNN</term><def><p id="Par9">Convolutional Neural Network</p></def></def-item><def-item><term>RNN</term><def><p id="Par10">Recurrent Neural Network</p></def></def-item><def-item><term>ENMO</term><def><p id="Par11">Euclidean Norm Minus One</p></def></def-item><def-item><term>GAF</term><def><p id="Par12">Gramian Angular Field</p></def></def-item><def-item><term>LPA</term><def><p id="Par13">Light Physical Activity</p></def></def-item><def-item><term>MVPA</term><def><p id="Par14">Moderate-to-Vigorous Physical Activity</p></def></def-item><def-item><term>SB</term><def><p id="Par15">Sedentary Behaviour</p></def></def-item><def-item><term>METs</term><def><p id="Par16">Metabolic Equivalents</p></def></def-item><def-item><term>ANOVA</term><def><p id="Par17">Analysis of Variance</p></def></def-item></def-list></glossary><fn-group><fn><p><bold>Publisher&#x02019;s Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>I would like to thank the China Scholarship Council for providing the scholarship funding.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>L.W. was responsible for the majority of the work, including the development and implementation of the ViT-BiLSTM model, data preprocessing, the generation of GAF images, and writing the entire manuscript. Z.L. primarily handled the statistical analysis and data computation, ensuring the accuracy and reliability of the results. T.Z. was mainly responsible for revising the manuscript and discussing the model concept. All authors reviewed and approved the final manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>No funds, grants, or other support was received.</p></notes><notes><title>Declarations</title><notes><title>Ethics approval and consent to participate</title><p id="Par96">Not applicable, this study used the open dataset Capture-24, which contains Axivity AX3 wrist-worn activity tracker data collected from 151 participants in the Oxfordshire area during 2014-2016. The the link of the dataset in the data availability: <ext-link ext-link-type="uri" xlink:href="https://ora.ox.ac.uk/objects/uuid:99d7c092-d865-4a19-b096-cc16440cd001">https://ora.ox.ac.uk/objects/uuid:99d7c092-d865-4a19-b096-cc16440cd001</ext-link>. For detailed data collection protocols, see the following references: Gershuny et al. (2020), Sociological Methodology, doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0081175019884591">10.1177/0081175019884591</ext-link>; Willetts et al. (2018), Scientific Reports, 8(1):7961.</p></notes><notes><title>Consent for publication</title><p id="Par97">Not applicable.</p></notes><notes notes-type="COI-statement"><title>Competing interests</title><p id="Par98">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Pate</surname><given-names>RR</given-names></name></person-group><article-title>Physical activity assessment in children and adolescents</article-title><source>Crit Rev Food Sci Nutr</source><year>1993</year><volume>33</volume><issue>4-5</issue><fpage>321</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1080/10408399309527627</pub-id><pub-id pub-id-type="pmid">8357491</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Pate RR. Physical activity assessment in children and adolescents. Crit Rev Food Sci Nutr. 1993;33(4-5):321&#x02013;26.<pub-id pub-id-type="pmid">8357491</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Rowlands</surname><given-names>AV</given-names></name></person-group><article-title>Accelerometer assessment of physical activity in children: an update</article-title><source>Pediatr Exerc Sci</source><year>2007</year><volume>19</volume><issue>3</issue><fpage>252</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1123/pes.19.3.252</pub-id><pub-id pub-id-type="pmid">18019585</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Rowlands AV. Accelerometer assessment of physical activity in children: an update. Pediatr Exerc Sci. 2007;19(3):252&#x02013;66.<pub-id pub-id-type="pmid">18019585</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Rowlands</surname><given-names>AV</given-names></name></person-group><article-title>Moving forward with accelerometer-assessed physical activity: two strategies to ensure meaningful, interpretable, and comparable measures</article-title><source>Pediatr Exerc Sci</source><year>2018</year><volume>30</volume><issue>4</issue><fpage>450</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1123/pes.2018-0201</pub-id><pub-id pub-id-type="pmid">30304982</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Rowlands AV. Moving forward with accelerometer-assessed physical activity: two strategies to ensure meaningful, interpretable, and comparable measures. Pediatr Exerc Sci. 2018;30(4):450&#x02013;56.<pub-id pub-id-type="pmid">30304982</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Rowlands</surname><given-names>AV</given-names></name><etal/></person-group><article-title>Beyond cut points: accelerometer metrics that capture the physical activity profile</article-title><source>Med Sci Sports Exerc</source><year>2018</year><volume>50</volume><fpage>1323</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1249/MSS.0000000000001561</pub-id><pub-id pub-id-type="pmid">29360664</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Rowlands AV, et al. Beyond cut points: accelerometer metrics that capture the physical activity profile. Med Sci Sports Exerc 2018;50:1323&#x02013;32.<pub-id pub-id-type="pmid">29360664</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Fairclough</surname><given-names>SJ</given-names></name><etal/></person-group><article-title>Average acceleration and intensity gradient of primary school children and associations with indicators of health and well-being</article-title><source>J Sports Sci</source><year>2019</year><volume>37</volume><fpage>2159-2167</fpage><pub-id pub-id-type="doi">10.1080/02640414.2019.1624313</pub-id><pub-id pub-id-type="pmid">31156048</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Fairclough SJ, et al. Average acceleration and intensity gradient of primary school children and associations with indicators of health and well-being. J Sports Sci 2019;37:2159-2167.<pub-id pub-id-type="pmid">31156048</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmadi</surname><given-names>MN</given-names></name><etal/></person-group><article-title>Brief bouts of device-measured intermittent lifestyle physical activity and its association with major adverse cardiovascular events and mortality in people who do not exercise: a prospective cohort study</article-title><source>Lancet Public Health</source><year>2023</year><volume>8</volume><fpage>e800</fpage><lpage>e810</lpage><pub-id pub-id-type="doi">10.1016/S2468-2667(23)00183-4</pub-id><pub-id pub-id-type="pmid">37777289</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Ahmadi MN, et al. Brief bouts of device-measured intermittent lifestyle physical activity and its association with major adverse cardiovascular events and mortality in people who do not exercise: a prospective cohort study. Lancet Public Health 2023;8:e800&#x02013;e810.<pub-id pub-id-type="pmid">37777289</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Marshall</surname><given-names>Z</given-names></name><name><surname>Mackintosh</surname><given-names>K</given-names></name><name><surname>Gregory</surname><given-names>J</given-names></name><name><surname>McNarry</surname><given-names>M</given-names></name></person-group><article-title>Using compositional analysis to explore the relationship between physical activity and cardiovascular health in children and adolescents with and without type 1 diabetes</article-title><source>Pediatr Diabetes</source><year>2022</year><volume>23</volume><issue>1</issue><fpage>115</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1111/pedi.13288</pub-id><pub-id pub-id-type="pmid">34780103</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Marshall Z, Mackintosh K, Gregory J, McNarry M. Using compositional analysis to explore the relationship between physical activity and cardiovascular health in children and adolescents with and without type 1 diabetes. Pediatr Diabetes. 2022;23(1):115&#x02013;25.<pub-id pub-id-type="pmid">34780103</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Migueles</surname><given-names>JH</given-names></name><etal/></person-group><article-title>GRANADA consensus on analytical approaches to assess associations with accelerometer-determined physical behaviours (physical activity, sedentary behaviour and sleep) in epidemiological studies</article-title><source>Br J Sports Med</source><year>2022</year><volume>56</volume><fpage>376</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1136/bjsports-2020-103604</pub-id><pub-id pub-id-type="pmid">33846158</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Migueles JH, et al. GRANADA consensus on analytical approaches to assess associations with accelerometer-determined physical behaviours (physical activity, sedentary behaviour and sleep) in epidemiological studies. Br J Sports Med 2022;56:376&#x02013;84.<pub-id pub-id-type="pmid">33846158</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Ng</surname><given-names>JYY</given-names></name><etal/></person-group><article-title>Development of a multi-wear-site, deep learning-based physical activity intensity classification algorithm using raw acceleration data</article-title><source>PLoS One</source><year>2024</year><volume>19</volume><fpage>e0299295</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0299295</pub-id><pub-id pub-id-type="pmid">38452147</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Ng JYY, et al. Development of a multi-wear-site, deep learning-based physical activity intensity classification algorithm using raw acceleration data. PLoS One 2024;19:e0299295.<pub-id pub-id-type="pmid">38452147</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>H</given-names></name><etal/></person-group><article-title>Self-supervised learning of accelerometer data provides new insights for sleep and its association with mortality</article-title><source>NPJ Digit Med</source><year>2024</year><volume>7</volume><fpage>86</fpage><pub-id pub-id-type="doi">10.1038/s41746-024-01065-0</pub-id><pub-id pub-id-type="pmid">38769347</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Yuan H, et al. Self-supervised learning of accelerometer data provides new insights for sleep and its association with mortality. NPJ Digit Med 2024;7:86.<pub-id pub-id-type="pmid">38769347</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Crouter</surname><given-names>SE</given-names></name><name><surname>Clowers</surname><given-names>KG</given-names></name><name><surname>Bassett</surname><given-names>DR</given-names><suffix>Jr</suffix></name></person-group><article-title>A novel method for using accelerometer data to predict energy expenditure</article-title><source>J Appl Physiol (1985)</source><year>2006</year><volume>100</volume><issue>4</issue><fpage>1324</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1152/japplphysiol.00818.2005</pub-id><pub-id pub-id-type="pmid">16322367</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Crouter SE, Clowers KG, Bassett Jr DR. A novel method for using accelerometer data to predict energy expenditure. J Appl Physiol (1985). 2006;100(4):1324&#x02013;31.<pub-id pub-id-type="pmid">16322367</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Freedson</surname><given-names>PS</given-names></name><name><surname>Melanson</surname><given-names>E</given-names></name><name><surname>Sirard</surname><given-names>J</given-names></name></person-group><article-title>Calibration of the computer science and applications, inc. accelerometer</article-title><source>Med Sci Sports Exerc</source><year>1998</year><volume>30</volume><issue>5</issue><fpage>777</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1097/00005768-199805000-00021</pub-id><pub-id pub-id-type="pmid">9588623</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Freedson PS, Melanson E, Sirard J. Calibration of the computer science and applications, inc. accelerometer. Med Sci Sports Exerc. 1998;30(5):777&#x02013;81.<pub-id pub-id-type="pmid">9588623</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Boyer</surname><given-names>WR</given-names></name></person-group><article-title>Accelerometer-derived total activity counts, bouted minutes of moderate to vigorous activity, and insulin resistance: NHANES 2003&#x02013;2006</article-title><source>Prev Chronic Dis</source><year>2016</year><volume>13</volume><fpage>E146</fpage><pub-id pub-id-type="doi">10.5888/pcd13.160159</pub-id><pub-id pub-id-type="pmid">27763832</pub-id>
</element-citation><mixed-citation id="mc-CR13" publication-type="journal">Boyer WR. Accelerometer-derived total activity counts, bouted minutes of moderate to vigorous activity, and insulin resistance: NHANES 2003&#x02013;2006. Prev Chronic Dis. 2016;13:E146.<pub-id pub-id-type="pmid">27763832</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Rowlands</surname><given-names>AV</given-names></name><etal/></person-group><article-title>A data-driven, meaningful, easy to interpret, standardised accelerometer outcome variable for global surveillance</article-title><source>J Sci Med Sport</source><year>2019</year><volume>22</volume><fpage>1132</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/j.jsams.2019.06.016</pub-id><pub-id pub-id-type="pmid">31288983</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Rowlands AV, et al. A data-driven, meaningful, easy to interpret, standardised accelerometer outcome variable for global surveillance. J Sci Med Sport 2019;22:1132&#x02013;38.<pub-id pub-id-type="pmid">31288983</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Farrahi</surname><given-names>V</given-names></name><name><surname>Muhammad</surname><given-names>U</given-names></name><name><surname>Rostami</surname><given-names>M</given-names></name><name><surname>Oussalah</surname><given-names>M</given-names></name></person-group><article-title>AccNet24: a deep learning framework for classifying 24-hour activity behaviours from wrist-worn accelerometer data under free-living environments</article-title><source>Int J Med Inform</source><year>2023</year><volume>172</volume><fpage>105004</fpage><pub-id pub-id-type="doi">10.1016/j.ijmedinf.2023.105004</pub-id><pub-id pub-id-type="pmid">36724729</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Farrahi V, Muhammad U, Rostami M, Oussalah M. AccNet24: a deep learning framework for classifying 24-hour activity behaviours from wrist-worn accelerometer data under free-living environments. Int J Med Inform. 2023;172:105004.<pub-id pub-id-type="pmid">36724729</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Nawaratne</surname><given-names>R</given-names></name><etal/></person-group><article-title>Deep learning to predict energy expenditure and activity intensity in free living conditions using wrist-specific accelerometry</article-title><source>J Sports Sci</source><year>2021</year><volume>39</volume><fpage>683</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1080/02640414.2020.1841394</pub-id><pub-id pub-id-type="pmid">33121379</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Nawaratne R, et al. Deep learning to predict energy expenditure and activity intensity in free living conditions using wrist-specific accelerometry. J Sports Sci 2021;39:683&#x02013;90.<pub-id pub-id-type="pmid">33121379</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Thornton</surname><given-names>CB</given-names></name><name><surname>Kolehmainen</surname><given-names>N</given-names></name><name><surname>Nazarpour</surname><given-names>K</given-names></name></person-group><article-title>Using unsupervised machine learning to quantify physical activity from accelerometry in a diverse and rapidly changing population</article-title><source>PLOS Digit Health</source><year>2023</year><volume>2</volume><issue>4</issue><fpage>e0000220</fpage><pub-id pub-id-type="doi">10.1371/journal.pdig.0000220</pub-id><pub-id pub-id-type="pmid">37018183</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Thornton CB, Kolehmainen N, Nazarpour K. Using unsupervised machine learning to quantify physical activity from accelerometry in a diverse and rapidly changing population. PLOS Digit Health. 2023;2(4):e0000220.<pub-id pub-id-type="pmid">37018183</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>van Kuppevelt</surname><given-names>D</given-names></name><etal/></person-group><article-title>Segmenting accelerometer data from daily life with unsupervised machine learning</article-title><source>PLoS One</source><year>2019</year><volume>14</volume><fpage>e0208692</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0208692</pub-id><pub-id pub-id-type="pmid">30625153</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">van Kuppevelt D, et al. Segmenting accelerometer data from daily life with unsupervised machine learning. PLoS One 2019;14:e0208692.<pub-id pub-id-type="pmid">30625153</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Muazu Musa</surname><given-names>R</given-names></name><etal/></person-group><article-title>The application of Artificial Neural Network and k-Nearest Neighbour classification models in the scouting of high-performance archers from a selected fitness and motor skill performance parameters</article-title><source>Sci Sports</source><year>2019</year><volume>34</volume><fpage>e241</fpage><lpage>e249</lpage><pub-id pub-id-type="doi">10.1016/j.scispo.2019.02.006</pub-id></element-citation><mixed-citation id="mc-CR19" publication-type="journal">Muazu Musa R, et al. The application of Artificial Neural Network and k-Nearest Neighbour classification models in the scouting of high-performance archers from a selected fitness and motor skill performance parameters. Sci Sports 2019;34:e241&#x02013;e249.</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>H</given-names></name><etal/></person-group><article-title>Self-supervised learning for human activity recognition using 700,000 person-days of wearable data</article-title><source>NPJ Digit Med</source><year>2024</year><volume>7</volume><fpage>91</fpage><pub-id pub-id-type="doi">10.1038/s41746-024-01062-3</pub-id><pub-id pub-id-type="pmid">38609437</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">Yuan H, et al. Self-supervised learning for human activity recognition using 700,000 person-days of wearable data. NPJ Digit Med 2024;7:91.<pub-id pub-id-type="pmid">38609437</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="book"><person-group person-group-type="author"><name><surname>Widianto</surname><given-names>A</given-names></name><etal/></person-group><source>Physical Activity Intensity Classification Using a Convolutional Neural Network and Wearable Accelerometer</source><year>2019</year></element-citation><mixed-citation id="mc-CR21" publication-type="book">Widianto A, et al. Physical Activity Intensity Classification Using a Convolutional Neural Network and Wearable Accelerometer. 2019.</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Chong</surname><given-names>J</given-names></name><etal/></person-group><article-title>Machine-learning models for activity class prediction: a comparative study of feature selection and classification algorithms</article-title><source>Gait Posture</source><year>2021</year><volume>89</volume><fpage>45</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1016/j.gaitpost.2021.06.017</pub-id><pub-id pub-id-type="pmid">34225240</pub-id>
</element-citation><mixed-citation id="mc-CR22" publication-type="journal">Chong J, et al. Machine-learning models for activity class prediction: a comparative study of feature selection and classification algorithms. Gait Posture. 2021;89:45&#x02013;53.<pub-id pub-id-type="pmid">34225240</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>KS</given-names></name><name><surname>Lim</surname><given-names>KM</given-names></name><name><surname>Lee</surname><given-names>CP</given-names></name><name><surname>Kwek</surname><given-names>LC</given-names></name></person-group><article-title>Bidirectional long short-term memory with temporal dense sampling for human action recognition</article-title><source>Expert Syst Appl</source><year>2022</year><volume>210</volume><fpage>118484</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2022.118484</pub-id></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Tan KS, Lim KM, Lee CP, Kwek LC. Bidirectional long short-term memory with temporal dense sampling for human action recognition. Expert Syst Appl. 2022;210:118484.</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Image expression of time series data of wearable IMU sensor and fusion classification of gymnastics action</article-title><source>Expert Syst Appl</source><year>2024</year><volume>238</volume><fpage>121978</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2023.121978</pub-id></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Zhao Y, et al. Image expression of time series data of wearable IMU sensor and fusion classification of gymnastics action. Expert Syst Appl. 2024;238:121978.</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Vela</surname><given-names>D</given-names></name><etal/></person-group><article-title>Temporal quality degradation in AI models</article-title><source>Sci Rep</source><year>2022</year><volume>12</volume><fpage>11654</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-15245-z</pub-id><pub-id pub-id-type="pmid">35803963</pub-id>
</element-citation><mixed-citation id="mc-CR25" publication-type="journal">Vela D, et al. Temporal quality degradation in AI models. Sci Rep 2022;12:11654.<pub-id pub-id-type="pmid">35803963</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Miatke</surname><given-names>A</given-names></name><etal/></person-group><article-title>The association between reallocations of time and health using compositional data analysis: a systematic scoping review with an interactive data exploration interface</article-title><source>Int J Behav Nutr Phys Act</source><year>2023</year><volume>20</volume><fpage>127</fpage><pub-id pub-id-type="doi">10.1186/s12966-023-01526-x</pub-id><pub-id pub-id-type="pmid">37858243</pub-id>
</element-citation><mixed-citation id="mc-CR26" publication-type="journal">Miatke A, et al. The association between reallocations of time and health using compositional data analysis: a systematic scoping review with an interactive data exploration interface. Int J Behav Nutr Phys Act 2023;20:127.<pub-id pub-id-type="pmid">37858243</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Rowlands</surname><given-names>AV</given-names></name><etal/></person-group><article-title>Enhancing the value of accelerometer-assessed physical activity: meaningful visual comparisons of data-driven translational accelerometer metrics</article-title><source>Sports Med Open</source><year>2019</year><volume>5</volume><fpage>47</fpage><pub-id pub-id-type="doi">10.1186/s40798-019-0225-9</pub-id><pub-id pub-id-type="pmid">31808014</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Rowlands AV, et al. Enhancing the value of accelerometer-assessed physical activity: meaningful visual comparisons of data-driven translational accelerometer metrics. Sports Med Open 2019;5:47.<pub-id pub-id-type="pmid">31808014</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Gershuny</surname><given-names>J</given-names></name><etal/></person-group><article-title>Testing self-report time-use diaries against objective instruments in real time</article-title><source>Socioll Methodol</source><year>2020</year><volume>50</volume><fpage>318</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1177/0081175019884591</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Gershuny J, et al. Testing self-report time-use diaries against objective instruments in real time. Socioll Methodol 2020;50:318&#x02013;49.</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Gershuny</surname><given-names>J</given-names></name><etal/></person-group><article-title>Testing self-report time-use diaries against objective instruments in real time</article-title><source>Socioll Methodol</source><year>2019</year><volume>50</volume><fpage>318</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1177/0081175019884591</pub-id></element-citation><mixed-citation id="mc-CR29" publication-type="journal">Gershuny J, et al. Testing self-report time-use diaries against objective instruments in real time. Socioll Methodol 2019;50:318&#x02013;49.</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>van Hees</surname><given-names>VT</given-names></name><etal/></person-group><article-title>Separating movement and gravity components in an acceleration signal and implications for the assessment of human daily physical activity</article-title><source>PLoS One</source><year>2013</year><volume>8</volume><fpage>e61691</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0061691</pub-id><pub-id pub-id-type="pmid">23626718</pub-id>
</element-citation><mixed-citation id="mc-CR30" publication-type="journal">van Hees VT, et al. Separating movement and gravity components in an acceleration signal and implications for the assessment of human daily physical activity. PLoS One 2013;8:e61691.<pub-id pub-id-type="pmid">23626718</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Hildebrand</surname><given-names>M</given-names></name><name><surname>van Hees</surname><given-names>VT</given-names></name><name><surname>Hansen</surname><given-names>BH</given-names></name><name><surname>Ekelund</surname><given-names>U</given-names></name></person-group><article-title>Age group comparability of raw accelerometer output from wrist- and hip-worn monitors</article-title><source>Med Sci Sports Exerc</source><year>2014</year><volume>46</volume><issue>9</issue><fpage>1816</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1249/MSS.0000000000000289</pub-id><pub-id pub-id-type="pmid">24887173</pub-id>
</element-citation><mixed-citation id="mc-CR31" publication-type="journal">Hildebrand M, van Hees VT, Hansen BH, Ekelund U. Age group comparability of raw accelerometer output from wrist- and hip-worn monitors. Med Sci Sports Exerc. 2014;46(9):1816&#x02013;24.<pub-id pub-id-type="pmid">24887173</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Willetts</surname><given-names>M</given-names></name><etal/></person-group><article-title>Statistical machine learning of sleep and physical activity phenotypes from sensor data in 96,220 UK Biobank participants</article-title><source>Sci Rep</source><year>2018</year><volume>8</volume><fpage>7961</fpage><pub-id pub-id-type="doi">10.1038/s41598-018-26174-1</pub-id><pub-id pub-id-type="pmid">29784928</pub-id>
</element-citation><mixed-citation id="mc-CR32" publication-type="journal">Willetts M, et al. Statistical machine learning of sleep and physical activity phenotypes from sensor data in 96,220 UK Biobank participants. Sci Rep 2018;8:7961.<pub-id pub-id-type="pmid">29784928</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Walmsley</surname><given-names>R</given-names></name><etal/></person-group><article-title>Reallocation of time between device-measured movement behaviours and risk of incident cardiovascular disease</article-title><source>Br J Sports Med</source><year>2022</year><volume>56</volume><fpage>1008</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1136/bjsports-2021-104050</pub-id></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Walmsley R, et al. Reallocation of time between device-measured movement behaviours and risk of incident cardiovascular disease. Br J Sports Med 2022;56:1008&#x02013;17.</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Caspersen</surname><given-names>CJ</given-names></name><name><surname>Powell</surname><given-names>KE</given-names></name><name><surname>Christenson</surname><given-names>GM</given-names></name></person-group><article-title>Physical activity, exercise, and physical fitness: definitions and distinctions for health-related research</article-title><source>Public Health Rep</source><year>1985</year><volume>100</volume><issue>2</issue><fpage>126</fpage><pub-id pub-id-type="pmid">3920711</pub-id>
</element-citation><mixed-citation id="mc-CR34" publication-type="journal">Caspersen CJ, Powell KE, Christenson GM. Physical activity, exercise, and physical fitness: definitions and distinctions for health-related research. Public Health Rep. 1985;100(2):126.<pub-id pub-id-type="pmid">3920711</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR35"><label>35.</label><citation-alternatives><element-citation id="ec-CR35" publication-type="journal"><person-group person-group-type="author"><name><surname>Verswijveren</surname><given-names>S</given-names></name><etal/></person-group><article-title>Using compositional data analysis to explore accumulation of sedentary behavior, physical activity and youth health</article-title><source>J Sport Health Sci</source><year>2022</year><volume>11</volume><fpage>234</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.jshs.2021.03.004</pub-id><pub-id pub-id-type="pmid">33737239</pub-id>
</element-citation><mixed-citation id="mc-CR35" publication-type="journal">Verswijveren S, et al. Using compositional data analysis to explore accumulation of sedentary behavior, physical activity and youth health. J Sport Health Sci 2022;11:234&#x02013;43.<pub-id pub-id-type="pmid">33737239</pub-id>
</mixed-citation></citation-alternatives></ref></ref-list></back></article>