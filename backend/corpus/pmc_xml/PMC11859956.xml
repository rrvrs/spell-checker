<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006417</article-id><article-id pub-id-type="pmc">PMC11859956</article-id><article-id pub-id-type="doi">10.3390/s25041188</article-id><article-id pub-id-type="publisher-id">sensors-25-01188</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>An Artificial Intelligence Model for Sensing Affective Valence and Arousal from Facial Images</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Nomiya</surname><given-names>Hiroki</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-01188" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Shimokawa</surname><given-names>Koh</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af2-sensors-25-01188" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Namba</surname><given-names>Shushi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af3-sensors-25-01188" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Osumi</surname><given-names>Masaki</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af4-sensors-25-01188" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5335-1272</contrib-id><name><surname>Sato</surname><given-names>Wataru</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af2-sensors-25-01188" ref-type="aff">2</xref><xref rid="c1-sensors-25-01188" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Szwoch</surname><given-names>Mariusz</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01188"><label>1</label>Faculty of Information and Human Sciences, Kyoto Institute of Technology, Kyoto 606-8585, Japan; <email>nomiya@kit.ac.jp</email></aff><aff id="af2-sensors-25-01188"><label>2</label>RIKEN, Psychological Process Research Team, Guardian Robot Project, Kyoto 619-0288, Japan; <email>koh.shimokawa@riken.jp</email></aff><aff id="af3-sensors-25-01188"><label>3</label>Department of Psychology, Hiroshima University, Hiroshima 739-8524, Japan; <email>nashushi@hiroshima-u.ac.jp</email></aff><aff id="af4-sensors-25-01188"><label>4</label>KOHINATA Limited Liability Company, Osaka 556-0020, Japan; <email>osumi@kohinet.com</email></aff><author-notes><corresp id="c1-sensors-25-01188"><label>*</label>Correspondence: <email>wataru.sato.ya@riken.jp</email></corresp></author-notes><pub-date pub-type="epub"><day>15</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1188</elocation-id><history><date date-type="received"><day>13</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>09</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>13</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Artificial intelligence (AI) models can sense subjective affective states from facial images. Although recent psychological studies have indicated that dimensional affective states of valence and arousal are systematically associated with facial expressions, no AI models have been developed to estimate these affective states from facial images based on empirical data. We developed a recurrent neural network-based AI model to estimate subjective valence and arousal states from facial images. We trained our model using a database containing participant valence/arousal states and facial images. Leave-one-out cross-validation supported the validity of the model for predicting subjective valence and arousal states. We further validated the effectiveness of the model by analyzing a dataset containing participant valence/arousal ratings and facial videos. The model predicted second-by-second valence and arousal states, with prediction performance comparable to that of FaceReader, a commercial AI model that estimates dimensional affective states based on a different approach. We constructed a graphical user interface to show real-time affective valence and arousal states by analyzing facial video data. Our model is the first distributable AI model for sensing affective valence and arousal from facial images/videos to be developed based on an empirical database; we anticipate that it will have many practical uses, such as in mental health monitoring and marketing research.</p></abstract><kwd-group><kwd>dimensional emotion rating</kwd><kwd>facial action units</kwd><kwd>machine learning</kwd><kwd>valence/arousal</kwd></kwd-group><funding-group><award-group><funding-source>Japan Science and Technology Agency-Mirai Program</funding-source><award-id>JPMJMI20D7</award-id></award-group><funding-statement>This study was supported by funds from Japan Science and Technology Agency-Mirai Program (JPMJMI20D7).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01188"><title>1. Introduction</title><p>Sensing subjective emotions using objective signals is useful in several ways. For example, emotional experiences are linked to mental and physical well-being [<xref rid="B1-sensors-25-01188" ref-type="bibr">1</xref>] and business success, such as in product development [<xref rid="B2-sensors-25-01188" ref-type="bibr">2</xref>]. Because subjective reports of emotional experiences can be biased and difficult to assess while performing active tasks, sensing emotions using objective signals can have practical applications [<xref rid="B3-sensors-25-01188" ref-type="bibr">3</xref>].</p><p>Recent psychological studies have reported that dimensions of subjective emotions, such as valence and arousal, correspond to facial expressions [<xref rid="B4-sensors-25-01188" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01188" ref-type="bibr">5</xref>]. Namba et al. [<xref rid="B4-sensors-25-01188" ref-type="bibr">4</xref>] asked participants to remember various affective valence and arousal states and express them through facial expressions; they acquired images of participants&#x02019; faces and analyzed facial action units (AUs) using the automated version of the Facial Action Coding System (FACS) [<xref rid="B6-sensors-25-01188" ref-type="bibr">6</xref>] and found associations between several AUs and valence or arousal states. Zhang et al. [<xref rid="B5-sensors-25-01188" ref-type="bibr">5</xref>] videotaped participants&#x02019; faces and assessed dynamic valence and arousal ratings while they observed emotion-inducing films. Automated AU analysis revealed several associations between AUs and dynamic ratings of valence or arousal. Although associations between affective states and facial expressions have long been investigated based on categorical emotion theory [<xref rid="B7-sensors-25-01188" ref-type="bibr">7</xref>], empirical research has not clearly supported the theory of facial expressions associated with emotional categories (for reviews, see [<xref rid="B8-sensors-25-01188" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01188" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01188" ref-type="bibr">10</xref>]). These findings suggest that it is possible to sense affective dimensions from facial expressions.</p><p>Despite these findings, no artificial intelligence (AI) system has been developed to estimate affective dimensions based on firm evidence. Although many AI systems have been developed for sensing emotions from facial expressions, most of these are based on emotion categories [<xref rid="B11-sensors-25-01188" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01188" ref-type="bibr">12</xref>]. Although the commercial AI system FaceReader can estimate dimensional affective states, it estimates these dimensional states indirectly, by learning the correspondence between facial expressions and emotion categories [<xref rid="B13-sensors-25-01188" ref-type="bibr">13</xref>]. These AI systems have not learned to associate a person&#x02019;s facial expressions with dimensional subjective affective states.</p><p>In this study, we developed an AI model that learns the correspondence between a person&#x02019;s facial expressions and their subjective emotions. Our model receives the intensity values of several AUs as input and outputs affective dimension values (valence and arousal intensity) in real time. We used the RIKEN facial expression database [<xref rid="B4-sensors-25-01188" ref-type="bibr">4</xref>] for training. We tested our AI model using facial video data provided by Zhang et al. [<xref rid="B5-sensors-25-01188" ref-type="bibr">5</xref>]. We evaluated the performance of our model by conducting leave-one-out cross-validation and analyzing the correlation coefficients between actual and estimated valence and arousal values. We also compared the estimation performance of our model and FaceReader, a commercial software that outputs indirect affective dimension values. In addition, feature importance analyses were conducted using the drop-column importance method to interpret our model. Based on the AI model, we developed a graphical user interface (GUI)-based software to estimate affective dimension values. The application effectively visualizes the estimated intensity of valence and arousal.</p></sec><sec id="sec2-sensors-25-01188"><title>2. Model Development</title><sec id="sec2dot1-sensors-25-01188"><title>2.1. Model</title><p>An AI model was developed to estimate valence and arousal intensity using the intensity values of several AUs input into the model. AUs are the units of specific movements of facial muscles. As valence and arousal are thought to be closely related to the movement of facial muscles, we used AU intensity as the AI model input. Considering temporal changes in AU intensity, the input of the AI model was time-series data.</p><p>The AI model was constructed using a gated recurrent unit (GRU), such that time-series data could be used to train the model. A GRU is a type of neural network architecture used in machine learning, particularly for processing sequential data, like time series or natural language. It is a variant of recurrent neural networks (RNNs) and is designed to solve some of the challenges RNNs face, like the vanishing gradient problem, which makes it difficult for them to learn long-term dependencies in data.</p><p>Long Short-Term Memory (LSTM) is also widely used to train AI models with time-series data. However, GRUs have fewer parameters than LSTMs, making them faster and less resource-intensive. While simpler, GRUs are still capable of capturing long-term dependencies in data, similar to LSTMs. This simplicity often leads to faster training times and may reduce the risk of overfitting, especially with smaller datasets. In fact, we found through a preliminary experiment that the GRU was comparable to LSTM in estimation accuracy, and made slightly faster estimates than LSTM. Therefore, we adopted a GRU for the AI model. In another preliminary experiment, we found that training separate models for estimating valence and arousal was more effective in terms of estimation accuracy. Therefore, our AI model consists of separate GRU-based valence and arousal models.</p><p><xref rid="sensors-25-01188-f001" ref-type="fig">Figure 1</xref> shows the structure of the GRU-based model, which was used for both the valence and arousal models. To consider the temporal change in AUs, time-series AU intensity data were used as the model input. We used the 17 AUs shown in <xref rid="sensors-25-01188-t001" ref-type="table">Table 1</xref>. The intensity values of these AUs were obtained using OpenFace. The time-series data were generated from five frames extracted from a 1 s facial video. Therefore, our model estimates valence and arousal intensity once per second. Using five frames appeared to be sufficient in the preliminary experiment.</p></sec><sec id="sec2dot2-sensors-25-01188"><title>2.2. Dataset</title><p>We used the RIKEN facial expression database [<xref rid="B4-sensors-25-01188" ref-type="bibr">4</xref>], which contains facial videos of 48 Japanese participants, to train the AI model. In this database, the participants were recorded using 10 Kinect devices to capture multi-angle facial expressions, including both color images and 3D data. However, for the purpose of training, we adopted only the frontal color image data, considering the practical constraints of implementing an emotion estimation application under commonly available conditions.</p><p>In this database, the participants rated valence and arousal on a scale from 1 to 5, corresponding to the intensity of each emotion. For each participant, there are 25 videos corresponding to all combinations of the five levels of valence and arousal. <xref rid="sensors-25-01188-f002" ref-type="fig">Figure 2</xref> shows the videos of a participant in this database. For the readability of the figure, this figure focuses on four patterns of extreme expressions, combining valence 1 or 5 with arousal 1 or 5, as well as one pattern of a neutral expression, resulting in a total of five expression patterns. Note that we used all 25 videos for each participant to train our model.</p><p>Each video consists of three phases: a 1-second transition from a neutral expression to the target expression, a 2-second maintenance phase of the target expression, and a 1-second return to the neutral expression. For each video, we extracted the 1-second segment in which the expression was most prominently displayed. Then, we used five frames extracted from the 1 s video at equal intervals. The extracted frames were converted into numerical data using OpenFace to obtain 17 action unit (AU) features, which were utilized as input for training. As the valence and arousal ratings were defined for each video, all five frames had the same rating.</p></sec><sec id="sec2dot3-sensors-25-01188"><title>2.3. Cross-Validation Performance</title><p>We evaluated the performance of the AI model using leave-one-out cross-validation based on the training dataset. In this experiment, facial videos of 1 participant were used for the test and those of the other 47 participants were used for training. The valence and arousal ratings were estimated for each facial video because such ratings were provided for each video in the training set. The optimal hyperparameters for the AI model were determined through preliminary analyses.</p><p><xref rid="sensors-25-01188-f003" ref-type="fig">Figure 3</xref> shows the correlation coefficients between actual and estimated valence and arousal values. The correlation coefficients were analyzed using single-sample <italic toggle="yes">t</italic>-tests against zero following Fisher transformation. The results showed that both valence and arousal rating prediction values were significantly positive (<inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>47</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>9.88</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5.23</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1.43</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.76</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p><xref rid="sensors-25-01188-f004" ref-type="fig">Figure 4</xref> and <xref rid="sensors-25-01188-f005" ref-type="fig">Figure 5</xref> show the correlation coefficients between actual and estimated valence and arousal values for each participant, respectively.</p><p>The correlation coefficients of most of the participants are positive values. The correlation coefficients of almost 90% of the participants (43 participants) are positive for valence. As for arousal, the correlation coefficients of more than 80% of the participants (40 participants) are positive.</p></sec><sec id="sec2dot4-sensors-25-01188"><title>2.4. Feature Importance Analysis for Valence and Arousal Estimation</title><p>To evaluate the contribution of each AU to the prediction of valence and arousal, a feature importance analysis was conducted using the drop-column importance method. Drop-column importance quantifies the significance of individual features by removing them from the model and measuring the resulting change in prediction error. Specifically, if the removal of a particular feature leads to a significant increase in prediction error, it indicates that the feature plays a crucial role in the estimation task. Conversely, if the removal results in a decrease in error, it suggests that the feature may act as noise rather than contributing to the prediction.</p><p>For feature importance evaluation, the conventional absolute change in error was replaced with a relative fluctuation rate as an indicator. This approach involved normalizing the difference between the post-removal error and the original error using the original error value. By adopting this method, the influence of error magnitude was eliminated, allowing for a more accurate assessment of each feature&#x02019;s relative contribution to the prediction task.</p><p>The drop-column importance of each AU is shown in <xref rid="sensors-25-01188-f006" ref-type="fig">Figure 6</xref>. The graph on the left side of the figure shows the drop-column importance for valence. In valence estimation, AU 4 (brow lowerer) and AU 1 (inner brow raiser) demonstrated particularly high importance. This finding aligns with previous studies that report a strong involvement of brow movements in the expression of valence [<xref rid="B4-sensors-25-01188" ref-type="bibr">4</xref>], suggesting that the model effectively captures these established relationships. Additionally, AU 12 (lip corner puller) was also identified as a significant factor for valence estimation, reinforcing the well-documented role of smiling expressions as indicators of positive valence.</p><p>The graph on the right side of <xref rid="sensors-25-01188-f006" ref-type="fig">Figure 6</xref> shows the drop-column importance for arousal. For arousal estimation, AU 4 (brow lowerer), AU 6 (cheek raiser), and AU 17 (chin raiser) exhibited high importance. This result suggests that in high-arousal states, increased facial muscle tension and activity, particularly in the cheeks and chin, contribute significantly to the model&#x02019;s predictions. Furthermore, AU 7 (lid tightener) was identified as a highly important feature for arousal estimation, supporting the idea that expressions of surprise or tension are closely associated with high-arousal states.</p><p>The results obtained in this study correspond well with the insights discussed by Namba et al. [<xref rid="B4-sensors-25-01188" ref-type="bibr">4</xref>] regarding the facial features associated with valence and arousal. This suggests that the features learned by the model align with actual emotional expressions, providing evidence that the proposed approach is a valid method for emotion estimation based on facial data.</p><p>The drop-column importance results are presented as numerical values for each AU. A positive value indicates that removing the feature increases prediction error, signifying that the feature is important for the model&#x02019;s estimation task. In contrast, a negative value implies that removing the feature reduces error, suggesting that it may act as noise. Furthermore, larger absolute values indicate that the model is more dependent on that particular feature. By analyzing these feature importance values, this study provides insights into how the trained model determines valence and arousal. The identified important features align with findings from prior research, reinforcing the validity of the proposed emotion estimation approach.</p></sec></sec><sec id="sec3-sensors-25-01188"><title>3. Experiment</title><sec id="sec3dot1-sensors-25-01188"><title>3.1. Dataset</title><p>We used the RIKEN facial expression database [<xref rid="B4-sensors-25-01188" ref-type="bibr">4</xref>] to train our AI model, and facial videos of 23 Japanese participants from Zhang et al. [<xref rid="B5-sensors-25-01188" ref-type="bibr">5</xref>] were used for the test. The facial videos from Zhang et al. [<xref rid="B5-sensors-25-01188" ref-type="bibr">5</xref>] were recorded while the participants were watching five videos designed to evoke different emotions (anger, sadness, neutral, contentment, and amusement). The recordings were made using a single frontal camera, and the lengths of the videos were as follows: anger, 157 s; sadness, 172 s; neutral, 206 s; contentment, 148 s; amusement, 196 s.</p><p>In these videos, both segments with strongly expressed emotions and segments with weaker expressions were observed. After recording, the participants retrospectively evaluated their affective states during the recordings by subjectively rating valence and arousal using real numbers ranging from 1 to 9. For each facial video, second-by-second ratings were provided. Therefore, the valence and arousal ratings were estimated for each second. Since the valence and arousal ratings in the RIKEN facial expression database used for training ranged from 1 to 5, we adjusted the range of the output values to match the range from 1 to 9 used in the test dataset using the equation <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>I</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mspace width="3.33333pt"/><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>I</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mn>5</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the original rating and <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mspace width="3.33333pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x02264;</mml:mo><mml:mn>9</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the converted rating. This conversion ensured compatibility between the training and test datasets.</p></sec><sec id="sec3dot2-sensors-25-01188"><title>3.2. Experimental Settings</title><p>The AI model was implemented using Keras. The valence and arousal models were trained separately. We performed hyperparameter optimization for each of the valence and arousal models. The hyperparameters were optimized to minimize loss (mean absolute error of the correlation coefficients). The optimized hyperparameters are listed in <xref rid="sensors-25-01188-t002" ref-type="table">Table 2</xref>; the default settings were used for all other hyperparameters.</p><p>We also compared the estimation performance of our model and FaceReader 9.0 (Noldus Information Technology, WCeningen, the Netherlands). The software detects the faces in the images based on the Viola&#x02013;Jones algorithm [<xref rid="B14-sensors-25-01188" ref-type="bibr">14</xref>], and constructs 3D face models based on the Active Appearance Method [<xref rid="B15-sensors-25-01188" ref-type="bibr">15</xref>] in combination with deep artificial neural network classification [<xref rid="B16-sensors-25-01188" ref-type="bibr">16</xref>]. Then, by using seven independent artificial neural networks trained on large databases of prototypical emotional facial expressions, the software quantifies the intensities of the seven emotion categories (i.e., six basic and neutral emotions) [<xref rid="B17-sensors-25-01188" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01188" ref-type="bibr">18</xref>]. The software can also give the FACS scoring of a face [<xref rid="B6-sensors-25-01188" ref-type="bibr">6</xref>] by using the similar artificial neural networks trained on large databases of AU-coded facial images [<xref rid="B17-sensors-25-01188" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01188" ref-type="bibr">18</xref>]. Although the main output of the software is the intensity values of emotion categories, the software also calculates the valence and arousal values based on the category and AU values [<xref rid="B13-sensors-25-01188" ref-type="bibr">13</xref>]. The valence is calculated as the intensity of &#x02018;happy&#x02019; minus the intensity of the negative expression with the highest intensity [<xref rid="B13-sensors-25-01188" ref-type="bibr">13</xref>]. For instance, if the intensity of &#x02018;happy&#x02019; is 0.8 and the intensities of &#x02018;sad&#x02019;, &#x02018;angry&#x02019;, &#x02018;fear&#x02019;, and &#x02018;disgusted&#x02019; are 0.2, 0.0, 0.3, and 0.2, respectively, then the valence is <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.8</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mn>0.3</mml:mn><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The arousal is calculated based on the 20 AU values as follows [<xref rid="B13-sensors-25-01188" ref-type="bibr">13</xref>]: (1) The 20 AU values are taken as input. These are AUs 1, 2, 4, 5, 6, 7, 9, 10, 12, 14, 15, 17, 20, 23, 24, 25, 26, 27, and the inverse of 43. (2) The average AU activation values are calculated over the last 60 s. During the first 60 s of the analysis, the average AAV is calculated over the analysis up to that moment. (3) The average AU activation values are subtracted from the current AU activation values as the corrected activation values. (4) The arousal is calculated from these corrected activation values by taking the mean of the five highest values.</p></sec><sec sec-type="results" id="sec3dot3-sensors-25-01188"><title>3.3. Results</title><p>We estimated the valence and arousal ratings using the facial videos in the test dataset for every second. Then, the correlation coefficients between the actual and predicted valence and arousal ratings were computed for each participant using our model, as well as FaceReader 9.</p><p><xref rid="sensors-25-01188-f007" ref-type="fig">Figure 7</xref> shows the correlation coefficients between actual and estimated valence and arousal ratings. The coefficients were Fisher-transformed and the values were subjected to single-sample <italic toggle="yes">t</italic>-tests against zero. The results showed that the valence and arousal ratings of both our model and FaceReader were significantly positive (<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>22</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>7.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5.12</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>6.37</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>7.91</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1.46</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.07</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.33</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.65</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively).</p><p>Next, the z-transformed coefficients of our model and FaceReader were compared using paired <italic toggle="yes">t</italic>-tests. There were no significant differences in either valence and arousal ratings between the models (<inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>22</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.80</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.73</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mn>0.43</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). Bayesian paired <italic toggle="yes">t</italic>-tests confirmed the lack of significant differences (Bayes factor = <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.29</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.28</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively).</p><p>The correlation coefficients of the 23 participants (participant 1 to 23) are shown in <xref rid="sensors-25-01188-f008" ref-type="fig">Figure 8</xref> and <xref rid="sensors-25-01188-f009" ref-type="fig">Figure 9</xref>. <xref rid="sensors-25-01188-f008" ref-type="fig">Figure 8</xref> shows the correlation coefficients for valence, and <xref rid="sensors-25-01188-f009" ref-type="fig">Figure 9</xref> shows those for arousal.</p><p>As for the estimation of valence ratings, our model outperformed FaceReader in 8 participants out of 23 participants. FaceReader has higher correlation coefficients for most participants; there are a few participants whose correlation coefficients calculated by our model are much higher than those by FaceReader such as participants 17 and 20.</p><p>Regarding the estimation of arousal ratings, our model outperformed FaceReader in 16 participants out of 23 participants. In contrast to the estimation of valence ratings, our model shows better performance on average. However, the correlation coefficient of participant 9 is much lower compared to FaceReader. The test data by Zhang et al. [<xref rid="B5-sensors-25-01188" ref-type="bibr">5</xref>] were created by showing five videos to the participants. The correlation coefficients are computed for the data including the five videos. When the correlation coefficients are separately computed for each video, the correlation coefficients of arousal for participant 9 are &#x02212;0.113, 0.188, 0.004, 0.346, and 0.287. This stems from the high variance of the intensity values of arousal among the videos. This analysis result indicates that our AI model is comparable to FaceReader for every participant.</p><p>The feature importance analysis using drop-column importance was also conducted on the test data to identify the features contributing to the estimation of valence and arousal. The drop-column importance for each AU is shown in <xref rid="sensors-25-01188-f010" ref-type="fig">Figure 10</xref>. The graph on the left side shows the drop-column importance for valence and the graph on the right side shows that for arousal.</p><p>For valence estimation, as in the analysis for the training data, AU 2 (outer brow raiser), AU 1 (inner brow raiser), and AU 4 (brow lowerer) demonstrated particularly high importance. Additionally, AU 7 highly contributed to the valence estimation of the test set.</p><p>On the other hand, for arousal estimation, AU 4 (brow lowerer), AU 2 (outer brow raiser), AU 7 (lid tightener), and AU 1 (inner brow raiser) showed high importance. These features imply that increased facial tension in high-arousal states leads to more active movements of the brows and eyes. Additionally, AU 5 (upper lip raiser) and AU 6 (cheek raiser) also exhibited certain levels of importance, suggesting that muscle activity around the mouth may be involved in high-arousal states.</p></sec></sec><sec id="sec4-sensors-25-01188"><title>4. Application</title><p>We developed a GUI-based system to visualize the estimation results of our AI model. The system estimates and visualizes the intensity of valence and arousal in real time from a video of a single face (<xref rid="sensors-25-01188-f011" ref-type="fig">Figure 11</xref>). Either a video file or a video data captured by a web camera can be used for the facial video. The GUI-based system and the estimation model are separated, and we can choose an estimation model before the estimation. This design makes it easy to use an estimation model trained by a user.</p><p>The GUI system summarizes the estimation results as shown in <xref rid="sensors-25-01188-f012" ref-type="fig">Figure 12</xref>, including temporal changes in valence and arousal intensity throughout the entire video, and the distribution and frequency of intensity values, indicated by color changes. Our system is easy to use and will be helpful for the analysis of the estimation result of valence and arousal. Our system, named &#x0201c;KKR Facial Affect Reader&#x0201d;, is available for non-commercial academic purposes (<uri xlink:href="https://github.com/prgshare/KKRFacialAffectReader">https://github.com/prgshare/KKRFacialAffectReader</uri> (accessed on 12 February 2025)).</p></sec><sec sec-type="discussion" id="sec5-sensors-25-01188"><title>5. Discussion</title><p>Our cross-validation using the training dataset showed significantly positive correlations between actual and estimated valence and arousal intensity values. Validation based on the test dataset also showed significant positive correlations between actual and estimated valence and arousal ratings. In our analysis of the test set, the correlation coefficients of our AI model and FaceReader were comparable for both the valence and arousal ratings. These results suggest that our AI model can estimate affective dimensions (i.e., valence and arousal) comparably to the commercial AI system. However, our model learned the relationships between participants&#x02019; facial expressions and their subjective affective states, whereas FaceReader did not. To our knowledge, although there are a few AI models that can estimate dimensional affective states from facial images based on empirical data, ours is the first one that is publicly available.</p><p>Our AI model is anticipated to have practical value because emotional processing influences many aspects of daily life, including well-being and decision-making [<xref rid="B19-sensors-25-01188" ref-type="bibr">19</xref>], and subjective ratings may be inappropriate or unavailable under some conditions [<xref rid="B3-sensors-25-01188" ref-type="bibr">3</xref>], which would make estimating affective states using facial images valuable. For example, one clinical study reported that although patients with semantic dementia had difficulty reporting their affective states using rating scales while viewing emotional films, they produced evident facial muscle activity [<xref rid="B20-sensors-25-01188" ref-type="bibr">20</xref>]. Because self-reports of emotional experiences could be biased, some marketing researchers recommended analyzing subtle facial muscle activity to predict customer behavior [<xref rid="B21-sensors-25-01188" ref-type="bibr">21</xref>]. These findings suggest that our AI model will be useful for estimating emotional experiences in applied research.</p><p>One limitation of this study is that our analysis included only 17 AUs, whereas human FACS coders can evaluate more AUs [<xref rid="B6-sensors-25-01188" ref-type="bibr">6</xref>]. Consequently, some associations between dimensional affective states and other AUs may have been missed. For example, Hyniewska et al. [<xref rid="B22-sensors-25-01188" ref-type="bibr">22</xref>] investigated the link between emotional category recognition and AUs and identified a positive relationship between fear recognition and AU 16 (lower lip depressor), which was excluded from our analysis. Future AI models will likely analyze more AUs.</p><p>The results of the feature importance analysis using drop-column importance confirmed that specific AUs significantly contribute to the model&#x02019;s predictions for valence and arousal estimation. For valence estimation, AU 2 (outer brow raiser), AU 1 (inner brow raiser), and AU 4 (brow lowerer) exhibited high importance. These features have also been reported in previous studies as crucial indicators in the perception of valence, suggesting that the model effectively learns the characteristics of human emotional expression.</p><p>Similarly, for arousal estimation, AU 4 (brow lowerer), AU 2 (outer brow raiser), and AU 1 (inner brow raiser) were found to be important, highlighting the strong influence of brow and eye tension on arousal perception. This finding reflects the fact that in high-arousal states, expressions of surprise and tension become more pronounced, making movements of the eyes and brows particularly noticeable. Additionally, AU 5 (upper lip raiser) and AU 6 (cheek raiser) also contributed to some extent, suggesting that muscle activity around the mouth may be involved in representing the intensity of emotions.</p><p>These results suggest that, as a characteristic specific to Japanese individuals, facial expressions may be more effectively conveyed through the eyes rather than the mouth. Additionally, while AU estimation using OpenFace has successfully captured AUs around the eyes, there are cases where it fails to accurately detect AUs around the mouth. Considering these findings, improving AU estimation not only for the eye region, which is already effective for emotion estimation, but also for the mouth region could lead to a more precise emotion recognition system. Further investigation into mouth movements is necessary to enhance the accuracy of emotion estimation.</p><p>Another limitation of this study is that we analyzed affective facial expressions while remembering personal events and watching film clips. Although these methods have advantages, such as allowing affect elicitation in laboratories, they lack the ecological validity of affective states. Further studies are needed to validate our model by analyzing other facial expression data recorded in realistic situations.</p></sec><sec sec-type="conclusions" id="sec6-sensors-25-01188"><title>6. Conclusions</title><p>Despite the utility of sensing subjective emotions using objective signals, no AI model which is publicly available has been developed to estimate dimensional affective states from facial images based on empirical data. We developed a recurrent neural network-based AI model to estimate the affective dimensions valence and arousal. The effectiveness of our model was confirmed in a cross-validation experiment using the RIKEN facial expression database. We then trained our AI model using the RIKEN facial expression database and tested it with the facial datasets of Zhang et al. [<xref rid="B5-sensors-25-01188" ref-type="bibr">5</xref>]. The results showed that our AI model demonstrated results comparable to those of FaceReader, a commercial software that depends on the estimation of emotion categories. This finding suggests that emotional categories and affective dimensions are not exclusively related. We also developed a GUI-based system to visualize estimated valence and arousal.</p><p>Although our AI model is comparable to FaceReader, further improvement of the estimation performance is essential because our model could not outperform FaceReader in the estimation of arousal. In addition, it was clarified through the experiment that the estimation performance of arousal as well as valence is not satisfactory considering the correlation coefficients of valence and arousal. Therefore, it is necessary to enhance the estimation performance of our model by improving the learning method. Another limitation of our AI model is that the number of AUs used for estimation was insufficient. Future research to develop a more effective AI model should analyze more AUs.</p></sec></body><back><ack><title>Acknowledgments</title><p>The author thanks Junyao Zhang for his technical support.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization and investigation, H.N., K.S., S.N., M.O. and W.S. Analysis, H.N., K.S. and W.S. Writing, H.N., K.S., S.N., M.O. and W.S. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Ethics approval was not required for this study because no participants were tested.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was not required for this study because no participants were tested.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The datasets analyzed during the current study are available from the corresponding author upon reasonable request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Author Masaki Osumi was employed by the company KOHINATA Limited Liability Company. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01188"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lyubomirsky</surname><given-names>S.</given-names></name>
</person-group><article-title>Why are some people happier than others? The role of cognitive and motivational processes in well-being</article-title><source>Am. Psychol.</source><year>2001</year><volume>56</volume><fpage>239</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1037/0003-066X.56.3.239</pub-id><pub-id pub-id-type="pmid">11315250</pub-id>
</element-citation></ref><ref id="B2-sensors-25-01188"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Meiselman</surname><given-names>H.L.</given-names></name>
</person-group><article-title>A review of the current state of emotion research in product development</article-title><source>Food Res. Int.</source><year>2015</year><volume>76</volume><fpage>192</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1016/j.foodres.2015.04.015</pub-id></element-citation></ref><ref id="B3-sensors-25-01188"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sato</surname><given-names>W.</given-names></name>
</person-group><article-title>Advancements in sensors and analyses for emotion sensing</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>4166</elocation-id><pub-id pub-id-type="doi">10.3390/s24134166</pub-id><pub-id pub-id-type="pmid">39000945</pub-id>
</element-citation></ref><ref id="B4-sensors-25-01188"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Namba</surname><given-names>S.</given-names></name>
<name><surname>Sato</surname><given-names>W.</given-names></name>
<name><surname>Namba</surname><given-names>S.</given-names></name>
<name><surname>Nomiya</surname><given-names>H.</given-names></name>
<name><surname>Shimokawa</surname><given-names>K.</given-names></name>
<name><surname>Osumi</surname><given-names>M.</given-names></name>
</person-group><article-title>Development of the RIKEN database for dynamic facial expressions with multiple angles</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><elocation-id>21785</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-49209-8</pub-id><pub-id pub-id-type="pmid">38066065</pub-id>
</element-citation></ref><ref id="B5-sensors-25-01188"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Sato</surname><given-names>W.</given-names></name>
<name><surname>Kawamura</surname><given-names>N.</given-names></name>
<name><surname>Shimokawa</surname><given-names>K.</given-names></name>
<name><surname>Tang</surname><given-names>B.</given-names></name>
<name><surname>Nakamura</surname><given-names>Y.</given-names></name>
</person-group><article-title>Sensing emotional valence and arousal dynamics through automated facial action unit analysis</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>19563</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-70563-8</pub-id><pub-id pub-id-type="pmid">39174675</pub-id>
</element-citation></ref><ref id="B6-sensors-25-01188"><label>6.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
<name><surname>Friesen</surname><given-names>W.V.</given-names></name>
<name><surname>Hager</surname><given-names>J.C.</given-names></name>
</person-group><source>Facial Action Coding System</source><edition>2nd ed.</edition><publisher-name>Research Nexus</publisher-name><publisher-loc>Charleston, SC, USA</publisher-loc><year>2002</year></element-citation></ref><ref id="B7-sensors-25-01188"><label>7.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
</person-group><article-title>Universals and cultural differences in facial expressions of emotion</article-title><source>Nebraska Symposium on Motivation 1971</source><person-group person-group-type="editor">
<name><surname>Cole</surname><given-names>J.K.</given-names></name>
</person-group><publisher-name>University of Nebraska Press</publisher-name><publisher-loc>Lincoln, NE, USA</publisher-loc><year>1971</year><fpage>207</fpage><lpage>283</lpage></element-citation></ref><ref id="B8-sensors-25-01188"><label>8.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Dur&#x000e1;n</surname><given-names>J.I.</given-names></name>
<name><surname>Reisenzein</surname><given-names>R.</given-names></name>
<name><surname>Fern&#x000e1;ndez-Dols</surname><given-names>J.-M.</given-names></name>
</person-group><article-title>Coherence between emotions and facial expressions: A research synthesis</article-title><source>The Science of Facial Expression</source><person-group person-group-type="editor">
<name><surname>Fern&#x000e1;ndez-Dols</surname><given-names>J.-M.</given-names></name>
<name><surname>Russell</surname><given-names>J.A.</given-names></name>
</person-group><publisher-name>Oxford University Press</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2017</year><fpage>107</fpage><lpage>129</lpage></element-citation></ref><ref id="B9-sensors-25-01188"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fern&#x000e1;ndez-Dols</surname><given-names>J.-M.</given-names></name>
<name><surname>Crivelli</surname><given-names>C.</given-names></name>
</person-group><article-title>Emotion and expression: Naturalistic studies</article-title><source>Emot. Rev.</source><year>2013</year><volume>5</volume><fpage>24</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1177/1754073912457229</pub-id></element-citation></ref><ref id="B10-sensors-25-01188"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Reisenzein</surname><given-names>R.</given-names></name>
<name><surname>Studtmann</surname><given-names>M.</given-names></name>
<name><surname>Horstmann</surname><given-names>G.</given-names></name>
</person-group><article-title>Coherence between emotion and facial expression: Evidence from laboratory experiments</article-title><source>Emot. Rev.</source><year>2013</year><volume>5</volume><fpage>16</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1177/1754073912457228</pub-id></element-citation></ref><ref id="B11-sensors-25-01188"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Barrett</surname><given-names>L.F.</given-names></name>
<name><surname>Adolphs</surname><given-names>R.</given-names></name>
<name><surname>Marsella</surname><given-names>S.</given-names></name>
<name><surname>Martinez</surname><given-names>A.M.</given-names></name>
<name><surname>Pollak</surname><given-names>S.D.</given-names></name>
</person-group><article-title>Emotional expressions reconsidered: Challenges to infer-ring emotion from human facial movements</article-title><source>Perspect. Psychol. Sci.</source><year>2019</year><volume>20</volume><fpage>1</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1177/1529100619832930</pub-id><pub-id pub-id-type="pmid">31313636</pub-id>
</element-citation></ref><ref id="B12-sensors-25-01188"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dupr&#x000e9;</surname><given-names>D.</given-names></name>
<name><surname>Krumhuber</surname><given-names>E.G.</given-names></name>
<name><surname>K&#x000fc;ster</surname><given-names>D.</given-names></name>
<name><surname>McKeown</surname><given-names>G.J.</given-names></name>
</person-group><article-title>A performance comparison of eight commercially available auto-matic classifiers for facial affect recognition</article-title><source>PLoS ONE</source><year>2020</year><volume>15</volume><elocation-id>e0231968</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0231968</pub-id><pub-id pub-id-type="pmid">32330178</pub-id>
</element-citation></ref><ref id="B13-sensors-25-01188"><label>13.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Loijens</surname><given-names>L.</given-names></name>
<name><surname>Krips</surname><given-names>O.</given-names></name>
</person-group><source>FaceReader Methodology Note</source><publisher-name>Noldus Information Technology</publisher-name><publisher-loc>Wageningen, The Netherlands</publisher-loc><year>2019</year></element-citation></ref><ref id="B14-sensors-25-01188"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Viola</surname><given-names>P.</given-names></name>
<name><surname>Jones</surname><given-names>M.</given-names></name>
</person-group><article-title>Rapid object detection using a boosted cascade of simple features</article-title><source>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</source><conf-loc>Kauai, HI, USA</conf-loc><conf-date>8&#x02013;14 December 2001</conf-date><fpage>183</fpage><lpage>195</lpage></element-citation></ref><ref id="B15-sensors-25-01188"><label>15.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Cootes</surname><given-names>T.</given-names></name>
<name><surname>Taylor</surname><given-names>C.</given-names></name>
</person-group><source>Statistical Models of Appearance for Computer Vision</source><publisher-name>University of Manchester</publisher-name><publisher-loc>Manchester, UK</publisher-loc><year>2000</year></element-citation></ref><ref id="B16-sensors-25-01188"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gudi</surname><given-names>A.</given-names></name>
</person-group><article-title>Recognizing semantic features in faces using deep learning</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1512.00743</pub-id></element-citation></ref><ref id="B17-sensors-25-01188"><label>17.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>van Kuilenburg</surname><given-names>H.</given-names></name>
<name><surname>Wiering</surname><given-names>M.</given-names></name>
<name><surname>den Uyl</surname><given-names>M.</given-names></name>
</person-group><article-title>A model-based method for facial expression recognition</article-title><source>Machine Learning: ECML 2005, Proceedings of the 16th European Conference on Machine Learning, Porto, Portugal, 3&#x02013;7 October 2005</source><person-group person-group-type="editor">
<name><surname>Gama</surname><given-names>J.</given-names></name>
<name><surname>Camacho</surname><given-names>R.</given-names></name>
<name><surname>Brazdil</surname><given-names>P.</given-names></name>
<name><surname>Jorge</surname><given-names>A.</given-names></name>
<name><surname>Torgo</surname><given-names>L.</given-names></name>
</person-group><comment>Lectures Notes in Computer Science</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2005</year><volume>Volume 3720</volume><fpage>194</fpage><lpage>205</lpage></element-citation></ref><ref id="B18-sensors-25-01188"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lewinski</surname><given-names>P.</given-names></name>
<name><surname>den Uyl</surname><given-names>T.M.</given-names></name>
<name><surname>Butler</surname><given-names>C.</given-names></name>
</person-group><article-title>Automated facial coding: Validation of basic emotions and FACS AUs in FaceReader</article-title><source>J. Neurosci. Psychol. Econ.</source><year>2014</year><volume>7</volume><fpage>227</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1037/npe0000028</pub-id></element-citation></ref><ref id="B19-sensors-25-01188"><label>19.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Kahneman</surname><given-names>D.</given-names></name>
</person-group><source>Thinking, Fast and Slow</source><publisher-name>Farrar Straus &#x00026; Giroux</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2011</year></element-citation></ref><ref id="B20-sensors-25-01188"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kumfor</surname><given-names>F.</given-names></name>
<name><surname>Hazelton</surname><given-names>J.L.</given-names></name>
<name><surname>Rushby</surname><given-names>J.A.</given-names></name>
<name><surname>Hodges</surname><given-names>J.R.</given-names></name>
<name><surname>Piguet</surname><given-names>O.</given-names></name>
</person-group><article-title>Facial expressiveness and physiological arousal in fronto-temporal dementia: Phenotypic clinical profiles and neural correlates</article-title><source>Cogn. Affect. Behav. Neurosci.</source><year>2019</year><volume>19</volume><fpage>197</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.3758/s13415-018-00658-z</pub-id><pub-id pub-id-type="pmid">30488224</pub-id>
</element-citation></ref><ref id="B21-sensors-25-01188"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Scott</surname><given-names>N.</given-names></name>
<name><surname>Walters</surname><given-names>G.</given-names></name>
</person-group><article-title>Current and potential methods for measuring emotion in tourism experiences: A review</article-title><source>Curr. Issues Tour.</source><year>2015</year><volume>18</volume><fpage>805</fpage><lpage>827</lpage><pub-id pub-id-type="doi">10.1080/13683500.2014.975679</pub-id></element-citation></ref><ref id="B22-sensors-25-01188"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hyniewska</surname><given-names>S.</given-names></name>
<name><surname>Sato</surname><given-names>W.</given-names></name>
<name><surname>Kaiser</surname><given-names>S.</given-names></name>
<name><surname>Pelachaud</surname><given-names>S.</given-names></name>
</person-group><article-title>Naturalistic emotion decoding from facial action sets</article-title><source>Front. Psychol.</source><year>2019</year><volume>9</volume><elocation-id>2678</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2018.02678</pub-id><pub-id pub-id-type="pmid">30713515</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01188-f001"><label>Figure 1</label><caption><p>Structure of a gated recurrent unit (GRU)-based estimation model.</p></caption><graphic xlink:href="sensors-25-01188-g001" position="float"/></fig><fig position="float" id="sensors-25-01188-f002"><label>Figure 2</label><caption><p>Videos of a participant in the RIKEN facial expression database.</p></caption><graphic xlink:href="sensors-25-01188-g002" position="float"/></fig><fig position="float" id="sensors-25-01188-f003"><label>Figure 3</label><caption><p>Mean (standard error) Pearson&#x02019;s correlation coefficients between the actual and estimated ratings. *** <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-25-01188-g003" position="float"/></fig><fig position="float" id="sensors-25-01188-f004"><label>Figure 4</label><caption><p>Pearson&#x02019;s correlation coefficients between the actual and estimated ratings of valence in the model development.</p></caption><graphic xlink:href="sensors-25-01188-g004" position="float"/></fig><fig position="float" id="sensors-25-01188-f005"><label>Figure 5</label><caption><p>Pearson&#x02019;s correlation coefficients between the actual and estimated ratings of arousal in the model development.</p></caption><graphic xlink:href="sensors-25-01188-g005" position="float"/></fig><fig position="float" id="sensors-25-01188-f006"><label>Figure 6</label><caption><p>Results of drop-column importance for training data.</p></caption><graphic xlink:href="sensors-25-01188-g006" position="float"/></fig><fig position="float" id="sensors-25-01188-f007"><label>Figure 7</label><caption><p>Mean (standard error) Pearson&#x02019;s correlation coefficients between actual and estimated valence and arousal ratings estimated by our model and FaceReader 9. *** <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-25-01188-g007" position="float"/></fig><fig position="float" id="sensors-25-01188-f008"><label>Figure 8</label><caption><p>Pearson&#x02019;s correlation coefficients between the actual and estimated ratings of valence in the experiment.</p></caption><graphic xlink:href="sensors-25-01188-g008" position="float"/></fig><fig position="float" id="sensors-25-01188-f009"><label>Figure 9</label><caption><p>Pearson&#x02019;s correlation coefficients between the actual and estimated ratings of arousal in the experiment.</p></caption><graphic xlink:href="sensors-25-01188-g009" position="float"/></fig><fig position="float" id="sensors-25-01188-f010"><label>Figure 10</label><caption><p>Results of drop-column importance for test data.</p></caption><graphic xlink:href="sensors-25-01188-g010" position="float"/></fig><fig position="float" id="sensors-25-01188-f011"><label>Figure 11</label><caption><p>The GUI-based valence/arousal estimation system. <bold>Top left</bold>, input video; <bold>top right</bold>, current valence and arousal intensity values, represented as a point in two-dimensional space; <bold>bottom</bold>, graph showing changes in intensity values. The person shown is one of authors who agreed to show his face.</p></caption><graphic xlink:href="sensors-25-01188-g011" position="float"/></fig><fig position="float" id="sensors-25-01188-f012"><label>Figure 12</label><caption><p>Summary of the estimation results. Top, temporal changes in valence and arousal intensity throughout the entire video. Bottom, distribution of intensity values. The two-dimensional space representing valence and arousal is divided into 5 &#x000d7; 5 regions; in each region, the frequency of the intensity value is represented by color intensity.</p></caption><graphic xlink:href="sensors-25-01188-g012" position="float"/></fig><table-wrap position="float" id="sensors-25-01188-t001"><object-id pub-id-type="pii">sensors-25-01188-t001_Table 1</object-id><label>Table 1</label><caption><p>Action units (AUs) used in the gated recurrent unit (GRU)-based estimation model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AU Number</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Description</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Image</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">Inner Brow Raiser</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i001.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">Outer Brow Raiser</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i002.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">Brow Lowerer</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i003.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">Upper Lid Raiser</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i004.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">Cheek Raiser</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i005.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">Lid Tightener</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i006.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">9</td><td align="center" valign="middle" rowspan="1" colspan="1">Nose Wrinkler</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i007.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">Upper Lip Raiser</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i008.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">12</td><td align="center" valign="middle" rowspan="1" colspan="1">Lip Corner Puller</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i009.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">14</td><td align="center" valign="middle" rowspan="1" colspan="1">Dimpler</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i010.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">Lip Corner Depressor</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i011.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">17</td><td align="center" valign="middle" rowspan="1" colspan="1">Chin Raiser</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i012.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">Lip Stretcher</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i013.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">23</td><td align="center" valign="middle" rowspan="1" colspan="1">Lip Tightener</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i014.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">25</td><td align="center" valign="middle" rowspan="1" colspan="1">Lips Part</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i015.jpg"/>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">26</td><td align="center" valign="middle" rowspan="1" colspan="1">Jaw Drop</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i016.jpg"/>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Blink</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-25-01188-i017.jpg"/>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01188-t002"><object-id pub-id-type="pii">sensors-25-01188-t002_Table 2</object-id><label>Table 2</label><caption><p>Hyperparameters of the valence and arousal models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Hyperparameter</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Valence Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Arousal Model</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">activation</td><td align="center" valign="middle" rowspan="1" colspan="1">ReLU</td><td align="center" valign="middle" rowspan="1" colspan="1">None</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">dropout</td><td align="center" valign="middle" rowspan="1" colspan="1">0.530</td><td align="center" valign="middle" rowspan="1" colspan="1">0.680</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">recurrent_dropout</td><td align="center" valign="middle" rowspan="1" colspan="1">0.266</td><td align="center" valign="middle" rowspan="1" colspan="1">0.301</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">learning_rate</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.23</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.01</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">batch_size</td><td align="center" valign="middle" rowspan="1" colspan="1">48</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">epochs</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">loss</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean absolute error</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean absolute error</td></tr></tbody></table></table-wrap></floats-group></article>