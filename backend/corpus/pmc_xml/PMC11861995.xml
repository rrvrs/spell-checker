<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Tomography</journal-id><journal-id journal-id-type="iso-abbrev">Tomography</journal-id><journal-id journal-id-type="publisher-id">tomography</journal-id><journal-title-group><journal-title>Tomography</journal-title></journal-title-group><issn pub-type="ppub">2379-1381</issn><issn pub-type="epub">2379-139X</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39997997</article-id><article-id pub-id-type="pmc">PMC11861995</article-id>
<article-id pub-id-type="doi">10.3390/tomography11020014</article-id><article-id pub-id-type="publisher-id">tomography-11-00014</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Graph Neural Network Learning on the Pediatric Structural Connectome</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Srinivasan</surname><given-names>Anand</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-tomography-11-00014" ref-type="aff">1</xref><xref rid="fn1-tomography-11-00014" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8452-858X</contrib-id><name><surname>Raja</surname><given-names>Rajikha</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-tomography-11-00014" ref-type="aff">1</xref><xref rid="fn1-tomography-11-00014" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9679-2054</contrib-id><name><surname>Glass</surname><given-names>John O.</given-names></name><xref rid="af1-tomography-11-00014" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6984-2407</contrib-id><name><surname>Hudson</surname><given-names>Melissa M.</given-names></name><xref rid="af2-tomography-11-00014" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Sabin</surname><given-names>Noah D.</given-names></name><xref rid="af1-tomography-11-00014" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0476-7001</contrib-id><name><surname>Krull</surname><given-names>Kevin R.</given-names></name><xref rid="af3-tomography-11-00014" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1054-1201</contrib-id><name><surname>Reddick</surname><given-names>Wilburn E.</given-names></name><xref rid="af1-tomography-11-00014" ref-type="aff">1</xref><xref rid="c1-tomography-11-00014" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Tyrrell</surname><given-names>Pascal N.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-tomography-11-00014"><label>1</label>Departments of Radiology, St. Jude Children&#x02019;s Research Hospital, Memphis, TN 38105, USA; <email>anand.srinivasan@yale.edu</email> (A.S.); <email>rajikha.raja@stjude.org</email> (R.R.); <email>john.glass@stjude.org</email> (J.O.G.); <email>noah.sabin@stjude.org</email> (N.D.S.)</aff><aff id="af2-tomography-11-00014"><label>2</label>Department of Oncology, St. Jude Children&#x02019;s Research Hospital, Memphis, TN 38105, USA; <email>melissa.hudson@stjude.org</email></aff><aff id="af3-tomography-11-00014"><label>3</label>Department of Psychology and Behavioral Sciences, St. Jude Children&#x02019;s Research Hospital, Memphis, TN 38105, USA; <email>kevin.krull@stjude.org</email></aff><author-notes><corresp id="c1-tomography-11-00014"><label>*</label>Correspondence: <email>gene.reddick@stjude.org</email>; Tel.: +1-901-595-3276</corresp><fn id="fn1-tomography-11-00014"><label>&#x02020;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>29</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>11</volume><issue>2</issue><elocation-id>14</elocation-id><history><date date-type="received"><day>07</day><month>11</month><year>2024</year></date><date date-type="rev-recd"><day>15</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>27</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Purpose: Sex classification is a major benchmark of previous work in learning on the structural connectome, a naturally occurring brain graph that has proven useful for studying cognitive function and impairment. While graph neural networks (GNNs), specifically graph convolutional networks (GCNs), have gained popularity lately for their effectiveness in learning on graph data, achieving strong performance in adult sex classification tasks, their application to pediatric populations remains unexplored. We seek to characterize the capacity for GNN models to learn connectomic patterns on pediatric data through an exploration of training techniques and architectural design choices. Methods: Two datasets comprising an adult BRIGHT dataset (N = 147 Hodgkin&#x02019;s lymphoma survivors and N = 162 age similar controls) and a pediatric Human Connectome Project in Development (HCP-D) dataset (N = 135 healthy subjects) were utilized. Two GNN models (GCN simple and GCN residual), a deep neural network (multi-layer perceptron), and two standard machine learning models (random forest and support vector machine) were trained. Architecture exploration experiments were conducted to evaluate the impact of network depth, pooling techniques, and skip connections on the ability of GNN models to capture connectomic patterns. Models were assessed across a range of metrics including accuracy, AUC score, and adversarial robustness. Results: GNNs outperformed other models across both populations. Notably, adult GNN models achieved 85.1% accuracy in sex classification on unseen adult participants, consistent with prior studies. The extension of the adult models to the pediatric dataset and training on the smaller pediatric dataset were sub-optimal in their performance. Using adult data to augment pediatric models, the best GNN achieved comparable accuracy across unseen pediatric (83.0%) and adult (81.3%) participants. Adversarial sensitivity experiments showed that the simple GCN remained the most robust to perturbations, followed by the multi-layer perceptron and the residual GCN. Conclusions: These findings underscore the potential of GNNs in advancing our understanding of sex-specific neurological development and disorders and highlight the importance of data augmentation in overcoming challenges associated with small pediatric datasets. Further, they highlight relevant tradeoffs in the design landscape of connectomic GNNs. For example, while the simpler GNN model tested exhibits marginally worse accuracy and AUC scores in comparison to the more complex residual GNN, it demonstrates a higher degree of adversarial robustness.</p></abstract><kwd-group><kwd>graph neural networks</kwd><kwd>structural brain connectomes</kwd><kwd>data enrichment</kwd></kwd-group><funding-group><award-group><funding-source>Cancer Center Support Grant</funding-source><award-id>P30 CA21765</award-id><award-id>R01 CA174794</award-id></award-group><award-group><funding-source>National Cancer Institute and ALSAC</funding-source><award-id>R25 CA23944</award-id></award-group><funding-statement>This research was supported in part by Cancer Center Support Grant P30 CA21765, R01 CA174794 (K.R.K. and N.D.S.), and R25 CA23944 (A.S.) from the National Cancer Institute and ALSAC.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-tomography-11-00014"><title>1. Introduction</title><p>The human structural connectome represents an individual&#x02019;s brain connectivity network by quantifying the white matter tracts connecting anatomical gray matter regions in the brain using diffusion-weighted magnetic resonance imaging (DW-MRI). Structural connectome data has been a valuable tool for linking brain structure to neurological function and dysfunction [<xref rid="B1-tomography-11-00014" ref-type="bibr">1</xref>]. Specifically, it has enhanced our understanding of typical brain development [<xref rid="B2-tomography-11-00014" ref-type="bibr">2</xref>,<xref rid="B3-tomography-11-00014" ref-type="bibr">3</xref>] as well as various neurological disorders, including epilepsy [<xref rid="B4-tomography-11-00014" ref-type="bibr">4</xref>,<xref rid="B5-tomography-11-00014" ref-type="bibr">5</xref>,<xref rid="B6-tomography-11-00014" ref-type="bibr">6</xref>], schizophrenia [<xref rid="B7-tomography-11-00014" ref-type="bibr">7</xref>,<xref rid="B8-tomography-11-00014" ref-type="bibr">8</xref>], Alzheimer&#x02019;s disease [<xref rid="B9-tomography-11-00014" ref-type="bibr">9</xref>,<xref rid="B10-tomography-11-00014" ref-type="bibr">10</xref>,<xref rid="B11-tomography-11-00014" ref-type="bibr">11</xref>,<xref rid="B12-tomography-11-00014" ref-type="bibr">12</xref>], and autism [<xref rid="B12-tomography-11-00014" ref-type="bibr">12</xref>,<xref rid="B13-tomography-11-00014" ref-type="bibr">13</xref>].</p><p>Due to its promising clinical relevance, the structural connectome has been the focus of multiple machine learning studies [<xref rid="B14-tomography-11-00014" ref-type="bibr">14</xref>,<xref rid="B15-tomography-11-00014" ref-type="bibr">15</xref>,<xref rid="B16-tomography-11-00014" ref-type="bibr">16</xref>]. Among these, structural connectome classification stands out as a clinically important task, as classification algorithms could facilitate early diagnosis of neurological disorders and improve our understanding of relevant clinical characteristics at a neuroanatomical level. Such analysis employs graph theory, where the structural connectome represents a brain graph G = (V, E), in which brain regions make up the set of vertices V, and tract connections between these regions make up the set of weighted edges E (with the number of tracts between two regions determining the relative weight of the corresponding edge).</p><p>Although the structural connectome includes many descriptive cerebral features, its inherent graph structure poses challenges for traditional learning methods. Conventional machine learning and deep learning approaches, which are designed for Euclidean data like text and images, struggle with non-Euclidean graph data due to their inability to capture a graph&#x02019;s topological structure [<xref rid="B17-tomography-11-00014" ref-type="bibr">17</xref>]. Graph neural networks (GNNs) were developed to address these shortcomings [<xref rid="B18-tomography-11-00014" ref-type="bibr">18</xref>]. GNNs are capable of learning and preserving topological patterns, enabling the training of generalizable, high-performance models on graph data [<xref rid="B19-tomography-11-00014" ref-type="bibr">19</xref>].</p><p>Sex classification has been a major focal point of previous brain connectivity research [<xref rid="B20-tomography-11-00014" ref-type="bibr">20</xref>,<xref rid="B21-tomography-11-00014" ref-type="bibr">21</xref>,<xref rid="B22-tomography-11-00014" ref-type="bibr">22</xref>]. Differentiating sex in the structural connectome may lead to a better understanding of neurological disorders with sex-specific presentations. For example, studies have shown sex-specific brain connectivity patterns in patients with autism [<xref rid="B23-tomography-11-00014" ref-type="bibr">23</xref>], mild cognitive impairment [<xref rid="B24-tomography-11-00014" ref-type="bibr">24</xref>], and conduct disorder [<xref rid="B25-tomography-11-00014" ref-type="bibr">25</xref>], which could help explain the differing presentations of these disorders across sexes. Furthermore, studies show differing developmental brain connectivity patterns between male and female adolescents [<xref rid="B22-tomography-11-00014" ref-type="bibr">22</xref>,<xref rid="B26-tomography-11-00014" ref-type="bibr">26</xref>]. Developing connectivity-based sex prediction models for pediatric patients therefore could improve the understanding of brain development in children and aid in identifying risk factors and early diagnosis of certain neurological disorders [<xref rid="B20-tomography-11-00014" ref-type="bibr">20</xref>]. Moreover, since early machine learning works on the structural connectome focused on sex and standard demographic information, this information is routinely available in most publicly available datasets. Sex classification also represents an intuitive classification benchmark for assessing how models perform across structural connectome datasets [<xref rid="B27-tomography-11-00014" ref-type="bibr">27</xref>].</p><p>While GNN models have achieved reasonable success in classifying sex in adult patients [<xref rid="B20-tomography-11-00014" ref-type="bibr">20</xref>,<xref rid="B27-tomography-11-00014" ref-type="bibr">27</xref>], this work has not been extended to pediatric populations. Pediatric patients exhibit distinct brain connectivity patterns, particularly in developing white matter tracts, which form the edges of the structural connectome [<xref rid="B3-tomography-11-00014" ref-type="bibr">3</xref>,<xref rid="B28-tomography-11-00014" ref-type="bibr">28</xref>,<xref rid="B29-tomography-11-00014" ref-type="bibr">29</xref>]. Consequently, models trained on adult structural connectome data may not be directly applicable to pediatric patients. Moreover, the typically smaller size of pediatric datasets presents additional challenges in training effective pediatric models.</p><p>One approach to address this problem of small sample sizes in pediatric datasets is to use few-shot leaning, which learns to make accurate classifications by training on a very small number of labeled cases when suitable training data is scarce. Few-shot learning approaches for GNNs in connectomics are not well studied [<xref rid="B19-tomography-11-00014" ref-type="bibr">19</xref>]. Here, as a few-shot learning approach, a small pediatric dataset was enriched with a much larger adult data to test if GNN models were able to leverage this additional connectomic data to learn to make more accurate predictions.</p><p>In this study, GNNs and other standard machine learning models were trained on whole brain structural connectome data for sex classification of both pediatric and adult participants. Classification accuracy for the larger adult dataset was compared to previous studies to compare the performance of the learning models [<xref rid="B20-tomography-11-00014" ref-type="bibr">20</xref>,<xref rid="B27-tomography-11-00014" ref-type="bibr">27</xref>]. Trained adult, pediatric, and enriched adult/pediatric models were tested on the pediatric dataset [<xref rid="B20-tomography-11-00014" ref-type="bibr">20</xref>].</p><p>After exploring training approaches using two selected GNN architectures and three other deep and machine learning models for comparison, a series of GNN architecture exploration experiments were conducted to determine the impact of pooling and aggregation function, model depth, and skip connections on the ability for GNNs to capture connectomic patterns. Further, adversarial robustness experiments were conducted to assess how models responded to adversarial attacks. These experiments sought to characterize the landscape of design choices and performance tradeoffs for connectomic models.</p><p>The primary objectives of this study are to evaluate whether an enriched training approach enables the GNN to generalize across age groups, achieving pediatric classification accuracy comparable to that of adult GNN models tested on adult datasets and to explore the impact of GNN architectural design choices on connectomic learning ability.</p></sec><sec id="sec2-tomography-11-00014"><title>2. Materials and Methods</title><sec id="sec2dot1-tomography-11-00014"><title>2.1. Datasets</title><p>The Human Connectome Project in Development (HCP-D) sought to characterize brain connectivity over the course of typical development. The pediatric HCP-D dataset included 135 healthy participants, ranging from 8 to 20 years of age with a mean age of 16 years. Participants with health conditions that might have impacted typical development or jeopardized their inclusion within the dataset were excluded. 3D T1 weighted images and DW-MRI sequences were acquired with 3T Siemens Prisma scanners in two shells of b = 1500 and 3000 s/mm<sup>2</sup> along 185 gradient directions [<xref rid="B3-tomography-11-00014" ref-type="bibr">3</xref>].</p><p>The BRIGHT dataset included 147 adult survivors of childhood Hodgkin&#x02019;s lymphoma (HL) and 162 control participants recruited to frequency match survivors across sex, age, and race/ethnicity. All survivors were 18 years of age or older with a mean age of 36 years. All survivors received thoracic radiation during initial treatment and were 5 or more years from diagnosis at the time of data collection. 3D T1 weighted images and DW-MRI sequences were acquired with a 3T scanner in a single shell of b = 700 s/mm<sup>2</sup> along 30 gradient directions. Demographics from both cohorts are reported in <xref rid="tomography-11-00014-t001" ref-type="table">Table 1</xref>.</p></sec><sec id="sec2dot2-tomography-11-00014"><title>2.2. Structural Connectome Processing</title><p>The processing pipeline includes multiple workflows implemented in succession. First, raw DW-MRI data were corrected for noise, motion, and Eddy currents [<xref rid="B30-tomography-11-00014" ref-type="bibr">30</xref>]. Images were then analyzed using a spherical deconvolution diffusion model to generate fiber orientation distribution functions (FOD) and probabilistic tractography was performed to attain whole brain tractograms using the MRtrix3 (<uri xlink:href="https://www.mrtrix.org">https://www.mrtrix.org</uri> (accessed on 12 March 2023)) iFOD2 algorithm [<xref rid="B31-tomography-11-00014" ref-type="bibr">31</xref>]. The whole brain streamlines were filtered using the SIFT2 method to extract realistic streamlines [<xref rid="B32-tomography-11-00014" ref-type="bibr">32</xref>]. Whole brain structural connectomes were computed for each subject based on weighted streamline density from the whole brain tractograms and based on the parcellations in the HCP-MMP1.0 atlas consisting of 379 cortical and sub-cortical regions as nodes [<xref rid="B33-tomography-11-00014" ref-type="bibr">33</xref>].</p></sec><sec id="sec2dot3-tomography-11-00014"><title>2.3. Graph Convolutional Network (GCN) Model Definition</title><p>Let a dataset <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of size N be defined as <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. The <italic toggle="yes">i</italic>-th patient in dataset <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> can be described as <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the patient&#x02019;s structural connectivity graph with its corresponding node feature matrix <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and class label <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. Any patient graph <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> can also be represented as an adjacency matrix <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. A model function <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>&#x02192;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was defined where <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the set of learnable parameters and <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the model&#x02019;s predicted class label. The objective was to learn a function <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> which minimizes the classification loss over the dataset <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>This study focuses on GNN models, specifically Graph Convolutional Networks (GCNs). GCNs aim to create s-dimensional node embeddings for each node in a graph <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. These individual node embeddings can subsequently be pooled into a single s-dimensional graph embedding vector, which can finally be fed into a downstream classifier for prediction of the output label. GCNs generate node embeddings through an iterative, message passing algorithm. Nodes send messages to their surrounding neighborhoods, and these messages are aggregated and used to update each node&#x02019;s current embedding.</p><p>A single layer of a GCN model can be defined as:<disp-formula id="FD1-tomography-11-00014"><label>(1)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">&#x003c3;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">*</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-tomography-11-00014"><label>(2)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is the normalized adjacency matrix representation of the patient graph <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, which is derived from structured connectome data. <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is a symmetric matrix with dimensions corresponding to the number of nodes in the graph. In this study, we utilized 379 nodes defined by the HCP-MMP1.0 atlas for the whole brain, resulting in <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> being a 379 &#x000d7; 379 matrix. <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is the node embedding matrix from the previous iteration, <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the learnable weight matrix, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the learnable bias term, <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003c3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the non-linear activation function. In this study, we use the ReLU (Rectified Linear Unit) and tanh activation functions for &#x003c3;. <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is the resulting node embedding matrix of the GCN layer at the <italic toggle="yes">i</italic>-th iteration. The node embedding matrix is initialized with the node feature matrix in the first iteration. In the above equation, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the messages passed from each node to its surrounding neighborhood, and <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">*</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the weighted aggregation of these outgoing messages based upon the strength of the connection between nodes in the normalized adjacency matrix. Individual GCN layers can be stacked or joined via skip connections to build larger networks.</p></sec><sec id="sec2dot4-tomography-11-00014"><title>2.4. Model Architecture</title><p>Two GCN models (simple [<xref rid="B34-tomography-11-00014" ref-type="bibr">34</xref>] and residual), a deep neural network (multi-layer perceptron (MLP)), and two standard machine learning models (random forest (RF) and support vector machine (SVM)) were trained. Random Forest and SVM were selected as baseline models to enable a meaningful comparison with GNNs. Random Forest was chosen for its resilience to noise, capability to manage high-dimensional data like structural connectome features. SVM was included for its proven effectiveness in binary classification tasks and its capacity to capture non-linear relationships through kernel methods. Unlike GNNs, which directly utilize the graph structure of the connectome, these models rely on predefined features, offering a contrasting perspective. This comparison allowed us to highlight the benefits of graph-based learning and demonstrate the superior performance of GNNs in this context. Model architectures for both GCN models and the MLP are shown in <xref rid="tomography-11-00014-f001" ref-type="fig">Figure 1</xref>, and model sizes for these models are displayed in <xref rid="tomography-11-00014-t002" ref-type="table">Table 2</xref>. The simple GCN model contains GCN layers with 64 neurons each, while the residual GCN model contains GCN layers with 32 neurons each. The simple GCN model uses ReLU activation functions between graph convolutional layers and mean global pooling for the generation of a graph embedding vector, while the residual GCN model uses tanh activation and mean aggregation functions between graph convolutional layers to create intermediate graph embedding matrices. The model architecture for the residual GCN is adapted from a model proposed in a brain connectomics benchmarking paper [<xref rid="B5-tomography-11-00014" ref-type="bibr">5</xref>]. For the MLP network, hidden layers with 512, 256, and 128 neurons are used with ReLU activation. All models were implemented with PyTorch Geometric (version 2.7.0).</p></sec><sec id="sec2dot5-tomography-11-00014"><title>2.5. Training Procedure</title><p>The hyperparameter space for all deep learning models was explored extensively. All deep learning models trained with a learning rate of 1 &#x000d7; 10<sup>&#x02212;3</sup> for 100 epochs using weighted binary cross entropy loss. A number of regularization techniques were employed to mitigate overfitting. 50% dropout was added between GCN layers and fully connected layers for GNN and MLP models, respectively. All deep learning models were trained using a weight decay of 5 &#x000d7; 10<sup>&#x02212;4</sup>, and early stopping was adopted with a patience of 35 epochs.</p><p>For the RF and SVM models, principal component analysis was used to reduce connectivity matrix data to 100 dimensions. The random forest model was initialized with 100 estimators, and the support vector machine applies the radial basis function (rbf) kernel. All other parameters are left as default.</p><p>Data was split with 70% for training, 10% for validation, and 20% testing in all deep learning experiments. For the two standard machine learning models (RF and SVM), 80% of the data was used for training and the remaining 20% was used for testing. Five-fold cross validation was employed to generate five unique testing sets. The mean accuracy and AUC scores across all sets is reported.</p></sec><sec id="sec2dot6-tomography-11-00014"><title>2.6. Model Evaluation Experiments</title><p>First, all models were trained and tested against the two available datasets individually. For experiments on individual datasets, data from a single dataset was split into training, validation, and testing sets and each model&#x02019;s mean accuracy and AUC score on the unseen test data is reported. Models trained on the adult BRIGHT dataset were also externally validated on pediatric HCP-D data to test how well structural connectivity models trained solely on adult participants generalize to pediatric participants.</p><p>Finally, models were trained and tested on the adult-enriched pediatric dataset. Data from both datasets was shuffled together and split into training, validation, and testing sets. The ratio of adults and pediatric data was held constant across all sets. Each model&#x02019;s accuracy on the unseen test data was reported both as an overall score and stratified by the dataset of origin.</p></sec><sec id="sec2dot7-tomography-11-00014"><title>2.7. GNN Architecture Exploration Experiments</title><p>After initial model evaluations, a variety of architectural exploration and ablation experiments were conducted to characterize the impact and effect of different model components on performance.</p><p>Mean and max pooling and aggregation approaches were compared to investigate the impact of these pooling and aggregation methods on the ability of the GNN models to learn connectomic patterns. The simple GCN, which initially used a mean global pooling function to generate a graph embedding vector, was tested with a max global pooling function replacement, while the residual GCN, which uses a mean node aggregation for graph feature generation, was tested with a max aggregation function replacement. Both tests were performed using the adult-enriched dataset for training and evaluation.</p><p>Next, the effect of model depth was investigated for both GNNs. The simple GNN was initially evaluated with two GCN layers. Deeper analogs with additional GCN layers were tested to determine if increasingly depth would improve the model. The residual GCN was initially evaluated with three GCN layers. Two- and four-layer analogs were tested on the adult-enriched dataset to determine the effect of decreasing or increasing the depth on the residual network.</p><p>Finally, an ablation experiment was performed to test how skip connections contributed to the ability of the residual GCN model to learn on connectomic data. All skip connections were removed, and the resulting model was trained and tested on the adult-enriched dataset.</p></sec><sec id="sec2dot8-tomography-11-00014"><title>2.8. Adversarial Sensitivity Experiments</title><p>Adversarial attacks are small, targeted perturbations applied to input data in order to mislead classification models. In the medical domain, robustness to adversarial attacks is important, as errors caused by such attacks could result in harmful or even fatal consequences. Further, a model&#x02019;s sensitivity to small adversarial perturbations may be an indicator of robustness to general noise, which is an inevitable component of real-world medical data.</p><p>Adversarial robustness for all deep learning models was evaluated using the fast gradient sign method (FGSM) adversarial attack. FGSM is a white-box adversarial evasion attack which can be defined as:<disp-formula id="FD3-tomography-11-00014"><label>(3)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">d</mml:mi><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">&#x003f5;</mml:mi><mml:mo mathvariant="bold-italic">&#x02217;</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mo>&#x02207;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a node feature matrix corresponding to an input graph <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the loss as function of the model&#x02019;s trainable parameters <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the node feature matrix <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the corresponding label <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003f5;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the size of the adversarial perturbation, and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">d</mml:mi><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is the resulting perturbed node feature matrix.</p><p>White box attacks like FGSM enjoy full access to a model&#x02019;s loss function and trainable parameters. As such, they can construct powerful, targeted adversarial examples. Increasing the value of epsilon increases the distance between the original graph and the adversarial example, therefore leading to stronger adversarial attacks. All deep learning models trained on the adult-enriched dataset were tested on target adversarial examples generated by the FGSM method for a range of epsilon values. The average resulting accuracy and AUC scores across the five cross validation test sets are reported.</p><p>All code used in this study is publicly available: <uri xlink:href="https://github.com/SrinivasanAnand/GNN_structural_connectivity">https://github.com/SrinivasanAnand/GNN_structural_connectivity</uri> (accessed on 10 December 2024).</p></sec></sec><sec sec-type="results" id="sec3-tomography-11-00014"><title>3. Results</title><sec id="sec3dot1-tomography-11-00014"><title>3.1. Adult Training and Adult Testing</title><p>The two standard machine learning models RF and SVM, the deep learning neural network MLP, and the two GCN models (simple and residual) were trained and tested on the adult BRIGHT dataset. The corresponding accuracy and AUC score for each model is shown in <xref rid="tomography-11-00014-t003" ref-type="table">Table 3</xref>. The GCN models outperform others on the adult dataset, with accuracies exceeding 82%, followed by the multi-layer perceptron model at 77%. GCN models also demonstrate the best AUC scores, exceeding 0.90 for both models.</p></sec><sec id="sec3dot2-tomography-11-00014"><title>3.2. Adult Training and Pediatric Testing</title><p>All classification models were next trained on the adult BRIGHT dataset and tested on the pediatric HCP-D dataset. Experimental results are shown in <xref rid="tomography-11-00014-t004" ref-type="table">Table 4</xref>. The residual GCN model remains the most robust, achieving an accuracy above 74% and an AUC score of 0.86. All other models lagged with accuracies of 50&#x02013;60%. Although the residual GCN exhibits a degree of robustness, all models trained on adult data performed sub-optimally for classification of the pediatric datasets.</p></sec><sec id="sec3dot3-tomography-11-00014"><title>3.3. Pediatric Training and Pediatric Testing</title><p>Given the anticipated sub-optimal performance of the adult models for external validation on the pediatric dataset, all classification models were next trained and tested on the pediatric HCP-D dataset. The corresponding accuracy and AUC score for each model is shown in <xref rid="tomography-11-00014-t005" ref-type="table">Table 5</xref>. The simple GCN model demonstrated the highest accuracy in the low-70% range, while the residual GCN achieved the highest AUC score of 0.85. The residual GCN shows strong discriminative power in terms of AUC score, but all models achieve sub-optimal classification accuracy.</p></sec><sec id="sec3dot4-tomography-11-00014"><title>3.4. Adult-Enriched Pediatric Dataset Training and Testing</title><p>Given the sub-optimal performance of the models trained on either the adult or pediatric datasets for classification of the pediatric testing set, an adult-enriched pediatric dataset was then used for both training and testing. <xref rid="tomography-11-00014-t006" ref-type="table">Table 6</xref> displays the performance of the models when classifying the HCP-D pediatric data within the adult-enriched pediatric testing dataset. The residual GCN model excels in the pediatric subset of the test set, achieving an accuracy of 83%, and the simple GCN model reaches 79% classification accuracy. These model performances demonstrate a noticeable improvement compared to the next best classifier within this training approach and any other classifier&#x02019;s performance across all training approaches on the HCP-D pediatric dataset.</p><p>For completeness, the overall classification accuracies of the models of the adult-enriched pediatric dataset are presented in <xref rid="tomography-11-00014-f002" ref-type="fig">Figure 2</xref> for the overall testing set and the sub-sets of adult and pediatrics. Overall, the GCN models demonstrate the strongest performance across accuracy and AUC metrics. Notably, the multi-layer perceptron also exhibits strong classification ability under this training approach.</p><p>Loss curves for all deep learning models from a representative cross validation split for this training approach are shown in <xref rid="tomography-11-00014-f003" ref-type="fig">Figure 3</xref>. All models exhibit an ability to learn connectomic features. Both GCNs show signs of overfitting, but this trend is more pronounced in the residual model.</p><p>Receiver operating characteristic (ROC) curves on the training and test set for all models from a representative cross validation split are depicted in <xref rid="tomography-11-00014-f004" ref-type="fig">Figure 4</xref>. Three ROC curves are displayed for each test set, corresponding to the overall test set and the pediatric and adult stratifications. As follows from the AUC score results, the GCN models demonstrate the strongest ROC curves.</p></sec><sec id="sec3dot5-tomography-11-00014"><title>3.5. GNN Architecture Exploration Results</title><p>Architecture exploration experiments were conducted in order to determine the impact of architectural design choices on the ability of the GNN models to learn connectomic patterns. Since the focus of this study is the pediatric connectome, pediatric classification accuracy and AUC scores were used as the primary measure of model performance for exploration experiments. All models were trained on the adult-enriched pediatric dataset since this training method was previously determined to produce the strongest models.</p><p>Max and mean pooling and aggregation types were compared across the two GCN models. <xref rid="tomography-11-00014-t007" ref-type="table">Table 7</xref> and <xref rid="tomography-11-00014-t008" ref-type="table">Table 8</xref> display accuracy and AUC scores for the simple and residual GCN models trained with mean and max pooling and aggregation approaches on the combined dataset. The choice of pooling or aggregation type does not show any major effect.</p><p>Simple and residual GCN models of varying depth were tested to assess the impact of depth on learning ability. Results for depth experiments are presented in <xref rid="tomography-11-00014-f005" ref-type="fig">Figure 5</xref>. Again, no major effect is observed, although it is noteworthy that the 3-layer residual network outperforms its 2-layer and 4-layer analogs by 3 accuracy points. For the simple GCN model architecture, shallow networks perform just as well as deeper ones.</p><p>Skip ablation experimental results are shown in <xref rid="tomography-11-00014-t009" ref-type="table">Table 9</xref>. Removing skip connections from the residual GCN network results in a large deterioration in performance, with a 14-point accuracy drop on the pediatric stratification of the dataset, highlighting the importance of skip connections for allowing the residual network to learn connectomic patterns.</p></sec><sec id="sec3dot6-tomography-11-00014"><title>3.6. Adversarial Sensitivity Experiment Results</title><p>All deep learning models trained on the adult-enriched dataset were tested on adversarial test sets, generated by targeted FGSM attacks on the pediatric stratification of the enriched test dataset. Perturbation sizes between 1.0 &#x000d7; 10<sup>&#x02212;5</sup> and 1.0 &#x000d7; 10<sup>&#x02212;3</sup> were tested. Accuracy and AUC score results for all deep learning models on perturbed examples are shown in <xref rid="tomography-11-00014-f006" ref-type="fig">Figure 6</xref>. The simple GCN model remains the most robust to adversarial attacks, followed by the multi-layer perceptron and the residual GCN, respectively.</p></sec></sec><sec sec-type="discussion" id="sec4-tomography-11-00014"><title>4. Discussion</title><p>Experimental results on the adult BRIGHT dataset indicated that graph neural networks outperform traditional machine learning and deep learning approaches in classifying adult structural connectome data. The highest classification accuracy (85.1%) was achieved by the simple GCN model, which surpasses the best non-GCN model by 7.5%. These findings were consistent with the 86.7% reported in previous studies [<xref rid="B20-tomography-11-00014" ref-type="bibr">20</xref>,<xref rid="B27-tomography-11-00014" ref-type="bibr">27</xref>].</p><p>To assess the robustness of the models trained on the adult dataset, external validation was conducted using the pediatric dataset, allowing us to evaluate the generalizability of these models across different patient age demographics. The results reveal that most models were not robust to this demographic shift, an outcome that was anticipated given the inherent variability between the two datasets. The adult dataset, comprising both HL survivors and community controls, used a slightly different diffusion imaging protocol compared to the pediatric dataset, which included only healthy controls. Moreover, it is well-documented that adults exhibit distinct structural connectivity patterns compared to neurologically developing children and teenagers [<xref rid="B3-tomography-11-00014" ref-type="bibr">3</xref>,<xref rid="B22-tomography-11-00014" ref-type="bibr">22</xref>,<xref rid="B26-tomography-11-00014" ref-type="bibr">26</xref>]. Given these distinctions, it is unsurprising that most models trained on the adult BRIGHT dataset struggled to generalize the HCP-D data.</p><p>However, it is noteworthy that the residual GCN model exhibited a degree of robustness in the external validation, maintaining an accuracy of over 74% on the pediatric dataset. This performance suggests that the residual GCN model could capture highly generalizable patterns within graph data, extending applicability from adult participants to their pediatric counterparts. Consequently, robust adult-trained graph neural network connectome models may hold potential for direct application to pediatric populations in future clinical settings. This observed generalizability could be particularly valuable in extending the benefits of future connectome-based algorithms to pediatric participants, especially given that the limited size of pediatric datasets often constrains the development of models trained exclusively on pediatric data.</p><p>Experiments on the pediatric HCP-D dataset demonstrated that all models were unable to learn highly generalizable patterns on the pediatric connectome without any form of data enrichment. The best model tested was the simple GCN model, which achieves a classification accuracy of 71.1%. The 14-point performance gap observed between the top-performing adult and pediatric models is likely attributable to the smaller size of the pediatric dataset, a limitation that subsequent experiments aimed to address.</p><p>An adult-enriched pediatric dataset training approach was employed to assess whether this method could yield stronger models for pediatric participants. This approach functions as a form of data enrichment, utilizing adult patient data to enhance the pediatric dataset and thereby facilitate the training of more robust pediatric models. Additionally, a subset of the adult data was reserved to enable testing of each model&#x02019;s performance on both unseen pediatric and adult datasets. The enrichment approach allowed deep learning models to achieve strong classification performance, with the residual GCN reaching 81.8% and the simple GCN and multi-layer perceptron both following at 79.3%. Notably, the residual GCN model also performed exceptionally well across both pediatric and adult participants, attaining an accuracy of 83.0% and 81.3%, respectively. The pediatric test accuracy of the residual GCN model is higher than that of all other pediatric models across all training approaches, narrowing the gap between the best achievable pediatric and adult test accuracies to within about two percentage points (83.0 vs. 85.1%). Thus, enriching smaller pediatric datasets with adult structural connectome data can enhance model performance, enabling the development of pediatric models that achieve performance levels comparable to state-of-the-art adult models. While this study enriches pediatric data using adult data to improve model performance, alternative strategies for addressing the limited availability of pediatric connectome datasets should be explored. One potential approach is to collaborate with other pediatric research institutes or initiatives, which may provide access to larger and more diverse pediatric datasets. Examples include publicly available repositories like the Pediatric Imaging, Neurocognition, and Genetics (PING) study [<xref rid="B35-tomography-11-00014" ref-type="bibr">35</xref>] or the ABCD dataset [<xref rid="B36-tomography-11-00014" ref-type="bibr">36</xref>], which, although limited in diffusion MRI, could still complement structural connectome analyses. Another promising strategy involves leveraging data augmentation techniques specific to connectome graphs, such as perturbation or simulation of biologically plausible variations. These approaches could mitigate the dependence on adult data enrichment while ensuring that the model remains focused on pediatric specific characteristics.</p><p>The limited availability of structural connectome data led to the selection of an adult dataset comprising both HL survivors and community controls, while the pediatric dataset included only healthy controls. These datasets were collected using slightly different DW-MRI protocols including different gradient directions and b-values. Additionally, preprocessing pipelines varied, with adult dataset employing standard motion correction and eddy current correction, while pediatric dataset incorporated advanced susceptibility artifact correction. These variations can impact the derived connectome features, with lower b-values or few-er directions potentially reducing tractography accuracy and voxel resolution differences influencing connectivity matrix representations. Consequently, the observed poor external validation results may reflect these protocol differences rather than a lack of model robustness to the population demographics. While this study focuses on improving model generalizability, future work could benefit from using datasets collected with consistent DW-MRI protocols to better isolate the effects of patient demographics. Alternatively, protocol harmonization techniques, such as advanced preprocessing or domain adaptation methods, could mitigate these challenges and enhance model robustness. Despite these limitations, the inclusion of datasets with differing DW-MRI protocols reflects the practical challenges of applying machine learning models across diverse clinical and research settings. Models that generalize well across such variations are more likely to succeed in real-world applications, making this an important avenue for further investigation. Nevertheless, the residual GCN model&#x02019;s strong external validation performance suggested its capacity to learn highly generalizable patterns that extend from adult to pediatric participants. These generalizable patterns likely represent biologically relevant and stable features within structural connectomes. For instance, the model appeared to leverage connectivity profiles of key hub regions such as the precuneus, posterior cingulate cortex, and superior frontal gyrus, which are central nodes in the brain&#x02019;s structural network and play a crucial role in maintaining global communication [<xref rid="B37-tomography-11-00014" ref-type="bibr">37</xref>,<xref rid="B38-tomography-11-00014" ref-type="bibr">38</xref>,<xref rid="B39-tomography-11-00014" ref-type="bibr">39</xref>]. These regions are recognized as stable and central nodes, showing relatively low variability across demographic groups [<xref rid="B40-tomography-11-00014" ref-type="bibr">40</xref>,<xref rid="B41-tomography-11-00014" ref-type="bibr">41</xref>]. Additionally, strong patterns were reported previously in inter-hemispheric connections, particularly in commissural pathways like the corpus callosum. These connections are highly conserved across individuals and age groups, making them reliable features for model generalization [<xref rid="B42-tomography-11-00014" ref-type="bibr">42</xref>,<xref rid="B43-tomography-11-00014" ref-type="bibr">43</xref>,<xref rid="B44-tomography-11-00014" ref-type="bibr">44</xref>,<xref rid="B45-tomography-11-00014" ref-type="bibr">45</xref>,<xref rid="B46-tomography-11-00014" ref-type="bibr">46</xref>]. The model likely captured patterns of hierarchical organization, including connectivity differences between primary sensory-motor regions, which mature earlier, and higher-order associative areas, which exhibit prolonged development and greater integration over time. These patterns align with established neurodevelopmental trajectories and demonstrate consistency across age groups [<xref rid="B47-tomography-11-00014" ref-type="bibr">47</xref>,<xref rid="B48-tomography-11-00014" ref-type="bibr">48</xref>,<xref rid="B49-tomography-11-00014" ref-type="bibr">49</xref>,<xref rid="B50-tomography-11-00014" ref-type="bibr">50</xref>,<xref rid="B51-tomography-11-00014" ref-type="bibr">51</xref>]. These patterns suggest that the residual GCN model leveraged features that are biologically consistent and less variable across demographics, contributing to its generalizability.</p><p>The differences between the two datasets may have limited the effectiveness of the adult-enriched pediatric dataset training approach. Ideally, data enrichment should be performed using data that closely resembles the original dataset. The pediatric dataset in this study covered ages 8&#x02013;20 years, while the adult dataset ranged from 18&#x02013;65 years with a mean age of 35 years. The overlap between the older pediatric participants (18&#x02013;20 years) and younger adults (18&#x02013;20 years) helped bridge the two datasets and reduce discontinuities in connectome patterns. Furthermore, the relatively stable nature of structural connectomes in adulthood supports the use of a broader adult age range [<xref rid="B52-tomography-11-00014" ref-type="bibr">52</xref>,<xref rid="B53-tomography-11-00014" ref-type="bibr">53</xref>,<xref rid="B54-tomography-11-00014" ref-type="bibr">54</xref>]. To minimize potential biases, stratified sampling was used during training and cross-validation to ensure proportional representation of both groups. While these strategies mitigated bias, future work could refine this approach by exploring harmonization methods or focusing on more age-restricted datasets to ensure even greater neurodevelopmental consistency. Additionally, employing an adult dataset composed solely of healthy controls and collected using the same DW-MRI protocol as the pediatric dataset might have further enhanced the models&#x02019; performance. Future research should explore the efficacy of the data enrichment approach using other adult and pediatric datasets across a range of tasks to validate its ability to improve pediatric models. Although the enrichment of pediatric datasets using adult structural connectomes improved classification performance, achieving 83% accuracy on pediatric data, further gains might be possible through explicit domain adaptation techniques. Methods such as adversarial domain adaptation, which aligns feature distributions between source (adult) and target (pediatric) domains, could help address inherent differences in brain connectivity patterns between age groups. Additionally, transfer learning approaches, where a model pre-trained on adult data is fine-tuned on pediatric data, could allow the model to learn pediatric-specific features more effectively. These strategies could complement the enrichment approach and enhance the generalizability and robustness of the GNN for pediatric classification tasks.</p><p>Graph neural network architecture exploration experiments elucidated the impact of several architectural design choices on model performance and learning ability. Depth experiments revealed that increasing the number of GCN layers did not improve the simple GCN model. A moderate 3% accuracy improvement was observed on the pediatric stratification of the combined test set on increasing the depth of the residual GCN from 2 to 3 layers, but the further addition of a fourth GCN layer led to a performance drop on the pediatric stratification. These results indicate that for connectomic applications, a shallow depth of 2 to 3 GCN layers is sufficient for strong performance.</p><p>Architecture experiments also highlighted the importance of skip connections for the residual GCN model&#x02019;s ability to learn pediatric connectomic patterns. An ablation study removing skip connections resulted in a 14-point performance drop in pediatric classification accuracy. Because the only major distinction between the ablated residual network and the simple GCN model was the use of aggregation versus pooling in order to create an embedded graph representation, these results may suggest that connectomic models which employ node aggregation approaches should introduce skip connections for optimal performance.</p><p>In addition to accuracy and AUC score metrics, models were assessed on adversarial robustness. The simple GCN model demonstrated the strongest adversarial robustness to FGSM attacks for larger perturbations. The residual GCN, on the other hand, demonstrated a notable lack of adversarial robustness, falling to below 40% classification accuracy for adversarial attacks with epsilon greater than or equal to 0.0005. The residual GCN model&#x02019;s lack of robustness may in part be a result of its higher complexity. Previous work has shown that higher complexity models exhibit greater sensitivity to adversarial attacks [<xref rid="B55-tomography-11-00014" ref-type="bibr">55</xref>]. Further, training and validation loss curves illustrated that the residual GCN overfitted the training set. It is likely that the residual GCN model learned patterns derived from noise in the training set, making it more susceptible to adversarial attacks. Therefore, although the residual GCN model&#x02019;s higher complexity likely allowed it to learn more generalizable connectomic patterns than the other tested models, this complexity also resulted in a weakness to adversarial attacks. This potential tradeoff between raw test accuracy and adversarial robustness should be considered during model development, especially in the medical domain, where robustness is a key consideration for deploying trustworthy models.</p><p>While structural connectome data has recently been applied in graph classification tasks (e.g., sex classification) and graph prediction tasks (e.g., functional connectome prediction), graph regression tasks such as cognitive score prediction remain largely unexplored. These regression tasks hold potential for linking specific brain structural patterns to cognitive functions, thereby enhancing our understanding of cognitive processes and dysfunctions. Although both datasets selected for this study included measures of working memory and sustained attention for most participants, models developed here were not successful in predicting cognitive scores from structural connectome data. Future research with larger datasets or alternative learning approaches may provide better outcomes in this area.</p></sec><sec sec-type="conclusions" id="sec5-tomography-11-00014"><title>5. Conclusions</title><p>This study demonstrated the effectiveness of GNNs in classifying sex based on structural connectome data, particularly in pediatric populations, a domain previously unexplored. Training and evaluating GNNs alongside standard machine learning models confirmed that GNNs outperformed other methods in both adult and pediatric datasets. While adult-trained models exhibit limited applicability to pediatric participants, enriching a pediatric dataset with adult data enables the development of a robust pediatric GNN model with performance comparable to adult models. A complex residual GCN architecture enables the best model performance in terms of classification accuracy and AUC score, while a simpler GCN architecture demonstrates greater adversarial robustness at the expense of a moderate performance drop. These findings underscored the potential of GNNs in advancing our understanding of sex-specific neurological development and disorders, highlighting important tradeoffs in the GNN architectural landscape and underscoring the importance of data enrichment in overcoming challenges associated with small pediatric datasets. Future research should continue to explore the application of GNNs to diverse clinical populations and investigate additional strategies for enhancing model generalizability across demographic groups.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.S., R.R. and W.E.R.; methodology, A.S. and R.R.; software, A.S., R.R. and J.O.G.; validation, A.S., R.R. and W.E.R.; formal analysis, A.S., R.R. and W.E.R.; investigation, A.S., R.R., J.O.G. and W.E.R.; resources, M.M.H., N.D.S., K.R.K. and W.E.R.; data curation, A.S., R.R. and W.E.R.; writing&#x02014;original draft preparation, A.S. and R.R.; writing&#x02014;review and editing, A.S., R.R., J.O.G., M.M.H., N.D.S., K.R.K. and W.E.R.; visualization, A.S. and W.E.R.; supervision, R.R. and W.E.R.; project administration, W.E.R.; funding acquisition, M.M.H., N.D.S., K.R.K. and W.E.R. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>The study was conducted in accordance with the Declaration of Helsinki, and approved by the Institutional Review Board of St. Jude Children&#x02019;s Research Hospital (Protocol BRIGHT Revision 4.2, approved: 3 September 2017).</p></notes><notes><title>Informed Consent Statement</title><p>Patient consent was waived due to secondary use of research data exempted under Category 4 of the federal regulations because the information is recorded by the investigator in such a manner that the resulting dataset contains no information that can identify subjects, directly or through identifiers linked to the subjects.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Pediatric dHCP data is publicly available. All code used in this study is publicly available: <uri xlink:href="https://github.com/SrinivasanAnand/GNN_structural_connectivity">https://github.com/SrinivasanAnand/GNN_structural_connectivity</uri>.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-tomography-11-00014"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Griffa</surname><given-names>A.</given-names></name>
<name><surname>Baumann</surname><given-names>P.S.</given-names></name>
<name><surname>Thiran</surname><given-names>J.P.</given-names></name>
<name><surname>Hagmann</surname><given-names>P.</given-names></name>
</person-group><article-title>Structural connectomics in brain diseases</article-title><source>Neuroimage</source><year>2013</year><volume>80</volume><fpage>515</fpage><lpage>526</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.056</pub-id><pub-id pub-id-type="pmid">23623973</pub-id>
</element-citation></ref><ref id="B2-tomography-11-00014"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dennis</surname><given-names>E.L.</given-names></name>
<name><surname>Jahanshad</surname><given-names>N.</given-names></name>
<name><surname>McMahon</surname><given-names>K.L.</given-names></name>
<name><surname>de Zubicaray</surname><given-names>G.I.</given-names></name>
<name><surname>Martin</surname><given-names>N.G.</given-names></name>
<name><surname>Hickie</surname><given-names>I.B.</given-names></name>
<name><surname>Toga</surname><given-names>A.W.</given-names></name>
<name><surname>Wright</surname><given-names>M.J.</given-names></name>
<name><surname>Thompson</surname><given-names>P.M.</given-names></name>
</person-group><article-title>Development of brain structural connectivity between ages 12 and 30: A 4-Tesla diffusion imaging study in 439 adolescents and adults</article-title><source>Neuroimage</source><year>2013</year><volume>64</volume><fpage>671</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.09.004</pub-id><pub-id pub-id-type="pmid">22982357</pub-id>
</element-citation></ref><ref id="B3-tomography-11-00014"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Somerville</surname><given-names>L.H.</given-names></name>
<name><surname>Bookheimer</surname><given-names>S.Y.</given-names></name>
<name><surname>Buckner</surname><given-names>R.L.</given-names></name>
<name><surname>Burgess</surname><given-names>G.C.</given-names></name>
<name><surname>Curtiss</surname><given-names>S.W.</given-names></name>
<name><surname>Dapretto</surname><given-names>M.</given-names></name>
<name><surname>Elam</surname><given-names>J.S.</given-names></name>
<name><surname>Gaffrey</surname><given-names>M.S.</given-names></name>
<name><surname>Harms</surname><given-names>M.P.</given-names></name>
<name><surname>Hodge</surname><given-names>C.</given-names></name>
<etal/>
</person-group><article-title>The Lifespan Human Connectome Project in Development: A large-scale study of brain connectivity development in 5&#x02013;21 year olds</article-title><source>Neuroimage</source><year>2018</year><volume>183</volume><fpage>456</fpage><lpage>468</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.08.050</pub-id><pub-id pub-id-type="pmid">30142446</pub-id>
</element-citation></ref><ref id="B4-tomography-11-00014"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bernhardt</surname><given-names>B.C.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
<name><surname>Evans</surname><given-names>A.C.</given-names></name>
<name><surname>Bernasconi</surname><given-names>N.</given-names></name>
</person-group><article-title>Graph-theoretical analysis reveals disrupted small-world organization of cortical thickness correlation networks in temporal lobe epilepsy</article-title><source>Cereb. Cortex</source><year>2011</year><volume>21</volume><fpage>2147</fpage><lpage>2157</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq291</pub-id><pub-id pub-id-type="pmid">21330467</pub-id>
</element-citation></ref><ref id="B5-tomography-11-00014"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gerster</surname><given-names>M.</given-names></name>
<name><surname>Taher</surname><given-names>H.</given-names></name>
<name><surname>Skoch</surname><given-names>A.</given-names></name>
<name><surname>Hlinka</surname><given-names>J.</given-names></name>
<name><surname>Guye</surname><given-names>M.</given-names></name>
<name><surname>Bartolomei</surname><given-names>F.</given-names></name>
<name><surname>Jirsa</surname><given-names>V.</given-names></name>
<name><surname>Zakharova</surname><given-names>A.</given-names></name>
<name><surname>Olmi</surname><given-names>S.</given-names></name>
</person-group><article-title>Patient-Specific Network Connectivity Combined with a Next Generation Neural Mass Model to Test Clinical Hypothesis of Seizure Propagation</article-title><source>Front. Syst. Neurosci.</source><year>2021</year><volume>15</volume><elocation-id>675272</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2021.675272</pub-id><pub-id pub-id-type="pmid">34539355</pub-id>
</element-citation></ref><ref id="B6-tomography-11-00014"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jirsa</surname><given-names>V.K.</given-names></name>
<name><surname>Proix</surname><given-names>T.</given-names></name>
<name><surname>Perdikis</surname><given-names>D.</given-names></name>
<name><surname>Woodman</surname><given-names>M.M.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Gonzalez-Martinez</surname><given-names>J.</given-names></name>
<name><surname>Bernard</surname><given-names>C.</given-names></name>
<name><surname>Benar</surname><given-names>C.</given-names></name>
<name><surname>Guye</surname><given-names>M.</given-names></name>
<name><surname>Chauvel</surname><given-names>P.</given-names></name>
<etal/>
</person-group><article-title>The Virtual Epileptic Patient: Individualized whole-brain models of epilepsy spread</article-title><source>Neuroimage</source><year>2017</year><volume>145</volume><fpage>377</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.04.049</pub-id><pub-id pub-id-type="pmid">27477535</pub-id>
</element-citation></ref><ref id="B7-tomography-11-00014"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Lin</surname><given-names>L.</given-names></name>
<name><surname>Lin</surname><given-names>C.P.</given-names></name>
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
<name><surname>Chou</surname><given-names>K.H.</given-names></name>
<name><surname>Lo</surname><given-names>C.Y.</given-names></name>
<name><surname>Su</surname><given-names>T.P.</given-names></name>
<name><surname>Jiang</surname><given-names>T.</given-names></name>
</person-group><article-title>Abnormal topological organization of structural brain networks in schizophrenia</article-title><source>Schizophr. Res.</source><year>2012</year><volume>141</volume><fpage>109</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1016/j.schres.2012.08.021</pub-id><pub-id pub-id-type="pmid">22981811</pub-id>
</element-citation></ref><ref id="B8-tomography-11-00014"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Skudlarski</surname><given-names>P.</given-names></name>
<name><surname>Jagannathan</surname><given-names>K.</given-names></name>
<name><surname>Anderson</surname><given-names>K.</given-names></name>
<name><surname>Stevens</surname><given-names>M.C.</given-names></name>
<name><surname>Calhoun</surname><given-names>V.D.</given-names></name>
<name><surname>Skudlarska</surname><given-names>B.A.</given-names></name>
<name><surname>Pearlson</surname><given-names>G.</given-names></name>
</person-group><article-title>Brain connectivity is not only lower but different in schizophrenia: A combined anatomical and functional approach</article-title><source>Biol. Psychiatry</source><year>2010</year><volume>68</volume><fpage>61</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2010.03.035</pub-id><pub-id pub-id-type="pmid">20497901</pub-id>
</element-citation></ref><ref id="B9-tomography-11-00014"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yao</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Lin</surname><given-names>L.</given-names></name>
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
<name><surname>Xu</surname><given-names>C.</given-names></name>
<name><surname>Jiang</surname><given-names>T.</given-names></name>
<collab>Alzheimer&#x02019;s Disease Neuroimaging Initiative</collab>
</person-group><article-title>Abnormal cortical networks in mild cognitive impairment and Alzheimer&#x02019;s disease</article-title><source>PLoS Comput. Biol.</source><year>2010</year><volume>6</volume><elocation-id>e1001006</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1001006</pub-id><pub-id pub-id-type="pmid">21124954</pub-id>
</element-citation></ref><ref id="B10-tomography-11-00014"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Khosrowabadi</surname><given-names>R.</given-names></name>
<name><surname>Ng</surname><given-names>K.K.</given-names></name>
<name><surname>Hong</surname><given-names>Z.</given-names></name>
<name><surname>Chong</surname><given-names>J.S.X.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>C.Y.</given-names></name>
<name><surname>Hilal</surname><given-names>S.</given-names></name>
<name><surname>Venketasubramanian</surname><given-names>N.</given-names></name>
<name><surname>Wong</surname><given-names>T.Y.</given-names></name>
<etal/>
</person-group><article-title>Alterations in Brain Network Topology and Structural-Functional Connectome Coupling Relate to Cognitive Impairment</article-title><source>Front. Aging Neurosci.</source><year>2018</year><volume>10</volume><elocation-id>404</elocation-id><pub-id pub-id-type="doi">10.3389/fnagi.2018.00404</pub-id><pub-id pub-id-type="pmid">30618711</pub-id>
</element-citation></ref><ref id="B11-tomography-11-00014"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lo</surname><given-names>C.Y.</given-names></name>
<name><surname>Wang</surname><given-names>P.N.</given-names></name>
<name><surname>Chou</surname><given-names>K.H.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
<name><surname>Lin</surname><given-names>C.P.</given-names></name>
</person-group><article-title>Diffusion tensor tractography reveals abnormal topological organization in structural cortical networks in Alzheimer&#x02019;s disease</article-title><source>J. Neurosci.</source><year>2010</year><volume>30</volume><fpage>16876</fpage><lpage>16885</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4136-10.2010</pub-id><pub-id pub-id-type="pmid">21159959</pub-id>
</element-citation></ref><ref id="B12-tomography-11-00014"><label>12.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Tolan</surname><given-names>E.</given-names></name>
<name><surname>Isik</surname><given-names>Z.</given-names></name>
</person-group><article-title>Graph Theory Based Classification of Brain Connectivity Network for Autism Spectrum Disorder</article-title><source>Proceedings of the Bioinformatics and Biomedical Engineering: 6th International Work-Conference, IWBBIO 2018, Granada, Spain, 25&#x02013;27 April 2018</source><comment>Proceedings, Part I 6</comment><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2018</year><fpage>520</fpage><lpage>530</lpage></element-citation></ref><ref id="B13-tomography-11-00014"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Xue</surname><given-names>Z.</given-names></name>
<name><surname>Ellmore</surname><given-names>T.M.</given-names></name>
<name><surname>Frye</surname><given-names>R.E.</given-names></name>
<name><surname>Wong</surname><given-names>S.T.</given-names></name>
</person-group><article-title>Network-based analysis reveals stronger local diffusion-based connectivity and different correlations with oral language skills in brains of children with high functioning autism spectrum disorders</article-title><source>Hum. Brain Mapp.</source><year>2014</year><volume>35</volume><fpage>396</fpage><lpage>413</lpage><pub-id pub-id-type="doi">10.1002/hbm.22185</pub-id><pub-id pub-id-type="pmid">23008187</pub-id>
</element-citation></ref><ref id="B14-tomography-11-00014"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tymofiyeva</surname><given-names>O.</given-names></name>
<name><surname>Yuan</surname><given-names>J.P.</given-names></name>
<name><surname>Huang</surname><given-names>C.Y.</given-names></name>
<name><surname>Connolly</surname><given-names>C.G.</given-names></name>
<name><surname>Henje Blom</surname><given-names>E.</given-names></name>
<name><surname>Xu</surname><given-names>D.</given-names></name>
<name><surname>Yang</surname><given-names>T.T.</given-names></name>
</person-group><article-title>Application of machine learning to structural connectome to predict symptom reduction in depressed adolescents with cognitive behavioral therapy (CBT)</article-title><source>Neuroimage Clin.</source><year>2019</year><volume>23</volume><fpage>101914</fpage><pub-id pub-id-type="doi">10.1016/j.nicl.2019.101914</pub-id><pub-id pub-id-type="pmid">31491813</pub-id>
</element-citation></ref><ref id="B15-tomography-11-00014"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kamiya</surname><given-names>K.</given-names></name>
<name><surname>Amemiya</surname><given-names>S.</given-names></name>
<name><surname>Suzuki</surname><given-names>Y.</given-names></name>
<name><surname>Kunii</surname><given-names>N.</given-names></name>
<name><surname>Kawai</surname><given-names>K.</given-names></name>
<name><surname>Mori</surname><given-names>H.</given-names></name>
<name><surname>Kunimatsu</surname><given-names>A.</given-names></name>
<name><surname>Saito</surname><given-names>N.</given-names></name>
<name><surname>Aoki</surname><given-names>S.</given-names></name>
<name><surname>Ohtomo</surname><given-names>K.</given-names></name>
</person-group><article-title>Machine Learning of DTI Structural Brain Connectomes for Lateralization of Temporal Lobe Epilepsy</article-title><source>Magn. Reson. Med. Sci.</source><year>2016</year><volume>15</volume><fpage>121</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.2463/mrms.2015-0027</pub-id><pub-id pub-id-type="pmid">26346404</pub-id>
</element-citation></ref><ref id="B16-tomography-11-00014"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>V.C.</given-names></name>
<name><surname>Lin</surname><given-names>T.Y.</given-names></name>
<name><surname>Yeh</surname><given-names>D.C.</given-names></name>
<name><surname>Chai</surname><given-names>J.W.</given-names></name>
<name><surname>Weng</surname><given-names>J.C.</given-names></name>
</person-group><article-title>Functional and Structural Connectome Features for Machine Learning Chemo-Brain Prediction in Women Treated for Breast Cancer with Chemotherapy</article-title><source>Brain Sci.</source><year>2020</year><volume>10</volume><elocation-id>851</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci10110851</pub-id><pub-id pub-id-type="pmid">33198294</pub-id>
</element-citation></ref><ref id="B17-tomography-11-00014"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>J.</given-names></name>
<name><surname>Cui</surname><given-names>G.</given-names></name>
<name><surname>Hu</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Yang</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Sun</surname><given-names>M.</given-names></name>
</person-group><article-title>Graph neural networks: A review of methods and applications</article-title><source>AI Open</source><year>2020</year><volume>1</volume><fpage>57</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/j.aiopen.2021.01.001</pub-id></element-citation></ref><ref id="B18-tomography-11-00014"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Scarselli</surname><given-names>F.</given-names></name>
<name><surname>Gori</surname><given-names>M.</given-names></name>
<name><surname>Tsoi</surname><given-names>A.C.</given-names></name>
<name><surname>Hagenbuchner</surname><given-names>M.</given-names></name>
<name><surname>Monfardini</surname><given-names>G.</given-names></name>
</person-group><article-title>The graph neural network model</article-title><source>IEEE Trans. Neural Netw.</source><year>2009</year><volume>20</volume><fpage>61</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1109/TNN.2008.2005605</pub-id><pub-id pub-id-type="pmid">19068426</pub-id>
</element-citation></ref><ref id="B19-tomography-11-00014"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bessadok</surname><given-names>A.</given-names></name>
<name><surname>Mahjoub</surname><given-names>M.A.</given-names></name>
<name><surname>Rekik</surname><given-names>I.</given-names></name>
</person-group><article-title>Graph Neural Networks in Network Neuroscience</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>45</volume><fpage>5833</fpage><lpage>5848</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3209686</pub-id><pub-id pub-id-type="pmid">36155474</pub-id>
</element-citation></ref><ref id="B20-tomography-11-00014"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Kazi</surname><given-names>A.</given-names></name>
<name><surname>Mora</surname><given-names>J.</given-names></name>
<name><surname>Fischl</surname><given-names>B.</given-names></name>
<name><surname>Dalca</surname><given-names>A.V.</given-names></name>
<name><surname>Aganj</surname><given-names>I.</given-names></name>
</person-group><article-title>Multi-Head Graph Convolutional Network for Structural Connectome Classification</article-title><source>Proceedings of the Graphs in Biomedical Image Analysis, and Overlapped Cell on Tissue Dataset for Histopathology: 5th MICCAI Workshop, GRAIL 2023 and 1st MICCAI Challenge, OCELOT 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, 23 September, and 4 October 2023, Proceedings</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2024</year><volume>Volume 14373</volume><fpage>27</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1007/978-3-031-55088-1_3</pub-id></element-citation></ref><ref id="B21-tomography-11-00014"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Weis</surname><given-names>S.</given-names></name>
<name><surname>Patil</surname><given-names>K.R.</given-names></name>
<name><surname>Hoffstaedter</surname><given-names>F.</given-names></name>
<name><surname>Nostro</surname><given-names>A.</given-names></name>
<name><surname>Yeo</surname><given-names>B.T.T.</given-names></name>
<name><surname>Eickhoff</surname><given-names>S.B.</given-names></name>
</person-group><article-title>Sex Classification by Resting State Brain Connectivity</article-title><source>Cereb. Cortex</source><year>2020</year><volume>30</volume><fpage>824</fpage><lpage>835</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhz129</pub-id><pub-id pub-id-type="pmid">31251328</pub-id>
</element-citation></ref><ref id="B22-tomography-11-00014"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gur</surname><given-names>R.E.</given-names></name>
<name><surname>Gur</surname><given-names>R.C.</given-names></name>
</person-group><article-title>Sex differences in brain and behavior in adolescence: Findings from the Philadelphia Neurodevelopmental Cohort</article-title><source>Neurosci. Biobehav. Rev.</source><year>2016</year><volume>70</volume><fpage>159</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.07.035</pub-id><pub-id pub-id-type="pmid">27498084</pub-id>
</element-citation></ref><ref id="B23-tomography-11-00014"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Beacher</surname><given-names>F.D.</given-names></name>
<name><surname>Minati</surname><given-names>L.</given-names></name>
<name><surname>Baron-Cohen</surname><given-names>S.</given-names></name>
<name><surname>Lombardo</surname><given-names>M.V.</given-names></name>
<name><surname>Lai</surname><given-names>M.C.</given-names></name>
<name><surname>Gray</surname><given-names>M.A.</given-names></name>
<name><surname>Harrison</surname><given-names>N.A.</given-names></name>
<name><surname>Critchley</surname><given-names>H.D.</given-names></name>
</person-group><article-title>Autism attenuates sex differences in brain structure: A combined voxel-based morphometry and diffusion tensor imaging study</article-title><source>AJNR Am. J. Neuroradiol.</source><year>2012</year><volume>33</volume><fpage>83</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.3174/ajnr.A2880</pub-id><pub-id pub-id-type="pmid">22173769</pub-id>
</element-citation></ref><ref id="B24-tomography-11-00014"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Williamson</surname><given-names>J.</given-names></name>
<name><surname>Yabluchanskiy</surname><given-names>A.</given-names></name>
<name><surname>Mukli</surname><given-names>P.</given-names></name>
<name><surname>Wu</surname><given-names>D.H.</given-names></name>
<name><surname>Sonntag</surname><given-names>W.</given-names></name>
<name><surname>Ciro</surname><given-names>C.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Sex differences in brain functional connectivity of hippocampus in mild cognitive impairment</article-title><source>Front. Aging Neurosci.</source><year>2022</year><volume>14</volume><elocation-id>959394</elocation-id><pub-id pub-id-type="doi">10.3389/fnagi.2022.959394</pub-id><pub-id pub-id-type="pmid">36034134</pub-id>
</element-citation></ref><ref id="B25-tomography-11-00014"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Gao</surname><given-names>J.</given-names></name>
<name><surname>Shi</surname><given-names>H.</given-names></name>
<name><surname>Huang</surname><given-names>B.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Situ</surname><given-names>W.</given-names></name>
<name><surname>Cai</surname><given-names>W.</given-names></name>
<name><surname>Yi</surname><given-names>J.</given-names></name>
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<name><surname>Yao</surname><given-names>S.</given-names></name>
</person-group><article-title>Sex differences of uncinate fasciculus structural connectivity in individuals with conduct disorder</article-title><source>Biomed. Res. Int.</source><year>2014</year><volume>2014</volume><elocation-id>673165</elocation-id><pub-id pub-id-type="doi">10.1155/2014/673165</pub-id><pub-id pub-id-type="pmid">24829912</pub-id>
</element-citation></ref><ref id="B26-tomography-11-00014"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tyan</surname><given-names>Y.S.</given-names></name>
<name><surname>Liao</surname><given-names>J.R.</given-names></name>
<name><surname>Shen</surname><given-names>C.Y.</given-names></name>
<name><surname>Lin</surname><given-names>Y.C.</given-names></name>
<name><surname>Weng</surname><given-names>J.C.</given-names></name>
</person-group><article-title>Gender differences in the structural connectome of the teenage brain revealed by generalized q-sampling MRI</article-title><source>Neuroimage Clin.</source><year>2017</year><volume>15</volume><fpage>376</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1016/j.nicl.2017.05.014</pub-id><pub-id pub-id-type="pmid">28580294</pub-id>
</element-citation></ref><ref id="B27-tomography-11-00014"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Said</surname><given-names>A.</given-names></name>
<name><surname>Bayrak</surname><given-names>R.G.</given-names></name>
<name><surname>Derr</surname><given-names>T.</given-names></name>
<name><surname>Shabbir</surname><given-names>M.</given-names></name>
<name><surname>Moyer</surname><given-names>D.</given-names></name>
<name><surname>Chang</surname><given-names>C.</given-names></name>
<name><surname>Koutsoukos</surname><given-names>X.</given-names></name>
</person-group><article-title>NeuroGraph: Benchmarks for graph machine learning in brain connectomics</article-title><source>Proceedings of the 37th International Conference on Neural Information Processing Systems</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>10&#x02013;16 December 2023</conf-date></element-citation></ref><ref id="B28-tomography-11-00014"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Supekar</surname><given-names>K.</given-names></name>
<name><surname>Uddin</surname><given-names>L.Q.</given-names></name>
<name><surname>Prater</surname><given-names>K.</given-names></name>
<name><surname>Amin</surname><given-names>H.</given-names></name>
<name><surname>Greicius</surname><given-names>M.D.</given-names></name>
<name><surname>Menon</surname><given-names>V.</given-names></name>
</person-group><article-title>Development of functional and structural connectivity within the default mode network in young children</article-title><source>Neuroimage</source><year>2010</year><volume>52</volume><fpage>290</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.04.009</pub-id><pub-id pub-id-type="pmid">20385244</pub-id>
</element-citation></ref><ref id="B29-tomography-11-00014"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mitchell</surname><given-names>W.J.</given-names></name>
<name><surname>Tepfer</surname><given-names>L.J.</given-names></name>
<name><surname>Henninger</surname><given-names>N.M.</given-names></name>
<name><surname>Perlman</surname><given-names>S.B.</given-names></name>
<name><surname>Murty</surname><given-names>V.P.</given-names></name>
<name><surname>Helion</surname><given-names>C.</given-names></name>
</person-group><article-title>Developmental Differences in Affective Representation Between Prefrontal and Subcortical Structures</article-title><source>Soc. Cogn. Affect. Neurosci.</source><year>2021</year><volume>17</volume><fpage>311</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1093/scan/nsab093</pub-id><pub-id pub-id-type="pmid">34331538</pub-id>
</element-citation></ref><ref id="B30-tomography-11-00014"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Smith</surname><given-names>S.M.</given-names></name>
<name><surname>Jenkinson</surname><given-names>M.</given-names></name>
<name><surname>Woolrich</surname><given-names>M.W.</given-names></name>
<name><surname>Beckmann</surname><given-names>C.F.</given-names></name>
<name><surname>Behrens</surname><given-names>T.E.</given-names></name>
<name><surname>Johansen-Berg</surname><given-names>H.</given-names></name>
<name><surname>Bannister</surname><given-names>P.R.</given-names></name>
<name><surname>De Luca</surname><given-names>M.</given-names></name>
<name><surname>Drobnjak</surname><given-names>I.</given-names></name>
<name><surname>Flitney</surname><given-names>D.E.</given-names></name>
<etal/>
</person-group><article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title><source>Neuroimage</source><year>2004</year><volume>23</volume><issue>(Suppl. S1)</issue><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id><pub-id pub-id-type="pmid">15501092</pub-id>
</element-citation></ref><ref id="B31-tomography-11-00014"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tournier</surname><given-names>J.D.</given-names></name>
<name><surname>Smith</surname><given-names>R.</given-names></name>
<name><surname>Raffelt</surname><given-names>D.</given-names></name>
<name><surname>Tabbara</surname><given-names>R.</given-names></name>
<name><surname>Dhollander</surname><given-names>T.</given-names></name>
<name><surname>Pietsch</surname><given-names>M.</given-names></name>
<name><surname>Christiaens</surname><given-names>D.</given-names></name>
<name><surname>Jeurissen</surname><given-names>B.</given-names></name>
<name><surname>Yeh</surname><given-names>C.H.</given-names></name>
<name><surname>Connelly</surname><given-names>A.</given-names></name>
</person-group><article-title>MRtrix3: A fast, flexible and open software framework for medical image processing and visualisation</article-title><source>Neuroimage</source><year>2019</year><volume>202</volume><fpage>116137</fpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116137</pub-id><pub-id pub-id-type="pmid">31473352</pub-id>
</element-citation></ref><ref id="B32-tomography-11-00014"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Smith</surname><given-names>R.E.</given-names></name>
<name><surname>Tournier</surname><given-names>J.D.</given-names></name>
<name><surname>Calamante</surname><given-names>F.</given-names></name>
<name><surname>Connelly</surname><given-names>A.</given-names></name>
</person-group><article-title>SIFT2: Enabling dense quantitative assessment of brain white matter connectivity using streamlines tractography</article-title><source>Neuroimage</source><year>2015</year><volume>119</volume><fpage>338</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.06.092</pub-id><pub-id pub-id-type="pmid">26163802</pub-id>
</element-citation></ref><ref id="B33-tomography-11-00014"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Glasser</surname><given-names>M.F.</given-names></name>
<name><surname>Coalson</surname><given-names>T.S.</given-names></name>
<name><surname>Robinson</surname><given-names>E.C.</given-names></name>
<name><surname>Hacker</surname><given-names>C.D.</given-names></name>
<name><surname>Harwell</surname><given-names>J.</given-names></name>
<name><surname>Yacoub</surname><given-names>E.</given-names></name>
<name><surname>Ugurbil</surname><given-names>K.</given-names></name>
<name><surname>Andersson</surname><given-names>J.</given-names></name>
<name><surname>Beckmann</surname><given-names>C.F.</given-names></name>
<name><surname>Jenkinson</surname><given-names>M.</given-names></name>
<etal/>
</person-group><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><year>2016</year><volume>536</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/nature18933</pub-id><pub-id pub-id-type="pmid">27437579</pub-id>
</element-citation></ref><ref id="B34-tomography-11-00014"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kipf</surname><given-names>T.</given-names></name>
<name><surname>Welling</surname><given-names>M.</given-names></name>
</person-group><article-title>Semi-Supervised Classification with Graph Convolutional Networks</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="doi">10.48550/arXiv.1609.02907</pub-id><pub-id pub-id-type="arxiv">1609.02907</pub-id></element-citation></ref><ref id="B35-tomography-11-00014"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jernigan</surname><given-names>T.L.</given-names></name>
<name><surname>Brown</surname><given-names>T.T.</given-names></name>
<name><surname>Hagler</surname><given-names>D.J.</given-names><suffix>Jr.</suffix></name>
<name><surname>Akshoomoff</surname><given-names>N.</given-names></name>
<name><surname>Bartsch</surname><given-names>H.</given-names></name>
<name><surname>Newman</surname><given-names>E.</given-names></name>
<name><surname>Thompson</surname><given-names>W.K.</given-names></name>
<name><surname>Bloss</surname><given-names>C.S.</given-names></name>
<name><surname>Murray</surname><given-names>S.S.</given-names></name>
<name><surname>Schork</surname><given-names>N.</given-names></name>
<etal/>
</person-group><article-title>The Pediatric Imaging, Neurocognition, and Genetics (PING) Data Repository</article-title><source>Neuroimage</source><year>2016</year><volume>124</volume><fpage>1149</fpage><lpage>1154</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.04.057</pub-id><pub-id pub-id-type="pmid">25937488</pub-id>
</element-citation></ref><ref id="B36-tomography-11-00014"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Casey</surname><given-names>B.J.</given-names></name>
<name><surname>Cannonier</surname><given-names>T.</given-names></name>
<name><surname>Conley</surname><given-names>M.I.</given-names></name>
<name><surname>Cohen</surname><given-names>A.O.</given-names></name>
<name><surname>Barch</surname><given-names>D.M.</given-names></name>
<name><surname>Heitzeg</surname><given-names>M.M.</given-names></name>
<name><surname>Soules</surname><given-names>M.E.</given-names></name>
<name><surname>Teslovich</surname><given-names>T.</given-names></name>
<name><surname>Dellarco</surname><given-names>D.V.</given-names></name>
<name><surname>Garavan</surname><given-names>H.</given-names></name>
<etal/>
</person-group><article-title>The Adolescent Brain Cognitive Development (ABCD) study: Imaging acquisition across 21 sites</article-title><source>Dev. Cogn. Neurosci.</source><year>2018</year><volume>32</volume><fpage>43</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.dcn.2018.03.001</pub-id><pub-id pub-id-type="pmid">29567376</pub-id>
</element-citation></ref><ref id="B37-tomography-11-00014"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bullmore</surname><given-names>E.</given-names></name>
<name><surname>Sporns</surname><given-names>O.</given-names></name>
</person-group><article-title>Complex brain networks: Graph theoretical analysis of structural and functional systems</article-title><source>Nat. Rev. Neurosci.</source><year>2009</year><volume>10</volume><fpage>186</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1038/nrn2575</pub-id><pub-id pub-id-type="pmid">19190637</pub-id>
</element-citation></ref><ref id="B38-tomography-11-00014"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hagmann</surname><given-names>P.</given-names></name>
<name><surname>Cammoun</surname><given-names>L.</given-names></name>
<name><surname>Gigandet</surname><given-names>X.</given-names></name>
<name><surname>Meuli</surname><given-names>R.</given-names></name>
<name><surname>Honey</surname><given-names>C.J.</given-names></name>
<name><surname>Wedeen</surname><given-names>V.J.</given-names></name>
<name><surname>Sporns</surname><given-names>O.</given-names></name>
</person-group><article-title>Mapping the structural core of human cerebral cortex</article-title><source>PLoS Biol.</source><year>2008</year><volume>6</volume><elocation-id>e159</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060159</pub-id><pub-id pub-id-type="pmid">18597554</pub-id>
</element-citation></ref><ref id="B39-tomography-11-00014"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>van den Heuvel</surname><given-names>M.P.</given-names></name>
<name><surname>Sporns</surname><given-names>O.</given-names></name>
</person-group><article-title>Network hubs in the human brain</article-title><source>Trends Cogn. Sci.</source><year>2013</year><volume>17</volume><fpage>683</fpage><lpage>696</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.09.012</pub-id><pub-id pub-id-type="pmid">24231140</pub-id>
</element-citation></ref><ref id="B40-tomography-11-00014"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tomasi</surname><given-names>D.</given-names></name>
<name><surname>Volkow</surname><given-names>N.D.</given-names></name>
</person-group><article-title>Functional connectivity hubs in the human brain</article-title><source>Neuroimage</source><year>2011</year><volume>57</volume><fpage>908</fpage><lpage>917</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.05.024</pub-id><pub-id pub-id-type="pmid">21609769</pub-id>
</element-citation></ref><ref id="B41-tomography-11-00014"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Collin</surname><given-names>G.</given-names></name>
<name><surname>Sporns</surname><given-names>O.</given-names></name>
<name><surname>Mandl</surname><given-names>R.C.</given-names></name>
<name><surname>van den Heuvel</surname><given-names>M.P.</given-names></name>
</person-group><article-title>Structural and functional aspects relating to cost and benefit of rich club organization in the human cerebral cortex</article-title><source>Cereb. Cortex</source><year>2014</year><volume>24</volume><fpage>2258</fpage><lpage>2267</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht064</pub-id><pub-id pub-id-type="pmid">23551922</pub-id>
</element-citation></ref><ref id="B42-tomography-11-00014"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hofer</surname><given-names>S.</given-names></name>
<name><surname>Frahm</surname><given-names>J.</given-names></name>
</person-group><article-title>Topography of the human corpus callosum revisited&#x02014;Comprehensive fiber tractography using diffusion tensor magnetic resonance imaging</article-title><source>Neuroimage</source><year>2006</year><volume>32</volume><fpage>989</fpage><lpage>994</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.05.044</pub-id><pub-id pub-id-type="pmid">16854598</pub-id>
</element-citation></ref><ref id="B43-tomography-11-00014"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wahl</surname><given-names>M.</given-names></name>
<name><surname>Lauterbach-Soon</surname><given-names>B.</given-names></name>
<name><surname>Hattingen</surname><given-names>E.</given-names></name>
<name><surname>Jung</surname><given-names>P.</given-names></name>
<name><surname>Singer</surname><given-names>O.</given-names></name>
<name><surname>Volz</surname><given-names>S.</given-names></name>
<name><surname>Klein</surname><given-names>J.C.</given-names></name>
<name><surname>Steinmetz</surname><given-names>H.</given-names></name>
<name><surname>Ziemann</surname><given-names>U.</given-names></name>
</person-group><article-title>Human motor corpus callosum: Topography, somatotopy, and link between microstructure and function</article-title><source>J. Neurosci.</source><year>2007</year><volume>27</volume><fpage>12132</fpage><lpage>12138</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2320-07.2007</pub-id><pub-id pub-id-type="pmid">17989279</pub-id>
</element-citation></ref><ref id="B44-tomography-11-00014"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lebel</surname><given-names>C.</given-names></name>
<name><surname>Caverhill-Godkewitsch</surname><given-names>S.</given-names></name>
<name><surname>Beaulieu</surname><given-names>C.</given-names></name>
</person-group><article-title>Age-related regional variations of the corpus callosum identified by diffusion tensor tractography</article-title><source>Neuroimage</source><year>2010</year><volume>52</volume><fpage>20</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.03.072</pub-id><pub-id pub-id-type="pmid">20362683</pub-id>
</element-citation></ref><ref id="B45-tomography-11-00014"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jancke</surname><given-names>L.</given-names></name>
<name><surname>Staiger</surname><given-names>J.F.</given-names></name>
<name><surname>Schlaug</surname><given-names>G.</given-names></name>
<name><surname>Huang</surname><given-names>Y.</given-names></name>
<name><surname>Steinmetz</surname><given-names>H.</given-names></name>
</person-group><article-title>The relationship between corpus callosum size and forebrain volume</article-title><source>Cereb. Cortex</source><year>1997</year><volume>7</volume><fpage>48</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1093/cercor/7.1.48</pub-id><pub-id pub-id-type="pmid">9023431</pub-id>
</element-citation></ref><ref id="B46-tomography-11-00014"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ingalhalikar</surname><given-names>M.</given-names></name>
<name><surname>Smith</surname><given-names>A.</given-names></name>
<name><surname>Parker</surname><given-names>D.</given-names></name>
<name><surname>Satterthwaite</surname><given-names>T.D.</given-names></name>
<name><surname>Elliott</surname><given-names>M.A.</given-names></name>
<name><surname>Ruparel</surname><given-names>K.</given-names></name>
<name><surname>Hakonarson</surname><given-names>H.</given-names></name>
<name><surname>Gur</surname><given-names>R.E.</given-names></name>
<name><surname>Gur</surname><given-names>R.C.</given-names></name>
<name><surname>Verma</surname><given-names>R.</given-names></name>
</person-group><article-title>Sex differences in the structural connectome of the human brain</article-title><source>Proc. Natl. Acad. Sci. USA</source><year>2014</year><volume>111</volume><fpage>823</fpage><lpage>828</lpage><pub-id pub-id-type="doi">10.1073/pnas.1316909110</pub-id><pub-id pub-id-type="pmid">24297904</pub-id>
</element-citation></ref><ref id="B47-tomography-11-00014"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gilmore</surname><given-names>J.H.</given-names></name>
<name><surname>Knickmeyer</surname><given-names>R.C.</given-names></name>
<name><surname>Gao</surname><given-names>W.</given-names></name>
</person-group><article-title>Imaging structural and functional brain development in early childhood</article-title><source>Nat. Rev. Neurosci.</source><year>2018</year><volume>19</volume><fpage>123</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1038/nrn.2018.1</pub-id><pub-id pub-id-type="pmid">29449712</pub-id>
</element-citation></ref><ref id="B48-tomography-11-00014"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cao</surname><given-names>M.</given-names></name>
<name><surname>Huang</surname><given-names>H.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
</person-group><article-title>Developmental Connectomics from Infancy through Early Childhood</article-title><source>Trends Neurosci.</source><year>2017</year><volume>40</volume><fpage>494</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2017.06.003</pub-id><pub-id pub-id-type="pmid">28684174</pub-id>
</element-citation></ref><ref id="B49-tomography-11-00014"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sydnor</surname><given-names>V.J.</given-names></name>
<name><surname>Larsen</surname><given-names>B.</given-names></name>
<name><surname>Bassett</surname><given-names>D.S.</given-names></name>
<name><surname>Alexander-Bloch</surname><given-names>A.</given-names></name>
<name><surname>Fair</surname><given-names>D.A.</given-names></name>
<name><surname>Liston</surname><given-names>C.</given-names></name>
<name><surname>Mackey</surname><given-names>A.P.</given-names></name>
<name><surname>Milham</surname><given-names>M.P.</given-names></name>
<name><surname>Pines</surname><given-names>A.</given-names></name>
<name><surname>Roalf</surname><given-names>D.R.</given-names></name>
<etal/>
</person-group><article-title>Neurodevelopment of the association cortices: Patterns, mechanisms, and implications for psychopathology</article-title><source>Neuron</source><year>2021</year><volume>109</volume><fpage>2820</fpage><lpage>2846</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.06.016</pub-id><pub-id pub-id-type="pmid">34270921</pub-id>
</element-citation></ref><ref id="B50-tomography-11-00014"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Margulies</surname><given-names>D.S.</given-names></name>
<name><surname>Ghosh</surname><given-names>S.S.</given-names></name>
<name><surname>Goulas</surname><given-names>A.</given-names></name>
<name><surname>Falkiewicz</surname><given-names>M.</given-names></name>
<name><surname>Huntenburg</surname><given-names>J.M.</given-names></name>
<name><surname>Langs</surname><given-names>G.</given-names></name>
<name><surname>Bezgin</surname><given-names>G.</given-names></name>
<name><surname>Eickhoff</surname><given-names>S.B.</given-names></name>
<name><surname>Castellanos</surname><given-names>F.X.</given-names></name>
<name><surname>Petrides</surname><given-names>M.</given-names></name>
<etal/>
</person-group><article-title>Situating the default-mode network along a principal gradient of macroscale cortical organization</article-title><source>Proc. Natl. Acad. Sci. USA</source><year>2016</year><volume>113</volume><fpage>12574</fpage><lpage>12579</lpage><pub-id pub-id-type="doi">10.1073/pnas.1608282113</pub-id><pub-id pub-id-type="pmid">27791099</pub-id>
</element-citation></ref><ref id="B51-tomography-11-00014"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hagmann</surname><given-names>P.</given-names></name>
<name><surname>Sporns</surname><given-names>O.</given-names></name>
<name><surname>Madan</surname><given-names>N.</given-names></name>
<name><surname>Cammoun</surname><given-names>L.</given-names></name>
<name><surname>Pienaar</surname><given-names>R.</given-names></name>
<name><surname>Wedeen</surname><given-names>V.J.</given-names></name>
<name><surname>Meuli</surname><given-names>R.</given-names></name>
<name><surname>Thiran</surname><given-names>J.P.</given-names></name>
<name><surname>Grant</surname><given-names>P.E.</given-names></name>
</person-group><article-title>White matter maturation reshapes structural connectivity in the late developing human brain</article-title><source>Proc. Natl. Acad. Sci. USA</source><year>2010</year><volume>107</volume><fpage>19067</fpage><lpage>19072</lpage><pub-id pub-id-type="doi">10.1073/pnas.1009073107</pub-id><pub-id pub-id-type="pmid">20956328</pub-id>
</element-citation></ref><ref id="B52-tomography-11-00014"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dennis</surname><given-names>E.L.</given-names></name>
<name><surname>Thompson</surname><given-names>P.M.</given-names></name>
</person-group><article-title>Typical and atypical brain development: A review of neuroimaging studies</article-title><source>Dialogues Clin. Neurosci.</source><year>2013</year><volume>15</volume><fpage>359</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.31887/DCNS.2013.15.3/edennis</pub-id><pub-id pub-id-type="pmid">24174907</pub-id>
</element-citation></ref><ref id="B53-tomography-11-00014"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lebel</surname><given-names>C.</given-names></name>
<name><surname>Beaulieu</surname><given-names>C.</given-names></name>
</person-group><article-title>Longitudinal development of human brain wiring continues from childhood into adulthood</article-title><source>J. Neurosci.</source><year>2011</year><volume>31</volume><fpage>10937</fpage><lpage>10947</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5302-10.2011</pub-id><pub-id pub-id-type="pmid">21795544</pub-id>
</element-citation></ref><ref id="B54-tomography-11-00014"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Westlye</surname><given-names>L.T.</given-names></name>
<name><surname>Walhovd</surname><given-names>K.B.</given-names></name>
<name><surname>Dale</surname><given-names>A.M.</given-names></name>
<name><surname>Bjornerud</surname><given-names>A.</given-names></name>
<name><surname>Due-Tonnessen</surname><given-names>P.</given-names></name>
<name><surname>Engvig</surname><given-names>A.</given-names></name>
<name><surname>Grydeland</surname><given-names>H.</given-names></name>
<name><surname>Tamnes</surname><given-names>C.K.</given-names></name>
<name><surname>Ostby</surname><given-names>Y.</given-names></name>
<name><surname>Fjell</surname><given-names>A.M.</given-names></name>
</person-group><article-title>Life-span changes of the human brain white matter: Diffusion tensor imaging (DTI) and volumetry</article-title><source>Cereb. Cortex</source><year>2010</year><volume>20</volume><fpage>2055</fpage><lpage>2068</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp280</pub-id><pub-id pub-id-type="pmid">20032062</pub-id>
</element-citation></ref><ref id="B55-tomography-11-00014"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rodriguez</surname><given-names>D.</given-names></name>
<name><surname>Nayak</surname><given-names>T.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Krishnan</surname><given-names>R.</given-names></name>
<name><surname>Huang</surname><given-names>Y.</given-names></name>
</person-group><article-title>On the role of deep learning model complexity in adversarial robustness for medical images</article-title><source>BMC Med. Inform. Decis. Mak.</source><year>2022</year><volume>22</volume><elocation-id>160</elocation-id><pub-id pub-id-type="doi">10.1186/s12911-022-01891-w</pub-id><pub-id pub-id-type="pmid">35725429</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="tomography-11-00014-f001"><label>Figure 1</label><caption><p>Model architecture illustrations for simple GCN (<bold>a</bold>), residual GCN (<bold>b</bold>), and multi-layer perceptron (<bold>c</bold>) networks.</p></caption><graphic xlink:href="tomography-11-00014-g001" position="float"/></fig><fig position="float" id="tomography-11-00014-f002"><label>Figure 2</label><caption><p>Sex classification mean accuracy (<bold>a</bold>) and AUC score (<bold>b</bold>) results for random forest (RF), support vector machine (SVM), multi-layer perceptron (MLP), simple graph convolutional neural network (GCN simple), and residual graph convolutional neural network (GCN residual) classifiers trained on the adult-enriched pediatric dataset. Results for pediatric (blue), adult (dark gray), and overall (black) subsets of the test dataset were displayed. Standard deviations of classification accuracy shown by whiskers. AUC and accuracy score results are displayed in tabular format in (<bold>c</bold>). Red indicates overall best results; bold indicates overall second-best results.</p></caption><graphic xlink:href="tomography-11-00014-g002" position="float"/></fig><fig position="float" id="tomography-11-00014-f003"><label>Figure 3</label><caption><p>Representative training and validation loss curves for multi-layer perceptron (<bold>a</bold>), simple graph convolutional neural network (<bold>b</bold>), and residual graph convolutional neural network (<bold>c</bold>) classifiers trained on the adult-enriched pediatric dataset.</p></caption><graphic xlink:href="tomography-11-00014-g003" position="float"/></fig><fig position="float" id="tomography-11-00014-f004"><label>Figure 4</label><caption><p>Representative receiver operating characteristic (ROC) curves for multi-layer perceptron (<bold>a</bold>), simple graph convolutional neural network (<bold>b</bold>), residual graph convolutional neural network (<bold>c</bold>), support vector machine (<bold>d</bold>), and random forest (<bold>e</bold>) classifiers trained on the adult-enriched pediatric dataset. Three test set ROC curves are displayed for each model corresponding to pediatric (blue), adult (gray), and overall (black) subsets. A single ROC curve corresponding to the overall training set (green) is displayed for each model.</p></caption><graphic xlink:href="tomography-11-00014-g004" position="float"/></fig><fig position="float" id="tomography-11-00014-f005"><label>Figure 5</label><caption><p>Sex classification mean accuracy (<bold>a</bold>) and AUC score (<bold>b</bold>) results for simple graph convolutional neural network (simple GCN) and residual graph convolutional neural network (residual GCN) architectures with varying model depth analogs.</p></caption><graphic xlink:href="tomography-11-00014-g005" position="float"/></fig><fig position="float" id="tomography-11-00014-f006"><label>Figure 6</label><caption><p>Adversarial accuracy (<bold>a</bold>) and AUC score (<bold>b</bold>) results for multi-layer perceptron (MLP), simple graph convolutional neural network (GCN simple), and residual graph convolutional neural network (GCN residual) classifiers trained on the adult-enriched pediatric dataset.</p></caption><graphic xlink:href="tomography-11-00014-g006" position="float"/></fig><table-wrap position="float" id="tomography-11-00014-t001"><object-id pub-id-type="pii">tomography-11-00014-t001_Table 1</object-id><label>Table 1</label><caption><p>Summary statistics for adult BRIGHT and pediatric HCP-D datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Patient Population</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">N</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">% Female</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">BRIGHT</td><td align="center" valign="middle" rowspan="1" colspan="1">Adult</td><td align="center" valign="middle" rowspan="1" colspan="1">309</td><td align="center" valign="middle" rowspan="1" colspan="1">53.40</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HCP-D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pediatric</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">135</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.30</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tomography-11-00014-t002"><object-id pub-id-type="pii">tomography-11-00014-t002_Table 2</object-id><label>Table 2</label><caption><p>Model sizes for multi-layer perceptron (MLP), simple graph convolutional neural network (GCN simple), and residual graph convolutional neural network (GCN residual).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MLP</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GCN (Simple)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GCN (Residual)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of Model Parameters</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.37 &#x000d7; 10<sup>7</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.89 &#x000d7; 10<sup>4</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.78 &#x000d7; 10<sup>6</sup></td></tr></tbody></table></table-wrap><table-wrap position="float" id="tomography-11-00014-t003"><object-id pub-id-type="pii">tomography-11-00014-t003_Table 3</object-id><label>Table 3</label><caption><p>Sex classification mean accuracy and AUC score results for random forest (RF), support vector machine (SVM), multi-layer perceptron (MLP), simple graph convolutional neural network (GCN simple), and residual graph convolutional neural network (GCN residual) classifiers trained and tested on the BRIGHT (adult) dataset with errors determined by the sample standard deviation. Red indicates overall best results; bold indicates overall second-best results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Metric</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RF</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SVM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MLP</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GCN<break/>(Simple)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GCN<break/>(Residual)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Accuracy (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">73.13 &#x000b1; 2.05</td><td align="center" valign="middle" rowspan="1" colspan="1">76.37 &#x000b1; 5.84</td><td align="center" valign="middle" rowspan="1" colspan="1">77.66 &#x000b1; 3.18</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>
<named-content content-type="color:red">85.10 &#x000b1; 2.84</named-content>
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>82.82 &#x000b1; 5.30</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.80 &#x000b1; 0.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.86 &#x000b1; 0.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.89 &#x000b1; 0.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.91 &#x000b1; 0.03</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>
<named-content content-type="color:red">0.93 &#x000b1; 0.03</named-content>
</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tomography-11-00014-t004"><object-id pub-id-type="pii">tomography-11-00014-t004_Table 4</object-id><label>Table 4</label><caption><p>Sex classification mean accuracy and AUC score results for random forest (RF), support vector machine (SVM), multi-layer perceptron (MLP), simple graph convolutional neural network (GCN simple), and residual graph convolutional neural network (GCN residual) classifiers trained on the BRIGHT (adult) dataset and tested on the HCP-D (pediatric) dataset with errors determined by the sample standard deviation. Red indicates overall best results; bold indicates overall second-best results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Metric</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RF</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SVM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MLP</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GCN<break/>(Simple)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GCN<break/>(Residual)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Accuracy (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">55.70 &#x000b1; 3.23</td><td align="center" valign="middle" rowspan="1" colspan="1">56.15 &#x000b1; 3.79</td><td align="center" valign="middle" rowspan="1" colspan="1">53.93 &#x000b1; 3.85</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>60.74 &#x000b1; 3.47</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>
<named-content content-type="color:red">74.96 &#x000b1; 2.41</named-content>
</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.62 &#x000b1; 0.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.64 &#x000b1; 0.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.60 &#x000b1; 0.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.71 &#x000b1; 0.03</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>
<named-content content-type="color:red">0.86 &#x000b1; 0.03</named-content>
</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tomography-11-00014-t005"><object-id pub-id-type="pii">tomography-11-00014-t005_Table 5</object-id><label>Table 5</label><caption><p>Sex classification mean accuracy and AUC score results for random forest (RF), support vector machine (SVM), multi-layer perceptron (MLP), simple graph convolutional neural network (GCN simple), and residual graph convolutional neural network (GCN residual) classifiers trained and tested on the HCP-D (pediatric) dataset with errors determined by the sample standard deviation. Red indicates overall best results; bold indicates overall second-best results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Metric</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RF</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SVM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MLP</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GCN<break/>(Simple)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GCN<break/>(Residual)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Accuracy (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>66.67 &#x000b1; 8.76</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">65.93 &#x000b1; 8.25</td><td align="center" valign="middle" rowspan="1" colspan="1">56.30 &#x000b1; 7.55</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>
<named-content content-type="color:red">71.11 &#x000b1; 10.84</named-content>
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>66.67 &#x000b1; 12.83</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.73 &#x000b1; 0.13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.80 &#x000b1; 0.04</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.67 &#x000b1; 0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.76 &#x000b1; 0.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>
<named-content content-type="color:red">0.85 &#x000b1; 0.05</named-content>
</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tomography-11-00014-t006"><object-id pub-id-type="pii">tomography-11-00014-t006_Table 6</object-id><label>Table 6</label><caption><p>Sex classification mean accuracy and AUC score results for random forest (RF), support vector machine (SVM), multi-layer perceptron (MLP), simple graph convolutional neural network (GCN simple), and residual graph convolutional neural network (GCN residual) classifiers trained on the adult-enriched pediatric dataset and tested on the pediatric subset of the test set with errors determined by the sample standard deviation. Red indicates overall best results; bold indicates overall second-best results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Metric</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RF</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SVM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MLP</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GCN<break/>(Simple)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GCN<break/>(Residual)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Accuracy (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">65.93 &#x000b1; 10.32</td><td align="center" valign="middle" rowspan="1" colspan="1">67.41 &#x000b1; 8.25</td><td align="center" valign="middle" rowspan="1" colspan="1">72.59 &#x000b1; 6.46</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>79.26 &#x000b1; 6.46</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>
<named-content content-type="color:red">82.96 &#x000b1; 5.02</named-content>
</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.74 &#x000b1; 0.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.80 &#x000b1; 0.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.79 &#x000b1; 0.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.84 &#x000b1; 0.05</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>
<named-content content-type="color:red">0.91 &#x000b1; 0.04</named-content>
</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tomography-11-00014-t007"><object-id pub-id-type="pii">tomography-11-00014-t007_Table 7</object-id><label>Table 7</label><caption><p>Sex classification mean accuracy and AUC score results simple graph convolutional neural network using mean and max pooling functions for graph embedding vector generation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pooling Function</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AUC Score</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">Mean</td><td align="center" valign="top" rowspan="1" colspan="1">79.26 &#x000b1; 6.46</td><td align="center" valign="top" rowspan="1" colspan="1">0.91 &#x000b1; 0.05</td></tr><tr><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Max</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">77.04 &#x000b1; 7.18</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.85 &#x000b1; 0.04</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tomography-11-00014-t008"><object-id pub-id-type="pii">tomography-11-00014-t008_Table 8</object-id><label>Table 8</label><caption><p>Sex classification mean accuracy and AUC score results for the residual graph convolutional neural network model using mean and max aggregation functions for graph embedding matrix generation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Aggregation Function</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AUC Score</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">Mean</td><td align="center" valign="top" rowspan="1" colspan="1">82.96 &#x000b1; 5.02</td><td align="center" valign="top" rowspan="1" colspan="1">0.91 &#x000b1; 0.04</td></tr><tr><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Max</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">81.48 &#x000b1; 5.24</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.90 &#x000b1; 0.04</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tomography-11-00014-t009"><object-id pub-id-type="pii">tomography-11-00014-t009_Table 9</object-id><label>Table 9</label><caption><p>Sex classification mean accuracy and AUC score results for the residual graph convolutional neural network with and without skip connections.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Residual GCN Model (With/Without Skips)</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AUC Score</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">Without Skips</td><td align="center" valign="top" rowspan="1" colspan="1">68.89 &#x000b1; 9.54</td><td align="center" valign="top" rowspan="1" colspan="1">0.74 &#x000b1; 0.07</td></tr><tr><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">With Skips</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">82.96 &#x000b1; 5.02</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.91 &#x000b1; 0.04</td></tr></tbody></table></table-wrap></floats-group></article>