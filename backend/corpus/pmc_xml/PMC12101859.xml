<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="iso-abbrev">PLoS Comput Biol</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408631</article-id><article-id pub-id-type="pmc">PMC12101859</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1013071</article-id><article-id pub-id-type="publisher-id">PCOMPBIOL-D-24-01171</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Cell Biology</subject><subj-group><subject>Cell Processes</subject><subj-group><subject>Cell Cycle and Cell Division</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Bioassays and Physiological Analysis</subject><subj-group><subject>Cell Analysis</subject><subj-group><subject>Cell Division Analysis</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Bacteria</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject><subj-group><subject>Deep Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Machine Learning Algorithms</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Machine Learning Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject><subj-group><subject>Machine Learning Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Computer Vision</subject></subj-group></subj-group></article-categories><title-group><article-title>Cell-TRACTR: A transformer-based model for end-to-end segmentation and tracking of cells</article-title><alt-title alt-title-type="running-head">Cell-TRACTR: Transformer-based cell segmentation and tracking</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6276-6269</contrib-id><name><surname>O&#x02019;Connor</surname><given-names>Owen M.</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9261-8216</contrib-id><name><surname>Dunlop</surname><given-names>Mary J.</given-names></name><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Biomedical Engineering, Boston University, Boston, Massachusetts, United States of America</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Biological Design Center, Boston University, Boston, Massachusetts, United States of America</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Mac Gabhann</surname><given-names>Feilim</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Johns Hopkins University, UNITED STATES OF AMERICA</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>mjdunlop@bu.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>5</month><year>2025</year></pub-date><volume>21</volume><issue>5</issue><elocation-id>e1013071</elocation-id><history><date date-type="received"><day>11</day><month>7</month><year>2024</year></date><date date-type="accepted"><day>21</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 O&#x02019;Connor, Dunlop</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>O&#x02019;Connor, Dunlop</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pcbi.1013071.pdf">
</self-uri><abstract><p>Deep learning-based methods for identifying and tracking cells within microscopy images have revolutionized the speed and throughput of data analysis. These methods for analyzing biological and medical data have capitalized on advances from the broader computer vision field. However, cell tracking can present unique challenges, with frequent cell division events and the need to track many objects with similar visual appearances complicating analysis. Existing architectures developed for cell tracking based on convolutional neural networks (CNNs) have tended to fall short in managing the spatial and global contextual dependencies that are crucial for tracking cells. To overcome these limitations, we introduce Cell-TRACTR (<underline>Tr</underline>ansformer with <underline>A</underline>ttention for <underline>C</underline>ell <underline>T</underline>racking and <underline>R</underline>ecognition), a novel deep learning model that uses a transformer-based architecture. Cell-TRACTR operates in an end-to-end manner, simultaneously segmenting and tracking cells without the need for post-processing. Alongside this model, we introduce the Cell-HOTA metric, an extension of the Higher Order Tracking Accuracy (HOTA) metric that we adapted to assess cell division. Cell-HOTA differs from standard cell tracking metrics by offering a balanced and easily interpretable assessment of detection, association, and division accuracy. We test our Cell-TRACTR model on datasets of bacteria growing within a defined microfluidic geometry and mammalian cells growing freely in two dimensions. Our results demonstrate that Cell-TRACTR exhibits strong performance in tracking and division accuracy compared to state-of-the-art algorithms, while also meeting traditional benchmarks in detection accuracy. This work establishes a new framework for employing transformer-based models in cell segmentation and tracking.</p></abstract><abstract abstract-type="summary"><title>Author summary</title><p>Understanding the growth, movement, and gene expression dynamics of individual cells is critical for studies in a wide range of areas, from antibiotic resistance to cancer. Monitoring individual cells can reveal unique insights that are obscured by population averages. Although modern microscopy techniques have vastly improved researchers&#x02019; ability to collect data, tracking individual cells over time remains a challenge, particularly due to complexities such as cell division and non-linear cell movements. To address this, we developed a new transformer-based model called Cell-TRACTR that can segment and track single cells without the need for post-processing. The strength of the transformer architecture lies in its attention mechanism, which integrates global context. Attention makes this model particularly well suited for tracking cells across a sequence of images. In addition to the Cell-TRACTR model, we introduce a new metric, Cell-HOTA, to evaluate tracking algorithms in terms of detection, association, and division accuracies. The metric breaks down performance into sub-metrics, helping researchers pinpoint the strengths and weaknesses of their tracking algorithms. When compared to state-of-the-art algorithms, Cell-TRACTR meets or exceeds many current benchmarks, offering excellent potential as a new tool for the analysis of series of images with single-cell resolution.</p></abstract><funding-group><award-group id="award001"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap>
</funding-source><award-id>MCB-2143289</award-id><principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9261-8216</contrib-id>
<name><surname>Dunlop</surname><given-names>Mary J.</given-names></name>
</principal-award-recipient></award-group><award-group id="award002"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap>
</funding-source><award-id>MCB-2032357</award-id><principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9261-8216</contrib-id>
<name><surname>Dunlop</surname><given-names>Mary J.</given-names></name>
</principal-award-recipient></award-group><funding-statement>This work was supported by the National Science Foundation (MCB-2143289 and MCB-2032357 to MJD). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="5"/><table-count count="0"/><page-count count="28"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>Code Availability We provide the code for Cell-TRACTR at <ext-link xlink:href="https://gitlab.com/dunloplab/Cell-TRACTR" ext-link-type="uri">https://gitlab.com/dunloplab/Cell-TRACTR</ext-link>. Code for Cell-HOTA is available at <ext-link xlink:href="https://gitlab.com/dunloplab/Cell-HOTA" ext-link-type="uri">https://gitlab.com/dunloplab/Cell-HOTA</ext-link>. Model checkpoints are available on Zenodo (<ext-link xlink:href="https://zenodo.org/records/14509424" ext-link-type="uri">https://zenodo.org/records/14509424</ext-link>). Datasets The bacterial mother machine dataset is hosted on Zenodo (<ext-link xlink:href="https://doi.org/10.5281/zenodo.11237127" ext-link-type="uri">https://doi.org/10.5281/zenodo.11237127</ext-link>). It is formatted in the style of the Cell Tracking Challenge. The DeepCell dataset was downloaded directly from the DeepCell website (<ext-link xlink:href="https://datasets.deepcell.org/data" ext-link-type="uri">https://datasets.deepcell.org/data</ext-link>). We used their &#x0201c;DynamicNuclearNet Tracking&#x0201d; dataset. This dataset was converted into the Cell Tracking Challenge format to be compatible for training the models.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>Code Availability We provide the code for Cell-TRACTR at <ext-link xlink:href="https://gitlab.com/dunloplab/Cell-TRACTR" ext-link-type="uri">https://gitlab.com/dunloplab/Cell-TRACTR</ext-link>. Code for Cell-HOTA is available at <ext-link xlink:href="https://gitlab.com/dunloplab/Cell-HOTA" ext-link-type="uri">https://gitlab.com/dunloplab/Cell-HOTA</ext-link>. Model checkpoints are available on Zenodo (<ext-link xlink:href="https://zenodo.org/records/14509424" ext-link-type="uri">https://zenodo.org/records/14509424</ext-link>). Datasets The bacterial mother machine dataset is hosted on Zenodo (<ext-link xlink:href="https://doi.org/10.5281/zenodo.11237127" ext-link-type="uri">https://doi.org/10.5281/zenodo.11237127</ext-link>). It is formatted in the style of the Cell Tracking Challenge. The DeepCell dataset was downloaded directly from the DeepCell website (<ext-link xlink:href="https://datasets.deepcell.org/data" ext-link-type="uri">https://datasets.deepcell.org/data</ext-link>). We used their &#x0201c;DynamicNuclearNet Tracking&#x0201d; dataset. This dataset was converted into the Cell Tracking Challenge format to be compatible for training the models.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Single-cell resolution experiments have significantly enhanced our understanding of cell physiology and survival in the context of a wide range of domains including antibiotic resistance [<xref rid="pcbi.1013071.ref001" ref-type="bibr">1</xref>], cancer [<xref rid="pcbi.1013071.ref002" ref-type="bibr">2</xref>], stress response [<xref rid="pcbi.1013071.ref003" ref-type="bibr">3</xref>,<xref rid="pcbi.1013071.ref004" ref-type="bibr">4</xref>], and aging [<xref rid="pcbi.1013071.ref005" ref-type="bibr">5</xref>,<xref rid="pcbi.1013071.ref006" ref-type="bibr">6</xref>]. Traditional population-level measurements can obscure heterogeneity among cells such as single-cell differences in gene expression that can emerge across space or time. Because this variation can be stochastic in origin [<xref rid="pcbi.1013071.ref007" ref-type="bibr">7</xref>], it is often necessary for researchers to gather large datasets to achieve statistically significant insights. Advances in automated imaging, coupled with increasing computer storage capacity, have dramatically increased the throughput of data collection. However, despite the abundance of data, analysis can remain a bottleneck, particularly for applications with many sequential images acquired over time. Although cell segmentation algorithms have achieved impressive performance levels [<xref rid="pcbi.1013071.ref008" ref-type="bibr">8</xref>&#x02013;<xref rid="pcbi.1013071.ref011" ref-type="bibr">11</xref>], cell tracking still presents a significant challenge.</p><p>Cell tracking datasets are often markedly different from standard multi-object tracking datasets. In contrast to classic computer vision datasets like those that feature human subjects or automobiles moving through a field of view, cells can divide, and division events can occur frequently during the time frame of an experiment. Moreover, cells often display a high degree of similarity in appearance, are small in resolution, and can morph in shape over time. In addition, time-lapse microscopy experiments, which can extend for hours or even days, may have a low frame rate to reduce the impact of imaging on cells. However, the reduced frame rate can complicate tracking due to the non-linear motion often exhibited by cells. For example, growth of a microcolony may produce unpredictable motion of an individual cell, in contrast to images involving pedestrians or vehicle motion, which tend to follow more predictable patterns. In addition, shifts in the field of view can occur due to changes in imaging conditions over time. Further complicating analysis, microscopy images can contain a high density of cells, especially for applications that involve the analysis of cellular communication or community dynamics [<xref rid="pcbi.1013071.ref012" ref-type="bibr">12</xref>,<xref rid="pcbi.1013071.ref013" ref-type="bibr">13</xref>]. Thus, cell tracking algorithms must contend with the complexities of cellular behavior coupled with the technical constraints of microscopy and live cell imaging.</p><p>While early cell tracking models employed non-machine learning techniques [<xref rid="pcbi.1013071.ref014" ref-type="bibr">14</xref>&#x02013;<xref rid="pcbi.1013071.ref017" ref-type="bibr">17</xref>], the landscape has shifted dramatically towards the adoption of deep learning-based methods [<xref rid="pcbi.1013071.ref018" ref-type="bibr">18</xref>]. The acceleration of AI in the past decade has ushered in a new era within the field of cell tracking, with deep learning models surpassing simpler machine learning approaches such as nearest-neighbor algorithms [<xref rid="pcbi.1013071.ref019" ref-type="bibr">19</xref>]. Traditionally, most deep learning algorithms have employed a tracking-by-detection approach where segmentation and tracking are performed in two separate steps [<xref rid="pcbi.1013071.ref014" ref-type="bibr">14</xref>,<xref rid="pcbi.1013071.ref020" ref-type="bibr">20</xref>&#x02013;<xref rid="pcbi.1013071.ref022" ref-type="bibr">22</xref>]. However, a recent trend in the field is a move towards models where segmentation and tracking are performed in one step [<xref rid="pcbi.1013071.ref023" ref-type="bibr">23</xref>&#x02013;<xref rid="pcbi.1013071.ref029" ref-type="bibr">29</xref>]. Combining segmentation and tracking into one step can help ensure coherent predictions, where tracking information can enhance segmentation accuracy. For example, temporal information helps the segmentation model understand that cells can divide but not merge. Furthermore, some models now incorporate long-term temporal information, providing more context for segmentation [<xref rid="pcbi.1013071.ref023" ref-type="bibr">23</xref>,<xref rid="pcbi.1013071.ref028" ref-type="bibr">28</xref>]. While these improvements have demonstrated superior performance compared to models that utilize the tracking-by-detection approach, these models typically still require separate decoders for segmentation and tracking [<xref rid="pcbi.1013071.ref023" ref-type="bibr">23</xref>,<xref rid="pcbi.1013071.ref024" ref-type="bibr">24</xref>] or rely on post-processing [<xref rid="pcbi.1013071.ref027" ref-type="bibr">27</xref>,<xref rid="pcbi.1013071.ref028" ref-type="bibr">28</xref>]. A unified decoder could enhance the synchronization of segmentation and tracking, leading to more consistent predictions, while eliminating post-processing steps would simplify workflows and improve model generalizability.</p><p>In cell tracking and segmentation software, convolutional neural networks (CNN) are currently the preferred method, evidenced by their widespread adoption within the field [<xref rid="pcbi.1013071.ref010" ref-type="bibr">10</xref>,<xref rid="pcbi.1013071.ref023" ref-type="bibr">23</xref>&#x02013;<xref rid="pcbi.1013071.ref025" ref-type="bibr">25</xref>,<xref rid="pcbi.1013071.ref027" ref-type="bibr">27</xref>,<xref rid="pcbi.1013071.ref028" ref-type="bibr">28</xref>]. In particular, U-Net [<xref rid="pcbi.1013071.ref030" ref-type="bibr">30</xref>] and its derivatives have been used extensively for cell tracking [<xref rid="pcbi.1013071.ref010" ref-type="bibr">10</xref>,<xref rid="pcbi.1013071.ref020" ref-type="bibr">20</xref>,<xref rid="pcbi.1013071.ref025" ref-type="bibr">25</xref>,<xref rid="pcbi.1013071.ref026" ref-type="bibr">26</xref>,<xref rid="pcbi.1013071.ref028" ref-type="bibr">28</xref>] and cell segmentation [<xref rid="pcbi.1013071.ref008" ref-type="bibr">8</xref>,<xref rid="pcbi.1013071.ref010" ref-type="bibr">10</xref>,<xref rid="pcbi.1013071.ref031" ref-type="bibr">31</xref>,<xref rid="pcbi.1013071.ref032" ref-type="bibr">32</xref>], demonstrating the versatility of this architecture. However, deep learning approaches in cell tracking also extend beyond CNNs. Other architectures, such as graph neural networks [<xref rid="pcbi.1013071.ref033" ref-type="bibr">33</xref>,<xref rid="pcbi.1013071.ref034" ref-type="bibr">34</xref>], reinforcement learning [<xref rid="pcbi.1013071.ref035" ref-type="bibr">35</xref>], recurrent neural networks [<xref rid="pcbi.1013071.ref022" ref-type="bibr">22</xref>,<xref rid="pcbi.1013071.ref025" ref-type="bibr">25</xref>], and unsupervised learning [<xref rid="pcbi.1013071.ref019" ref-type="bibr">19</xref>] approaches, have also been successfully employed. Despite these advances, the utilization of transformers [<xref rid="pcbi.1013071.ref036" ref-type="bibr">36</xref>], which have revolutionized many areas of machine learning including computer vision [<xref rid="pcbi.1013071.ref037" ref-type="bibr">37</xref>,<xref rid="pcbi.1013071.ref038" ref-type="bibr">38</xref>], remains relatively underexplored in cell tracking. Transformers use an attention mechanism that can effectively model the relationships between different parts of an image, making it easier to follow the same object across frames. This attention mechanism has demonstrated great success in cell segmentation, where transformers have made significant strides [<xref rid="pcbi.1013071.ref009" ref-type="bibr">9</xref>,<xref rid="pcbi.1013071.ref011" ref-type="bibr">11</xref>,<xref rid="pcbi.1013071.ref039" ref-type="bibr">39</xref>,<xref rid="pcbi.1013071.ref040" ref-type="bibr">40</xref>]. While emerging work, like Trackastra [<xref rid="pcbi.1013071.ref041" ref-type="bibr">41</xref>], has demonstrated the promising potential of transformers in cell tracking, the significant capabilities of transformers, as demonstrated by the state-of-the-art multi-object tracking algorithm MOTRv3 [<xref rid="pcbi.1013071.ref042" ref-type="bibr">42</xref>], have not been fully leveraged for cell tracking. We were inspired by the top performance of MOTRv3 on the DanceTrack dataset [<xref rid="pcbi.1013071.ref043" ref-type="bibr">43</xref>], which features objects (dancers) with similar appearances and diverse motion patterns. This tracking dataset closely parallels cell tracking, where cells can have nearly identical appearances and migrate in non-linear directions. We reasoned that top performing algorithms on the DanceTrack dataset, like MOTRv3, would be well-suited to track cells.</p><p>To leverage these capabilities, here we introduce a novel approach to cell segmentation and tracking that uses a transformer-based model to enhance tracking performance. Inspired by TrackFormer [<xref rid="pcbi.1013071.ref044" ref-type="bibr">44</xref>] and MOTR [<xref rid="pcbi.1013071.ref045" ref-type="bibr">45</xref>], which are multi-object tracking algorithms from the broader computer vision community, we developed a transformer-based model that can simultaneously segment and track dividing cells. This model builds upon the framework of the Detection Transformer (DETR) [<xref rid="pcbi.1013071.ref046" ref-type="bibr">46</xref>] which was the first deep learning model to perform end-to-end object detection without the need for any post-processing like non-maximum suppression. Although DETR demonstrates remarkable performance, it requires longer training periods and greater computational resources than CNN-based approaches.</p><p>The evolution of DETR has led to significant enhancements in terms of accuracy and reduced training times through innovations such as deformable attention [<xref rid="pcbi.1013071.ref047" ref-type="bibr">47</xref>], dynamic anchor boxes [<xref rid="pcbi.1013071.ref048" ref-type="bibr">48</xref>], contrastive denoised training [<xref rid="pcbi.1013071.ref049" ref-type="bibr">49</xref>,<xref rid="pcbi.1013071.ref050" ref-type="bibr">50</xref>], and query selection [<xref rid="pcbi.1013071.ref051" ref-type="bibr">51</xref>,<xref rid="pcbi.1013071.ref052" ref-type="bibr">52</xref>]. Among these innovations, Mask-DINO [<xref rid="pcbi.1013071.ref053" ref-type="bibr">53</xref>] and Co-DETR [<xref rid="pcbi.1013071.ref054" ref-type="bibr">54</xref>] have emerged as state-of-the-art models for instance segmentation and object detection, respectively. Meanwhile, TrackFormer [<xref rid="pcbi.1013071.ref044" ref-type="bibr">44</xref>] and MOTR [<xref rid="pcbi.1013071.ref045" ref-type="bibr">45</xref>] have been instrumental in advancing tracking capabilities for DETR-based models. Recent iterations have built upon this foundation, offering further improvements [<xref rid="pcbi.1013071.ref042" ref-type="bibr">42</xref>,<xref rid="pcbi.1013071.ref055" ref-type="bibr">55</xref>&#x02013;<xref rid="pcbi.1013071.ref059" ref-type="bibr">59</xref>], culminating in the latest iteration of MOTR, called MOTRv3 [<xref rid="pcbi.1013071.ref042" ref-type="bibr">42</xref>], which has established itself as a state-of-the-art model in multi-object tracking.</p><p>Historically, cell tracking algorithms used datasets from the Cell Tracking Challenge [<xref rid="pcbi.1013071.ref060" ref-type="bibr">60</xref>] to benchmark performance. However, a notable portion of researchers have opted to use custom datasets or the increasingly popular DeepCell dataset [<xref rid="pcbi.1013071.ref022" ref-type="bibr">22</xref>]. Although the Cell Tracking Challenge has provided a useful way to standardize evaluations, it has drawbacks since most of the datasets are limited in the number of images and are only partially annotated, posing challenges for deep learning models that can require large, fully annotated datasets. The DeepCell dataset addresses this by providing a large and diverse dataset complete with full annotations.</p><p>In addition to providing datasets, the Cell Tracking Challenge also developed associated metrics to quantify cell segmentation and tracking accuracy. These metrics have been widely adopted for assessing model performance, however recent critiques, especially regarding their handling of cell division, have prompted researchers to modify these metrics or add new metrics [<xref rid="pcbi.1013071.ref022" ref-type="bibr">22</xref>,<xref rid="pcbi.1013071.ref023" ref-type="bibr">23</xref>,<xref rid="pcbi.1013071.ref061" ref-type="bibr">61</xref>]. Similarly, we found that the tracking metric from the Cell Tracking Challenge tends to de-emphasize division accuracy while over-emphasizing segmentation precision. Meanwhile, the general multi-object tracking community has adopted the Higher Order Tracking Accuracy (HOTA) metric due to its balanced assessment between tracking and segmentation. HOTA offers interpretable results that can help identify where a model is under performing, however the classic version of this metric does not handle division events.</p><p>In this work, we introduce Cell-TRACTR, a new transformer-based model to segment and track cells. The model can simultaneously segment and track without requiring any post-processing and it can handle frequent division events. Transformers offer a powerful architecture to handle the difficult challenge of tracking cells. In addition, we introduce a new cell tracking metric which adapts the HOTA metric [<xref rid="pcbi.1013071.ref062" ref-type="bibr">62</xref>] to effectively account for cell division. Cell-HOTA assesses overall tracking performance by balancing the algorithm&#x02019;s performance on detection, association, and localization. This metric can be broken down into interpretable sub-metrics, providing useful insight into sources of error. We compare our new transformer-based model (Cell-TRACTR) to other state-of-the-art models in the cell tracking field (DeLTA [<xref rid="pcbi.1013071.ref010" ref-type="bibr">10</xref>,<xref rid="pcbi.1013071.ref020" ref-type="bibr">20</xref>], EmbedTrack [<xref rid="pcbi.1013071.ref024" ref-type="bibr">24</xref>], Trackastra [<xref rid="pcbi.1013071.ref041" ref-type="bibr">41</xref>], and Caliban [<xref rid="pcbi.1013071.ref022" ref-type="bibr">22</xref>]). We evaluate these models on both bacterial and mammalian datasets. Our results demonstrate Cell-TRACTR&#x02019;s excellent performance in cell segmentation and tracking, achieving accuracy on par with state-of-the-art cell tracking algorithms. Additionally, we illustrate the interpretability of the Cell-HOTA metric for evaluating tracking outcomes.</p></sec><sec sec-type="results" id="sec002"><title>Results</title><sec id="sec003"><title>Cell-TRACTR simultaneously segments and tracks cells</title><p>Our overall goal was to achieve accurate segmentation and tracking of cells given time-lapse microscopy data. We tested our algorithm on two types of data: microscopy images of <italic toggle="yes">Escherichia coli</italic> growing in the &#x0201c;mother machine,&#x0201d; [<xref rid="pcbi.1013071.ref063" ref-type="bibr">63</xref>] a widely used microfluidic device where bacteria are constrained to grow in single-file lines (<xref rid="pcbi.1013071.g001" ref-type="fig">Fig 1A</xref>), and mammalian cells with stained nuclei growing in a two-dimensional framework (<xref rid="pcbi.1013071.g001" ref-type="fig">Fig 1B</xref>) [<xref rid="pcbi.1013071.ref022" ref-type="bibr">22</xref>]. These types of experiments can generate large amounts of data, motivating the need for automated image analysis. For example, the mother machine device contains thousands of parallel chambers, each of which is imaged over many hours. Cells growing in two dimensions present additional challenges, where they may move in any direction and morph in shape over time.</p><fig position="float" id="pcbi.1013071.g001"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013071.g001</object-id><label>Fig 1</label><caption><title>Results from Cell-TRACTR and model overview for object detection.</title><p>(A) Representative output from Cell-TRACTR for a single chamber from a time-lapse microscopy movie of <italic toggle="yes">E. coli</italic> bacteria growing in the mother machine microfluidic device. The output is displayed as a kymograph where the same chamber is shown over time. Each color represents a unique cell being tracked and black arrows signify cell divisions. This movie is from the test set. (B) Overview of Cell-TRACTR tracking mammalian nuclei across multiple frames. Each color represents a unique cell being tracked. These frames are from the DeepCell dataset using data from the test set. (C) Cell-TRACTR is a DETR-based model that first uses a CNN backbone to extract image features, then uses a transformer encoder-decoder framework that leverages attention mechanisms. Encoded multi-scale features are used for query selection to initialize the object queries. Object queries attend to the encoded multi-scale features to generate the output embeddings needed to predict class labels, bounding boxes, and segmentation masks. If the model predicts an object query to be a cell, then the output embedding is shown as a colored box in the schematic. Object queries predicted to be of class &#x0201c;no object&#x0201d; are colored black and are discarded. The squares represent content embeddings and the circles represent positional embeddings.</p></caption><graphic xlink:href="pcbi.1013071.g001" position="float"/></fig><p>We developed Cell-TRACTR by employing a DETR-based framework that couples a CNN backbone with a transformer encoder-decoder architecture [<xref rid="pcbi.1013071.ref046" ref-type="bibr">46</xref>] (<xref rid="pcbi.1013071.g001" ref-type="fig">Fig 1C</xref>). This end-to-end model, where the input is the raw image and the outputs are embeddings that represent the class and position of each cell in the frame, eliminates the need for post-processing techniques like non-maximum suppression that are required for other tracking-by-detection approaches [<xref rid="pcbi.1013071.ref064" ref-type="bibr">64</xref>&#x02013;<xref rid="pcbi.1013071.ref066" ref-type="bibr">66</xref>]. When an image is input into the model, we first use a CNN to preprocess the image into a more compact and rich representation since it would be computationally prohibitive to process the full image directly with a transformer. The goal of the CNN is to produce multi-scale features that can be fed into the transformer. Multi-scale features consist of the final layers that are output from the CNN, and they capture properties such as edges, colors, and simple textures. We adopted a ResNet [<xref rid="pcbi.1013071.ref067" ref-type="bibr">67</xref>] CNN architecture as the backbone to minimize model size while preserving high performance.</p></sec><sec id="sec004"><title>Cell-TRACTR architecture: Cell segmentation</title><p>After the CNN extracts information from the input image, an encoder further refines these features, preparing them for the decoder. The multi-scale features along with their positional embeddings are fed into the encoder. The multi-scale features serve as content embeddings which are latent vector spaces that store semantic information about the image. The positional embeddings are latent vector spaces that supply spatial context to the multi-scale features, compensating for the transformer&#x02019;s inherent lack of spatial processing capability. In the encoder, deformable self-attention is used to further enhance the multi-scale features, following the approach in Deformable DETR [<xref rid="pcbi.1013071.ref047" ref-type="bibr">47</xref>]. The model next performs &#x0201c;query selection,&#x0201d; also called &#x0201c;two stage&#x0201d; in [<xref rid="pcbi.1013071.ref047" ref-type="bibr">47</xref>], which aligns with the Deformable DETR framework, to convert the encoded multi-scale features into region proposals that are used to initialize the object queries (<xref rid="pcbi.1013071.s005" ref-type="supplementary-material">S1 Fig</xref> and <xref rid="pcbi.1013071.s001" ref-type="supplementary-material">S1 Text</xref>). Briefly, for each encoded multi-scale feature, a class label, bounding box, and segmentation mask prediction are generated. The class label represents the category of an object (&#x02018;cell&#x02019; or &#x02018;no object&#x02019;) while the bounding box represents the normalized center coordinates and the height and width of the box with respect to the input image. The segmentation mask is an image which represents the pixels associated with each predicted cell. These masks are generated by integrating information from the output embeddings, the encoded multi-scale features, and the original multi-scale features; this process of mask generation is described in further detail below after tracking and cell division are introduced. The top-K encoded multi-scale features are selected based on the highest values of the class labels (<xref rid="pcbi.1013071.s005" ref-type="supplementary-material">S1 Fig</xref>). Bounding boxes, extracted from the top-K segmentation masks, are used as the positional embeddings and the top-K encoded multi-scale features are used as the content embeddings. K is determined by the number of the object queries. Combining these positional and content embeddings forms the object queries which are passed to the decoder.</p><p>Within the decoder, the object queries interact with the encoded multi-scale features to generate output embeddings that are used to refine an object&#x02019;s bounding box and predict the segmentation mask. The object queries first perform self-attention to avoid duplicate predictions and then deformable cross-attention, where the object queries spatially attend to the encoded multi-scale features (<xref rid="pcbi.1013071.s005" ref-type="supplementary-material">S1 Fig</xref>). Output embeddings that are not predicted to be cells (i.e., their class label is &#x02018;no object&#x02019;) are discarded. Thus, the final output of the decoder is a set of output embeddings, where each embedding encodes information about the class label, and location of each predicted cell within the image given by both a bounding box that defines its location and a segmentation mask that defines the pixels associated with the cell.</p></sec><sec id="sec005"><title>Cell-TRACTR architecture: Cell tracking</title><p>We next leveraged the output embeddings to track cells from frame to frame using the same network architecture. We developed a model that tracks cells over multiple frames using a transformer architecture (<xref rid="pcbi.1013071.g002" ref-type="fig">Fig 2A</xref>). We based the tracking scheme on the architecture of TrackFormer [<xref rid="pcbi.1013071.ref044" ref-type="bibr">44</xref>] and MOTR [<xref rid="pcbi.1013071.ref045" ref-type="bibr">45</xref>], which are DETR derivatives, where output embeddings are converted into track queries for analysis of the subsequent movie frame (<xref rid="pcbi.1013071.g002" ref-type="fig">Fig 2B</xref>). At time t<sub>0</sub>, the model performs object detection and locates all cells in the image. For all subsequent frames, the output embeddings associated with each detected cell from the previous frame serve as track queries. Track queries predicted as &#x02018;no object&#x02019;, which represent cells that exited the field of view, are discarded. Track queries that are predicted to be cells are then concatenated with the object queries of the present frame and fed to the decoder. Self-attention is a critical step in the decoder where track and object queries need to communicate so that an object query does not detect a cell that is already being tracked (S2 Fig). For example, when a cell enters the field the view, an object query is needed to detect the new cell. However, if the cell existed in the previous frame, it needs to be handled by a track query and not an object query. In addition, if a track query makes an error and misses a cell, an object query is needed to detect the missed cell in the next frame. In these scenarios, it is important that the object and track queries can communicate to avoid duplicate predictions. By utilizing object and track queries, Cell-TRACTR simultaneously segments and tracks cells, leading to more accurate and coherent predictions.</p><fig position="float" id="pcbi.1013071.g002"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013071.g002</object-id><label>Fig 2</label><caption><title>Visual representation of how Cell-TRACTR uses track queries to track cells and detect divisions.</title><p>(A) Overview of Cell-TRACTR performing cell tracking, where a black arrow connecting two cells signifies a cell division event, as seen in frame t<sub>2</sub>. (B) In frame t<sub>0</sub>, the model performs object detection and locates all cells in the frame. In subsequent frames, the output embeddings serve as track queries, which track the locations of the cells from frame to frame. An output embedding split with two colors indicates that the model predicted a cell division. A black output embedding indicates when a cell, represented by a track query, is lost, such as when it exits the field of view. (C) Each output embedding makes two predictions. Each prediction can be classified either as a cell or no object. The class labels determine which prediction is used. If only the first prediction for an output embedding is classified as a cell, then just the first prediction is used. We ignore predictions from output embeddings that predict no object for both class labels. When both predictions are classified as cells, the output embedding is labeled as a division.</p></caption><graphic xlink:href="pcbi.1013071.g002" position="float"/></fig></sec><sec id="sec006"><title>Cell-TRACTR architecture: Cell division</title><p>Cell division is an important property that needs to be tracked accurately in single-cell imaging experiments. When a division occurs, as shown in the frame at time t<sub>2</sub>, the output embedding must record a division event, and this is represented schematically by a split light/dark blue output embedding (<xref rid="pcbi.1013071.g002" ref-type="fig">Fig 2B</xref>). For each output embedding, the model makes a class label, bounding box, and segmentation mask prediction (<xref rid="pcbi.1013071.g002" ref-type="fig">Fig 2C</xref>). For each of these tasks, two predictions are generated. The first prediction corresponds to the cell that is being detected or tracked. The second prediction corresponds to a potential daughter cell generated in a division event, which is only applicable for track queries since object queries can detect at most one cell. The classification branch uses a linear layer to classify each output embedding as &#x02018;cell&#x02019; or &#x02018;no object&#x02019; (<xref rid="pcbi.1013071.s007" ref-type="supplementary-material">S3 Fig</xref>). An output embedding is classified as a &#x02018;division&#x02019; when both outputs are classified as &#x02018;cell&#x02019;. The object detection branch uses a multi-layer perception (MLP) to generate a bounding box prediction for each cell. In addition, the segmentation branch produces a segmentation mask for each cell. Closely following the architecture of Mask-DINO [<xref rid="pcbi.1013071.ref053" ref-type="bibr">53</xref>], multi-scale features are combined with the largest encoded multi-scale feature map to form a pixel embedding map (<xref rid="pcbi.1013071.s007" ref-type="supplementary-material">S3 Fig</xref>). The dot product between each output embedding and the pixel embedding map is used to generate the segmentation masks. This process assigns a value ranging from 0 to 1 to every pixel within the mask, indicating the likelihood of each pixel being part of a segmented cell. The maximum value attained by any pixel across all generated masks is combined into one unified mask that encapsulates all predictions. Pixels with values greater than 0.5 in this final mask are classified as belonging to a segmented object. If an output embedding is not predicted to be a cell (i.e., it has a class label of &#x02018;no object&#x02019;), then the associated bounding boxes and segmentation masks are disregarded. Using a transformer-based model allows for seamless segmentation and tracking of cells in a unified step.</p><p>As an example, we used Cell-TRACTR to process a time-lapse microscopy movie of <italic toggle="yes">E. coli</italic> growing in the mother machine device (<xref rid="pcbi.1013071.s002" ref-type="supplementary-material">S1 Movie</xref>). In the first frame, the object queries detect all cells in the chamber. Then, these object queries are converted into track queries and tracked throughout the rest of the movie. These data provide an example demonstrating that Cell-TRACTR can accurately detect cells, track cells, and predict cell divisions over the course of many generations of growth.</p></sec><sec id="sec007"><title>Multi-object tracking and cell tracking metrics</title><p>To assess the performance of Cell-TRACTR, we developed an extended version of the Higher Order Tracking Accuracy (HOTA) metric [<xref rid="pcbi.1013071.ref062" ref-type="bibr">62</xref>]. Multi-object tracking requires the detection and association of several objects over consecutive frames. When evaluating multi-object tracking, it is important to consider a model&#x02019;s performance across a range of tasks including the detection, association, and localization of objects. Various tracking metrics have been proposed, but many metrics like Multiple Object Tracking Accuracy (MOTA) [<xref rid="pcbi.1013071.ref068" ref-type="bibr">68</xref>] and ID Switches (IDS) [<xref rid="pcbi.1013071.ref069" ref-type="bibr">69</xref>] have been shown to disproportionately emphasize either detection or association [<xref rid="pcbi.1013071.ref062" ref-type="bibr">62</xref>,<xref rid="pcbi.1013071.ref070" ref-type="bibr">70</xref>]. An ideal tracking metric should value both detection and association, providing an accurate assessment of the overall performance. Recently, Higher Order Tracking Accuracy (HOTA) [<xref rid="pcbi.1013071.ref062" ref-type="bibr">62</xref>] has emerged as a leading metric for assessing tracking accuracy due to its balanced approach towards assessing detection, association, and localization. However, most multi-object tracking metrics from computer vision, including HOTA, are designed for objects that do not divide, which is a major limitation for cell tracking applications where division events can be common. In this work, we extended the HOTA metric to include division in its evaluation.</p><p>Other approaches exist for measuring the overall performance of cell tracking algorithms. For example, the Cell Tracking Challenge developed a widely used metric, OP<sub>CTB</sub> (overall performance for the cell tracking benchmark). OP<sub>CTB</sub> is calculated as the mean of the tracking accuracy measure (TRA) and segmentation accuracy measure (SEG), <inline-formula id="pcbi.1013071.e001"><alternatives><graphic xlink:href="pcbi.1013071.e001.jpg" id="pcbi.1013071.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>T</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>*</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where tracking is assessed using a graph matching-based approach and segmentation is assessed using the Jaccard Similarity index (<xref rid="pcbi.1013071.s001" ref-type="supplementary-material">S1 Text</xref>). Although OP<sub>CTB</sub> is widely used, recently it has been criticized for not properly handling cell division [<xref rid="pcbi.1013071.ref022" ref-type="bibr">22</xref>,<xref rid="pcbi.1013071.ref023" ref-type="bibr">23</xref>,<xref rid="pcbi.1013071.ref061" ref-type="bibr">61</xref>]. In some cases, removing division links can increase the tracking score. For example, a division event predicted one frame early or late with respect to the ground truth can result in a lower score than removing the division all together (<xref rid="pcbi.1013071.s001" ref-type="supplementary-material">S1 Text</xref> and <xref rid="pcbi.1013071.s008" ref-type="supplementary-material">S4 Fig</xref>). This is a problem because in real microscopy data it can be challenging to pinpoint the precise frame when a cell divides, and this issue is exacerbated in the case of rapidly growing cells where there are many division events (<xref rid="pcbi.1013071.g003" ref-type="fig">Fig 3A</xref>).</p><fig position="float" id="pcbi.1013071.g003"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013071.g003</object-id><label>Fig 3</label><caption><title>Cell-HOTA metric incorporates cell division accuracy.</title><p>(A) Kymograph of bacteria growing in the mother machine. Colored triangles indicate potential times at which a cell division could have occurred, where different colors correspond to different cells. Multiple triangles of the same color indicate that it can be subjective to determine where division occurs. (B) The intersection over union (IOU) between a tracker cell and ground truth cell must exceed the similarity threshold &#x003b1; to be considered a match. HOTA is evaluated across various values of &#x003b1;, with a higher &#x003b1; imposing stricter criteria for matching. (C) Schematic showing the criteria required to determine if a division qualifies as a true positive division (TPD). A tracker cell and ground truth cell are shown dividing at time t<sub>1</sub>. First, the set of daughter cells that divided at time t<sub>1</sub> needs to match. Second, the parent cells of those daughter cells at time t<sub>0</sub> needs to match. Third, the average IOU between the set of daughter cells needs to exceed the similarity threshold &#x003b1;. Red lines indicate matching cells. The blue dotted line indicates the set of cells used to calculate the IOU to determine if the division is a TPD. (D) An early or late division occurs when a cell division is predicted one frame too early or late. Black lines indicate temporal associations and brown lines are used to highlight division events. (E) DivA rewards division events within a frame of the ground truth. Flexible divisions may be disabled so that divisions events are only counted as correct if they occur in the same frame as the ground truth. No Flex indicates that DivA has disabled flexible divisions.</p></caption><graphic xlink:href="pcbi.1013071.g003" position="float"/></fig></sec><sec id="sec008"><title>Cell-HOTA metric</title><p>We sought to take advantage of the benefits of the HOTA metric&#x02014;namely its ability to assess performance in a balanced manner across a range of segmentation and tracking associated tasks&#x02014;while extending it to handle cell division events. Further, because HOTA is the emerging standard in the broader multi-object tracking community, we wished to align the cell tracking community more closely with this larger group.</p><p>The HOTA metric is composed of two main sub-metrics for evaluating tracking: detection accuracy (DetA) and association accuracy (AssA). DetA measures detection accuracy by comparing the number of correct object detections to the total number of detections made. AssA evaluates how closely the predicted paths of objects match their actual paths, averaged over the entire set of correctly detected objects. At a high level, DetA is focused on assessing segmentation accuracy and AssA is specifically tailored to measure tracking accuracy. HOTA incorporates localization accuracy into the score by measuring DetA and AssA across varying similarity thresholds <italic toggle="yes">&#x003b1;</italic> (<xref rid="pcbi.1013071.g003" ref-type="fig">Fig 3B</xref>). The intersection over union (IOU) between a predicted object and a ground truth object needs to exceed <italic toggle="yes">&#x003b1;</italic> for the two objects to be considered a match, thus the <italic toggle="yes">&#x003b1;</italic> value determines the stringency for these matches. HOTA is measured at various values of <italic toggle="yes">&#x003b1;</italic> to provide insight into how stringency in matching affects DetA and AssA. To improve the readability and follow the terminology used in HOTA, we define a &#x02018;tracker cell&#x02019; as a cell predicted by the tracking algorithm and a &#x02018;ground truth cell&#x02019; as a cell defined in the ground truth.</p><p>There are several advantages that HOTA offers compared to OP<sub>CTB</sub> (<xref rid="pcbi.1013071.s001" ref-type="supplementary-material">S1 Text</xref>). First, HOTA is a comprehensive standalone metric that can rank tracking algorithms and can also be decomposed into sub-metrics, allowing for straightforward analysis of performance. Second, HOTA integrates localization accuracy by calculating the average score across all HOTA<sub>&#x003b1;</sub> scores for each value of &#x003b1;. This is important because it allows the user to assess the detection and tracking performance at the level of stringency that is appropriate for their application. Third, HOTA functions as a monotonic metric with DetA and AssA, meaning an improvement in DetA or AssA will always increase the final HOTA score. Overall, HOTA presents a robust framework for evaluating and ranking tracking algorithms.</p><p>To adapt HOTA for cell tracking, we introduced a metric for division accuracy (DivA) alongside the scores for detection and association accuracy. We call this modified metric Cell-HOTA. DivA<sub>&#x003b1;</sub> is the mean division accuracy score across all cell divisions at a similarity threshold <italic toggle="yes">&#x003b1;</italic>. A true positive division (TPD) is defined by three criteria (<xref rid="pcbi.1013071.g003" ref-type="fig">Fig 3C</xref>): First, two divided tracker cells with the same parent cell need to match with two divided ground truth cells with that same parent cell. Second, the parents of the divided tracker and ground truth cells need to match in the previous frame. Finally, the average IOU between the two matches needs to be greater than <italic toggle="yes">&#x003b1;</italic>. A false positive division (FPD) occurs when a tracker division does not match with a ground truth division. A false negative (FND) division occurs when a ground truth division does not match with a tracker cell division.</p><p>Due to the importance of tracking cell divisions in time-lapse microscopy experiments, we set DivA<sub>&#x003b1;</sub> to be of equal importance to the association score AssA<sub>&#x003b1;</sub>. AssDivA<sub>&#x003b1;</sub> is the geometric mean of AssA<sub>&#x003b1;</sub> and DivA<sub>&#x003b1;</sub> and we define Cell-HOTA<sub>&#x003b1;</sub> to be equal to the geometric mean of DetA<sub>&#x003b1;</sub> and AssDivA<sub>&#x003b1;</sub>. Similarly to HOTA, Cell-HOTA is a calculated by taking the average Cell-HOTA<sub>&#x003b1;</sub> score across all values of &#x003b1;. The Cell-HOTA metric is a single unified score that can also be broken down into multiple informative sub-metrics. This makes it easy to analyze whether the algorithm had issues detecting cells, tracking cells, or predicting divisions.</p></sec><sec id="sec009"><title>Allowing for flexibility in determining time of cell division</title><p>Another key consideration for cell tracking metrics is that the ground truth is often a subjective assessment of the cell division time rather than an unequivocal record of this event (<xref rid="pcbi.1013071.g003" ref-type="fig">Fig 3A</xref>). For cells that divide frequently, such as in time-lapse images of bacteria, these issues can compound and result in inaccurate estimates of tracking performance. Ultimately, we want to measure a model&#x02019;s capability to generate a coherent cell track rather than replicate the precise ground truths. There are two possible scenarios associated with data where the precise division timing is uncertain and the ground truth is a close, but not exact, record of cell division. We define these alternatives as &#x02018;early division&#x02019; and &#x02018;late division&#x02019; (<xref rid="pcbi.1013071.g003" ref-type="fig">Fig 3D</xref>). In early division, the tracking algorithm predicts that a division event occurs before it does in the ground truth, while in late division it occurs after. In both cases, the overall lineage is preserved outside of this time window. To accommodate the flexible timing of divisions, we modified the DetA<sub>&#x003b1;</sub>, AssA<sub>&#x003b1;</sub>, and DivA<sub>&#x003b1;</sub> to handle early or late divisions that were one frame away from the ground truth (<xref rid="pcbi.1013071.g003" ref-type="fig">Fig 3E</xref>). In our analysis, we assume any division event within one frame of the ground truth is acceptable, while any division event more than one frame away from the ground truth is erroneous. The flexible division feature within Cell-HOTA is optional and may be turned off. More details on how Cell-HOTA handles flexible divisions are provided in the Methods.</p></sec><sec id="sec010"><title>Benchmarking Cell-TRACTR and other algorithms</title><p>To benchmark the performance of Cell-TRACTR, we compared it to three other deep learning-based models and analyzed performance on the same datasets. The first model we benchmarked against was the Deep Learning for Time-Lapse Analysis (DeLTA) [<xref rid="pcbi.1013071.ref010" ref-type="bibr">10</xref>] algorithm we developed previously. DeLTA consists of two CNNs based on the U-Net [<xref rid="pcbi.1013071.ref030" ref-type="bibr">30</xref>] architecture, where the two CNNs are used sequentially for segmentation and tracking. DeLTA is a popular model for the analysis of single-cell time-lapse movies of growing bacteria. The second model we benchmarked against was EmbedTrack [<xref rid="pcbi.1013071.ref024" ref-type="bibr">24</xref>], which is a CNN consisting of a shared encoder, two decoders for segmentation, and one decoder for tracking. We chose EmbedTrack due to its high performance on the Cell Tracking Challenge. The third model we benchmarked against was Trackastra [<xref rid="pcbi.1013071.ref041" ref-type="bibr">41</xref>], which consists of a transformer decoder specifically designed for cell tracking. Unlike the other models, Trackastra only performs tracking, requiring precomputed segmentation masks to function. We provided Trackastra with the highest-quality segmentation masks (which we determined to be those we derived from our results using EmbedTrack) to maximize its tracking performance. However, it is important to note that the segmentation mask quality directly influences Cell-HOTA and OP<sub>CTB</sub> scores, which may affect comparison across models.</p></sec><sec id="sec011"><title>Analyzing bacterial mother machine movies</title><p>We first trained the four models (Cell-TRACTR, DeLTA, EmbedTrack, Trackastra) on a dataset consisting of 73 time-lapse microscopy movies of bacteria growing in the mother machine. The dataset was split 80%/20% into training and validation data. We trained each model on the training set, tuned using the validation set, and then evaluated on a separate test set which consisted of 29 time-lapse microscopy movies. As a note, we initially tested Trackastra&#x02019;s &#x0201c;General 2D&#x0201d; pretrained model, but it performed poorly on the test data. Therefore, we trained a model specifically for the bacterial mother machine dataset. Since Trackastra only performs tracking, our evaluation focuses on its tracking performance. Its detection performance directly reflects the quality of segmentation masks provided by EmbedTrack.</p><p>When analyzing Cell-HOTA<sub>&#x003b1;</sub> at various similarity thresholds &#x003b1;, we found that all four algorithms performed well on the dataset, especially at lower values of &#x003b1; (<xref rid="pcbi.1013071.g004" ref-type="fig">Fig 4A</xref>). However, there were nuanced differences in performance, with Cell-TRACTR and Trackastra exhibiting superior performance for all &#x003b1;<italic toggle="yes">&#x02009;</italic>&#x0003c;&#x02009;0.75. When &#x003b1;&#x02009;&#x0003e;&#x02009;0.8, Trackastra, EmbedTrack, and DeLTA&#x02019;s performance slightly exceed Cell-TRACTR, but all dropped off rapidly at high values of &#x003b1;. This rapid decrease is expected given the stringency of the similarity threshold in this regime. Overall, we found that all models exhibit similar performance, with Cell-TRACTR surpassing EmbedTrack and DeLTA in overall tracking performance, albeit with slightly less localization precision.</p><fig position="float" id="pcbi.1013071.g004"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013071.g004</object-id><label>Fig 4</label><caption><title>Evaluation of model performance on the bacterial mother machine dataset.</title><p>(A) Cell-HOTA evaluated at various similarity thresholds &#x003b1; on the bacterial mother machine dataset, using data from the test set. Four different algorithms are evaluated: Cell-TRACTR, DeLTA, EmbedTrack, and Trackastra. (B) Cell-HOTA<sub>0.5</sub> results on the test set. (C) Overall results on the test set. Cell-HOTA has three sub-metrics: DetA, AssA, and DivA. OP<sub>CTB</sub> has 2 sub-metrics: SEG and TRA. (D) Cell-HOTA score with and without flexible divisions. (E) Comparison of model efficiency. FPS is the inference rate measured in frames per second, so higher values correspond to faster inference times. Params is the number of parameters in each model. M, million. Training time is the number of hours needed to train each model. Since Trackastra uses precomputed segmentation masks (from EmbedTrack), its performance does not reflect segmentation capability, unlike other models, which generate masks as a part of their pipeline. These values are denoted with a * because they only include tracking.</p></caption><graphic xlink:href="pcbi.1013071.g004" position="float"/></fig><p>To gain a comprehensive understanding of model performance, we next looked at the sub-metrics of Cell-HOTA. To report a single number, it is common to see the metrics evaluated either at a specific value of &#x003b1; or averaged across all values of &#x003b1;. We first selected a single &#x003b1;, choosing &#x003b1;&#x02009;=&#x02009;0.5 because it is a typical value used in the literature [<xref rid="pcbi.1013071.ref053" ref-type="bibr">53</xref>,<xref rid="pcbi.1013071.ref071" ref-type="bibr">71</xref>]. Cell-TRACTR had the highest overall tracking performance as indicated by Cell-HOTA<sub>0.5</sub> (<xref rid="pcbi.1013071.g004" ref-type="fig">Fig 4B</xref>). Decomposing this into sub-metrics, we found that all four models have comparable performance for DetA<sub>0.5</sub>, with EmbedTrack slightly outperforming the other models. In contrast, Cell-TRACTR performs well on tracking and cell division, scoring the highest for AssA<sub>0.5</sub> and DivA<sub>0.5</sub> at levels that exceed the performance of the other three algorithms. This indicates that while all models exhibit strong detection accuracy, Cell-TRACTR performs very well on tracking and division, while EmbedTrack has the most precise segmentation. More generally, we found that Cell-TRACTR exhibited performance that was on par with current state-of-the-art algorithms, and although there were minor differences between the performance of the individual algorithms, all performed well. These results demonstrate that the transformer-based approach can generate high quality results for cell segmentation and tracking.</p><p>We also looked at the overall Cell-HOTA score, which integrates across all values of &#x003b1;, and compared it to the Cell Tracking Challenge OP<sub>CTB</sub> score. With Cell-HOTA, we found that Trackastra had the highest overall score due to its strong tracking when coupled with the high-quality segmentation masks provided by EmbedTrack. When we provided Trackastra with segmentation masks generated from Cell-TRACTR, the Cell-HOTA score dropped from 91.16 to 89.29, demonstrating the reliance of Trackastra on high quality inputs from an outside source. Cell-TRACTR and EmbedTrack exhibited closely matched performance, while DeLTA performed slightly less well (<xref rid="pcbi.1013071.g004" ref-type="fig">Fig 4C</xref>). When using the OP<sub>CTB</sub> metric, Trackastra exhibited slightly higher overall performance across the four algorithms due to its strong TRA score and high SEG score. The TRA scores for all four models showed minimal variation, while there were differences in the SEG scores. This underscores the excessive weight given to the SEG score in OP<sub>CTB</sub>. In addition, the TRA score offers limited insight into the model&#x02019;s actual detection and tracking performance (<xref rid="pcbi.1013071.s001" ref-type="supplementary-material">S1 Text</xref>). Overall, we found that the Cell-HOTA score offers an interpretable and balanced method for assessing model performance.</p><p>To understand the impact of permitting early and late divisions in Cell-HOTA, we evaluated the performance with and without flexible divisions (<xref rid="pcbi.1013071.g004" ref-type="fig">Fig 4D</xref>). All scores dropped significantly when flexible divisions were not allowed. Most notably, DivA consistently had the largest drop, demonstrating that all models struggled with pinpointing the exact frame a cell divides. It is clear that early and late divisions have a significant impact on the assessment of model performance. As the ground truth is often a subjective assessment of cell division time rather than the absolute truth, providing some leniency is likely to be critical for applications like bacterial cell tracking that exhibit many division events.</p><p>We also evaluated the time needed to process the test dataset in terms of frames per second (<xref rid="pcbi.1013071.g004" ref-type="fig">Fig 4E</xref>). The more frames a model can evaluate per second, the faster the model, thus larger values correspond to faster inference. Out of the models that performed segmentation and tracking, Cell-TRACTR had the fastest inference, with DeLTA performing second fastest. Although EmbedTrack had the slowest inference, its model utilized the least number of parameters and required the least amount of time to train. DeLTA utilized the most parameters and required the most time to train because it uses a separate model for segmentation and tracking whereas Cell-TRACTR and EmbedTrack only use one model. Since Trackastra only performs tracking, directly comparing inference speed and the total number of parameters is challenging. Therefore, we report these numbers for reference but note that they cannot be directly compared to the other models since they only include the time associated with tracking but not segmentation.</p></sec><sec id="sec012"><title>Analyzing mammalian DeepCell microscopy movies</title><p>We next asked whether Cell-TRACTR would perform well on a very different cell segmentation and tracking task. For this, we considered data from the DynamicNuclearNet Tracking dataset from DeepCell [<xref rid="pcbi.1013071.ref022" ref-type="bibr">22</xref>]. In this dataset, five mammalian cells lines were cultured on 96-well plates. Nuclear staining was performed to assist with cell identification. In this two-dimensional dataset, the mammalian cells can grow in any direction, in contrast to bacterial growth in the mother machine which is constrained to a single direction. The DeepCell dataset included images containing as few as 3 cells and as many as 281 cells in a single frame. In addition to DeLTA, EmbedTrack, and Trackastra, we also benchmarked against Caliban, which was specifically designed to perform well on the DeepCell dataset. Caliban utilizes a feature pyramid network [<xref rid="pcbi.1013071.ref072" ref-type="bibr">72</xref>] for segmentation and a long short-term memory model [<xref rid="pcbi.1013071.ref073" ref-type="bibr">73</xref>] for tracking. Both Caliban and Trackastra have demonstrated state-of-the-art results on the DynamicNuclearNet Tracking dataset. Following the Trackastra paper [<xref rid="pcbi.1013071.ref041" ref-type="bibr">41</xref>], we utilized the &#x0201c;General 2D&#x0201d; pretrained model and provided it the segmentation masks generated by Caliban.</p><p>Using the Cell-HOTA tracking metric, we evaluated the performance of Cell-TRACTR, DeLTA, EmbedTrack, Caliban, and Trackastra on the DeepCell dataset. Caliban demonstrated the highest overall performance, and Cell-TRACTR exhibited comparable performance for &#x003b1;<italic toggle="yes">&#x02009;</italic>&#x0003c;&#x02009;0.50 (<xref rid="pcbi.1013071.g005" ref-type="fig">Fig 5A</xref>). As before, we evaluated Cell-HOTA at &#x003b1;&#x02009;=&#x02009;0.5 (<xref rid="pcbi.1013071.g005" ref-type="fig">Fig 5B</xref>) and used the integrated Cell-HOTA score (<xref rid="pcbi.1013071.g005" ref-type="fig">Fig 5C</xref>). We found Caliban to have the highest overall Cell-HOTA<sub>0.5</sub>, DetA<sub>0.5</sub> and DivA<sub>0.5</sub> score whereas Cell-TRACTR had the best AssA<sub>0.5</sub> score, though both models performed well across all these metrics (<xref rid="pcbi.1013071.g005" ref-type="fig">Fig 5B</xref>). For the overall Cell-HOTA score, we found that Caliban had the highest DetA and DivA scores whereas Trackastra had the highest AssA score (<xref rid="pcbi.1013071.g005" ref-type="fig">Fig 5C</xref>). The OP<sub>CTB</sub> metrics confirm EmbedTrack&#x02019;s overall quality in segmentation precision (SEG score), and Caliban achieved the top OP<sub>CTB</sub> TRA score (<xref rid="pcbi.1013071.s001" ref-type="supplementary-material">S1 Text</xref>). Overall, the results from the DeepCell dataset suggest that Cell-TRACTR exhibits detection, tracking, and division accuracy that are comparable to state-of-the-art algorithms (<xref rid="pcbi.1013071.s003" ref-type="supplementary-material">S2 Movie</xref>). This includes comparison to the Caliban algorithm, which was specifically tailored using the DeepCell dataset. As with the mother machine data, performance decreases with very stringent values of &#x003b1;. Furthermore, these results also demonstrate that Cell-TRACTR can handle diverse cell segmentation and tracking tasks, as evidenced by the algorithm&#x02019;s strong performance on both bacterial and mammalian time-lapse microscopy data.</p><fig position="float" id="pcbi.1013071.g005"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013071.g005</object-id><label>Fig 5</label><caption><title>Evaluation of model performance on the mammalian DeepCell dataset.</title><p>(A) Cell-HOTA evaluated at various similarity thresholds &#x003b1; on the mammalian DeepCell dataset, using data from the test set. Five different algorithms are evaluated: Cell-TRACTR, DeLTA, EmbedTrack, Caliban, and Trackastra. (B) Cell-HOTA<sub>0.5</sub> results on the test set. (C) Overall results on the test set. Cell-HOTA has three sub-metrics: DetA, AssA, and DivA. OP<sub>CTB</sub> has 2 sub-metrics: SEG and TRA. (D) Comparison of model efficiency. FPS is the inference rate measured in frames per second, so higher values correspond to faster inference times. Params is the number of parameters in each model. M, million. Training time is the number of hours needed to train each model. Since Trackastra uses precomputed segmentation masks (from Caliban), its performance does not reflect segmentation capability, unlike other models, which generate masks as a part of their pipeline. These values are denoted with a * because they only include tracking. Training times are not reported for Caliban and Trackastra because we used pretrained models.</p></caption><graphic xlink:href="pcbi.1013071.g005" position="float"/></fig><p>When analyzing inference rate, we found that Caliban had the fastest inference. Cell-TRACTR, EmbedTrack, and DeLTA had slower inference rates compared to the bacterial mother machine test data due to the larger image sizes and increased number of cells. EmbedTrack utilized the least number of parameters and required the least amount of training time. While Cell-TRACTR needed far fewer parameters than DeLTA, both required extensive training times. Although training times for Cell-TRACTR are currently long when image sizes are large, as we discuss below, recent improvements in computational efficiency for DETR-based algorithms could address this issue. Since we used pretrained models for Trackastra and Caliban, no training times are reported.</p></sec></sec><sec sec-type="conclusions" id="sec013"><title>Discussion</title><p>In this work, we developed a new transformer-based architecture, Cell-TRACTR, and showed that it can segment and track cells within time-lapse microscopy movies, exhibiting performance on par with current state-of-the-art software. This architecture, based on TrackFormer [<xref rid="pcbi.1013071.ref044" ref-type="bibr">44</xref>], can track cells in an end-to-end manner without any post-processing. The attention mechanism allows the transformer to make long-range connections, integrating information across distant parts of the image [<xref rid="pcbi.1013071.ref037" ref-type="bibr">37</xref>]. DETR-based models, such as those in [<xref rid="pcbi.1013071.ref042" ref-type="bibr">42</xref>,<xref rid="pcbi.1013071.ref059" ref-type="bibr">59</xref>], have emerged as state-of-the-art tracking models on the DanceTrack dataset, which features objects with irregular motion and uniform appearance, motivating our focus on them in this work. In practice, we found Cell-TRACTR to be well suited to address challenges associated with non-linear and rapid cell motion.</p><p>Cell-TRACTR uses track queries to take advantage of a cell&#x02019;s past history to make a coherent prediction in the subsequent frame. These track queries also allow for segmentation and tracking to be accomplished in one unified step which may result in higher accuracy. Jointly addressing these tasks allows the model to leverage shared information between segmentation and tracking. Prior studies have shown that this can lead to better performance. For example, studies with TrackFormer showed that training a model jointly on tracking and segmentation improved the overall tracking performance compared to training a model on tracking alone [<xref rid="pcbi.1013071.ref044" ref-type="bibr">44</xref>]. In addition, MOTIP demonstrated that training a model jointly on object detection and tracking improved the HOTA score by a significant margin compared to first training a model on object detection and then using it as the foundation for training the tracking module separately [<xref rid="pcbi.1013071.ref059" ref-type="bibr">59</xref>]. Although other deep learning models, like EmbedTrack [<xref rid="pcbi.1013071.ref024" ref-type="bibr">24</xref>] and DistNet2D [<xref rid="pcbi.1013071.ref023" ref-type="bibr">23</xref>], can simultaneously segment and track cells, they typically require multiple decoders, whereas Cell-TRACTR uses a single decoder. This helps ensure that segmentation and tracking predictions are in sync across subsequent frames because the use of separate decoders for segmentation and tracking can lead to discrepancies in the predictions. A potential future advantage of this DETR-based architecture is that it could allow Cell-TRACTR to handle objects that are occluded or disappear. For example, Cell-TRACTR could discard track queries only if they are classified as &#x02018;no object&#x02019; for multiple frames, as in TrackFormer [<xref rid="pcbi.1013071.ref044" ref-type="bibr">44</xref>]. To the best of our knowledge, Cell-TRACTR is the first algorithm to simultaneously segment and track cells with a single decoder while requiring no post-processing.</p><p>To assess algorithm performance, we extended the widely-used HOTA metric to incorporate cell division, creating the Cell-HOTA metric. We also calculated performance using the traditional Cell Tracking Challenge OP<sub>CTB</sub> metric as a point of comparison. Cell-HOTA can be broken down into detection, association, and division accuracy, all of which are calculated across a range of similarity thresholds,&#x003b1;. We found that Cell-TRACTR performed well on association and division accuracy while maintaining solid detection capabilities. However, Cell-TRACTR had reduced segmentation precision compared to other algorithms at high similarity thresholds. It is important to note that Cell-TRACTR may not be the optimal choice for datasets where segmentation precision is the top priority (i.e., cases where very high values of the similarity threshold &#x003b1; are required). Overall, the Cell-HOTA metric enables the assessment of detection, association, and division, offering crucial insights for evaluating algorithm performance in cell tracking applications.</p><p>We found that Cell-TRACTR delivered strong performance on both a bacterial mother machine dataset and a mammalian dataset from DeepCell. However, we did encounter computational limits when working with large two-dimensional datasets. For example, training times for two-dimensional images containing hundreds of cells in 584x600 pixel images exceeded a week, making hyperparameter optimization infeasible, leading us to conclude that this goal was impractical with the current version of Cell-TRACTR. A well-documented limitation of transformers is their computational complexity, a challenge that can be pronounced with microscopy data, where image sizes can be large, and images contain many cells at low resolution. The need to process large images leads to large multi-scale feature maps, while the low resolution of the cells necessitates an increase in the number of multi-scale feature maps, making this a computationally demanding task. However, there is recent promising work focused on making DETR-based models more computationally efficient, which offers future potential for this area. Examples include RT-DETRv3 [<xref rid="pcbi.1013071.ref074" ref-type="bibr">74</xref>], DEIM [<xref rid="pcbi.1013071.ref075" ref-type="bibr">75</xref>], and Efficient DETR [<xref rid="pcbi.1013071.ref052" ref-type="bibr">52</xref>], which increase computational efficiency, while SOF-DETR [<xref rid="pcbi.1013071.ref076" ref-type="bibr">76</xref>] and work by Huang <italic toggle="yes">et al</italic>. [<xref rid="pcbi.1013071.ref077" ref-type="bibr">77</xref>] offer approaches that are better at handling small objects. These new models mainly focus on engineering the encoder to be more efficient. Building upon the momentum for increased computational efficiency, DEFA [<xref rid="pcbi.1013071.ref078" ref-type="bibr">78</xref>] proposed a more efficient form of deformable attention that significantly reduces memory usage and training time. Departing from the tracking-by-query paradigm, MOTIP [<xref rid="pcbi.1013071.ref059" ref-type="bibr">59</xref>] utilizes a DETR-based model to generate object embeddings for each frame which are used to address an ID prediction problem similar to Trackastra [<xref rid="pcbi.1013071.ref041" ref-type="bibr">41</xref>]. Specifically focusing on segmentation, CelloType [<xref rid="pcbi.1013071.ref040" ref-type="bibr">40</xref>], which is based on Mask DINO [<xref rid="pcbi.1013071.ref053" ref-type="bibr">53</xref>], has demonstrated top performance compared to other state-of-the-art cell segmentation models. Similarly, it would be interesting to explore a two-stage training approach for Cell-TRACTR, first pretraining the model on cell segmentation and then fine-tuning it for cell tracking. This training strategy could improve computational efficiency by allowing parts of the model to be frozen during fine-tuning. For instance, when adapting the model to tracking, the backbone weights could be frozen, reducing computational overhead. Given the recent focus on improving the computational efficiency of DETR-like algorithms, we anticipate that future advances in this space will likely address some of the limitations we encountered. Integrating these improvements into Cell-TRACTR could enable cell segmentation and tracking of larger and increasingly complex time-lapse images.</p><p>Another computational challenge is that DETR-based models that perform segmentation require a large amount of memory, as each output embedding produces a segmentation mask of the whole image. This is computationally demanding, for example when there are 250 cells in an image, 250 full-sized segmentation masks will be produced. When working with large two-dimensional images, we sometimes encountered CUDA out-of-memory errors during backpropagation for the segmentation masks. Future efforts could focus on only backpropagating the loss through sampled regions within the segmentation mask to reduce the computational load. Recently, Frozen Mask-DETR [<xref rid="pcbi.1013071.ref071" ref-type="bibr">71</xref>] developed a computationally efficient way to segment images with DETR. The model uses DINO-DETR [<xref rid="pcbi.1013071.ref050" ref-type="bibr">50</xref>] to predict bounding boxes, which are used as regions of interest that are segmented by an Instance Mask Decoder. This method significantly reduces the computational load by only segmenting the area within the bounding box while also outperforming Mask-DINO [<xref rid="pcbi.1013071.ref053" ref-type="bibr">53</xref>] in instance segmentation.</p><p>Similar to our work, Trackastra developed a computationally efficient way to track cells utilizing transformers [<xref rid="pcbi.1013071.ref041" ref-type="bibr">41</xref>]. Their method consists of feeding object positions and shallow object features to a transformer and outputting an association matrix. Shallow object features include characteristics such as cell area and cell shape. The association matrix details how cells are related between two adjacent frames. Different from Trackastra, our work utilizes a tracking-by-query paradigm, can segment cells in addition to tracking cells, and requires no post-processing. While Trackastra has developed a Napari plugin, a future direction for Cell-TRACTR would be to integrate our algorithm into a user-friendly workflow to improve accessibility. Trackastra is a nice complement to our work and is a lightweight transformer-based model for cell tracking.</p><p>Cell-TRACTR currently processes one frame at a time, whereas some other cell tracking algorithms analyze multiple frames at a time to incorporate temporal information into their models [<xref rid="pcbi.1013071.ref022" ref-type="bibr">22</xref>,<xref rid="pcbi.1013071.ref023" ref-type="bibr">23</xref>,<xref rid="pcbi.1013071.ref028" ref-type="bibr">28</xref>] and this is a potential extension of this work. Specifically, in Cell-TRACTR, track queries only attend to image features in the current frame of interest, however, they could attend to the previous and future frames for additional context. This could be achieved by stacking multi-scale features from a sequence of frames and feeding them to the encoder, as in TrackFormer [<xref rid="pcbi.1013071.ref044" ref-type="bibr">44</xref>]. However, the feasibility of this approach is contingent on significant improvements in the computational efficiency of the encoder and decoder. Another extension of this work could include using the output embeddings for other auxiliary tasks like performing classification based on cell morphology [<xref rid="pcbi.1013071.ref040" ref-type="bibr">40</xref>]. For such tasks, the weights of Cell-TRACTR could be frozen to ensure the segmentation and tracking performance remain high while minimizing the computational load.</p><p>The Cell-HOTA metric could also be extended in several ways. We designed Cell-HOTA to work well on time-lapse images of cells that divide frequently. As implemented, Cell-HOTA weighs DivA and AssA equally, which places a high emphasis on division accuracy. For other use cases where cell division is less common, it may be more appropriate to adjust the metric to de-emphasize DivA relative to the other accuracy values. In addition, a current limitation of Cell-HOTA is that it only accepts flexible divisions that are one frame early or late. This could be modified to allow early or late divisions that occur two or more frames away from the ground truth.</p><p>While this manuscript was under review, Kaiser et al. [<xref rid="pcbi.1013071.ref079" ref-type="bibr">79</xref>] introduced a novel cell tracking metric called CHOTA, which is similar to Cell-HOTA. CHOTA also extends the HOTA framework by leveraging the DetA and AssA sub-metrics. However, unlike Cell-HOTA, which incorporates a Division Accuracy (DivA) sub-metric to explicitly evaluate the accuracy of division predictions, CHOTA shifts the focus to lineage-level trajectory analysis. In Cell-HOTA, AssA is calculated for each individual cell, regardless of whether it divides. In contrast, CHOTA evaluates AssA at the lineage level, measuring association not just for a single cell, but also for its progeny. For instance, if a cell divides, CHOTA measures AssA for the parent cell as well as its two daughter cells, providing a more holistic view of lineage continuity. While CHOTA incorporates lineage information in the metric, this approach can decrease interpretability. Cell-HOTA, on the other hand, provides a distinct assessment of model performance in predicting cell division events, making it easier to diagnose errors. Additionally, Cell-HOTA places a greater emphasis on division accuracy and uniquely accounts for early and late divisions. Overall, Cell-HOTA is better suited for applications where interpretable feedback is prioritized, while CHOTA is more appropriate for applications where lineage continuity takes precedence. Both metrics offer marked improvements over the traditional OP<sub>CTB</sub> metric for balancing segmentation and tracking accuracy.</p><p>In summary, we introduce Cell-TRACTR, a novel deep learning model that can segment and track cells across subsequent frames in time-lapse images. Our model uses track queries, which leverage a cell&#x02019;s history, to simultaneously segment and track cells. The self-attention mechanism in the decoder plays a crucial role in eliminating duplicate predictions and differentiating between object and track queries. We also introduced a new cell tracking metric, Cell-HOTA, and demonstrated that Cell-TRACTR performs favorably compared to other state-of-the-art cell tracking algorithms and is particularly well-suited to association and division tasks. As transformers and DETR-based models continue to progress, we anticipate that Cell-TRACTR can be adapted to increase efficiency, enabling it to handle predictions using larger images and those crowded with cells.</p></sec><sec sec-type="materials|methods" id="sec014"><title>Methods</title><sec id="sec015"><title>Training Cell-TRACTR</title><p>For the bacterial mother machine dataset, we trained Cell-TRACTR for 12 epochs with a batch size of 2. The number of object queries was set to 30. The transformer consisted of 4 encoder layers and 4 decoder layers. The target size was 256x32 pixels. Images had to be resized to fit the target size. All configurations and hyperparameters are detailed in train_moma.yaml file within the cfgs folder on the GitLab repository. A V100 GPU was utilized.</p><p>For the DeepCell dataset, we trained Cell-TRACTR for 24 epochs with a batch size of 1. The number of object queries was set to 400. The transformer consisted of 4 encoder layers and 4 decoder layers. The target size was 584x600 pixels. Images were not resized since they were already the correct target size. All configurations and hyperparameters are detailed in train_DynamicNuclearNet-tracking-v1_0.yaml file within the cfgs folder on the GitLab repository. We used the A100 GPU to alleviate out-of-memory issues (<xref rid="pcbi.1013071.s001" ref-type="supplementary-material">S1 Text</xref>).</p><p>Model checkpoints are available on Zenodo (<ext-link xlink:href="https://zenodo.org/records/14509424" ext-link-type="uri">https://zenodo.org/records/14509424</ext-link>).</p><p>Next, we detail important parameters used to train Cell-TRACTR for both datasets. We used a learning rate of 0.0002 for the encoder-decoder and a learning rate of 0.00002 for the CNN backbone. Both learning rates were dropped by a factor of 10 at epoch 12 and 20 for the models trained on the bacterial mother machine and DeepCell datasets, respectively. We used AdamW [<xref rid="pcbi.1013071.ref080" ref-type="bibr">80</xref>] as the optimizer with a weight decay of 0.0001. We adopted ResNet-18 as the backbone CNN for the bacterial mother machine dataset and ResNet-50 as the backbone CNN for the DeepCell dataset and fed the last 3 feature maps to the encoder. The first decoder layer was used for object detection and subsequent layers were used for tracking (<xref rid="pcbi.1013071.s009" ref-type="supplementary-material">S5 Fig</xref> and <xref rid="pcbi.1013071.s001" ref-type="supplementary-material">S1 Text</xref>).</p><p>We used a clip size of up to 3 frames to train the model. The model is either fed 1, 2, or 3 frames. When 1 frame is fed to the model, only object detection is performed. When 2 frames are fed to the model, object detection is performed in the first frame and tracking is performed on the subsequent frame. When 3 frames are fed to the model, object detection is performed in the first frame and tracking is performed on the two subsequent frames. We do not exclusively feed the model 3 frames because it needs to be trained explicitly on object detection as the loss is only calculated for the final frame within the clip. For example, when 3 frames are fed to the model, backpropagation will not affect false positives predicted during object detection in the first frame. It is important that the model learns to track for at least 3 sequential frames because the model needs to track cells that have just divided. This is a unique task for DETR-based tracking models, as cells that have just divided contain different positional embeddings but the same content embeddings (<xref rid="pcbi.1013071.s009" ref-type="supplementary-material">S6 Fig</xref>).</p><p>Following the approach used in TrackFormer [<xref rid="pcbi.1013071.ref044" ref-type="bibr">44</xref>], we utilize false negatives and false positives during training to improve the robustness of the model. False negatives are incorporated to teach the model to identify cells that were previously undetected due to errors. We implemented conditions where there is a probability of 0.5 that at least one of the track queries will be dropped, with a maximum of 20% of the track queries potentially being dropped. When a track query is dropped, an object query is forced to detect the cell. False positives are also incorporated to ensure that the model can ignore erroneous predictions made in previous frames. In our implementation, there is a probability of 0.5 that at least one false positive will be added, with a maximum of 20% of the total number of track queries added as false positives. False positives are generated as noisy track queries. To generate the positional embedding for the false positive, random noise is added to the bounding box of a track query with &#x003bb;<sub>1</sub>&#x02009;=&#x02009;0.2 and &#x003bb;<sub>2</sub>&#x02009;=&#x02009;0.1 (<xref rid="pcbi.1013071.s001" ref-type="supplementary-material">S1 Text</xref>). To generate the content embedding for the false positive, random noise drawn from a normal distribution with a mean of 0 and a standard deviation of 0.25 is added to the content embedding of the respective track query. False positives and false negatives were only incorporated in the final frame of the clip when the clip size was greater than 1. Compared to TrackFormer [<xref rid="pcbi.1013071.ref044" ref-type="bibr">44</xref>], we adjusted the frequency of false positives and false negatives and the generation of the positional and content embeddings for the false positives to simplify the code.</p></sec><sec id="sec016"><title>Loss function</title><p>The loss function is a culmination of losses, including the class label loss, bounding box loss, and segmentation mask loss. The class label loss is calculated using focal loss with class weighting factor &#x003b1;&#x02009;=&#x02009;0.25 and focusing parameter &#x003b3;&#x02009;=&#x02009;2. The bounding box loss is a linear combination of the L1 loss and generalized IOU loss. The segmentation mask loss is a linear combination of the Dice Loss and binary cross entropy. To account for cell divisions, we average the loss across the two divided cells. Therefore, a cell division would have equal weight compared to a single cell. The loss functions were taken directly from DETR [<xref rid="pcbi.1013071.ref046" ref-type="bibr">46</xref>] and modified to handle cell divisions.</p></sec><sec id="sec017"><title>Data augmentations</title><p>To help generalize our model, we applied three data augmentation techniques during training: random Gaussian blur, random Gaussian noise, and random illumination adjustments. All three techniques were applied with a probability of 0.4 for each augmentation for a sequence of frames. These were chosen to reflect imaging conditions seen in microscopy data. These data augmentations were all implemented using the approach described in [<xref rid="pcbi.1013071.ref010" ref-type="bibr">10</xref>,<xref rid="pcbi.1013071.ref020" ref-type="bibr">20</xref>].</p></sec><sec id="sec018"><title>Training DeLTA</title><p>For the bacterial mother machine dataset, we trained the segmentation and tracking models for 12 epochs with a batch size of 1. The target size was 256x32 pixels. The &#x0201c;mother machine&#x0201d; configuration in DeLTA was utilized. All default data augmentations were included.</p><p>For the DeepCell dataset, we trained the segmentation and tracking model for 24 epochs with a batch size of 1, incorporating a patience parameter of 5 epochs. The target size was 256x256 pixels. The &#x0201c;2D&#x0201d; configuration in DeLTA was used, in addition to all default data augmentations.</p><p>Additionally, we modified the code to directly train on Cell Tracking Challenge formatted datasets.</p></sec><sec id="sec019"><title>Training EmbedTrack</title><p>For the bacterial mother machine dataset, we trained EmbedTrack for 12 epochs with a batch size of 16. Although cropping images is the default for EmbedTrack, the small size of the mother machine images made this step unnecessary as cropping is used to circumvent out-of-memory errors. Instead, we adjusted the code to resize all images to a uniform input size of 256x32 pixels. We removed the random rotation data augmentations from EmbedTrack, as they were incompatible with a crop size that was not a square. A V100 GPU was used for training.</p><p>For the DeepCell dataset, we trained EmbedTrack for 24 epochs with a batch size of 4. The batch size was reduced from the default size of 16 to prevent out-of-memory errors. The images were cropped to 256x256 pixels, which is the default crop size for EmbedTrack. Since the images in this dataset were cropped during training and inference, there was no need for additional resizing. All default data augmentations from EmbedTrack were utilized. A V100 GPU was used for training.</p><p>All models were trained for 12 epochs on the bacterial mother machine dataset and 24 epochs on the mammalian DeepCell dataset. Each epoch involved iterating through every image in the dataset once, ensuring equal training time for each model. We used the default configurations and training parameters when applicable.</p></sec><sec id="sec020"><title>Training Trackastra</title><p>For the bacterial mother machine dataset, we trained Trackastra using the example_config.yaml file provided on the GitHub page (<ext-link xlink:href="https://github.com/weigertlab/trackastra" ext-link-type="uri">https://github.com/weigertlab/trackastra</ext-link>). For the DeepCell dataset, we used the code and pretrained weights provided by the Weigert lab.&#x000a0; Specifically, we used the &#x0201c;general_2d&#x0201d; model and the ILP based linking as that was shown to perform better than the greedy linking approach [<xref rid="pcbi.1013071.ref041" ref-type="bibr">41</xref>].</p></sec><sec id="sec021"><title>Training Caliban</title><p>We used the code and pretrained weights provided by the Van Valen lab (<ext-link xlink:href="https://github.com/vanvalenlab/Caliban-2024_Schwartz_et_al" ext-link-type="uri">https://github.com/vanvalenlab/Caliban-2024_Schwartz_et_al</ext-link>).</p></sec><sec id="sec022"><title>HOTA metric overview</title><p>To calculate the HOTA score, we used the approach from [<xref rid="pcbi.1013071.ref062" ref-type="bibr">62</xref>], which we summarize here for completeness. We first used the Hungarian algorithm [<xref rid="pcbi.1013071.ref081" ref-type="bibr">81</xref>] to match the tracker cells with the ground truth cells that generated the highest HOTA score. DetA is measured as the Jaccard Similarity where a match is considered a true positive (TP) if IOU&#x02009;&#x0003e;&#x02009;&#x003b1; (<xref rid="pcbi.1013071.e002" ref-type="disp-formula">Equation 1</xref>). False positives (FP) occur when a tracker cell does not match to any ground truth cells and false negatives (FN) occur when a ground truth cell does not match to any tracker cells. A match with an IOU below the threshold &#x003b1; is considered a FP for the tracker cell and a FN for the ground truth cell. To instill localization accuracy into the detection accuracy, DetA<sub>&#x003b1;</sub> is measured at multiple thresholds of &#x003b1;. In other words, the detection accuracy depends upon the value of the threshold &#x003b1;.</p><disp-formula id="pcbi.1013071.e002">
<alternatives><graphic xlink:href="pcbi.1013071.e002.jpg" id="pcbi.1013071.e002g" position="anchor"/><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(1)</label>
</disp-formula><p>To calculate AssA, for all the true positives detected at each &#x003b1;, the number of matches for each tracker and ground truth cell are enumerated (<xref rid="pcbi.1013071.e003" ref-type="disp-formula">Equation 2</xref>). A false positive association (FPA) occurs when the tracker cell matches to a different ground truth cell. A false negative association (FNA) occurs when the ground truth cell matches to a different tracker cell. A(c) is the association score for a single detected true positive where c is a detected true positive. AssA<sub>&#x003b1;</sub> calculates the association score over all detected true positives for a given value of &#x003b1; (<xref rid="pcbi.1013071.e004" ref-type="disp-formula">Equation 3</xref>).</p><disp-formula id="pcbi.1013071.e003">
<alternatives><graphic xlink:href="pcbi.1013071.e003.jpg" id="pcbi.1013071.e003g" position="anchor"/><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(2)</label>
</disp-formula><disp-formula id="pcbi.1013071.e004">
<alternatives><graphic xlink:href="pcbi.1013071.e004.jpg" id="pcbi.1013071.e004g" position="anchor"/><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>&#x003f5;</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(3)</label>
</disp-formula><p>HOTA<sub>&#x003b1;</sub> is calculated by taking the geometric mean of DetA<sub>&#x003b1;</sub> and AssA<sub>&#x003b1;</sub> (<xref rid="pcbi.1013071.e005" ref-type="disp-formula">Equation 4</xref>). This is typically measured at values of &#x003b1; ranging from 0.05 to 0.95 in increments of 0.05.</p><disp-formula id="pcbi.1013071.e005">
<alternatives><graphic xlink:href="pcbi.1013071.e005.jpg" id="pcbi.1013071.e005g" position="anchor"/><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(4)</label>
</disp-formula><p>The final HOTA score is calculated by taking the average Cell-HOTA<sub>&#x003b1;</sub> score across all &#x003b1; values (<xref rid="pcbi.1013071.e006" ref-type="disp-formula">Equation 5</xref>).</p><disp-formula id="pcbi.1013071.e006">
<alternatives><graphic xlink:href="pcbi.1013071.e006.jpg" id="pcbi.1013071.e006g" position="anchor"/><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mo>&#x02248;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>19</mml:mn></mml:mrow></mml:mfrac><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mn>0.05</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mn>0.9</mml:mn><mml:mo>,</mml:mo><mml:mn>0.95</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(5)</label>
</disp-formula></sec><sec id="sec023"><title>Cell-HOTA metric</title><p>To allow Cell-HOTA to handle cell division, we added a new sub-metric called division accuracy, DivA. DivA<sub>&#x003b1;</sub> is measured as the Jaccard Similarity across all cell divisions at a similarity threshold <italic toggle="yes">&#x003b1;</italic> (<xref rid="pcbi.1013071.e007" ref-type="disp-formula">Equation 6</xref>). DivA<sub>&#x003b1;</sub> is calculated using the same approach as DetA<sub>&#x003b1;</sub></p><disp-formula id="pcbi.1013071.e007">
<alternatives><graphic xlink:href="pcbi.1013071.e007.jpg" id="pcbi.1013071.e007g" position="anchor"/><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>(6)</label>
</disp-formula><p>We define AssDivA<sub>&#x003b1;</sub> as the geometric mean of AssA<sub>&#x003b1;</sub> and DivA<sub>&#x003b1;</sub> at a similarity threshold <italic toggle="yes">&#x003b1;</italic> (<xref rid="pcbi.1013071.e008" ref-type="disp-formula">Equation 7</xref>). Association and division accuracy are equally weighted to emphasize the importance of division accuracy.</p><disp-formula id="pcbi.1013071.e008">
<alternatives><graphic xlink:href="pcbi.1013071.e008.jpg" id="pcbi.1013071.e008g" position="anchor"/><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives>
<label>(7)</label>
</disp-formula><p>Cell-HOTA<sub>&#x003b1;</sub> is the geometric mean of DetA<sub>&#x003b1;</sub> and AssDivA<sub>&#x003b1;</sub> at a similarity threshold <italic toggle="yes">&#x003b1;</italic> (<xref rid="pcbi.1013071.e009" ref-type="disp-formula">Equation 8</xref>). This calculation is the same as HOTA<sub>&#x003b1;</sub> except AssDivA<sub>&#x003b1;</sub> replaces AssA<sub>&#x003b1;</sub>.</p><disp-formula id="pcbi.1013071.e009">
<alternatives><graphic xlink:href="pcbi.1013071.e009.jpg" id="pcbi.1013071.e009g" position="anchor"/><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mtext>Cell-</mml:mtext><mml:msub><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives>
<label>(8)</label>
</disp-formula><p>Cell-HOTA is a measured by taking the average Cell-HOTA<sub>&#x003b1;</sub> score across all values of &#x003b1; (<xref rid="pcbi.1013071.e010" ref-type="disp-formula">Equation 9</xref>). This is calculated using the same approach as with HOTA.</p><disp-formula id="pcbi.1013071.e010">
<alternatives><graphic xlink:href="pcbi.1013071.e010.jpg" id="pcbi.1013071.e010g" position="anchor"/><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:mtext>Cell-</mml:mtext><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mtext>Cell-</mml:mtext><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mo>&#x02248;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>19</mml:mn></mml:mrow></mml:mfrac><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mn>0.05</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mn>0.9</mml:mn><mml:mo>,</mml:mo><mml:mn>0.95</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mtext>Cell-</mml:mtext><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(9)</label>
</disp-formula></sec><sec id="sec024"><title>Cell-HOTA and flexible divisions</title><p>A flexible early division occurs when a tracker cell divides at time, for example, t<sub>1</sub> and the matched ground truth cell divides in the next frame, t<sub>2</sub> in this example (<xref rid="pcbi.1013071.s010" ref-type="supplementary-material">S7 Fig</xref>). To check for flexible early divisions at time t<sub>1</sub>, we iterate through all ground truth cells that divide at time t<sub>2</sub> and match to a tracker cell that divides at time t<sub>1</sub>. Next, we check that the tracker and ground truth parent cells at time t<sub>0</sub> match and the tracker and ground truth daughter cells match at time t<sub>2</sub>. Similarly, a flexible late division occurs when a ground truth cell divides at time t<sub>1</sub> and the matched tracker cell divides in the next frame at time t<sub>2</sub>. To check for flexible late divisions at time t<sub>1</sub>, we iterate through all tracker cells at time t<sub>1</sub> and check if any divide at time t<sub>2</sub> and match to a ground truth cell that divides at time t<sub>1</sub>. Next, we check that the tracker and ground truth parent cells at time t<sub>0</sub> match and the tracker and ground truth daughter cells match at time t<sub>2</sub>. For both flexible early and late divisions, we check that the average IOU between the daughter cells at time t<sub>2</sub> is greater than the similarity threshold &#x003b1;.</p><p>When a flexible division is detected, the DetA<sub>&#x003b1;</sub> score is adjusted as if the tracker cell divided in the same frame as the ground truth cell. For example, when there is a flexible early division, the IOU is calculated between two tracker cells and one ground truth cell where the two tracker cells are treated as one object (<xref rid="pcbi.1013071.s010" ref-type="supplementary-material">S7 Fig</xref>). When a flexible late division occurs, the IOU is calculated between the two ground truth cells and one tracker cell where the two ground truth cells are treated as one. This IOU value is counted twice since there are two ground truth cells. This is done to ensure each ground truth cell has the same weight and the score remains the same whether a tracker division was predicted at the correct time or a frame early or late.</p><p>As with DetA<sub>&#x003b1;</sub>, we adjust the AssA<sub>&#x003b1;</sub> score to match the ground truth for flexible divisions. When a flexible early division occurs, a match is added between the ground truth cell and parent of the tracker cells and a match is removed from the ground truth cell and one of the daughter tracker cells. When a flexible late division occurs, a match is added between the two ground truth cells and the two divided tracker cells in the subsequent frame and a match is removed between one of the ground truth cells and the tracker cell in the current frame.</p></sec></sec><sec id="sec025" sec-type="supplementary-material"><title>Supporting information</title><supplementary-material id="pcbi.1013071.s001" position="float" content-type="local-data"><label>S1 Text</label><caption><title>Includes supplementary text, methods, and references.</title><p>(PDF)</p></caption><media xlink:href="pcbi.1013071.s001.pdf"/></supplementary-material><supplementary-material id="pcbi.1013071.s002" position="float" content-type="local-data"><label>S1 Movie</label><caption><title>Time-lapse microscopy movie from the bacterial mother machine test set processed by Cell-TRACTR.</title><p>Different colors represent unique cells being tracked. Black arrows indicate cell division. The temporal resolution of the movie is one frame every five minutes. Both the bounding boxes and segmentation masks are displayed for visualization purposes. However, during inference, only the segmentation masks are used.</p><p>(MP4)</p></caption><media xlink:href="pcbi.1013071.s002.mp4" mimetype="video" mime-subtype="mp4"/></supplementary-material><supplementary-material id="pcbi.1013071.s003" position="float" content-type="local-data"><label>S2 Movie</label><caption><title>Time-lapse microscopy movie from the mammalian DeepCell test set processed by Cell-TRACTR.</title><p>Different colors represent unique cells being tracked. Black arrows indicate cell division. The temporal resolution of the movie is one frame every five minutes. Both the bounding boxes and segmentation masks are displayed for visualization purposes. However, during inference, only the segmentation masks are used.</p><p>(MP4)</p></caption><media xlink:href="pcbi.1013071.s003.mp4" mimetype="video" mime-subtype="mp4"/></supplementary-material><supplementary-material id="pcbi.1013071.s004" position="float" content-type="local-data"><label>S3 Movie</label><caption><title>Time-lapse microscopy movie from the bacterial mother machine test set processed by Cell-TRACTR and evaluated with the Cell Tracking Challenge (CTC) OP<sub>CTB</sub> metrics.</title><p>From left to right, the first column shows the raw images, the second column shows the ground truths, and the third column shows the predictions. Distinct colors represent unique cells being tracked over time. The right six columns show the operations used to transform the predicted graph into the reference graph provided by the ground truth. The operations include delete vertex (FP), add vertex (FN), split vertex (NS), add edge (EA), delete edge (ED), and alter the edge semantics (EC). When one of these operations is applied, the cell is shown in red.</p><p>(MP4)</p></caption><media xlink:href="pcbi.1013071.s004.mp4" mimetype="video" mime-subtype="mp4"/></supplementary-material><supplementary-material id="pcbi.1013071.s005" position="float" content-type="local-data"><label>S1 Fig</label><caption><title>The encoder employs deformable self-attention to transform multi-scale features into encoded multi-scale features. Query selection is used to derive the object queries from these encoded multi-scale features. Each encoded image feature undergoes classification, and the top-K features are selected as the content embeddings for the object queries. Additionally, bounding boxes are extracted from the top-K segmentation masks and are subsequently converted into positional embeddings. Following this, the object queries perform self-attention and deformable cross-attention with the encoded multi-scale features to produce the output embeddings. MLP, multi-layer perceptron.</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013071.s005.tif"/></supplementary-material><supplementary-material id="pcbi.1013071.s006" position="float" content-type="local-data"><label>S2 Fig</label><caption><title>Example illustrating how object and track queries interact using an attention map processed by Cell-TRACTR across two subsequent frames. The schematic displays the actual output alongside the output the from top matching object queries. Extracted from the self-attention mechanism in the decoder, the attention map highlights the strength of interactions among the queries. Lighter colors indicate stronger interactions. The track queries tend to focus on themselves, while the object queries attend to the track query that corresponds most closely in location.</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013071.s006.tif"/></supplementary-material><supplementary-material id="pcbi.1013071.s007" position="float" content-type="local-data"><label>S3 Fig</label><caption><title>Depiction of how predictions are made from the output embeddings. The largest encoded multi-scale feature map is resized and combined with the original multi-scale features to form the pixel embedding map. The dot product of the output embeddings and the pixel embedding map produces the segmentation masks. A linear layer is used to predict the class label and a multi-layer perceptron (MLP) is used to generate the bounding box. For output embeddings classified as &#x0201c;no object&#x0201d;, the bounding boxes and segmentation masks are ignored.</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013071.s007.tif"/></supplementary-material><supplementary-material id="pcbi.1013071.s008" position="float" content-type="local-data"><label>S4 Fig</label><caption><title>Example showing how OP<sub>CTB</sub> can reward a tracker for predicting no division rather than predicting a division one frame too early or late. Acyclic oriented graph matching (AGOM) is the weighted sum of the operations needed to transform the tracker graph into the reference graph. A lower AGOM score is better and a higher TRA score is better.</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013071.s008.tif"/></supplementary-material><supplementary-material id="pcbi.1013071.s009" position="float" content-type="local-data"><label>S5 Fig</label><caption><title>Cells are initially detected in frame t<sub>0</sub> and then tracked to frame t<sub>1</sub>. Object queries are processed in the first layer of the decoder for object detection. In the subsequent decoder layers, track queries are concatenated with the object queries to perform tracking.</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013071.s009.tif"/></supplementary-material><supplementary-material id="pcbi.1013071.s010" position="float" content-type="local-data"><label>S6 Fig</label><caption><title>Schematic demonstrating how Cell-TRACTR tracks cells post-division. The dark blue cell at time t<sub>1</sub> is predicted to divide at time t<sub>2</sub>. The content embedding is duplicated for both divided cells. The positional embeddings are generated from each divided cell&#x02019;s segmentation mask. The content embeddings are shown as squares and the positional embeddings are circles.</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013071.s010.tif"/></supplementary-material><supplementary-material id="pcbi.1013071.s011" position="float" content-type="local-data"><label>S7 Fig</label><caption><title>Schematic demonstrating how Cell-HOTA handles early and late divisions. For early divisions, the ground truth cell is originally matched with one of the tracker cells at time t<sub>1</sub>. The red line indicates the matched cells, and the dotted blue box shows how the IOU is computed. To account for an early division, the IOU is computed between the ground truth cell in t<sub>1</sub> and both tracker cells in t<sub>1</sub>. In addition, the temporal association for the ground truth cell in t<sub>1</sub> is altered to match the tracker cell in t<sub>0</sub>. Similarly, for late divisions, the tracker cell is originally matched to one of the ground truth cells at time t<sub>1</sub>. To account for a late division, the IOU is computed between the tracker cell in t<sub>1</sub> and both ground truth cells in t<sub>1</sub>. The temporal associations are added between the ground truth cells in t<sub>1</sub> and the tracker cells in t<sub>2</sub>. Note that for clarity this schematic only shows changes that are affected by the early or late division.</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013071.s011.tif"/></supplementary-material><supplementary-material id="pcbi.1013071.s012" position="float" content-type="local-data"><label>S8 Fig</label><caption><title>(A) Example illustrating the correlation between the final prediction of an object query and where the object query is derived from within the multi-scale features. Colored pixels represent multi-scale features that were classified as &#x0201c;cell&#x0201d; by the decoder. Black pixels represent multi-scale feature pixels that were classified as &#x0201c;no object&#x0201d; by the decoder. The colored pixels are overlaid onto the phase contrast image for visualization purposes. (B) A more complex example illustrating the correlation between the multi-scale features and object queries.</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013071.s012.tif"/></supplementary-material><supplementary-material id="pcbi.1013071.s013" position="float" content-type="local-data"><label>S9 Fig</label><caption><title>Heatmap illustrating which multi-scale feature pixels were classified as &#x0201c;cell&#x0201d; by query selection. This data was collected by processing the full test set containing 29 movies from the bacterial mother machine dataset with Cell-TRACTR. Note that for these movies, the model preferentially uses pixels from the lowest resolution multi-scale features. The black pixel at the top corresponds to the top of the mother machine image where there are not usually cells.</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013071.s013.tif"/></supplementary-material><supplementary-material id="pcbi.1013071.s014" position="float" content-type="local-data"><label>S10 Fig</label><caption><title>Heatmap illustrating which multi-scale feature pixels were classified as &#x0201c;cell&#x0201d; by query selection. This data was collected by processing the full test set for the DeepCell dataset. Note that for these movies, Cell-TRACTR preferentially uses pixels from the highest resolution multi-scale features.</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013071.s014.tif"/></supplementary-material><supplementary-material id="pcbi.1013071.s015" position="float" content-type="local-data"><label>S11 Fig</label><caption><title>Diagram illustrating how the iterative bounding box refinement method works with cell divisions. When tracking the cell from t<sub>0</sub> to t<sub>1</sub>, the bounding box from t<sub>0</sub> serves as the initial bounding box that is iteratively refined three times into a bounding box that locates the cell in t<sub>1</sub>. The second bounding box is ignored here since cell division is not predicted at time t<sub>1</sub>. This method consists of predicting a relative offset with respect to the previous bounding box, represented by a change in cell location and size (&#x00394;y, &#x00394;x, &#x00394;h, &#x00394;w). This offset is added to the previous bounding box to get the new prediction. After the first layer in the decoder, the predicted relative offset is added to the initial bounding box in t<sub>1</sub>. This new bounding box serves as the reference point for the next layer. The reference point indicates where the query should focus within the encoded multi-scale features. In the two subsequent layers, the predicted relative offset is added to the previous bounding box to get the final bounding box location. At time t<sub>2</sub>, the model predicts a cell division resulting in both relative offsets being utilized. After the first layer in the decoder, the predicted relative offsets are added to the same initial bounding box in t<sub>2</sub>. In the two subsequent layers, the two predicted relative offsets are added to their respective previous bounding box. We generate a new bounding box around both divided cells that serves as the new reference point. This only occurs when both class labels for a track query are greater than 0.5.</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013071.s015.tif"/></supplementary-material><supplementary-material id="pcbi.1013071.s016" position="float" content-type="local-data"><label>S12 Fig</label><caption><title>Cell-TRACTR detects cells in the first frame at time t 0. Object queries predicted to be a &#x0201c;cell&#x0201d; are converted to track queries. For Track Group Denoising (TGD), random noise is added to the track queries to get noised track queries. For Query Denoising (QD), random noise is added to the ground truth bounding boxes to generate noised track queries. All queries are processed together by the decoder. Attention masks are used to prevent information leakage between the object and track queries, the noised track queries generated from TGD, and the noised track queries generated from QD. The gray boxes indicate information being blocked between sets of queries while colored boxes indicate the flow of information.</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013071.s016.tif"/></supplementary-material><supplementary-material id="pcbi.1013071.s017" position="float" content-type="local-data"><label>S13 Fig</label><caption><title>Effect of segmentation mask resolution on CUDA memory usage and performance metrics. (A) Memory usage as a function of queries when Cell-TRACTR produces a segmentation mask with the same resolution as the original image. (B) 1/4 Cell-TRACTR produces a segmentation mask with one-fourth the resolution of the input image.</title><p>(TIF) </p></caption><media xlink:href="pcbi.1013071.s017.tif"/></supplementary-material></sec></body><back><ack><p>We thank Jean-Baptiste Lugagne and all members of the Dunlop Lab for helpful discussions.</p></ack><ref-list><title>References</title><ref id="pcbi.1013071.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Brandis</surname><given-names>G</given-names></name>, <name><surname>Larsson</surname><given-names>J</given-names></name>, <name><surname>Elf</surname><given-names>J</given-names></name>. <article-title>Antibiotic perseverance increases the risk of resistance development</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2023</year>;<volume>120</volume>(<issue>2</issue>):e2216216120. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.2216216120</pub-id>
<pub-id pub-id-type="pmid">36595701</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Patel</surname><given-names>AP</given-names></name>, <name><surname>Tirosh</surname><given-names>I</given-names></name>, <name><surname>Trombetta</surname><given-names>JJ</given-names></name>, <name><surname>Shalek</surname><given-names>AK</given-names></name>, <name><surname>Gillespie</surname><given-names>SM</given-names></name>, <name><surname>Wakimoto</surname><given-names>H</given-names></name>. <article-title>Single-cell RNA-seq highlights intratumoral heterogeneity in primary glioblastoma</article-title>. <source>Science</source>. <year>2014</year>;<volume>344</volume>(<issue>6190</issue>):<fpage>1396</fpage>&#x02013;<lpage>401</lpage>.<pub-id pub-id-type="pmid">24925914</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Choudhary</surname><given-names>D</given-names></name>, <name><surname>Lagage</surname><given-names>V</given-names></name>, <name><surname>Foster</surname><given-names>KR</given-names></name>, <name><surname>Uphoff</surname><given-names>S</given-names></name>. <article-title>Phenotypic heterogeneity in the bacterial oxidative stress response is driven by cell-cell interactions</article-title>. <source>Cell Rep</source>. <year>2023</year>;<volume>42</volume>(<issue>3</issue>):<fpage>112168</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.celrep.2023.112168</pub-id>
<pub-id pub-id-type="pmid">36848288</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Sampaio</surname><given-names>N</given-names></name>, <name><surname>Blassick</surname><given-names>C</given-names></name>, <name><surname>Andreani</surname><given-names>V</given-names></name>, <name><surname>Lugagne</surname><given-names>J</given-names></name>, <name><surname>Dunlop</surname><given-names>M</given-names></name>. <article-title>Dynamic gene expression and growth underlie cell-to-cell heterogeneity in Escherichia coli stress response</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2022</year>;<volume>119</volume>(<issue>14</issue>):e2115032119.</mixed-citation></ref><ref id="pcbi.1013071.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Jiang</surname><given-names>Y</given-names></name>, <name><surname>Paxman</surname><given-names>J</given-names></name>, <name><surname>O&#x02019;Laughlin</surname><given-names>R</given-names></name>, <name><surname>Klepin</surname><given-names>S</given-names></name>, <name><surname>Zhu</surname><given-names>Y</given-names></name>. <article-title>A programmable fate decision landscape underlies single-cell aging in yeast</article-title>. <source>Sci</source>. <year>2020</year>;<volume>369</volume>(<issue>6501</issue>):<fpage>325</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="pcbi.1013071.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>Z</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Feng</surname><given-names>Y</given-names></name>, <name><surname>Klepin</surname><given-names>S</given-names></name>, <name><surname>Tsimring</surname><given-names>LS</given-names></name>, <name><surname>Pillus</surname><given-names>L</given-names></name>, <etal>et al</etal>. <article-title>Engineering longevity-design of a synthetic gene oscillator to slow cellular aging</article-title>. <source>Science</source>. <year>2023</year>;<volume>380</volume>(<issue>6643</issue>):<fpage>376</fpage>&#x02013;<lpage>81</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.add7631</pub-id>
<pub-id pub-id-type="pmid">37104589</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Elowitz</surname><given-names>MB</given-names></name>, <name><surname>Levine</surname><given-names>AJ</given-names></name>, <name><surname>Siggia</surname><given-names>ED</given-names></name>, <name><surname>Swain</surname><given-names>PS</given-names></name>. <article-title>Stochastic gene expression in a single cell</article-title>. <source>Science</source>. <year>2002</year>;<volume>297</volume>(<issue>5584</issue>):<fpage>1183</fpage>&#x02013;<lpage>6</lpage>.<pub-id pub-id-type="pmid">12183631</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Cutler</surname><given-names>KJ</given-names></name>, <name><surname>Stringer</surname><given-names>C</given-names></name>, <name><surname>Lo</surname><given-names>TW</given-names></name>, <name><surname>Rappez</surname><given-names>L</given-names></name>, <name><surname>Stroustrup</surname><given-names>N</given-names></name>, <name><surname>Brook Peterson</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Omnipose: a high-precision morphology-independent solution for bacterial cell segmentation</article-title>. <source>Nat Methods</source>. <year>2022</year>;<volume>19</volume>(<issue>11</issue>):<fpage>1438</fpage>&#x02013;<lpage>48</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41592-022-01639-4</pub-id>
<pub-id pub-id-type="pmid">36253643</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>G</given-names></name>, <name><surname>Kim</surname><given-names>S</given-names></name>, <name><surname>Kim</surname><given-names>J</given-names></name>, <name><surname>Yun</surname><given-names>SY</given-names></name>. MEDIAR: Harmony of Data-Centric and Model-Centric for Multi-Modality Microscopy [Internet]. arXiv; <year>2022</year> [cited 2023 Oct 20]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2212.03465" ext-link-type="uri">http://arxiv.org/abs/2212.03465</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>O&#x02019;Connor</surname><given-names>OM</given-names></name>, <name><surname>Alnahhas</surname><given-names>RN</given-names></name>, <name><surname>Lugagne</surname><given-names>J-B</given-names></name>, <name><surname>Dunlop</surname><given-names>MJ</given-names></name>. <article-title>DeLTA 2.0: A deep learning pipeline for quantifying single-cell spatial and temporal dynamics</article-title>. <source>PLoS Comput Biol</source>. <year>2022</year>;<volume>18</volume>(<issue>1</issue>):e1009797. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009797</pub-id>
<pub-id pub-id-type="pmid">35041653</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Israel</surname><given-names>U</given-names></name>, <name><surname>Marks</surname><given-names>M</given-names></name>, <name><surname>Dilip</surname><given-names>R</given-names></name>, <name><surname>Li</surname><given-names>Q</given-names></name>, <name><surname>Schwartz</surname><given-names>M</given-names></name>, <name><surname>Pradhan</surname><given-names>E</given-names></name>, <etal>et al</etal>. A Foundation Model for Cell Segmentation [Internet]. bioRxiv; <year>2023</year> [cited 2023 Dec 11]. p. 2023.11.17.567630. Available from: <ext-link xlink:href="https://www.biorxiv.org/content/10.1101/2023.11.17.567630v2" ext-link-type="uri">https://www.biorxiv.org/content/10.1101/2023.11.17.567630v2</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>van Gestel</surname><given-names>J</given-names></name>, <name><surname>Bareia</surname><given-names>T</given-names></name>, <name><surname>Tenennbaum</surname><given-names>B</given-names></name>, <name><surname>Dal Co</surname><given-names>A</given-names></name>, <name><surname>Guler</surname><given-names>P</given-names></name>, <name><surname>Aframian</surname><given-names>N</given-names></name>, <etal>et al</etal>. <article-title>Short-range quorum sensing controls horizontal gene transfer at micron scale in bacterial communities</article-title>. <source>Nat Commun</source>. <year>2021</year>;<volume>12</volume>(<issue>1</issue>):<fpage>2324</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41467-021-22649-4</pub-id>
<pub-id pub-id-type="pmid">33875666</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>van Vliet</surname><given-names>S</given-names></name>, <name><surname>Dal Co</surname><given-names>A</given-names></name>, <name><surname>Winkler</surname><given-names>A</given-names></name>, <name><surname>Spriewald</surname><given-names>S</given-names></name>, <name><surname>Stecher</surname><given-names>B</given-names></name>, <name><surname>Ackermann</surname><given-names>M</given-names></name>. <article-title>Spatially correlated gene expression in bacterial groups: the role of lineage history, spatial gradients, and cell-cell interactions</article-title>. <source>Cell Syst</source>. <year>2018</year>;<volume>6</volume>(<issue>4</issue>):496-507.e6.</mixed-citation></ref><ref id="pcbi.1013071.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Magnusson</surname><given-names>KEG</given-names></name>, <name><surname>Jalden</surname><given-names>J</given-names></name>, <name><surname>Gilbert</surname><given-names>PM</given-names></name>, <name><surname>Blau</surname><given-names>HM</given-names></name>. <article-title>Global linking of cell tracks using the Viterbi algorithm</article-title>. <source>IEEE Trans Med Imaging</source>. <year>2015</year>;<volume>34</volume>(<issue>4</issue>):<fpage>911</fpage>&#x02013;<lpage>29</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TMI.2014.2370951</pub-id>
<pub-id pub-id-type="pmid">25415983</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Bise</surname><given-names>R</given-names></name>, <name><surname>Yin</surname><given-names>Z</given-names></name>, <name><surname>Kanade</surname><given-names>T</given-names></name>. <article-title>Reliable cell tracking by global data association.</article-title> In: 2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro [Internet]. <year>2011</year> [cited 2024 Apr 4]. p. <fpage>1004</fpage>&#x02013;<lpage>10</lpage>. Available from: <ext-link xlink:href="https://ieeexplore.ieee.org/document/5872571" ext-link-type="uri">https://ieeexplore.ieee.org/document/5872571</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Dzyubachyk</surname><given-names>O</given-names></name>, <name><surname>van Cappellen</surname><given-names>WA</given-names></name>, <name><surname>Essers</surname><given-names>J</given-names></name>, <name><surname>Niessen</surname><given-names>WJ</given-names></name>, <name><surname>Meijering</surname><given-names>E</given-names></name>. <article-title>Advanced level-set-based cell tracking in time-lapse fluorescence microscopy</article-title>. <source>IEEE Trans Med Imaging</source>. <year>2010</year>;<volume>29</volume>(<issue>3</issue>):<fpage>852</fpage>&#x02013;<lpage>67</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TMI.2009.2038693</pub-id>
<pub-id pub-id-type="pmid">20199920</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>X</given-names></name>, <name><surname>Li</surname><given-names>H</given-names></name>, <name><surname>Zhou</surname><given-names>X</given-names></name>. <article-title>Nuclei Segmentation Using Marker-Controlled Watershed, Tracking Using Mean-Shift, and Kalman Filter in Time-Lapse Microscopy</article-title>. <source>IEEE Trans Circuits Syst I</source>. <year>2006</year>;<volume>53</volume>(<issue>11</issue>):<fpage>2405</fpage>&#x02013;<lpage>14</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tcsi.2006.884469</pub-id></mixed-citation></ref><ref id="pcbi.1013071.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>He</surname><given-names>T</given-names></name>, <name><surname>Mao</surname><given-names>H</given-names></name>, <name><surname>Guo</surname><given-names>J</given-names></name>, <name><surname>Yi</surname><given-names>Z</given-names></name>. <article-title>Cell tracking using deep neural networks with multi-task learning</article-title>. <source>Image Vis Comput</source>. <year>2017</year>;<volume>60</volume>:<fpage>142</fpage>&#x02013;<lpage>53</lpage>.</mixed-citation></ref><ref id="pcbi.1013071.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Meacock</surname><given-names>OJ</given-names></name>, <name><surname>Durham</surname><given-names>WM</given-names></name>. <article-title>Tracking bacteria at high density with FAST, the Feature-Assisted Segmenter/Tracker</article-title>. <source>PLoS Comput Biol</source>. <year>2023</year>;<volume>19</volume>(<issue>10</issue>):e1011524. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011524</pub-id>
<pub-id pub-id-type="pmid">37812642</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Lugagne</surname><given-names>J-B</given-names></name>, <name><surname>Lin</surname><given-names>H</given-names></name>, <name><surname>Dunlop</surname><given-names>MJ</given-names></name>. <article-title>DeLTA: Automated cell segmentation, tracking, and lineage reconstruction using deep learning</article-title>. <source>PLoS Comput Biol</source>. <year>2020</year>;<volume>16</volume>(<issue>4</issue>):e1007673. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007673</pub-id>
<pub-id pub-id-type="pmid">32282792</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref021"><label>21</label><mixed-citation publication-type="confproc"><name><surname>Hayashida</surname><given-names>J</given-names></name>, <name><surname>Bise</surname><given-names>R.</given-names></name>
<article-title>Cell tracking with deep learning for cell detection and motion estimation in low-frame-rate.</article-title> In: <name><surname>Shen</surname><given-names>D</given-names></name>, <name><surname>Yap</surname><given-names>P</given-names></name>, <name><surname>Liu</surname><given-names>T</given-names></name>, <name><surname>Peters</surname><given-names>T</given-names></name>, <name><surname>Khan</surname><given-names>A</given-names></name>, <name><surname>Staib</surname><given-names>L</given-names></name>, editors. <conf-name>Med Image Comput Comput Assist Interv &#x02013; MICCAI 2019 - 22nd Int Conf Proc</conf-name>. <publisher-name>Springer</publisher-name>; <year>2019</year>. p. <fpage>397</fpage>&#x02013;<lpage>405</lpage>.</mixed-citation></ref><ref id="pcbi.1013071.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Schwartz</surname><given-names>MS</given-names></name>, <name><surname>Moen</surname><given-names>E</given-names></name>, <name><surname>Miller</surname><given-names>G</given-names></name>, <name><surname>Dougherty</surname><given-names>T</given-names></name>, <name><surname>Borba</surname><given-names>E</given-names></name>, <name><surname>Ding</surname><given-names>R</given-names></name>, <etal>et al</etal>. Caliban: Accurate cell tracking and lineage construction in live-cell imaging experiments with deep learning [Internet]. bioRxiv; <year>2023</year> [cited 2024 Jan 22]. p. <fpage>803205</fpage>. Available from: <ext-link xlink:href="https://www.biorxiv.org/content/10.1101/803205v4" ext-link-type="uri">https://www.biorxiv.org/content/10.1101/803205v4</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Ollion</surname><given-names>J</given-names></name>, <name><surname>Maliet</surname><given-names>M</given-names></name>, <name><surname>Giuglaris</surname><given-names>C</given-names></name>, <name><surname>Vacher</surname><given-names>E</given-names></name>, <name><surname>Deforet</surname><given-names>M</given-names></name>. DistNet2D: Leveraging long-range temporal information for efficient segmentation and tracking [Internet]. arXiv; <year>2023</year> [cited 2023 Nov 5]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2310.19641" ext-link-type="uri">http://arxiv.org/abs/2310.19641</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Loffler</surname><given-names>K</given-names></name>, <name><surname>Mikut</surname><given-names>R</given-names></name>. <article-title>EmbedTrack&#x02014;simultaneous cell segmentation and tracking through learning offsets and clustering bandwidths</article-title>. <source>IEEE Access</source>. <year>2022</year>;<volume>10</volume>:<fpage>77147</fpage>&#x02013;<lpage>57</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/access.2022.3192880</pub-id></mixed-citation></ref><ref id="pcbi.1013071.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Payer</surname><given-names>C</given-names></name>, <name><surname>&#x00160;tern</surname><given-names>D</given-names></name>, <name><surname>Feiner</surname><given-names>M</given-names></name>, <name><surname>Bischof</surname><given-names>H</given-names></name>, <name><surname>Urschler</surname><given-names>M</given-names></name>. <article-title>Segmenting and tracking cell instances with cosine embeddings and recurrent hourglass networks</article-title>. <source>Med Image Anal</source>. <year>2019</year>;<volume>57</volume>:<fpage>106</fpage>&#x02013;<lpage>19</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.media.2019.06.015</pub-id>
<pub-id pub-id-type="pmid">31299493</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref026"><label>26</label><mixed-citation publication-type="book"><name><surname>Hayashida</surname><given-names>J</given-names></name>, <name><surname>Nishimura</surname><given-names>K</given-names></name>, <name><surname>Bise</surname><given-names>R</given-names></name>. <article-title>MPM: Joint Representation of Motion and Position Map for Cell Tracking.</article-title> In: <source>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) [Internet]</source>. <publisher-loc>Seattle, WA, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2020</year> [cited 2024 Jan 22]. p. <fpage>3822</fpage>&#x02013;<lpage>31</lpage>. Available from: <ext-link xlink:href="https://ieeexplore.ieee.org/document/9156603/" ext-link-type="uri">https://ieeexplore.ieee.org/document/9156603/</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref027"><label>27</label><mixed-citation publication-type="book"><name><surname>Chen</surname><given-names>Y</given-names></name>, <name><surname>Song</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>C</given-names></name>, <name><surname>Zhang</surname><given-names>F</given-names></name>, <name><surname>O&#x02019;Donnell</surname><given-names>L</given-names></name>, <name><surname>Chrzanowski</surname><given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Celltrack R-CNN: A novel end-to-end deep neural network for cell segmentation and tracking in microscopy images.</article-title> In:<source> 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI) [Interne</source>t]. <publisher-loc>Nice, France</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2021</year> [cited 2023 Feb 19]. p. <fpage>779</fpage>&#x02013;<lpage>82</lpage>. Available from: <ext-link xlink:href="https://ieeexplore.ieee.org/document/9434057/" ext-link-type="uri">https://ieeexplore.ieee.org/document/9434057/</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref028"><label>28</label><mixed-citation publication-type="book"><name><surname>Hayashida</surname><given-names>J</given-names></name>, <name><surname>Nishimura</surname><given-names>K</given-names></name>, <name><surname>Bise</surname><given-names>R</given-names></name>. <article-title>Consistent Cell Tracking in Multi-frames with Spatio-Temporal Context by Object-Level Warping Loss.</article-title> In: <source>2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) [Internet]</source>. <publisher-loc>Waikoloa, HI, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2022</year> [cited 2024 Jan 22]. p. <fpage>1759</fpage>&#x02013;<lpage>68</lpage>. Available from: <ext-link xlink:href="https://ieeexplore.ieee.org/document/9707080/" ext-link-type="uri">https://ieeexplore.ieee.org/document/9707080/</ext-link>.</mixed-citation></ref><ref id="pcbi.1013071.ref029"><label>29</label><mixed-citation publication-type="book"><name><surname>Ollion</surname><given-names>J</given-names></name>, <name><surname>Ollion</surname><given-names>C.</given-names></name>
<article-title>DistNet: deep tracking by displacement regression: application to bacteria growing in the mother machine.</article-title> In: <name><surname>Martel</surname><given-names>AL</given-names></name>, <name><surname>Abolmaesumi</surname><given-names>P</given-names></name>, <name><surname>Stoyanov</surname><given-names>D</given-names></name>, <name><surname>Mateus</surname><given-names>D</given-names></name>, <name><surname>Zuluaga</surname><given-names>MA</given-names></name>, <name><surname>Zhou</surname><given-names>SK</given-names></name>, <etal>et al</etal>., editors. <source>Medical Image Computing and Computer Assisted Intervention &#x02013; MICCAI 2020 [Internet]</source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; <year>2020</year> [cited 2022 Apr 13]. p. <fpage>215</fpage>&#x02013;<lpage>25</lpage>. (Lecture Notes in Computer Science; vol. 12265). Available from: <ext-link xlink:href="https://link.springer.com/10.1007/978-3-030-59722-1_21" ext-link-type="uri">https://link.springer.com/10.1007/978-3-030-59722-1_21</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Ronneberger</surname><given-names>O</given-names></name>, <name><surname>Fischer</surname><given-names>P</given-names></name>, <name><surname>Brox</surname><given-names>T</given-names></name>. U-Net: Convolutional Networks for Biomedical Image Segmentation. ArXiv150504597 Cs [Internet]. <year>2015</year> May 18 [cited 2022 Apr 13]; Available from: <ext-link xlink:href="http://arxiv.org/abs/1505.04597" ext-link-type="uri">http://arxiv.org/abs/1505.04597</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Stringer</surname><given-names>C</given-names></name>, <name><surname>Wang</surname><given-names>T</given-names></name>, <name><surname>Michaelos</surname><given-names>M</given-names></name>, <name><surname>Pachitariu</surname><given-names>M</given-names></name>. <article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title>. <source>Nat Methods</source>. <year>2021</year>;<volume>18</volume>(<issue>1</issue>):<fpage>100</fpage>&#x02013;<lpage>6</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id>
<pub-id pub-id-type="pmid">33318659</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Panigrahi</surname><given-names>S</given-names></name>, <name><surname>Murat</surname><given-names>D</given-names></name>, <name><surname>Le Gall</surname><given-names>A</given-names></name>, <name><surname>Martineau</surname><given-names>E</given-names></name>, <name><surname>Goldlust</surname><given-names>K</given-names></name>, <name><surname>Fiche</surname><given-names>J</given-names></name>. <article-title>Misic, a general deep learning-based method for the high-throughput cell segmentation of complex bacterial communities</article-title>. <source>eLife</source>. <year>2021</year>;<volume>10</volume>:e65151.</mixed-citation></ref><ref id="pcbi.1013071.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Ben-Haim</surname><given-names>T</given-names></name>, <name><surname>Raviv</surname><given-names>TR</given-names></name>. Graph Neural Network for Cell Tracking in Microscopy Videos [Internet]. arXiv; <year>2022</year> [cited 2024 Feb 29]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2202.04731" ext-link-type="uri">http://arxiv.org/abs/2202.04731</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref034"><label>34</label><mixed-citation publication-type="book"><name><surname>Zhao</surname><given-names>K</given-names></name>, <name><surname>Niyogisubizo</surname><given-names>J</given-names></name>, <name><surname>Xiao</surname><given-names>L</given-names></name>, <name><surname>Pan</surname><given-names>Y</given-names></name>, <name><surname>Wei</surname><given-names>D</given-names></name>, <name><surname>Rosiyadi</surname><given-names>D</given-names></name>, <etal>et al</etal>. <article-title>A novel deep learning approach featuring graph-based algorithm for cell segmentation and tracking.</article-title> In: <source>2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) [Internet]. Istanbul, Turkiye</source>: <publisher-name>IEEE</publisher-name>; <year>2023</year> [cited 2024 Jan 23]. p. <fpage>1752</fpage>&#x02013;<lpage>7</lpage>. Available from: <ext-link xlink:href="https://ieeexplore.ieee.org/document/10385935/" ext-link-type="uri">https://ieeexplore.ieee.org/document/10385935/</ext-link>.</mixed-citation></ref><ref id="pcbi.1013071.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Su</surname><given-names>X</given-names></name>, <name><surname>Zhao</surname><given-names>L</given-names></name>, <name><surname>Zhang</surname><given-names>J</given-names></name>. <article-title>Deep reinforcement learning for data association in cell tracking</article-title>. <source>Front Bioeng Biotechnol</source>. <year>2020</year>;<volume>8</volume>:<fpage>298</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fbioe.2020.00298</pub-id>
<pub-id pub-id-type="pmid">32328484</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref036"><label>36</label><mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is All you Need.:11.</mixed-citation></ref><ref id="pcbi.1013071.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Dosovitskiy</surname><given-names>A</given-names></name>, <name><surname>Beyer</surname><given-names>L</given-names></name>, <name><surname>Kolesnikov</surname><given-names>A</given-names></name>, <name><surname>Weissenborn</surname><given-names>D</given-names></name>, <name><surname>Zhai</surname><given-names>X</given-names></name>, <name><surname>Unterthiner</surname><given-names>T</given-names></name>, <etal>et al</etal>. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ArXiv201011929 Cs [Internet]. <year>2021</year> Jun 3 [cited 2022 Apr 13]; Available from: <ext-link xlink:href="http://arxiv.org/abs/2010.11929" ext-link-type="uri">http://arxiv.org/abs/2010.11929</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref038"><label>38</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Z</given-names></name>, <name><surname>Lin</surname><given-names>Y</given-names></name>, <name><surname>Cao</surname><given-names>Y</given-names></name>, <name><surname>Hu</surname><given-names>H</given-names></name>, <name><surname>Wei</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name>, <etal>et al</etal>. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. ArXiv210314030 Cs [Internet]. <year>2021</year> Aug 17 [cited 2022 Apr 13]; Available from: <ext-link xlink:href="http://arxiv.org/abs/2103.14030" ext-link-type="uri">http://arxiv.org/abs/2103.14030</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref039"><label>39</label><mixed-citation publication-type="confproc"><name><surname>Prangemeier</surname><given-names>T</given-names></name>, <name><surname>Reich</surname><given-names>C</given-names></name>, <name><surname>Koeppl</surname><given-names>H</given-names></name>. <conf-name>Attention-based transformers for instance segmentation of cells in microstructures</conf-name>. In: <conf-name>2020 IEEE Int Conf Bioinforma Biomed BIBM</conf-name>. <publisher-name>IEEE</publisher-name>. <year>2020</year>. p. <fpage>700</fpage>&#x02013;<lpage>7</lpage>.</mixed-citation></ref><ref id="pcbi.1013071.ref040"><label>40</label><mixed-citation publication-type="journal"><name><surname>Pang</surname><given-names>M</given-names></name>, <name><surname>Roy</surname><given-names>T</given-names></name>, <name><surname>Wu</surname><given-names>X</given-names></name>, <name><surname>Tan</surname><given-names>K</given-names></name>. <article-title>Cellotype: a unified model for segmentation and classification of tissue images</article-title>. <source>Nat Methods</source>. <year>2024</year>. <ext-link xlink:href="https://www.nature.com/articles/s41592-024-02513-1" ext-link-type="uri">https://www.nature.com/articles/s41592-024-02513-1</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref041"><label>41</label><mixed-citation publication-type="journal"><name><surname>Gallusser</surname><given-names>B</given-names></name>, <name><surname>Weigert</surname><given-names>M</given-names></name>. Trackastra: Transformer-based cell tracking for live-cell microscopy [Internet]. arXiv; <year>2024</year> [cited 2024 May 30]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2405.15700" ext-link-type="uri">http://arxiv.org/abs/2405.15700</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref042"><label>42</label><mixed-citation publication-type="journal"><name><surname>Yu</surname><given-names>E</given-names></name>, <name><surname>Wang</surname><given-names>T</given-names></name>, <name><surname>Li</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Tao</surname><given-names>W</given-names></name>. MOTRv3: Release-Fetch Supervision for End-to-End Multi-Object Tracking [Internet]. arXiv; <year>2023</year> [cited 2023 Jul 17]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2305.14298" ext-link-type="uri">http://arxiv.org/abs/2305.14298</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref043"><label>43</label><mixed-citation publication-type="journal"><name><surname>Sun</surname><given-names>P</given-names></name>, <name><surname>Cao</surname><given-names>J</given-names></name>, <name><surname>Jiang</surname><given-names>Y</given-names></name>, <name><surname>Yuan</surname><given-names>Z</given-names></name>, <name><surname>Bai</surname><given-names>S</given-names></name>, <name><surname>Kitani</surname><given-names>K</given-names></name>, <etal>et al</etal>. DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion [Internet]. arXiv; <year>2022</year> [cited 2023 Jan 16]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2111.14690" ext-link-type="uri">http://arxiv.org/abs/2111.14690</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref044"><label>44</label><mixed-citation publication-type="journal"><name><surname>Meinhardt</surname><given-names>T</given-names></name>, <name><surname>Kirillov</surname><given-names>A</given-names></name>, <name><surname>Leal-Taixe</surname><given-names>L</given-names></name>, <name><surname>Feichtenhofer</surname><given-names>C</given-names></name>. TrackFormer: Multi-Object Tracking with Transformers [Internet]. arXiv; <year>2022</year> [cited 2022 Nov 14]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2101.02702" ext-link-type="uri">http://arxiv.org/abs/2101.02702</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref045"><label>45</label><mixed-citation publication-type="other">Zeng F, Dong B, Zhang Y, Wang T, Zhang X, Wei Y. MOTR: End-to-End Multiple-Object Tracking with Transformer. 2021 May 7;17.</mixed-citation></ref><ref id="pcbi.1013071.ref046"><label>46</label><mixed-citation publication-type="journal"><name><surname>Carion</surname><given-names>N</given-names></name>, <name><surname>Massa</surname><given-names>F</given-names></name>, <name><surname>Synnaeve</surname><given-names>G</given-names></name>, <name><surname>Usunier</surname><given-names>N</given-names></name>, <name><surname>Kirillov</surname><given-names>A</given-names></name>, <name><surname>Zagoruyko</surname><given-names>S</given-names></name>. End-to-End Object Detection with Transformers [Internet]. arXiv; <year>2020</year> [cited 2022 Nov 14]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2005.12872" ext-link-type="uri">http://arxiv.org/abs/2005.12872</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref047"><label>47</label><mixed-citation publication-type="journal"><name><surname>Zhu</surname><given-names>X</given-names></name>, <name><surname>Su</surname><given-names>W</given-names></name>, <name><surname>Lu</surname><given-names>L</given-names></name>, <name><surname>Li</surname><given-names>B</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Dai</surname><given-names>J</given-names></name>. Deformable DETR: Deformable Transformers for End-to-End Object Detection [Internet]. arXiv; <year>2021</year> [cited 2022 Nov 14]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2010.04159" ext-link-type="uri">http://arxiv.org/abs/2010.04159</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref048"><label>48</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>S</given-names></name>, <name><surname>Li</surname><given-names>F</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Yang</surname><given-names>X</given-names></name>, <name><surname>Qi</surname><given-names>X</given-names></name>, <name><surname>Su</surname><given-names>H</given-names></name>, <etal>et al</etal>. DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR [Internet]. arXiv; <year>2022</year> [cited 2022 Nov 14]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2201.12329" ext-link-type="uri">http://arxiv.org/abs/2201.12329</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref049"><label>49</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>F</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Liu</surname><given-names>S</given-names></name>, <name><surname>Guo</surname><given-names>J</given-names></name>, <name><surname>Ni</surname><given-names>LM</given-names></name>, <name><surname>Zhang</surname><given-names>L</given-names></name>. DN-DETR: Accelerate DETR Training by Introducing Query DeNoising [Internet]. arXiv; <year>2022</year> [cited 2022 Nov 14]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2203.01305" ext-link-type="uri">http://arxiv.org/abs/2203.01305</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref050"><label>50</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Li</surname><given-names>F</given-names></name>, <name><surname>Liu</surname><given-names>S</given-names></name>, <name><surname>Zhang</surname><given-names>L</given-names></name>, <name><surname>Su</surname><given-names>H</given-names></name>, <name><surname>Zhu</surname><given-names>J</given-names></name>, <etal>et al</etal>. DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection [Internet]. arXiv; <year>2022</year> [cited 2022 Nov 14]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2203.03605" ext-link-type="uri">http://arxiv.org/abs/2203.03605</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref051"><label>51</label><mixed-citation publication-type="journal"><name><surname>Zhu</surname><given-names>X</given-names></name>, <name><surname>Su</surname><given-names>W</given-names></name>, <name><surname>Lu</surname><given-names>L</given-names></name>, <name><surname>Li</surname><given-names>B</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Dai</surname><given-names>J</given-names></name>. Deformable DETR: Deformable Transformers for End-to-End Object Detection. ArXiv201004159 Cs [Internet]. <year>2021</year> Mar 17 [cited 2022 Apr 13]; Available from: <ext-link xlink:href="http://arxiv.org/abs/2010.04159" ext-link-type="uri">http://arxiv.org/abs/2010.04159</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref052"><label>52</label><mixed-citation publication-type="journal"><name><surname>Yao</surname><given-names>Z</given-names></name>, <name><surname>Ai</surname><given-names>J</given-names></name>, <name><surname>Li</surname><given-names>B</given-names></name>, <name><surname>Zhang</surname><given-names>C</given-names></name>. Efficient DETR: Improving End-to-End Object Detector with Dense Prior [Internet]. arXiv; <year>2021</year> [cited 2022 Nov 14]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2104.01318" ext-link-type="uri">http://arxiv.org/abs/2104.01318</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref053"><label>53</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>F</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, xu H, <name><surname>Liu</surname><given-names>S</given-names></name>, <name><surname>Zhang</surname><given-names>L</given-names></name>, <name><surname>Ni</surname><given-names>LM</given-names></name>, <etal>et al</etal>. Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation [Internet]. arXiv; <year>2022</year> [cited 2022 Nov 14]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2206.02777" ext-link-type="uri">http://arxiv.org/abs/2206.02777</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref054"><label>54</label><mixed-citation publication-type="journal"><name><surname>Zong</surname><given-names>Z</given-names></name>, <name><surname>Song</surname><given-names>G</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>. DETRs with Collaborative Hybrid Assignments Training [Internet]. arXiv; <year>2023</year> [cited 2023 Jul 10]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2211.12860" ext-link-type="uri">http://arxiv.org/abs/2211.12860</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref055"><label>55</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>T</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>. MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors [Internet]. arXiv; <year>2022</year> [cited 2022 Nov 27]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2211.09791" ext-link-type="uri">http://arxiv.org/abs/2211.09791</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref056"><label>56</label><mixed-citation publication-type="journal"><name><surname>Gao</surname><given-names>R</given-names></name>, <name><surname>Wang</surname><given-names>L</given-names></name>. MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking [Internet]. arXiv; <year>2023</year> [cited 2023 Aug 7]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2307.15700" ext-link-type="uri">http://arxiv.org/abs/2307.15700</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref057"><label>57</label><mixed-citation publication-type="journal"><name><surname>Yan</surname><given-names>F</given-names></name>, <name><surname>Luo</surname><given-names>W</given-names></name>, <name><surname>Zhong</surname><given-names>Y</given-names></name>, <name><surname>Gan</surname><given-names>Y</given-names></name>, <name><surname>Ma</surname><given-names>L</given-names></name>. Bridging the Gap Between End-to-end and Non-End-to-end Multi-Object Tracking [Internet]. arXiv; <year>2023</year> [cited 2023 Jul 10]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2305.12724" ext-link-type="uri">http://arxiv.org/abs/2305.12724</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref058"><label>58</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Wu</surname><given-names>J</given-names></name>, <name><surname>Fu</surname><given-names>Y</given-names></name>. Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking [Internet]. arXiv; <year>2023</year> [cited 2023 Aug 16]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2308.05911" ext-link-type="uri">http://arxiv.org/abs/2308.05911</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref059"><label>59</label><mixed-citation publication-type="journal"><name><surname>Gao</surname><given-names>R</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>L</given-names></name>. Multiple Object Tracking as ID Prediction [Internet]. arXiv; <year>2024</year> [cited 2024 Mar 31]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2403.16848" ext-link-type="uri">http://arxiv.org/abs/2403.16848</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref060"><label>60</label><mixed-citation publication-type="journal"><name><surname>Ma&#x00161;ka</surname><given-names>M</given-names></name>, <name><surname>Ulman</surname><given-names>V</given-names></name>, <name><surname>Delgado-Rodriguez</surname><given-names>P</given-names></name>, G&#x000f3;mez-de-<name><surname>Mariscal</surname><given-names>E</given-names></name>, <name><surname>Ne&#x0010d;asov&#x000e1;</surname><given-names>T</given-names></name>, <name><surname>Guerrero Pe&#x000f1;a</surname><given-names>FA</given-names></name>, <etal>et al</etal>. <article-title>The Cell Tracking Challenge: 10 years of objective benchmarking.</article-title><source> Nat Methods</source>. <year>2023</year> May 18;<fpage>1</fpage>&#x02013;<lpage>11</lpage>.<pub-id pub-id-type="pmid">36635552</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref061"><label>61</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>Y</given-names></name>, <name><surname>Huo</surname><given-names>Y</given-names></name>. Limitation of Acyclic Oriented Graphs Matching as Cell Tracking Accuracy Measure when Evaluating Mitosis. arXiv.org. <year>2020</year> [cited 2023 Jun 8]. Available from: <ext-link xlink:href="https://arxiv.org/abs/2012.12084v1" ext-link-type="uri">https://arxiv.org/abs/2012.12084v1</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref062"><label>62</label><mixed-citation publication-type="journal"><name><surname>Luiten</surname><given-names>J</given-names></name>, <name><surname>Os Ep</surname><given-names>AA</given-names></name>, <name><surname>Dendorfer</surname><given-names>P</given-names></name>, <name><surname>Torr</surname><given-names>P</given-names></name>, <name><surname>Geiger</surname><given-names>A</given-names></name>, <name><surname>Leal-Taix&#x000e9;</surname><given-names>L</given-names></name>, <etal>et al</etal>. <article-title>HOTA: A higher order metric for evaluating multi-object tracking</article-title>. <source>Int J Comput Vis</source>. <year>2021</year>;<volume>129</volume>(<issue>2</issue>):<fpage>548</fpage>&#x02013;<lpage>78</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11263-020-01375-2</pub-id>
<pub-id pub-id-type="pmid">33642696</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref063"><label>63</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>P</given-names></name>, <name><surname>Robert</surname><given-names>L</given-names></name>, <name><surname>Pelletier</surname><given-names>J</given-names></name>, <name><surname>Dang</surname><given-names>WL</given-names></name>, <name><surname>Taddei</surname><given-names>F</given-names></name>, <name><surname>Wright</surname><given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Robust growth of Escherichia coli</article-title>. <source>Curr Biol</source>. <year>2010</year>;<volume>20</volume>(<issue>12</issue>):<fpage>1099</fpage>&#x02013;<lpage>103</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cub.2010.04.045</pub-id>
<pub-id pub-id-type="pmid">20537537</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref064"><label>64</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Sun</surname><given-names>P</given-names></name>, <name><surname>Jiang</surname><given-names>Y</given-names></name>, <name><surname>Yu</surname><given-names>D</given-names></name>, <name><surname>Weng</surname><given-names>F</given-names></name>, <name><surname>Yuan</surname><given-names>Z</given-names></name>, <etal>et al</etal>. ByteTrack: Multi-Object Tracking by Associating Every Detection Box [Internet]. arXiv; <year>2022</year> [cited 2022 Nov 14]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2110.06864" ext-link-type="uri">http://arxiv.org/abs/2110.06864</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref065"><label>65</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>YH</given-names></name>, <name><surname>Hsieh</surname><given-names>JW</given-names></name>, <name><surname>Chen</surname><given-names>PY</given-names></name>, <name><surname>Chang</surname><given-names>MC</given-names></name>, <name><surname>So</surname><given-names>HH</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>. SMILEtrack: SiMIlarity LEarning for Occlusion-Aware Multiple Object Tracking [Internet]. arXiv; <year>2023</year> [cited 2023 Oct 20]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2211.08824" ext-link-type="uri">http://arxiv.org/abs/2211.08824</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref066"><label>66</label><mixed-citation publication-type="book"><name><surname>Bewley</surname><given-names>A</given-names></name>, <name><surname>Ge</surname><given-names>Z</given-names></name>, <name><surname>Ott</surname><given-names>L</given-names></name>, <name><surname>Ramos</surname><given-names>F</given-names></name>, <name><surname>Upcroft</surname><given-names>B</given-names></name>. <source>Simple Online and Realtime Tracking. In: 2016 IEEE International Conference on Image Processing (ICIP) [Interne</source>t]. <year>2016</year> [cited 2022 Nov 14]. p. <fpage>3464</fpage>&#x02013;<lpage>8</lpage>. Available from: <ext-link xlink:href="http://arxiv.org/abs/1602.00763" ext-link-type="uri">http://arxiv.org/abs/1602.00763</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref067"><label>67</label><mixed-citation publication-type="book"><name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Ren</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <article-title>Deep residual learning for image recognition.</article-title> In: <source>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) [Internet]</source>. <publisher-loc>Las Vegas, NV, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2016</year> [cited 2022 Apr 13]. p. <fpage>770</fpage>&#x02013;<lpage>8</lpage>. Available from: <ext-link xlink:href="http://ieeexplore.ieee.org/document/7780459/" ext-link-type="uri">http://ieeexplore.ieee.org/document/7780459/</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref068"><label>68</label><mixed-citation publication-type="journal"><name><surname>Bernardin</surname><given-names>K</given-names></name>, <name><surname>Stiefelhagen</surname><given-names>R</given-names></name>. <article-title>Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics</article-title>. <source>EURASIP J Image Video Process</source>. <year>2008</year>;<volume>2008</volume>:<fpage>1</fpage>&#x02013;<lpage>10</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1155/2008/246309</pub-id></mixed-citation></ref><ref id="pcbi.1013071.ref069"><label>69</label><mixed-citation publication-type="journal"><name><surname>Ristani</surname><given-names>E</given-names></name>, <name><surname>Solera</surname><given-names>F</given-names></name>, <name><surname>Zou</surname><given-names>RS</given-names></name>, <name><surname>Cucchiara</surname><given-names>R</given-names></name>, <name><surname>Tomasi</surname><given-names>C</given-names></name>. Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking [Internet]. arXiv; <year>2016</year> [cited 2023 Oct 2]. Available from: <ext-link xlink:href="http://arxiv.org/abs/1609.01775" ext-link-type="uri">http://arxiv.org/abs/1609.01775</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref070"><label>70</label><mixed-citation publication-type="journal"><name><surname>Dendorfer</surname><given-names>P</given-names></name>, <name><surname>Rezatofighi</surname><given-names>H</given-names></name>, <name><surname>Milan</surname><given-names>A</given-names></name>, <name><surname>Shi</surname><given-names>J</given-names></name>, <name><surname>Cremers</surname><given-names>D</given-names></name>, <name><surname>Reid</surname><given-names>I</given-names></name>, <etal>et al</etal>. CVPR19 Tracking and Detection Challenge: How crowded can it get? [Internet]. arXiv; <year>2019</year> [cited 2024 Feb 6]. Available from: <ext-link xlink:href="http://arxiv.org/abs/1906.04567" ext-link-type="uri">http://arxiv.org/abs/1906.04567</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref071"><label>71</label><mixed-citation publication-type="book"><name><surname>Liang</surname><given-names>Z</given-names></name>, <name><surname>Yuan</surname><given-names>Y</given-names></name>. <source>Mask Frozen-DETR: High Quali</source>ty Instance Segmentation with One GPU [Internet]. arXiv; <year>2023</year> [cited 2023 Aug 9]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2308.03747" ext-link-type="uri">http://arxiv.org/abs/2308.03747</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref072"><label>72</label><mixed-citation publication-type="journal"><name><surname>Lin</surname><given-names>TY</given-names></name>, <name><surname>Doll&#x000e1;r</surname><given-names>P</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>, <name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Hariharan</surname><given-names>B</given-names></name>, <name><surname>Belongie</surname><given-names>S</given-names></name>. Feature Pyramid Networks for Object Detection [Internet]. arXiv; <year>2017</year> [cited 2022 Nov 28]. Available from: <ext-link xlink:href="http://arxiv.org/abs/1612.03144" ext-link-type="uri">http://arxiv.org/abs/1612.03144</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref073"><label>73</label><mixed-citation publication-type="journal"><name><surname>Hochreiter</surname><given-names>S</given-names></name>, <name><surname>Schmidhuber</surname><given-names>J</given-names></name>. <article-title>Long short-term memory</article-title>. <source>Neural Comput</source>. <year>1997</year>;<volume>9</volume>(<issue>8</issue>):<fpage>1735</fpage>&#x02013;<lpage>80</lpage>.<pub-id pub-id-type="pmid">9377276</pub-id>
</mixed-citation></ref><ref id="pcbi.1013071.ref074"><label>74</label><mixed-citation publication-type="book"><name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Xia</surname><given-names>C</given-names></name>, <name><surname>Lv</surname><given-names>F</given-names></name>, <name><surname>Shi</surname><given-names>Y</given-names></name>. <source>RT-DETRv3: Real</source>-time End-to-End Object Detection with Hierarchical Dense Positive Supervision [Internet]. arXiv; 2<year>024</year> [cited 2024 Sep 19]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2409.08475" ext-link-type="uri">http://arxiv.org/abs/2409.08475</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref075"><label>75</label><mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>S</given-names></name>, <name><surname>Lu</surname><given-names>Z</given-names></name>, <name><surname>Cun</surname><given-names>X</given-names></name>, <name><surname>Yu</surname><given-names>Y</given-names></name>, <name><surname>Zhou</surname><given-names>X</given-names></name>, <name><surname>Shen</surname><given-names>X</given-names></name>. DEIM: DETR with Improved Matching for Fast Convergence [Internet]. arXiv; <year>2024</year> [cited 2024 Dec 8]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2412.04234" ext-link-type="uri">http://arxiv.org/abs/2412.04234</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref076"><label>76</label><mixed-citation publication-type="journal"><name><surname>Dubey</surname><given-names>S</given-names></name>, <name><surname>Olimov</surname><given-names>F</given-names></name>, <name><surname>Rafique</surname><given-names>M</given-names></name>, <name><surname>Jeon</surname><given-names>M</given-names></name>. <article-title>Improving small objects detection using transformer</article-title>. <source>J Vis Commun Image Represent</source>. <year>2022</year>;<volume>89</volume>:<fpage>103620</fpage>.</mixed-citation></ref><ref id="pcbi.1013071.ref077"><label>77</label><mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>H</given-names></name>. Small Object Detection by DETR via Information Augmentation and Adaptive Feature Fusion [Internet]. arXiv; <year>2024</year> [cited 2025 Mar 23]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2401.08017" ext-link-type="uri">http://arxiv.org/abs/2401.08017</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref078"><label>78</label><mixed-citation publication-type="journal"><name><surname>Xu</surname><given-names>Y</given-names></name>, <name><surname>Lyu</surname><given-names>D</given-names></name>, <name><surname>Li</surname><given-names>Z</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Chen</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>G</given-names></name>, <etal>et al</etal>. DEFA: Efficient Deformable Attention Acceleration via Pruning-Assisted Grid-Sampling and Multi-Scale Parallel Processing [Internet]. arXiv; <year>2024</year> [cited 2024 Mar 27]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2403.10913" ext-link-type="uri">http://arxiv.org/abs/2403.10913</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref079"><label>79</label><mixed-citation publication-type="journal"><name><surname>Kaiser</surname><given-names>T</given-names></name>, <name><surname>Ulman</surname><given-names>V</given-names></name>, <name><surname>Rosenhahn</surname><given-names>B</given-names></name>. CHOTA: A Higher Order Accuracy Metric for Cell Tracking [Internet]. arXiv; <year>2024</year> [cited 2024 Aug 23]. Available from: <ext-link xlink:href="http://arxiv.org/abs/2408.11571" ext-link-type="uri">http://arxiv.org/abs/2408.11571</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref080"><label>80</label><mixed-citation publication-type="book"><name><surname>Loshchilov</surname><given-names>I</given-names></name>, <name><surname>Hutter</surname><given-names>F</given-names></name>. Decoupled Weight Decay Regularization. ArXiv171105101 Cs Math [I<source>nternet]</source>. <year>2019</year> Jan 4 [cited 2022 Apr 13]; Available from: <ext-link xlink:href="http://arxiv.org/abs/1711.05101" ext-link-type="uri">http://arxiv.org/abs/1711.05101</ext-link></mixed-citation></ref><ref id="pcbi.1013071.ref081"><label>81</label><mixed-citation publication-type="journal"><name><surname>Kuhn</surname><given-names>H</given-names></name>. <article-title>The Hungarian method for the assignment problem</article-title>. <source>Nav Res Logist Q</source>. <year>1955</year>;<volume>2</volume>(1&#x02013;2):<fpage>83</fpage>&#x02013;<lpage>97</lpage>.</mixed-citation></ref></ref-list></back><sub-article article-type="author-comment" id="pcbi.1013071.r001" specific-use="rebutted-decision-letter-unavailable"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013071.r001</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">11 Jul 2024</named-content>
</p></body></sub-article><sub-article article-type="aggregated-review-documents" id="pcbi.1013071.r002" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013071.r002</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Uhlmann</surname><given-names>Virginie</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Virginie Uhlmann</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Virginie Uhlmann</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1013071" id="rel-obj002" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">24 Oct 2024</named-content>
</p><p>Dear Dr Dunlop,</p><p>Thank you very much for submitting your manuscript "Cell-TRACTR: A transformer-based model for end-to-end segmentation and tracking of cells" for consideration at PLOS Computational Biology.</p><p>As with all papers reviewed by the journal, your manuscript was reviewed by members of the editorial board and by several independent reviewers. In light of the reviews (below this email), we would like to invite the resubmission of a significantly-revised version that takes into account the reviewers' comments.</p><p>Please give special attention to the reviewers' concerns about reproducibility and comparison with existing methods, especially Trackastra and CALIBAN.</p><p>We cannot make any decision about publication until we have seen the revised manuscript and your response to the reviewers' comments. Your revised manuscript is also likely to be sent to reviewers for further evaluation.</p><p>When you are ready to resubmit, please upload the following:</p><p>[1] A letter containing a detailed list of your responses to the review comments and a description of the changes you have made in the manuscript.&#x000a0;Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out.</p><p>[2] Two versions of the revised manuscript: one with either highlights or tracked changes denoting where the text has been changed; the other a clean version (uploaded as the manuscript file).</p><p>Important additional instructions are given below your reviewer comments.</p><p>Please prepare and submit your revised manuscript within 60 days. If you anticipate any delay, please let us know the expected resubmission date by replying to this email. Please note that revised manuscripts received after the 60-day due date may require evaluation and peer review similar to newly submitted manuscripts.</p><p>Thank you again for your submission. We hope that our editorial process has been constructive so far, and we welcome your feedback at any time. Please don't hesitate to contact us if you have any questions or comments.</p><p>Sincerely,</p><p>Virginie Uhlmann</p><p>Academic Editor</p><p>PLOS Computational Biology</p><p>Jason Haugh</p><p>Section Editor</p><p>PLOS Computational Biology</p><p>***********************</p><p>Please give special attention to the reviewers' concerns about reproducibility and comparison with existing methods, especially Trackastra and CALIBAN.</p><p>Reviewer's Responses to Questions</p><p>
<bold>Comments to the Authors:</bold>
</p><p>
<bold>Please note here if the review is uploaded as an attachment.</bold>
</p><p>Reviewer #1:&#x000a0;Cell-TRACTR is a transformer-based deep learning algorithm that combines a convolutional neural network for visual feature extraction with a transformer encoder-decoder architecture, inspired by the DETR model from Carion et al. (2020). This method is the first to apply DETR to biological image tracking, but it has some limitations:</p><p>Major points</p><p>1. Limited Improvement: in comparison to the non-transformer-based methods, the improvement by Cell-TRACTR needs more investigation. The presented method shows little improvement with the first dataset and inconsistent improvement with the second dataset. Especially with the second dataset, the real performance gain is in the cell division accuracy component (DivA) of Cell-HOTA. The reason for this is not clear. This does not necessarily mean that the presented method is ineffective, but it will be more useful for the audience if the authors present more specific use cases where the DivA component would make a crucial difference. Adding such datasets or more detailed analysis on the DeepCell Dataset, such as ablation studies, might prove helpful.</p><p>2. Scalability Concerns: Due to DETR&#x02019;s bipartite matching, the number of detectable/trackable objects is limited by the specified number of queries (class label, bounding box, and segmentation mask). How does Cell-TRACTR handle images with numerous objects or significant variance in object numbers between samples? Additionally, insights into how computational cost scales with the expected maximum number of objects would be valuable.</p><p>Minor points</p><p>1. Lack of Comparison with Transformer-Based Models: The method is not compared to other transformer-based models. It would be useful to evaluate its performance against models like Trackastra, followed by cell-segmentation methods, to assess the impact of the transformer architecture.</p><p>2. Comparison with Common Workflows: Comparing Cell-TRACTR with popular workflows, such as commonly used configurations from Trackmate software, would be beneficial for biologists.</p><p>3. Figure 2 could be more concise by reducing the number of repetitive objects.</p><p>Reviewer #2:&#x000a0;The review is uploaded as an attachment (202409_Review_PLOS_Comp_Biol.md).</p><p>Reviewer #3:&#x000a0;Major comments:</p><p>1. It's difficult to assess the proposed method's performance in relation to the existing literature as a whole because it was evaluated in only two datasets against two algorithms (one of which is from the same authors). This is especially true since they propose a new metric, which is indeed needed. However, it reduces the comparability to existing work despite them reporting the CTC score, which, as they said, is problematic. We recommend comparing it with CALIBAN [22], which produced the DeepCell dataset used in this paper, and Trackastra [40], which is also a transformer-based method for tracking and the current state-of-the-art in the DeepCell dataset. Both of these methods have their code and weights publicly available.</p><p>2. A concurrent work, Cell-specific Higher Order Tracking Accuracy (CHOTA)[1*, 2*], which extends HOTA for cell tracking, was recently proposed. It doesn't seem to be the same metric as yours, so it would be interesting to highlight the differences between the two from the theoretical and experimental point of view (e.g., how they measure different things).</p><p>3. It's not clear how to obtain the network weights used in the experiments. The weights are as important as making the code available, especially in this case where server-grade GPU (V100/A100) are needed to train the models. Otherwise, the reproducibility and application of the proposed method are greatly reduced.</p><p>4. This method's application seems a bit limited because it requires an extensive amount of GPU RAM, an A100 for 584x600 pixel images with a batch size of 1, which are small images for today's standards. Plus, it seems to require a considerable amount of training data. How does it perform in videos from the Cell Tracking Challenge which are often more challenging than the DeepCell dataset?</p><p>Minor comments:</p><p>5. Omnipose appears twice in the references [8, 32]</p><p>6. From section "Training EmbedTrack." How are (horizontal or vertical) flip augmentations incompatible with a fixed crop size?</p><p>[1*] Kaiser, Timo, Vladimir Ulman, and Bodo Rosenhahn. "CHOTA: A Higher Order Accuracy Metric for Cell Tracking." arXiv preprint arXiv:2408.11571 (2024).</p><p>[2*] <ext-link xlink:href="http://www.bioimagecomputing.com/program/selected-contributions/" ext-link-type="uri">http://www.bioimagecomputing.com/program/selected-contributions/</ext-link></p><p>**********</p><p>
<bold>Have the authors made all data and (if applicable) computational code underlying the findings in their manuscript fully available?</bold>
</p><p>The <ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/materials-and-software-sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data and code underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data and code should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data or code &#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.</p><p>Reviewer #1:&#x000a0;<bold>No:&#x000a0;</bold> The code repository is not visible in the Data and Code Availability section.</p><p>Reviewer #2:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p>PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link> ). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link> .</p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>Reviewer #3:&#x000a0;No</p><p>
<underline>Figure Files:</underline>
</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com</ext-link>.&#x000a0;PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <email>figures@plos.org</email>.</p><p>
<underline>Data Requirements:</underline>
</p><p>Please note that, as a condition of publication, PLOS' data policy requires that you make available all data used to draw the conclusions outlined in your manuscript. Data must be deposited in an appropriate repository, included within the body of the manuscript, or uploaded as supporting information. This includes all numerical values that were used to generate graphs, histograms etc.. For an example in PLOS Biology see here: <ext-link xlink:href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5." ext-link-type="uri">http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5</ext-link>.</p><p>
<underline>Reproducibility:</underline>
</p><p>To enhance the reproducibility of your results, we recommend that you deposit your laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. Additionally, PLOS ONE offers an option to publish peer-reviewed clinical study protocols. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link></p><supplementary-material id="pcbi.1013071.s018" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">202409_Review_PLOS_Comp_Biol.md</named-content></p></caption><media xlink:href="pcbi.1013071.s018.md"/></supplementary-material></body></sub-article><sub-article article-type="author-comment" id="pcbi.1013071.r003"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013071.r003</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1013071" id="rel-obj003" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">9 Jan 2025</named-content>
</p><supplementary-material id="pcbi.1013071.s020" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Response to Reviewers.pdf</named-content></p></caption><media xlink:href="pcbi.1013071.s020.pdf"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pcbi.1013071.r004" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013071.r004</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Haugh</surname><given-names>Jason</given-names></name><role>Section Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Jason Haugh</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Jason Haugh</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1013071" id="rel-obj004" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">12 Feb 2025</named-content>
</p><p>PCOMPBIOL-D-24-01171R1</p><p>Cell-TRACTR: A transformer-based model for end-to-end segmentation and tracking of cells</p><p>PLOS Computational Biology</p><p>Dear Dr. Dunlop,</p><p>Thank you for submitting your manuscript to PLOS Computational Biology. After careful consideration, we feel that it has merit but does not fully meet PLOS Computational Biology's publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>As you can read from the reviews, Reviewers 1 and 2 are satisfied with the revised manuscript (though note the suggestions for code changes raised by Rev. 2), Reviewer 3 has expressed&#x000a0;lingering concern&#x000a0;about the ability to generalize the method -- in a computationally efficient manner -- to other datasets; this affects the prospects for widespread adoption in their view.</p><p>Please submit your revised manuscript within 30 days Apr 14 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at ploscompbiol@plos.org. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pcompbiol/" ext-link-type="uri">https://www.editorialmanager.com/pcompbiol/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:</p><p>* A rebuttal letter that responds to each point raised by the editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'. This file does not need to include responses to formatting updates and technical items listed in the 'Journal Requirements' section below.</p><p>* A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p><p>* An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p><p>If you would like to make changes to your financial disclosure, competing interests statement, or data availability statement, please make these updates within the submission form at the time of resubmission. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Jason M. Haugh</p><p>Section Editor</p><p>PLOS Computational Biology</p><p>
<bold>Journal Requirements:</bold>
</p><p>We have noticed that you have a list of Supporting Information legends in your manuscript for <bold>S1 Movie, S2 Movie, S3 Movie</bold> . However, there are no corresponding files uploaded to the submission. Please upload them as separate files with the item type 'Supporting Information'.</p><p>
<bold>Reviewers' comments:</bold>
</p><p>Reviewer's Responses to Questions</p><p>
<bold>Comments to the Authors:</bold>
</p><p>
<bold>Please note here if the review is uploaded as an attachment.</bold>
</p><p>Reviewer #1:&#x000a0;The authors have thoroughly addressed all of my concerns and those of the other reviewers.</p><p>Reviewer #2:&#x000a0;My concerns regarding the manuscript have been addressed by the authors.</p><p>Also, following the improvements in installation instructions and the code, I've been able to successfully train and apply Cell-TRACTR on the "moma" dataset, after fixing two (minor) code problems:</p><p>First, during training the following error occurred:</p><p>```</p><p>File ".../Cell-TRACTR/src/trackformer/datasets/mot.py", line 101, in __getitem__</p><p>man_track_id = self.coco.imgs[idx]['man_track_id']</p><p>KeyError: 'man_track_id'</p><p>```</p><p>I could fix this by adding the following line in the script which transforms datasets from CTC to COCO format, scripts/create_coco_dataset_from_CTC.py, after line 98 (relative to commit 32d37554b2cb9b83b271d5ea0f163f6b29250df6):</p><p>`'man_track_id': f'{dataset_name}',</p><p>Second, also during training there was a problem with the `datapath` in src/pipeline.py, which I could fix by changing line 127 from</p><p>`datapath = args.data_dir / 'CTC_datasets' / dataset / 'CTC' / 'test'`</p><p>to</p><p>`datapath = args.data_dir / dataset / 'CTC' / 'test'`</p><p>Given that all of my points have been addressed by authors and the code problems above are minor and easily fixed, I recommend the manuscript for publication.</p><p>Reviewer #3:&#x000a0;While the authors have addressed some concerns raised during the previous review, key issues remain unresolved, and the additional experiments fail to strengthen the manuscript&#x02019;s contributions.</p><p>
<bold>Major comments:</bold>
</p><p>- Computational Inefficiency and Resource Intensity: Despite the added discussion on computational constraints, the reliance on an A100 GPU for training for images with the size of 584x600, which is not that large in the broad scope of microscopy. Thus, the method is impractical for widespread adoption unless they show their pre-trained weights can be generalized to other datasets.</p><p>Performance on DeepCell: The results on the DeepCell dataset do not favor the claims that this method's performance is comparable with current approaches such as Trackastra and Caliban. While it could be argued that it works on the Mother Machine dataset, it&#x02019;s an incredibly limited scenario of 1D tracking because the cells' movement is mostly vertical because of the Mother Machine device.</p><p>- Unaddressed Reviewer Comments: Reviewer 2's suggestions for experiments corroborating the authors&#x02019; original claims were mainly addressed by tuning down their contributions. While this makes the paper more factual, it highlights how the method's performance could be improved.</p><p>In summary, the authors have made efforts to address the reviewers' feedback. However, the</p><p>limitations of the method persist, and the application of this method beyond the experiments shown here is questionable due to the availability of more computationally efficient and accurate methods.</p><p>**********</p><p>
<bold>Have the authors made all data and (if applicable) computational code underlying the findings in their manuscript fully available?</bold>
</p><p>The <ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/materials-and-software-sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data and code underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data and code should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data or code &#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.</p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p>PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link> ). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link> .</p><p>Reviewer #1:&#x000a0;<bold>Yes:&#x000a0;</bold> Hyoungjun Park</p><p>Reviewer #2:&#x000a0;No</p><p>Reviewer #3:&#x000a0;No</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>
<bold>Figure resubmission:</bold>
</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xlink:href="https://pacev2.apexcovantage.com/." ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at figures@plos.org. Please note that Supporting Information files do not need this step. If there are other versions of figure files still present in your submission file inventory at resubmission, please replace them with the PACE-processed versions.</p><p>
<bold>Reproducibility:</bold>
</p><p>To enhance the reproducibility of your results, we recommend that authors of applicable studies deposit laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. Additionally, PLOS ONE offers an option to publish peer-reviewed clinical study protocols. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link></p></body></sub-article><sub-article article-type="author-comment" id="pcbi.1013071.r005"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013071.r005</article-id><title-group><article-title>Author response to Decision Letter 2</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1013071" id="rel-obj005" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">4 Apr 2025</named-content>
</p><supplementary-material id="pcbi.1013071.s021" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Response_to_Reviewers_auresp_2.pdf</named-content></p></caption><media xlink:href="pcbi.1013071.s021.pdf"/></supplementary-material></body></sub-article><sub-article article-type="editor-report" id="pcbi.1013071.r006" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013071.r006</article-id><title-group><article-title>Decision Letter 2</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Mac Gabhann</surname><given-names>Feilim</given-names></name><role>Editor-in-Chief</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Feilim Mac Gabhann</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Feilim Mac Gabhann</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1013071" id="rel-obj006" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">21 Apr 2025</named-content>
</p><p>Dear Dr Dunlop,</p><p>We are pleased to inform you that your manuscript 'Cell-TRACTR: A transformer-based model for end-to-end segmentation and tracking of cells' has been provisionally accepted for publication in PLOS Computational Biology.</p><p>Before your manuscript can be formally accepted you will need to complete some formatting changes, which you will receive in a follow up email. A member of our team will be in touch with a set of requests.</p><p>Please note that your manuscript will not be scheduled for publication until you have made the required changes, so a swift response is appreciated.</p><p>IMPORTANT: The editorial review process is now complete. PLOS will only permit corrections to spelling, formatting or significant scientific errors from this point onwards. Requests for major changes, or any which affect the scientific understanding of your work, will cause delays to the publication date of your manuscript.</p><p>Should you, your institution's press office or the journal office choose to press release your paper, you will automatically be opted out of early publication. We ask that you notify us now if you or your institution is planning to press release the article. All press must be co-ordinated with PLOS.</p><p>Thank you again for supporting Open Access publishing; we are looking forward to publishing your work in PLOS Computational Biology.&#x000a0;</p><p>Best regards,</p><p>Feilim Mac Gabhann, Ph.D.</p><p>Editor-in-Chief</p><p>PLOS Computational Biology</p><p>***********************************************************</p></body></sub-article><sub-article article-type="editor-report" id="pcbi.1013071.r007" specific-use="acceptance-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013071.r007</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Mac Gabhann</surname><given-names>Feilim</given-names></name><role>Editor-in-Chief</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Feilim Mac Gabhann</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Feilim Mac Gabhann</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1013071" id="rel-obj007" related-article-type="reviewed-article"/></front-stub><body><p>PCOMPBIOL-D-24-01171R2</p><p>Cell-TRACTR: A transformer-based model for end-to-end segmentation and tracking of cells</p><p>Dear Dr Dunlop,</p><p>I am pleased to inform you that your manuscript has been formally accepted for publication in PLOS Computational Biology. Your manuscript is now with our production department and you will be notified of the publication date in due course.</p><p>The corresponding author will soon be receiving a typeset proof for review, to ensure errors have not been introduced during production. Please review the PDF proof of your manuscript carefully, as this is the last chance to correct any errors. Please note that major changes, or those which affect the scientific understanding of the work, will likely cause delays to the publication date of your manuscript.</p><p>Soon after your final files are uploaded, unless you have opted out, the early version of your manuscript will be published online. The date of the early version will be your article's publication date. The final article will be published to the same URL, and all versions of the paper will be accessible to readers.</p><p>Thank you again for supporting PLOS Computational Biology and open-access publishing. We are looking forward to publishing your work!</p><p>With kind regards,</p><p>Anita Estes</p><p>PLOS Computational Biology | Carlyle House, Carlyle Road, Cambridge CB4 3DN | United Kingdom ploscompbiol@plos.org | Phone +44 (0) 1223-442824 | ploscompbiol.org | @PLOSCompBiol</p></body></sub-article></article>