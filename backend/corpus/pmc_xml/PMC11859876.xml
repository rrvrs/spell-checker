<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006344</article-id><article-id pub-id-type="pmc">PMC11859876</article-id><article-id pub-id-type="doi">10.3390/s25041114</article-id><article-id pub-id-type="publisher-id">sensors-25-01114</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Vehicle Trajectory Repair Under Full Occlusion and Limited Datapoints with Roadside LiDAR</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3318-0004</contrib-id><name><surname>Luo</surname><given-names>Qiyang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01114" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-6724-9807</contrib-id><name><surname>Xu</surname><given-names>Zhenyu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-01114" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1165-7783</contrib-id><name><surname>Zhang</surname><given-names>Yibin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af2-sensors-25-01114" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4055-2750</contrib-id><name><surname>Igene</surname><given-names>Morris</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01114" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-3338-1890</contrib-id><name><surname>Bataineh</surname><given-names>Tamer</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-01114" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-6148-3165</contrib-id><name><surname>Soltanirad</surname><given-names>Mohammad</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-01114" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7140-5927</contrib-id><name><surname>Jimee</surname><given-names>Keshav</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-01114" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Hongchao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-01114" ref-type="aff">1</xref><xref rid="c1-sensors-25-01114" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Zi&#x00119;bi&#x00144;ski</surname><given-names>Adam</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01114"><label>1</label>Department of Civil, Environmental and Construction Engineering, Texas Tech University, Lubbock, TX 79409, USA; <email>qiyang.luo@ttu.edu</email> (Q.L.); <email>zhenxu@ttu.edu</email> (Z.X.); <email>morris.igene@ttu.edu</email> (M.I.); <email>tamer.bataineh@ttu.edu</email> (T.B.); <email>mohammad.soltanirad@ttu.edu</email> (M.S.); <email>kjimee@ttu.edu</email> (K.J.)</aff><aff id="af2-sensors-25-01114"><label>2</label>Department of Construction and Transportation Engineering, Ningbo University of Technology, Ningbo 315211, China; <email>yibinzhang@nbut.edu.cn</email></aff><author-notes><corresp id="c1-sensors-25-01114"><label>*</label>Correspondence: <email>hongchao.liu@ttu.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1114</elocation-id><history><date date-type="received"><day>31</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>08</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>09</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Object occlusion is a common challenge in roadside LiDAR-based vehicle tracking. This issue can cause variances in vehicle location and speed calculations. This paper proposes a vehicle tracking post-processing method designed to handle full occlusion and limited datapoint conditions. The first part of the method focuses on linking the disconnected trajectories of the same vehicle caused by full occlusion. The second part refines the vehicle representative point to enhance tracking accuracy. Performance evaluation demonstrates that the proposed method can detect and reconnect the trajectories of the same vehicle, even under prolonged full occlusion. Moreover, the refined vehicle representative point provides more stable speed estimates, even with sparse datapoints. This significantly increases the effective detection range of roadside LiDAR. This approach lays a strong foundation for the application of roadside LiDAR in emission analysis and near-crash studies.</p></abstract><kwd-group><kwd>roadside LiDAR</kwd><kwd>full occlusion</kwd><kwd>limited datapoints</kwd><kwd>trajectory repair</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01114"><title>1. Introduction</title><p>High-resolution vehicle trajectory data play a critical role in advancing various fields such as traffic engineering, autonomous driving, and intelligent transportation systems. These data are instrumental for applications such as crash prediction [<xref rid="B1-sensors-25-01114" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01114" ref-type="bibr">2</xref>], traffic density estimation [<xref rid="B3-sensors-25-01114" ref-type="bibr">3</xref>], flow monitoring [<xref rid="B4-sensors-25-01114" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01114" ref-type="bibr">5</xref>], car-following analysis [<xref rid="B6-sensors-25-01114" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-01114" ref-type="bibr">7</xref>], driver behavior modeling [<xref rid="B8-sensors-25-01114" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01114" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01114" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01114" ref-type="bibr">11</xref>], fuel consumption estimation [<xref rid="B12-sensors-25-01114" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01114" ref-type="bibr">13</xref>], adaptive traffic signal control [<xref rid="B14-sensors-25-01114" ref-type="bibr">14</xref>], and the development of advanced driver assistance systems (ADAS) [<xref rid="B15-sensors-25-01114" ref-type="bibr">15</xref>]. Moreover, they are essential for improving route navigation [<xref rid="B16-sensors-25-01114" ref-type="bibr">16</xref>], traffic demand analysis [<xref rid="B17-sensors-25-01114" ref-type="bibr">17</xref>], and overall traffic operations in complex urban environments.</p><p>Currently, multiple sensor technologies, including cameras, Light Detection and Ranging (LiDAR), radar, and Bluetooth, can provide vehicle trajectory data [<xref rid="B5-sensors-25-01114" ref-type="bibr">5</xref>,<xref rid="B18-sensors-25-01114" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-01114" ref-type="bibr">19</xref>]. Among these, LiDAR and cameras are known for delivering high-resolution data, which are particularly valuable for detailed trajectory analysis. However, occlusion remains a significant challenge in extracting accurate trajectories, especially in high-traffic intersections or dense urban areas [<xref rid="B20-sensors-25-01114" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-01114" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01114" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-01114" ref-type="bibr">23</xref>]. Occlusion occurs when one vehicle is blocked by another or obscured by objects in the environment, such as the background environment and dynamic moving vehicles [<xref rid="B24-sensors-25-01114" ref-type="bibr">24</xref>]. Occlusion has two types, full occlusion and partial occlusion. If full occlusion occurs, the datapoint of a vehicle could totally disappear. In some cases, the full occlusion could be more than one second [<xref rid="B25-sensors-25-01114" ref-type="bibr">25</xref>]. When the partial occlusion occurs, the vehicle datapoint could be chopped into two parts or only one part could be detected. Additionally, when the vehicle is far from the LiDAR, only sparse datapoints are captured. This complicates multi-object tracking tasks and reduces data reliability. This paper introduces a novel approach to extract high-resolution vehicle trajectory data using roadside LiDAR. The proposed method addresses two primary challenges: prolonged full occlusion and vehicle representative point refinement under limited datapoints.</p><p>The procedure of pre-tracking for roadside LiDAR consists of three main steps: background filtering, object clustering, and identification [<xref rid="B16-sensors-25-01114" ref-type="bibr">16</xref>]. To increase the accuracy of the tracking trajectory, the key is to track the same vehicle accurately and continuously [<xref rid="B26-sensors-25-01114" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01114" ref-type="bibr">27</xref>]. A couple of techniques are widely used, including Graph Neural Network (GNN) [<xref rid="B1-sensors-25-01114" ref-type="bibr">1</xref>], Joint Probabilistic Data Association (JPDA) [<xref rid="B28-sensors-25-01114" ref-type="bibr">28</xref>], Multiple Hypothesis Tracking (MTH) [<xref rid="B29-sensors-25-01114" ref-type="bibr">29</xref>], and Hungarian algorithms [<xref rid="B30-sensors-25-01114" ref-type="bibr">30</xref>]. GNN utilizes the Euclidean distance to associate the closest vehicle across different point cloud frames. This approach is straightforward to implement and requires relatively low computational resources [<xref rid="B31-sensors-25-01114" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-01114" ref-type="bibr">32</xref>]. For example, Zhao [<xref rid="B33-sensors-25-01114" ref-type="bibr">33</xref>] employed GNN to track vehicles and pedestrians, achieving a tracking accuracy of approximately 95%. However, GNN often struggles with false tracking and data loss, especially in high-density traffic conditions. To mitigate these issues, Zhang et al. [<xref rid="B34-sensors-25-01114" ref-type="bibr">34</xref>] introduced JPDA for tracking in complex traffic environments, which significantly enhanced tracking performance. Despite its advantages, JPDA demands substantial heavy computational power and is prone to combinatorial explosion. Kim and Park [<xref rid="B35-sensors-25-01114" ref-type="bibr">35</xref>] introduced an extended Kalman filter (EKF) that incorporates the distance characteristics of LiDAR and radar sensors. By leveraging sensor fusion, their method achieves highly accurate distance estimations. Wu [<xref rid="B32-sensors-25-01114" ref-type="bibr">32</xref>] developed a systematic approach for vehicle tracking using roadside LiDAR sensors. Their procedure, structured into five distinct parts, was validated through field testing, demonstrating its effectiveness. Similarly, Vimal Kumar et al. [<xref rid="B36-sensors-25-01114" ref-type="bibr">36</xref>] utilized a low-density solid-state flash LiDAR to collect sparse data. By combining the JPDA algorithm with a Kalman filter, they successfully extracted vehicle trajectories, with the results indicating strong performance in trajectory reconstruction using their method. However, when the full occlusion occurs for a long time, such as more than one second, it is still difficult to ensure tracking the same vehicle. It is inevitable for the GNN to generate a new vehicle ID. For the EKF and JPDA, the EKF is a recursive algorithm for estimating the state of dynamic systems by combining predictions with actual observations. Its performance relies on accurate noise models, and under sparse data conditions, noise can cause significant deviations. When sensors lose target information (e.g., prolonged occlusion), the EKF depends solely on predictions, leading to cumulative errors and reduced accuracy. JPDA tracks multiple objects by associating observations with targets using probabilistic hypotheses. Sparse data and occlusion increase uncertainty, degrading association quality and causing trajectory fragmentation after prolonged occlusion. Thus, the EKF and JPDA are unsuitable for prolonged full occlusions. Therefore, a post-tracking method becomes essential to increase the trajectory accuracy.</p><p>Another challenge is how to accurately represent the vehicle&#x02019;s location for association. Two commonly used methods are representative points and boundary boxes. Changes in distance and orientation lead to variations in the vehicle&#x02019;s shape, resulting in significant differences in the number of datapoints for the same vehicle across different frames. This inconsistency in datapoints affects the stability of the tracking point and boundary box [<xref rid="B36-sensors-25-01114" ref-type="bibr">36</xref>]. Zhang et al. [<xref rid="B34-sensors-25-01114" ref-type="bibr">34</xref>] used corner points for vehicle tracking. Their results demonstrated that tracking vehicles using corner points significantly enhances accuracy compared to relying on bounding boxes. However, this method relies on enough datapoints. When there are sparse datapoints, this method may not be effective. Later, Wu [<xref rid="B37-sensors-25-01114" ref-type="bibr">37</xref>] and Sun et al. [<xref rid="B38-sensors-25-01114" ref-type="bibr">38</xref>] further confirmed that, compared to using the vehicle&#x02019;s center point, employing corner points helps reduce speed estimation errors, making them a more reliable choice for precise tracking. However, the reason why the vehicle&#x02019;s center point is not considered sufficient as a tracking point lies in the instability of datapoints across frames, particularly in cases of full occlusion or when only a very limited number of datapoints are available. In such scenarios, accurately determining the vehicle&#x02019;s tracking point becomes challenging, especially during severe partial occlusion. Developing a method to reliably identify the vehicle&#x02019;s center point using only a limited number of datapoints could effectively resolve this issue.</p><p>This post-tracking method is specifically designed to address prolonged full occlusion and scenarios with extremely limited data. The method utilizes information derived from traffic intersection geometry maps, which enriches each vehicle&#x02019;s data with additional context, such as lane information, etc. (details are provided later). Using the proposed approach, the system identifies and reconnects different vehicle IDs belonging to the same vehicle generated by the full occlusion. Subsequently, the vehicle representative point is adjusted, even under sparse datapoints, to improve tracking accuracy. Experimental results demonstrate that the proposed method is effective in reconnecting trajectories under prolonged full occlusion. Furthermore, the newly adjusted vehicle representative point results in more stable speed estimates.</p></sec><sec id="sec2-sensors-25-01114"><title>2. Materials and Methods</title><p>The proposed method comprises four main steps: initial tracking, trajectory reconnection, tracking point repair, and trajectory supplementation, as illustrated in <xref rid="sensors-25-01114-f001" ref-type="fig">Figure 1</xref>. In the initial tracking step, various tracking methods exhibit differing levels of performance. However, during prolonged full occlusions, reliably identifying the same vehicle remains a challenge regardless of the tracking method used. Therefore, this study employs the Simple Online Real-time Tracking (SORT) algorithm in the initial tracking phase. The geometric map of traffic intersections, which includes land location in the LiDAR map, is generated by Lin&#x02019;s method [<xref rid="B39-sensors-25-01114" ref-type="bibr">39</xref>]. Based on this geometry map, the coordinate of the lane entrance is acquired, and each vehicle is assigned a lane ID to facilitate subsequent steps. During the trajectory reconnection step, the proposed method links different vehicle IDs that correspond to the same vehicle during periods of full occlusion. The next step involves selecting the most representative frame for the vehicle among the various IDs and using its bounding box to determine the vehicle&#x02019;s maximum length and width. The representative frame information is then translated along the direction of the vehicle&#x02019;s predicted trajectory to align with other frames, thereby identifying new representative points for those frames. Finally, a supplement trajectory is generated for the vehicle under prolong full occlusions.</p><sec id="sec2dot1-sensors-25-01114"><title>2.1. Initial Tracking</title><p>The pre-tracking step involves background filtering, object clustering, and object classification. The foundation of this method relies on the initial tracking results and a geometric map. The widely used SORT algorithm was chosen for the initial tracking phase. Since the proposed method utilizes changes in geometric information before and after a vehicle disappears due to full occlusion, a reliable geometric map of the traffic intersection is crucial. This study employs Lin&#x02019;s method to generate the geometric map [<xref rid="B39-sensors-25-01114" ref-type="bibr">39</xref>]. Each vehicle in each frame is assigned a lane ID, such as N_S_1, indicating that the vehicle is moving straight in the No. 1 Northbound Lane.</p></sec><sec id="sec2dot2-sensors-25-01114"><title>2.2. Trajectory Reconnection</title><p>This step addresses the issue of vehicles under prolong full occlusion. The core principle is to detect changes in distance between each vehicle and its front and rear neighbors in every frame. The process begins by ordering vehicles based on their distance from the lane entrance to assign a rank ID for each frame. For example, as illustrated in <xref rid="sensors-25-01114-f002" ref-type="fig">Figure 2</xref>, in Frame <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, three vehicles, such as <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>#</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>#</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>#</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are present, with Rank ID 1, 2, 3, respectively. For <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>#</mml:mo><mml:mn>1</mml:mn><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (Rank ID 1), there is no vehicle in front of it, so it only has the parameter <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, which represents the distance to the rear vehicle, as shown in Equation (1). In this step, the center of the cluster is used as the representative point. For <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>#</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (Rank ID 2), it has both <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> stands for &#x0201c;front.&#x0201d; The parameter <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the distance between the current vehicle and the front vehicle. Similarly, for <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>#</mml:mo><mml:mn>3</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (Rank ID 3), since it is the last vehicle in the lane, it only has <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. In the next frame, due to dynamic factors such as vehicle movement or occlusion caused by vehicles in adjacent lanes, full occlusion occurs. <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> &#x0201c;disappears&#x0201d;, and the rank IDs of the surrounding vehicles (<inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) and their associated <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> values will change accordingly. Specifically, the <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>#</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> will increase significantly because <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is no longer present, and now <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> measures the distance between <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in Frame <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Similarly, the <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in Frame <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> will also increase (previously <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> measured the distance to <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>B</mml:mi><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in Frame <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), as it now measures the distance between <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To account for dynamic changes such as vehicle speeds and errors in the representative points, a speed and error parameter (&#x003b3;) is introduced. This parameter is used to evaluate whether the <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> value of a vehicle in the current frame falls within an allowable range derived from the <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of the previous frame (<inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). If the <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> exceeds this range (Equations (2) and (3)), full occlusion is determined to have occurred. For instance, in Frame <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the full occlusion of <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> leads to a sharp increase in both <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, surpassing the acceptable range of fluctuations caused by the speed changes of nearby vehicles.<disp-formula id="FD1-sensors-25-01114"><label>(1)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x000a0;</mml:mo><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the distance between the current vehicle to the following vehicle. <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the distance between the current vehicle to the following vehicle. <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are the frame ID and rank ID, respectively.<disp-formula id="FD2-sensors-25-01114"><label>(2)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>]</mml:mo><mml:mo>&#x0003c;</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x000a0;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x02264;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>[</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>&#x02217;</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-sensors-25-01114"><label>(3)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD5-sensors-25-01114"><label>(4)</label><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mo>[</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where the <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the vehicle velocity of Rank ID <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in Frame ID <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the time interval 0.1 s. The parameter <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is used to adjust for the possible variation in the vehicle&#x02019;s center position, and it is set as 0.5 m. The parameter &#x003b3; is introduced to address the uncertainties associated with estimating the vehicle center and speed variations. This is necessary because, at this stage, the vehicle center is determined based on the cluster center, which may not perfectly represent the actual vehicle center. The true vehicle center should ideally be obtained by incorporating as many vehicles point clouds as possible. If &#x003b3; is set too small, the system becomes overly sensitive to minor variations in the distance between vehicles, which can result in a higher false positive rate by mistakenly detecting occlusions. Conversely, if &#x003b3; is too large, the false positive rate decreases, but the false negative rate may increase, meaning the system could fail to detect actual cases of full occlusion. To strike a balance between these risks, &#x003b3; is set to 0.5. This value effectively manages the trade-off, ensuring that the system maintains sensitivity to true occlusion events while minimizing false detections.</p></sec><sec id="sec2dot3-sensors-25-01114"><title>2.3. Tracking Point Repair</title><p>This step is designed to address sparse datapoints caused by either partial occlusion or vehicles being too far from the LiDAR sensor. For partial occlusion, there are two scenarios: the first involves a single vehicle being split into two clusters, for which Song&#x02019;s method is applicable [<xref rid="B25-sensors-25-01114" ref-type="bibr">25</xref>]. The second scenario, which is the focus of this paper, occurs when the clustering result contains only a small number of datapoints. After the trajectory reconnection step, datapoints for the same vehicle with different vehicle IDs are consolidated, significantly enhancing the likelihood of obtaining the most comprehensive information about a single vehicle. In previous studies, researchers often relied on corner points, as they are easier to extract than center points. However, if Song&#x02019;s method [<xref rid="B25-sensors-25-01114" ref-type="bibr">25</xref>], shown in Equation (5), is applied, a single clustering result may yield multiple corner points, as shown in <xref rid="sensors-25-01114-f003" ref-type="fig">Figure 3</xref>c. This can lead to errors in determining the corner point of the vehicle, thereby affecting speed calculations. Similarly, if Zhang&#x02019;s method [<xref rid="B34-sensors-25-01114" ref-type="bibr">34</xref>] is used to locate the center point, it also requires enough datapoints. Furthermore, neither Song&#x02019;s nor Zhang&#x02019;s methods are suitable for situations with very sparse datapoints, as illustrated in <xref rid="sensors-25-01114-f003" ref-type="fig">Figure 3</xref>d.<disp-formula id="FD7-sensors-25-01114"><label>(5)</label><mml:math id="mm56" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>n</mml:mi><mml:mi>&#x003f5;</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>N</mml:mi><mml:mi>V</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x00026;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where slope is denoted as &#x003b1;. The angle <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> between max &#x003b1; and min &#x003b1; can then be calculated. Here, <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is set as <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. If the <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mn>90</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x02264;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msup><mml:mrow><mml:mn>90</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, point <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is selected as the corner point.</p><p>To address this limitation, a novel approach is proposed, which translates the boundary box of the representative frame to encapsulate the remaining frames and refine the vehicle&#x02019;s center point. The bounding box is selected from the representative frame with the most comprehensive data collected for a specific <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> during the trajectory reconnection step. <xref rid="sensors-25-01114-f003" ref-type="fig">Figure 3</xref>a shows a known representative frame, which is then used to process three different cases of data density: high, medium, and low, corresponding to <xref rid="sensors-25-01114-f003" ref-type="fig">Figure 3</xref>b, <xref rid="sensors-25-01114-f003" ref-type="fig">Figure 3</xref>c, and <xref rid="sensors-25-01114-f003" ref-type="fig">Figure 3</xref>d, respectively. To identify alignment points, different methods are applied based on the density of datapoints. The pseudo-code is illustrated in Algorithm 1. For cases with a moderate number of datapoints (<inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x0003e;</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), the point within the boundary box that is closest to the LiDAR and the lane entrance is selected as the alignment point for the representative frame. For cases with a high number of datapoints (<inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x02264;</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), the alignment point is chosen from the cluster data, specifically the point closest to the LiDAR and the lane entrance.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1.</bold> Selection of the aligned point.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Input</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">k</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Note</mml:mi><mml:mo>:</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>is</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>the</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>target</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>frame</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>ID</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">k</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>is</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>the</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>representative</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>frame</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>ID</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">j</mml:mi></mml:msub><mml:mo>&#x000a0;</mml:mo><mml:mi>is</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>the</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>datapoint</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>number</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>frame</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:msub><mml:mo>&#x000a0;</mml:mo><mml:mi>is</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>the</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>datapoint</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>number</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>frame</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">k</mml:mi><mml:mo>.</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mi>is</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>the</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>coordinate</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>the</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>entrance</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>the</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>lane</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mi>is</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>the</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>coordinate</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>the</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>LiDAR</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>weighted</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>parameter</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Begin</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><list list-type="order"><list-item><p>If <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x02264;</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the selected aligned point is the cluster point that is closest to both the LiDAR and the lane entrance. P is the set of points in the cluster points.</p></list-item></list><break/>&#x02003;
(<inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) = <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi>&#x003f5;</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:munder></mml:mrow></mml:mrow></mml:math></inline-formula> [<inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula>]</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><list list-type="simple"><list-item><label>2.</label><p>If <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x0003e;</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the selected aligned point is the corner point of the boundary box that is closest to both the LiDAR and the lane entrance. B is the set of corner points in the boundary box.</p></list-item></list><break/>&#x02003;
(<inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) = <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi>&#x003f5;</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:munder></mml:mrow></mml:mrow></mml:math></inline-formula> [<inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula>]</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><list list-type="simple"><list-item><label>3.</label><p>End if</p></list-item></list></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">END</td></tr></tbody></array></p></sec><sec id="sec2dot4-sensors-25-01114"><title>2.4. Virtual Trajectory Supplement</title><p>This step aims to supplement the &#x0201c;missing&#x0201d; trajectory caused by full occlusion. Following the trajectory reconnection and trajectory point relocation steps, the frame ID and positions of the vehicle before and after the full occlusion have been identified. This step involves connecting the trajectory points associated with the two previously identified vehicle IDs. Using the geometric map, a reference trajectory is provided. The positions of the two frame IDs are then projected onto this virtual trajectory. Interpolation is applied to reconstruct the &#x0201c;missing&#x0201d; portion of the trajectory. The process is illustrated in Algorithm 2. The first step is to project the coordinates of the two vehicle IDs (<inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) and (<inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>), both before and after disappearance, onto the predicted trajectory to obtain (<inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) and (<inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>). Then, apply linear interpolation to compute the &#x0201c;missing&#x0201d; segments of the trajectory.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 2.</bold> Generating the virtual trajectory.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Input: a, b, m, n</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Note</mml:mi><mml:mo>:</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>and</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">b</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>are</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>two</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>vehicle</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>IDs</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>belonging</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>to</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>the</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>same</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>vehicle</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>in</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>frame</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>and</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>frame</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>and</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>n</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Begin</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><list list-type="order"><list-item><p>Find the coordinate (<inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) and (<inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>)</p></list-item></list></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><list list-type="simple"><list-item><label>2.</label><p>Project the (<inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) and (<inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) to the reference lane provided by the geometry map, and acquire the (<inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) and (<inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>)</p></list-item></list></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><list list-type="simple"><list-item><label>3.</label><p>Calculate <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, for each intermediate frame <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, calculate the position <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> using linear interpolation along the virtual road segment:</p></list-item></list><break/>&#x02003;<inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">END</td></tr></tbody></array></p></sec><sec id="sec2dot5-sensors-25-01114"><title>2.5. Data Collection</title><p>In this study, the VLP-32 LiDAR sensor, is sourced from Velodyne Lidar, Inc., San Jose, CA, USA, was used for data collection. The sensor has 32 channels with vertical scan angles from &#x02212;15&#x000b0; to +15&#x000b0; and a 360&#x000b0; horizontal scan range. Operating at a rotational frequency of 10 Hz, it captures 10 frames per second, with each frame containing approximately 600,000 3D points. An urban high-traffic intersection, Frankford Ave and 4th St, in Lubbock, Texas, in the USA was selected for the performance evaluation of the proposed method, as shown in <xref rid="sensors-25-01114-f004" ref-type="fig">Figure 4</xref>. The data were collected in the peak hours from 4 p.m. to 6 p.m.</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-01114"><title>3. Results</title><sec id="sec3dot1-sensors-25-01114"><title>3.1. Full Occlusion Detection</title><p><xref rid="sensors-25-01114-f005" ref-type="fig">Figure 5</xref> illustrates an example of the results for full occlusion detection. In this case, the vehicle is fully occluded between frame ID 296 and frame 305, disappearing for a total of 9 frames (0.9 s). This duration is sufficient for SORT to generate a new vehicle ID. However, using the proposed method, vehicle ID 457 and vehicle ID 413 are identified as belonging to the same vehicle. The underlying principle is that when vehicle ID 457 disappears in frame 297, the <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of the preceding vehicle and the <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of the following vehicle increase significantly. These increases exceed the normal ranges of <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, thereby triggering full occlusion detection. This example demonstrates the effectiveness of the proposed method in detecting prolonged full occlusion.</p></sec><sec id="sec3dot2-sensors-25-01114"><title>3.2. Tracking Point Refinement and Virtual Trajectory Generation</title><p><xref rid="sensors-25-01114-f006" ref-type="fig">Figure 6</xref> illustrates the results of tracking point refinement, where the proposed method is applied to identify representative points under a varying number of datapoints. As shown in <xref rid="sensors-25-01114-f006" ref-type="fig">Figure 6</xref>a&#x02013;c, directly using the boundary box to represent the tracking point is unavailable when the number of datapoints is small, let alone relying on the cluster center. This is because, in cases of partial occlusion or when the vehicle is far from the LiDAR, the number of datapoints per vehicle is significantly reduced. By employing the proposed method (<inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac bevelled="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x0003e;</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), a relatively accurate vehicle center can be determined, especially when the data volume is extremely low, as demonstrated in <xref rid="sensors-25-01114-f006" ref-type="fig">Figure 6</xref>c. For situations where the number of datapoints is substantial, the proposed method can still provide a relatively accurate vehicle position. This is because partial occlusion persists in these cases; however, it is less severe. Moreover, by translating the boundary box of the representative frame, a more accurate vehicle center can be acquired.</p><p><xref rid="sensors-25-01114-f007" ref-type="fig">Figure 7</xref> and <xref rid="sensors-25-01114-f008" ref-type="fig">Figure 8</xref> present a comparison of the trajectories of representative points and their corresponding speeds using the cluster center, boundary box, and the proposed method, from frame 290 to frame 341. For points with a small number of LiDAR detections on a single vehicle, the representative points provided by the proposed method are relatively stable and continuous, with minimal deviation from the reference trajectory (e.g., frame 296 and frame 308). For points with a larger number of LiDAR detections on a single vehicle, the speeds generated by the proposed method are more stable compared to the other two methods, exhibiting lower speed variance. Moreover, the proposed method ensures that the speed is less influenced by fluctuations in the data volume.</p></sec><sec id="sec3dot3-sensors-25-01114"><title>3.3. Compared to the State-of-the-Art Method</title><p>To validate the proposed method, we selected the traffic flow in four directions of a traditional intersection as a test scenario, consisting of straight roads and curved roads (left and right turns). To evaluate the method&#x02019;s performance in detecting prolonged full occlusion, we divided the discontinuous trajectories into two categories: those lasting less than 0.5 s (NDLS) and those greater than 1.0 s (NDGS), as shown <xref rid="sensors-25-01114-f009" ref-type="fig">Figure 9</xref>. The results were compared with the state-of-the-art methods proposed by Zhao [<xref rid="B33-sensors-25-01114" ref-type="bibr">33</xref>] and Song [<xref rid="B25-sensors-25-01114" ref-type="bibr">25</xref>], as shown in <xref rid="sensors-25-01114-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-25-01114-t002" ref-type="table">Table 2</xref>. To the best of our knowledge, Zhao&#x02019;s [<xref rid="B33-sensors-25-01114" ref-type="bibr">33</xref>] and Song&#x02019;s [<xref rid="B25-sensors-25-01114" ref-type="bibr">25</xref>] methods represent some of the most advanced approaches currently available.</p><p>The proposed method outperforms Zhao&#x02019;s method [<xref rid="B33-sensors-25-01114" ref-type="bibr">33</xref>] in reconstructing long-term full occlusions on straight roads, particularly in cases of prolonged full occlusions, as shown in <xref rid="sensors-25-01114-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-25-01114-t002" ref-type="table">Table 2</xref>. When compared to Song&#x02019;s method [<xref rid="B25-sensors-25-01114" ref-type="bibr">25</xref>], the performance on straight roads is similar for discontinuous trajectories shorter than 0.5 s. However, for discontinuous trajectories lasting longer than 1.0s on straight roads, the proposed method demonstrates superior performance. This improvement can be attributed to two key reasons: 1. Song [<xref rid="B25-sensors-25-01114" ref-type="bibr">25</xref>] detects full occlusion within a spatial range rather than on a lane-by-lane basis. 2. Song&#x02019;s [<xref rid="B25-sensors-25-01114" ref-type="bibr">25</xref>] time threshold for detecting full occlusion is limited to 1.5 s, after which the search is terminated. Since Song&#x02019;s [<xref rid="B25-sensors-25-01114" ref-type="bibr">25</xref>] approach relies on a range-based detection rather than a lane-specific one, its performance on curved roads is inferior to the proposed method. This is expected, as range-based searches are more susceptible to interference from adjacent lanes. In contrast, the proposed method performs lane-by-lane detection, effectively mitigating this issue. However, as this method is based on a geometric map, it may encounter challenges during turns, including traditional left and right turns, as well as special cases such as S-curves and other irregular road geometries. The accuracy of the predicted trajectory plays a significant role in the algorithm&#x02019;s performance under these conditions. When a vehicle turns, there are different potential turning scenarios. If a prolonged full occlusion occurs during a turn, the algorithm may face risks of failure. This is because the method assigns a lane ID to the vehicle in each frame, and inaccurate trajectory prediction will affect the judgment of the vehicle&#x02019;s lane. This explains why the proposed method consistently performs better on straight roads than on curved roads: the predicted trajectory for straight roads is easier to obtain and more reliable compared to that for curved roads.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-01114"><title>4. Discussion</title><p>The proposed method proves to be highly effective in addressing the challenge of full occlusion at busy intersections. It successfully connects different vehicle IDs belonging to the same vehicle, even during prolonged full occlusion. In terms of finding representative points, the method is capable of accurately tracking the center of the vehicle, even when datapoints are extremely limited. This capability not only enhances tracking accuracy and stability but also significantly extends the effective detection range of LiDAR, as illustrated in <xref rid="sensors-25-01114-f010" ref-type="fig">Figure 10</xref>. Traditional methods often fail to track vehicles under such sparse data conditions, but the proposed method demonstrates remarkable success.</p><p>However, certain factors may influence the accuracy of the proposed method. The first is the geometric map. While the examples in this study are based on straight roads, the method can be applied to curved road sections as well. The challenge, however, lies in accurately defining road boundaries, as the method depends on the geometric map to provide lane information for vehicles. The second limitation is related to lane changes. The proposed method cannot detect lane change behaviors. Nevertheless, since the method is designed for intersections with heavy traffic, lane change occurrences are relatively rare in this scenario due to the small gaps between vehicles and the limited space available for lane changes.</p><p>Although the proposed method utilizes the VLP-32 LiDAR, it is applicable to different ranges of LiDAR devices, including both VLP-64 and VLP-16. This is because the principle behind detecting full occlusion in the proposed method is based on the impact of the occluded vehicle on the preceding and following vehicles during prolonged full occlusion, which is independent of the LiDAR&#x02019;s frequency. Moreover, regardless of the LiDAR&#x02019;s frequency, vehicles are prone to experiencing partial occlusion and full occlusion during movement. In such scenarios, the proposed method&#x02019;s trajectory reconnection and tracking point repair functions effectively, making it suitable for LiDAR devices across various frequency ranges.</p><p>With the advancement of Artificial Intelligence (AI), it has become a powerful tool for enhancing roadside LiDAR-based vehicle tracking. Future work can leverage AI to improve performance in real-time prediction of missing trajectories, refined trajectory interpolation, and adaptive occlusion recovery beyond predefined geometric maps: 1. Real-Time Prediction of Missing Trajectories: Generative AI models can predict missing trajectory segments by learning from historical and real-time roadside LiDAR data, complementing trajectory reconnection and reducing errors from prolonged occlusions. 2. Refining Trajectory Interpolation: By using probabilistic diffusion methods, foundation models can refine trajectory interpolation while addressing noise and sparse data, improving tracking accuracy and speed estimates. 3. Adaptive Occlusion Recovery: Generative models can dynamically adapt to various road geometries and environmental conditions without relying solely on predefined maps, effectively handling occlusions in challenging layouts like sharp turns or complex intersections. Integrating AI into roadside LiDAR-based tracking offers a promising direction for future research and enhances the robustness of the proposed method under sparse data and prolonged occlusion scenarios.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-01114"><title>5. Conclusions</title><p>This paper introduces a novel vehicle trajectory supplementation method designed specifically for high-density intersections, addressing the situation of tracking vehicles in prolong full occlusion and limited datapoint situations. This method improves the relia-bility of vehicle tracking in high-density intersections. The main contributions of this work are summarized as follows: 1. Improved trajectory accuracy under full occlusion: This method can track vehicles even in cases where full occlusion causes large track gaps. 2. Vehicle center localization under sparse data conditions: The center of the vehicle can be accurately identified even when the LiDAR data points are limited due to partial occlusion. 3. Improved practicality of LiDAR at long distances: This method extends the effective detection range of LiDAR and enables vehicle tracking even at the outer limit of the LiDAR range (where the single vehicle data points are significantly reduced). This work offers potential benefits for a variety of transportation applications, including traffic counting, vehicle speed tracking, adaptive traffic signal control, and traffic safety analysis. However, certain limitations of the proposed method must be acknowledged. There are specific scenarios that the method cannot effectively handle, and developing solutions for these situations will be an avenue for future research. Previous studies have shown that the use of multiple sensors can improve object tracking performance. While this study focused only on vehicle detection and tracking using roadside LiDAR sensors, integrating data from other sensors such as cameras and radars may improve accuracy in specific situations. Exploring the fusion of data from different traffic sensors to further improve tracking accuracy provides an exciting direction for future research.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Q.L.; data curation, Q.L., Y.Z., M.I., T.B., M.S. and K.J.; formal analysis, Q.L.; methodology, Q.L.; resources, H.L.; software, Q.L. and Y.Z.; supervision, H.L.; validation, Q.L. and Z.X.; visualization, Q.L.; writing&#x02014;original draft, Q.L.; writing&#x02014;review and editing, M.I. and H.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data will be made available on request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01114"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cui</surname><given-names>Y.</given-names></name>
<name><surname>Xu</surname><given-names>H.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Zhao</surname><given-names>J.</given-names></name>
</person-group><article-title>Automatic Vehicle Tracking with Roadside LiDAR Data for the Connected-Vehicles System</article-title><source>IEEE Intell. Syst.</source><year>2019</year><volume>34</volume><fpage>44</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1109/MIS.2019.2918115</pub-id></element-citation></ref><ref id="B2-sensors-25-01114"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Xu</surname><given-names>H.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Zheng</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
</person-group><article-title>Trajectory tracking and prediction of pedestrian&#x02019;s crossing intention using roadside LiDAR</article-title><source>IET Intell. Transp. Syst.</source><year>2019</year><volume>13</volume><fpage>789</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1049/iet-its.2018.5258</pub-id></element-citation></ref><ref id="B3-sensors-25-01114"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Arr&#x000f3;spide</surname><given-names>J.</given-names></name>
<name><surname>Salgado</surname><given-names>L.</given-names></name>
</person-group><article-title>A Study of Feature Combination for Vehicle Detection Based on Image Processing</article-title><source>Sci. World J.</source><year>2014</year><volume>2014</volume><fpage>196251</fpage><pub-id pub-id-type="doi">10.1155/2014/196251</pub-id></element-citation></ref><ref id="B4-sensors-25-01114"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fan</surname><given-names>J.</given-names></name>
<name><surname>Fu</surname><given-names>C.</given-names></name>
<name><surname>Stewart</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
</person-group><article-title>Using big GPS trajectory data analytics for vehicle miles traveled estimation</article-title><source>Transp. Res. Part C Emerg. Technol.</source><year>2019</year><volume>103</volume><fpage>298</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1016/j.trc.2019.04.019</pub-id></element-citation></ref><ref id="B5-sensors-25-01114"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
<name><surname>Qi</surname><given-names>L.</given-names></name>
<name><surname>Ke</surname><given-names>R.</given-names></name>
</person-group><article-title>High-Resolution Vehicle Trajectory Extraction and Denoising from Aerial Videos</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2021</year><volume>22</volume><fpage>3190</fpage><lpage>3202</lpage><pub-id pub-id-type="doi">10.1109/TITS.2020.3003782</pub-id></element-citation></ref><ref id="B6-sensors-25-01114"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vasconcelos</surname><given-names>L.</given-names></name>
<name><surname>Neto</surname><given-names>L.</given-names></name>
<name><surname>Santos</surname><given-names>S.</given-names></name>
<name><surname>Silva</surname><given-names>A.B.</given-names></name>
<name><surname>Seco</surname><given-names>&#x000c1;.</given-names></name>
</person-group><article-title>Calibration of the Gipps Car-following Model Using Trajectory Data</article-title><source>Transp. Res. Procedia</source><year>2014</year><volume>3</volume><fpage>952</fpage><lpage>961</lpage><pub-id pub-id-type="doi">10.1016/j.trpro.2014.10.075</pub-id></element-citation></ref><ref id="B7-sensors-25-01114"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xue</surname><given-names>Q.</given-names></name>
<name><surname>Wang</surname><given-names>K.</given-names></name>
<name><surname>Lu</surname><given-names>J.J.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Rapid Driving Style Recognition in Car-Following Using Machine Learning and Vehicle Trajectory Data</article-title><source>J. Adv. Transp.</source><year>2019</year><volume>2019</volume><fpage>9085238</fpage><pub-id pub-id-type="doi">10.1155/2019/9085238</pub-id></element-citation></ref><ref id="B8-sensors-25-01114"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tang</surname><given-names>J.</given-names></name>
<name><surname>Liang</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Huang</surname><given-names>H.</given-names></name>
<name><surname>Liu</surname><given-names>F.</given-names></name>
</person-group><article-title>Inferring driving trajectories based on probabilistic model from large scale taxi GPS data</article-title><source>Phys. Stat. Mech. Its Appl.</source><year>2018</year><volume>506</volume><fpage>566</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1016/j.physa.2018.04.073</pub-id></element-citation></ref><ref id="B9-sensors-25-01114"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>W.</given-names></name>
<name><surname>Wei</surname><given-names>J.</given-names></name>
<name><surname>Dolan</surname><given-names>J.M.</given-names></name>
<name><surname>Zhao</surname><given-names>H.</given-names></name>
<name><surname>Zha</surname><given-names>H.</given-names></name>
</person-group><article-title>A real-time motion planner with trajectory optimization for autonomous vehicles</article-title><source>Proceedings of the 2012 IEEE International Conference on Robotics and Automation</source><conf-loc>St. Paul, MN, USA</conf-loc><conf-date>14&#x02013;18 May 2012</conf-date><fpage>2061</fpage><lpage>2067</lpage></element-citation></ref><ref id="B10-sensors-25-01114"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ma</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>X.</given-names></name>
<name><surname>Yu</surname><given-names>G.</given-names></name>
<name><surname>Xu</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Pedestrian Detection and Tracking from Low-Resolution Unmanned Aerial Vehicle Thermal Imagery</article-title><source>Sensors</source><year>2016</year><volume>16</volume><elocation-id>446</elocation-id><pub-id pub-id-type="doi">10.3390/s16040446</pub-id><pub-id pub-id-type="pmid">27023564</pub-id>
</element-citation></ref><ref id="B11-sensors-25-01114"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Lu</surname><given-names>C.</given-names></name>
<name><surname>Luo</surname><given-names>Z.</given-names></name>
<name><surname>Xue</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>C.</given-names></name>
</person-group><article-title>LiDAR-Video Driving Dataset: Learning Driving Policies Effectively</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#x02013;22 June 2018</conf-date><fpage>5870</fpage><lpage>5878</lpage></element-citation></ref><ref id="B12-sensors-25-01114"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kan</surname><given-names>Z.</given-names></name>
<name><surname>Tang</surname><given-names>L.</given-names></name>
<name><surname>Kwan</surname><given-names>M.-P.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
</person-group><article-title>Estimating Vehicle Fuel Consumption and Emissions Using GPS Big Data</article-title><source>Int. J. Environ. Res. Public. Health</source><year>2018</year><volume>15</volume><elocation-id>566</elocation-id><pub-id pub-id-type="doi">10.3390/ijerph15040566</pub-id><pub-id pub-id-type="pmid">29561813</pub-id>
</element-citation></ref><ref id="B13-sensors-25-01114"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shan</surname><given-names>X.</given-names></name>
<name><surname>Hao</surname><given-names>P.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Boriboonsomsin</surname><given-names>K.</given-names></name>
<name><surname>Wu</surname><given-names>G.</given-names></name>
<name><surname>Barth</surname><given-names>M.J.</given-names></name>
</person-group><article-title>Vehicle Energy/Emissions Estimation Based on Vehicle Trajectory Reconstruction Using Sparse Mobile Sensor Data</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2019</year><volume>20</volume><fpage>716</fpage><lpage>726</lpage><pub-id pub-id-type="doi">10.1109/TITS.2018.2826571</pub-id></element-citation></ref><ref id="B14-sensors-25-01114"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Saldivar-Carranza</surname><given-names>E.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Mathew</surname><given-names>J.</given-names></name>
<name><surname>Hunter</surname><given-names>M.</given-names></name>
<name><surname>Sturdevant</surname><given-names>J.</given-names></name>
<name><surname>Bullock</surname><given-names>D.M.</given-names></name>
</person-group><article-title>Deriving Operational Traffic Signal Performance Measures from Vehicle Trajectory Data</article-title><source>Transp. Res. Rec.</source><year>2021</year><volume>2675</volume><fpage>1250</fpage><lpage>1264</lpage><pub-id pub-id-type="doi">10.1177/03611981211006725</pub-id></element-citation></ref><ref id="B15-sensors-25-01114"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jim&#x000e9;nez</surname><given-names>F.</given-names></name>
<name><surname>Naranjo</surname><given-names>J.E.</given-names></name>
<name><surname>Anaya</surname><given-names>J.J.</given-names></name>
<name><surname>Garc&#x000ed;a</surname><given-names>F.</given-names></name>
<name><surname>Ponz</surname><given-names>A.</given-names></name>
<name><surname>Armingol</surname><given-names>J.M.</given-names></name>
</person-group><article-title>Advanced Driver Assistance System for Road Environments to Improve Safety and Efficiency</article-title><source>Transp. Res. Procedia</source><year>2016</year><volume>14</volume><fpage>2245</fpage><lpage>2254</lpage><pub-id pub-id-type="doi">10.1016/j.trpro.2016.05.240</pub-id></element-citation></ref><ref id="B16-sensors-25-01114"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pierzcha&#x00142;a</surname><given-names>M.</given-names></name>
<name><surname>Gigu&#x000e8;re</surname><given-names>P.</given-names></name>
<name><surname>Astrup</surname><given-names>R.</given-names></name>
</person-group><article-title>Mapping forests using an unmanned ground vehicle with 3D LiDAR and graph-SLAM</article-title><source>Comput. Electron. Agric.</source><year>2018</year><volume>145</volume><fpage>217</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2017.12.034</pub-id></element-citation></ref><ref id="B17-sensors-25-01114"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Das</surname><given-names>R.D.</given-names></name>
<name><surname>Winter</surname><given-names>S.</given-names></name>
</person-group><article-title>Automated Urban Travel Interpretation: A Bottom-up Approach for Trajectory Segmentation</article-title><source>Sensors</source><year>2016</year><volume>16</volume><elocation-id>1962</elocation-id><pub-id pub-id-type="doi">10.3390/s16111962</pub-id><pub-id pub-id-type="pmid">27886053</pub-id>
</element-citation></ref><ref id="B18-sensors-25-01114"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Patoliya</surname><given-names>J.</given-names></name>
<name><surname>Mewada</surname><given-names>H.</given-names></name>
<name><surname>Hassaballah</surname><given-names>M.</given-names></name>
<name><surname>Khan</surname><given-names>M.A.</given-names></name>
<name><surname>Kadry</surname><given-names>S.</given-names></name>
</person-group><article-title>A robust autonomous navigation and mapping system based on GPS and LiDAR data for unconstraint environment</article-title><source>Earth Sci. Inform.</source><year>2022</year><volume>15</volume><fpage>2703</fpage><lpage>2715</lpage><pub-id pub-id-type="doi">10.1007/s12145-022-00791-x</pub-id></element-citation></ref><ref id="B19-sensors-25-01114"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Das</surname><given-names>R.D.</given-names></name>
<name><surname>Winter</surname><given-names>S.</given-names></name>
</person-group><article-title>A context-sensitive conceptual framework for activity modeling</article-title><source>J. Spat. Inf. Sci.</source><year>2016</year><volume>2016</volume><fpage>45</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.5311/JOSIS.2016.12.260</pub-id></element-citation></ref><ref id="B20-sensors-25-01114"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schneider</surname><given-names>F.D.</given-names></name>
<name><surname>K&#x000fc;kenbrink</surname><given-names>D.</given-names></name>
<name><surname>Schaepman</surname><given-names>M.E.</given-names></name>
<name><surname>Schimel</surname><given-names>D.S.</given-names></name>
<name><surname>Morsdorf</surname><given-names>F.</given-names></name>
</person-group><article-title>Quantifying 3D structure and occlusion in dense tropical and temperate forests using close-range LiDAR</article-title><source>Agric. For. Meteorol.</source><year>2019</year><volume>268</volume><fpage>249</fpage><lpage>257</lpage><pub-id pub-id-type="doi">10.1016/j.agrformet.2019.01.033</pub-id></element-citation></ref><ref id="B21-sensors-25-01114"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gilroy</surname><given-names>S.</given-names></name>
<name><surname>Jones</surname><given-names>E.</given-names></name>
<name><surname>Glavin</surname><given-names>M.</given-names></name>
</person-group><article-title>Overcoming Occlusion in the Automotive Environment&#x02014;A Review</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2021</year><volume>22</volume><fpage>23</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1109/TITS.2019.2956813</pub-id></element-citation></ref><ref id="B22-sensors-25-01114"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Schneider</surname><given-names>S.</given-names></name>
<name><surname>Himmelsbach</surname><given-names>M.</given-names></name>
<name><surname>Luettel</surname><given-names>T.</given-names></name>
<name><surname>Wuensche</surname><given-names>H.-J.</given-names></name>
</person-group><article-title>Fusing vision and LIDAR&#x02014;Synchronization, correction and occlusion reasoning</article-title><source>Proceedings of the 2010 IEEE Intelligent Vehicles Symposium</source><conf-loc>La Jolla, CA, USA</conf-loc><conf-date>21&#x02013;24 June 2010</conf-date><fpage>388</fpage><lpage>393</lpage></element-citation></ref><ref id="B23-sensors-25-01114"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Ibanez-Guzman</surname><given-names>J.</given-names></name>
</person-group><article-title>Lidar for Autonomous Driving: The Principles, Challenges, and Trends for Automotive Lidar and Perception Systems</article-title><source>IEEE Signal Process. Mag.</source><year>2020</year><volume>37</volume><fpage>50</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1109/MSP.2020.2973615</pub-id></element-citation></ref><ref id="B24-sensors-25-01114"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Gong</surname><given-names>B.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
</person-group><article-title>Vehicle detection and tracking using low-channel roadside LiDAR</article-title><source>Measurement</source><year>2023</year><volume>218</volume><fpage>113159</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2023.113159</pub-id></element-citation></ref><ref id="B25-sensors-25-01114"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>X.</given-names></name>
<name><surname>Pi</surname><given-names>R.</given-names></name>
<name><surname>Lv</surname><given-names>C.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Zheng</surname><given-names>H.</given-names></name>
<name><surname>Jiang</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>H.</given-names></name>
</person-group><article-title>Augmented Multiple Vehicles&#x02019; Trajectories Extraction Under Occlusions with Roadside LiDAR Data</article-title><source>IEEE Sens. J.</source><year>2021</year><volume>21</volume><fpage>21921</fpage><lpage>21930</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2021.3079257</pub-id></element-citation></ref><ref id="B26-sensors-25-01114"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Xu</surname><given-names>H.</given-names></name>
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Zheng</surname><given-names>J.</given-names></name>
<name><surname>Yue</surname><given-names>R.</given-names></name>
</person-group><article-title>Automatic Background Filtering Method for Roadside LiDAR Data</article-title><source>Transp. Res. Rec.</source><year>2018</year><volume>2672</volume><fpage>106</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1177/0361198118775841</pub-id></element-citation></ref><ref id="B27-sensors-25-01114"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Bhattarai</surname><given-names>N.</given-names></name>
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Xu</surname><given-names>H.</given-names></name>
</person-group><article-title>An unsupervised clustering method for processing roadside LiDAR data with improved computational efficiency</article-title><source>IEEE Sens. J.</source><year>2022</year><volume>22</volume><fpage>10684</fpage><lpage>10691</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2022.3166957</pub-id></element-citation></ref><ref id="B28-sensors-25-01114"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Xiao</surname><given-names>W.</given-names></name>
<name><surname>Mills</surname><given-names>J.P.</given-names></name>
</person-group><article-title>Optimizing Moving Object Trajectories from Roadside Lidar Data by Joint Detection and Tracking</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>2124</elocation-id><pub-id pub-id-type="doi">10.3390/rs14092124</pub-id></element-citation></ref><ref id="B29-sensors-25-01114"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Blackman</surname><given-names>S.S.</given-names></name>
</person-group><article-title>Multiple hypothesis tracking for multiple target tracking</article-title><source>IEEE Aerosp. Electron. Syst. Mag.</source><year>2004</year><volume>19</volume><fpage>5</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1109/MAES.2004.1263228</pub-id></element-citation></ref><ref id="B30-sensors-25-01114"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mei</surname><given-names>W.</given-names></name>
<name><surname>Xiong</surname><given-names>G.</given-names></name>
<name><surname>Gong</surname><given-names>J.</given-names></name>
<name><surname>Yong</surname><given-names>Z.</given-names></name>
<name><surname>Chen</surname><given-names>H.</given-names></name>
<name><surname>Di</surname><given-names>H.</given-names></name>
</person-group><article-title>Multiple moving target tracking with hypothesis trajectory model for autonomous vehicles</article-title><source>Proceedings of the 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>16&#x02013;19 October 2017</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B31-sensors-25-01114"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Tian</surname><given-names>S.</given-names></name>
<name><surname>Xu</surname><given-names>H.</given-names></name>
<name><surname>Yue</surname><given-names>R.</given-names></name>
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Cui</surname><given-names>Y.</given-names></name>
</person-group><article-title>Architecture of Vehicle Trajectories Extraction with Roadside LiDAR Serving Connected Vehicles</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>100406</fpage><lpage>100415</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2929795</pub-id></element-citation></ref><ref id="B32-sensors-25-01114"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Xu</surname><given-names>H.</given-names></name>
<name><surname>Zheng</surname><given-names>J.</given-names></name>
</person-group><article-title>Automatic background filtering and lane identification with roadside LiDAR data</article-title><source>Proceedings of the 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>16&#x02013;19 October 2017</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B33-sensors-25-01114"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Xu</surname><given-names>H.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Zheng</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>D.</given-names></name>
</person-group><article-title>Detection and tracking of pedestrians and vehicles using roadside LiDAR sensors</article-title><source>Transp. Res. Part C Emerg. Technol.</source><year>2019</year><volume>100</volume><fpage>68</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1016/j.trc.2019.01.007</pub-id></element-citation></ref><ref id="B34-sensors-25-01114"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Xiao</surname><given-names>W.</given-names></name>
<name><surname>Coifman</surname><given-names>B.</given-names></name>
<name><surname>Mills</surname><given-names>J.P.</given-names></name>
</person-group><article-title>Vehicle Tracking and Speed Estimation from Roadside Lidar</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2020</year><volume>13</volume><fpage>5597</fpage><lpage>5608</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2020.3024921</pub-id></element-citation></ref><ref id="B35-sensors-25-01114"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>T.</given-names></name>
<name><surname>Park</surname><given-names>T.-H.</given-names></name>
</person-group><article-title>Extended Kalman Filter (EKF) Design for Vehicle Position Tracking Using Reliability Function of Radar and Lidar</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>4126</elocation-id><pub-id pub-id-type="doi">10.3390/s20154126</pub-id><pub-id pub-id-type="pmid">32722239</pub-id>
</element-citation></ref><ref id="B36-sensors-25-01114"><label>36.</label><element-citation publication-type="webpage"><article-title>On Using a Low-Density Flash Lidar for Road Vehicle Tracking|ASME Digital Collection</article-title><comment>Available online: <ext-link xlink:href="https://www.researchgate.net/publication/349465522_On_Using_a_Low-Density_Flash_Lidar_for_Road_Vehicle_Tracking" ext-link-type="uri">https://www.researchgate.net/publication/349465522_On_Using_a_Low-Density_Flash_Lidar_for_Road_Vehicle_Tracking</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-22">(accessed on 22 December 2024)</date-in-citation></element-citation></ref><ref id="B37-sensors-25-01114"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>J.</given-names></name>
</person-group><article-title>An Automatic Procedure for Vehicle Tracking with a Roadside LiDAR Sensor</article-title><source>Inst. Transp. Eng. ITE J.</source><year>2018</year><volume>88</volume><fpage>32</fpage><lpage>37</lpage></element-citation></ref><ref id="B38-sensors-25-01114"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Xu</surname><given-names>H.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Zheng</surname><given-names>J.</given-names></name>
<name><surname>Dietrich</surname><given-names>K.M.</given-names></name>
</person-group><article-title>3-D Data Processing to Extract Vehicle Trajectories from Roadside LiDAR Data</article-title><source>Transp. Res. Rec.</source><year>2018</year><volume>2672</volume><fpage>14</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1177/0361198118775839</pub-id></element-citation></ref><ref id="B39-sensors-25-01114"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>C.</given-names></name>
<name><surname>Guo</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Wu</surname><given-names>D.</given-names></name>
</person-group><article-title>An Automatic Lane Marking Detection Method with Low-Density Roadside LiDAR Data</article-title><source>IEEE Sens. J.</source><year>2021</year><volume>21</volume><fpage>10029</fpage><lpage>10038</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2021.3057999</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01114-f001"><label>Figure 1</label><caption><p>Workflow of the methodology.</p></caption><graphic xlink:href="sensors-25-01114-g001" position="float"/></fig><fig position="float" id="sensors-25-01114-f002"><label>Figure 2</label><caption><p>Full occlusion issue.</p></caption><graphic xlink:href="sensors-25-01114-g002" position="float"/></fig><fig position="float" id="sensors-25-01114-f003"><label>Figure 3</label><caption><p>(<bold>a</bold>) The discontinuous trajectory reconnection. (<bold>b</bold>&#x02013;<bold>d</bold>) Partial occlusion of different datapoints.</p></caption><graphic xlink:href="sensors-25-01114-g003" position="float"/></fig><fig position="float" id="sensors-25-01114-f004"><label>Figure 4</label><caption><p>(<bold>a</bold>) Area of study. (<bold>b</bold>) A VLP-32 LiDAR sensor mounted on a traffic signal pole.</p></caption><graphic xlink:href="sensors-25-01114-g004" position="float"/></fig><fig position="float" id="sensors-25-01114-f005"><label>Figure 5</label><caption><p>A vehicle trajectory connection (<bold>a</bold>) after the full occlusion (<bold>b</bold>) during the full occlusion (<bold>c</bold>) comparison before and after the full occlusion (<bold>d</bold>) before the full occlusion.</p></caption><graphic xlink:href="sensors-25-01114-g005" position="float"/></fig><fig position="float" id="sensors-25-01114-f006"><label>Figure 6</label><caption><p>Vehicle representative repair for (<bold>a</bold>&#x02013;<bold>f</bold>) frames 292, 294, 296, 317, 325, and 349.</p></caption><graphic xlink:href="sensors-25-01114-g006" position="float"/></fig><fig position="float" id="sensors-25-01114-f007"><label>Figure 7</label><caption><p>Different representative point location adjustments.</p></caption><graphic xlink:href="sensors-25-01114-g007" position="float"/></fig><fig position="float" id="sensors-25-01114-f008"><label>Figure 8</label><caption><p>Velocity comparison with different representative point methods.</p></caption><graphic xlink:href="sensors-25-01114-g008" position="float"/></fig><fig position="float" id="sensors-25-01114-f009"><label>Figure 9</label><caption><p>Multiple vehicle trajectory Reappear for prolonged full occlusion.</p></caption><graphic xlink:href="sensors-25-01114-g009" position="float"/></fig><fig position="float" id="sensors-25-01114-f010"><label>Figure 10</label><caption><p>LiDAR effective detection range increase.</p></caption><graphic xlink:href="sensors-25-01114-g010" position="float"/></fig><table-wrap position="float" id="sensors-25-01114-t001"><object-id pub-id-type="pii">sensors-25-01114-t001_Table 1</object-id><label>Table 1</label><caption><p>Data processing results with different methods for short discontinuous trajectories.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">NDLS</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Percentage of Fixed Occlusion with Zhao [<xref rid="B33-sensors-25-01114" ref-type="bibr">33</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Percentage of Fixed Occlusion with Song [<xref rid="B25-sensors-25-01114" ref-type="bibr">25</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Percentage of Fixed Occlusion with the Proposed Method </th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Straight Road</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.48%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.70%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.57%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Curved Road</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.50%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.14%</td></tr></tbody></table><table-wrap-foot><fn><p>NDLS: number of detected discontinuous trajectories less than 0.5 s.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-01114-t002"><object-id pub-id-type="pii">sensors-25-01114-t002_Table 2</object-id><label>Table 2</label><caption><p>Data processing results with different methods for long discontinuous trajectories.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">NDGS</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Percentage of Fixed Occlusion with Zhao [<xref rid="B33-sensors-25-01114" ref-type="bibr">33</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Percentage of Fixed Occlusion with Song [<xref rid="B25-sensors-25-01114" ref-type="bibr">25</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Percentage of Fixed Occlusion with the Proposed Method </th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Straight Road</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.22%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.44%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.11%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Curved Road</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30.30%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.82%</td></tr></tbody></table><table-wrap-foot><fn><p>NDGS: number of detected discontinuous trajectories greater than 1.0 s.</p></fn></table-wrap-foot></table-wrap></floats-group></article>