<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006410</article-id><article-id pub-id-type="pmc">PMC11859322</article-id><article-id pub-id-type="doi">10.3390/s25041178</article-id><article-id pub-id-type="publisher-id">sensors-25-01178</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Vision-Based Localization in Urban Areas for Mobile Robots</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0909-2047</contrib-id><name><surname>Alimovski</surname><given-names>Erdal</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01178" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4095-6333</contrib-id><name><surname>Erdemir</surname><given-names>Gokhan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af2-sensors-25-01178" ref-type="aff">2</xref><xref rid="c1-sensors-25-01178" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7769-6451</contrib-id><name><surname>Kuzucuoglu</surname><given-names>Ahmet Emin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af3-sensors-25-01178" ref-type="aff">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Sand</surname><given-names>Stephan</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01178"><label>1</label>Computer Engineering Department, Istanbul Sabahattin Zaim University, 34303 Istanbul, T&#x000fc;rkiye; <email>erdal.alimovski@izu.edu.tr</email></aff><aff id="af2-sensors-25-01178"><label>2</label>Department of Engineering Management and Technology, University of Tennessee at Chattanooga, Chattanooga, TN 37405, USA</aff><aff id="af3-sensors-25-01178"><label>3</label>Department of Electrical and Electronics Engineering, Faculty of Technology, Marmara University, 34722 Istanbul, T&#x000fc;rkiye; <email>kuzucuoglu@marmara.edu.tr</email></aff><author-notes><corresp id="c1-sensors-25-01178"><label>*</label>Correspondence: <email>gokhan-erdemir@utc.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>14</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1178</elocation-id><history><date date-type="received"><day>16</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>05</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>07</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Robust autonomous navigation systems rely on mapping, locomotion, path planning, and localization factors. Localization, one of the most essential factors of navigation, is a crucial requirement for a mobile robot because it needs the capability to localize itself in the environment. Global Positioning Systems (GPSs) are commonly used for outdoor mobile robot localization tasks. However, various environmental circumstances, such as high-rise buildings and trees, affect GPS signal quality, which leads to reduced precision or complete signal blockage. This study proposes a visual-based localization system for outdoor mobile robots in crowded urban environments. The proposed system comprises three steps. The first step is to detect the text in urban areas using the &#x0201c;Efficient and Accurate Scene Text Detector (EAST)&#x0201d; algorithm. Then, EasyOCR was applied to the detected text for the recognition phase to extract text from images that were obtained from EAST. The results from text detection and recognition algorithms were enhanced by applying post-processing and word similarity algorithms. In the second step, once the text detection and recognition process is completed, the recognized word (label/tag) is sent to the Places API in order to return the recognized word&#x02019;s coordinates that are passed within the specified radius. Parallely, points of interest (POI) data are collected for a defined area by a certain radius while the robot has an accurate internet connection. The proposed system was tested in three distinct urban areas by creating five scenarios under different lighting conditions, such as morning and evening, using the outdoor delivery robot utilized in this study. In the case studies, it has been shown that the proposed system provides a low error of around 4 m for localization tasks. Compared to existing works, the proposed system consistently outperforms all other approaches using just one sensor. The results indicate the efficacy of the proposed system for localization tasks in environments where GPS signals are limited or completely blocked.</p></abstract><kwd-group><kwd>visual localization</kwd><kwd>mobile robot</kwd><kwd>text recognition</kwd><kwd>map</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01178"><title>1. Introduction</title><p>In recent years, outdoor mobile robots have become crucial due to their versatility and extensive range of applications. Outdoor mobile robots are employed for various tasks, including delivery [<xref rid="B1-sensors-25-01178" ref-type="bibr">1</xref>], planting and harvesting in agriculture [<xref rid="B2-sensors-25-01178" ref-type="bibr">2</xref>], security and surveillance [<xref rid="B3-sensors-25-01178" ref-type="bibr">3</xref>], and maintenance of the supporting infrastructure [<xref rid="B4-sensors-25-01178" ref-type="bibr">4</xref>]. Navigation is the most critical issue for mobile robots in accomplishing their assigned tasks successfully [<xref rid="B5-sensors-25-01178" ref-type="bibr">5</xref>]. The success of navigation systems is based on factors such as location, mapping, path planning, and locomotion. Localization refers to determining the robot&#x02019;s position based on its environment, a previous point, or a given map. It can be performed incrementally, where the position is tracked over time and changed by the robot&#x02019;s motions, or globally, where the pose is computed just once based on preliminary observations [<xref rid="B6-sensors-25-01178" ref-type="bibr">6</xref>]. Numerous localization techniques have recently been proposed and developed for various applications in indoor and outdoor environments. For outdoor environments, Global Positioning Systems (GPSs), a satellite-based navigation system, forms a crucial part of Global Navigation Satellite Systems (GNSSs), which are widely utilized methods in the literature in various outdoor applications to determine the precise locations of mobile robots.</p><p>Despite its widespread implementation, localization through GPS might face some problems, such as accuracy problems or signal loss due to various environmental factors. Common reasons for accuracy or signal loss, which is illustrated in <xref rid="sensors-25-01178-f001" ref-type="fig">Figure 1</xref>, according to official US government information on GPS and related subjects [<xref rid="B7-sensors-25-01178" ref-type="bibr">7</xref>] are (1) blockage of satellite signals by significant environmental factors such as skyscrapers, buildings, bridges, and trees; (2) reflection of signals from walls or buildings; and (3) both indoor and underground use. Depending on these factors, signal reception between the receiver and the satellite plays an important role. There are three forms of reception of GPS signal: line of sight (LOS), multipath, and non-line of sight (NLOS) [<xref rid="B8-sensors-25-01178" ref-type="bibr">8</xref>]. In the LOS type, there are no obstructions or signal disturbances in the direct signal line between the satellite and the GPS receiver. This type of signal reception is ideal for obtaining the most reliable and accurate GPS signals. Conversely, in multipath reception types, the GPS receiver receives the direct LOS signals and the reflected signals from surrounding objects such as buildings, trees, and bridges. These reflected signals can negatively interfere with the accuracy of the location data received by the receiver. In the context of GPS signal reception types, the signal can be blocked or less reliable in an NLOS scenario, where the receiver can only receive the reflected signals. NLOS scenarios frequently occur in cities where towering structures surround a GPS receiver. Comprehending the various types of GPS signal reception is crucial to understanding the difficulties in achieving accuracy and dependability, especially in urban environments where the direct LOS connection between satellites and GPS receivers is blocked. The challenges in NLOS scenarios have an enormous effect on robotics, affecting the capability of robots to navigate, perform the given tasks, and move effectively in environments where direct GPS signal reception is blocked. Developing and implementing sensor fusion systems, such as adding more sensors, like cameras, lidars, or inertial measurement units, is necessary to overcome these challenges. For example, using camera sensors, environmental information can be sensed, such as street labels, store names, and other contextual information, which can later, with a few processes, obtain the robot&#x02019;s actual position. Thus, it can significantly help overcome challenges associated with blocked GPS signals in urban areas.</p><p>Given the significant limitations of GPS signals in urban environments, such as signal blockage and accuracy issues caused by LOS, multipath, and NLOS scenarios, it is imperative to develop alternative systems that do not rely solely on GPS. Advanced localization methods, such as sensor fusion and leveraging environmental data, are essential to ensure reliable navigation and positioning for mobile robots, especially in challenging outdoor scenarios.</p><p>In urban areas, the perception of environmental information and scene texts, such as names of streets, stores, restaurants, etc., is a crucial feature for not only mobile robot navigation but also intelligent traffic systems, visual assistance, and other applications [<xref rid="B9-sensors-25-01178" ref-type="bibr">9</xref>]. If adequately processed, such data can provide essential contextual cues for that environment or place. However, real-time text detection and recognition in urban areas pose significant challenges due to natural environmental factors such as lighting, obstructions, weather conditions, shooting angles, and considerable variability in scene characteristics in terms of text size, color, and background type [<xref rid="B10-sensors-25-01178" ref-type="bibr">10</xref>]. Text detection, a prerequisite for text recognition, is a critical phase in extracting and understanding textual information. There are two types of methods for text detection in literature: traditional and deep learning-based methods. Conventional methods rely on manually designed features to capture the characteristics of scene text [<xref rid="B11-sensors-25-01178" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01178" ref-type="bibr">12</xref>], whereas deep learning-based techniques extract features entirely from training data [<xref rid="B13-sensors-25-01178" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01178" ref-type="bibr">14</xref>]. When text is accurately detected in real-time scenarios, it provides a clearer and more refined input to the Optical Character Recognition (OCR) system, resulting in higher recognition accuracy. OCR technology excels in recognizing documents with consistent background colors, basic fonts, and well-aligned text. However, the performance of scene text recognition, such as street names, store names, restaurant names, etc., is limited due to the complex backgrounds, distinctive and distorted fonts, uneven illumination, and color variations [<xref rid="B15-sensors-25-01178" ref-type="bibr">15</xref>]. Obtaining and integrating this data type with other fields, such as location-based web mining, augmented reality, and geo-tagging, will facilitate various applications, including mobile robot localization.</p><p>Multi-sensor fusion systems significantly enhance navigation accuracy by addressing the limitations of single-sensor systems. These systems integrate various sensors, such as laser sensors, cameras, and sonar, to provide robust and reliable navigation solutions [<xref rid="B16-sensors-25-01178" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01178" ref-type="bibr">17</xref>]. Sensor fusion enables robots to operate effectively in both structured and unstructured environments. Techniques like Space and Time Sensor Fusion (STSF) utilize temporal data sequences to improve measurement accuracy without requiring additional sensors [<xref rid="B18-sensors-25-01178" ref-type="bibr">18</xref>]. Robots can achieve real-time obstacle avoidance by combining data from vision and sonar sensors, allowing them to navigate efficiently in unknown and dynamic environments [<xref rid="B19-sensors-25-01178" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01178" ref-type="bibr">20</xref>]. However, implementing sensor fusion systems is complex and costly due to the need for integrating multiple sensors and developing sophisticated algorithms for effective data processing [<xref rid="B18-sensors-25-01178" ref-type="bibr">18</xref>,<xref rid="B20-sensors-25-01178" ref-type="bibr">20</xref>]. While sensor fusion significantly improves navigation, challenges remain in ensuring reliability and robustness, particularly in highly dynamic or poorly lit environments. Continued research is essential to address these limitations and enhance the adaptability of these systems [<xref rid="B21-sensors-25-01178" ref-type="bibr">21</xref>].</p><p>Previous studies focused on localization in urban environments have been valuable in shedding light on effective technologies and solutions. However, they have certain limitations. Most previous studies in outdoor localization, particularly in scenarios where GPS signals are denied, commonly employ multiple sensors or sensor fusion techniques. Additionally, despite implementing diverse techniques, the challenge of high localization error persists in urban environments for robots. The primary objective of this study is to develop and implement a visual-based localization system for mobile robots in urban environments, particularly for areas where GPS signals are not accurate or totally denied. In addition, to represent the coordinates from the proposed system and GPS module, we developed a web-based map utilizing the Mapbox tool. The flowchart of the proposed system is demonstrated in <xref rid="sensors-25-01178-f002" ref-type="fig">Figure 2</xref>. Among the noteworthy contributions are:<list list-type="bullet"><list-item><p>Optimize the EAST algorithm by utilizing a post-processing approach.</p></list-item><list-item><p>Enhancement of the OCR algorithm by combining it with a word similarity algorithm for improved recognition accuracy and making it suitable for real-time applications.</p></list-item><list-item><p>Develop a map that can operate offline and integrate it with the proposed system to display generated GPS coordinates in real-time.</p></list-item><list-item><p>Obtaining a comprehensive and up-to-date point of interest (POI) database to achieve more accurate and reliable localization. Moreover, the POI data can be utilized for various applications in specific areas.</p></list-item><list-item><p>Verify the proposed system&#x02019;s reliable and simultaneous operation in real-time cases where GPS signals cannot be received or are weak in urban areas with high-rise buildings.</p></list-item><list-item><p>The performance analysis of the proposed system was tested in real-time utilizing the eight-wheeled delivery robot by creating five different scenarios in different crowded urban environments such as crowded streets, pedestrian streets, and commercial districts.</p></list-item><list-item><p>Analyzing the performance of the proposed system in different light conditions by performing experiments in both morning and evening hours.</p></list-item><list-item><p>In contrast to prior research, which employed costly sensors or sensor fusion, in our study, localization was accomplished using only a basic webcam. This increases the usability of the proposed system.</p></list-item><list-item><p>In contrast to previous studies that omitted the presentation of localization time for their systems, we showcase the localization timing of the proposed system.</p></list-item></list></p><p>This paper is organized as follows. <xref rid="sec2-sensors-25-01178" ref-type="sec">Section 2</xref> presents the related works on outdoor mobile robot navigation. <xref rid="sec3-sensors-25-01178" ref-type="sec">Section 3</xref> explains the methods of the overall system, while <xref rid="sec4-sensors-25-01178" ref-type="sec">Section 4</xref> presents validation of the proposed approach on the mobile robot for real-world applications. This paper is concluded in <xref rid="sec5-sensors-25-01178" ref-type="sec">Section 5</xref>.</p></sec><sec id="sec2-sensors-25-01178"><title>2. Related Works</title><p>Localization is an essential requirement that allows the robot to determine its location, as well as features such as map creation, optimal route planning, etc., [<xref rid="B6-sensors-25-01178" ref-type="bibr">6</xref>]. In general, the localization can be accomplished with the aid of GPS signals, which provide a global position, or by onboard sensors. However, GPS localization reliability is insufficient, especially in urban environments where buildings may obstruct or reflect satellite signals. To overcome these limitations, many different approaches have been proposed to solve the localization problem. Encompassing approaches based on vision, light detection, range (LiDAR), inertial measurement unit (IMU), etc.</p><p>In [<xref rid="B22-sensors-25-01178" ref-type="bibr">22</xref>], a visual localization method based on place recognition is introduced. This method realizes localization by recognizing recently visited places through the utilization of sequence-matching techniques. It operates by comparing query image sequences with a previously acquired image database. Moreover, a global GIST descriptor and a local binary feature CSLBP (center-symmetric local binary pattern) are combined to create a multi-feature to improve the matching accuracy. Finally, a Chi-square distance similarity measurement is employed for efficient sequence matching. The proposed system was evaluated on the Nordland dataset [<xref rid="B23-sensors-25-01178" ref-type="bibr">23</xref>]. The dataset includes video footage capturing a 728 km long train ride between two cities in northern Norway. Precision and recall metrics are performed to evaluate the proposed method. The results show that the proposed system achieves more than 87% recall for spring&#x02013;summer and spring&#x02013;fall situations. The localization results also show that the suggested method can localize at least 60% of the test data. Experiments were conducted using a simulation; the paper does not include real-time localization experiments by the proposed system [<xref rid="B22-sensors-25-01178" ref-type="bibr">22</xref>].</p><p>For accurate robot localization in challenging outdoor environments, [<xref rid="B24-sensors-25-01178" ref-type="bibr">24</xref>] applied the Pozyx Algorithm, a hardware-based Ultra-Wideband system. The EKF was performed additionally. Thus, the output from the Pozyx algorithm was fed to EKF to fuse odometry with the Pozyx Range measurements. The experiments were carried out with AGROBv.16, a cost-effective outdoor robot. In [<xref rid="B24-sensors-25-01178" ref-type="bibr">24</xref>], since the obtained GPS data were not accurate and dependable, the authors used a Laser Scan as the ground truth to analyze the results. From the experiments, it can be concluded that the proposed EKF implementation presents better results than the Pozyx algorithm. However, it requires careful tuning to ensure proper convergence.</p><p>In [<xref rid="B25-sensors-25-01178" ref-type="bibr">25</xref>], a real-time CNN-based architecture was proposed, integrating data from low-cost sensors on a mobile robot with information derived from images captured by a single monocular camera. The system utilizes an EKF to execute precise relocalization of the robot in real-time. The proposed approach begins with the training of a CNN, which takes RGB images from the camera as input and applies regression for robot pose. Subsequently, the approach integrates the relocalization output obtained from the trained CNN into an EKF for robot localization. The system was tested in GPS-denied indoor and outdoor environments. The results show that the proposed CNN-EKF approach can generate accurate localization. However, the system faces difficulty providing accurate and simultaneous predictions for the robot&#x02019;s location in environments characterized by repetitive scenes or visual content.</p><p>In [<xref rid="B26-sensors-25-01178" ref-type="bibr">26</xref>], two approaches were proposed for localization: A Multi-Layer Perception Neural Network (MLPNN) for indoor localization and a sensor fusion-based approach for outdoor localization. In the sensor fusion system, the collected data from the inertial navigation system and GPS module are fused using recursive state estimation and a Kalman Filter. Then, the output from the Kalman Filter is combined with the odometer-based position data using a weighting scheme to estimate the robot&#x02019;s location. The results show that the proposed scheme can provide location in environments where GPS signals are available or not. However, the authors do not provide the system&#x02019;s accuracy or error rate. Additionally, the proposed system was evaluated in a simulation rather than a real environment.</p><p>To tackle the localization challenge, [<xref rid="B27-sensors-25-01178" ref-type="bibr">27</xref>] proposed a localization system that fuses the data obtained from GPS, IMU, and visual odometry. In addition, the system involves EKF. The proposed system was evaluated in two outdoor environments. The results obtained by the system were compared with the raw data from the GPS module to measure the system&#x02019;s robustness. Comparison results demonstrate that the proposed system exhibited an error rate of approximately 4 m, in contrast to the GPS module, which showed an error rate of about 79 m.</p><p>In [<xref rid="B28-sensors-25-01178" ref-type="bibr">28</xref>], a low-cost approach was proposed for mobile navigation robots in indoor and outdoor environments. The system integrates data from multiple sensors, particularly low-cost visual and inertial sensors. For outdoor environments, an Extended Kalman Filter (EKF) was employed alongside GPS, wheel encoders, and a Reduced Inertial Sensor System (RISS) to estimate the robot&#x02019;s position. Another EKF algorithm was introduced for indoor environments, utilizing a low-depth sensor called Microsoft Kinect Stream. Experimental results demonstrated that the proposed approach provides acceptable performance for real-time applications.</p><p>Based on deep learning and landmark detection, two methods were proposed to localize the mobile robot in outdoor environments [<xref rid="B29-sensors-25-01178" ref-type="bibr">29</xref>]. The first proposed method was based on Faster Regional Convolutional Neural Network (Faster R-CNN) landmark detection in the acquired image [<xref rid="B29-sensors-25-01178" ref-type="bibr">29</xref>], which obtains the robot&#x02019;s location coordinates and compass orientation by utilizing detected landmarks. The second method performs a single convolutional neural network (CNN) to predict the location and compass orientation from the entire image. The experiments were conducted in two outdoor areas. The experimental results show that the Faster R-CNN average distance error was 28 m, while the distance error of the CNN model was around 70 m.</p><p>In [<xref rid="B30-sensors-25-01178" ref-type="bibr">30</xref>], a visual localization approach was introduced, integrating depth and semantic information. The proposed approach employs semantic segmentation to capture a stable scene representation. It addresses variations in appearance between images caused by environmental changes by utilizing depth information obtained through depth prediction. The model was trained on the VKITTI 2 [<xref rid="B31-sensors-25-01178" ref-type="bibr">31</xref>] and KITTI [<xref rid="B32-sensors-25-01178" ref-type="bibr">32</xref>] datasets and subsequently evaluated on the Extended CMU Seasons and RobotCar Seasons datasets. The experimental results demonstrate that the proposed method exhibits remarkable performance in visual localization under various conditions, including weather, vegetation, regional environment, and illumination, based on evaluations conducted on the Extended CMU Seasons and RobotCar Seasons datasets.</p><p>In [<xref rid="B33-sensors-25-01178" ref-type="bibr">33</xref>], a global image descriptor was introduced to address the challenges of image-based localization in demanding scenarios such as cross-season, cross-weather, and day&#x02013;night conditions. The proposed descriptor can handle visual changes between images by learning the scene&#x02019;s geometry. The strength of the proposed method lies in the fact that it only requires geometric information during the learning process. The proposed method was tested on the Oxford Robotcar public dataset [<xref rid="B34-sensors-25-01178" ref-type="bibr">34</xref>] and the CMU Visual localization dataset [<xref rid="B35-sensors-25-01178" ref-type="bibr">35</xref>]. The proposed descriptor demonstrates remarkable performance in challenging cross-season localization scenarios, making it a valuable solution for long-term place recognition. Moreover, promising results are achieved in the context of night-to-day image retrieval.</p><p>In [<xref rid="B36-sensors-25-01178" ref-type="bibr">36</xref>], an end-to-end DL-based visual localization algorithm was proposed. Pre-processing tasks, including cropping, averaging, and timestamp alignment, are executed on datasets to minimize computational cost and time. Then, the processed dataset was fed to the proposed CNN-RNN-based model to identify the most impactful features for matching. Finally, the system predicts and provides output for the robot&#x02019;s current 3D translation and 4D angle information, thereby realizing a fully integrated end-to-end localization system. The proposed method was evaluated using three distinct datasets: the Cambridge Landmarks outdoor dataset [<xref rid="B37-sensors-25-01178" ref-type="bibr">37</xref>], the Microsoft 7-Scenes indoor dataset [<xref rid="B38-sensors-25-01178" ref-type="bibr">38</xref>], and the TUM Handheld SLAM dataset [<xref rid="B39-sensors-25-01178" ref-type="bibr">39</xref>]. During testing in the Cambridge Landmarks for outdoor environments, the proposed method exhibited promising results for localization. Nevertheless, there is currently a deficiency in implementing the proposed model for real-time applications in mobile robots.</p><p>In [<xref rid="B40-sensors-25-01178" ref-type="bibr">40</xref>], an innovative method for long-term robot localization using solely monocular image data is proposed. The proposed approach involves a unique data association technique to match incoming image streams with a stored image sequence in a database. Leveraging network flows, the method enhances localization performance by incorporating sequential information and maintaining multiple trajectory hypotheses simultaneously. Image comparison relies on a semi-dense description employing a histogram of oriented gradients&#x02019; features and global descriptors from deep convolutional neural networks trained on ImageNet, ensuring robust localization. The proposed approach demonstrates respectable performance in localization tasks through comprehensive evaluations across diverse datasets. The evaluation was carried out on datasets that were collected by driving through a city with a camera-equipped car during different seasons, including summer and winter.</p></sec><sec sec-type="methods" id="sec3-sensors-25-01178"><title>3. Methodology</title><p>This section explains the processes of text detection, recognition, word correction, web-based coordinate generation, map development, overall system integration, and the mobile robot platform in detail.</p><p>Firstly, hardware modifications were carried out on the mobile robot to align it with the objectives of this study and prepare it for experimental scenarios. After reviewing the literature and conducting experiments on text detection and recognition in urban areas, we implemented the EAST algorithm for text detection and EasyOCR for text recognition. To enhance the performance of the EAST algorithm, we applied Non-Maximum Suppression (NMS), which eliminates redundant bounding boxes and ensures more accurate text detection. Although EasyOCR is commonly used for documents, it has limitations in real-time applications, such as missing or incorrectly recognized characters, making it insufficient for standalone use. To address these issues, the Sequence Matcher algorithm was incorporated to correct errors in character recognition. This step significantly improved the performance of EasyOCR in real-time text recognition tasks. Another challenge in text recognition involved handling different fonts and variations in uppercase and lowercase letters. All POI data, mainly business names, are stored in lowercase for consistency. However, text recognition outputs often include mixed cases. All recognized text is converted to lowercase using the toLowerCase() function to ensure accurate searches and coordinate retrieval from the JSON file. To localize the mobile robot in scenarios where GPS signals are weak or unavailable, we obtained POI data for specific areas using the NS API. The retrieved POI data was stored in a JSON file. Once a business name is recognized, the algorithm searches the JSON file and retrieves the corresponding POI data. The Haversine formula is applied to calculate the nearest coordinates within a defined radius, returning the business name and its associated coordinates. These generated coordinates are displayed on a map as blue points, while coordinates obtained from the GPS module are shown as red points. We developed a web-based map using Mapbox to visualize and track the coordinates. The system-generated coordinates and the GPS module coordinates are displayed on the map for comparison. To evaluate the accuracy of the proposed system, the distance between the system-generated coordinates and the GPS coordinates is calculated using the Haversine formula. This comparison provides a visual and quantitative measure of the system&#x02019;s performance, with GPS coordinates displayed as red points and system-generated coordinates as blue points.</p><sec id="sec3dot1-sensors-25-01178"><title>3.1. Text Detection and Recognition</title><p>This part will explain the used sensors, text detection, post-processing, text recognition, and word correction processes.</p><sec id="sec3dot1dot1-sensors-25-01178"><title>3.1.1. Sensors</title><p>This study employs two sensors: a camera for visual perception and a GPS module for obtaining reference coordinates. We utilized the Logitech C922 [<xref rid="B41-sensors-25-01178" ref-type="bibr">41</xref>] for the camera. This camera captures videos at 1080p High Definition (HD) resolution with a 78-degree field of view and a frame rate of 30 frames per second. The Logitech C922 was selected for its numerous advantages, including high resolution, autofocus capability, lightweight design, and affordability.</p><p>For GPS modules, we tested the Quectel L80 and the Ublox Neo-M8 [<xref rid="B42-sensors-25-01178" ref-type="bibr">42</xref>]. To determine the most suitable module for the experiment, we measured their errors as follows: First, the actual coordinates of the test location were obtained from Google Maps. Then, the coordinates provided by each GPS module were recorded. Finally, the error for each module was calculated using the &#x0201c;measure distance&#x0201d; tool in Google Maps to compare the recorded coordinates with the actual location. The results indicated that the Ublox Neo-M8 GPS module had a lower error. Consequently, the Ublox Neo-M8 was chosen for this study.</p></sec><sec id="sec3dot1dot2-sensors-25-01178"><title>3.1.2. Scene Detection</title><p>Scene detection algorithms use image processing and computer vision techniques to detect text in complex scenes with high accuracy and efficiency. These algorithms facilitate detection and recognition in numerous fields, including security systems, logo and signage identification, traffic signal interpretation, etc. Based on the findings in [<xref rid="B43-sensors-25-01178" ref-type="bibr">43</xref>], the EAST algorithm outperformed other methods in terms of speed and accuracy, leading to its selection for use in this study. The EAST [<xref rid="B44-sensors-25-01178" ref-type="bibr">44</xref>] algorithm, introduced in 2017, was designed to overcome the challenges of scene detection in complex environments. It utilizes only one neural network to predict words or lines of text by passing the need for candidate aggregation and word segmentation. The model architecture is demonstrated in <xref rid="sensors-25-01178-f003" ref-type="fig">Figure 3</xref>.</p><p>EAST employs a convolutional neural network (CNN) as its backbone to extract feature maps from input images. The feature maps from various layers of the CNN backbone are merged to create a more detailed and spatially precise feature map. This is achieved through multiple branches and concatenation, which enhance the model&#x02019;s ability to detect text effectively in complex scenes. In the following equation:<disp-formula id="FD1-sensors-25-01178"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>F</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are feature maps from different layers of the backbone network. The model has two key output branches: score map and geometry map. The score map predicts the probability of each pixel belonging to text or non-text. It outputs a score map; the model has two key output branches: the score map and the geometry map. The score map estimates the likelihood of each pixel being part of text or non-text regions. The following equation represents this:<disp-formula id="FD2-sensors-25-01178"><label>(2)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>f</mml:mi><mml:mi>score</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo></mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>fused</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>fused</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the fused feature map at the position &#x0201c;(x,y)&#x0201d; and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>score</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> applies a transformation to predict the score, followed by a sigmoid activation function <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula> to produce the final probability. The geometry map predicts the geometry of the text boxes using two formats: RBOX and QUAD. The RBOX format is defined by 5 parameters, with 4 representing the corners of a rotated rectangle and 1 representing the rotation angle, while the QUAD format uses 8 parameters to describe the four vertices of a quadrilateral. The output of this branch is a geometry map represented by the following equation:<disp-formula id="FD3-sensors-25-01178"><label>(3)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>f</mml:mi><mml:mi>geo</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo></mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>fused</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>geo</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a convolutional layer that outputs geometry information.</p></sec><sec id="sec3dot1dot3-sensors-25-01178"><title>3.1.3. Post Processing</title><p>After the EAST model generated the score map and geometry map, we applied Non-Maximum Suppression (NMS) to remove redundant overlapping boxes, retaining only the most confident ones. The process begins by sorting all predicted boxes based on their confidence scores, selecting the box with the highest score, and suppressing other boxes that overlap significantly with it based on the Intersection over Union (IoU) metric. The IoU between two boxes, A and B, is calculated using the following equation:<disp-formula id="FD4-sensors-25-01178"><label>(4)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>IoU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x02229;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x0222a;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x02229;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the area of overlap between the two boxes and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x0222a;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the total area covered by both boxes. Boxes with an &#x0201c;IoU&#x0201d; above a predefined threshold are suppressed.</p></sec><sec id="sec3dot1dot4-sensors-25-01178"><title>3.1.4. Optical Character Recognition</title><p>Easy OCR is a highly efficient OCR library that supports over 80 languages, developed using Python and PyTorch. It employs a Convolutional Recurrent Neural Network (CRNN) [<xref rid="B45-sensors-25-01178" ref-type="bibr">45</xref>] for text recognition, consisting of three main components: ResNet [<xref rid="B46-sensors-25-01178" ref-type="bibr">46</xref>] for feature extraction, LSTM [<xref rid="B47-sensors-25-01178" ref-type="bibr">47</xref>] for sequence labeling, and Connectionist Temporal Classification (CTC) [<xref rid="B48-sensors-25-01178" ref-type="bibr">48</xref>] decoding. The workflow of Easy OCR is illustrated in <xref rid="sensors-25-01178-f004" ref-type="fig">Figure 4</xref>. This engine is particularly effective due to the advanced pre-processing steps included in its pipeline, making it one of the top OCR solutions available. Easy OCR was reconfigured to recognize Turkish text for our work, as our dataset is in Turkish. Once the text is detected and enclosed within a bounding box, it is processed by the OCR engine to recognize the text in the environment. The Easy OCR was used due to its performance, which is presented in [<xref rid="B49-sensors-25-01178" ref-type="bibr">49</xref>].</p></sec><sec id="sec3dot1dot5-sensors-25-01178"><title>3.1.5. Sequence Matcher</title><p>Sequence Matcher (SM) [<xref rid="B50-sensors-25-01178" ref-type="bibr">50</xref>] is a class of the &#x0201c;difflib&#x0201d; module used to compare the similarity of two given strings. The Ratcliff/Obershelp algorithm [<xref rid="B51-sensors-25-01178" ref-type="bibr">51</xref>] is run in the background. After comparing the two given strings, the algorithm returns a score between 0 and 1. After comparing two strings, if the obtained score is greater than 0.7, it will be regarded as a keyword and stored as an actual word or label. The equation of the algorithm is as follows:<disp-formula id="FD5-sensors-25-01178"><label>(5)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x02217;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mn>2</mml:mn><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the number of the same characters in sequence, whereas <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mn>2</mml:mn><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> give the corresponding length for each of these two strings.</p></sec></sec><sec id="sec3dot2-sensors-25-01178"><title>3.2. Localization Based Web Mining</title><p>This part presents a detailed description of generating coordinates for localization tasks. In addition, the creation of the web-based map was explained. Furthermore, the developed algorithm for the overall system, encompassing various methods, is presented. Finally, the workflow of the proposed system is summarized before delving into the experiments.</p><sec id="sec3dot2dot1-sensors-25-01178"><title>3.2.1. Google Places Data</title><p>Google Places is a comprehensive service provided by Google that offers detailed information about places and locations all around the world. It includes an extensive database with geographic data, business listings, and points of interest (POI). POI data define digital representations of specific locations or areas that interest some population segments [<xref rid="B52-sensors-25-01178" ref-type="bibr">52</xref>]. POI data cover various types of locations, such as restaurants, markets, stores, grocery stores, schools, hospitals, and so on. POI data are pivotal for robotic localization, enabling them to navigate more efficiently for given tasks, particularly when GPS signals are limited or blocked. The Places API is crucial in providing necessary POI data for various locations. The Google Places extensive data can be accessed utilizing the Google Places API [<xref rid="B53-sensors-25-01178" ref-type="bibr">53</xref>], which essentially serves as a search function. There are four search techniques available for locations:<list list-type="bullet"><list-item><p><bold>NS</bold> provides a list of nearby places based on a given location; input location data type should be in terms of latitude and longitude coordinates.</p></list-item><list-item><p><bold>Text Search</bold> returns a compilation of locations in the region based on a search string, for example, &#x0201c;Spaghetti&#x0201d;.</p></list-item><list-item><p><bold>Place Details</bold> returns more thorough details about a certain location, including user reviews.</p></list-item><list-item><p><bold>Place Photo</bold> enables the retrieval of photos associated with a specific place.</p></list-item></list></p><p>The Nearby Search (NS) [<xref rid="B54-sensors-25-01178" ref-type="bibr">54</xref>] is a crucial search technique for location-based services. It enables users to find POIs near a specified geographic location. Developers can customize search queries based on various parameters such as keywords, language, open now, rank by, and type. However, two fundamental parameters are required for the NS function: location and radius. Users should define a central &#x0201c;location&#x0201d; by providing latitude and longitude coordinates, which will serve as the epicenter of the search. The &#x0201c;radius&#x0201d; parameter defines the region where the NS will be conducted. It is a virtual boundary, limiting the search&#x02019;s range to a particular distance. Utilizing the &#x0201c;keywords&#x0201d; parameter is crucial in enhancing the search results. Searching by providing &#x0201c;keywords&#x0201d; or phrases as parameters is important for finding specific types of businesses, services, or any POI. The response from an NS provides extensive information on locations within a specified geographic area. These data usually include each place&#x02019;s name, address, and geographical coordinates, allowing developers to present a dynamic and rich exploration of their environment.</p><p>In this paper, we utilized the NS option with JavaScript programming language. In accordance with the scope of the study, it was intended to extract POI data from a certain region and save it to a file with a &#x0201c;.json&#x0201d; extension by utilizing NS. To employ the NS, firstly, we feed the coordinates from the GPS module placed in the robot as a location parameter. As they represent the robot&#x02019;s actual position, the given coordinates will serve as the epicenter of the search. Secondly, we define a parameter &#x0201c;keyword&#x0201d; to specify the business names. This keyword parameter enables us to retrieve more precise information about the businesses in the region. After providing the location and keyword parameters, the last step is defining a radius to search for businesses in a specific region. We set the radius between 1.500 to 2.000 m depending on the scenario. After applying all the steps, we collect POI data for particular regions where the experiments will be performed regarding business name, coordinates in latitude and longitude form, business status, rating of the business, and so on. All the collected data are stored in a JSON file for further use in the final system. In the final system, where localization is the main goal, the crucial elements of each obtained POI data are the business names and their corresponding coordinates. As can be seen, the data include location in the form of lat and long, business name, business status, rating, address, type of place, and so on.</p></sec><sec id="sec3dot2dot2-sensors-25-01178"><title>3.2.2. Mapbox</title><p>Mapbox [<xref rid="B55-sensors-25-01178" ref-type="bibr">55</xref>] is a web platform that offers powerful tools and services for incorporating interactive and customizable maps. Mapbox provides various mapping services, including navigation, geocoding, spatial data analysis, etc. Mapbox creates compelling visual and dynamic maps by combining raster and vector tiles. Mapbox offers a variety of map styles, and developers can choose from pre-designed styles or make their own using Mapbox Studio. Additionally, the platform provides software development kits (SDKs) for other programming languages, opening it up to a large developer community. On the other hand, Mapbox offers a variety of map styles that developers can utilize to customize the appearance of their maps. Different map styles can be chosen depending on the application&#x02019;s purpose, audience, and design preferences. Mapbox has offline map functionality. Thus, developers can create custom offline maps without requiring an internet connection. This is especially helpful when users might not always have internet connectivity, such as in remote areas or with applications requiring reliable offline functionality. In this paper, a Mapbox-based web map was developed to demonstrate the coordinates obtained from the GPS module and those generated by the proposed system. The development of the map involves the incorporation of Mapbox GL JS, a powerful JavaScript library designed for creating interactive and customizable maps. Utilizing Mapbox GL JS enables us to render map data from Mapbox Vector Tiles, employing the Mapbox Style specification and hardware-accelerated graphics (WebGL). As a map style, we employed &#x02019;streets-v12&#x02019; [<xref rid="B56-sensors-25-01178" ref-type="bibr">56</xref>], which is Mapbox&#x02019;s predefined style, offering a detailed representation of streets, landmarks, and geographical elements. In addition, Google Places was integrated with the map for utilizing Google Maps services. While developing the map, the map&#x02019;s initial view is centered at defined coordinates, with a zoom level set to 6. The map has been configured to operate offline without an internet connection. Once the map has been created, we define two points on the map. The first point, denoted in red, represents the coordinates obtained from the GPS module, while the blue point represents the coordinates generated from the proposed system. In the final system, as we will receive coordinates sequentially from the GPS module and proposed system, the respective points will continuously update their location on the map.</p></sec><sec id="sec3dot2dot3-sensors-25-01178"><title>3.2.3. Haversine</title><p>The Haversine Formula is a mathematical equation based on trigonometric principles used to calculate the distance between two points on the surface of a sphere, often applied to measure the great-circle distance between two locations on the Earth. The initial tabulation of Haversines was introduced by Andrew in 1805. However, Inman officially coined the term &#x0201c;Haversine&#x0201d; in 1835 [<xref rid="B57-sensors-25-01178" ref-type="bibr">57</xref>]. The Haversine formula represents a specific instance of the law of Haversines, establishing connections between the sides and angles of spherical triangles. The Haversine distance [<xref rid="B58-sensors-25-01178" ref-type="bibr">58</xref>] measures the great-circle distance in kilometers between two points on a sphere. The formula is defined as follows:<disp-formula id="FD6-sensors-25-01178"><label>(6)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mo form="prefix">sin</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>+</mml:mo><mml:mo form="prefix">cos</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mo form="prefix">cos</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mo form="prefix">sin</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are the latitude values at the two specified coordinates, <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is change in latitude and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is change in longitude.<disp-formula id="FD7-sensors-25-01178"><label>(7)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mo form="prefix">atan2</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msqrt><mml:mi>a</mml:mi></mml:msqrt><mml:mo>,</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msqrt></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The center angle between two points is determined by Equation (<xref rid="FD7-sensors-25-01178" ref-type="disp-formula">7</xref>).<disp-formula id="FD8-sensors-25-01178"><label>(8)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The shortest distance is determined by multiplying the value of <italic toggle="yes">R</italic> by <italic toggle="yes">c</italic>, where <italic toggle="yes">R</italic> corresponds to the Earth&#x02019;s radius. Considering previous research in the field, specifically regarding distance computation between points and subsequent path length determination, we will employ the Haversine distance instead of the Euclidean in this paper. In this case, we implemented the Haversine formula to calculate distances between GPS module coordinates and generated coordinates. Furthermore, we integrated it into our developed algorithm to obtain the most relevant coordinate corresponding to a given label.</p><p>The Haversine formula is based on trigonometric functions, which normally require radians rather than degrees. Before applying the Haversine Formula to measure the distance between two points, we convert the latitude and longitude coordinates from degrees to radians with the following equation:<disp-formula id="FD9-sensors-25-01178"><label>(9)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>&#x003c0;</mml:mi></mml:mrow><mml:mn>180</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-01178"><label>(10)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>&#x003c0;</mml:mi></mml:mrow><mml:mn>180</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>After converting the degrees to radians, we apply the Haversine Formula to calculate the distance between two points. The output distance is typically in kilometers, reflecting the Earth&#x02019;s radius <italic toggle="yes">R</italic> used in the Formula (<xref rid="FD8-sensors-25-01178" ref-type="disp-formula">8</xref>), which is approximately 6371 km. Finally, to obtain the distance in meters, we convert from kilometers to meters as follows <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec></sec><sec id="sec3dot3-sensors-25-01178"><title>3.3. Mobile Robot Platform</title><p>The experiments were conducted using an 8-wheeled mobile robot developed in [<xref rid="B59-sensors-25-01178" ref-type="bibr">59</xref>]. This robot features eight rubber wheels, providing the ability to climb pavements, and is powered by eight brushed DC motors. High gear-ratio motors were selected to ensure reliable performance on uneven outdoor terrains. The motors are controlled via an Arduino Mega 2560, a microcontroller board based on the Atmega2560.</p><p>In the robot&#x02019;s default setup, the camera&#x02019;s position was misaligned with the target views, which could adversely affect the performance of the text detection and recognition algorithms. To address this, a platform was designed and mounted on top of the robot, aligning the camera with the target views.</p><p>During the experiments in [<xref rid="B49-sensors-25-01178" ref-type="bibr">49</xref>], vibrations caused by the rubber wheels and uneven ground resulted in blurred video footage, negatively impacting the algorithms&#x02019; performance. To mitigate this issue, a gimbal equipped with a Logitech camera was installed on top of the platform, stabilizing the camera. After these modifications, the camera was positioned 1.08 m above the ground.</p><p>The robot was equipped with a laptop running a 64-bit Ubuntu 20.04 Linux operating system. The laptop featured an Intel i5 processor, 8 GB of RAM, and an Nvidia GeForce 1680 graphics card. GPU functionalities were utilized by installing CUDA and CuDNN libraries. The final appearance of the robot, following these modifications, is illustrated in <xref rid="sensors-25-01178-f005" ref-type="fig">Figure 5</xref>.</p></sec></sec><sec id="sec4-sensors-25-01178"><title>4. Case Studies</title><p>This section will present the experiments conducted using the proposed system and compare the results with existing works.</p><p>In order to evaluate the accuracy of the proposed system and determine its capability to localize the robot in cases where GPS signals are lost, two types of scenarios were performed: (1) Obtaining coordinates from the GPS module and proposed system and (2) Obtaining coordinates from the proposed system. The coordinates from the GPS module are demonstrated on the map with a red point, while the coordinates from the proposed system are demonstrated with a blue point. The experiments were carried out on three distinct urban areas: Crowded Street (Area 1), Pedestrian Street (Area 2), and Commercial District Street (Area 3). In <xref rid="sensors-25-01178-f006" ref-type="fig">Figure 6</xref>, the robot samples for each area are illustrated.</p><p>A crowded street operates as a one-way street for traffic movements, experiencing particularly heavy traffic, especially during the morning hours. The most common types of businesses along the street are banks and stores. The street surface includes only asphalt. The street with pedestrians operates as a thoroughfare exclusive to pedestrians and is closed to traffic. Commercial establishments, including stores and food stores, characterize the street. The street surface consists of tiles as well as asphalt in some regions. The crowded street is open to two-way traffic and has pavement for pedestrians&#x02019; use. The street includes a variety of businesses, such as coffee shops, markets, restaurants, and so on. The surface varies from region to region, including asphalt, concrete, and cobblestone. Considering the area in which the robot can move within the streets, the test area length of the experiments varies. The robot was generally operated at an approximate velocity of 5 km/h. However, owing to traffic jams and the presence of pedestrians in the experimental area, there were instances where it was necessary to either stop or reduce the velocity.</p><sec id="sec4dot1-sensors-25-01178"><title>4.1. Case Study 1: Localization in Crowded Urban Streets</title><p>The first experiment was conducted in a crowded urban street in test Area 1. As illustrated in <xref rid="sensors-25-01178-f007" ref-type="fig">Figure 7</xref>, start and end points were defined for the motion of the mobile robot. The length of the test Area 1 is around 130 m. The street of test Area 1 is surrounded by five or six-storey buildings. The surface of the street is asphalt. Thus, utilizing the gimble on the robot can avoid the vibrations caused by the surface, resulting in a good video stream. The numbers in <xref rid="sensors-25-01178-f007" ref-type="fig">Figure 7</xref> represent the businesses that are referred to in <xref rid="sensors-25-01178-t001" ref-type="table">Table 1</xref>. There are eight businesses, six of which are banks, one store, and one cargo store. Each business consists of different signboards with its name with various types of fonts and backgrounds.
<list list-type="simple"><list-item><p><bold>Scenario 1</bold></p></list-item></list></p><p>In this scenario, coordinates from the GPS module and generated coordinates are obtained to evaluate the performance of the proposed system. The experiment was conducted in the late afternoon; it began at 5:55 PM and ended at 6:01 PM. The robot&#x02019;s starting point is at the beginning of the business building marked with number one, and its ending point is at the building denoted by number eight in <xref rid="sensors-25-01178-f007" ref-type="fig">Figure 7</xref>.</p><p>As the mobile robot started moving, text detection and recognition were performed. In <xref rid="sensors-25-01178-f008" ref-type="fig">Figure 8</xref>, from the (a) to the (h), the texts detected and recognized in real-time by the enhanced system are demonstrated, respectively, as mentioned in <xref rid="sensors-25-01178-t001" ref-type="table">Table 1</xref>. When the real-time video stream was handled, it was seen that the enhanced system could easily identify the label &#x0201c;alBaraka&#x0201d; from a variety of angles, even from the angles where sunlight exists, as can be seen in <xref rid="sensors-25-01178-f008" ref-type="fig">Figure 8</xref> under option (a). As can be seen under option (b), the business, namely &#x0201c;A-101&#x0201d;, does not have a signboard where its name is mentioned. The algorithm successfully recognized the business&#x02019;s name from the small label on the door. It was comparatively easier for the algorithm to identify the business name in options (c) and (e) due to the same background color and font used for businesses&#x02019; signboards. In particular, &#x0201c;HALKBANK&#x0201d; was detected more often during the stream because of its name&#x02019;s capitalization. The enhanced system could not recognize the business name &#x0201c;Ptt&#x0201d;. As can be seen from option (d), the characters of the business name are adjacent to each other. Therefore, the algorithm can not separate the characters, resulting in the interpretation of the name as a single character rather than individual characters. It has been noted that the algorithm performs more efficiently when handling business names (signboards) with dark color tones and white backgrounds. The algorithm frequently recognized the businesses &#x0201c;Vak&#x00131;fbank&#x0201d; and &#x0201c;KuveyT&#x000fc;rk&#x0201d; even when blurriness occurs in the video stream, as can be seen under options (f) and (g). The last one, &#x0201c;Ziraat&#x0201d; was easily recognized from various angles due to its capital characters. In the overall process of detecting and recognizing text on the street in the first scenario, the enhanced system effectively recognized all business names along the route except one name. This success was achieved under various challenges such as lighting variations, image blurring due to the vibrations, and traffic jams.</p><p>In <xref rid="sensors-25-01178-t002" ref-type="table">Table 2</xref>, recognized business names (labels) with EasyOCR and corrected labels by Sequence Matcher with the similarity rate are presented. The Word Similarity Rate in <xref rid="sensors-25-01178-t002" ref-type="table">Table 2</xref> reflects the similarity between the word recognized by the OCR algorithm and the actual word before applying the Sequence Matcher. In scenario 1, EasyOCR recognized the business names &#x0201c;alBaraka&#x0201d;, &#x0201c;A-101&#x0201d;, &#x0201c;Yap&#x00131;Kredi&#x0201d; and &#x0201c;Vak&#x00131;f&#x0201d; correctly. Thus, the similarity rate is 100%. The algorithm recognized the labels &#x0201c;HALKBANK&#x0201d;, &#x0201c;KUVEYTT&#x000dc;RK&#x0201d; and &#x0201c;Ziraat&#x0201d; as missing one or two characters. Therefore, the similarity rate varies. The reason for missing one or two characters is that some look similar. For example, the algorithm recognized &#x0201c;HALKBANK&#x0201d; as &#x0201c;HAIKBANK&#x0201d;; here, the similar look of the characters &#x0201c;I&#x0201d; and &#x0201c;L&#x0201d; negatively affects the EasyOCR algorithm. Finally, as mentioned above, the algorithm could not recognize the label &#x0201c;Ptt&#x0201d;. Therefore, it is demonstrated in the table with a dash (-). The obtained results show that utilizing the Sequence Matcher algorithm enhances the text detection and recognition system in Scenario 1.</p><p><xref rid="sensors-25-01178-t003" ref-type="table">Table 3</xref> compares the GPS module coordinates and generated coordinates regarding distance error. In this context, distance error refers to the difference between the GPS module coordinate and the generated coordinate. In addition, the precision of the generated coordinate for each business was verified on Google Maps. We take GPS and generate coordinates for each business when the robot is in front of the business. When <xref rid="sensors-25-01178-t003" ref-type="table">Table 3</xref> is examined, it can be seen that the proposed system generated coordinates with 1.23 m error when the robot was positioned in front of the business &#x0201c;Yap&#x00131;Kredi&#x0201d;. This signifies the lowest error rate. On the other hand, the highest error rate occurred when the robot was positioned near business &#x0201c;alBaraka&#x0201d; with a 6 m error rate. When the robot approached the businesses &#x0201c;A101&#x0201d;, &#x0201c;HALKBANK&#x0201d;, &#x0201c;Vak&#x00131;fBank&#x0201d;, &#x0201c;KUVEYT&#x000dc;RK&#x0201d; and &#x0201c;Ziraat&#x0201d;, the error rates were measured as 3.27, 3.24, 5.67, 6.49, and 5.43 m, respectively. Consequently, the average error of the generated coordinates with respect to the coordinates of the GPS module is 4.48 m. This shows the robustness of the proposed system, affirming its capability to uphold stability and performance in urban environments.</p><p>The proposed system generates coordinates once the business names are detected and correctly recognized. As illustrated in <xref rid="sensors-25-01178-f009" ref-type="fig">Figure 9</xref>, the generated coordinates and the coordinates obtained from the GPS module are demonstrated on the developed map. Note that the coordinates obtained from the GPS module are represented with red points, while generated coordinates from the proposed system are demonstrated with blue points. As the mobile robot moves, the red point on the map updates its location according to the coordinates from the GPS module, while the blue point is updated as the robot recognizes the business names and the system generates coordinates. When <xref rid="sensors-25-01178-f009" ref-type="fig">Figure 9</xref> is examined from option (a) to (h), it can be seen that the blue point updates itself in synchronization with the red point as mobile robots come near the businesses and recognize the business names. Except in option (h), where the label &#x0201c;Ptt&#x0201d; was not recognized, the system could not generate a coordinate and update the blue point. The effective updating of the blue point on the map along the 130 m demonstrates the success of the suggested system.
<list list-type="simple"><list-item><p><bold>Scenario 2</bold></p></list-item></list></p><p>This scenario was carried out in the morning hours, under sunny weather conditions. The experiment started at 09:06 AM and ended at 09:11 AM. Despite dozens of attempts from various locations on the street, we could not get a signal from the GPS module due to environmental factors. Hence, in this scenario, while the mobile robot moves, it only updates its location on the map with the coordinates obtained from the proposed system. To measure the difference in the endpoint of the scenario, we manually define an end coordinate for the red point. Consequently, the robot will update its position based on the proposed system. At the end of the scenario, we will measure the distance between the manually defined coordinate and the robot&#x02019;s position. The robot&#x02019;s starting and end points are in contrast to scenario 1. In this case, the robot starting point is the business marked with the number eight, and the endpoint is the business denoted with the number one in <xref rid="sensors-25-01178-f007" ref-type="fig">Figure 7</xref>.</p><p>Due to the traffic jam, in contrast to scenario 1, we drove the mobile robot closer to the businesses in this scenario. As the mobile robots move, the detected and recognized business names are illustrated in <xref rid="sensors-25-01178-f010" ref-type="fig">Figure 10</xref> sequentially based on their locations in the street. As depicted in <xref rid="sensors-25-01178-f010" ref-type="fig">Figure 10</xref>, from option (a) to (h), the enhanced system successfully recognized each business name except &#x0201c;Ptt&#x0201d;. When the real-time video stream was handled, it was seen that even if the weather is sunny and there are good lighting conditions, the camera angle negatively affects the system&#x02019;s detection and recognition. Due to the business names being too high concerning the camera angle, the improved system encountered challenges in recognizing the name &#x0201c;Vak&#x00131;fBank&#x0201d;; thereby, it recognizes the related name in the last phase of the video stream, as can be seen in <xref rid="sensors-25-01178-f010" ref-type="fig">Figure 10</xref> under option (c). In addition to the camera angle issue, another notable challenge in Scenario 2 was the presence of trees and their shadows, which partially obscured parts of the signboards or created uneven lighting conditions. For instance, in <xref rid="sensors-25-01178-f010" ref-type="fig">Figure 10</xref>, under options (b), (c), and (f), tree branches and leaves blocked portions of the signboards, making it difficult for the detection algorithm to accurately identify the text. Environmental factors such as moving vehicles introduced blurriness on the video stream in some cases, such as in option (e). Despite these challenges, the rest of the business names were easily recognized multiple times from different angles, as seen under options (a), (b), (d), (f), (g), and (h), due to the Sequence Matcher.</p><p>In <xref rid="sensors-25-01178-t004" ref-type="table">Table 4</xref>, it can be seen that EasyOCR, except for business names &#x0201c;A-101&#x0201d; and &#x0201c;alBaraka&#x0201d;, encounters challenges in recognizing business names. The EasyOCR algorithm recognizes the &#x0201c;Ziraat&#x0201d; as &#x0201c;iraa&#x0201d;, &#x0201c;KUVEYT&#x0201d; as &#x0201c;KUUEYT&#x0201d;, and &#x0201c;Vak&#x00131;f&#x0201d; as &#x0201c;ak&#x00131;fB&#x0201d;, &#x0201c;HALKBANK&#x0201d; as &#x0201c;IHALKBA&#x0201d;, &#x0201c;Yap&#x00131;Kredi&#x0201d; as &#x0201c;Tap&#x00131;Kredi&#x0201d;. This is caused by the camera angle and the traffic sequence, making it blurry. To overcome this limitation, we integrated the OCR algorithm with the Sequence Matcher, which allowed us to correct misrecognized words and significantly improve accuracy, reaching from 80% to 100%. This low rate (around 80%) shows that the OCR algorithm alone may not always achieve high accuracy in real-time, particularly under challenging conditions. This integration enhances the real-time usability and reliability of the text recognition process. As a result, each business name was correctly recognized for the further process.</p><p>When <xref rid="sensors-25-01178-t005" ref-type="table">Table 5</xref> is evaluated, it can be seen that as the mobile robot reaches the last business, the distance difference with the red point decreases to 7.15 m. When the distance difference was evaluated for each business, it can be seen that the distance difference decreased simultaneously, which reveals that the proposed system creates smooth coordinates.</p><p>Upon correctly recognizing the business names, the proposed system generates coordinates. As was mentioned before, we define constant coordinates for the red point due to the GPS signal blockage. Consequently, while the mobile robot is in motion and the proposed system operates in the background, the blue point continuously updates its position, as depicted in <xref rid="sensors-25-01178-f011" ref-type="fig">Figure 11</xref>. When the mobile robot reaches the end of the route, the blue point comes closer to the red point. <xref rid="sensors-25-01178-f011" ref-type="fig">Figure 11</xref> reveals that as the mobile robot reaches the end of the route, the blue point approaches the red point.</p><sec id="sec4dot1dot1-sensors-25-01178"><title>4.1.1. Case Study 2: Localization in Pedestrian Streets</title><p>The second experiment was carried out in the street with pedestrians, namely Test Area 2. <xref rid="sensors-25-01178-f012" ref-type="fig">Figure 12</xref> presents the testing with the start and end points. The test area is approximately 105 m long and obstructed by six- or seven-story buildings. Most of the street consists of tiles, while a small part is asphalt. The numerical labels in <xref rid="sensors-25-01178-f012" ref-type="fig">Figure 12</xref> correspond to businesses referred to in <xref rid="sensors-25-01178-t006" ref-type="table">Table 6</xref>. As seen in <xref rid="sensors-25-01178-f012" ref-type="fig">Figure 12</xref>, there are seven businesses, and each business has unique signboards with various fonts and backgrounds.
<list list-type="simple"><list-item><p><bold>Scenario 3</bold></p></list-item></list></p><p>The scenario took place in the evening, under ideal weather conditions. The experiment began at 09:34 PM and concluded at 09:38 PM. The starting point of the robot is at the middle of the business building marked with number seven, and its ending point is at the building denoted by number one in <xref rid="sensors-25-01178-f012" ref-type="fig">Figure 12</xref>. In this scenario, we obtain coordinates from the GPS module and the proposed system.</p><p>Each detected and recognized business name is depicted in <xref rid="sensors-25-01178-f013" ref-type="fig">Figure 13</xref>, in the sequence of their real location in the street. When the real-time video stream was examined, it was observed that the enhanced system performs well even in the evening under limited lighting conditions.</p><p>The enhanced system recognized the business names &#x0201c;Atasun Optik&#x0201d;, &#x0201c;ERCAN&#x0201d;, &#x0201c;GLORIA PERFUME&#x0201d;, &#x0201c;TAVUK&#x0201d; and &#x0201c;KING&#x0201d; many times from different angles. Moreover, the system recognizes the business names &#x0201c;Metin&#x0201d; and &#x0201c;Watsons&#x0201d; a few times. Due to the handwriting name of the business and constant objects in the street, the system encountered challenges in detecting and recognizing the label &#x0201c;Metin&#x0201d;. One important challenge in this scenario is lighting conditions, especially in night-time settings. Due to excessive light on the signboard in <xref rid="sensors-25-01178-f013" ref-type="fig">Figure 13</xref> under option (g), the system could not detect the label &#x0201c;Watsons&#x0201d;. However, it detected and recognized the business&#x02019;s name from the advertising section above the signboard, as seen in <xref rid="sensors-25-01178-f013" ref-type="fig">Figure 13</xref> under option (g).</p><p>The recognized and corrected business names during Scenario 3 are presented in <xref rid="sensors-25-01178-t007" ref-type="table">Table 7</xref>. The Easy OCR algorithm misrecognized one character for the business names &#x0201c;ERCAN&#x0201d;, &#x0201c;Metin&#x0201d;, and &#x0201c;watsons&#x0201d; and recognized them as &#x0201c;ERCA&#x0201d;, &#x0201c;Jetin&#x0201d; and &#x0201c;atsons&#x0201d;, respectively. Meanwhile, the rest of the business names are recognized without any difficulties. Due to the handwriting of the name &#x0201c;Metin&#x0201d; and the camera angle, the system failed to detect the character &#x0201c;M&#x0201d; properly, as can be seen in <xref rid="sensors-25-01178-f013" ref-type="fig">Figure 13</xref> under option (d). In addition, due to the light in the signboards, EasyOCR failed to capture the characters &#x0201c;N&#x0201d; in &#x0201c;ERCAN&#x0201d; and &#x0201c;w&#x0201d; in &#x0201c;Watsons&#x0201d;. Finally, each misrecognized name was corrected successfully with Sequence Matcher.</p><p>For scenario 3, we consider the robot&#x02019;s actual position as generated coordinates due to the lack of GPS signal sensitivity. Therefore, we measure distance difference in terms of GPS module error. As shown in <xref rid="sensors-25-01178-t008" ref-type="table">Table 8</xref>, the distance difference between the GPS coordinates and the actual position of the robot when the robot was near the businesses &#x0201c;Atasun Optik&#x0201d;, &#x0201c;ERCAN BURGER&#x0201d;, &#x0201c;Metin&#x0201d; was around 43.25, 31.72, and 27.63 m, respectively. Further, the GPS signal improved its sensitivity when the robot approached the businesses &#x0201c;GLORIA PERFUME&#x0201d; and &#x0201c;Tavuk D&#x000fc;nyas&#x00131;&#x0201d;, resulting in a reduced distance difference of approximately 13 m. In the last business, we faced a lack of GPS signals. Therefore, the distance increases to 217 m. This scenario shows the importance of developing a localization support system that is not dependent on GPS signals.</p><p>As mentioned before, we obtain coordinates from GPS and the proposed system during this scenario. Note that GPS coordinates are shown as red points, while the proposed system&#x02019;s coordinates are shown as blue points. However, due to environmental factors such as tall buildings, we lost the GPS signal in some regions while the robot was in motion. In addition, when tracking the robot&#x02019;s movement on the developed map, it was observed that the sensitivity of the GPS signal decreased. The red point updated itself over the buildings, whereas it should have been in the middle of the road, considering the robot&#x02019;s actual position. As depicted in <xref rid="sensors-25-01178-f014" ref-type="fig">Figure 14</xref>, specifically under options (a), (c), (d), and (e), the red point is situated above the buildings, while the actual position of the robot is in the middle of the street &#x0201c;H&#x000fc;rriyet&#x0201d;. Additionally, in <xref rid="sensors-25-01178-f014" ref-type="fig">Figure 14</xref> under option (g), the red point appears several streets above the robot&#x02019;s actual position, showing the lack of robustness and low accuracy of GPS signals in this scenario. On the other hand, while the robot is moving in the street &#x0201c;H&#x000fc;rriyet&#x0201d;, option (a) to (g), it can be seen that the blue point is updating itself according to business places near the street, except for option (b). Here, the coordinate was expected to be near the street and in front of the building; however, the system generated the coordinate on the back side of the building. In general, updating its position near the street and in accordance with the business locations demonstrates the effectiveness of the proposed system in scenarios where GPS signals are either not sensitive or completely lost. <list list-type="simple"><list-item><p><bold>Scenario 4</bold></p></list-item></list></p><p>This scenario was conducted during midday, under clear sunny weather conditions. The experiment started at 12:53 AM and ended at 12:58 AM. As in the second scenario, we could not obtain a signal from the GPS module due to environmental factors. Therefore, in this scenario, as the mobile robot is in motion, it updates its location on the map solely with the coordinates obtained from the proposed system, which is denoted with a blue point. We manually define a coordinate for the red point near the last business of the scenario to measure the difference with the blue point, in other words, with the generated coordinate. The starting and end points of the robot are opposite to Scenario 3.</p><p>In contrast to Scenario 3, here, the enhanced system easily detected and recognized the business names &#x0201c;watson&#x0201d; as well as &#x0201c;Metin&#x0201d; dozens of times, as shown in <xref rid="sensors-25-01178-f015" ref-type="fig">Figure 15</xref> under options (a) and (d). When the robot was in front of the business &#x0201c;Tavuk D&#x000fc;nyas&#x00131;&#x0201d;, some vibrations occurred due to the cobblestone surface of that region. Thus, the camera angle was altered, but the gimble quickly readjusted it to its standard position. Even in the challenging angles, the system recognizes the name &#x0201c;Tavuk D&#x000fc;nyas&#x00131;&#x0201d;, as can be seen under option (c). The system recognizes the rest of the businesses without any remarkable challenges. Some frames during the recognition of the names &#x0201c;watson&#x0201d;, &#x0201c;Burger King&#x0201d;, &#x0201c;GLORIA PARFUME&#x0201d;, &#x0201c;ERCAN BURGER&#x0201d; and &#x0201c;Atasun Optik&#x0201d; are demonstrated in <xref rid="sensors-25-01178-f015" ref-type="fig">Figure 15</xref>.</p><p>In <xref rid="sensors-25-01178-t009" ref-type="table">Table 9</xref>, each recognized and corrected business name is shown with their similarity rate. The Easy OCR algorithm recognizes each business name except &#x0201c;Tavuk D&#x000fc;nyas&#x00131;&#x0201d; and &#x0201c;Metin&#x0201d;. The reason behind this was the vibration that occurred during the robot&#x02019;s movement near the business &#x0201c;Tavuk D&#x000fc;nyas&#x00131;&#x0201d;. In addition, due to the limitations of the Easy OCR algorithm for handwritten characters, it could not recognize the character &#x0201c;M&#x0201d; when it comes to the business &#x0201c;Metin&#x0201d;.</p><p>When <xref rid="sensors-25-01178-t010" ref-type="table">Table 10</xref> was evaluated, it is clear that the distance difference with the red point decreases as the mobile robot approaches the endpoint. However, when the robot approaches the last two businesses that are placed in two adjacent buildings, the distance error does not decrease. This is due to the inaccurate coordinate placement on Google Maps, highlighting the limitation of the proposed system in scenarios where the reference coordinates are imprecisely defined on the map.</p><p>As mentioned before, the red point in this scenario was set to a constant position near the last business, &#x0201c;Atasun Optik&#x0201d;, to measure the generated system&#x02019;s effectiveness when the mobile robot reaches the endpoint. Examining <xref rid="sensors-25-01178-f016" ref-type="fig">Figure 16</xref> demonstrates that as the robot moves and recognizes business names, the blue point updates its position in accordance with the business places. However, when it recognizes the name &#x0201c;Ercan Burger&#x0201d; the generated coordinate is not located near the road or in front of the building; instead, it is situated on the back side of the building, as can be seen under option (f). This discrepancy is due to an inaccurate coordinate placement on Google Maps. The same discrepancy arises with the name &#x0201c;Atasun Optik&#x0201d;; the generated coordinate by the proposed system is not positioned in front of the business &#x0201c;Atasun Optik&#x0201d; but rather in front of a neighboring building, as seen in option (g).</p></sec><sec id="sec4dot1dot2-sensors-25-01178"><title>4.1.2. Case Study 3: Localization in Commercial District</title><p>The final experiment was conducted in a commercial district in test Area 3. As shown in <xref rid="sensors-25-01178-f017" ref-type="fig">Figure 17</xref>, we designated starting and ending points for the motion of the mobile robot. The test Area 3 has a length of about 90 m. One side of test Area 3 is bordered by seven- or eight-story buildings, while on the opposite side, there is a road, followed by another set of multi-story buildings. The numbers in <xref rid="sensors-25-01178-f017" ref-type="fig">Figure 17</xref> represent the businesses that are referred to in <xref rid="sensors-25-01178-t011" ref-type="table">Table 11</xref>. Similar to the preceding testing areas, each business in this location features complex signboards presented in various fonts and set against diverse backgrounds.
<list list-type="simple"><list-item><p><bold>Scenario 5</bold></p></list-item></list></p><p>The experiment was conducted in the evening, under challenging conditions such as low light and a strong storm. To evaluate the performance of the proposed system in challenging conditions, including low light, a strong storm, and cobblestone terrain, the experiment was deliberately conducted during the evening hours on a surface with cobblestones. The robot&#x02019;s initial position is at the start of the business building identified as number one, while its final destination is the building marked with the number five in <xref rid="sensors-25-01178-f017" ref-type="fig">Figure 17</xref>. In this experiment, we acquire coordinates from the GPS module and the proposed system. The experiments started at 05:33 PM and ended at 05:38 PM.</p><p>In this experiment, the performance of the proposed system for text detection and recognition was robust despite challenges such as low light conditions, surface-induced vibrations leading to video stream blurriness, and the presence of a strong storm. The system effectively recognized all businesses within the test area except one name. Each recognized business name is demonstrated in <xref rid="sensors-25-01178-f018" ref-type="fig">Figure 18</xref> under options (a), (b), (d), and (e). However, under option (c), the proposed system encountered difficulty in recognizing the business &#x0201c;STARBUCKS&#x0201d;. The reason behind this is the white light on the signboard name and blurriness caused by the storm, which altered the camera position. The reflection of the lights resulted in the characters appearing connected without clear spacing, posing a challenge for the system in recognizing the business name. On the other hand, the strong storm intermittently shifted the camera position throughout the experiments. Fortunately, the gimbal played a crucial role in restoring stability to the camera. The proposed system generally demonstrated remarkable performance in challenging conditions, including low light, a strong storm, and a cobblestone surface.</p><p>As indicated in <xref rid="sensors-25-01178-t012" ref-type="table">Table 12</xref>, the EasyOCR algorithm accurately recognizes &#x0201c;VIP&#x0201d;, &#x0201c;Maydonoz&#x0201d; and &#x0201c;LOTUS&#x0201d; without missing any characters. However, it erroneously recognizes the name &#x0201c;HALIS&#x0201d; as &#x0201c;nHALIS&#x0201d;. Subsequently, this misrecognizing was corrected by the Sequence Matcher algorithm.</p><p><xref rid="sensors-25-01178-t013" ref-type="table">Table 13</xref> compares the coordinates from the GPS module with the generated coordinates, emphasizing the differences in distance between the two sets of coordinates. Initially, when the robot was in front of business number 1, the distance difference was approximately 18 m. Subsequently, as GPS signals were lost, the distance difference increased to thousands of kilometers. At one point during the test when GPS signals briefly returned, the distance difference at that position was around 15 m, although this occurrence was short-lived.</p><p>As stated at the beginning of this scenario, during this experiment, while the mobile robot moving in the defined test area, we are acquiring coordinates from both the GPS and the proposed system. As can be seen in <xref rid="sensors-25-01178-f019" ref-type="fig">Figure 19</xref> under option (a), when the mobile robot was in front of the business &#x0201c;G&#x00026;O VIP&#x0201d;, the blue point accurately positioned itself in the front of the building, whereas the GPS coordinate, represented by the red point, was situated in the side street. Subsequently, when the mobile robot reached the business marked with number 2 in <xref rid="sensors-25-01178-t011" ref-type="table">Table 11</xref>, the GPS signals were lost. By default, the module provided default coordinates, such as (0.0000, 0.0000). Consequently, the red point positioned itself near the intersection of Ecuador (latitude 0 degrees), as depicted in option (b). After the initial loss of GPS, there was a brief period during which GPS signals were available, but then signals were lost once again, and we were unable to acquire data for the rest of the test. However, the robot&#x02019;s localization persisted using the generated coordinates from the proposed system until the end of the test. As evident in options (c), (d), and (e), the blue point consistently updated its position as the business names were recognized. This underscores the significance of developing a localization system that does not solely rely on GPS signals in urban areas.</p></sec></sec><sec id="sec4dot2-sensors-25-01178"><title>4.2. System Evaluation</title><p>The localization performance of the proposed system is presented in <xref rid="sensors-25-01178-t014" ref-type="table">Table 14</xref>. The accuracy described in <xref rid="sensors-25-01178-t014" ref-type="table">Table 14</xref> of the proposed system corresponds to Scenario 1, where good GPS signals were available. As highlighted in the manuscript, in the remaining scenarios, either no GPS signals were received, or the signals were noisy. Therefore, the performance evaluation of the proposed system&#x02019;s accuracy was possible only in Scenario 1, where reliable GPS data were present. In other scenarios, we demonstrated that even when GPS signals were unavailable or when noisy signals were present, the proposed system enabled the robot to successfully localize itself.</p><p>When similar studies are examined in <xref rid="sensors-25-01178-t014" ref-type="table">Table 14</xref>, most of the studies use few sensors in order to localize mobile robots, and the distance error between the reference and generated coordinates are given as minimum, maximum and average. In [<xref rid="B29-sensors-25-01178" ref-type="bibr">29</xref>], the three different sensors are utilized. Even so, the average error rate was around 28 m. While it is crucial to conduct localization tests under various scenarios and in real-time, ref. [<xref rid="B61-sensors-25-01178" ref-type="bibr">61</xref>] carried out their experiments through simulation and [<xref rid="B60-sensors-25-01178" ref-type="bibr">60</xref>] performed their experiments via the video recordings. Although they achieved error rates of 4.23 m and 7.5 m, their studies lack real-time localization in urban environments. By employing three different sensors, ref. [<xref rid="B27-sensors-25-01178" ref-type="bibr">27</xref>] achieved an average error rate of 4.12 m. While this is a closely comparable error rate to our work, it is noteworthy that we attained a marginally higher error rate of 4.20 m using only one sensor. Considering both the average and minimum error rates in our system, it is lower than the other studies. Consequently, the high accuracy and sensitivity of the proposed system was demonstrated through real-time experiments conducted in various test areas and light conditions. Taking into account both the average and minimum error rates in our system, they are lower than those presented in <xref rid="sensors-25-01178-t014" ref-type="table">Table 14</xref>, except for the study by [<xref rid="B27-sensors-25-01178" ref-type="bibr">27</xref>], where they achieved slightly lower error rates by a few centimeters. In general, the proposed system demonstrated high accurate localization through real-time experiments conducted in various test areas and light conditions.</p><p>In <xref rid="sensors-25-01178-t015" ref-type="table">Table 15</xref>, we evaluated the operational efficiency of the proposed system, emphasizing its processing speed and average Frames Per Second (FPS). The processing speed refers to the duration elapsed from text detection to the reflection of coordinates on the map. The proposed system exhibited an average processing speed of approximately 2.5 s. Given the significance of time in the context of mobile robot localization, this performance can be considered remarkable. On the other hand, the proposed system, running on a laptop with limited computational capacity, achieved an average of 8 FPS.</p></sec><sec id="sec4dot3-sensors-25-01178"><title>4.3. Challenges and Discussion</title><p>During real-world experiments, several challenges were encountered in text detection, recognition, and localization, highlighting the complexity of operating in dynamic urban environments. In Scenario 1, the EAST text detection algorithm failed to detect PTT due to the closely spaced characters on the signboard, which were interpreted as a single merged character. Another challenge was the absence of a traditional signboard for A101. Despite this, the system detected the business name by recognizing the text near the store entrance. EasyOCR also struggled to recognize some businesses due to stylized fonts and closely positioned capital letters. However, these recognition errors were effectively corrected by integrating EasyOCR with the Sequence Matcher algorithm.</p><p>In Scenario 2, the mobile robot had to move closer to the businesses due to heavy traffic. Despite the closer distance, the system still failed to detect PTT because of the closely spaced characters on its signboard. Additionally, the higher camera angle compared to Scenario 1 caused further text detection and recognition difficulties. As a result, EasyOCR produced worse results than in the previous scenario. However, these issues were resolved by integrating EasyOCR with the Sequence Matcher algorithm, and the correct store names were recognized. Moreover, despite multiple attempts, no reliable GPS signals were obtained during this scenario, demonstrating the limitations of relying solely on GPS for localization, again. These circumstances highlight the importance of using a robust visual-based localization system that does not depend entirely on GPS signals.</p><p>The experiments in Scenario 3 were conducted during the evening, which presents distinct challenges. Several business names were handwritten, complicating the detection and recognition process. Additionally, nighttime conditions introduced some lighting issues. In some instances, excessive lighting caused glare on the signboards, preventing the system from directly detecting one business name. However, the system detected and recognized the business name from the advertising panel above the signboard. Moreover, GPS signals were inconsistent throughout the route, resulting in occasional inaccuracies where the robot&#x02019;s position was displayed far from its actual location on the map. Despite these unreliable GPS signals, the proposed system effectively localized the robot. However, at one point, an incorrect coordinate caused temporary mislocalization. Once the following business was detected and recognized, the system promptly corrected the robot&#x02019;s position, restoring accurate localization.</p><p>In Scenario 4, the experiments were conducted on the same street as in Scenario 3 but in the opposite direction and during the daytime. Consequently, lighting-related issues encountered during the nighttime experiment in Scenario 3 were not present in this scenario. However, at one point, ground-induced vibrations caused significant changes in the camera angle. This issue was mitigated by the gimbal mounted on the robot, which stabilized the camera and prevented further complications in text detection and recognition. Despite multiple attempts, reliable GPS signals could not be obtained in this scenario, similar to Scenario 3. As a result, the robot&#x02019;s localization relied solely on the proposed visual-based system. As observed in the previous scenario, the system generated an incorrect coordinate at one point, leading to temporary mislocalization. However, once the next business was detected and recognized, the system quickly corrected the robot&#x02019;s position, ensuring accurate localization.</p><p>In the final scenario, the primary challenges were the cobblestone surface and strong winds, which negatively impacted the system&#x02019;s performance. The uneven cobblestone surface caused significant vibrations, leading to temporary blurriness in the video stream, which affected text detection and recognition. Additionally, the lighting conditions on certain signboards varied significantly, and in one instance, excessive lighting prevented the system from accurately detecting a business name. Moreover, the strong winds intermittently shifted the camera position during the experiments. However, this issue was effectively mitigated by the gimbal, which played a crucial role in maintaining camera stability and minimizing the negative effects of the storm. Throughout the experiment, GPS signals were either wholly unavailable or highly inconsistent. Although GPS signals were briefly received at a few points, they were quickly lost, which shows that relying on GPS for localization is unreliable.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01178"><title>5. Conclusions</title><p>This paper introduces a visual localization system designed for use in urban environments with low or blocked GPS signals. The proposed system consists of three steps: (1) Text detection-recognition, (2) generating coordinates, and (3) map development. For the first step, a text detection and recognition system was proposed. The EAST text detection algorithm was applied to detect texts in urban environments. The Easy OCR algorithm was employed to recognize the detected text. In addition, the applied text detection and recognition algorithms were enhanced by applying post-processing and word similarity algorithms. In the second step, recognized labels were fed to Places API to obtain POI data, especially coordinates, for a particular area. This process collected POI data for specific areas, including the names of businesses, their coordinates in terms of longitude and latitude format, operational status, etc. Moreover, an algorithm was developed to search the obtained data for the relevant label and return the most suitable coordinates using the Haversine formula in scenarios where GPS signals are blocked. In the last step, a map was developed using the Mapbox tool. The coordinates obtained from the GPS module and those generated by the proposed system are represented on the map with red and blue points. The red points demonstrate the coordinates of GPS, while the blue point shows the coordinates generated by the proposed system. Moreover, to evaluate the error rate of the proposed system, the distance between the generated coordinate and the GPS module coordinate is calculated by utilizing the Haversine Formula. The experiments carried out for text detection and recognition show that enhancing the system by applying post-processing and word similarity improves the recognition accuracy of the OCR algorithm. The proposed system&#x02019;s localization performance with a delivery robot was evaluated in three different areas. Effective text detection and recognition performance, obtaining precise POI data, and developing an algorithm that returns respective coordinates contributed to the effective localization performance of the proposed system, resulting in an approximate error of around 4 m. Compared with other studies that employ methods like extended Kalman filter and Bayesian estimation, they reported similar error rates but relied on multiple sensors or were tested in simulations, increasing complexity. In contrast, our system uses only one sensor and was tested in real-world applications, making it more straightforward and practical. In addition, other studies that used more sensors reported higher error rates, such as 16 or even 28 m, highlighting the advantage of our approach. These results emphasize the significance of the proposed system in providing accurate and reliable localization with minimal sensor requirements in real-world urban environments.</p><p>In future work, we aim to address the issue of blurriness in the video stream caused by surface vibrations by employing advanced image processing filters and computer vision techniques. Additionally, we plan to reduce the error rate in the localization tasks of the proposed system. It will involve determining the robot&#x02019;s pose and measuring the distance between the camera and the detected business name. These enhancements will enable more accurate localization of the robot within its environment.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors thank Asiye Demirta&#x0015f; and &#x000d6;mer Mutlu T&#x000fc;rk Kaya for their help and support during the experimental stage of the study.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, E.A., G.E. and A.E.K.; methodology, E.A., G.E. and A.E.K.; software, E.A., G.E. and A.E.K.; supervision, G.E. and A.E.K.; writing&#x02014;original draft, E.A. and G.E.; writing&#x02014;review and editing, E.A., G.E. and A.E.K. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01178"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jiang</surname><given-names>S.</given-names></name>
<name><surname>Song</surname><given-names>W.</given-names></name>
<name><surname>Zhou</surname><given-names>Z.</given-names></name>
<name><surname>Sun</surname><given-names>S.</given-names></name>
</person-group><article-title>Stability analysis of the food delivery robot with suspension damping structure</article-title><source>Heliyon</source><year>2022</year><volume>8</volume><fpage>e12127</fpage><pub-id pub-id-type="doi">10.1016/j.heliyon.2022.e12127</pub-id><pub-id pub-id-type="pmid">36561695</pub-id>
</element-citation></ref><ref id="B2-sensors-25-01178"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yu</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>K.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Yang</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>D.</given-names></name>
</person-group><article-title>Real-time visual localization of the picking points for a ridge-planting strawberry harvesting robot</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>116556</fpage><lpage>116568</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.3003034</pub-id></element-citation></ref><ref id="B3-sensors-25-01178"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Meddeb</surname><given-names>H.</given-names></name>
<name><surname>Abdellaoui</surname><given-names>Z.</given-names></name>
<name><surname>Houaidi</surname><given-names>F.</given-names></name>
</person-group><article-title>Development of surveillance robot based on face recognition using Raspberry-PI and IOT</article-title><source>Microprocess. Microsystems</source><year>2023</year><volume>96</volume><fpage>104728</fpage><pub-id pub-id-type="doi">10.1016/j.micpro.2022.104728</pub-id></element-citation></ref><ref id="B4-sensors-25-01178"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>A.J.</given-names></name>
<name><surname>Song</surname><given-names>W.</given-names></name>
<name><surname>Yu</surname><given-names>B.</given-names></name>
<name><surname>Choi</surname><given-names>D.</given-names></name>
<name><surname>Tirtawardhana</surname><given-names>C.</given-names></name>
<name><surname>Myung</surname><given-names>H.</given-names></name>
</person-group><article-title>Survey of robotics technologies for civil infrastructure inspection</article-title><source>J. Infrastruct. Intell. Resil.</source><year>2023</year><volume>2</volume><fpage>100018</fpage><pub-id pub-id-type="doi">10.1016/j.iintel.2022.100018</pub-id></element-citation></ref><ref id="B5-sensors-25-01178"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ravankar</surname><given-names>A.</given-names></name>
<name><surname>Ravankar</surname><given-names>A.A.</given-names></name>
<name><surname>Kobayashi</surname><given-names>Y.</given-names></name>
<name><surname>Hoshino</surname><given-names>Y.</given-names></name>
<name><surname>Peng</surname><given-names>C.C.</given-names></name>
</person-group><article-title>Path smoothing techniques in robot navigation: State-of-the-art, current and future challenges</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>3170</elocation-id><pub-id pub-id-type="doi">10.3390/s18093170</pub-id><pub-id pub-id-type="pmid">30235894</pub-id>
</element-citation></ref><ref id="B6-sensors-25-01178"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yasuda</surname><given-names>Y.D.</given-names></name>
<name><surname>Martins</surname><given-names>L.E.G.</given-names></name>
<name><surname>Cappabianco</surname><given-names>F.A.</given-names></name>
</person-group><article-title>Autonomous visual navigation for mobile robots: A systematic literature review</article-title><source>ACM Comput. Surv. (CSUR)</source><year>2020</year><volume>53</volume><fpage>1</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1145/3368961</pub-id></element-citation></ref><ref id="B7-sensors-25-01178"><label>7.</label><element-citation publication-type="gov"><person-group person-group-type="author">
<collab>U.S. Government</collab>
</person-group><article-title>GPS Technical Information</article-title><comment>Available online: <ext-link xlink:href="https://www.gps.gov/technical/" ext-link-type="uri">https://www.gps.gov/technical/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-13">(accessed on 13 December 2024)</date-in-citation></element-citation></ref><ref id="B8-sensors-25-01178"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>R.</given-names></name>
<name><surname>Wang</surname><given-names>G.</given-names></name>
<name><surname>Zhang</surname><given-names>W.</given-names></name>
<name><surname>Hsu</surname><given-names>L.T.</given-names></name>
<name><surname>Ochieng</surname><given-names>W.Y.</given-names></name>
</person-group><article-title>A gradient boosting decision tree based GPS signal reception classification algorithm</article-title><source>Appl. Soft Comput.</source><year>2020</year><volume>86</volume><fpage>105942</fpage><pub-id pub-id-type="doi">10.1016/j.asoc.2019.105942</pub-id></element-citation></ref><ref id="B9-sensors-25-01178"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ranjbarzadeh</surname><given-names>R.</given-names></name>
<name><surname>Jafarzadeh Ghoushchi</surname><given-names>S.</given-names></name>
<name><surname>Anari</surname><given-names>S.</given-names></name>
<name><surname>Safavi</surname><given-names>S.</given-names></name>
<name><surname>Tataei Sarshar</surname><given-names>N.</given-names></name>
<name><surname>Babaee Tirkolaee</surname><given-names>E.</given-names></name>
<name><surname>Bendechache</surname><given-names>M.</given-names></name>
</person-group><article-title>A deep learning approach for robust, multi-oriented, and curved text detection</article-title><source>Cogn. Comput.</source><year>2024</year><volume>16</volume><fpage>1979</fpage><lpage>1991</lpage><pub-id pub-id-type="doi">10.1007/s12559-022-10072-w</pub-id></element-citation></ref><ref id="B10-sensors-25-01178"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yousef</surname><given-names>M.</given-names></name>
<name><surname>Hussain</surname><given-names>K.F.</given-names></name>
<name><surname>Mohammed</surname><given-names>U.S.</given-names></name>
</person-group><article-title>Accurate, data-efficient, unconstrained text recognition with convolutional neural networks</article-title><source>Pattern Recognit.</source><year>2020</year><volume>108</volume><fpage>107482</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2020.107482</pub-id></element-citation></ref><ref id="B11-sensors-25-01178"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Epshtein</surname><given-names>B.</given-names></name>
<name><surname>Ofek</surname><given-names>E.</given-names></name>
<name><surname>Wexler</surname><given-names>Y.</given-names></name>
</person-group><article-title>Detecting text in natural scenes with stroke width transform</article-title><source>Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision And Pattern Recognition</source><conf-loc>San Francisco, CA, USA</conf-loc><conf-date>13&#x02013;18 June 2010</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway Township, NJ, USA</publisher-loc><year>2010</year><fpage>2963</fpage><lpage>2970</lpage></element-citation></ref><ref id="B12-sensors-25-01178"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yao</surname><given-names>C.</given-names></name>
<name><surname>Bai</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>W.</given-names></name>
<name><surname>Ma</surname><given-names>Y.</given-names></name>
<name><surname>Tu</surname><given-names>Z.</given-names></name>
</person-group><article-title>Detecting texts of arbitrary orientations in natural images</article-title><source>Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Providence, RI, USA</conf-loc><conf-date>16&#x02013;21 June 2012</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway Township, NJ, USA</publisher-loc><year>2012</year><fpage>1083</fpage><lpage>1090</lpage></element-citation></ref><ref id="B13-sensors-25-01178"><label>13.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Fleet</surname><given-names>D.</given-names></name>
<name><surname>Pajdla</surname><given-names>T.</given-names></name>
<name><surname>Schiele</surname><given-names>B.</given-names></name>
<name><surname>Tuytelaars</surname><given-names>T.</given-names></name>
</person-group><source>Computer Vision&#x02013;ECCV 2014: 13th European Conference, Zurich, Switzerland, 6&#x02013;12 September 2014; Proceedings, Part I</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2014</year><volume>Volume 8689</volume></element-citation></ref><ref id="B14-sensors-25-01178"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Netzer</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>T.</given-names></name>
<name><surname>Coates</surname><given-names>A.</given-names></name>
<name><surname>Bissacco</surname><given-names>A.</given-names></name>
<name><surname>Wu</surname><given-names>B.</given-names></name>
<name><surname>Ng</surname><given-names>A.Y.</given-names></name>
</person-group><article-title>Reading digits in natural images with unsupervised feature learning</article-title><source>Proceedings of the NIPS Workshop on Deep Learning and Unsupervised Feature Learning</source><conf-loc>Granada, Spain</conf-loc><conf-date>15 December 2011</conf-date><volume>Volume 2011</volume><fpage>4</fpage></element-citation></ref><ref id="B15-sensors-25-01178"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zuo</surname><given-names>L.Q.</given-names></name>
<name><surname>Sun</surname><given-names>H.M.</given-names></name>
<name><surname>Mao</surname><given-names>Q.C.</given-names></name>
<name><surname>Qi</surname><given-names>R.</given-names></name>
<name><surname>Jia</surname><given-names>R.S.</given-names></name>
</person-group><article-title>Natural scene text recognition based on encoder-decoder framework</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>62616</fpage><lpage>62623</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2916616</pub-id></element-citation></ref><ref id="B16-sensors-25-01178"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Deng</surname><given-names>M.</given-names></name>
</person-group><article-title>Robot navigation based on multi-sensor fusion</article-title><source>J. Physics: Conf. Ser.</source><year>2023</year><volume>2580</volume><elocation-id>012020</elocation-id><pub-id pub-id-type="doi">10.1088/1742-6596/2580/1/012020</pub-id></element-citation></ref><ref id="B17-sensors-25-01178"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>He</surname><given-names>C.</given-names></name>
<name><surname>Ma</surname><given-names>R.</given-names></name>
<name><surname>Qu</surname><given-names>H.</given-names></name>
<name><surname>Gu</surname><given-names>R.</given-names></name>
</person-group><article-title>Research on Mobile Robot Positioning and Navigation System Based on Multi-sensor Fusion</article-title><source>J. Phys. Conf. Ser.</source><year>2020</year><volume>1684</volume><fpage>012011</fpage><pub-id pub-id-type="doi">10.1088/1742-6596/1684/1/012011</pub-id></element-citation></ref><ref id="B18-sensors-25-01178"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jin</surname><given-names>T.</given-names></name>
<name><surname>Lee</surname><given-names>J.</given-names></name>
<name><surname>Tso</surname><given-names>S.K.</given-names></name>
</person-group><article-title>A new approach using sensor data fusion for mobile robot navigation</article-title><source>Robotica</source><year>2004</year><volume>22</volume><fpage>51</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1017/S0263574703005381</pub-id></element-citation></ref><ref id="B19-sensors-25-01178"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Anjum</surname><given-names>M.L.</given-names></name>
<name><surname>Ahmad</surname><given-names>O.</given-names></name>
<name><surname>Bona</surname><given-names>B.</given-names></name>
<name><surname>&#x0201c;Dan&#x0201d; Cho</surname><given-names>D.i.</given-names></name>
</person-group><person-group person-group-type="editor">
<name><surname>Natraj</surname><given-names>A.</given-names></name>
<name><surname>Cameron</surname><given-names>S.</given-names></name>
<name><surname>Melhuish</surname><given-names>C.</given-names></name>
<name><surname>Witkowski</surname><given-names>M.</given-names></name>
</person-group><article-title>Sensor Data Fusion Using Unscented Kalman Filter for VOR-Based Vision Tracking System for Mobile Robots</article-title><source>Proceedings of the Towards Autonomous Robotic Systems</source><conf-loc>Oxford, UK</conf-loc><conf-date>28&#x02013;30 August 2013</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2014</year><fpage>103</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1007/978-3-662-43645-5_12</pub-id></element-citation></ref><ref id="B20-sensors-25-01178"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jin</surname><given-names>T.</given-names></name>
<name><surname>Kim</surname><given-names>H.</given-names></name>
<name><surname>Kim</surname><given-names>J.</given-names></name>
</person-group><article-title>Landmark detection based on sensor fusion for mobile robot navigation in a varying environment</article-title><source>Int. J. Fuzzy Log. Intell. Syst.</source><year>2010</year><volume>10</volume><fpage>281</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.5391/IJFIS.2010.10.4.281</pub-id></element-citation></ref><ref id="B21-sensors-25-01178"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nam</surname><given-names>D.</given-names></name>
<name><surname>Kim</surname><given-names>G.</given-names></name>
</person-group><article-title>Robust stereo visual inertial navigation system based on multi-stage outlier removal in dynamic environments</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>2922</elocation-id><pub-id pub-id-type="doi">10.3390/s20102922</pub-id><pub-id pub-id-type="pmid">32455697</pub-id>
</element-citation></ref><ref id="B22-sensors-25-01178"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qiao</surname><given-names>Y.</given-names></name>
<name><surname>Cappelle</surname><given-names>C.</given-names></name>
<name><surname>Ruichek</surname><given-names>Y.</given-names></name>
</person-group><article-title>Visual localization across seasons using sequence matching based on multi-feature combination</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>2442</elocation-id><pub-id pub-id-type="doi">10.3390/s17112442</pub-id><pub-id pub-id-type="pmid">29068358</pub-id>
</element-citation></ref><ref id="B23-sensors-25-01178"><label>23.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Martello</surname><given-names>C.</given-names></name>
</person-group><article-title>Nordland Dataset</article-title><comment>Available online: <ext-link xlink:href="https://www.kaggle.com/datasets/carlomartello/nordland" ext-link-type="uri">https://www.kaggle.com/datasets/carlomartello/nordland</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-13">(accessed on 13 December 2024)</date-in-citation></element-citation></ref><ref id="B24-sensors-25-01178"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Concei&#x000e7;&#x000e3;o</surname><given-names>T.</given-names></name>
<name><surname>Neves dos Santos</surname><given-names>F.</given-names></name>
<name><surname>Costa</surname><given-names>P.</given-names></name>
<name><surname>Moreira</surname><given-names>A.P.</given-names></name>
</person-group><article-title>Robot localization system in a hard outdoor environment</article-title><source>Proceedings of the ROBOT 2017: Third Iberian Robotics Conference: Volume 1</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2018</year><fpage>215</fpage><lpage>227</lpage></element-citation></ref><ref id="B25-sensors-25-01178"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sinha</surname><given-names>H.</given-names></name>
<name><surname>Patrikar</surname><given-names>J.</given-names></name>
<name><surname>Dhekane</surname><given-names>E.G.</given-names></name>
<name><surname>Pandey</surname><given-names>G.</given-names></name>
<name><surname>Kothari</surname><given-names>M.</given-names></name>
</person-group><article-title>Convolutional neural network based sensors for mobile robot relocalization</article-title><source>Proceedings of the 2018 23rd International Conference on Methods &#x00026; Models in Automation &#x00026; Robotics (MMAR)</source><conf-loc>Miedzyzdroje, Poland</conf-loc><conf-date>27&#x02013;30 August 2018</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway Township, NJ, USA</publisher-loc><year>2018</year><fpage>774</fpage><lpage>779</lpage></element-citation></ref><ref id="B26-sensors-25-01178"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yousuf</surname><given-names>S.</given-names></name>
<name><surname>Kadri</surname><given-names>M.B.</given-names></name>
</person-group><article-title>Robot localization in indoor and outdoor environments by multi-sensor fusion</article-title><source>Proceedings of the 2018 14th International Conference on Emerging Technologies (ICET)</source><conf-loc>Islamabad, Pakistan</conf-loc><conf-date>21&#x02013;22 November 2018</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway Township, NJ, USA</publisher-loc><year>2018</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B27-sensors-25-01178"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Cai</surname><given-names>G.S.</given-names></name>
<name><surname>Lin</surname><given-names>H.Y.</given-names></name>
<name><surname>Kao</surname><given-names>S.F.</given-names></name>
</person-group><article-title>Mobile robot localization using gps, imu and visual odometry</article-title><source>Proceedings of the 2019 International Automatic Control Conference (CACS)</source><conf-loc>Keelung, Taiwan</conf-loc><conf-date>13&#x02013;16 November 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway Township, NJ, USA</publisher-loc><year>2019</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B28-sensors-25-01178"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Al Khatib</surname><given-names>E.I.</given-names></name>
<name><surname>Jaradat</surname><given-names>M.A.K.</given-names></name>
<name><surname>Abdel-Hafez</surname><given-names>M.F.</given-names></name>
</person-group><article-title>Low-cost reduced navigation system for mobile robot in indoor/outdoor environments</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>25014</fpage><lpage>25026</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.2971169</pub-id></element-citation></ref><ref id="B29-sensors-25-01178"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nilwong</surname><given-names>S.</given-names></name>
<name><surname>Hossain</surname><given-names>D.</given-names></name>
<name><surname>Kaneko</surname><given-names>S.I.</given-names></name>
<name><surname>Capi</surname><given-names>G.</given-names></name>
</person-group><article-title>Deep learning-based landmark detection for mobile robot outdoor localization</article-title><source>Machines</source><year>2019</year><volume>7</volume><elocation-id>25</elocation-id><pub-id pub-id-type="doi">10.3390/machines7020025</pub-id></element-citation></ref><ref id="B30-sensors-25-01178"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Shi</surname><given-names>Q.</given-names></name>
<name><surname>Lu</surname><given-names>Q.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<name><surname>Lin</surname><given-names>Z.</given-names></name>
</person-group><article-title>Learning invariant semantic representation for long-term robust visual localization</article-title><source>Eng. Appl. Artif. Intell.</source><year>2022</year><volume>111</volume><fpage>104793</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2022.104793</pub-id></element-citation></ref><ref id="B31-sensors-25-01178"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cabon</surname><given-names>Y.</given-names></name>
<name><surname>Murray</surname><given-names>N.</given-names></name>
<name><surname>Humenberger</surname><given-names>M.</given-names></name>
</person-group><article-title>Virtual kitti 2</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2001.10773</pub-id></element-citation></ref><ref id="B32-sensors-25-01178"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gaidon</surname><given-names>A.</given-names></name>
<name><surname>Wang</surname><given-names>Q.</given-names></name>
<name><surname>Cabon</surname><given-names>Y.</given-names></name>
<name><surname>Vig</surname><given-names>E.</given-names></name>
</person-group><article-title>Virtual worlds as proxy for multi-object tracking analysis</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>4340</fpage><lpage>4349</lpage></element-citation></ref><ref id="B33-sensors-25-01178"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Piasco</surname><given-names>N.</given-names></name>
<name><surname>Sidib&#x000e9;</surname><given-names>D.</given-names></name>
<name><surname>Gouet-Brunet</surname><given-names>V.</given-names></name>
<name><surname>Demonceaux</surname><given-names>C.</given-names></name>
</person-group><article-title>Improving image description with auxiliary modality for visual localization in challenging conditions</article-title><source>Int. J. Comput. Vis.</source><year>2021</year><volume>129</volume><fpage>185</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1007/s11263-020-01363-6</pub-id></element-citation></ref><ref id="B34-sensors-25-01178"><label>34.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Institute</surname><given-names>O.R.</given-names></name>
</person-group><article-title>RobotCar Dataset</article-title><comment>Available online: <ext-link xlink:href="https://robotcar-dataset.robots.ox.ac.uk/" ext-link-type="uri">https://robotcar-dataset.robots.ox.ac.uk/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-13">(accessed on 13 December 2024)</date-in-citation></element-citation></ref><ref id="B35-sensors-25-01178"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bansal</surname><given-names>A.</given-names></name>
<name><surname>Badino</surname><given-names>H.</given-names></name>
<name><surname>Huber</surname><given-names>D.</given-names></name>
</person-group><article-title>Understanding how camera configuration and environmental conditions affect appearance-based localization</article-title><source>Proceedings of the 2014 IEEE Intelligent Vehicles Symposium Proceedings</source><conf-loc>Ypsilanti, MI, USA</conf-loc><conf-date>8&#x02013;11 June 2014</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway Township, NJ, USA</publisher-loc><year>2014</year><fpage>800</fpage><lpage>807</lpage></element-citation></ref><ref id="B36-sensors-25-01178"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>N.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Fan</surname><given-names>G.</given-names></name>
<name><surname>Yang</surname><given-names>D.</given-names></name>
<name><surname>Rao</surname><given-names>L.</given-names></name>
</person-group><article-title>An End-to-End Robotic Visual Localization Algorithm Based on Deep Learning</article-title><source>J. Sens.</source><year>2023</year><volume>2023</volume><fpage>2396911</fpage><pub-id pub-id-type="doi">10.1155/2023/2396911</pub-id></element-citation></ref><ref id="B37-sensors-25-01178"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kendall</surname><given-names>A.</given-names></name>
<name><surname>Grimes</surname><given-names>M.</given-names></name>
<name><surname>Cipolla</surname><given-names>R.</given-names></name>
</person-group><article-title>Posenet: A convolutional network for real-time 6-dof camera relocalization</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Santiago, Chile</conf-loc><conf-date>7&#x02013;13 December 2015</conf-date><fpage>2938</fpage><lpage>2946</lpage></element-citation></ref><ref id="B38-sensors-25-01178"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Shotton</surname><given-names>J.</given-names></name>
<name><surname>Glocker</surname><given-names>B.</given-names></name>
<name><surname>Zach</surname><given-names>C.</given-names></name>
<name><surname>Izadi</surname><given-names>S.</given-names></name>
<name><surname>Criminisi</surname><given-names>A.</given-names></name>
<name><surname>Fitzgibbon</surname><given-names>A.</given-names></name>
</person-group><article-title>Scene coordinate regression forests for camera relocalization in RGB-D images</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Portland, OR, USA</conf-loc><conf-date>23&#x02013;28 June 2013</conf-date><fpage>2930</fpage><lpage>2937</lpage></element-citation></ref><ref id="B39-sensors-25-01178"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sturm</surname><given-names>J.</given-names></name>
<name><surname>Engelhard</surname><given-names>N.</given-names></name>
<name><surname>Endres</surname><given-names>F.</given-names></name>
<name><surname>Burgard</surname><given-names>W.</given-names></name>
<name><surname>Cremers</surname><given-names>D.</given-names></name>
</person-group><article-title>A benchmark for the evaluation of RGB-D SLAM systems</article-title><source>Proceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</source><conf-loc>Vilamoura-Algarve, Portugal</conf-loc><conf-date>7&#x02013;12 October 2012</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway Township, NJ, USA</publisher-loc><year>2012</year><fpage>573</fpage><lpage>580</lpage></element-citation></ref><ref id="B40-sensors-25-01178"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Naseer</surname><given-names>T.</given-names></name>
<name><surname>Burgard</surname><given-names>W.</given-names></name>
<name><surname>Stachniss</surname><given-names>C.</given-names></name>
</person-group><article-title>Robust visual localization across seasons</article-title><source>IEEE Trans. Robot.</source><year>2018</year><volume>34</volume><fpage>289</fpage><lpage>302</lpage><pub-id pub-id-type="doi">10.1109/TRO.2017.2788045</pub-id></element-citation></ref><ref id="B41-sensors-25-01178"><label>41.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Logitech</collab>
</person-group><article-title>C922 Pro Stream Webcam</article-title><comment>Available online: <ext-link xlink:href="https://www.logitech.com/en-sg/products/webcams/c922-pro-stream-webcam.960-001090.html" ext-link-type="uri">https://www.logitech.com/en-sg/products/webcams/c922-pro-stream-webcam.960-001090.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-13">(accessed on 13 December 2024)</date-in-citation></element-citation></ref><ref id="B42-sensors-25-01178"><label>42.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>U-blox</collab>
</person-group><article-title>NEO-6 DataSheet</article-title><comment>Available online: <ext-link xlink:href="https://content.u-blox.com/sites/default/files/products/documents/NEO-6_DataSheet_%28GPS.G6-HW-09005%29.pdf" ext-link-type="uri">https://content.u-blox.com/sites/default/files/products/documents/NEO-6_DataSheet_%28GPS.G6-HW-09005%29.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-13">(accessed on 13 December 2024)</date-in-citation></element-citation></ref><ref id="B43-sensors-25-01178"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cao</surname><given-names>D.</given-names></name>
<name><surname>Zhong</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
<name><surname>Dang</surname><given-names>J.</given-names></name>
</person-group><article-title>Scene text detection in natural images: A review</article-title><source>Symmetry</source><year>2020</year><volume>12</volume><elocation-id>1956</elocation-id><pub-id pub-id-type="doi">10.3390/sym12121956</pub-id></element-citation></ref><ref id="B44-sensors-25-01178"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>X.</given-names></name>
<name><surname>Yao</surname><given-names>C.</given-names></name>
<name><surname>Wen</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Zhou</surname><given-names>S.</given-names></name>
<name><surname>He</surname><given-names>W.</given-names></name>
<name><surname>Liang</surname><given-names>J.</given-names></name>
</person-group><article-title>East: An efficient and accurate scene text detector</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;26 July 2017</conf-date><fpage>5551</fpage><lpage>5560</lpage></element-citation></ref><ref id="B45-sensors-25-01178"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shi</surname><given-names>B.</given-names></name>
<name><surname>Bai</surname><given-names>X.</given-names></name>
<name><surname>Yao</surname><given-names>C.</given-names></name>
</person-group><article-title>An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2016</year><volume>39</volume><fpage>2298</fpage><lpage>2304</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2646371</pub-id><pub-id pub-id-type="pmid">28055850</pub-id>
</element-citation></ref><ref id="B46-sensors-25-01178"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep residual learning for image recognition</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="B47-sensors-25-01178"><label>47.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Hochreiter</surname><given-names>S.</given-names></name>
</person-group><source>Long Short-Term Memory</source><publisher-name>Neural Computation MIT-Press</publisher-name><publisher-loc>La Jolla, CA, USA</publisher-loc><year>1997</year></element-citation></ref><ref id="B48-sensors-25-01178"><label>48.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Graves</surname><given-names>A.</given-names></name>
<name><surname>Fern&#x000e1;ndez</surname><given-names>S.</given-names></name>
<name><surname>Gomez</surname><given-names>F.</given-names></name>
<name><surname>Schmidhuber</surname><given-names>J.</given-names></name>
</person-group><article-title>Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</article-title><source>Proceedings of the 23rd International Conference on Machine Learning</source><conf-loc>Pittsburgh, PA, USA</conf-loc><conf-date>25&#x02013;29 June 2006</conf-date><fpage>369</fpage><lpage>376</lpage></element-citation></ref><ref id="B49-sensors-25-01178"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alimovski</surname><given-names>E.</given-names></name>
<name><surname>Erdemir</surname><given-names>G.</given-names></name>
<name><surname>Kuzucuoglu</surname><given-names>A.E.</given-names></name>
</person-group><article-title>Text Detection and Recognition in Natural Scenes by Mobile Robot</article-title><source>Eur. J. Tech. (EJT)</source><year>2024</year><volume>14</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.36222/ejt.1407231</pub-id></element-citation></ref><ref id="B50-sensors-25-01178"><label>50.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Foundation</surname><given-names>P.S.</given-names></name>
</person-group><article-title>difflib &#x02014; Helpers for Computing Deltas</article-title><comment>Available online: <ext-link xlink:href="https://docs.python.org/3/library/difflib.html" ext-link-type="uri">https://docs.python.org/3/library/difflib.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-13">(accessed on 13 December 2024)</date-in-citation></element-citation></ref><ref id="B51-sensors-25-01178"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ratcliff</surname><given-names>J.W.</given-names></name>
<name><surname>Metzener</surname><given-names>D.E.</given-names></name>
</person-group><article-title>Pattern matching: The gestalt approach</article-title><source>Dr. Dobb&#x02019;s J.</source><year>1988</year><volume>13</volume><fpage>46</fpage></element-citation></ref><ref id="B52-sensors-25-01178"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>K.</given-names></name>
<name><surname>Hu</surname><given-names>Y.</given-names></name>
<name><surname>Ma</surname><given-names>Y.</given-names></name>
<name><surname>Zhou</surname><given-names>R.Z.</given-names></name>
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Conflating point of interest (POI) data: A systematic review of matching methods</article-title><source>Comput. Environ. Urban Syst.</source><year>2023</year><volume>103</volume><fpage>101977</fpage><pub-id pub-id-type="doi">10.1016/j.compenvurbsys.2023.101977</pub-id></element-citation></ref><ref id="B53-sensors-25-01178"><label>53.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Developers</surname><given-names>G.</given-names></name>
</person-group><article-title>Google Maps Places API Documentation</article-title><comment>Available online: <ext-link xlink:href="https://developers.google.com/maps/documentation/places/web-service" ext-link-type="uri">https://developers.google.com/maps/documentation/places/web-service</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-13">(accessed on 13 December 2024)</date-in-citation></element-citation></ref><ref id="B54-sensors-25-01178"><label>54.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Developers</surname><given-names>G.</given-names></name>
</person-group><article-title>Google Maps Places API: Nearby Search Documentation</article-title><comment>Available online: <ext-link xlink:href="https://developers.google.com/maps/documentation/places/web-service/nearby-search" ext-link-type="uri">https://developers.google.com/maps/documentation/places/web-service/nearby-search</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-13">(accessed on 13 December 2024)</date-in-citation></element-citation></ref><ref id="B55-sensors-25-01178"><label>55.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Mapbox</collab>
</person-group><article-title>Mapbox: Maps and Location for Developers</article-title><comment>Available online: <ext-link xlink:href="https://www.mapbox.com/" ext-link-type="uri">https://www.mapbox.com/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-13">(accessed on 13 December 2024)</date-in-citation></element-citation></ref><ref id="B56-sensors-25-01178"><label>56.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Mapbox</collab>
</person-group><article-title>Mapbox Styles API Documentation</article-title><comment>Available online: <ext-link xlink:href="https://docs.mapbox.com/api/maps/styles/" ext-link-type="uri">https://docs.mapbox.com/api/maps/styles/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-13">(accessed on 13 December 2024)</date-in-citation></element-citation></ref><ref id="B57-sensors-25-01178"><label>57.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Inman</surname><given-names>J.</given-names></name>
</person-group><source>Navigation and Nautical Astronomy: For the Use of British Seamen</source><publisher-name>F. and J. Rivington</publisher-name><publisher-loc>St. Paul, MN, USA</publisher-loc><year>1849</year></element-citation></ref><ref id="B58-sensors-25-01178"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sinnott</surname><given-names>R.W.</given-names></name>
</person-group><article-title>Virtues of the Haversine</article-title><source>Sky Telesc.</source><year>1984</year><volume>68</volume><fpage>158</fpage></element-citation></ref><ref id="B59-sensors-25-01178"><label>59.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kaya</surname><given-names>O.M.T.</given-names></name>
<name><surname>Erdemir</surname><given-names>G.</given-names></name>
</person-group><article-title>Design of an Eight-Wheeled Mobile Delivery Robot and Its Climbing Simulations</article-title><source>Proceedings of the SoutheastCon 2023</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>1&#x02013;16 April 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway Township, NJ, USA</publisher-loc><year>2023</year><fpage>895</fpage><lpage>900</lpage></element-citation></ref><ref id="B60-sensors-25-01178"><label>60.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Vishal</surname><given-names>K.</given-names></name>
<name><surname>Jawahar</surname><given-names>C.</given-names></name>
<name><surname>Chari</surname><given-names>V.</given-names></name>
</person-group><article-title>Accurate localization by fusing images and GPS signals</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#x02013;12 June 2015</conf-date><fpage>17</fpage><lpage>24</lpage></element-citation></ref><ref id="B61-sensors-25-01178"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Correa</surname><given-names>J.</given-names></name>
<name><surname>Soto</surname><given-names>A.</given-names></name>
</person-group><article-title>Active visual perception for mobile robot localization</article-title><source>J. Intell. Robot. Syst.</source><year>2010</year><volume>58</volume><fpage>339</fpage><lpage>354</lpage><pub-id pub-id-type="doi">10.1007/s10846-009-9348-4</pub-id></element-citation></ref><ref id="B62-sensors-25-01178"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chiang</surname><given-names>K.W.</given-names></name>
<name><surname>Huang</surname><given-names>Y.W.</given-names></name>
</person-group><article-title>An intelligent navigator for seamless INS/GPS integrated land vehicle navigation applications</article-title><source>Appl. Soft Comput.</source><year>2008</year><volume>8</volume><fpage>722</fpage><lpage>733</lpage><pub-id pub-id-type="doi">10.1016/j.asoc.2007.05.010</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01178-f001"><label>Figure 1</label><caption><p>Problem illustration.</p></caption><graphic xlink:href="sensors-25-01178-g001" position="float"/></fig><fig position="float" id="sensors-25-01178-f002"><label>Figure 2</label><caption><p>Flowchart of the proposed system.</p></caption><graphic xlink:href="sensors-25-01178-g002" position="float"/></fig><fig position="float" id="sensors-25-01178-f003"><label>Figure 3</label><caption><p>Architecture of EAST algorithm.</p></caption><graphic xlink:href="sensors-25-01178-g003" position="float"/></fig><fig position="float" id="sensors-25-01178-f004"><label>Figure 4</label><caption><p>Architecture of EASY OCR algorithm.</p></caption><graphic xlink:href="sensors-25-01178-g004" position="float"/></fig><fig position="float" id="sensors-25-01178-f005"><label>Figure 5</label><caption><p>A view of the robotic platform from the laboratory environment.</p></caption><graphic xlink:href="sensors-25-01178-g005" position="float"/></fig><fig position="float" id="sensors-25-01178-f006"><label>Figure 6</label><caption><p>Robot samples for each Area. (<bold>a</bold>) Test Area 1, (<bold>b</bold>) Test Area 2, (<bold>c</bold>) Test Area 3.</p></caption><graphic xlink:href="sensors-25-01178-g006" position="float"/></fig><fig position="float" id="sensors-25-01178-f007"><label>Figure 7</label><caption><p>Start and end points of the robot for Test Area 1.</p></caption><graphic xlink:href="sensors-25-01178-g007" position="float"/></fig><fig position="float" id="sensors-25-01178-f008"><label>Figure 8</label><caption><p>Detected and recognized business names by the proposed system in Scenario 1. (<bold>a</bold>) &#x0201c;alBaraka&#x0201d;, (<bold>b</bold>) &#x0201c;A-101&#x0201d;, (<bold>c</bold>) &#x0201c;Yap&#x00131;Kredi&#x0201d;, (<bold>d</bold>) &#x0201c;PTT&#x0201d;, (<bold>e</bold>) &#x0201c;HALKBANK&#x0201d;, (<bold>f</bold>) &#x0201c;Vak&#x00131;fBank&#x0201d;, (<bold>g</bold>) &#x0201c;KUVEYTT&#x000dc;RK&#x0201d;, (<bold>h</bold>) &#x0201c;Ziraat Bankas&#x00131;&#x0201d;.</p></caption><graphic xlink:href="sensors-25-01178-g008" position="float"/></fig><fig position="float" id="sensors-25-01178-f009"><label>Figure 9</label><caption><p>Representation of GPS coordinates and generated coordinates on map in Scenario 1. (<bold>a</bold>) &#x0201c;alBaraka&#x0201d;, (<bold>b</bold>) &#x0201c;A-101&#x0201d;, (<bold>c</bold>) &#x0201c;Yap&#x00131;Kredi&#x0201d;, (<bold>d</bold>) &#x0201c;PTT&#x0201d;, (<bold>e</bold>) &#x0201c;HALKBANK&#x0201d;, (<bold>f</bold>) &#x0201c;Vak&#x00131;fBank&#x0201d;, (<bold>g</bold>) &#x0201c;KUVEYTT&#x000dc;RK&#x0201d;, (<bold>h</bold>) &#x0201c;Ziraat Bankas&#x00131;&#x0201d;.</p></caption><graphic xlink:href="sensors-25-01178-g009" position="float"/></fig><fig position="float" id="sensors-25-01178-f010"><label>Figure 10</label><caption><p>Detected and recognized business names by the proposed system in Scenario 2. (<bold>a</bold>) &#x0201c;Ziraat Bankas&#x00131;&#x0201d;, (<bold>b</bold>) &#x0201c;KUVEYTT&#x000dc;RK&#x0201d;, (<bold>c</bold>) &#x0201c;Vak&#x00131;fBank&#x0201d;, (<bold>d</bold>) &#x0201c;HALKBANK&#x0201d;, (<bold>e</bold>) &#x0201c;PTT&#x0201d;, (<bold>f</bold>) &#x0201c;Yap&#x00131;Kredi&#x0201d;, (<bold>g</bold>) &#x0201c;A-101&#x0201d;, (<bold>h</bold>) &#x0201c;alBaraka&#x0201d;.</p></caption><graphic xlink:href="sensors-25-01178-g010" position="float"/></fig><fig position="float" id="sensors-25-01178-f011"><label>Figure 11</label><caption><p>Representation of GPS coordinates and generated coordinates on map in Scenario 2. (<bold>a</bold>) &#x0201c;Ziraat Bankas&#x00131;&#x0201d;, (<bold>b</bold>) &#x0201c;KUVEYTT&#x000dc;RK&#x0201d;, (<bold>c</bold>) &#x0201c;Vak&#x00131;fBank&#x0201d;, (<bold>d</bold>) &#x0201c;HALKBANK&#x0201d;, (<bold>e</bold>) &#x0201c;PTT&#x0201d;, (<bold>f</bold>) &#x0201c;Yap&#x00131;Kredi&#x0201d;, (<bold>g</bold>) &#x0201c;A-101&#x0201d;, (<bold>h</bold>) &#x0201c;alBaraka&#x0201d;.</p></caption><graphic xlink:href="sensors-25-01178-g011" position="float"/></fig><fig position="float" id="sensors-25-01178-f012"><label>Figure 12</label><caption><p>Start and end points of the robot for Test Area 2.</p></caption><graphic xlink:href="sensors-25-01178-g012" position="float"/></fig><fig position="float" id="sensors-25-01178-f013"><label>Figure 13</label><caption><p>Detected and recognized business names by the proposed system in Scenario 3. (<bold>a</bold>) &#x0201c;Atasun Optik&#x0201d;, (<bold>b</bold>) &#x0201c;ERCAN BURGER&#x0201d;, (<bold>c</bold>) &#x0201c;GLORIA PARFUME&#x0201d;, (<bold>d</bold>) &#x0201c;Metin&#x0201d;, (<bold>e</bold>) &#x0201c;Tavuk D&#x000fc;nyas&#x00131;&#x0201d;, (<bold>f</bold>) &#x0201c;BURGER KING&#x0201d;, (<bold>g</bold>) &#x0201c;watsons&#x0201d;.</p></caption><graphic xlink:href="sensors-25-01178-g013" position="float"/></fig><fig position="float" id="sensors-25-01178-f014"><label>Figure 14</label><caption><p>Display of GPS coordinates and generated coordinates on Mapbox during Scenario 3. (<bold>a</bold>) &#x0201c;Atasun Optik&#x0201d;, (<bold>b</bold>) &#x0201c;ERCAN BURGER&#x0201d;, (<bold>c</bold>) &#x0201c;GLORIA PARFUME&#x0201d;, (<bold>d</bold>) &#x0201c;Metin&#x0201d;, (<bold>e</bold>) &#x0201c;Tavuk D&#x000fc;nyas&#x00131;&#x0201d;, (<bold>f</bold>) &#x0201c;BURGER KING&#x0201d;, (<bold>g</bold>) &#x0201c;watsons&#x0201d;.</p></caption><graphic xlink:href="sensors-25-01178-g014" position="float"/></fig><fig position="float" id="sensors-25-01178-f015"><label>Figure 15</label><caption><p>Detected and recognized business names by the proposed system during Scenario 4. (<bold>a</bold>) &#x0201c;watsons&#x0201d;, (<bold>b</bold>) &#x0201c;BURGER KING&#x0201d;, (<bold>c</bold>) &#x0201c;Tavuk D&#x000fc;nyas&#x00131;&#x0201d;, (<bold>d</bold>) &#x0201c;Metin&#x0201d;, (<bold>e</bold>) &#x0201c;GLORIA PERFUME&#x0201d;, (<bold>f</bold>) &#x0201c;ERCAN BURGER&#x0201d;, (<bold>g</bold>) &#x0201c;Atasun Optik&#x0201d;.</p></caption><graphic xlink:href="sensors-25-01178-g015" position="float"/></fig><fig position="float" id="sensors-25-01178-f016"><label>Figure 16</label><caption><p>Display of GPS coordinates and generated coordinates on Mapbox during Scenario 4. (<bold>a</bold>) &#x0201c;watsons&#x0201d;, (<bold>b</bold>) &#x0201c;BURGER KING&#x0201d;, (<bold>c</bold>) &#x0201c;Tavuk D&#x000fc;nyas&#x00131;&#x0201d;, (<bold>d</bold>) &#x0201c;Metin&#x0201d;, (<bold>e</bold>) &#x0201c;GLORIA PERFUME&#x0201d;, (<bold>f</bold>) &#x0201c;ERCAN BURGER&#x0201d;, (<bold>g</bold>) &#x0201c;Atasun Optik&#x0201d;.</p></caption><graphic xlink:href="sensors-25-01178-g016" position="float"/></fig><fig position="float" id="sensors-25-01178-f017"><label>Figure 17</label><caption><p>Start and end points of the robot for Test Area 3.</p></caption><graphic xlink:href="sensors-25-01178-g017" position="float"/></fig><fig position="float" id="sensors-25-01178-f018"><label>Figure 18</label><caption><p>Detected and recognized business names by the proposed system during Scenario 5. (<bold>a</bold>) &#x0201c;G&#x00026;O VIP&#x0201d;, (<bold>b</bold>) &#x0201c;Maydonoz&#x0201d;, (<bold>c</bold>) &#x0201c;Starbucks&#x0201d;, (<bold>d</bold>) &#x0201c;Lotus Eczane&#x0201d;, (<bold>e</bold>) &#x0201c;Halis D&#x000fc;r&#x000fc;m&#x0201d;.</p></caption><graphic xlink:href="sensors-25-01178-g018" position="float"/></fig><fig position="float" id="sensors-25-01178-f019"><label>Figure 19</label><caption><p>Display of GPS coordinates and generated coordinates on Mapbox during Scenario 5. (<bold>a</bold>) &#x0201c;G&#x00026;O VIP&#x0201d;, (<bold>b</bold>) &#x0201c;Maydonoz&#x0201d;, (<bold>c</bold>) &#x0201c;Starbucks&#x0201d;, (<bold>d</bold>) &#x0201c;Lotus Eczane&#x0201d;, (<bold>e</bold>) &#x0201c;Halis D&#x000fc;r&#x000fc;m&#x0201d;.</p></caption><graphic xlink:href="sensors-25-01178-g019" position="float"/></fig><table-wrap position="float" id="sensors-25-01178-t001"><object-id pub-id-type="pii">sensors-25-01178-t001_Table 1</object-id><label>Table 1</label><caption><p>Businesses and their addresses in Test Area 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Address</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">alBaraka</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, Barbaros Cd. No: 73/B, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A101</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, Barbaros Cd. No: 73, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yap&#x00131;Kredi</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, Barbaros Cd. No: 71, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ptt</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, Barbaros Cd. 69B, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HALKBANK</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, Barbaros Cd. No: 69, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Vak&#x00131;fBank</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, Barbaros Cd. No: 65B, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KuveytT&#x000fc;rk</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet Barbaros Cd. No: 65/A, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ziraat</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, Barbaros Cd. No: 63A, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t002"><object-id pub-id-type="pii">sensors-25-01178-t002_Table 2</object-id><label>Table 2</label><caption><p>Recognized and corrected labels with the similarity rate in Scenario 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recognized Label</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Corrected Label with Sequence Matcher</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Word Similarity <break/>
Rate (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">alBaraka</td><td align="center" valign="middle" rowspan="1" colspan="1">alBaraka</td><td align="center" valign="middle" rowspan="1" colspan="1">alBaraka</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A-101</td><td align="center" valign="middle" rowspan="1" colspan="1">A-101</td><td align="center" valign="middle" rowspan="1" colspan="1">A-101</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Yap&#x00131;Kredi</td><td align="center" valign="middle" rowspan="1" colspan="1">Yap&#x00131;Kredi</td><td align="center" valign="middle" rowspan="1" colspan="1">Yap&#x00131;Kredi</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PTT</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HALKBANK</td><td align="center" valign="middle" rowspan="1" colspan="1">HAIKBANK</td><td align="center" valign="middle" rowspan="1" colspan="1">HALKBANK</td><td align="center" valign="middle" rowspan="1" colspan="1">87.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Vak&#x00131;fBank</td><td align="center" valign="middle" rowspan="1" colspan="1">Vak&#x00131;f</td><td align="center" valign="middle" rowspan="1" colspan="1">Vak&#x00131;f</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KUVEYTT&#x000dc;RK</td><td align="center" valign="middle" rowspan="1" colspan="1">KuUEYT</td><td align="center" valign="middle" rowspan="1" colspan="1">KUVEYT</td><td align="center" valign="middle" rowspan="1" colspan="1">83.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ziraat Bankas&#x00131;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7iraat</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ziraat</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t003"><object-id pub-id-type="pii">sensors-25-01178-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of the GPS and generated coordinate in Scenario 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name<break/>
(Label)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GPS Coordinate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Generated Coordinate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Verification with<break/>
Google Map</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Distance Error<break/>
(m)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">alBaraka</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.991123<break/>
long:28.778287</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9910718<break/>
long:28.778255</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.05</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A101</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.991047<break/>
long:28.778276</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9910718<break/>
long:28.7782551</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.27</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yap&#x00131;Kredi</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.990872<break/>
long:28.778350</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9908774<break/>
long:28.7783372</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.23</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PTT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.990768<break/>
long:28.778468</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HALKBANK</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.990647<break/>
long:28.778462</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9906612<break/>
long:28.7784283</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.24</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Vak&#x00131;fBank</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.990503<break/>
long:28.778564</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9904952<break/>
long:28.7784983</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.67</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KUVEYT&#x000dc;RK</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.990434<break/>
long:28.778574</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9904682<break/>
long:28.7785097</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.49</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ziraat Bankas&#x00131;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.99005 <break/>
long:28.778730</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.990093 <break/>
long:28.778666</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.43</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t004"><object-id pub-id-type="pii">sensors-25-01178-t004_Table 4</object-id><label>Table 4</label><caption><p>Recognized and corrected labels with the similarity rate in Scenario 2.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recognized Label</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Corrected Label with Sequence Matcher</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Word Similarity Rate (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ziraat Bankas&#x00131;</td><td align="center" valign="middle" rowspan="1" colspan="1">iraa</td><td align="center" valign="middle" rowspan="1" colspan="1">Ziraat</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KUVEYTT&#x000dc;RK</td><td align="center" valign="middle" rowspan="1" colspan="1">KUUEYT</td><td align="center" valign="middle" rowspan="1" colspan="1">KUVEYT</td><td align="center" valign="middle" rowspan="1" colspan="1">83.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Vak&#x00131;fBank</td><td align="center" valign="middle" rowspan="1" colspan="1">ak&#x00131;fB</td><td align="center" valign="middle" rowspan="1" colspan="1">Vak&#x00131;f</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HALKBANK</td><td align="center" valign="middle" rowspan="1" colspan="1">IHALKBA</td><td align="center" valign="middle" rowspan="1" colspan="1">HALKBANK</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PTT</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Yap&#x00131;Kredi</td><td align="center" valign="middle" rowspan="1" colspan="1">Tap&#x00131;kredi</td><td align="center" valign="middle" rowspan="1" colspan="1">Yap&#x00131;Kredi</td><td align="center" valign="middle" rowspan="1" colspan="1">88.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A-101</td><td align="center" valign="middle" rowspan="1" colspan="1">A-101</td><td align="center" valign="middle" rowspan="1" colspan="1">A-101</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">alBaraka</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">alBaraka</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">alBaraka</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t005"><object-id pub-id-type="pii">sensors-25-01178-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of the GPS and generated coordinates in Scenario 2.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name (Label)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GPS Coordinate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Generated Coordinate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Verification with Google Map</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Distance Difference (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ziraat Bankas&#x00131;</td><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">lat:40.991024803056135<break/>
long:28.7783132543341</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9902392<break/>
long:28.7786062</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.75</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KUVEYTT&#x000dc;RK</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9904682<break/>
long:28.7785097</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.05</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Vak&#x00131;fBank</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9904952<break/>
long:28.7784983</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.90</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HALKBANK</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9906612<break/>
long:28.7784283</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.57</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PTT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yap&#x00131;Kredi</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9908774<break/>
long:28.7783372</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.51</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A101</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9910718<break/>
long:28.7782551</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.15</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">alBaraka</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9910718<break/>
long:28.7782551</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.15</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t006"><object-id pub-id-type="pii">sensors-25-01178-t006_Table 6</object-id><label>Table 6</label><caption><p>Businesses and corresponding addresses in Test Area 2.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Address</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Watsons</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, H&#x000fc;rriyet St. No: 62/B, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Burger King</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, H&#x000fc;rriyet St. No: 60, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tavuk D&#x000fc;nyas&#x00131;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, H&#x000fc;rriyet St. No: 56C, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Metin Kuyumculuk</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, H&#x000fc;rriyet St. No: 56, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GLORIA PERFUME</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, H&#x000fc;rriyet St. No: 54/A, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ERCAN BURGER</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, H&#x000fc;rriyet St. No: 52/C, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Atasun Optik</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cennet, H&#x000fc;rriyet St. No: 52/B, 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t007"><object-id pub-id-type="pii">sensors-25-01178-t007_Table 7</object-id><label>Table 7</label><caption><p>Recognized and corrected labels with the similarity rate in Scenario 3.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recognized Label</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Corrected Label with Sequence Matcher</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Word Similarity Rate (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Atasun Optik</td><td align="center" valign="middle" rowspan="1" colspan="1">Atasun</td><td align="center" valign="middle" rowspan="1" colspan="1">Atasun</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ERCAN BURGER</td><td align="center" valign="middle" rowspan="1" colspan="1">ERCA</td><td align="center" valign="middle" rowspan="1" colspan="1">ERCAN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GLORIA PERFUME</td><td align="center" valign="middle" rowspan="1" colspan="1">GLORIA PERFUME</td><td align="center" valign="middle" rowspan="1" colspan="1">GLORIA PERFUME</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Metin</td><td align="center" valign="middle" rowspan="1" colspan="1">Jetin</td><td align="center" valign="middle" rowspan="1" colspan="1">Metin</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Tavuk D&#x000fc;nyas&#x00131;</td><td align="center" valign="middle" rowspan="1" colspan="1">Tavuk</td><td align="center" valign="middle" rowspan="1" colspan="1">Tavuk</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BURGER KING</td><td align="center" valign="middle" rowspan="1" colspan="1">KING</td><td align="center" valign="middle" rowspan="1" colspan="1">KING</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">watsons</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">atsons</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">watsons</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.6</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t008"><object-id pub-id-type="pii">sensors-25-01178-t008_Table 8</object-id><label>Table 8</label><caption><p>Comparison of the GPS and generated coordinates in Scenario 3.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name (Label)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GPS Coordinate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Generated Coordinate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Verification with Google Map</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Distance Difference (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Atasun Optik</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.989670<break/>
long:28.780326</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9900175<break/>
long:28.7800944</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.25</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ERCAN BURGER</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.989714<break/>
long:28.780182</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.98992640000001<break/>
long:28.7804342</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.72</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GLORIA PERFUME</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.989914<break/>
long:28.780121</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9900573<break/>
long:28.780811</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.28</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Metin</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.989959<break/>
long:28.779832</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9900289<break/>
long:28.7801479</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.63</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tavuk D&#x000fc;nyas&#x00131;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.990104<break/>
long:28.780083</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9900417<break/>
long:28.7802054</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.32</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Burger King</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.990328<break/>
long:28.779948</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9904457<break/>
long:28.7799321</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.16</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Watsons</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.991573<break/>
long:28.782114</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.99064<break/>
long:28.77983</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">217.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t009"><object-id pub-id-type="pii">sensors-25-01178-t009_Table 9</object-id><label>Table 9</label><caption><p>Recognized and corrected labels with the similarity rate in Scenario 4.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recognized Label</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Corrected Label with Sequence Matcher</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Word Similarity Rate (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Watsons</td><td align="center" valign="middle" rowspan="1" colspan="1">watsons</td><td align="center" valign="middle" rowspan="1" colspan="1">watsons</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BURGER KING</td><td align="center" valign="middle" rowspan="1" colspan="1">KING</td><td align="center" valign="middle" rowspan="1" colspan="1">KING</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Tavuk D&#x000fc;nyas&#x00131;</td><td align="center" valign="middle" rowspan="1" colspan="1">Tavul</td><td align="center" valign="middle" rowspan="1" colspan="1">Tavuk</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Metin</td><td align="center" valign="middle" rowspan="1" colspan="1">Ketin</td><td align="center" valign="middle" rowspan="1" colspan="1">Metin</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GLORIA PERFUME</td><td align="center" valign="middle" rowspan="1" colspan="1">GLORIA PERFUME</td><td align="center" valign="middle" rowspan="1" colspan="1">GLORIA PERFUME</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ercan Burger</td><td align="center" valign="middle" rowspan="1" colspan="1">ERCAN</td><td align="center" valign="middle" rowspan="1" colspan="1">ERCAN</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Atasun Optik</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Atasun</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Atasun</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t010"><object-id pub-id-type="pii">sensors-25-01178-t010_Table 10</object-id><label>Table 10</label><caption><p>Comparison of the GPS and generated coordinates in Scenario 4.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name (Label)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GPS Coordinate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Generated Coordinate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Verification with Google Map</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Distance Difference (meter)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Watsons</td><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">lat:40.98985981445177<break/>
long:28.78013560421319</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9906355<break/>
long:28.779864</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.21</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Burger King</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:48.9964457<break/>
long:28.7799321</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.35</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tavuk D&#x000fc;nyas&#x00131;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9906417<break/>
long:28.7802054</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.06</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Metin Kuyumculuk</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9900289<break/>
long:28.7801479</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.83</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GLORIA PERFUME</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9900573<break/>
long:28.7800811</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.43</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ERCAN BURGER</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.98992640000001<break/>
long:28.7804342</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.13</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Atasun Optik</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:40.9900175<break/>
long:28.7800944</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.87</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t011"><object-id pub-id-type="pii">sensors-25-01178-t011_Table 11</object-id><label>Table 11</label><caption><p>Businesses and corresponding addresses in Test Area 3.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Address</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">G&#x00026;O VIP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Halkal&#x00131; Caddesi, Atakent 4 No: 206L 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maydonoz</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Halkal&#x00131; Caddesi, Atakent 4 No: 206L 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Starbucks</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Halkal&#x00131; Caddesi, No: 206 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lotus Eczane</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Halkal&#x00131; Caddesi, No: 206, D:E 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbu</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Halis D&#x000fc;r&#x000fc;m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Avrupa Konutlar&#x00131; 4, &#x00130;n&#x000f6;n&#x000fc; Mah. No: 206 34290 K&#x000fc;&#x000e7;&#x000fc;k&#x000e7;ekmece/&#x00130;stanbul</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t012"><object-id pub-id-type="pii">sensors-25-01178-t012_Table 12</object-id><label>Table 12</label><caption><p>Recognized and corrected labels with the similarity rate in Scenario 5.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recognized Label</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Corrected Label with Sequence Matcher</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Word Similarity Rate (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">G&#x00026;O VIP</td><td align="center" valign="middle" rowspan="1" colspan="1">VIP</td><td align="center" valign="middle" rowspan="1" colspan="1">VIP</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Maydonoz</td><td align="center" valign="middle" rowspan="1" colspan="1">Maydonoz</td><td align="center" valign="middle" rowspan="1" colspan="1">Maydonoz</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">STARBUCKS</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOTUS</td><td align="center" valign="middle" rowspan="1" colspan="1">LOTUS</td><td align="center" valign="middle" rowspan="1" colspan="1">LOTUS</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HALIS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">nHALIS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HALIS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.5</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t013"><object-id pub-id-type="pii">sensors-25-01178-t013_Table 13</object-id><label>Table 13</label><caption><p>Comparison of the GPS and generated coordinates in Scenario 5.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business Name (Label)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GPS Coordinate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Generated Coordinate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Verification with Google Map</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Distance Difference (kms)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">G&#x00026;O VIP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:41.021457<break/>
long:28.791342</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:41.021316<break/>
long:28.791462</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.018</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maydonoz</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:0.0000<break/>
long:0.0000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:41.02093<break/>
long:28.791677</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5405.025</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">STARBUCKS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:0.0000<break/>
long:0.0000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lotus Eczane</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:0.0000<break/>
long:0.0000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:41.0205738<break/>
long:28.7916786</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5404.995</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Halis D&#x000fc;r&#x000fc;m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:0.0000<break/>
long:0.0000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">lat:41.020566<break/>
long:28.791589</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">verified</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5404.989</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t014"><object-id pub-id-type="pii">sensors-25-01178-t014_Table 14</object-id><label>Table 14</label><caption><p>Comparison of the proposed model and existing studies.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Research</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensors</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Environment</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Experiment Type</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Error Rate<break/>
(meters)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B29-sensors-25-01178" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">GPS/Compass/<break/>
Camera</td><td align="center" valign="middle" rowspan="1" colspan="1">Faster R-CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">Outdoor</td><td align="center" valign="middle" rowspan="1" colspan="1">Real-Time</td><td align="center" valign="middle" rowspan="1" colspan="1">Average:28<break/>
Minimum:1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B27-sensors-25-01178" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">GPS, IMU,<break/>
Visual Odometry</td><td align="center" valign="middle" rowspan="1" colspan="1">Extended Kalman Filter</td><td align="center" valign="middle" rowspan="1" colspan="1">Outdoor</td><td align="center" valign="middle" rowspan="1" colspan="1">Real-Time</td><td align="center" valign="middle" rowspan="1" colspan="1">Average:4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B60-sensors-25-01178" ref-type="bibr">60</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">GPS/<break/>
Contour Action Camera</td><td align="center" valign="middle" rowspan="1" colspan="1">Image Retrieval Algorthim</td><td align="center" valign="middle" rowspan="1" colspan="1">Outdoor</td><td align="center" valign="middle" rowspan="1" colspan="1">Video Record</td><td align="center" valign="middle" rowspan="1" colspan="1">Average:7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B61-sensors-25-01178" ref-type="bibr">61</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">Camera</td><td align="center" valign="middle" rowspan="1" colspan="1">Bayesian non-parametric<break/>
estimation &#x00026; particle filter</td><td align="center" valign="middle" rowspan="1" colspan="1">Outdoor</td><td align="center" valign="middle" rowspan="1" colspan="1">Simulation</td><td align="center" valign="middle" rowspan="1" colspan="1">Average:4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B62-sensors-25-01178" ref-type="bibr">62</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">INS/GPS</td><td align="center" valign="middle" rowspan="1" colspan="1">Kalman Filter</td><td align="center" valign="middle" rowspan="1" colspan="1">Outdoor</td><td align="center" valign="middle" rowspan="1" colspan="1">Real-Time</td><td align="center" valign="middle" rowspan="1" colspan="1">Average:16</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Proposed</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>GPS/Camera</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>EAST+EASYOCR+SM</bold>
<break/>
<bold>&#x00026;</bold>
<break/>
<bold>WEB MINING</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Outdoor</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Real-Time</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Average:4</bold>
<break/>
<bold>Minimum:1</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01178-t015"><object-id pub-id-type="pii">sensors-25-01178-t015_Table 15</object-id><label>Table 15</label><caption><p>Performance evaluation of the proposed system in terms of FPS and processing time.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Scenario</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Business</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Processing Time (s)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average FPS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Processing Time Average (s)</th></tr></thead><tbody><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">alBaraka</td><td align="center" valign="middle" rowspan="1" colspan="1">2.9</td><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">9.1</td><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">2.93</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A-101</td><td align="center" valign="middle" rowspan="1" colspan="1">3.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Yap&#x00131;Kredi</td><td align="center" valign="middle" rowspan="1" colspan="1">3.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PTT</td><td align="center" valign="middle" rowspan="1" colspan="1">n/a</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HALKBANK</td><td align="center" valign="middle" rowspan="1" colspan="1">2.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Vak&#x00131;fBank</td><td align="center" valign="middle" rowspan="1" colspan="1">3.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KUVEYTT&#x000dc;RK</td><td align="center" valign="middle" rowspan="1" colspan="1">3.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ziraat Bankas&#x00131;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.3</td></tr><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">Ziraat Bankas&#x00131;</td><td align="center" valign="middle" rowspan="1" colspan="1">2.7</td><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">7.5</td><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">2.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KUVEYTT&#x000dc;RK</td><td align="center" valign="middle" rowspan="1" colspan="1">2.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Vak&#x00131;fBank</td><td align="center" valign="middle" rowspan="1" colspan="1">2.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HALKBANK</td><td align="center" valign="middle" rowspan="1" colspan="1">n/a</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PTT</td><td align="center" valign="middle" rowspan="1" colspan="1">2.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Yap&#x00131;Kredi</td><td align="center" valign="middle" rowspan="1" colspan="1">2.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A-101</td><td align="center" valign="middle" rowspan="1" colspan="1">2.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">alBaraka</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.3</td></tr><tr><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">Atasun Optik</td><td align="center" valign="middle" rowspan="1" colspan="1">2.5</td><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">5.9</td><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">2.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ERCAN BURGER</td><td align="center" valign="middle" rowspan="1" colspan="1">2.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GLORIA PERFUME</td><td align="center" valign="middle" rowspan="1" colspan="1">2.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Metin</td><td align="center" valign="middle" rowspan="1" colspan="1">2.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Tavuk D&#x000fc;nyas&#x00131;</td><td align="center" valign="middle" rowspan="1" colspan="1">2.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BURGER KING</td><td align="center" valign="middle" rowspan="1" colspan="1">2.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Watsons</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.3</td></tr><tr><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">Watsons</td><td align="center" valign="middle" rowspan="1" colspan="1">1.8</td><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">5.10</td><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">2.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BURGER KING</td><td align="center" valign="middle" rowspan="1" colspan="1">1.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Tavuk D&#x000fc;nyas&#x00131;</td><td align="center" valign="middle" rowspan="1" colspan="1">1.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Metin</td><td align="center" valign="middle" rowspan="1" colspan="1">2.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GLORIA PERFUME</td><td align="center" valign="middle" rowspan="1" colspan="1">1.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ERCAN BURGER</td><td align="center" valign="middle" rowspan="1" colspan="1">2.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Atasun Optik</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.5</td></tr><tr><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">G&#x00026;O VIP</td><td align="center" valign="middle" rowspan="1" colspan="1">2.4</td><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">8.5</td><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">2.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Maydonoz</td><td align="center" valign="middle" rowspan="1" colspan="1">2.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">STARBUCKS</td><td align="center" valign="middle" rowspan="1" colspan="1">n/a</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOTUS</td><td align="center" valign="middle" rowspan="1" colspan="1">2.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HALIS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.2</td></tr></tbody></table></table-wrap></floats-group></article>