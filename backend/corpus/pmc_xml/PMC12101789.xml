<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">JMIR Med Inform</journal-id><journal-id journal-id-type="iso-abbrev">JMIR Med Inform</journal-id><journal-id journal-id-type="hwp">JMI</journal-id><journal-id journal-id-type="publisher-id">medinform</journal-id><journal-id journal-id-type="index">7</journal-id><journal-title-group><journal-title>JMIR Medical Informatics</journal-title></journal-title-group><issn pub-type="epub">2291-9694</issn><publisher><publisher-name>JMIR Publications</publisher-name><publisher-loc>Toronto, Canada</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40378406</article-id><article-id pub-id-type="pmc">PMC12101789</article-id>
<article-id pub-id-type="doi">10.2196/66917</article-id><article-id pub-id-type="publisher-id">66917</article-id><article-categories><subj-group subj-group-type="primary-section"><subject>AI Language Models in Health Care</subject></subj-group><subj-group subj-group-type="secondary-section"><subject>Generative Language Models Including ChatGPT</subject></subj-group><subj-group subj-group-type="secondary-section"><subject>Natural Language Processing</subject></subj-group><subj-group subj-group-type="secondary-section"><subject>Artificial Intelligence, Machine Learning, and Natural Language Processing for Public Health</subject></subj-group><subj-group subj-group-type="secondary-section"><subject>Foundation Models and Their Applications in AI</subject></subj-group><subj-group subj-group-type="secondary-section"><subject>Artificial Intelligence</subject></subj-group><subj-group subj-group-type="secondary-section"><subject>Clinical Information and Decision Making</subject></subj-group><subj-group subj-group-type="heading"><subject>Original Paper</subject></subj-group></article-categories><title-group><article-title>Benchmarking the Confidence of Large Language Models in Answering Clinical Questions: Cross-Sectional Evaluation Study</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0009-0001-0438-0827</contrib-id><name><surname>Omar</surname><given-names>Mahmud</given-names></name><degrees>MD</degrees><xref rid="aff1" ref-type="aff">1</xref><xref rid="cor1" ref-type="corresp"/><xref rid="equal-contrib1" ref-type="author-notes">*</xref><email>mahmudomar70@gmail.com</email></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0009-0000-8030-9232</contrib-id><name><surname>Agbareia</surname><given-names>Reem</given-names></name><degrees>MD</degrees><xref rid="aff2" ref-type="aff">2</xref><email>reem.egb@gmail.com</email></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-4515-8090</contrib-id><name><surname>Glicksberg</surname><given-names>Benjamin S</given-names></name><degrees>MD</degrees><xref rid="aff1" ref-type="aff">1</xref><email>ben.glicksberg@gmail.com</email></contrib><contrib contrib-type="author" equal-contrib="yes"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-6319-4314</contrib-id><name><surname>Nadkarni</surname><given-names>Girish N</given-names></name><degrees>MD</degrees><xref rid="aff1" ref-type="aff">1</xref><xref rid="equal-contrib1" ref-type="author-notes">*</xref><email>girish.nadkarni@mountsinai.org</email></contrib><contrib contrib-type="author" equal-contrib="yes"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4567-3108</contrib-id><name><surname>Klang</surname><given-names>Eyal</given-names></name><degrees>MD</degrees><xref rid="aff1" ref-type="aff">1</xref><xref rid="equal-contrib1" ref-type="author-notes">*</xref><email>eyal.klang@mountsinai.org</email></contrib><aff id="aff1"><label>1</label><institution content-type="department">Division of Data-Driven and Digital Medicine (D3M), Department of Medicine</institution>, <institution>Icahn School of Medicine at Mount Sinai</institution>, <addr-line>Gustave L. Levy Place New York</addr-line>, <addr-line content-type="city">New York</addr-line>, <addr-line content-type="state">NY</addr-line>, <addr-line>10029</addr-line>, <country>United States</country>, <phone>1 212 241 6500</phone></aff><aff id="aff2"><label>2</label><institution content-type="department">Ophthalmology Department</institution>, <institution>Hadassah Medical Center</institution>, <addr-line content-type="city">Jerusalem</addr-line>, <country>Israel</country></aff></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Castonguay</surname><given-names>Alexandre</given-names></name></contrib></contrib-group><author-notes><corresp id="cor1">Mahmud Omar, MD, Division of Data-Driven and Digital Medicine (D3M), Department of Medicine, Icahn School of Medicine at Mount Sinai, Gustave L. Levy Place New York, New York, NY, 10029, United States, <phone>1 212 241 6500</phone>; <email xlink:href="mahmudomar70@gmail.com">mahmudomar70@gmail.com</email></corresp><fn fn-type="equal" id="equal-contrib1"><label>*</label><p>these authors contributed equally</p></fn><fn fn-type="COI-statement"><p>None declared.</p></fn></author-notes><pub-date pub-type="collection"><year>2025</year></pub-date><pub-date pub-type="epub"><day>16</day><month>5</month><year>2025</year></pub-date><volume>13</volume><elocation-id>e66917</elocation-id><history><date date-type="received"><day>26</day><month>9</month><year>2024</year></date><date date-type="rev-recd"><day>31</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>31</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; Mahmud Omar, Reem Agbareia, Benjamin S Glicksberg, Girish N Nadkarni, Eyal Klang. Originally published in JMIR Medical Informatics (https://medinform.jmir.org)</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Medical Informatics, is properly cited. The complete bibliographic information, a link to the original publication on <ext-link ext-link-type="uri" xlink:href="https://medinform.jmir.org/">https://medinform.jmir.org/</ext-link>, as well as this copyright and license information must be included.</license-p></license></permissions><self-uri xlink:title="pdf" xlink:href="medinform-v13-e66917.pdf"/><abstract><title>Abstract</title><sec><title>Background</title><p>The capabilities of large language models (LLMs) to self-assess their own confidence in answering questions within the biomedical realm remain underexplored.</p></sec><sec><title>Objective</title><p>This study evaluates the confidence levels of 12 LLMs across 5 medical specialties to assess LLMs&#x02019; ability to accurately judge their own responses.</p></sec><sec><title>Methods</title><p>We used 1965 multiple-choice questions that assessed clinical knowledge in the following areas: internal medicine, obstetrics and gynecology, psychiatry, pediatrics, and general surgery. Models were prompted to provide answers and to also provide their confidence for the correct answers (score: range 0%&#x02010;100%). We calculated the correlation between each model&#x02019;s mean confidence score for correct answers and the overall accuracy of each model across all questions. The confidence scores for correct and incorrect answers were also analyzed to determine the mean difference in confidence, using 2-sample, 2-tailed <italic toggle="yes">t</italic> tests.</p></sec><sec><title>Results</title><p>The correlation between the mean confidence scores for correct answers and model accuracy was inverse and statistically significant (<italic toggle="yes">r</italic>=&#x02212;0.40; <italic toggle="yes">P</italic>=.001), indicating that worse-performing models exhibited paradoxically higher confidence. For instance, a top-performing model&#x02014;GPT-4o&#x02014;had a mean accuracy of 74% (SD 9.4%), with a mean confidence of 63% (SD 8.3%), whereas a low-performing model&#x02014;Qwen2-7B&#x02014;showed a mean accuracy of 46% (SD 10.5%) but a mean confidence of 76% (SD 11.7%). The mean difference in confidence between correct and incorrect responses was low for all models, ranging from 0.6% to 5.4%, with GPT-4o having the highest mean difference (5.4%, SD 2.3%; <italic toggle="yes">P</italic>=.003).</p></sec><sec><title>Conclusions</title><p>Better-performing LLMs show more aligned overall confidence levels. However, even the most accurate models still show minimal variation in confidence between right and wrong answers. This may limit their safe use in clinical settings. Addressing overconfidence could involve refining calibration methods, performing domain-specific fine-tuning, and involving human oversight when decisions carry high risks. Further research is needed to improve these strategies before broader clinical adoption of LLMs.</p></sec></abstract><kwd-group kwd-group-type="author-keywords"><title>Keywords</title><kwd>safe AI</kwd><kwd>artificial intelligence</kwd><kwd>AI</kwd><kwd>algorithm</kwd><kwd>large language model</kwd><kwd>LLM</kwd><kwd>natural language processing</kwd><kwd>NLP</kwd><kwd>deep learning</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>With their capacity to understand and generate human-like text, large language models (LLMs) are poised to support health care professionals in complex clinical decisions [<xref rid="R1" ref-type="bibr">1-3</xref>]. A wide array of LLMs is now accessible, including open-source models, offering solutions that cater to both the public and medical professionals [<xref rid="R1" ref-type="bibr">1</xref><xref rid="R4" ref-type="bibr">4</xref>].</p><p>The efficacy of these models has been demonstrated in a variety of tasks, albeit with some limitations [<xref rid="R5" ref-type="bibr">5</xref><xref rid="R6" ref-type="bibr">6</xref>]. For instance, LLMs, such as GPT, have shown promise in providing diagnostic assistance and answering medical queries [<xref rid="R5" ref-type="bibr">5</xref><xref rid="R7" ref-type="bibr">7-9</xref><xref rid="R8" ref-type="bibr">undefined</xref><xref rid="R9" ref-type="bibr">undefined</xref>]. Katz et al [<xref rid="R10" ref-type="bibr">10</xref>] demonstrated that GPT-4 not only improved clinically when compared to its predecessor, GPT-3.5, but also matched physician performance in certain areas. However, there is evidence of hallucinations and inaccuracies in model outputs, which could lead to harm in clinical decision-making [<xref rid="R11" ref-type="bibr">11</xref><xref rid="R12" ref-type="bibr">12</xref>]. Specifically, LLMs have occasionally generated completely fabricated evidence (eg, information and references) and have presented such evidence as factual [<xref rid="R11" ref-type="bibr">11</xref><xref rid="R12" ref-type="bibr">12</xref>].</p><p>One way of building confidence in applying models within health care is the use of explainable artificial intelligence (AI) [<xref rid="R13" ref-type="bibr">13</xref><xref rid="R14" ref-type="bibr">14</xref>]. However, easily explainable outputs are difficult to evaluate due to the complexity of how LLMs process and output data [<xref rid="R13" ref-type="bibr">13</xref><xref rid="R15" ref-type="bibr">15</xref><xref rid="R16" ref-type="bibr">16</xref>]. Recent work revealed that these models often exhibit high confidence even when presenting incorrect information [<xref rid="R17" ref-type="bibr">17</xref>]. This raises questions about the underlying mechanisms that prompt an LLM to label certain statements as &#x0201c;more factual.&#x0201d; For example, one possible explanation could be that data-rich or frequently discussed topics in training sets may be perceived as more certain [<xref rid="R18" ref-type="bibr">18</xref>], even if this does not translate into clinical accuracy. Additionally, retrieval-augmented generation (RAG) has been proposed to ground LLM outputs in external data, which potentially mitigates hallucinations [<xref rid="R19" ref-type="bibr">19</xref>]. Nevertheless, these approaches do not fully resolve whether models can reliably judge their own correctness. Accurate and well-calibrated confidence scores may be vital for establishing trust in these systems, as such scores can alert users to approach certain responses with caution. If a model consistently shows undue confidence in wrong answers, it poses a subtle but potentially dangerous form of hallucination. Clinicians might adopt decisions based on erroneous advice that is delivered with overt certainty. By investigating how these models generate and express their confidence, we aimed to illuminate whether LLMs can reliably self-assess correctness.</p><p>The goal of this study was to benchmark LLMs (both proprietary LLMs, like GPT-4o and Claude 3.5 Sonnet, and open-source LLMs, like Qwen) in terms of accuracy and associated confidence in answering clinical questions. Our aim was to determine if these models can accurately judge when to be confident in their responses and, in doing so, allow for better explainability in their application.</p></sec><sec sec-type="methods" id="s2"><title>Methods</title><sec id="s2-1"><title>Study Design and Data Source</title><p>This study used a public compiled dataset from a previous study by Katz et al [<xref rid="R10" ref-type="bibr">10</xref>], which includes 655 questions for the following five medical specialties: internal medicine, obstetrics and gynecology (OBGYN), psychiatry, pediatrics, and general surgery. These questions were sourced from official 2023 licensing examinations for each field and were crafted from internationally recognized textbooks and guidelines. This dataset serves as a standardized framework for assessment [<xref rid="R20" ref-type="bibr">20-24</xref>].</p><p>To enhance benchmarking reliability, each original question was rephrased twice by using the GPT-4 application programming interface (API) in Python (Python Software Foundation), yielding 1965 questions (we include the full prompt in <xref rid="SAP1" ref-type="supplementary-material">Multimedia Appendix 1</xref>). The prompts were carefully designed to modify only the writing style, without altering any clinical details, such as medical terms, laboratory values, or answer choices [<xref rid="R25" ref-type="bibr">25</xref>]. This approach aimed to preserve all clinical details, ensuring that rephrased questions stayed faithful to the original intent and information. To confirm this, 2 board-certified physicians separately reviewed a 20% random sample of questions from each specialty. They compared the rephrased and original questions side by side, focusing on consistency in medical terminology, laboratory values, and answer choices. Both reviewers concluded that the paraphrased items remained unchanged in terms of clinical meaning and required no further edits, thereby confirming overall integrity and accuracy.</p></sec><sec id="s2-2"><title>Model Setup and Configuration</title><p>The LLMs used in this study were prompted (using 1 structured prompt) to return the correct answer, along with a confidence score for each choice (&#x0201c;A,&#x0201d; &#x0201c;B,&#x0201d; &#x0201c;C,&#x0201d; and &#x0201c;D&#x0201d;), in JSON format. These confidence scores were expressed as percentages between 0% and 100% for each option, resulting in a total confidence score of 100% for all options combined. The open access models were executed by using API codes in a dedicated server with 4 H100 80-GB graphics processing units; the corresponding codebase is accessible on GitHub for the original database by Katz et al [<xref rid="R10" ref-type="bibr">10</xref>], and we provide the full prompts, which can be used locally, in <xref rid="SAP1" ref-type="supplementary-material">Multimedia Appendix 1</xref>. We used Python 3.10 for data analyses. The commercial models were used via the corresponding companies&#x02019; API interfaces. We used several Python libraries to facilitate data processing, model interaction, and analysis&#x02014;NumPy 1.26.4, Pandas 2.1.4, Scikit-Learn 1.3.0, Hugging Face&#x02019;s Transformers 4.37.2, and torch 2.2.2+cu121&#x02014;as well as JSON module 2.0.9. We used the default hyperparameters for each model to reflect typical user settings and provide a balanced baseline [<xref rid="R26" ref-type="bibr">26</xref>]. For the open access models, we used the &#x0201c;instruct&#x0201d; versions, which perform better on zero-shot questioning.</p></sec><sec id="s2-3"><title>Benchmarked LLMs</title><p>We selected 12 LLMs that varied in terms of size, architectures, and intended domains (<xref rid="F1" ref-type="fig">Figure 1</xref>). This set included established &#x0201c;household&#x0201d; models and newly introduced or domain-focused alternatives, ensuring diverse coverage (<xref rid="F1" ref-type="fig">Figure 1</xref>). The benchmarked models are shown in Table S1 in <xref rid="SAP1" ref-type="supplementary-material">Multimedia Appendix 1</xref>.</p><fig position="float" id="F1" fig-type="figure"><label>Figure 1.</label><caption><title>A flowchart representing the evaluation methodology. The 655 questions were sourced from a study by Katz et al [<xref rid="R10" ref-type="bibr">10</xref>]. MCQ: multiple-choice question.</title></caption><graphic xlink:href="medinform-v13-e66917-g001" position="float"/></fig></sec><sec id="s2-4"><title>Statistical Analysis</title><p>The Pearson correlation coefficient was used to correlate models&#x02019; mean confidence scores for correct answers and accuracies across models and medical fields. Chi-square tests assessed overall performance differences within each field, using proportions of correct responses. Post hoc pairwise comparisons with Bonferroni correction identified specific intermodel differences. Confidence levels were compared between correct and incorrect responses for each model, using 2-sample, 2-tailed <italic toggle="yes">t</italic> tests. Mean confidence scores were calculated for higher-tier and lower-tier models, as well as across all models. Performance consistency was evaluated by comparing confidence gaps between correct and incorrect responses. All statistical tests used a significance level of &#x003b1;=.05. Analyses were performed using R version 4.1.2 (R Foundation for Statistical Computing).</p></sec></sec><sec sec-type="results" id="s3"><title>Results</title><sec id="s3-1"><title>Confidence Analysis</title><p><xref rid="T1" ref-type="table">Table 1</xref> summarizes accuracies and confidence levels across the models, and Table S2 in <xref rid="SAP1" ref-type="supplementary-material">Multimedia Appendix 1</xref> presents the data across all inspected fields and all models. An inverse correlation between the mean confidence scores for correct answers and the overall accuracy of the models is demonstrated (<italic toggle="yes">r</italic>=&#x02013;0.40; <italic toggle="yes">P</italic>=.001); better-performing models generally showed lower confidence.</p><table-wrap position="float" id="T1"><label>Table 1.</label><caption><title>Accuracies and confidence levels across the models.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom" rowspan="1" colspan="1">Model</th><th valign="bottom" rowspan="1" colspan="1">Accuracy, %</th><th valign="bottom" rowspan="1" colspan="1">Total confidence, %</th><th valign="bottom" rowspan="1" colspan="1">Confidence for correct answer, %</th><th valign="bottom" rowspan="1" colspan="1">Confidence for incorrect answer, %</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Claude 3.5 Sonnet</td><td align="left" valign="top" rowspan="1" colspan="1">74</td><td align="left" valign="top" rowspan="1" colspan="1">69.7</td><td align="left" valign="top" rowspan="1" colspan="1">70.5</td><td align="left" valign="top" rowspan="1" colspan="1">67.4</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">GPT-4o</td><td align="left" valign="top" rowspan="1" colspan="1">73.8</td><td align="left" valign="top" rowspan="1" colspan="1">63</td><td align="left" valign="top" rowspan="1" colspan="1">64.4</td><td align="left" valign="top" rowspan="1" colspan="1">59</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Claude 3 Opus</td><td align="left" valign="top" rowspan="1" colspan="1">71.7</td><td align="left" valign="top" rowspan="1" colspan="1">68.5</td><td align="left" valign="top" rowspan="1" colspan="1">68.9</td><td align="left" valign="top" rowspan="1" colspan="1">67.3</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">GPT-4</td><td align="left" valign="top" rowspan="1" colspan="1">66</td><td align="left" valign="top" rowspan="1" colspan="1">84.1</td><td align="left" valign="top" rowspan="1" colspan="1">84.5</td><td align="left" valign="top" rowspan="1" colspan="1">83.3</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Llama-3-70B</td><td align="left" valign="top" rowspan="1" colspan="1">63.4</td><td align="left" valign="top" rowspan="1" colspan="1">57.3</td><td align="left" valign="top" rowspan="1" colspan="1">59.5</td><td align="left" valign="top" rowspan="1" colspan="1">53.6</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Llama OpenBio</td><td align="left" valign="top" rowspan="1" colspan="1">59.2</td><td align="left" valign="top" rowspan="1" colspan="1">77.9</td><td align="left" valign="top" rowspan="1" colspan="1">77.7</td><td align="left" valign="top" rowspan="1" colspan="1">78.1</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Gemini</td><td align="left" valign="top" rowspan="1" colspan="1">59.1</td><td align="left" valign="top" rowspan="1" colspan="1">86.5</td><td align="left" valign="top" rowspan="1" colspan="1">87.2</td><td align="left" valign="top" rowspan="1" colspan="1">85.5</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Qwen2-72B</td><td align="left" valign="top" rowspan="1" colspan="1">57.8</td><td align="left" valign="top" rowspan="1" colspan="1">57.7</td><td align="left" valign="top" rowspan="1" colspan="1">58.6</td><td align="left" valign="top" rowspan="1" colspan="1">56.5</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mixtral-8&#x000d7;7B</td><td align="left" valign="top" rowspan="1" colspan="1">50.6</td><td align="left" valign="top" rowspan="1" colspan="1">84.3</td><td align="left" valign="top" rowspan="1" colspan="1">85.5</td><td align="left" valign="top" rowspan="1" colspan="1">83</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">GPT-3.5</td><td align="left" valign="top" rowspan="1" colspan="1">49</td><td align="left" valign="top" rowspan="1" colspan="1">82.3</td><td align="left" valign="top" rowspan="1" colspan="1">81.6</td><td align="left" valign="top" rowspan="1" colspan="1">82.9</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Llama-3-8B</td><td align="left" valign="top" rowspan="1" colspan="1">48.4</td><td align="left" valign="top" rowspan="1" colspan="1">80</td><td align="left" valign="top" rowspan="1" colspan="1">79.7</td><td align="left" valign="top" rowspan="1" colspan="1">80.3</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Qwen2-7B</td><td align="left" valign="top" rowspan="1" colspan="1">46</td><td align="left" valign="top" rowspan="1" colspan="1">75.5</td><td align="left" valign="top" rowspan="1" colspan="1">74.4</td><td align="left" valign="top" rowspan="1" colspan="1">76.4</td></tr></tbody></table></table-wrap><p>The mean confidence score for all 12 models was 76.1% when they were correct and 74.4% when they were incorrect. The 6 top-performing models showed a mean confidence score of 72.5% when they were correct and a mean confidence score of 69.4% when incorrect, while the 6 lowest-performing models displayed 79.6% confidence when they were correct and 79.5% confidence when they were incorrect (<xref rid="T2" ref-type="table">Table 2</xref>).</p><table-wrap position="float" id="T2"><label>Table 2.</label><caption><title>Large language models&#x02019; mean confidence scores for correct and incorrect answers.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom" rowspan="1" colspan="1">Model</th><th valign="bottom" rowspan="1" colspan="1">Confidence when incorrect (%), mean (SD)</th><th valign="bottom" rowspan="1" colspan="1">Confidence when correct (%), mean (SD)</th><th valign="bottom" rowspan="1" colspan="1"><italic toggle="yes">P</italic> value</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">GPT-4o</td><td align="left" valign="top" rowspan="1" colspan="1">58.99 (14.31)</td><td align="left" valign="top" rowspan="1" colspan="1">64.38 (16.11)</td><td align="left" valign="top" rowspan="1" colspan="1">.006</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Llama-3-70B</td><td align="left" valign="top" rowspan="1" colspan="1">53.59 (22.38)</td><td align="left" valign="top" rowspan="1" colspan="1">59.50 (23.54)</td><td align="left" valign="top" rowspan="1" colspan="1">.006</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Claude 3.5 Sonnet</td><td align="left" valign="top" rowspan="1" colspan="1">67.37 (9.08)</td><td align="left" valign="top" rowspan="1" colspan="1">70.52 (11.07)</td><td align="left" valign="top" rowspan="1" colspan="1">.003</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Gemini</td><td align="left" valign="top" rowspan="1" colspan="1">85.55 (16.23)</td><td align="left" valign="top" rowspan="1" colspan="1">87.17 (16.58)</td><td align="left" valign="top" rowspan="1" colspan="1">.35</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Claude 3 Opus</td><td align="left" valign="top" rowspan="1" colspan="1">67.32 (13.06)</td><td align="left" valign="top" rowspan="1" colspan="1">68.90 (15.65)</td><td align="left" valign="top" rowspan="1" colspan="1">.61</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">GPT-4</td><td align="left" valign="top" rowspan="1" colspan="1">83.34 (23.30)</td><td align="left" valign="top" rowspan="1" colspan="1">84.52 (22.43)</td><td align="left" valign="top" rowspan="1" colspan="1">.07</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Qwen2-72B</td><td align="left" valign="top" rowspan="1" colspan="1">56.49 (18.55)</td><td align="left" valign="top" rowspan="1" colspan="1">58.59 (20.03)</td><td align="left" valign="top" rowspan="1" colspan="1">.004</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Qwen2-7B</td><td align="left" valign="top" rowspan="1" colspan="1">76.37 (17.11)</td><td align="left" valign="top" rowspan="1" colspan="1">74.45 (20.30)</td><td align="left" valign="top" rowspan="1" colspan="1">.01</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mixtral-8&#x000d7;7B</td><td align="left" valign="top" rowspan="1" colspan="1">82.99 (16.52)</td><td align="left" valign="top" rowspan="1" colspan="1">85.49 (14.62)</td><td align="left" valign="top" rowspan="1" colspan="1">.04</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Llama-3-8B</td><td align="left" valign="top" rowspan="1" colspan="1">80.25 (17.40)</td><td align="left" valign="top" rowspan="1" colspan="1">79.67 (21.59)</td><td align="left" valign="top" rowspan="1" colspan="1">.31</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Llama OpenBio</td><td align="left" valign="top" rowspan="1" colspan="1">78.14 (27.59)</td><td align="left" valign="top" rowspan="1" colspan="1">77.73 (28.78)</td><td align="left" valign="top" rowspan="1" colspan="1">.83</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">GPT-3.5</td><td align="left" valign="top" rowspan="1" colspan="1">82.85 (27.17)</td><td align="left" valign="top" rowspan="1" colspan="1">81.63 (28.66)</td><td align="left" valign="top" rowspan="1" colspan="1">.81</td></tr></tbody></table></table-wrap><p>Four models (GPT-4o, Llama-3-70B, Claude 3.5 Sonnet, and Qwen2-72B) demonstrated significantly higher confidence when they were correct (all <italic toggle="yes">P</italic> values were &#x0003c;.01) across the different fields and subsets. Gemini exhibited the highest overall confidence levels (when incorrect: mean 85.6%, SD 16.2%; when correct: mean 87.2%, SD 16.6%). Qwen2-7B was unique in that it displayed higher confidence when incorrect (mean 76.4%, SD 17.1% vs mean 74.5%, SD 20.3% when correct; <italic toggle="yes">P</italic>=.01).</p><p>GPT-3.5 and Llama-OpenBio-70B revealed minimal differences in confidence between correct and incorrect answers (<italic toggle="yes">P</italic>=.80). The largest confidence gap was observed in GPT-4 (5.4%, SD 2.3%; <italic toggle="yes">P</italic>=.003), while Llama-3-8B had the smallest gap (0.6%; <xref rid="F2" ref-type="fig">Figure 2</xref>).</p><fig position="float" id="F2" fig-type="figure"><label>Figure 2.</label><caption><title>Large language models&#x02019; confidence results for correct and incorrect answers. The left graph displays the average confidence and 95% CIs for each model, categorized by correct answers (green) and incorrect answers (red). The right graph shows the differences in average confidence for each model, where green indicates higher confidence in correct answers, and red indicates higher confidence in incorrect answers.</title></caption><graphic xlink:href="medinform-v13-e66917-g002" position="float"/></fig></sec><sec id="s3-2"><title>Models&#x02019; Performances Across Fields</title><p>Significant differences were seen in model performance across all 5 medical specialties (at the <italic toggle="yes">P</italic>&#x0003c;.01 level). GPT-4o and Claude 3.5 Sonnet consistently outperformed other models. For internal medicine, GPT-4o (accuracy: 70.9%) and Claude 3.5 Sonnet (accuracy: 73.5%) showed no significant difference (<italic toggle="yes">P</italic>&#x0003e;.99) but outperformed lower-tier models, such as Qwen-7b (accuracy: 43.7%; <italic toggle="yes">P</italic>&#x0003c;.001). For OBGYN, Claude 3.5 Sonnet (accuracy: 71.0%) significantly outperformed most models, including GPT-4 (accuracy: 54.0%; <italic toggle="yes">P</italic>&#x0003c;.001). For pediatrics, the top 5 models (GPT-4o, Llama-3-70b, Claude 3.5 Sonnet, Claude 3 Opus, and GPT-4) showed no significant differences among themselves (all <italic toggle="yes">P</italic> values were &#x0003e;.05) but outperformed lower-tier models. Psychiatry results mirrored this pattern, with GPT-4o (accuracy: 84.4%) and Claude 3.5 Sonnet (accuracy: 82.4%) showing the best performance. For surgery, GPT-4o (accuracy: 70.9%) and Claude 3.5 Sonnet (accuracy: 70.5%) again showed no significant difference (<italic toggle="yes">P</italic>&#x0003e;.99) but outperformed lower-performing models, such as Qwen-7b (accuracy: 45.6%; <italic toggle="yes">P</italic>&#x0003c;.01; Tables S3 and S4 in <xref rid="SAP1" ref-type="supplementary-material">Multimedia Appendix 1</xref>).</p></sec></sec><sec sec-type="discussion" id="s4"><title>Discussion</title><p>In our evaluation, accuracy and confidence were inversely correlated for LLMs. Some lower-complexity models were notably more confident in incorrect answers. Despite GPT-4o showing the best performance, its largest observed gap between confidence scores for correct and incorrect answers was only 5.4%. This indicates that it may be insufficient for reliably guiding clinical choices, although the difference was statistically significant, and the model&#x02019;s confidence levels for correct and incorrect responses were generally high. Consequently, this gap does not provide a meaningful threshold for differentiating safe decision-making from potentially harmful decision-making in real-world practice. These results highlight potential risks in clinical applications, where model confidence, regardless of answer correctness, could lead to misinformed decisions.</p><p>We think that the observed miscalibration between correctness and confidence may pose risks in daily clinical practice if it remains unresolved. Overconfident models may recommend unsafe dosages or overlook key signs in a patient&#x02019;s presentation, especially under the fast-paced pressures of modern practice. This could lead to incorrect prescriptions or treatments. For example, the model might prescribe an incorrect antibiotic for a resistant infection, thereby delaying proper care. In other cases, a model&#x02019;s unwarranted confidence in a wrong triage decision could divert urgent attention from a critical patient. Such errors can increase morbidity and may undermine trust in AI-assisted clinical tools.</p><p>A brief comparison across models of various sizes did not reveal a consistent relationship between model size and confidence gaps. For instance, Qwen2-72B showed about a 2% difference in confidence between correct and incorrect responses, while Qwen2-7B exhibited a similarly small difference. This pattern was noted across multiple specialties, suggesting that architecture or domain-specific factors may play a more pivotal role than sheer model size in determining confidence behaviors.</p><p>Katz et al [<xref rid="R10" ref-type="bibr">10</xref>] reported that GPT-4 outperformed physicians in psychiatry and performed comparably to physicians in general surgery and internal medicine. Our study corroborates GPT-4&#x02019;s strong performance, particularly in psychiatry, where GPT-4o achieved 84.4% accuracy. However, our findings suggest that more cautious interpretation is needed, given the high confidence levels observed for incorrect answers. Xiong et al&#x02019;s [<xref rid="R17" ref-type="bibr">17</xref>] work on LLM confidence elicitation aligns with our observations of overconfidence. They noted improved calibration and failure prediction as model capability increased, which parallels our finding of better confidence calibration in more complex models.</p><p>If prompted confidence scores are truly driven by a model&#x02019;s internal representations and are not random or uncontextualized outputs, then consistently arbitrary numbers would suggest a disconnect between the model&#x02019;s knowledge state and its confidence estimates. Such misalignment can arise if the model&#x02019;s architecture, training data, or prompting strategies do not calibrate confidence with genuine certainty [<xref rid="R17" ref-type="bibr">17</xref>]. In other words, a system might systematically generate high confidence, regardless of accuracy, if it lacks mechanisms or fine-tuning for self-regulating uncertainty [<xref rid="R27" ref-type="bibr">27</xref>]. Even larger models sometimes yield small or inconsistent confidence gaps, indicating that domain-specific refinements or improved calibration may be required. Without such refinements, confidence levels may remain weakly tied to actual reasoning processes, meaning that they would not reflect well-grounded internal assessments.</p><p>The implications for clinical practice warrant careful consideration. Although the performance leap of newer models is promising, their inability to accurately self-assess confidence across wrong answers poses risks. Two possible strategies for addressing these challenges can be the use of human-in-the-loop protocols and the implementation of ensemble methods [<xref rid="R28" ref-type="bibr">28</xref>].</p><p>Human-AI collaboration may offer a balanced approach to leveraging AI strengths while maintaining necessary human oversight in health care [<xref rid="R29" ref-type="bibr">29</xref>]. Sezgin [<xref rid="R29" ref-type="bibr">29</xref>] suggested a human-in-the-loop approach for ensuring that AI systems are supervised via human expertise. However, the effective implementation of this approach faces challenges. The careful design of user interfaces is important for preventing automation bias [<xref rid="R29" ref-type="bibr">29</xref><xref rid="R30" ref-type="bibr">30</xref>]. There are also concerns about the potential erosion of clinical skills as a result of overreliance on AI [<xref rid="R31" ref-type="bibr">31</xref>].</p><p>Emerging evidence also suggests that some prompt engineering techniques can reduce but not completely eliminate sociodemographic bias in model outputs [<xref rid="R32" ref-type="bibr">32</xref>]. However, studies continue to reveal significant sociodemographic biases in LLMs, such as a large-scale study by Omar et al [<xref rid="R33" ref-type="bibr">33</xref>]. These biases may affect patient prioritization, treatment recommendations, and mental health screening across different groups, potentially driving disparities in care [<xref rid="R33" ref-type="bibr">33</xref>]. Simply removing demographic variables (eg, gender and race) may also risk overlooking clinically relevant distinctions. In the context of our study, better-calibrated confidence outputs may help to mitigate such biases by allowing models to reliably signal uncertainty, which is especially important for sensitive medical decisions. Nonetheless, the comprehensive evaluation of these strategies requires longitudinal studies that monitor the evolution of biases and large-scale, globally diverse datasets, which can be used to refine mitigation approaches.</p><p>Ensemble methods, which aggregate multiple models, present another possible strategy [<xref rid="R34" ref-type="bibr">34</xref>]. Mahajan et al [<xref rid="R35" ref-type="bibr">35</xref>] conducted a review of ensemble learning techniques for disease prediction. They found that stacking&#x02014;an ensemble method that combines multiple classifiers&#x02014;showed the most accurate performance in 19 out of 23 cases. The voting approach was identified as the second-best ensemble method. However, ensemble methods are computationally intensive and may introduce latency in real-time clinical applications [<xref rid="R36" ref-type="bibr">36</xref>]. In some scenarios, a slight increase in overall accuracy might justify extra processing time, yet in urgent applications (eg, emergency triage), even brief delays can be problematic. Ensemble methods aggregate outputs from multiple models, distributing the &#x0201c;confidence load&#x0201d; so that individual sources of skewed certainty are less influential. However, our findings suggest that many current models show miscalibrated confidence levels. If all component models in an ensemble are prone to the same calibration issues, combining them may amplify rather than correct erroneous certainty.</p><p>Both strategies&#x02014;human-in-the-loop protocols and the implementation of ensemble methods&#x02014;would require extensive clinical trials for validation and the development of model-specific calibration curves for each medical specialty.</p><p>Our study has several limitations. The dataset was limited to 1965 multiple-choice questions for 5 medical specialties; therefore, the dataset may not fully represent the breadth of clinical scenarios. Further, the combination of automatic rephrasing and manual validation could have introduced bias [<xref rid="R25" ref-type="bibr">25</xref>]. We also used default model hyperparameters, which potentially limited performance optimization. To address these constraints, future work could expand the question sets (eg, by including a broader array of medical domains) and adopt real-world clinical data rather than purely examination-style questions. Additionally, custom hyperparameter tuning or advanced methods, such as RAG and fine-tuning, could be used to further refine model accuracy and confidence calibration [<xref rid="R37" ref-type="bibr">37</xref>], as the use of default hyperparameters, which may have varied across the evaluated LLMs, could have influenced their reported confidence levels. Finally, investigating computational cost and the time efficiency of deploying these models in clinical workflows would help to clarify practical feasibility.</p><p>In conclusion, better-performing LLMs show more aligned overall confidence levels, yet even the most accurate models still display minimal variation between right and wrong answers. This highlights a limitation in current self-assessment mechanisms and calls for further research. Future efforts could include larger and more diverse clinical datasets, domain-specific calibration strategies, and real-world testing to refine confidence estimates. Such work is critical before broader implementation of LLMs in clinical settings.</p></sec><sec sec-type="supplementary-material" id="s5"><title>Supplementary material</title><supplementary-material id="SAP1" position="float" content-type="local-data"><object-id pub-id-type="doi">10.2196/66917</object-id><label>Multimedia Appendix 1</label><caption><title>Supplementary materials with further information on the benchmarked large language models, their performance across different fields and specialties, and the prompt used for rephrasing the questions.</title></caption><media xlink:href="medinform-v13-e66917-s001.docx" xlink:title="DOCX File, 36 KB" id="d67e795" position="anchor"/></supplementary-material></sec></body><back><ack><title>Acknowledgments</title><p>We thank Dr Uriel Katz and the coauthors of the paper <italic toggle="yes">GPT versus Resident Physicians &#x02014; A Benchmark Based on Official Board Scores</italic> [<xref rid="R10" ref-type="bibr">10</xref>] for sharing the multiple-choice question dataset.</p></ack><notes><def-list><title>Abbreviations</title><def-item><term>AI</term><def><p>artificial intelligence</p></def></def-item><def-item><term>API</term><def><p>application programming interface</p></def></def-item><def-item><term>LLM</term><def><p>large language model</p></def></def-item><def-item><term>OBGYN</term><def><p>obstetrics and gynecology</p></def></def-item><def-item><term>RAG</term><def><p>retrieval-augmented generation</p></def></def-item></def-list></notes><ref-list><title>References</title><ref id="R1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Thirunavukarasu</surname><given-names>AJ</given-names></name>
<name><surname>Ting</surname><given-names>DSJ</given-names></name>
<name><surname>Elangovan</surname><given-names>K</given-names></name>
<name><surname>Gutierrez</surname><given-names>L</given-names></name>
<name><surname>Tan</surname><given-names>TF</given-names></name>
<name><surname>Ting</surname><given-names>DSW</given-names></name>
</person-group><article-title>Large language models in medicine</article-title><source>Nat Med</source><month>Aug</month><year>2023</year><volume>29</volume><issue>8</issue><fpage>1930</fpage><lpage>1940</lpage><comment>doi</comment><pub-id pub-id-type="doi">10.1038/s41591-023-02448-8</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">37460753</pub-id>
</element-citation></ref><ref id="R2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Clusmann</surname><given-names>J</given-names></name>
<name><surname>Kolbinger</surname><given-names>FR</given-names></name>
<name><surname>Muti</surname><given-names>HS</given-names></name>
<etal>et al</etal>
</person-group><article-title>The future landscape of large language models in medicine</article-title><source>Commun Med (Lond)</source><month>Oct</month><day>10</day><year>2023</year><volume>3</volume><issue>1</issue><elocation-id>141</elocation-id><comment>doi</comment><pub-id pub-id-type="doi">10.1038/s43856-023-00370-1</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">37816837</pub-id>
</element-citation></ref><ref id="R3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tayebi Arasteh</surname><given-names>S</given-names></name>
<name><surname>Han</surname><given-names>T</given-names></name>
<name><surname>Lotfinia</surname><given-names>M</given-names></name>
<etal>et al</etal>
</person-group><article-title>Large language models streamline automated machine learning for clinical studies</article-title><source>Nat Commun</source><month>Feb</month><day>21</day><year>2024</year><volume>15</volume><issue>1</issue><elocation-id>1603</elocation-id><comment>doi</comment><pub-id pub-id-type="doi">10.1038/s41467-024-45879-8</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">38383555</pub-id>
</element-citation></ref><ref id="R4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shah</surname><given-names>NH</given-names></name>
<name><surname>Entwistle</surname><given-names>D</given-names></name>
<name><surname>Pfeffer</surname><given-names>MA</given-names></name>
</person-group><article-title>Creation and adoption of large language models in medicine</article-title><source>JAMA</source><month>Sep</month><day>5</day><year>2023</year><volume>330</volume><issue>9</issue><fpage>866</fpage><lpage>869</lpage><comment>doi</comment><pub-id pub-id-type="doi">10.1001/jama.2023.14217</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">37548965</pub-id>
</element-citation></ref><ref id="R5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kanjee</surname><given-names>Z</given-names></name>
<name><surname>Crowe</surname><given-names>B</given-names></name>
<name><surname>Rodman</surname><given-names>A</given-names></name>
</person-group><article-title>Accuracy of a generative artificial intelligence model in a complex diagnostic challenge</article-title><source>JAMA</source><month>07</month><day>3</day><year>2023</year><volume>330</volume><issue>1</issue><fpage>78</fpage><lpage>80</lpage><comment>doi</comment><pub-id pub-id-type="doi">10.1001/jama.2023.8288</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">37318797</pub-id>
</element-citation></ref><ref id="R6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Singhal</surname><given-names>K</given-names></name>
<name><surname>Azizi</surname><given-names>S</given-names></name>
<name><surname>Tu</surname><given-names>T</given-names></name>
<etal>et al</etal>
</person-group><article-title>Large language models encode clinical knowledge</article-title><source>Nature</source><month>Aug</month><year>2023</year><volume>620</volume><issue>7972</issue><fpage>172</fpage><lpage>180</lpage><comment>doi</comment><pub-id pub-id-type="doi">10.1038/s41586-023-06291-2</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">37438534</pub-id>
</element-citation></ref><ref id="R7"><label>7.</label><element-citation publication-type="preprint"><person-group person-group-type="author">
<name><surname>Johnson</surname><given-names>D</given-names></name>
<name><surname>Goodman</surname><given-names>R</given-names></name>
<name><surname>Patrinely</surname><given-names>J</given-names></name>
<etal>et al</etal>
</person-group><article-title>Assessing the accuracy and reliability of AI-generated medical responses: an evaluation of the Chat-GPT model</article-title><source>Res Sq</source><comment>Preprint posted online on</comment><month>Feb</month><day>28</day><year>2023</year><comment>doi</comment><pub-id pub-id-type="doi">10.21203/rs.3.rs-2566942/v1</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">36909565</pub-id>
</element-citation></ref><ref id="R8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Giannos</surname><given-names>P</given-names></name>
</person-group><article-title>Evaluating the limits of AI in medical specialisation: ChatGPT&#x02019;s performance on the UK Neurology Specialty Certificate Examination</article-title><source>BMJ Neurol Open</source><month>Jun</month><day>15</day><year>2023</year><volume>5</volume><issue>1</issue><elocation-id>e000451</elocation-id><comment>doi</comment><pub-id pub-id-type="doi">10.1136/bmjno-2023-000451</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">37337531</pub-id>
</element-citation></ref><ref id="R9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hoch</surname><given-names>CC</given-names></name>
<name><surname>Wollenberg</surname><given-names>B</given-names></name>
<name><surname>L&#x000fc;ers</surname><given-names>JC</given-names></name>
<etal>et al</etal>
</person-group><article-title>ChatGPT&#x02019;s quiz skills in different otolaryngology subspecialties: an analysis of 2576 single-choice and multiple-choice board certification preparation questions</article-title><source>Eur Arch Otorhinolaryngol</source><month>Sep</month><year>2023</year><volume>280</volume><issue>9</issue><fpage>4271</fpage><lpage>4278</lpage><comment>doi</comment><pub-id pub-id-type="doi">10.1007/s00405-023-08051-4</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">37285018</pub-id>
</element-citation></ref><ref id="R10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Katz</surname><given-names>U</given-names></name>
<name><surname>Cohen</surname><given-names>E</given-names></name>
<name><surname>Shachar</surname><given-names>E</given-names></name>
<etal>et al</etal>
</person-group><article-title>GPT versus resident physicians &#x02014; a benchmark based on official board scores</article-title><source>NEJM AI</source><month>Apr</month><day>12</day><year>2024</year><volume>1</volume><issue>5</issue><elocation-id>AIdbp2300192</elocation-id><comment>doi</comment><pub-id pub-id-type="doi">10.1056/AIdbp2300192</pub-id></element-citation></ref><ref id="R11"><label>11.</label><element-citation publication-type="preprint"><person-group person-group-type="author">
<name><surname>Omar</surname><given-names>M</given-names></name>
<name><surname>Nassar</surname><given-names>S</given-names></name>
<name><surname>Hijaze</surname><given-names>K</given-names></name>
<name><surname>Glicksberg</surname><given-names>BS</given-names></name>
<name><surname>Nadkarni</surname><given-names>GN</given-names></name>
<name><surname>Klang</surname><given-names>E</given-names></name>
</person-group><article-title>Generating credible referenced medical research: a comparative study of OpenAI&#x02019;s Gpt-4 and Google&#x02019;s Gemini</article-title><source>SSRN</source><comment>Preprint posted online on</comment><month>Apr</month><day>2</day><year>2024</year><comment>doi</comment><pub-id pub-id-type="doi">10.2139/ssrn.4780940</pub-id></element-citation></ref><ref id="R12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Azamfirei</surname><given-names>R</given-names></name>
<name><surname>Kudchadkar</surname><given-names>SR</given-names></name>
<name><surname>Fackler</surname><given-names>J</given-names></name>
</person-group><article-title>Large language models and the perils of their hallucinations</article-title><source>Crit Care</source><month>Mar</month><day>21</day><year>2023</year><volume>27</volume><issue>1</issue><elocation-id>120</elocation-id><comment>doi</comment><pub-id pub-id-type="doi">10.1186/s13054-023-04393-x</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">36945051</pub-id>
</element-citation></ref><ref id="R13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>G</given-names></name>
<name><surname>Ye</surname><given-names>Q</given-names></name>
<name><surname>Xia</surname><given-names>J</given-names></name>
</person-group><article-title>Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: a mini-review, two showcases and beyond</article-title><source>Inf Fusion</source><month>01</month><year>2022</year><volume>77</volume><fpage>29</fpage><lpage>52</lpage><comment>doi</comment><pub-id pub-id-type="doi">10.1016/j.inffus.2021.07.016</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">34980946</pub-id>
</element-citation></ref><ref id="R14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Soroush</surname><given-names>A</given-names></name>
<name><surname>Glicksberg</surname><given-names>BS</given-names></name>
<name><surname>Zimlichman</surname><given-names>E</given-names></name>
<etal>et al</etal>
</person-group><article-title>Large language models are poor medical coders &#x02014; benchmarking of medical code querying</article-title><source>NEJM AI</source><month>Apr</month><day>19</day><year>2024</year><volume>1</volume><issue>5</issue><elocation-id>AIdbp2300040</elocation-id><comment>doi</comment><pub-id pub-id-type="doi">10.1056/AIdbp2300040</pub-id></element-citation></ref><ref id="R15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schwartz</surname><given-names>IS</given-names></name>
<name><surname>Link</surname><given-names>KE</given-names></name>
<name><surname>Daneshjou</surname><given-names>R</given-names></name>
<name><surname>Cort&#x000e9;s-Penfield</surname><given-names>N</given-names></name>
</person-group><article-title>Black box warning: large language models and the future of infectious diseases consultation</article-title><source>Clin Infect Dis</source><month>Apr</month><day>10</day><year>2024</year><volume>78</volume><issue>4</issue><fpage>860</fpage><lpage>866</lpage><comment>doi</comment><pub-id pub-id-type="doi">10.1093/cid/ciad633</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">37971399</pub-id>
</element-citation></ref><ref id="R16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Poon</surname><given-names>AIF</given-names></name>
<name><surname>Sung</surname><given-names>JJY</given-names></name>
</person-group><article-title>Opening the black box of AI-medicine</article-title><source>J Gastroenterol Hepatol</source><month>Mar</month><year>2021</year><volume>36</volume><issue>3</issue><fpage>581</fpage><lpage>584</lpage><comment>doi</comment><pub-id pub-id-type="doi">10.1111/jgh.15384</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">33709609</pub-id>
</element-citation></ref><ref id="R17"><label>17.</label><element-citation publication-type="preprint"><person-group person-group-type="author">
<name><surname>Xiong</surname><given-names>M</given-names></name>
<name><surname>Hu</surname><given-names>Z</given-names></name>
<name><surname>Lu</surname><given-names>X</given-names></name>
<etal>et al</etal>
</person-group><article-title>Can LLMs express their uncertainty? An empirical evaluation of confidence elicitation in LLMs</article-title><source>arXiv</source><comment>Preprint posted online on</comment><month>Mar</month><day>17</day><year>2024</year><comment>doi</comment><pub-id pub-id-type="doi">10.48550/arXiv.2306.13063</pub-id></element-citation></ref><ref id="R18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sarker</surname><given-names>IH</given-names></name>
</person-group><article-title>Machine learning: algorithms, real-world applications and research directions</article-title><source>SN Comput Sci</source><year>2021</year><volume>2</volume><issue>3</issue><elocation-id>160</elocation-id><comment>doi</comment><pub-id pub-id-type="doi">10.1007/s42979-021-00592-x</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">33778771</pub-id>
</element-citation></ref><ref id="R19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xiong</surname><given-names>G</given-names></name>
<name><surname>Jin</surname><given-names>Q</given-names></name>
<name><surname>Wang</surname><given-names>X</given-names></name>
<name><surname>Zhang</surname><given-names>M</given-names></name>
<name><surname>Lu</surname><given-names>Z</given-names></name>
<name><surname>Zhang</surname><given-names>A</given-names></name>
</person-group><article-title>Improving retrieval-augmented generation in medicine with iterative follow-up questions</article-title><source>Pac Symp Biocomput</source><year>2025</year><volume>30</volume><fpage>199</fpage><lpage>214</lpage><comment>Medline</comment><pub-id pub-id-type="pmid">39670371</pub-id>
</element-citation></ref><ref id="R20"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Townsend</surname><given-names>CM</given-names></name>
<name><surname>Beauchamp</surname><given-names>RD</given-names></name>
<name><surname>Evers</surname><given-names>BM</given-names></name>
<name><surname>Mattox</surname><given-names>KL</given-names></name>
</person-group><source>Sabiston Textbook of Surgery: The Biological Basis of Modern Surgical Practice</source><publisher-name>Elsevier Health Sciences</publisher-name><year>2016</year><comment>ISBN</comment><pub-id pub-id-type="isbn">9780323401630</pub-id></element-citation></ref><ref id="R21"><label>21.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Loscalzo</surname><given-names>J</given-names></name>
<name><surname>Fauci</surname><given-names>AS</given-names></name>
<name><surname>Kasper</surname><given-names>DL</given-names></name>
<name><surname>Hauser</surname><given-names>SL</given-names></name>
<name><surname>Longo</surname><given-names>DL</given-names></name>
<name><surname>Jameson</surname><given-names>JL</given-names></name>
</person-group><source>Harrison&#x02019;s Principles of Internal Medicine</source><publisher-name>McGraw Hill</publisher-name><year>2022</year><comment>URL</comment><ext-link xlink:href="https://cir.nii.ac.jp/crid/1130573781693502243" ext-link-type="uri">https://cir.nii.ac.jp/crid/1130573781693502243</ext-link><comment>Accessed</comment><date-in-citation>02-05-2025</date-in-citation></element-citation></ref><ref id="R22"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Kliegman</surname><given-names>RM</given-names></name>
<name><surname>Behrman</surname><given-names>RE</given-names></name>
<name><surname>Jenson</surname><given-names>HB</given-names></name>
<name><surname>Stanton</surname><given-names>BMD</given-names></name>
</person-group><source>Nelson Textbook of Pediatrics E-Book</source><publisher-name>Elsevier Health Sciences</publisher-name><year>2007</year></element-citation></ref><ref id="R23"><label>23.</label><element-citation publication-type="book"><person-group person-group-type="author">
<collab>American Psychiatric Association</collab>
</person-group><source>Diagnostic and Statistical Manual of Mental Disorders</source><publisher-name>American Psychiatric Publishing</publisher-name><year>2000</year><comment>URL</comment><ext-link xlink:href="https://cir.nii.ac.jp/crid/1573950399819987840" ext-link-type="uri">https://cir.nii.ac.jp/crid/1573950399819987840</ext-link><comment>Accessed</comment><date-in-citation>02-05-2025</date-in-citation></element-citation></ref><ref id="R24"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Gabbe</surname><given-names>SG</given-names></name>
<name><surname>Niebyl</surname><given-names>JR</given-names></name>
<name><surname>Simpson</surname><given-names>JL</given-names></name>
<etal>et al</etal>
</person-group><source>Obstetrics: Normal and Problem Pregnancies E-Book</source><publisher-name>Elsevier Health Sciences</publisher-name><year>2016</year><comment>ISBN</comment><pub-id pub-id-type="isbn">9781455733958</pub-id></element-citation></ref><ref id="R25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Soni</surname><given-names>S</given-names></name>
<name><surname>Roberts</surname><given-names>K</given-names></name>
</person-group><article-title>Paraphrasing to improve the performance of electronic health records question answering</article-title><source>AMIA Jt Summits Transl Sci Proc</source><month>05</month><day>30</day><year>2020</year><volume>2020</volume><fpage>626</fpage><lpage>635</lpage><comment>Medline</comment><pub-id pub-id-type="pmid">32477685</pub-id>
</element-citation></ref><ref id="R26"><label>26.</label><element-citation publication-type="preprint"><person-group person-group-type="author">
<collab>OpenAI</collab>
<name><surname>Achiam</surname><given-names>J</given-names></name>
<name><surname>Adler</surname><given-names>S</given-names></name>
<etal>et al</etal>
</person-group><article-title>GPT-4 technical report</article-title><source>arXiv</source><comment>Preprint posted online on</comment><month>Mar</month><day>4</day><year>2024</year><comment>doi</comment><pub-id pub-id-type="doi">10.48550/arXiv.2303.08774</pub-id></element-citation></ref><ref id="R27"><label>27.</label><element-citation publication-type="preprint"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>L</given-names></name>
<name><surname>Pan</surname><given-names>Y</given-names></name>
<name><surname>Li</surname><given-names>X</given-names></name>
<name><surname>Chen</surname><given-names>G</given-names></name>
</person-group><article-title>Uncertainty estimation and quantification for llms: a simple supervised approach</article-title><source>arXiv</source><comment>Preprint posted online on</comment><month>Oct</month><day>23</day><year>2024</year><comment>doi</comment><pub-id pub-id-type="doi">10.48550/arXiv.2404.15993</pub-id></element-citation></ref><ref id="R28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Longhurst</surname><given-names>CA</given-names></name>
<name><surname>Singh</surname><given-names>K</given-names></name>
<name><surname>Chopra</surname><given-names>A</given-names></name>
<name><surname>Atreja</surname><given-names>A</given-names></name>
<name><surname>Brownstein</surname><given-names>JS</given-names></name>
</person-group><article-title>A call for artificial intelligence implementation science centers to evaluate clinical effectiveness</article-title><source>NEJM AI</source><month>07</month><day>10</day><year>2024</year><volume>1</volume><issue>8</issue><elocation-id>AIp2400223</elocation-id><comment>doi</comment><pub-id pub-id-type="doi">10.1056/AIp2400223</pub-id></element-citation></ref><ref id="R29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sezgin</surname><given-names>E</given-names></name>
</person-group><article-title>Artificial intelligence in healthcare: complementing, not replacing, doctors and healthcare providers</article-title><source>Digit Health</source><month>07</month><year>2023</year><volume>9</volume><elocation-id>20552076231186520</elocation-id><comment>doi</comment><pub-id pub-id-type="doi">10.1177/20552076231186520</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">37426593</pub-id>
</element-citation></ref><ref id="R30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Straw</surname><given-names>I</given-names></name>
</person-group><article-title>The automation of bias in medical artificial intelligence (AI): decoding the past to create a better future</article-title><source>Artif Intell Med</source><month>Nov</month><year>2020</year><volume>110</volume><fpage>101965</fpage><comment>doi</comment><pub-id pub-id-type="doi">10.1016/j.artmed.2020.101965</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">33250145</pub-id>
</element-citation></ref><ref id="R31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>&#x0010c;artolovni</surname><given-names>A</given-names></name>
<name><surname>Male&#x00161;evi&#x00107;</surname><given-names>A</given-names></name>
<name><surname>Poslon</surname><given-names>L</given-names></name>
</person-group><article-title>Critical analysis of the AI impact on the patient-physician relationship: a multi-stakeholder qualitative study</article-title><source>Digit Health</source><month>Dec</month><day>19</day><year>2023</year><volume>9</volume><elocation-id>20552076231220833</elocation-id><comment>doi</comment><pub-id pub-id-type="doi">10.1177/20552076231220833</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">38130798</pub-id>
</element-citation></ref><ref id="R32"><label>32.</label><element-citation publication-type="preprint"><person-group person-group-type="author">
<name><surname>Omar</surname><given-names>M</given-names></name>
<name><surname>Sorin</surname><given-names>V</given-names></name>
<name><surname>Agbareia</surname><given-names>R</given-names></name>
<etal>et al</etal>
</person-group><article-title>Evaluating and addressing demographic disparities in medical large language models: a systematic review</article-title><source>medRxiv</source><comment>Preprint posted online on</comment><month>Sep</month><day>9</day><year>2024</year><comment>doi</comment><pub-id pub-id-type="doi">10.1101/2024.09.09.24313295</pub-id></element-citation></ref><ref id="R33"><label>33.</label><element-citation publication-type="preprint"><person-group person-group-type="author">
<name><surname>Omar</surname><given-names>M</given-names></name>
<name><surname>Soffer</surname><given-names>S</given-names></name>
<name><surname>Agbareia</surname><given-names>R</given-names></name>
<etal>et al</etal>
</person-group><article-title>Socio-demographic biases in medical decision-making by large language models: a large-scale multi-model analysis</article-title><source>medRxiv</source><comment>Preprint posted online on</comment><month>Oct</month><day>30</day><year>2024</year><comment>doi</comment><pub-id pub-id-type="doi">10.1101/2024.10.29.24316368</pub-id></element-citation></ref><ref id="R34"><label>34.</label><element-citation publication-type="preprint"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>H</given-names></name>
<name><surname>Li</surname><given-names>M</given-names></name>
<name><surname>Zhou</surname><given-names>H</given-names></name>
<name><surname>Xiao</surname><given-names>Y</given-names></name>
<name><surname>Fang</surname><given-names>Q</given-names></name>
<name><surname>Zhang</surname><given-names>R</given-names></name>
</person-group><article-title>One LLM is not enough: harnessing the power of ensemble learning for medical question answering</article-title><source>medRxiv</source><comment>Preprint posted online on</comment><month>Dec</month><day>24</day><year>2023</year><comment>doi</comment><pub-id pub-id-type="doi">10.1101/2023.12.21.23300380</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">38196648</pub-id>
</element-citation></ref><ref id="R35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mahajan</surname><given-names>P</given-names></name>
<name><surname>Uddin</surname><given-names>S</given-names></name>
<name><surname>Hajati</surname><given-names>F</given-names></name>
<name><surname>Moni</surname><given-names>MA</given-names></name>
</person-group><article-title>Ensemble learning for disease prediction: a review</article-title><source>Healthcare (Basel)</source><month>Jun</month><day>20</day><year>2023</year><volume>11</volume><issue>12</issue><elocation-id>1808</elocation-id><comment>doi</comment><pub-id pub-id-type="doi">10.3390/healthcare11121808</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">37372925</pub-id>
</element-citation></ref><ref id="R36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Edeh</surname><given-names>MO</given-names></name>
<name><surname>Dalal</surname><given-names>S</given-names></name>
<name><surname>Dhaou</surname><given-names>IB</given-names></name>
<etal>et al</etal>
</person-group><article-title>Artificial intelligence-based ensemble learning model for prediction of hepatitis C disease</article-title><source>Front Public Health</source><month>Apr</month><day>27</day><year>2022</year><volume>10</volume><elocation-id>892371</elocation-id><comment>doi</comment><pub-id pub-id-type="doi">10.3389/fpubh.2022.892371</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">35570979</pub-id>
</element-citation></ref><ref id="R37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Glicksberg</surname><given-names>BS</given-names></name>
<name><surname>Timsina</surname><given-names>P</given-names></name>
<name><surname>Patel</surname><given-names>D</given-names></name>
<etal>et al</etal>
</person-group><article-title>Evaluating the accuracy of a state-of-the-art large language model for prediction of admissions from the emergency room</article-title><source>J Am Med Inform Assoc</source><month>Sep</month><day>1</day><year>2024</year><volume>31</volume><issue>9</issue><fpage>1921</fpage><lpage>1928</lpage><comment>doi</comment><pub-id pub-id-type="doi">10.1093/jamia/ocae103</pub-id><comment>Medline</comment><pub-id pub-id-type="pmid">38771093</pub-id>
</element-citation></ref></ref-list></back></article>