<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40397888</article-id><article-id pub-id-type="pmc">PMC12094730</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0323689</article-id><article-id pub-id-type="publisher-id">PONE-D-24-59060</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Physiological Processes</subject><subj-group><subject>Sleep</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Electronics Engineering</subject><subj-group><subject>Electronics</subject><subj-group><subject>Accelerometers</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Cardiology</subject><subj-group><subject>Heart Rate</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject><subj-group><subject>Deep Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Medical Conditions</subject><subj-group><subject>Sleep Disorders</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Neurology</subject><subj-group><subject>Sleep Disorders</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Mathematical and Statistical Techniques</subject><subj-group><subject>Fourier Analysis</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Time-series visual representations for sleep stages classification</article-title><alt-title alt-title-type="running-head">Time-series visual representations for sleep stages classification</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0008-8283-0812</contrib-id><name><surname>Padovani Ederli</surname><given-names>Rebeca</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Vega-Oliveros</surname><given-names>Didier A.</given-names></name><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8429-4119</contrib-id><name><surname>Soriano-Vargas</surname><given-names>Aurea</given-names></name><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff003" ref-type="aff">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Rocha</surname><given-names>Anderson</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="currentaff001" ref-type="author-notes">
<sup>&#x02021;</sup>
</xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3333-6822</contrib-id><name><surname>Dias</surname><given-names>Zanoni</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="currentaff001" ref-type="author-notes">
<sup>&#x02021;</sup>
</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Institute of Computing, University of Campinas (Unicamp), Campinas, SP, Brazil</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Department of Science and Technology, Federal University of Sao Paulo (Unifesp), S&#x000e3;o Jos&#x000e9; dos Campos, SP, Brazil</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Departamento Acad&#x000e9;mico de Ciencia de Computaci&#x000f3;n y Datos, Universidad de Ingenier&#x000ed;a y Tecnolog&#x000ed;a (UTEC), Peru</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Zhang</surname><given-names>Xiaohui</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Bayer Crop Science United States: Bayer CropScience LP, UNITED STATES OF AMERICA</addr-line>
</aff><author-notes><corresp id="cor001">* E-mail: <email>rebeca@ic.unicamp.br</email></corresp><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="current-aff" id="currentaff001"><p>&#x02021; These authors also contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>21</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0323689</elocation-id><history><date date-type="received"><day>20</day><month>12</month><year>2024</year></date><date date-type="accepted"><day>13</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Padovani Ederli et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Padovani Ederli et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0323689.pdf">
</self-uri><abstract><p>Polysomnography is the standard method for sleep stage classification; however, it is costly and requires controlled environments, which can disrupt natural sleep patterns. Smartwatches offer a practical, non-invasive, and cost-effective alternative for sleep monitoring. Equipped with multiple sensors, smartwatches allow continuous data collection in home environments, making them valuable for promoting health and improving sleep habits. Traditional methods for sleep stage classification using smartwatch data often rely on raw data or extracted features combined with artificial intelligence techniques. Transforming time series into visual representations enables the application of two-dimensional convolutional neural networks, which excel in classification tasks. Despite their success in other domains, these methods are underexplored for sleep stage classification. To address this, we evaluated visual representations of time series data collected from accelerometer and heart rate sensors in smartwatches. Techniques such as Gramian Angular Field, Recurrence Plots, Markov Transition Field, and spectrograms were implemented. Additionally, image patching and ensemble methods were applied to enhance classification performance. The results demonstrated that Gramian Angular Field, combined with patching and ensembles, achieved superior performance, exceeding 82% balanced accuracy for two-stage classification and 62% for three-stage classification. A comparison with traditional approaches, conducted under identical conditions, showed that the proposed method outperformed others, offering improvements of up to 8 percentage points in two-stage classification and 9 percentage points in three-stage classification. These findings show that visual representations effectively capture key sleep patterns, enhancing classification accuracy and enabling more reliable health monitoring and earlier interventions. This study highlights that visual representations not only surpass traditional methods but also emerge as a competitive and effective approach for sleep stage classification based on smartwatch data, paving the way for future research.</p></abstract><funding-group><award-group id="award001"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100020144</institution-id><institution>Samsung Eletr&#x000f4;nica da Amaz&#x000f4;nia</institution></institution-wrap>
</funding-source><principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0008-8283-0812</contrib-id>
<name><surname>Padovani Ederli</surname><given-names>Rebeca</given-names></name>
</principal-award-recipient></award-group><funding-statement>Part of the results presented in this work was obtained through the project &#x0201c;Hub of Artificial Intelligence in Health and Wellbeing - Viva Bem,&#x0201d; funded by Samsung Eletr&#x000f4;nica da Amaz&#x000f4;nia Ltda., within the scope of the Information Technology Law 8.248/91. There was no additional external funding received for this study. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="16"/><table-count count="9"/><page-count count="30"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>The data underlying the results presented in the study are available from PhysioNet (<ext-link xlink:href="https://physionet.org/content/sleep-accel/1.0.0/" ext-link-type="uri">https://physionet.org/content/sleep-accel/1.0.0/</ext-link>).</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>The data underlying the results presented in the study are available from PhysioNet (<ext-link xlink:href="https://physionet.org/content/sleep-accel/1.0.0/" ext-link-type="uri">https://physionet.org/content/sleep-accel/1.0.0/</ext-link>).</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>The sleep stages are essential for maintaining health and diagnosing sleep disorders. According to the guidelines of the American Academy of Sleep Medicine (AASM), sleep stages are classified into five distinct categories: wake, NREM N1, NREM N2, NREM N3, and REM, each with specific physiological characteristics [<xref rid="pone.0323689.ref001" ref-type="bibr">1</xref>]. Sleep cycles, composed of NREM and REM stages, occur every 90 to 120 minutes throughout the night and are critical for restorative sleep. Alterations in these patterns often indicate the presence of sleep disorders [<xref rid="pone.0323689.ref002" ref-type="bibr">2</xref>]. To simplify analysis, some studies group these stages into four (wake, light sleep, deep sleep, and REM), three (wake, NREM, and REM), or two stages (wake and sleep) [<xref rid="pone.0323689.ref003" ref-type="bibr">3</xref>].</p><p>Polysomnography (PSG) is the gold standard for evaluating sleep stages in clinical settings, providing detailed data from multiple physiological signals. However, PSG presents notable challenges, including high equipment costs, patient discomfort, and the need for monitoring in controlled environments, which may disrupt natural sleep [<xref rid="pone.0323689.ref004" ref-type="bibr">4</xref>]. Smartwatches have emerged as a practical, less invasive alternative for home sleep monitoring. These devices, equipped with sensors such as accelerometers and heart rate monitors, enable continuous data collection, promoting habit adjustments and health improvements [<xref rid="pone.0323689.ref005" ref-type="bibr">5</xref>].</p><p>Sensor data, often organized as time series, have been widely used in various sleep-related applications [<xref rid="pone.0323689.ref006" ref-type="bibr">6</xref>&#x02013;<xref rid="pone.0323689.ref008" ref-type="bibr">8</xref>]. A common approach involves extracting features from these time series and using classical machine learning algorithms like Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Random Forest (RF). With advancements in deep learning, two prominent methodologies for processing raw data have gained attention. The first uses Recurrent Neural Networks (RNNs), such as Long Short-Term Memory (LSTM) [<xref rid="pone.0323689.ref009" ref-type="bibr">9</xref>] and Gated Recurrent Unit (GRU) [<xref rid="pone.0323689.ref010" ref-type="bibr">10</xref>], which are designed to capture temporal patterns and long-term dependencies. The second employs 1-dimensional convolutional neural networks (1D-CNNs), effective for learning local patterns in sequential data.</p><p>Studies have explored various devices and approaches to classify sleep stages. Single sensors, such as photoplethysmography (PPG), have been used to extract features and achieve promising results with classifiers like SVM [<xref rid="pone.0323689.ref011" ref-type="bibr">11</xref>]. Other studies combined wavelets with RF, revealing that variables like age and sleep periods influence performance [<xref rid="pone.0323689.ref012" ref-type="bibr">12</xref>]. Smartwatches equipped with accelerometers and PPG have demonstrated their potential as viable PSG alternatives by applying recurrent neural networks for classification [<xref rid="pone.0323689.ref013" ref-type="bibr">13</xref>]. Additionally, methods directly processing raw data with models such as LSTMs have shown success in analyzing activity and heart rate data [<xref rid="pone.0323689.ref014" ref-type="bibr">14</xref>]. Deep networks trained on multimodal PSG data further highlight the potential of end-to-end learning directly from raw inputs [<xref rid="pone.0323689.ref015" ref-type="bibr">15</xref>].</p><p>Despite advancements, existing approaches have limitations. Manual feature extraction often requires domain expertise, is sensitive to noise, and fails to capture complex relationships within the data [<xref rid="pone.0323689.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0323689.ref016" ref-type="bibr">16</xref>]. In contrast, raw data processing struggles with high dimensionality, noise, and difficulty identifying temporal and spatial patterns, leading to compromised interpretability [<xref rid="pone.0323689.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0323689.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0323689.ref015" ref-type="bibr">15</xref>].</p><p>The rise of 2D-CNNs has introduced a promising alternative. These networks are designed to detect local patterns, such as edges, textures, and shapes, making them highly effective for computer vision tasks. Pooling layers reduce data dimensionality while preserving crucial features, and the hierarchical structure of 2D-CNNs enables the analysis of visual patterns in two-dimensional data [<xref rid="pone.0323689.ref017" ref-type="bibr">17</xref>].</p><p>Transforming time series into visual representations has proven to be an effective technique for sensor data analysis. This approach converts one-dimensional time series into two-dimensional images, allowing the direct application of deep learning models. Representations like Recurrence Plots (RP) [<xref rid="pone.0323689.ref018" ref-type="bibr">18</xref>], Gramian Angular Fields (GAF) [<xref rid="pone.0323689.ref019" ref-type="bibr">19</xref>], Markov Transition Fields (MTF) [<xref rid="pone.0323689.ref019" ref-type="bibr">19</xref>], and spectrograms capture diverse aspects such as temporal features, state transitions, and phase space representations [<xref rid="pone.0323689.ref020" ref-type="bibr">20</xref>, <xref rid="pone.0323689.ref021" ref-type="bibr">21</xref>].</p><p>Image-based methods have achieved remarkable results in human activity recognition (HAR). For instance, integrating GAF, RP, and MTF into a convolutional model enhanced gymnastics action recognition [<xref rid="pone.0323689.ref022" ref-type="bibr">22</xref>]. Similarly, spectrograms of inertial and biological signals have improved the classification of activity intensity levels [<xref rid="pone.0323689.ref023" ref-type="bibr">23</xref>]. Gesture recognition studies have also utilized GAF and MTF, achieving high accuracy in classifying wrist movements related to food intake [<xref rid="pone.0323689.ref024" ref-type="bibr">24</xref>].</p><p>Although the transformation of smartwatch data into images is established in fields like HAR, its application in sleep stage classification remains underexplored. Addressing this gap, this study aims to classify sleep stages by leveraging smartwatch sensor data transformed into visual representations and applying deep learning techniques. The methodology uses the publicly available Sleep Accel dataset [<xref rid="pone.0323689.ref025" ref-type="bibr">25</xref>], containing accelerometer and heart rate data from Apple Watch devices, annotated with PSG-based sleep stages. Time series data were transformed into image representations, including RP, GAF, MTF, and spectrograms, to capture temporal and spatial patterns. Additionally, images were divided into patches, enabling the classification models to focus on local details. These models, combined with ensemble techniques, demonstrated improved prediction accuracy. Performance was evaluated using balanced accuracy and Cohen&#x02019;s kappa coefficient. Comparisons with traditional methods, such as raw data models and feature extraction approaches, highlighted the advantages of visual representations.</p><p>This study addresses two distinct classification tasks: (1) two-stage classification (sleep/wake) and (2) sleep stages classification (wake/NREM/REM). The primary goal of binary classification is to distinguish between wake and sleep periods, making it useful for applications that require basic sleep detection, such as large-scale monitoring of sleep-wake patterns or the initial assessment of sleep disorders. On the other hand, the sleep stages classification aims to identify more detailed patterns by separating sleep into NREM and REM stages, which is essential for more in-depth clinical analyses, such as detecting specific sleep disorders (e.g., sleep apnea or insomnia) and assessing sleep quality based on the sleep-wake cycle architecture.</p><p>The transformation of time series into visual representations, combined with deep learning techniques, has proven effective for both classification tasks, highlighting the versatility of the proposed methodology. This strategy advances the understanding of sleep patterns, representing a promising avenue for future research.</p></sec><sec sec-type="materials|methods" id="sec002"><title>Materials and methods</title><p>This section describes the publicly available database, methodology, and evaluation metrics adopted in this work.</p><sec id="sec003"><title>Dataset</title><p>The Sleep Accel dataset [<xref rid="pone.0323689.ref025" ref-type="bibr">25</xref>], collected with Apple Watch (series 2 and 3, <italic toggle="yes">Apple Inc.</italic>), is the most suitable publicly available dataset to date. It was collected at the University of Michigan between June 2017 and March 2019 and contains data from 31 subjects. The data includes step count, acceleration, heart rate recorded with the Apple Watch, and sleep stages labels, scored with PSG recordings according to AASM&#x02019;s standards. The time information (in seconds from the start of the PSG) is provided for each data point.</p><p>The Apple Watch uses a triaxial MEMS accelerometer, which measures acceleration in the <italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, and <italic toggle="yes">z</italic> directions (in <italic toggle="yes">g</italic>), and photoplethysmography (PPG) on the dorsal wrist, which obtains heart rate in beats per minute (bpm). The study uniquely utilized the Apple Watch for seven to 14 days, with participants spending the final night in a sleep lab for an 8-hour PSG recording while wearing the smartwatch. Notably, the participants in this study did not have any known sleep disorder diagnoses, ensuring the reliability of the data.</p><p>It is important to note that this dataset is imbalanced, considering the two-stage classification (sleep and wake) and the sleep stages classification (wake, NREM, and REM). The class proportions are 12 &#x0201c;Sleep&#x0201d; samples for each &#x0201c;Wake&#x0201d; sample (sleep/wake classification), and nine &#x0201c;NREM&#x0201d; samples for every three &#x0201c;REM&#x0201d; samples and each &#x0201c;Wake&#x0201d; sample (sleep stages classification). The class proportions and imbalance are visible in the graph in <xref rid="pone.0323689.g001" ref-type="fig">Fig 1</xref>, which shows the percentage of data for each class every hour.</p><fig position="float" id="pone.0323689.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g001</object-id><label>Fig 1</label><caption><title>Label distribution in sleep accel dataset.</title><p>Number of labels in the Sleep Accel dataset per time interval over 8 hours (considering three sleep stages). The dataset shows class imbalance, with 9 &#x0201c;NREM&#x0201d; labels for every 3 &#x0201c;REM&#x0201d; labels and 1 &#x0201c;Wake&#x0201d; label.</p></caption><graphic xlink:href="pone.0323689.g001" position="float"/></fig><p>In addition to the temporal distribution of labels shown in <xref rid="pone.0323689.g001" ref-type="fig">Fig 1</xref>, <xref rid="pone.0323689.t001" ref-type="table">Table 1</xref> provides a detailed breakdown of the dataset, presenting the absolute number of samples per class and their respective proportions. The data confirms the class imbalance, particularly in the two-stage classification, where the number of &#x0201c;Sleep&#x0201d; samples is significantly higher than &#x0201c;Wake&#x0201d; samples, and in the sleep stages classification, where &#x0201c;NREM&#x0201d; is the most frequent class.</p><table-wrap position="float" id="pone.0323689.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.t001</object-id><label>Table 1</label><caption><title>Distribution of samples per class for each classification task.</title><p>The percentages indicate the proportion of each class in relation to the total dataset.</p></caption><alternatives><graphic xlink:href="pone.0323689.t001" id="pone.0323689.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Classification</th><th align="left" rowspan="1" colspan="1">Class</th><th align="left" rowspan="1" colspan="1">Number of samples</th><th align="left" rowspan="1" colspan="1">Percentage (%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Two-stage</td><td align="left" rowspan="1" colspan="1">Wake</td><td align="left" rowspan="1" colspan="1">1935</td><td align="left" rowspan="1" colspan="1">7.7%</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Sleep</td><td align="left" rowspan="1" colspan="1">23364</td><td align="left" rowspan="1" colspan="1">92.3%</td></tr><tr><td align="left" rowspan="1" colspan="1">Sleep stages</td><td align="left" rowspan="1" colspan="1">Wake</td><td align="left" rowspan="1" colspan="1">1935</td><td align="left" rowspan="1" colspan="1">7.7%</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">NREM</td><td align="left" rowspan="1" colspan="1">17811</td><td align="left" rowspan="1" colspan="1">70.4%</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">REM</td><td align="left" rowspan="1" colspan="1">5553</td><td align="left" rowspan="1" colspan="1">21.9%</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec004"><title>Overview of visual representations for sleep stages classification</title><p>A time series is a set of observations collected sequentially over time, according to a specific sampling rate. In this work, time series are obtained for the entire duration of the sleep recording, with sampling rates varying according to the sensor used. These data, which may have timestamps in different formats, are analyzed in periods of 30 seconds, called &#x0201c;epochs&#x0201d;. The time series of PSG recordings are standardized into 30-second epochs, each with a corresponding label for a sleep stage. Accelerometer data from smartwatches are also time series, measuring the device&#x02019;s acceleration over time. The start and end times of the time series from different types of data (smartwatch and PSG) must be synchronized.</p><p>This paper investigates using visual representations of smartwatch data to classify sleep stages. The approach involves methods that incorporate both the spatial and temporal aspects of the data, integrating accelerometer (ACC) and heart rate (HR) information from smartwatches. The classification of sleep stages was simplified into classifying sleep/wake (binary classification) and wake/NREM/REM (sleep stages classification). Notably, the data are analyzed in real-time, meaning only the data available up to the current moment are considered, demonstrating the practical application of the research.</p><p>To provide a high-level overview of the proposed method, <xref rid="pone.0323689.g002" ref-type="fig">Fig 2</xref> illustrates the general pipeline followed in this study. The process starts with raw time series data from smartwatches, which are transformed into visual representations. The final output consists of predictions for two scenarios: two-stage (wake/sleep) and sleep stages classification (wake/NREM/REM).</p><fig position="float" id="pone.0323689.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g002</object-id><label>Fig 2</label><caption><title>General pipeline of the proposed method.</title><p>The process begins with time series data, which are transformed into visual representations. The final output consists of predictions for two scenarios: two-stage (wake/sleep) and sleep stages classification (wake/NREM/REM).</p></caption><graphic xlink:href="pone.0323689.g002" position="float"/></fig><p>A more detailed methodology breakdown is presented in <xref rid="pone.0323689.g003" ref-type="fig">Fig 3</xref>, where the illustrated process is applied to both classification scenarios. The transformation of raw data into images was applied using techniques found in some related works: RP, GAF, MTF, and spectrograms. The images were generated from ACC data and HR rate data separately, and ensemble techniques were also performed to combine the classifications obtained with each data type.</p><fig position="float" id="pone.0323689.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g003</object-id><label>Fig 3</label><caption><title>Proposed methodology scheme.</title><p>The process begins with transforming raw accelerometer (ACC) data (pink) and heart rate (HR) data (blue) into visual representations. These visual representations serve as the input for training and validation. An ACC and HR data ensemble is created and validated (gray). The images are divided into patches used as inputs for their respective training sessions. Validation is carried out based on the ensemble results obtained from all patches. Finally, the ACC + HR ensemble is performed and validated again after obtaining the ensemble results from the patches (gray).</p></caption><graphic xlink:href="pone.0323689.g003" position="float"/></fig><p>Using images to perform classification tasks allows the application of a technique that divides the original image into sub-images or patches. As shown in <xref rid="pone.0323689.g003" ref-type="fig">Fig 3</xref>, the images of different representations (RP, GAF, MTF, and spectrograms), in addition to being processed in the format in which they were initially generated, are also divided into patches. Ensembles are applied to combine the predictions of the models trained with the following inputs: original ACC and HR data, patches of ACC data, patches of HR data, and patches of ACC data and patches of HR data.</p><p>The following subsections present the details of the methodology regarding data preparation, representations, patches, and ensembles.</p></sec><sec id="sec005"><title>Data preparation</title><p>A measurement refers to a single data point collected from sensors. These measurements form a time series, a sequence of data points recorded at regular intervals. Before generating the images, the raw motion and heart rate data were interpolated. Each image was created using 600 measurements [<xref rid="pone.0323689.ref025" ref-type="bibr">25</xref>], resulting in 600<inline-formula id="pone.0323689.e001"><alternatives><graphic xlink:href="pone.0323689.e001.jpg" id="pone.0323689.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>600 pixels, except for spectrograms, with varied dimensions. For processing as input into a network, the images are resized to 224<inline-formula id="pone.0323689.e002"><alternatives><graphic xlink:href="pone.0323689.e002.jpg" id="pone.0323689.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>224 pixels.</p><p>Each image corresponds to a 30-second window by the PSG recording standard used in the literature. Therefore, each image is classified as sleep or wake in the binary classification problem and as Wake, NREM, or &#x0201c;REM&#x0201d; in the three-stage classification.</p><p>The different image representations were obtained for each accelerometer axis (<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, and <italic toggle="yes">z</italic>), and an RGB image combining <italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, and <italic toggle="yes">z</italic> is generated to support information from all three axes. This strategy is addressed in related works [<xref rid="pone.0323689.ref026" ref-type="bibr">26</xref>], and it is essential to highlight that using images generated from individual axes does not take advantage of all the motion information [<xref rid="pone.0323689.ref027" ref-type="bibr">27</xref>]. Since the heart rate data consists of a single value (in bpm), the images are generated in grayscale (without performing RGB composition).</p></sec><sec id="sec006"><title>Transforming time series into visual representations</title><p>Visual representations of time series, including Gramian Angular Field (GAF), Markov Transition Field (MTF), Recurrence Plots (RP) and spectrograms, provide practical advantages for analyzing one-dimensional data. Transforming time series into images captures complex features, such as temporal patterns, state transition dynamics, and recurrence structures. These methods allow data exploration in high-dimensional spaces, facilitating the extraction of robust features that are difficult to identify in one-dimensional formats. Additionally, this approach supports advanced deep learning techniques like CNNs, which are optimized for detecting visual patterns and can automatically learn features without requiring manual segmentation or domain-specific expertise. This improves classification accuracy while reducing preprocessing requirements, making the process more efficient and scalable for various applications [<xref rid="pone.0323689.ref028" ref-type="bibr">28</xref>, <xref rid="pone.0323689.ref029" ref-type="bibr">29</xref>].</p><p>Time series were transformed into images using four types of representations: RP, GAF, MTF, and spectrograms. The details of each representation are presented in this subsection. Some examples of images obtained from accelerometer and heart rate data can be seen in <xref rid="pone.0323689.g004" ref-type="fig">Fig 4</xref> and <xref rid="pone.0323689.g005" ref-type="fig">Fig 5</xref>, respectively.</p><fig position="float" id="pone.0323689.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g004</object-id><label>Fig 4</label><caption><title>Accelerometer data images.</title><p>Images generated from the accelerometer data for each type of representation and class. RGB images combine the x, y, and z axes to utilize all motion information. It is possible to observe visual differences between the classes, indicating that the visual representations capture specific motion patterns associated with each sleep stage.</p></caption><graphic xlink:href="pone.0323689.g004" position="float"/></fig><fig position="float" id="pone.0323689.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g005</object-id><label>Fig 5</label><caption><title>Heart rate data images.</title><p>Images generated from the heart rate data for each type of representation and class. Grayscale images are used since heart rate data consists of a single value (in bpm). It is possible to observe visual differences between the classes, indicating that the visual representations capture specific heart rate patterns associated with each sleep stage.</p></caption><graphic xlink:href="pone.0323689.g005" position="float"/></fig><sec id="sec007"><title>Recurrence plots (RP)</title><p>RP representations were proposed by Eckmann <italic toggle="yes">et al</italic>. [<xref rid="pone.0323689.ref030" ref-type="bibr">30</xref>] for nonlinear analysis of time series data. These representations enable the visualization and acquisition of information about recurrent behavior in time series. The RP is an <italic toggle="yes">N</italic>
<inline-formula id="pone.0323689.e003"><alternatives><graphic xlink:href="pone.0323689.e003.jpg" id="pone.0323689.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
<italic toggle="yes">N</italic> matrix of points, where <italic toggle="yes">N</italic> is the number of states, and a recurrence occurs when a trajectory revisits the same neighborhood in phase space as at some previous time.</p><p>The recurrence matrix <italic toggle="yes">R</italic> can be described by <xref rid="pone.0323689.e006" ref-type="disp-formula">Eq 1</xref>, where <inline-formula id="pone.0323689.e004"><alternatives><graphic xlink:href="pone.0323689.e004.jpg" id="pone.0323689.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b5;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the recurrence threshold; <inline-formula id="pone.0323689.e005"><alternatives><graphic xlink:href="pone.0323689.e005.jpg" id="pone.0323689.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> represents the distance between the values corresponding to time <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> in the time series <italic toggle="yes">x</italic>. The values of the time series are normalized before the transformation to RP.</p><disp-formula id="pone.0323689.e006"><alternatives><graphic xlink:href="pone.0323689.e006.jpg" id="pone.0323689.e006g" position="anchor"/><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mtext>,&#x000a0;if&#x000a0;</mml:mtext><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02264;</mml:mo><mml:mi>&#x003b5;</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mtext>,&#x000a0;otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(1)</label></disp-formula><p>The traditional method of constructing RP can binarize the resulting matrix using different values for <inline-formula id="pone.0323689.e007"><alternatives><graphic xlink:href="pone.0323689.e007.jpg" id="pone.0323689.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b5;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, some of which were explored in a previous work [<xref rid="pone.0323689.ref027" ref-type="bibr">27</xref>].</p><p>In addition to <xref rid="pone.0323689.e006" ref-type="disp-formula">Eq 1</xref>, there are different variations of RP [<xref rid="pone.0323689.ref031" ref-type="bibr">31</xref>]. In this paper, the non-thresholded RP approach [<xref rid="pone.0323689.ref032" ref-type="bibr">32</xref>] (<xref rid="pone.0323689.e008" ref-type="disp-formula">Eq 2</xref>) was used, which maps the distances between pairs of points in a time series to a grayscale, providing a more compact visualization. For this, the calculated distances are normalized. Then, the values are inverted so that smaller distances correspond to darker shades in the grayscale, indicating greater proximity or similarity between the analyzed points:</p><disp-formula id="pone.0323689.e008"><alternatives><graphic xlink:href="pone.0323689.e008.jpg" id="pone.0323689.e008g" position="anchor"/><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo maxsize="2.470em" minsize="2.470em">&#x023a3;</mml:mo><mml:mn>255</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mo>min</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mo>max</mml:mo></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mo>min</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo maxsize="2.470em" minsize="2.470em">&#x023a6;</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(2)</label></disp-formula><p>where:</p><list list-type="bullet"><list-item><p><inline-formula id="pone.0323689.e009"><alternatives><graphic xlink:href="pone.0323689.e009.jpg" id="pone.0323689.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mo>min</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is the minimum value in the distance matrix <inline-formula id="pone.0323689.e010"><alternatives><graphic xlink:href="pone.0323689.e010.jpg" id="pone.0323689.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>;</p></list-item><list-item><p><inline-formula id="pone.0323689.e011"><alternatives><graphic xlink:href="pone.0323689.e011.jpg" id="pone.0323689.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mo>max</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is the maximum value in the distance matrix <inline-formula id="pone.0323689.e012"><alternatives><graphic xlink:href="pone.0323689.e012.jpg" id="pone.0323689.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>;</p></list-item><list-item><p><inline-formula id="pone.0323689.e013"><alternatives><graphic xlink:href="pone.0323689.e013.jpg" id="pone.0323689.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">&#x0230a;</mml:mo><mml:mi>&#x000b7;</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">&#x0230b;</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> represents the floor function, which rounds down to the nearest integer.</p></list-item></list><p>This equation normalizes the distances to the range of 0 to 255, transforming the distance matrix <inline-formula id="pone.0323689.e014"><alternatives><graphic xlink:href="pone.0323689.e014.jpg" id="pone.0323689.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> into a pixel matrix <inline-formula id="pone.0323689.e015"><alternatives><graphic xlink:href="pone.0323689.e015.jpg" id="pone.0323689.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for visualization as a grayscale image.</p></sec><sec id="sec008"><title>Gramian angular field (GAF)</title><p>The GAF representation [<xref rid="pone.0323689.ref019" ref-type="bibr">19</xref>] converts time series values into angles and uses these angles to generate a matrix that captures temporal relationships.</p><p>Given a time series <inline-formula id="pone.0323689.e016"><alternatives><graphic xlink:href="pone.0323689.e016.jpg" id="pone.0323689.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, the transformation to GAF involves normalizing <italic toggle="yes">X</italic> so that all its values are within the range <inline-formula id="pone.0323689.e017"><alternatives><graphic xlink:href="pone.0323689.e017.jpg" id="pone.0323689.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Then, the angular transformation converts each normalized value <inline-formula id="pone.0323689.e018"><alternatives><graphic xlink:href="pone.0323689.e018.jpg" id="pone.0323689.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> into an angle <inline-formula id="pone.0323689.e019"><alternatives><graphic xlink:href="pone.0323689.e019.jpg" id="pone.0323689.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> using the arccosine function:</p><disp-formula id="pone.0323689.e020"><alternatives><graphic xlink:href="pone.0323689.e020.jpg" id="pone.0323689.e020g" position="anchor"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>arccos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(3)</label></disp-formula><p>There are two main variants of GAF: Gramian Angular Summation Field (GASF) and Gramian Angular Difference Field (GADF). In this paper, the GADF representation, described in <xref rid="pone.0323689.e021" ref-type="disp-formula">Eq 4</xref>, was used because it is the most suitable for highlighting trend changes in time series, as GADF emphasizes the angular differences between consecutive time points [<xref rid="pone.0323689.ref028" ref-type="bibr">28</xref>]:</p><disp-formula id="pone.0323689.e021"><alternatives><graphic xlink:href="pone.0323689.e021.jpg" id="pone.0323689.e021g" position="anchor"/><mml:math id="M21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>GADF</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>sin</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(4)</label></disp-formula><p>This matrix encodes the angular relationships between the time series points. It can be visualized as images in which patterns and structures can be identified.</p></sec><sec id="sec009"><title>Markov transition field (MTF)</title><p>Each element of the MTF matrix [<xref rid="pone.0323689.ref019" ref-type="bibr">19</xref>] reflects the probability of transitioning between two states at different times in the time series, capturing the temporal dynamics of the series in a two-dimensional representation.</p><p>Given a time series <inline-formula id="pone.0323689.e022"><alternatives><graphic xlink:href="pone.0323689.e022.jpg" id="pone.0323689.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, the construction of the MTF is carried out by discretizing <italic toggle="yes">X</italic> into <italic toggle="yes">Q</italic> quantiles. This paper used <italic toggle="yes">Q</italic>&#x02009;=&#x02009;8, as described in a related work [<xref rid="pone.0323689.ref033" ref-type="bibr">33</xref>]. Each value <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub> in <italic toggle="yes">X</italic> is assigned to a corresponding quantile, resulting in a discretized series <inline-formula id="pone.0323689.e023"><alternatives><graphic xlink:href="pone.0323689.e023.jpg" id="pone.0323689.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pone.0323689.e024"><alternatives><graphic xlink:href="pone.0323689.e024.jpg" id="pone.0323689.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> represents the quantile to which <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub> belongs.</p><p>The Markov transition matrix <italic toggle="yes">W</italic>, of dimension <inline-formula id="pone.0323689.e025"><alternatives><graphic xlink:href="pone.0323689.e025.jpg" id="pone.0323689.e025g" position="anchor"/><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:mi>Q</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>Q</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, represents <italic toggle="yes">w</italic><sub><italic toggle="yes">ij</italic></sub> as the frequency with which a point in quantile <italic toggle="yes">q</italic><sub><italic toggle="yes">j</italic></sub> is followed by a point in quantile <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub> (<inline-formula id="pone.0323689.e026"><alternatives><graphic xlink:href="pone.0323689.e026.jpg" id="pone.0323689.e026g" position="anchor"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02192;</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>). After normalization, where <inline-formula id="pone.0323689.e027"><alternatives><graphic xlink:href="pone.0323689.e027.jpg" id="pone.0323689.e027g" position="anchor"/><mml:math id="M27" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>Q</mml:mi></mml:msubsup><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for each <italic toggle="yes">i</italic>, <italic toggle="yes">W</italic> becomes the Markov transition matrix. The matrix <italic toggle="yes">W</italic> before normalization is a count transition matrix and can be visualized as follows:</p><disp-formula id="pone.0323689.e028"><alternatives><graphic xlink:href="pone.0323689.e028.jpg" id="pone.0323689.e028g" position="anchor"/><mml:math id="M28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo><mml:mspace width="negativethinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(5)</label></disp-formula><p>where <italic toggle="yes">w</italic><sub><italic toggle="yes">ij</italic></sub> is the count of transition occurrences in the time series.</p><p>The MTF is a <italic toggle="yes">Q</italic>
<inline-formula id="pone.0323689.e029"><alternatives><graphic xlink:href="pone.0323689.e029.jpg" id="pone.0323689.e029g" position="anchor"/><mml:math id="M29" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
<italic toggle="yes">Q</italic> matrix, and in its construction, each element <italic toggle="yes">M</italic><sub><italic toggle="yes">ij</italic></sub> denotes the probability of transitioning from quantile <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub> to quantile <italic toggle="yes">q</italic><sub><italic toggle="yes">j</italic></sub>, considering the temporal positions in the series. MTF thus encodes the multi-scale transition probabilities of the time series. For instance, <italic toggle="yes">M</italic><sub><italic toggle="yes">ij</italic></sub> with <inline-formula id="pone.0323689.e030"><alternatives><graphic xlink:href="pone.0323689.e030.jpg" id="pone.0323689.e030g" position="anchor"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> &#x02212; <inline-formula id="pone.0323689.e031"><alternatives><graphic xlink:href="pone.0323689.e031.jpg" id="pone.0323689.e031g" position="anchor"/><mml:math id="M31" display="inline" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> represents the probability of transition between points with a temporal interval <italic toggle="yes">k</italic>. The main diagonal <italic toggle="yes">M</italic><sub><italic toggle="yes">ii</italic></sub> captures the self-transition probabilities.</p></sec><sec id="sec010"><title>Spectrograms</title><p>This transformation is performed by applying the Short-Time Fourier Transform (STFT), which essentially decomposes the signal into its component frequencies at different time instances, allowing visualization of how the signal&#x02019;s frequency spectrum varies over time [<xref rid="pone.0323689.ref034" ref-type="bibr">34</xref>].</p><p>Given a time series <italic toggle="yes">x</italic>(<italic toggle="yes">t</italic>), the STFT is defined as:</p><disp-formula id="pone.0323689.e032"><alternatives><graphic xlink:href="pone.0323689.e032.jpg" id="pone.0323689.e032g" position="anchor"/><mml:math id="M32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mo>&#x0221e;</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x0221e;</mml:mo></mml:mrow></mml:msubsup><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>j</mml:mi><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(6)</label></disp-formula><p>where</p><list list-type="bullet"><list-item><p><italic toggle="yes">x</italic>(<italic toggle="yes">n</italic>) is the value of the time series at time <italic toggle="yes">n</italic>;</p></list-item><list-item><p><italic toggle="yes">w</italic>(<italic toggle="yes">n</italic>&#x02013;<italic toggle="yes">m</italic>) is the window function applied to the signal, shifted by the frame index <italic toggle="yes">m</italic>;</p></list-item><list-item><p><italic toggle="yes">N</italic> is the total number of points used in the Fourier transform, influencing frequency resolution;</p></list-item><list-item><p><inline-formula id="pone.0323689.e033"><alternatives><graphic xlink:href="pone.0323689.e033.jpg" id="pone.0323689.e033g" position="anchor"/><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>j</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> represents the Fourier transform basis;</p></list-item><list-item><p><italic toggle="yes">m</italic> indicates the current frame position;</p></list-item><list-item><p><italic toggle="yes">k</italic> is the frequency index.</p></list-item></list><p>The Hann window function is defined as</p><disp-formula id="pone.0323689.e034"><alternatives><graphic xlink:href="pone.0323689.e034.jpg" id="pone.0323689.e034g" position="anchor"/><mml:math id="M34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>0.5</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">[</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>cos</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">]</mml:mo></mml:mrow><mml:mspace width="negativethinmathspace"/><mml:mo>,</mml:mo><mml:mtext>&#x000a0;if&#x000a0;</mml:mtext></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>M</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(7)</label></disp-formula><p>The time series is divided into smaller, overlapping time segments. The Fourier transform assumes the signal is periodic, which is not valid for most real signals. Therefore, the window limits the signal to a finite time interval, allowing for local frequency analysis. The Fourier transform is then applied to each time segment, transforming the data from the time domain to the frequency domain. This provides the amplitude and phase of the frequencies present in each time segment.</p><p>The results of the Fourier transform for each segment are organized into a matrix, where one dimension represents time (the time segments), and the other represents frequency. The matrix values represent the frequencies&#x02019; magnitude in each time segment.</p><p>The squared magnitude of the STFT is often used to construct the spectrogram of the signal, which is a visual representation of the intensity of frequencies as a function of time:</p><disp-formula id="pone.0323689.e035"><alternatives><graphic xlink:href="pone.0323689.e035.jpg" id="pone.0323689.e035g" position="anchor"/><mml:math id="M35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(8)</label></disp-formula><p>In the spectrogram, the <italic toggle="yes">x</italic> axis represents time, the <italic toggle="yes">y</italic> axis represents frequency, and the intensity of grayscale at a specific point represents the magnitude of the frequency at that time point.</p></sec></sec><sec id="sec011"><title>Patches</title><p>The technique of dividing an image into sub-images, or patches, is a common approach in image visualization and machine learning applications [<xref rid="pone.0323689.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0323689.ref036" ref-type="bibr">36</xref>], primarily aimed at improving focus on local details of the image. This allows the model to learn finer features that might be missed when observing the original image.</p><p>As <xref rid="pone.0323689.g006" ref-type="fig">Fig 6</xref> depicts, each image generated by different representations (600<inline-formula id="pone.0323689.e036"><alternatives><graphic xlink:href="pone.0323689.e036.jpg" id="pone.0323689.e036g" position="anchor"/><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>600 pixels or other dimensions, in the case of spectrograms) is divided into nine patches, and each patch (224<inline-formula id="pone.0323689.e037"><alternatives><graphic xlink:href="pone.0323689.e037.jpg" id="pone.0323689.e037g" position="anchor"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>224 pixels, with approximately 16% overlap between patches to avoid losing local information) is treated as an independent input for the training process. After training, the patches can be regrouped to make predictions about the entire image. A model is trained on each patch to predict the class of the corresponding patch region. Then, these predictions are combined to obtain the complete segmented image prediction.</p><fig position="float" id="pone.0323689.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g006</object-id><label>Fig 6</label><caption><title>&#x0201c;Wake&#x0201d; images generated with GAF and accelerometer data.</title><p>Example of an original image (600<inline-formula id="pone.0323689.e038"><alternatives><graphic xlink:href="pone.0323689.e038" id="pone.0323689.e038g" position="anchor"/><mml:math id="M38" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>600) and examples of patches (224<inline-formula id="pone.0323689.e039"><alternatives><graphic xlink:href="pone.0323689.e039" id="pone.0323689.e039g" position="anchor"/><mml:math id="M39" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>224 each).</p></caption><graphic xlink:href="pone.0323689.g006" position="float"/></fig></sec><sec id="sec012"><title>Ensembles</title><p>Ensemble techniques combine the predictions of multiple models to improve the robustness and accuracy of the final prediction, aiming to leverage individual decisions and mitigate the drawbacks of each model. In this context, an ensemble uses the predictions of models trained with:</p><list list-type="bullet"><list-item><p>original images of accelerometer data + original images of heart rate data: two models (one for each type of sensor);</p></list-item><list-item><p>patches of accelerometer data: nine models (one for each patch);</p></list-item><list-item><p>patches of heart rate data: nine models (one for each patch);</p></list-item><list-item><p>patches of accelerometer data + patches of heart rate data: 18 models (nine models for accelerometer data patches + nine models for heart rate data patches).</p></list-item></list><p>These techniques are helpful in complex problems where more than a single model may be required to capture all the nuances of the data [<xref rid="pone.0323689.ref037" ref-type="bibr">37</xref>]. The ensembles of simple averaging, weighted averaging, simple network, and deep features are described below.</p><sec id="sec013"><title>Simple averaging</title><p>In the simple averaging ensemble, the predictions of each classifier are combined by calculating the arithmetic mean of the predictions. For the binary classification problem, this means calculating the mean of the predicted probabilities for the classes &#x0201c;Sleep&#x0201d; and &#x0201c;Wake&#x0201d; by the classifiers. For the three-class classification, the mean of the predictions for each class is calculated individually. This type of ensemble was applied to combine the predictions of models trained with:</p><list list-type="bullet"><list-item><p>original images from accelerometer data + original images from heart rate data;</p></list-item><list-item><p>patches from accelerometer data;</p></list-item><list-item><p>patches from heart rate data; and</p></list-item><list-item><p>patches from accelerometer data + patches from heart rate data.</p></list-item></list></sec><sec id="sec014"><title>Weighted averaging</title><p>Weighted averaging is a variation of the simple averaging method, where each classifier&#x02019;s prediction contributes a different weight to the final prediction. The weights are usually assigned based on each classifier&#x02019;s performance. We generated 1000 random sets of <italic toggle="yes">n</italic> weights to find the best combination in each case. This type of ensemble was applied to combine the predictions of models trained with:</p><list list-type="bullet"><list-item><p>original images from accelerometer data + original images from heart rate data (<italic toggle="yes">n</italic>&#x02009;=&#x02009;2 number of sensors);</p></list-item><list-item><p>patches from accelerometer data (<italic toggle="yes">n</italic>&#x02009;=&#x02009;9 number of patches);</p></list-item><list-item><p>patches from heart rate data (<italic toggle="yes">n</italic>&#x02009;=&#x02009;9 number of patches); and</p></list-item><list-item><p>patches from accelerometer data + patches from heart rate data (<italic toggle="yes">n</italic>&#x02009;=&#x02009;9 <inline-formula id="pone.0323689.e040"><alternatives><graphic xlink:href="pone.0323689.e040.jpg" id="pone.0323689.e040g" position="anchor"/><mml:math id="M40" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 2&#x02009;=&#x02009; number of patches <inline-formula id="pone.0323689.e041"><alternatives><graphic xlink:href="pone.0323689.e041.jpg" id="pone.0323689.e041g" position="anchor"/><mml:math id="M41" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> number of data types).</p></list-item></list></sec><sec id="sec015"><title>Simple network</title><p>In this method, the classifiers&#x02019; predictions are input to a simple neural network, which learns the best way to combine these predictions. The predicted probabilities for the classes of interest by a classifier trained with patches are the inputs to this network, providing the final prediction. To maintain only one simple network, this ensemble was applied only with the predictions of models trained with:</p><list list-type="bullet"><list-item><p>patches from accelerometer data; and</p></list-item><list-item><p>patches from heart rate data.</p></list-item></list><p>Using models trained only with patches, multiple predictions contribute to a more informative input, unlike using accelerometer and heart rate data, where only two models would contribute. This makes a simple network less advantageous for combining only two predictions due to the additional effort not justified by the problem&#x02019;s complexity.</p></sec><sec id="sec016"><title>Deep features</title><p>In this type of ensemble, feature vectors are extracted from the deep layers of each classifier and combined to be used as input for a final model. This final model is trained to make the final prediction using these combined features, leveraging the data representations provided by the different classifiers. This type of ensemble was applied to combine the predictions of models trained with original images from accelerometer data + original images from heart rate data.</p><p>Although the diversity of information provided by the various deep feature vectors from patches can be informative, it also introduces significant complexity to the modeling process. This complexity manifests in the data dimensions to be processed. Working with only two deep feature vectors generated from accelerometer and heart rate data simplifies the modeling process.</p></sec></sec><sec id="sec017"><title>Training and validation</title><p>For training, we employed transfer learning using the EfficientNet-B0 model, a specific variant of the EfficientNet family [<xref rid="pone.0323689.ref038" ref-type="bibr">38</xref>], pre-trained on the extensive ImageNet dataset [<xref rid="pone.0323689.ref039" ref-type="bibr">39</xref>]. The strategy includes freezing some of the initial layers of these networks to preserve the learned generic features while the deeper layers are adapted to the specific dataset. This adaptation was carried out by adding a dense network at the end of the architecture, a process known as fine-tuning, allowing fine adjustments of the network parameters to fit the classes of interest better. EfficientNet-B0 was chosen due to its balance between high accuracy and computational efficiency, making it well-suited for tasks involving image classification. Additionally, other architectures, such as ResNet network [<xref rid="pone.0323689.ref040" ref-type="bibr">40</xref>], were also tested. However, EfficientNet-B0 consistently provided better performance in terms of accuracy and training time for the specific dataset used in this study.</p><p>The decision to freeze 90% of the initial layers of the EfficientNet-B0 network was based on a layer-freezing experiment. Different percentages of layers were frozen, ranging from 50% to 100%, and their impact on model performance was analyzed. Freezing 90% of the layers yielded the best trade-off, preserving general features while allowing the deeper layers to adapt to the dataset. Through these layer-freezing tests, it was established to use 90% of the first layers of the EfficientNet-B0 network frozen. Additionally, a dense layer of size 512 with <inline-formula id="pone.0323689.e042"><alternatives><graphic xlink:href="pone.0323689.e042.jpg" id="pone.0323689.e042g" position="anchor"/><mml:math id="M42" display="inline" overflow="scroll"><mml:mrow><mml:mn>50</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> dropout and a dense layer of size 256 with <inline-formula id="pone.0323689.e043"><alternatives><graphic xlink:href="pone.0323689.e043.jpg" id="pone.0323689.e043g" position="anchor"/><mml:math id="M43" display="inline" overflow="scroll"><mml:mrow><mml:mn>20</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> dropout were added.</p><p>The <italic toggle="yes">k-fold</italic> cross-validation technique was applied, with <italic toggle="yes">k</italic>&#x02009;=&#x02009;5, dividing the dataset into five distinct partitions to ensure that each sample was not used for both training and validation to evaluate the robustness and generalization of the models. We recall that data from the same subject were not simultaneously used for training and validation. Unlike stratified cross-validation, which preserves label distributions in each fold, we opted for a random split to maintain the natural variability of sleep stage transitions in real-world sleep patterns.</p><p>To determine whether the random split introduced significant discrepancies, we analyzed the class distribution in each split. <xref rid="pone.0323689.t002" ref-type="table">Table 2</xref> presents the percentage of Wake, NREM, and REM samples in the training and validation sets for sleep stage classification. For sleep/wake classification, where Sleep includes both NREM and REM, the distribution for Wake remains the same, while Sleep corresponds to the sum of NREM and REM. The results indicate that the overall distribution remains stable across splits, particularly in the training data, ensuring a balanced representation during model learning. While the validation distribution shows some variability, particularly in Split 1 and Split 2 for Wake and Split 5 for REM, this reflects real-world sleep data, where sleep stages are inherently imbalanced across different nights and individuals. Since the training data maintains a consistent distribution and the model is evaluated across multiple folds, the impact of these variations is minimized. Thus, the use of a random split does not introduce substantial bias or compromise the reliability of the results, as it allows the model to be tested under conditions that resemble real sleep patterns.</p><table-wrap position="float" id="pone.0323689.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.t002</object-id><label>Table 2</label><caption><title>Class distribution per split.</title><p>Percentage of Wake, NREM, and REM samples in the training and validation sets across the five splits of the cross-validation. The data indicate that the class distribution remains stable across splits, suggesting that the random split does not introduce substantial bias.</p></caption><alternatives><graphic xlink:href="pone.0323689.t002" id="pone.0323689.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1">Split</th><th align="left" colspan="4" rowspan="1">Train</th><th align="left" colspan="4" rowspan="1">Validation</th></tr><tr><th align="left" rowspan="1" colspan="1">Folds</th><th align="left" rowspan="1" colspan="1">Wake (%)</th><th align="left" rowspan="1" colspan="1">NREM (%)</th><th align="left" rowspan="1" colspan="1">REM (%)</th><th align="left" rowspan="1" colspan="1">Folds</th><th align="left" rowspan="1" colspan="1">Wake (%)</th><th align="left" rowspan="1" colspan="1">NREM (%)</th><th align="left" rowspan="1" colspan="1">REM (%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">2, 3, 4, 5</td><td align="left" rowspan="1" colspan="1">8.0</td><td align="left" rowspan="1" colspan="1">69.8</td><td align="left" rowspan="1" colspan="1">22.2</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">10.5</td><td align="left" rowspan="1" colspan="1">69.6</td><td align="left" rowspan="1" colspan="1">19.9</td></tr><tr><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">1, 3, 4, 5</td><td align="left" rowspan="1" colspan="1">9.3</td><td align="left" rowspan="1" colspan="1">69.8</td><td align="left" rowspan="1" colspan="1">20.9</td><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">5.3</td><td align="left" rowspan="1" colspan="1">69.6</td><td align="left" rowspan="1" colspan="1">25.1</td></tr><tr><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">1, 2, 4, 5</td><td align="left" rowspan="1" colspan="1">8.4</td><td align="left" rowspan="1" colspan="1">70.3</td><td align="left" rowspan="1" colspan="1">21.3</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">9.1</td><td align="left" rowspan="1" colspan="1">67.3</td><td align="left" rowspan="1" colspan="1">23.6</td></tr><tr><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">1, 2, 3, 5</td><td align="left" rowspan="1" colspan="1">8.6</td><td align="left" rowspan="1" colspan="1">70.1</td><td align="left" rowspan="1" colspan="1">21.3</td><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">8.3</td><td align="left" rowspan="1" colspan="1">68.3</td><td align="left" rowspan="1" colspan="1">23.4</td></tr><tr><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">1, 2, 3, 4</td><td align="left" rowspan="1" colspan="1">8.3</td><td align="left" rowspan="1" colspan="1">68.7</td><td align="left" rowspan="1" colspan="1">23.0</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">9.4</td><td align="left" rowspan="1" colspan="1">74.0</td><td align="left" rowspan="1" colspan="1">16.6</td></tr></tbody></table></alternatives></table-wrap><p>Recognizing the challenge posed by class imbalance in the dataset, the class weighting technique was applied, where weights are assigned to each class inversely proportional to their frequency in the dataset. This approach ensures that minority classes contribute more significantly to the loss function during training, preventing the model from being biased toward the majority class. By adjusting the importance of each class in this way, class weighting helps mitigate the imbalance effect, leading to a more equitable and representative training process.</p><p>The class weights <italic toggle="yes">w</italic><sub><italic toggle="yes">c</italic></sub> were calculated as the inverse of the class frequencies, normalized by the total number of samples:</p><disp-formula id="pone.0323689.e044"><alternatives><graphic xlink:href="pone.0323689.e044.jpg" id="pone.0323689.e044g" position="anchor"/><mml:math id="M44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(9)</label></disp-formula><p>where <italic toggle="yes">N</italic> is the total number of samples, <italic toggle="yes">C</italic> is the number of classes, <italic toggle="yes">n</italic><sub><italic toggle="yes">c</italic></sub> is the number of samples in class <italic toggle="yes">c</italic>. For binary cross-entropy, which is used for sleep/wake classification, the weighted loss <inline-formula id="pone.0323689.e045"><alternatives><graphic xlink:href="pone.0323689.e045.jpg" id="pone.0323689.e045g" position="anchor"/><mml:math id="M45" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x02112;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is computed as:</p><disp-formula id="pone.0323689.e046"><alternatives><graphic xlink:href="pone.0323689.e046.jpg" id="pone.0323689.e046g" position="anchor"/><mml:math id="M46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x02112;</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>pos</mml:mtext></mml:mrow></mml:msub><mml:mi>&#x000d7;</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>&#x000d7;</mml:mi><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>neg</mml:mtext></mml:mrow></mml:msub><mml:mi>&#x000d7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>&#x000d7;</mml:mi><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix" stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(10)</label></disp-formula><p>where <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub> the true label (0 or 1) for sample <italic toggle="yes">i</italic>, <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub> is the predicted probability for the positive class, <italic toggle="yes">w</italic><sub><italic toggle="yes">pos</italic></sub> and <italic toggle="yes">w</italic><sub><italic toggle="yes">neg</italic></sub> are the weights for the positive and negative classes, respectively. For categorical cross-entropy, which is used for sleep stages classification (three classes), the weighted loss <inline-formula id="pone.0323689.e047"><alternatives><graphic xlink:href="pone.0323689.e047.jpg" id="pone.0323689.e047g" position="anchor"/><mml:math id="M47" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x02112;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is computed as:</p><disp-formula id="pone.0323689.e048"><alternatives><graphic xlink:href="pone.0323689.e048.jpg" id="pone.0323689.e048g" position="anchor"/><mml:math id="M48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x02112;</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>w</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mi>&#x000d7;</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mi>&#x000d7;</mml:mi><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(11)</label></disp-formula><p>where <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic>,<italic toggle="yes">c</italic></sub> is a binary indicator (0 or 1) for whether class <italic toggle="yes">c</italic> is the correct classification for sample <italic toggle="yes">i</italic>, <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic>,<italic toggle="yes">c</italic></sub> is the predicted probability for class <italic toggle="yes">c</italic>, <italic toggle="yes">w</italic><sub><italic toggle="yes">c</italic></sub> is the weight assigned to class <italic toggle="yes">c</italic>.</p></sec><sec id="sec018"><title>Performance metrics and model evaluation</title><p>Although related works present accuracy as the main performance measure of the model, this paper uses balanced accuracy, given that the data contains imbalanced classes and that accuracy provides an optimistic estimate when a classifier is tested on an unbalanced dataset [<xref rid="pone.0323689.ref041" ref-type="bibr">41</xref>]. <xref rid="pone.0323689.e052" ref-type="disp-formula">Eq 13</xref> describes balanced accuracy, where <italic toggle="yes">c</italic> is the number of classes. In <xref rid="pone.0323689.e051" ref-type="disp-formula">Eq 12</xref>, <italic toggle="yes">TP</italic><sub><italic toggle="yes">n</italic></sub> is the number of true positives of class <italic toggle="yes">n</italic>, and <italic toggle="yes">FN</italic><sub><italic toggle="yes">n</italic></sub> is the number of false negatives of class <italic toggle="yes">n</italic>. Sensitivity and Cohen&#x02019;s kappa coefficient (<inline-formula id="pone.0323689.e049"><alternatives><graphic xlink:href="pone.0323689.e049.jpg" id="pone.0323689.e049g" position="anchor"/><mml:math id="M49" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>) were also used as metrics. In this context, the evaluators are the PSG labels and the automatic sleep stages classification algorithm. Cohen&#x02019;s definition of <inline-formula id="pone.0323689.e050"><alternatives><graphic xlink:href="pone.0323689.e050.jpg" id="pone.0323689.e050g" position="anchor"/><mml:math id="M50" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> [<xref rid="pone.0323689.ref042" ref-type="bibr">42</xref>] is described in <xref rid="pone.0323689.e053" ref-type="disp-formula">Eq 14</xref>.</p><p>The reported results correspond to the average obtained from 5-fold cross-validation. The standard deviation of balanced accuracy across the folds is also presented to quantify performance variability.</p><disp-formula id="pone.0323689.e051"><alternatives><graphic xlink:href="pone.0323689.e051.jpg" id="pone.0323689.e051g" position="anchor"/><mml:math id="M51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mtext>sensitivity</mml:mtext><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(12)</label></disp-formula><disp-formula id="pone.0323689.e052"><alternatives><graphic xlink:href="pone.0323689.e052.jpg" id="pone.0323689.e052g" position="anchor"/><mml:math id="M52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>balanced&#x000a0;accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mtext>sensitivity</mml:mtext><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(13)</label></disp-formula><disp-formula id="pone.0323689.e053"><alternatives><graphic xlink:href="pone.0323689.e053.jpg" id="pone.0323689.e053g" position="anchor"/><mml:math id="M53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003ba;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>%</mml:mi><mml:mtext>observed&#x000a0;agreement</mml:mtext><mml:mo>&#x02212;</mml:mo><mml:mi>%</mml:mi><mml:mtext>agreement&#x000a0;by&#x000a0;chance</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>%</mml:mi><mml:mtext>agreement&#x000a0;by&#x000a0;chance</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(14)</label></disp-formula></sec></sec><sec sec-type="conclusions" id="sec019"><title>Results and discussion</title><p>Here, we present the main experiments and results obtained with the Sleep Accel database [<xref rid="pone.0323689.ref025" ref-type="bibr">25</xref>] for sleep/wake and sleep stages classifications, along with the discussions. Subsequently, a comparison of the visual representation with other data representations typically used in related works is conducted.</p><sec id="sec020"><title>Sleep/wake classification</title><p>This subsection presents the balanced accuracies, <inline-formula id="pone.0323689.e054"><alternatives><graphic xlink:href="pone.0323689.e054.jpg" id="pone.0323689.e054g" position="anchor"/><mml:math id="M54" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> coefficients, and confusion matrices for sleep/wake classification. Additionally, we analyze a subject&#x02019;s night&#x02019;s sleep within this context.</p><sec id="sec021"><title>Balanced accuracy and Cohen&#x02019;s <inline-formula id="pone.0323689.e055"><alternatives><graphic xlink:href="pone.0323689.e055.jpg" id="pone.0323689.e055g" position="anchor"/><mml:math id="M55" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula></title><p><xref rid="pone.0323689.t003" ref-type="table">Table 3</xref> presents the balanced accuracies obtained with each sleep/wake classification representation. The complete table, which shows the results for individual patches, is available in <xref rid="pone.0323689.s001" ref-type="supplementary-material">S1 Table</xref>.</p><table-wrap position="float" id="pone.0323689.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.t003</object-id><label>Table 3</label><caption><title>Balanced accuracies obtained with each representation for sleep/wake classification.</title><p>Accelerometer data consistently outperformed heart rate data in all scenarios, with the GAF achieving the highest balanced accuracy (82.36% <inline-formula id="pone.0323689.e056"><alternatives><graphic xlink:href="pone.0323689.e056" id="pone.0323689.e056g" position="anchor"/><mml:math id="M56" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 3.24%) when using patch ensembles. Patch-based ensembles significantly improved balanced accuracy compared to original images.</p></caption><alternatives><graphic xlink:href="pone.0323689.t003" id="pone.0323689.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="2" rowspan="1">RP</th><th align="left" colspan="2" rowspan="1">GAF</th><th align="left" colspan="2" rowspan="1">MTF</th><th align="left" colspan="2" rowspan="1">Spectrograms</th></tr><tr><th align="left" rowspan="1" colspan="1">Network</th><th align="left" rowspan="1" colspan="1">Config.</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Eff.Net</td><td align="left" rowspan="1" colspan="1">Original</td><td align="left" rowspan="1" colspan="1">76.62</td><td align="left" rowspan="1" colspan="1">69.91</td><td align="left" rowspan="1" colspan="1">79.63</td><td align="left" rowspan="1" colspan="1">69.32</td><td align="left" rowspan="1" colspan="1">77.44</td><td align="left" rowspan="1" colspan="1">64.88</td><td align="left" rowspan="1" colspan="1">78.34</td><td align="left" rowspan="1" colspan="1">61.22</td></tr><tr><td align="left" rowspan="1" colspan="1">ACC + HR Ensembles</td><td align="left" rowspan="1" colspan="1">Simple Average</td><td align="center" colspan="2" rowspan="1">71.38</td><td align="center" colspan="2" rowspan="1">69.04</td><td align="center" colspan="2" rowspan="1">71.27</td><td align="center" colspan="2" rowspan="1">75.94</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Weighted Average</td><td align="center" colspan="2" rowspan="1">
<underline>76.54</underline>
</td><td align="center" colspan="2" rowspan="1">77.57</td><td align="center" colspan="2" rowspan="1">74.95</td><td align="center" colspan="2" rowspan="1">
<underline>78.19</underline>
</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Deep Features</td><td align="center" colspan="2" rowspan="1">75.59</td><td align="center" colspan="2" rowspan="1">
<underline>77.61</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>75.43</underline>
</td><td align="center" colspan="2" rowspan="1">77.12</td></tr><tr><td align="left" rowspan="1" colspan="1">Ensembles of Patches</td><td align="left" rowspan="1" colspan="1">Simple Average</td><td align="left" rowspan="1" colspan="1">
<underline>80.39</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>71.39</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>82.36</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>73.71</underline>
</td><td align="left" rowspan="1" colspan="1">80.03</td><td align="left" rowspan="1" colspan="1">
<underline>70.21</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>79.11</underline>
</td><td align="left" rowspan="1" colspan="1">54.90</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Weighted Average</td><td align="left" rowspan="1" colspan="1">80.38</td><td align="left" rowspan="1" colspan="1">71.28</td><td align="left" rowspan="1" colspan="1">82.04</td><td align="left" rowspan="1" colspan="1">72.15</td><td align="left" rowspan="1" colspan="1">
<underline>80.20</underline>
</td><td align="left" rowspan="1" colspan="1">68.78</td><td align="left" rowspan="1" colspan="1">79.01</td><td align="left" rowspan="1" colspan="1">
<underline>54.91</underline>
</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Simple Network</td><td align="left" rowspan="1" colspan="1">80.26</td><td align="left" rowspan="1" colspan="1">70.81</td><td align="left" rowspan="1" colspan="1">81.54</td><td align="left" rowspan="1" colspan="1">72.87</td><td align="left" rowspan="1" colspan="1">79.85</td><td align="left" rowspan="1" colspan="1">69.41</td><td align="left" rowspan="1" colspan="1">79.03</td><td align="left" rowspan="1" colspan="1">53.30</td></tr><tr><td align="left" rowspan="1" colspan="1">P. ACC + P. HR Ensembles</td><td align="left" rowspan="1" colspan="1">Simple Average</td><td align="center" colspan="2" rowspan="1">76.14</td><td align="center" colspan="2" rowspan="1">77.25</td><td align="center" colspan="2" rowspan="1">78.28</td><td align="center" colspan="2" rowspan="1">77.78</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Weighted Average</td><td align="center" colspan="2" rowspan="1">
<underline>79.28</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>81.44</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>80.32</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>78.64</underline>
</td></tr></tbody></table></alternatives><table-wrap-foot><fn><p>Note: The underlined values represent the highest balanced accuracies for the corresponding data/network configurations.</p></fn></table-wrap-foot></table-wrap><p>Compared to the heart rate data, the accelerometer data presents better results with all representations, both for the original images and for the individual patches and the ensembles of individual patches, for this scenario.</p><p>Observing the results with the ensembles that combine the different types of data (ACC and HR), it is noted that in no case do these present better-balanced accuracies compared to those obtained with the accelerometer data. In other words, the results obtained with heart rate data combined with accelerometer data do not contribute to a gain in balanced accuracy in this scenario. In turn, the results obtained with the ensembles of individual patches, both for the accelerometer data and the heart rate data, show a significant gain compared to the results obtained with the original images (up to 3.7 percentage points for RP, up to 2.7 for GAF, up to 2.9 for MTF, and up to 0.7 for Spectrograms).</p><p>Comparing the different representations, GAF achieved <inline-formula id="pone.0323689.e057"><alternatives><graphic xlink:href="pone.0323689.e057.jpg" id="pone.0323689.e057g" position="anchor"/><mml:math id="M57" display="inline" overflow="scroll"><mml:mrow><mml:mn>82</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> balanced accuracy with the ensemble of patches through simple averaging and accelerometer data, indicating the best sleep/wake classification result. The RP and MTF representations show balanced accuracies greater than <inline-formula id="pone.0323689.e058"><alternatives><graphic xlink:href="pone.0323689.e058.jpg" id="pone.0323689.e058g" position="anchor"/><mml:math id="M58" display="inline" overflow="scroll"><mml:mrow><mml:mn>80</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> with ensemble patches and accelerometer data (except MTF and an ensemble of patches with the simple network). With GAF and MTF, the ensemble of ACC patches + HR patches also shows results greater than <inline-formula id="pone.0323689.e059"><alternatives><graphic xlink:href="pone.0323689.e059.jpg" id="pone.0323689.e059g" position="anchor"/><mml:math id="M59" display="inline" overflow="scroll"><mml:mrow><mml:mn>80</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, and the Spectrograms exceed <inline-formula id="pone.0323689.e060"><alternatives><graphic xlink:href="pone.0323689.e060.jpg" id="pone.0323689.e060g" position="anchor"/><mml:math id="M60" display="inline" overflow="scroll"><mml:mrow><mml:mn>78</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</p><p>The standard deviations obtained from the balanced accuracies for two-stage sleep classification are up to <inline-formula id="pone.0323689.e061"><alternatives><graphic xlink:href="pone.0323689.e061.jpg" id="pone.0323689.e061g" position="anchor"/><mml:math id="M61" display="inline" overflow="scroll"><mml:mrow><mml:mn>3</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for RP, up to <inline-formula id="pone.0323689.e062"><alternatives><graphic xlink:href="pone.0323689.e062.jpg" id="pone.0323689.e062g" position="anchor"/><mml:math id="M62" display="inline" overflow="scroll"><mml:mrow><mml:mn>4</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for GAF, up to <inline-formula id="pone.0323689.e063"><alternatives><graphic xlink:href="pone.0323689.e063.jpg" id="pone.0323689.e063g" position="anchor"/><mml:math id="M63" display="inline" overflow="scroll"><mml:mrow><mml:mn>6</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for MTF, and up to <inline-formula id="pone.0323689.e064"><alternatives><graphic xlink:href="pone.0323689.e064.jpg" id="pone.0323689.e064g" position="anchor"/><mml:math id="M64" display="inline" overflow="scroll"><mml:mrow><mml:mn>4</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for Spectrograms.</p><p>The best <inline-formula id="pone.0323689.e065"><alternatives><graphic xlink:href="pone.0323689.e065.jpg" id="pone.0323689.e065g" position="anchor"/><mml:math id="M65" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> coefficients are concentrated in the ACC + HR ensemble results with deep features, exceeding 0.4 for the GAF representation (moderate agreement). For most configurations (except MTF patches 1 and 3), the <inline-formula id="pone.0323689.e066"><alternatives><graphic xlink:href="pone.0323689.e066.jpg" id="pone.0323689.e066g" position="anchor"/><mml:math id="M66" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> obtained with accelerometer data exceeds 0.2, indicating fair agreement. Whereas with heart rate data, <inline-formula id="pone.0323689.e067"><alternatives><graphic xlink:href="pone.0323689.e067.jpg" id="pone.0323689.e067g" position="anchor"/><mml:math id="M67" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> exceeds 0.2 only with the application of ensembles (and with GAF patch 6) and is lower with Spectrograms. This again highlights the importance of accelerometer data for sleep/wake classification.</p></sec><sec id="sec022"><title>Confusion matrices</title><p><xref rid="pone.0323689.g007" ref-type="fig">Fig 7</xref>-<xref rid="pone.0323689.g010" ref-type="fig">10</xref> show the confusion matrices for sleep/wake classification using RP, GAF, MTF and Spectrogram representations, respectively. These are generated with original accelerometer data, heart rate data and the best accelerometer + heart rate ensembles. Additionally, the figures include the confusion matrices for the best ensembles of accelerometer data patches heart rate data patches and accelerometer data patches + heart rate data patches.</p><fig position="float" id="pone.0323689.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g007</object-id><label>Fig 7</label><caption><title>RP confusion matrices for sleep/wake classification.</title><p>The highest balanced accuracy (80.39% <inline-formula id="pone.0323689.e068"><alternatives><graphic xlink:href="pone.0323689.e068" id="pone.0323689.e068g" position="anchor"/><mml:math id="M68" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 2.43%) was achieved with accelerometer data and the ensemble of patches, while the highest sensitivity (84%) was observed with the ensemble combining accelerometer and heart rate. Ensemble combining the original accelerometer and heart rate data achieves higher sensitivity than other approaches. Comparing confusion matrices from original data versus patches highlights an improvement in classifying the &#x0201c;Wake&#x0201d; stage. For accelerometer data and heart rate data, the use of patches increased the correct classification of &#x0201c;Wake&#x0201d;. Sleep/wake classification is more balanced when using accelerometer data, and this balance is further enhanced in the ensemble combining accelerometer and heart rate patches.</p></caption><graphic xlink:href="pone.0323689.g007" position="float"/></fig><fig position="float" id="pone.0323689.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g008</object-id><label>Fig 8</label><caption><title>GAF confusion matrices for sleep/wake classification.</title><p>The highest balanced accuracy (82.36% <inline-formula id="pone.0323689.e069"><alternatives><graphic xlink:href="pone.0323689.e069" id="pone.0323689.e069g" position="anchor"/><mml:math id="M69" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 3.24%) was achieved with accelerometer data and the ensemble of patches, while the highest sensitivity (85%) was observed with the ensemble combining accelerometer and heart rate patches. Sleep/wake classification using original heart rate data is more balanced than with original accelerometer data, where both classes are confused to a similar extent. Classification with original accelerometer data tends to overestimate &#x0201c;Sleep&#x0201d;. However, by improving predictions for &#x0201c;Wake&#x0201d; using patches, the accelerometer-based classification becomes more balanced.</p></caption><graphic xlink:href="pone.0323689.g008" position="float"/></fig><fig position="float" id="pone.0323689.g009"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g009</object-id><label>Fig 9</label><caption><title>MTF confusion matrices for sleep/wake classification.</title><p>The highest balanced accuracy (80.32% <inline-formula id="pone.0323689.e070"><alternatives><graphic xlink:href="pone.0323689.e070" id="pone.0323689.e070g" position="anchor"/><mml:math id="M70" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 3.43%) was achieved with the ensemble combining accelerometer and heart rate patches, while the highest sensitivity (87%) was observed with the ensemble combining accelerometer and heart rate. Using original accelerometer and heart rate data, &#x0201c;Sleep&#x0201d; is classified more accurately than &#x0201c;Wake&#x0201d;. This pattern is reflected in the ensemble of original data. As observed in the RP and GAF representations, the use of patches for accelerometer data leads to a more balanced classification.</p></caption><graphic xlink:href="pone.0323689.g009" position="float"/></fig><fig position="float" id="pone.0323689.g010"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g010</object-id><label>Fig 10</label><caption><title>Spectrograms confusion matrices for sleep/wake classification.</title><p>The highest balanced accuracy (79.11% <inline-formula id="pone.0323689.e071"><alternatives><graphic xlink:href="pone.0323689.e071" id="pone.0323689.e071g" position="anchor"/><mml:math id="M71" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 3.93%) was achieved with accelerometer data and the ensemble of patches, while the highest sensitivity (80%) was observed with both ensembles combining accelerometer and heart rate. Spectrogram representation shows balanced classifications for both original accelerometer and heart rate data. However, the ensemble of these original data better classifies &#x0201c;Sleep&#x0201d; stages. Using patches for accelerometer data improves the classification of &#x0201c;Wake&#x0201d;.</p></caption><graphic xlink:href="pone.0323689.g010" position="float"/></fig><p>The confusion matrices for the RP representation (<xref rid="pone.0323689.g007" ref-type="fig">Fig 7</xref>) reveal that the ensemble combining the original accelerometer and heart rate data achieves higher sensitivity than other approaches. Comparing confusion matrices from original data versus patches highlights an improvement in classifying the &#x0201c;Wake&#x0201d; stage. For accelerometer data and heart rate data, the use of patches increased the correct classification of &#x0201c;Wake.&#x0201d; Sleep/wake classification is more balanced when using accelerometer data, and this balance is further enhanced in the ensemble combining accelerometer and heart rate patches.</p><p>For the GAF representation (<xref rid="pone.0323689.g008" ref-type="fig">Fig 8</xref>), sleep/wake classification using original heart rate data is more balanced than with original accelerometer data, where both classes are confused to a similar extent. Classification with original accelerometer data tends to overestimate &#x0201c;Sleep.&#x0201d; However, by improving predictions for &#x0201c;Wake&#x0201d; using patches, the accelerometer-based classification becomes more balanced. The ensemble of accelerometer and heart rate patches achieves the highest sensitivity for this representation at 85%.</p><p>In the MTF representation (<xref rid="pone.0323689.g009" ref-type="fig">Fig 9</xref>), using original accelerometer and heart rate data, &#x0201c;Sleep&#x0201d; is classified more accurately than &#x0201c;Wake.&#x0201d; This pattern is reflected in the ensemble of original data. As observed in the RP and GAF representations, the use of patches for accelerometer data leads to a more balanced classification. Consequently, the ensemble of accelerometer and heart rate patches is also more balanced than the ensemble of original data, achieving a sensitivity of 87%.</p><p>The Spectrogram representation (<xref rid="pone.0323689.g010" ref-type="fig">Fig 10</xref>) shows balanced classifications for both original accelerometer and heart rate data. However, the ensemble of these original data better classifies &#x0201c;Sleep&#x0201d; stages. Using patches for accelerometer data improves the classification of &#x0201c;Wake&#x0201d;. Both ensembles show slightly better performance in classifying &#x0201c;Sleep&#x0201d; stages, achieving the highest sensitivity for this representation at 80%.</p></sec><sec id="sec023"><title>Analysis of a night of sleep</title><p>Analyzing an entire night of sleep provides a comprehensive view of the performance of the model in a real world scenario, where transitions between sleep and wake states occur naturally. This type of analysis is particularly useful for evaluating the consistency of predictions over extended periods and identifying potential limitations in the model&#x02019;s ability to capture subtle transitions or irregularities.</p><p><xref rid="pone.0323689.g011" ref-type="fig">Fig 11</xref> illustrates the predictions of a night&#x02019;s sleep for the same subject for sleep/wake classification using original accelerometer data and the ensemble of patches of accelerometer data. With these figures, it is possible to compare the best night&#x02019;s sleep obtained for sleep/wake classification (ensemble of accelerometer data patches) obtained through GAF with the corresponding night&#x02019;s sleep using original data. It can be observed, primarily, the decrease in &#x0201c;Sleep&#x0201d; prediction errors when the true class is &#x0201c;Wake&#x0201d; using the ensemble of patches. This fact was also illustrated in the corresponding confusion matrix (<xref rid="pone.0323689.g008" ref-type="fig">Fig 8</xref>).</p><fig position="float" id="pone.0323689.g011"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g011</object-id><label>Fig 11</label><caption><title>Sleep/wake classification over a night of sleep for a subject using the GAF representation.</title><p>Original data shows more &#x0201c;Sleep&#x0201d; errors for &#x0201c;Wake&#x0201d; at the beginning and around 6 hours, and frequent &#x0201c;Wake&#x0201d; errors for &#x0201c;Sleep&#x0201d; early on. Ensemble of patches reduces &#x0201c;Sleep&#x0201d; errors for &#x0201c;Wake&#x0201d;, with most &#x0201c;Wake&#x0201d; errors for &#x0201c;Sleep&#x0201d; around 2 and 6 hours.</p></caption><graphic xlink:href="pone.0323689.g011" position="float"/></fig><p>With the original data from the accelerometer, errors from &#x0201c;Sleep&#x0201d; to &#x0201c;Wake&#x0201d; occur more frequently in the early part of the night and, around 6 hours of sleep, this type of error occurs occasionally. The &#x0201c;Wake&#x0201d; errors for &#x0201c;Sleep&#x0201d; also occur more often during this initial phase. Using the ensemble of patches of accelerometer data, some &#x0201c;Sleep&#x0201d; errors for &#x0201c;Wake&#x0201d; occur only at the beginning of the night, while the &#x0201c;Wake&#x0201d; errors for &#x0201c;Sleep&#x0201d; occur more frequently around 2 and 6 hours of sleep.</p></sec></sec><sec id="sec024"><title>Sleep stages classification</title><p>This subsection presents the balanced accuracies, <inline-formula id="pone.0323689.e072"><alternatives><graphic xlink:href="pone.0323689.e072.jpg" id="pone.0323689.e072g" position="anchor"/><mml:math id="M72" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> coefficients, and confusion matrices obtained for sleep stages classification, along with an analysis of a subject&#x02019;s night of sleep in this scenario.</p><sec id="sec025"><title>Balanced accuracy and Cohen&#x02019;s <inline-formula id="pone.0323689.e073"><alternatives><graphic xlink:href="pone.0323689.e073.jpg" id="pone.0323689.e073g" position="anchor"/><mml:math id="M73" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula></title><p><xref rid="pone.0323689.t004" ref-type="table">Table 4</xref> presents the balanced accuracies obtained with each representation for sleep stages classification. The complete table, which shows the results for individual patches, is available in <xref rid="pone.0323689.s002" ref-type="supplementary-material">S2 Table</xref>.</p><table-wrap position="float" id="pone.0323689.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.t004</object-id><label>Table 4</label><caption><title>Balanced accuracies obtained with each representation for sleep stages classification.</title><p>Heart rate data often outperformed accelerometer data in balanced accuracies (except for the Spectrogram), with the GAF achieving the highest balanced accuracy (62.18% <inline-formula id="pone.0323689.e079"><alternatives><graphic xlink:href="pone.0323689.e079" id="pone.0323689.e079g" position="anchor"/><mml:math id="M79" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.95%) when using patch ensemble. Patch-based ensembles significantly improved balanced accuracy compared to original images.</p></caption><alternatives><graphic xlink:href="pone.0323689.t004" id="pone.0323689.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="2" rowspan="1">RP</th><th align="left" colspan="2" rowspan="1">GAF</th><th align="left" colspan="2" rowspan="1">MTF</th><th align="left" colspan="2" rowspan="1">Spectrograms</th></tr><tr><th align="left" rowspan="1" colspan="1">Network</th><th align="left" rowspan="1" colspan="1">Config.</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Eff.Net</td><td align="left" rowspan="1" colspan="1">Original</td><td align="left" rowspan="1" colspan="1">55.73</td><td align="left" rowspan="1" colspan="1">57.85</td><td align="left" rowspan="1" colspan="1">57.68</td><td align="left" rowspan="1" colspan="1">57.09</td><td align="left" rowspan="1" colspan="1">55.00</td><td align="left" rowspan="1" colspan="1">53.35</td><td align="left" rowspan="1" colspan="1">55.96</td><td align="left" rowspan="1" colspan="1">40.32</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Simple Average</td><td align="center" colspan="2" rowspan="1">46.01</td><td align="center" colspan="2" rowspan="1">46.14</td><td align="center" colspan="2" rowspan="1">46.35</td><td align="center" colspan="2" rowspan="1">52.41</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Weighted Average</td><td align="center" colspan="2" rowspan="1">50.24</td><td align="center" colspan="2" rowspan="1">50.10</td><td align="center" colspan="2" rowspan="1">50.37</td><td align="center" colspan="2" rowspan="1">
<underline>56.00</underline>
</td></tr><tr><td align="left" rowspan="1" colspan="1">ACC + HR Ensembles</td><td align="left" rowspan="1" colspan="1">Deep Features</td><td align="center" colspan="2" rowspan="1">
<underline>53.60</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>53.39</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>54.82</underline>
</td><td align="center" colspan="2" rowspan="1">51.01</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Simple Average</td><td align="left" rowspan="1" colspan="1">59.19</td><td align="left" rowspan="1" colspan="1">
<underline>61.87</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>60.66</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>62.18</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>58.38</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>57.81</underline>
</td><td align="left" rowspan="1" colspan="1">55.53</td><td align="left" rowspan="1" colspan="1">
<underline>39.50</underline>
</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Weighted Average</td><td align="left" rowspan="1" colspan="1">
<underline>59.41</underline>
</td><td align="left" rowspan="1" colspan="1">61.46</td><td align="left" rowspan="1" colspan="1">60.16</td><td align="left" rowspan="1" colspan="1">61.57</td><td align="left" rowspan="1" colspan="1">58.30</td><td align="left" rowspan="1" colspan="1">57.34</td><td align="left" rowspan="1" colspan="1">
<underline>57.36</underline>
</td><td align="left" rowspan="1" colspan="1">39.28</td></tr><tr><td align="left" rowspan="1" colspan="1">Ensembles of Patches</td><td align="left" rowspan="1" colspan="1">Simple Network</td><td align="left" rowspan="1" colspan="1">49.96</td><td align="left" rowspan="1" colspan="1">52.44</td><td align="left" rowspan="1" colspan="1">51.19</td><td align="left" rowspan="1" colspan="1">51.25</td><td align="left" rowspan="1" colspan="1">48.29</td><td align="left" rowspan="1" colspan="1">51.29</td><td align="left" rowspan="1" colspan="1">48.69</td><td align="left" rowspan="1" colspan="1">39.33</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Simple Average</td><td align="center" colspan="2" rowspan="1">49.85</td><td align="center" colspan="2" rowspan="1">49.98</td><td align="center" colspan="2" rowspan="1">50.95</td><td align="center" colspan="2" rowspan="1">53.10</td></tr><tr><td align="left" rowspan="1" colspan="1">P. ACC + P. HR Ensembles</td><td align="left" rowspan="1" colspan="1">Weighted Average</td><td align="center" colspan="2" rowspan="1">
<underline>61.48</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>61.17</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>60.97</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>55.35</underline>
</td></tr></tbody></table></alternatives><table-wrap-foot><fn><p>Note: The underlined values represent the highest balanced accuracies for the corresponding data/network configurations.</p></fn></table-wrap-foot></table-wrap><p>Analyzing <xref rid="pone.0323689.t004" ref-type="table">Table 4</xref>, it is noted that, in contrast to the results obtained for sleep/wake classification, the balanced accuracies displayed with heart rate data are higher than those with accelerometer data in many cases (except for the Spectrogram representation). To combine ACC + HR, the technique that presented the best-balanced accuracies was the ensemble of ACC patches + HR patches through weighted averaging, obtaining balanced accuracy above <inline-formula id="pone.0323689.e074"><alternatives><graphic xlink:href="pone.0323689.e074.jpg" id="pone.0323689.e074g" position="anchor"/><mml:math id="M74" display="inline" overflow="scroll"><mml:mrow><mml:mn>60</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for most representations (except Spectrograms).</p><p>Using Spectrograms, the ACC + HR ensemble, through weighted averaging with the original configuration data, showed an advantage over the individual original accelerometer and heart rate data. In contrast, the other representations did not improve balanced accuracy when applying this type of ensemble. On the other hand, the ensemble of ACC patches + HR patches with weighted averaging presented better balanced accuracies than the ensemble of accelerometer data patches for most representations. This again highlights the importance of heart rate data patches for this scenario.</p><p>As with sleep/wake classification, the best results of all representations involve ensembles of patches, exceeding <inline-formula id="pone.0323689.e075"><alternatives><graphic xlink:href="pone.0323689.e075.jpg" id="pone.0323689.e075g" position="anchor"/><mml:math id="M75" display="inline" overflow="scroll"><mml:mrow><mml:mn>62</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> with GAF, <inline-formula id="pone.0323689.e076"><alternatives><graphic xlink:href="pone.0323689.e076.jpg" id="pone.0323689.e076g" position="anchor"/><mml:math id="M76" display="inline" overflow="scroll"><mml:mrow><mml:mn>61</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> with RP, <inline-formula id="pone.0323689.e077"><alternatives><graphic xlink:href="pone.0323689.e077.jpg" id="pone.0323689.e077g" position="anchor"/><mml:math id="M77" display="inline" overflow="scroll"><mml:mrow><mml:mn>60</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> with MTF, and <inline-formula id="pone.0323689.e078"><alternatives><graphic xlink:href="pone.0323689.e078.jpg" id="pone.0323689.e078g" position="anchor"/><mml:math id="M78" display="inline" overflow="scroll"><mml:mrow><mml:mn>57</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> with Spectrograms. Comparing the best-balanced accuracies obtained and the original configuration of each representation, it is possible to observe gains of up to 4.0 percentage points for RP, 5.0 for GAF, 6.0 for MTF, and 1.4 for Spectrograms.</p><p>The standard deviations obtained from the balanced accuracies for sleep stages classification are up to <inline-formula id="pone.0323689.e080"><alternatives><graphic xlink:href="pone.0323689.e080.jpg" id="pone.0323689.e080g" position="anchor"/><mml:math id="M80" display="inline" overflow="scroll"><mml:mrow><mml:mn>6</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for RP, up to <inline-formula id="pone.0323689.e081"><alternatives><graphic xlink:href="pone.0323689.e081.jpg" id="pone.0323689.e081g" position="anchor"/><mml:math id="M81" display="inline" overflow="scroll"><mml:mrow><mml:mn>3</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for GAF, up to <inline-formula id="pone.0323689.e082"><alternatives><graphic xlink:href="pone.0323689.e082.jpg" id="pone.0323689.e082g" position="anchor"/><mml:math id="M82" display="inline" overflow="scroll"><mml:mrow><mml:mn>2</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for MTF, and up to <inline-formula id="pone.0323689.e083"><alternatives><graphic xlink:href="pone.0323689.e083.jpg" id="pone.0323689.e083g" position="anchor"/><mml:math id="M83" display="inline" overflow="scroll"><mml:mrow><mml:mn>3</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for Spectrograms.</p><p>As observed with the balanced accuracies, the best <inline-formula id="pone.0323689.e084"><alternatives><graphic xlink:href="pone.0323689.e084.jpg" id="pone.0323689.e084g" position="anchor"/><mml:math id="M84" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> were obtained with experiments involving ensembles of patches. Except for Spectrograms, the best <inline-formula id="pone.0323689.e085"><alternatives><graphic xlink:href="pone.0323689.e085.jpg" id="pone.0323689.e085g" position="anchor"/><mml:math id="M85" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> obtained for sleep stages classification were achieved with the ensemble of heart rate data patches (weighted averaging for RP and GAF and simple network for MTF), being <inline-formula id="pone.0323689.e086"><alternatives><graphic xlink:href="pone.0323689.e086.jpg" id="pone.0323689.e086g" position="anchor"/><mml:math id="M86" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.38</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for RP (fair agreement), <inline-formula id="pone.0323689.e087"><alternatives><graphic xlink:href="pone.0323689.e087.jpg" id="pone.0323689.e087g" position="anchor"/><mml:math id="M87" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.41</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for GAF (moderate agreement), and <inline-formula id="pone.0323689.e088"><alternatives><graphic xlink:href="pone.0323689.e088.jpg" id="pone.0323689.e088g" position="anchor"/><mml:math id="M88" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003ba;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.32</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for MTF (fair agreement). Again, this indicates the relevance of this type of data for sleep stages classification.</p></sec><sec id="sec026"><title>Confusion matrices</title><p><xref rid="pone.0323689.g012" ref-type="fig">Figs 12</xref>-<xref rid="pone.0323689.g015" ref-type="fig">15</xref> respectively show the confusion matrices generated for sleep stages classification using the RP, GAF, MTF and Spectrogram representations with the original accelerometer data, heart rate data and the best accelerometer + heart rate ensembles. The figures also include the confusion matrices of the best ensembles of accelerometer data patches, heart rate data patches and the best ensembles of accelerometer data patches + heart rate data patches.</p><fig position="float" id="pone.0323689.g012"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g012</object-id><label>Fig 12</label><caption><title>RP confusion matrices for sleep stages classification.</title><p>The highest balanced accuracy (61.87% <inline-formula id="pone.0323689.e089"><alternatives><graphic xlink:href="pone.0323689.e089" id="pone.0323689.e089g" position="anchor"/><mml:math id="M89" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 1.67%) was achieved with heart rate data and the ensemble of patches. Accelerometer data, including the ensemble results, achieved a higher number of correct classifications for &#x0201c;Wake&#x0201d;. In contrast, matrices generated with heart rate data alone showed more accurate classifications of &#x0201c;NREM&#x0201d; and &#x0201c;REM&#x0201d;. Additionally, with accelerometer data (both original and patches), the most frequent misclassification was labeling &#x0201c;NREM&#x0201d; as &#x0201c;REM&#x0201d;. For heart rate data, the most common error was classifying &#x0201c;Wake&#x0201d; as &#x0201c;REM&#x0201d;.</p></caption><graphic xlink:href="pone.0323689.g012" position="float"/></fig><fig position="float" id="pone.0323689.g013"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g013</object-id><label>Fig 13</label><caption><title>GAF confusion matrices for sleep stages classification.</title><p>The highest balanced accuracy (62.18% <inline-formula id="pone.0323689.e090"><alternatives><graphic xlink:href="pone.0323689.e090" id="pone.0323689.e090g" position="anchor"/><mml:math id="M90" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.95%) was achieved with heart rate data and the ensemble of patches. The confusion matrix with heart rate patches demonstrated an increase in correct classifications of &#x0201c;NREM&#x0201d; and &#x0201c;REM&#x0201d;. The most common misclassifications with accelerometer data were labeling &#x0201c;NREM&#x0201d; as &#x0201c;REM&#x0201d; and &#x0201c;REM&#x0201d; as &#x0201c;NREM&#x0201d;. Meanwhile, with heart rate data, the most frequent error was classifying &#x0201c;Wake&#x0201d; as &#x0201c;REM&#x0201d;.</p></caption><graphic xlink:href="pone.0323689.g013" position="float"/></fig><fig position="float" id="pone.0323689.g014"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g014</object-id><label>Fig 14</label><caption><title>MTF confusion matrices for sleep stages classification.</title><p>The highest balanced accuracy (60.97% <inline-formula id="pone.0323689.e091"><alternatives><graphic xlink:href="pone.0323689.e091" id="pone.0323689.e091g" position="anchor"/><mml:math id="M91" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 1.85%) was achieved with the ensemble combining accelerometer and heart rate patches. Accelerometer data and both types of ensembles most frequently classified &#x0201c;Wake&#x0201d; correctly, similar to other representations. For heart rate data, the incorrect classification of &#x0201c;Wake&#x0201d; as &#x0201c;REM&#x0201d; observed with original data decreased with the use of patches, resulting in a more balanced classification.</p></caption><graphic xlink:href="pone.0323689.g014" position="float"/></fig><fig position="float" id="pone.0323689.g015"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g015</object-id><label>Fig 15</label><caption><title>Spectrograms confusion matrices for sleep stages classification.</title><p>The highest balanced accuracy (57.36% <inline-formula id="pone.0323689.e092"><alternatives><graphic xlink:href="pone.0323689.e092" id="pone.0323689.e092g" position="anchor"/><mml:math id="M92" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 2.68%) was achieved with heart rate data and the ensemble of patches. Heart rate data, both in its original and patched forms, resulted in fewer classifications of &#x0201c;NREM&#x0201d; and &#x0201c;REM,&#x0201d; overestimating &#x0201c;Wake&#x0201d;. However, the patched configuration increased the number of correct &#x0201c;REM&#x0201d; classifications while reducing incorrect &#x0201c;Wake&#x0201d; predictions. Both ensemble configurations improved correct classifications of &#x0201c;Wake&#x0201d; but showed increased confusion for &#x0201c;NREM&#x0201d; when the true class was &#x0201c;REM&#x0201d;.</p></caption><graphic xlink:href="pone.0323689.g015" position="float"/></fig><p>The confusion matrices generated with RP (<xref rid="pone.0323689.g012" ref-type="fig">Fig 12</xref>) indicate that accelerometer data, including the ensemble results, achieved a higher number of correct classifications for &#x0201c;Wake.&#x0201d; In contrast, matrices generated with heart rate data alone showed more accurate classifications of &#x0201c;NREM&#x0201d; and &#x0201c;REM&#x0201d;. Additionally, with accelerometer data (both original and patches), the most frequent misclassification was labeling &#x0201c;NREM&#x0201d; as &#x0201c;REM.&#x0201d; For heart rate data, the most common error was classifying &#x0201c;Wake&#x0201d; as &#x0201c;REM.&#x0201d;</p><p>For the GAF representation (<xref rid="pone.0323689.g013" ref-type="fig">Fig 13</xref>), the confusion matrix with heart rate patches demonstrated an increase in correct classifications of &#x0201c;NREM&#x0201d; and &#x0201c;REM&#x0201d;. The most common misclassifications with accelerometer data were labeling &#x0201c;NREM&#x0201d; as &#x0201c;REM&#x0201d; and &#x0201c;REM&#x0201d; as &#x0201c;NREM.&#x0201d; Meanwhile, with heart rate data, the most frequent error was classifying &#x0201c;Wake&#x0201d; as &#x0201c;REM.&#x0201d;</p><p>The confusion matrices for the MTF representation (<xref rid="pone.0323689.g014" ref-type="fig">Fig 14</xref>) show that accelerometer data and both types of ensembles most frequently classified &#x0201c;Wake&#x0201d; correctly, similar to other representations. For heart rate data, the incorrect classification of &#x0201c;Wake&#x0201d; as &#x0201c;REM&#x0201d; observed with original data decreased with the use of patches, resulting in a more balanced classification.</p><p>With the Spectrogram representation (<xref rid="pone.0323689.g015" ref-type="fig">Fig 15</xref>), the confusion matrices suggest that heart rate data, both in its original and patched forms, resulted in fewer classifications of &#x0201c;NREM&#x0201d; and &#x0201c;REM,&#x0201d; overestimating &#x0201c;Wake.&#x0201d; However, the patched configuration increased the number of correct &#x0201c;REM&#x0201d; classifications while reducing incorrect &#x0201c;Wake&#x0201d; predictions. Both ensemble configurations improved correct classifications of &#x0201c;Wake&#x0201d; but showed increased confusion for &#x0201c;NREM&#x0201d; when the true class was &#x0201c;REM.&#x0201d;</p><p>For accelerometer data, &#x0201c;NREM&#x0201d; is often misclassified as &#x0201c;REM,&#x0201d; suggesting overlapping movement features. In contrast, heart rate data frequently misclassifies &#x0201c;Wake&#x0201d; as &#x0201c;REM,&#x0201d; possibly due to pattern similarities. Patched configurations reduce these errors, improving classification balance. For example, &#x0201c;Wake&#x0201d; misclassified as &#x0201c;REM&#x0201d; decreases with heart rate patches, showing their effectiveness in refining feature representation. &#x0201c;Wake&#x0201d; is consistently the most accurately classified stage across all representations (RP, GAF, MTF, Spectrogram), reflecting its distinct features. However, frequent misclassifications between &#x0201c;REM&#x0201d; and &#x0201c;NREM&#x0201d; indicate shared physiological traits or feature extraction limitations.</p></sec><sec id="sec027"><title>Analysis of a night of sleep</title><p><xref rid="pone.0323689.g016" ref-type="fig">Fig 16</xref> illustrates the predictions of a night&#x02019;s sleep for the same subject for sleep stages classification using original heart rate data and the ensemble of heart rate patches. By observing these figures, we can compare the best night&#x02019;s sleep for sleep stages classification (ensemble of heart rate patches using GAF) with the corresponding night&#x02019;s sleep using the original data.</p><fig position="float" id="pone.0323689.g016"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.g016</object-id><label>Fig 16</label><caption><title>Sleep stages classification over a night of sleep for a subject using the GAF representation.</title><p>Original data shows frequent &#x0201c;Wake&#x0201d; for &#x0201c;NREM&#x0201d; and &#x0201c;REM&#x0201d; for &#x0201c;NREM&#x0201d; errors. Ensemble of patches reduces these errors, but &#x0201c;REM&#x0201d; to &#x0201c;Wake&#x0201d; errors persist early on, and &#x0201c;NREM&#x0201d; to &#x0201c;Wake&#x0201d; errors appear around the 8-hour mark.</p></caption><graphic xlink:href="pone.0323689.g016" position="float"/></fig><p>With the original data, the most frequent errors are predictions of &#x0201c;Wake&#x0201d; for &#x0201c;NREM&#x0201d; and predictions of &#x0201c;REM&#x0201d; for &#x0201c;NREM.&#x0201d; It is also noted that predictions of &#x0201c;Wake&#x0201d; are overestimated for the &#x0201c;REM&#x0201d; class. Analyzing the errors related to &#x0201c;NREM&#x0201d; predictions, the model confused this class more with &#x0201c;REM&#x0201d; than with &#x0201c;Wake.&#x0201d; Finally, the least common misclassifications are &#x0201c;REM&#x0201d; and &#x0201c;Wake.&#x0201d;</p><p>When analyzing the results of the ensemble of heart rate patches, a significant reduction in incorrect predictions of &#x0201c;Wake&#x0201d; for &#x0201c;NREM&#x0201d; is observed. The erroneous predictions of &#x0201c;NREM&#x0201d; to &#x0201c;REM&#x0201d; also showed a decrease. However, the classification errors of &#x0201c;REM&#x0201d; to &#x0201c;Wake&#x0201d; at the beginning of the night remained. Additionally, there is a notable occurrence of incorrect predictions of &#x0201c;NREM&#x0201d; to &#x0201c;Wake&#x0201d; around the 8-hour mark of the test.</p></sec></sec><sec id="sec028"><title>Other representations</title><p>To compare the visual representation methods of time series with the traditional approaches discussed in the related works, we used the raw data as input for 1D-CNN, LSTM, and GRU-based networks. Additionally, we compared the results of performing feature extraction from raw data as input for RF and Logistic Regression (LR).</p><sec id="sec029"><title>Raw data</title><p>Models based on 1D-CNN, LSTM, and GRU were implemented directly on the raw data for comparison with visual representations. The network architecture is described by Mekruksavanich and Jitpattanakul [<xref rid="pone.0323689.ref043" ref-type="bibr">43</xref>] for HAR, with the first layer being a 1D convolution; the second, 1D <italic toggle="yes">MaxPooling</italic>; the third layer is another 1D convolution, followed by another <italic toggle="yes">MaxPooling</italic>; the penultimate layer is LSTM, and the last, a dense layer. To compare with models based on 1D-CNN and GRU, the penultimate layer of this architecture is replaced by a 1D-CNN and a GRU, respectively.</p><p>As with the visual representations, the networks received accelerometer data and heart rate data as input, and with the trained models, ensembles were performed using simple averaging, weighted averaging, and deep features to combine the results obtained with accelerometer data + heart rate data.</p><p>The results obtained for sleep/wake classification and sleep stages classification, with each configuration and each network, are presented in <xref rid="pone.0323689.t005" ref-type="table">Tables 5</xref> and <xref rid="pone.0323689.t006" ref-type="table">6</xref>.</p><table-wrap position="float" id="pone.0323689.t005"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.t005</object-id><label>Table 5</label><caption><title>Balanced accuracies obtained with raw data for sleep/wake classification.</title></caption><alternatives><graphic xlink:href="pone.0323689.t005" id="pone.0323689.t005g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" colspan="2" rowspan="1"/><th align="left" colspan="2" rowspan="1">CNN</th><th align="left" colspan="2" rowspan="1">LSTM</th><th align="left" colspan="2" rowspan="1">GRU</th></tr><tr><th align="center" colspan="2" rowspan="1">Config.</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th></tr></thead><tbody><tr><td align="left" colspan="2" rowspan="1">Without ensemble</td><td align="left" rowspan="1" colspan="1">71.60</td><td align="left" rowspan="1" colspan="1">52.00</td><td align="left" rowspan="1" colspan="1">71.55</td><td align="left" rowspan="1" colspan="1">56.29</td><td align="left" rowspan="1" colspan="1">72.54</td><td align="left" rowspan="1" colspan="1">63.88</td></tr><tr><td align="left" rowspan="1" colspan="1">ACC + HR Ensembles</td><td align="left" rowspan="1" colspan="1">Simple Average</td><td align="center" colspan="2" rowspan="1">71.30</td><td align="center" colspan="2" rowspan="1">72.78</td><td align="center" colspan="2" rowspan="1">
<underline>75.62</underline>
</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Weighted Average</td><td align="center" colspan="2" rowspan="1">
<underline>71.94</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>72.84</underline>
</td><td align="center" colspan="2" rowspan="1">74.96</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Deep Features</td><td align="center" colspan="2" rowspan="1">70.05</td><td align="center" colspan="2" rowspan="1">71.91</td><td align="center" colspan="2" rowspan="1">61.99</td></tr></tbody></table></alternatives><table-wrap-foot><fn><p>Note: The underlined values represent the highest balanced accuracies for the corresponding data configurations.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="pone.0323689.t006"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.t006</object-id><label>Table 6</label><caption><title>Balanced accuracies obtained with raw data for sleep stages classification.</title></caption><alternatives><graphic xlink:href="pone.0323689.t006" id="pone.0323689.t006g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" colspan="2" rowspan="1"/><th align="left" colspan="2" rowspan="1">CNN</th><th align="left" colspan="2" rowspan="1">LSTM</th><th align="left" colspan="2" rowspan="1">GRU</th></tr><tr><th align="center" colspan="2" rowspan="1">Config.</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th></tr></thead><tbody><tr><td align="left" colspan="2" rowspan="1">Without ensemble</td><td align="left" rowspan="1" colspan="1">46.66</td><td align="left" rowspan="1" colspan="1">
<underline>46.70</underline>
</td><td align="left" rowspan="1" colspan="1">47.87</td><td align="left" rowspan="1" colspan="1">
<underline>57.55</underline>
</td><td align="left" rowspan="1" colspan="1">46.84</td><td align="left" rowspan="1" colspan="1">
<underline>56.03</underline>
</td></tr><tr><td align="left" rowspan="1" colspan="1">ACC + HR Ensembles</td><td align="left" rowspan="1" colspan="1">Simple Average</td><td align="center" colspan="2" rowspan="1">45.66</td><td align="center" colspan="2" rowspan="1">48.72</td><td align="center" colspan="2" rowspan="1">
<underline>45.08</underline>
</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Weighted Average</td><td align="center" colspan="2" rowspan="1">
<underline>46.27</underline>
</td><td align="center" colspan="2" rowspan="1">48.42</td><td align="center" colspan="2" rowspan="1">45.04</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Deep Features</td><td align="center" colspan="2" rowspan="1">45.52</td><td align="center" colspan="2" rowspan="1">
<underline>51.75</underline>
</td><td align="center" colspan="2" rowspan="1">36.77</td></tr></tbody></table></alternatives><table-wrap-foot><fn><p>Note: The underlined values represent the highest balanced accuracies for the corresponding data configurations.</p></fn></table-wrap-foot></table-wrap><p>For sleep/wake classification, similar to the visual representations, the accelerometer data present higher balanced accuracies than those obtained with heart rate data. The best results involve ensembles of both types of data for all networks. The highest balanced accuracy exceeds <inline-formula id="pone.0323689.e093"><alternatives><graphic xlink:href="pone.0323689.e093.jpg" id="pone.0323689.e093g" position="anchor"/><mml:math id="M93" display="inline" overflow="scroll"><mml:mrow><mml:mn>75</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> with the GRU network and simple averaging ensemble. The best results obtained with only accelerometer or heart rate data were also achieved with GRU, exceeding <inline-formula id="pone.0323689.e094"><alternatives><graphic xlink:href="pone.0323689.e094.jpg" id="pone.0323689.e094g" position="anchor"/><mml:math id="M94" display="inline" overflow="scroll"><mml:mrow><mml:mn>72</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0323689.e095"><alternatives><graphic xlink:href="pone.0323689.e095.jpg" id="pone.0323689.e095g" position="anchor"/><mml:math id="M95" display="inline" overflow="scroll"><mml:mrow><mml:mn>63</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, respectively.</p><p>For sleep stages classification, it is possible to observe that, similar to the visual representations, the best-balanced accuracies were obtained with heart rate data, exceeding <inline-formula id="pone.0323689.e096"><alternatives><graphic xlink:href="pone.0323689.e096.jpg" id="pone.0323689.e096g" position="anchor"/><mml:math id="M96" display="inline" overflow="scroll"><mml:mrow><mml:mn>57</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> using the LSTM network. The results obtained with ensembles did not show improvements for the 1D-CNN and GRU networks. In contrast, the ensembles performed with the LSTM network improved the results presented with only accelerometer data.</p></sec><sec id="sec030"><title>Features extraction</title><p>One could also wonder how good the classification would be if performing the characterization of the signal instead of using the raw data. Here, the feature extraction from accelerometer and heart rate data, as well as the performed ensemble, were based on the work of Walch <italic toggle="yes">et al</italic>. [<xref rid="pone.0323689.ref025" ref-type="bibr">25</xref>], which uses activity counts as a feature extracted from accelerometer data and local standard deviations extracted from heart rate data. The ensemble used the accelerometer data feature and the heart rate data feature as inputs to the models. <xref rid="pone.0323689.t007" ref-type="table">Table 7</xref> presents the balanced accuracies obtained for sleep/wake and sleep stages classifications.</p><table-wrap position="float" id="pone.0323689.t007"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.t007</object-id><label>Table 7</label><caption><title>Balanced accuracies obtained with feature extraction for sleep/wake and sleep stages classifications.</title></caption><alternatives><graphic xlink:href="pone.0323689.t007" id="pone.0323689.t007g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="4" rowspan="1">Sleep/wake classification</th><th align="left" colspan="4" rowspan="1">Sleep stages classification</th></tr><tr><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="2" rowspan="1">RF</th><th align="left" colspan="2" rowspan="1">LR</th><th align="left" colspan="2" rowspan="1">RF</th><th align="left" colspan="2" rowspan="1">LR</th></tr><tr><th align="left" rowspan="1" colspan="1">Config.</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th><th align="left" rowspan="1" colspan="1">ACC</th><th align="left" rowspan="1" colspan="1">HR</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Without ensemble</td><td align="left" rowspan="1" colspan="1">76.55</td><td align="left" rowspan="1" colspan="1">64.81</td><td align="left" rowspan="1" colspan="1">73.85</td><td align="left" rowspan="1" colspan="1">63.59</td><td align="left" rowspan="1" colspan="1">51.55</td><td align="left" rowspan="1" colspan="1">48.73</td><td align="left" rowspan="1" colspan="1">44.50</td><td align="left" rowspan="1" colspan="1">40.50</td></tr><tr><td align="left" rowspan="1" colspan="1">ACC + HR Ensembles</td><td align="center" colspan="2" rowspan="1">
<underline>76.65</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>73.98</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>58.51</underline>
</td><td align="center" colspan="2" rowspan="1">
<underline>46.51</underline>
</td></tr></tbody></table></alternatives><table-wrap-foot><fn><p>Note: The underlined values represent the highest balanced accuracies for the corresponding data/network configurations.</p></fn></table-wrap-foot></table-wrap><p>Similar to the visual representation, the accelerometer data, compared to the heart rate data, present the best-balanced accuracies for sleep/wake classification, exceeding <inline-formula id="pone.0323689.e097"><alternatives><graphic xlink:href="pone.0323689.e097.jpg" id="pone.0323689.e097g" position="anchor"/><mml:math id="M97" display="inline" overflow="scroll"><mml:mrow><mml:mn>76</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> with RF. For sleep stages classification, the best result exceeds <inline-formula id="pone.0323689.e098"><alternatives><graphic xlink:href="pone.0323689.e098.jpg" id="pone.0323689.e098g" position="anchor"/><mml:math id="M98" display="inline" overflow="scroll"><mml:mrow><mml:mn>58</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> using RF and the feature ensemble.</p></sec><sec id="sec031"><title>Comparison between representations</title><p><xref rid="pone.0323689.t008" ref-type="table">Table 8</xref> compares balanced accuracies obtained with each data representation using an accelerometer, heart rate, and ensemble for sleep/wake classification. It can be observed that the visual representation achieves the best results in all cases, with an advantage of up to 5.8 percentage points with accelerometer data, 8.9 with heart rate data, and 4.8 with ensemble.</p><table-wrap position="float" id="pone.0323689.t008"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.t008</object-id><label>Table 8</label><caption><title>Comparison of the best-balanced accuracies obtained with different data representations for sleep/wake classification.</title></caption><alternatives><graphic xlink:href="pone.0323689.t008" id="pone.0323689.t008g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="3" rowspan="1">ACC</th><th align="left" colspan="3" rowspan="1">HR</th><th align="left" colspan="3" rowspan="1">ACC + HR Ensemble</th></tr><tr><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1">Images</th><th align="left" rowspan="1" colspan="1">Raw Data</th><th align="left" rowspan="1" colspan="1">Features</th><th align="left" rowspan="1" colspan="1">Images</th><th align="left" rowspan="1" colspan="1">Raw Data</th><th align="left" rowspan="1" colspan="1">Features</th><th align="left" rowspan="1" colspan="1">Images</th><th align="left" rowspan="1" colspan="1">Raw Data</th><th align="left" rowspan="1" colspan="1">Features</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Bal. ac.</td><td align="left" rowspan="1" colspan="1">
<underline>82.36</underline>
</td><td align="left" rowspan="1" colspan="1">72.54</td><td align="left" rowspan="1" colspan="1">76.55</td><td align="left" rowspan="1" colspan="1">
<underline>73.71</underline>
</td><td align="left" rowspan="1" colspan="1">63.88</td><td align="left" rowspan="1" colspan="1">64.81</td><td align="left" rowspan="1" colspan="1">
<underline>81.44</underline>
</td><td align="left" rowspan="1" colspan="1">75.62</td><td align="left" rowspan="1" colspan="1">76.65</td></tr><tr><td align="left" rowspan="1" colspan="1">Config.</td><td align="left" rowspan="1" colspan="1">GAF Patches</td><td align="left" rowspan="1" colspan="1">GRU</td><td align="left" rowspan="1" colspan="1">RF</td><td align="left" rowspan="1" colspan="1">GAF Patches</td><td align="left" rowspan="1" colspan="1">GRU</td><td align="left" rowspan="1" colspan="1">RF</td><td align="left" rowspan="1" colspan="1">GAF Patches</td><td align="left" rowspan="1" colspan="1">GRU</td><td align="left" rowspan="1" colspan="1">RF</td></tr></tbody></table></alternatives><table-wrap-foot><fn><p>Note: The underlined values represent the highest balanced accuracies for the corresponding data type.</p></fn></table-wrap-foot></table-wrap><p><xref rid="pone.0323689.t009" ref-type="table">Table 9</xref> presents the best-balanced accuracies obtained with each type of representation using accelerometer, heart rate, and ensemble data and provides a comparison for sleep stages classification. As observed in the sleep/wake classification, the visual representation presents the best results in all cases. The difference in balanced accuracy using images reaches at least 9.1 percentage points with accelerometer data, 4.6 with heart rate data, and 3.0 with ensemble.</p><table-wrap position="float" id="pone.0323689.t009"><object-id pub-id-type="doi">10.1371/journal.pone.0323689.t009</object-id><label>Table 9</label><caption><title>Comparison of the best-balanced accuracies obtained with different data representations for sleep stages classification.</title></caption><alternatives><graphic xlink:href="pone.0323689.t009" id="pone.0323689.t009g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="3" rowspan="1">ACC</th><th align="left" colspan="3" rowspan="1">HR</th><th align="left" colspan="3" rowspan="1">ACC + HR Ensemble</th></tr><tr><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1">Images</th><th align="left" rowspan="1" colspan="1">Raw Data</th><th align="left" rowspan="1" colspan="1">Features</th><th align="left" rowspan="1" colspan="1">Images</th><th align="left" rowspan="1" colspan="1">Raw Data</th><th align="left" rowspan="1" colspan="1">Features</th><th align="left" rowspan="1" colspan="1">Images</th><th align="left" rowspan="1" colspan="1">Raw Data</th><th align="left" rowspan="1" colspan="1">Features</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Bal. ac.</td><td align="left" rowspan="1" colspan="1">
<underline>60.66</underline>
</td><td align="left" rowspan="1" colspan="1">47.87</td><td align="left" rowspan="1" colspan="1">51.55</td><td align="left" rowspan="1" colspan="1">
<underline>62.18</underline>
</td><td align="left" rowspan="1" colspan="1">57.55</td><td align="left" rowspan="1" colspan="1">48.73</td><td align="left" rowspan="1" colspan="1">
<underline>61.48</underline>
</td><td align="left" rowspan="1" colspan="1">51.75</td><td align="left" rowspan="1" colspan="1">58.51</td></tr><tr><td align="left" rowspan="1" colspan="1">Config.</td><td align="left" rowspan="1" colspan="1">GAF Patches</td><td align="left" rowspan="1" colspan="1">LSTM</td><td align="left" rowspan="1" colspan="1">RF</td><td align="left" rowspan="1" colspan="1">GAF Patches</td><td align="left" rowspan="1" colspan="1">LSTM</td><td align="left" rowspan="1" colspan="1">RF</td><td align="left" rowspan="1" colspan="1">RP Patches</td><td align="left" rowspan="1" colspan="1">LSTM</td><td align="left" rowspan="1" colspan="1">RF</td></tr></tbody></table></alternatives><table-wrap-foot><fn><p>Note: The underlined values represent the highest balanced accuracies for the corresponding data type.</p></fn></table-wrap-foot></table-wrap><p>Although the use of images for data representation reduces the advantage when employing ensemble techniques compared to other forms of representation, it stands out by presenting significant gains in the isolated accelerometer data (leading to the best result for sleep/wake classification) and heart rate data (leading to the best result for sleep stages classification).</p></sec></sec></sec><sec sec-type="conclusions" id="sec032"><title>Conclusions</title><p>Sleep stage classification is critical for evaluating sleep quality and identifying disorders. While PSG remains the gold standard, its high cost and requirement for controlled environments limit its accessibility. Smartwatches provide a practical alternative, but traditional methods, such as manual feature extraction for classical models and direct neural network application to raw data, face challenges related to noise, high dimensionality, and difficulty in capturing complex temporal patterns. This study investigated the use of visual representations of time series to enhance sleep stage classification using deep learning.</p><p>The results show that converting time series data into images allows the application of 2D-CNNs, which effectively capture spatial and temporal patterns. Among the tested visual representations, GAF achieved the highest performance, surpassing 82% balanced accuracy for sleep/wake classification and 62% for sleep stages classification when combined with patching and ensemble techniques. Compared to traditional approaches, visual representations outperformed raw data-based deep learning models and feature extraction techniques, with gains of up to 8.9 percentage points in sleep/wake classification and up to 9.1 percentage points in sleep stages classification.</p><p>Additionally, the study highlights the distinct contributions of accelerometer and heart rate data. Accelerometer data were more effective for sleep/wake classification, while heart rate data played a key role in distinguishing between sleep stages. The use of image patching and ensembles improved classification performance by emphasizing local details (up to 3.7 percentage points for sleep/wake classification and up to 6.0 percentage points for sleep stages classification).</p><p>These findings have significant implications for sleep research and health monitoring. The proposed method enables more accurate sleep classification using affordable and widely available wearable devices. This could support large-scale sleep studies, early detection of sleep disorders, and personalized sleep improvement strategies. By providing a non-invasive alternative to PSG, this approach advances sleep research and may contribute to better health outcomes.</p><p>For future work, patches have shown promise in classifying sleep/wake states and sleep stages, suggesting exploring Transformer-based networks, such as the Vision Transformer (ViT) [<xref rid="pone.0323689.ref044" ref-type="bibr">44</xref>, <xref rid="pone.0323689.ref045" ref-type="bibr">45</xref>]. Normalization and filtering methods for accelerometer and heart rate data and post-processing techniques are crucial to improving data quality and classification accuracy. Temporal fusion for a night&#x02019;s sleep analysis is a future step for eliminating false positives, as it increases accuracy by integrating temporal patterns throughout the night, improving the detection of sleep stage transitions, and reducing false positives for more reliable sleep analysis. Additionally, explainability techniques are essential to make Deep Learning models more understandable and validatable by specialists [<xref rid="pone.0323689.ref046" ref-type="bibr">46</xref>].</p><p>Deploying Deep Learning technologies in wearable devices is challenging due to their computational and energy limitations [<xref rid="pone.0323689.ref047" ref-type="bibr">47</xref>]. Running these models and the processing of images demands substantial resources. Future research should prioritize optimizing visual representations to reduce computational costs without compromising performance, thereby enabling more efficient use of these technologies in wearables.</p><p>This study demonstrates that visual representations of time series data provide an effective alternative for sleep stage classification. These findings pave the way for advancements in wearable sleep monitoring and sleep disorder diagnosis.</p></sec><sec id="sec033" sec-type="supplementary-material"><title>Supporting information</title><supplementary-material id="pone.0323689.s001" position="float" content-type="local-data"><label>S1 Table</label><caption><title>Balanced accuracies obtained with each representation for sleep/wake classification.</title><p>Accelerometer data consistently outperformed heart rate data in all scenarios, with the GAF achieving the highest balanced accuracy (82.36% <inline-formula id="pone.0323689.e099"><alternatives><graphic xlink:href="pone.0323689.e099.jpg" id="pone.0323689.e099g" position="anchor"/><mml:math id="M99" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 3.24%) when using patch ensembles. Patch-based ensembles significantly improved balanced accuracy compared to original images.</p><p>(PDF)</p></caption><media xlink:href="pone.0323689.s001.pdf"/></supplementary-material><supplementary-material id="pone.0323689.s002" position="float" content-type="local-data"><label>S2 Table</label><caption><title>Balanced accuracies obtained with each representation for sleep stages classification.</title><p>Heart rate data often outperformed accelerometer data in balanced accuracies (except for the Spectrogram), with the GAF achieving the highest balanced accuracy (62.18% <inline-formula id="pone.0323689.e100"><alternatives><graphic xlink:href="pone.0323689.e100.jpg" id="pone.0323689.e100g" position="anchor"/><mml:math id="M100" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 0.95%) when using patch ensemble. Patch-based ensembles significantly improved balanced accuracy compared to original images.</p><p>(PDF)</p></caption><media xlink:href="pone.0323689.s002.pdf"/></supplementary-material></sec></body><back><ref-list><title>References</title><ref id="pone.0323689.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Berry</surname><given-names>RB</given-names></name>, <name><surname>Brooks</surname><given-names>R</given-names></name>, <name><surname>Gamaldo</surname><given-names>C</given-names></name>, <name><surname>Harding</surname><given-names>SM</given-names></name>, <name><surname>Lloyd</surname><given-names>RM</given-names></name>, <name><surname>Quan</surname><given-names>SF</given-names></name>, <etal>et al</etal>. <article-title>AASM scoring manual updates for 2017 (version 2.4).</article-title>
<source>J Clin Sleep Med</source>. <year>2017</year>;<volume>13</volume>(<issue>5</issue>):<fpage>665</fpage>&#x02013;<lpage>666</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.5664/jcsm.6576</pub-id>
<pub-id pub-id-type="pmid">28416048</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Di Marco</surname><given-names>T</given-names></name>, <name><surname>Scammell</surname><given-names>TE</given-names></name>, <name><surname>Sadeghi</surname><given-names>K</given-names></name>, <name><surname>Datta</surname><given-names>AN</given-names></name>, <name><surname>Little</surname><given-names>D</given-names></name>, <name><surname>Tjiptarto</surname><given-names>N</given-names></name>, <etal>et al</etal>. <article-title>Hyperarousal features in the sleep architecture of individuals with and without insomnia</article-title>. <source>J Sleep Res</source>. <year>2025</year>;<volume>34</volume>(<issue>1</issue>):e14256. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/jsr.14256</pub-id>
<pub-id pub-id-type="pmid">38853521</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Almutairi</surname><given-names>H</given-names></name>, <name><surname>Hassan</surname><given-names>GM</given-names></name>, <name><surname>Datta</surname><given-names>A</given-names></name>. <article-title>Machine-learning-based-approaches for sleep stage classification utilising a combination of physiological signals: a systematic review</article-title>. <source>Appl Sci</source>. <year>2023</year>;<volume>13</volume>(<issue>24</issue>):<fpage>13280</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/app132413280</pub-id></mixed-citation></ref><ref id="pone.0323689.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Newell</surname><given-names>J</given-names></name>, <name><surname>Mairesse</surname><given-names>O</given-names></name>, <name><surname>Verbanck</surname><given-names>P</given-names></name>, <name><surname>Neu</surname><given-names>D</given-names></name>. <article-title>Is a one-night stay in the lab really enough to conclude? First-night effect and night-to-night variability in polysomnographic recordings among different clinical population samples</article-title>. <source>Psychiatry Res</source>. <year>2012</year>;<volume>200</volume>(2&#x02013;3):<fpage>795</fpage>&#x02013;<lpage>801</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.psychres.2012.07.045</pub-id>
<pub-id pub-id-type="pmid">22901399</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Shama</surname><given-names>H</given-names></name>, <name><surname>Gabinet</surname><given-names>N</given-names></name>, <name><surname>Tzischinsky</surname><given-names>O</given-names></name>, <name><surname>Portnov</surname><given-names>B</given-names></name>. <article-title>Monitoring sleep in real-world conditions using low-cost technology tools</article-title>. <source>Biol Rhythm Res</source>. <year>2022</year>;<volume>54</volume>(<issue>2</issue>):<fpage>232</fpage>&#x02013;<lpage>48</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/09291016.2022.2131990</pub-id></mixed-citation></ref><ref id="pone.0323689.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>W</given-names></name>, <name><surname>Li</surname><given-names>J</given-names></name>, <name><surname>Fang</surname><given-names>Y</given-names></name>, <name><surname>Zheng</surname><given-names>Y</given-names></name>, <name><surname>You</surname><given-names>F</given-names></name>. <article-title>A multimodal attention-fusion convolutional neural network for automatic detection of sleep disorders</article-title>. <source>Appl Intell</source>. <year>2024</year>;<volume>54</volume>(11&#x02013;12):<fpage>7086</fpage>&#x02013;<lpage>98</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10489-024-05499-7</pub-id></mixed-citation></ref><ref id="pone.0323689.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Loh</surname><given-names>HW</given-names></name>, <name><surname>Ooi</surname><given-names>CP</given-names></name>, <name><surname>Dhok</surname><given-names>SG</given-names></name>, <name><surname>Sharma</surname><given-names>M</given-names></name>, <name><surname>Bhurane</surname><given-names>AA</given-names></name>, <name><surname>Acharya</surname><given-names>UR</given-names></name>. <article-title>Automated detection of cyclic alternating pattern and classification of sleep stages using deep neural network</article-title>. <source>Appl Intell</source>. <year>2021</year>;<volume>52</volume>(<issue>3</issue>):<fpage>2903</fpage>&#x02013;<lpage>17</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10489-021-02597-8</pub-id></mixed-citation></ref><ref id="pone.0323689.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Decat</surname><given-names>N</given-names></name>, <name><surname>Walter</surname><given-names>J</given-names></name>, <name><surname>Koh</surname><given-names>ZH</given-names></name>, <name><surname>Sribanditmongkol</surname><given-names>P</given-names></name>, <name><surname>Fulcher</surname><given-names>BD</given-names></name>, <name><surname>Windt</surname><given-names>JM</given-names></name>, <etal>et al</etal>. <article-title>Beyond traditional sleep scoring: Massive feature extraction and data-driven clustering of sleep time series</article-title>. <source>Sleep Med</source>. <year>2022</year>;<volume>98</volume>:<fpage>39</fpage>&#x02013;<lpage>52</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.sleep.2022.06.013</pub-id>
<pub-id pub-id-type="pmid">35779380</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Hochreiter</surname><given-names>S</given-names></name>, <name><surname>Schmidhuber</surname><given-names>J</given-names></name>. <article-title>Long short-term memory</article-title>. <source>Neural Comput</source>. <year>1997</year>;<volume>9</volume>(<issue>8</issue>):<fpage>1735</fpage>&#x02013;<lpage>80</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
<pub-id pub-id-type="pmid">9377276</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref010"><label>10</label><mixed-citation publication-type="confproc"><name><surname>Cho</surname><given-names>K</given-names></name>, <name><surname>van Merri&#x000eb;nboer</surname><given-names>B</given-names></name>, <name><surname>Gulcehre</surname><given-names>C</given-names></name>, <name><surname>Bahdanau</surname><given-names>D</given-names></name>, <name><surname>Bougares</surname><given-names>F</given-names></name>, <name><surname>Schwenk</surname><given-names>H</given-names></name>. <article-title>Learning phrase representations using RNN encoder&#x02013;decoder for statistical machine translation.</article-title> In: <conf-name>Proceedings of the Conference on Empirical Methods in Natural Language Processing</conf-name>. <publisher-name>Association for Computational Linguistics</publisher-name>; <year>2014</year>, pp. <fpage>1724</fpage>&#x02013;<lpage>34</lpage>.</mixed-citation></ref><ref id="pone.0323689.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Motin</surname><given-names>MA</given-names></name>, <name><surname>Karmakar</surname><given-names>C</given-names></name>, <name><surname>Palaniswami</surname><given-names>M</given-names></name>, <name><surname>Penzel</surname><given-names>T</given-names></name>, <name><surname>Kumar</surname><given-names>D</given-names></name>. <article-title>Multi-stage sleep classification using photoplethysmographic sensor</article-title>. <source>R Soc Open Sci</source>. <year>2023</year>;<volume>10</volume>(<issue>4</issue>):<fpage>221517</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1098/rsos.221517</pub-id>
<pub-id pub-id-type="pmid">37063995</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Moris</surname><given-names>E</given-names></name>, <name><surname>Larrabide</surname><given-names>I</given-names></name>. <article-title>Evaluating sleep-stage classification: how age and early-late sleep affects classification performance</article-title>. <source>Med Biol Eng Comput</source>. <year>2024</year>;<volume>62</volume>(<issue>2</issue>):<fpage>343</fpage>&#x02013;<lpage>55</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11517-023-02943-7</pub-id>
<pub-id pub-id-type="pmid">37932584</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Silva</surname><given-names>FB</given-names></name>, <name><surname>Uribe</surname><given-names>LFS</given-names></name>, <name><surname>Cepeda</surname><given-names>FX</given-names></name>, <name><surname>Alquati</surname><given-names>VFS</given-names></name>, <name><surname>Guimar&#x000e3;es</surname><given-names>JPS</given-names></name>, <name><surname>Silva</surname><given-names>YGA</given-names></name>, <etal>et al</etal>. <article-title>Sleep staging algorithm based on smartwatch sensors for healthy and sleep apnea populations</article-title>. <source>Sleep Med</source>. <year>2024</year>;<volume>119</volume>:<fpage>535</fpage>&#x02013;<lpage>48</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.sleep.2024.05.033</pub-id>
<pub-id pub-id-type="pmid">38810479</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Song</surname><given-names>T-A</given-names></name>, <name><surname>Chowdhury</surname><given-names>SR</given-names></name>, <name><surname>Malekzadeh</surname><given-names>M</given-names></name>, <name><surname>Harrison</surname><given-names>S</given-names></name>, <name><surname>Hoge</surname><given-names>TB</given-names></name>, <name><surname>Redline</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>AI-driven sleep staging from actigraphy and heart rate</article-title>. <source>PLoS One</source>. <year>2023</year>;<volume>18</volume>(<issue>5</issue>):e0285703. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0285703</pub-id>
<pub-id pub-id-type="pmid">37195925</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Yan</surname><given-names>R</given-names></name>, <name><surname>Li</surname><given-names>F</given-names></name>, <name><surname>Zhou</surname><given-names>DD</given-names></name>, <name><surname>Ristaniemi</surname><given-names>T</given-names></name>, <name><surname>Cong</surname><given-names>F</given-names></name>. <article-title>Automatic sleep scoring: a deep learning architecture for multi-modality time series</article-title>. <source>J Neurosci Methods</source>. <year>2021</year>;<volume>348</volume>:<fpage>108971</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jneumeth.2020.108971</pub-id>
<pub-id pub-id-type="pmid">33160019</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Xu</surname><given-names>X</given-names></name>, <name><surname>Cong</surname><given-names>F</given-names></name>, <name><surname>Chen</surname><given-names>Y</given-names></name>, <name><surname>Chen</surname><given-names>J</given-names></name>. <article-title>Sleep stage classification with multi-modal fusion and denoising diffusion model</article-title>. <source>IEEE J Biomed Health Inform</source>. <year>2024</year>:10.1109/JBHI.2024.3422472. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/JBHI.2024.3422472</pub-id>
<pub-id pub-id-type="pmid">38959148</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref017"><label>17</label><mixed-citation publication-type="book"><name><surname>Vakalopoulou</surname><given-names>M</given-names></name>, <name><surname>Christodoulidis</surname><given-names>S</given-names></name>, <name><surname>Burgos</surname><given-names>N</given-names></name>, <name><surname>Colliot</surname><given-names>O</given-names></name>, <name><surname>Lepetit</surname><given-names>V.</given-names></name>
<article-title>Deep learning: basics and convolutional neural networks (CNNs).</article-title> In: <name><surname>Colliot</surname><given-names>O</given-names></name>, editor. <source>Machine learning for brain disorders</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Humana</publisher-name>; <year>2023</year>, pp. <fpage>77</fpage>&#x02013;<lpage>115</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/978-1-0716-3195-9_3</pub-id></mixed-citation></ref><ref id="pone.0323689.ref018"><label>18</label><mixed-citation publication-type="book"><name><surname>Eckmann</surname><given-names>J-P</given-names></name>, <name><surname>Kamphorst</surname><given-names>So</given-names></name>, <name><surname>Ruelle</surname><given-names>D</given-names></name>. <source>Recurrence plots of dynamical systems. World Scientific Series on Nonlinear Science Series A</source>. <publisher-name>World Scientific</publisher-name>. <year>1995</year>, pp. <fpage>441</fpage>&#x02013;<lpage>5</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1142/9789812833709_0030</pub-id></mixed-citation></ref><ref id="pone.0323689.ref019"><label>19</label><mixed-citation publication-type="confproc"><name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Oates</surname><given-names>T</given-names></name>. <article-title>Encoding time series as images for visual inspection and classification using tiled convolutional neural networks.</article-title> In: <conf-name>Proceedings of the AAAI</conf-name>. <publisher-name>Association for the Advancement of Artificial Intelligence</publisher-name>; <year>2015</year>, pp. <fpage>40</fpage>&#x02013;<lpage>6</lpage>.</mixed-citation></ref><ref id="pone.0323689.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Fan</surname><given-names>X</given-names></name>, <name><surname>Deng</surname><given-names>S</given-names></name>, <name><surname>Wu</surname><given-names>Z</given-names></name>, <name><surname>Fan</surname><given-names>J</given-names></name>, <name><surname>Zhou</surname><given-names>C</given-names></name>. <article-title>Spatial domain image fusion with particle swarm optimization and lightweight AlexNet for robotic fish sensor fault diagnosis</article-title>. <source>Biomimetics (Basel)</source>. <year>2023</year>;<volume>8</volume>(<issue>6</issue>):<fpage>489</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/biomimetics8060489</pub-id>
<pub-id pub-id-type="pmid">37887620</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>X</given-names></name>, <name><surname>Sun</surname><given-names>H</given-names></name>, <name><surname>Lin</surname><given-names>B</given-names></name>, <name><surname>Zhao</surname><given-names>H</given-names></name>, <name><surname>Niu</surname><given-names>Y</given-names></name>, <name><surname>Zhong</surname><given-names>X</given-names></name>, <etal>et al</etal>. <article-title>Markov transition fields and deep learning-based event-classification and vibration-frequency measurement for &#x003c6;-OTDR</article-title>. <source>IEEE Sensors J</source>. <year>2022</year>;<volume>22</volume>(<issue>4</issue>):<fpage>3348</fpage>&#x02013;<lpage>57</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/jsen.2021.3137006</pub-id></mixed-citation></ref><ref id="pone.0323689.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>Y</given-names></name>, <name><surname>Dong</surname><given-names>F</given-names></name>, <name><surname>Sun</surname><given-names>T</given-names></name>, <name><surname>Ju</surname><given-names>Z</given-names></name>, <name><surname>Yang</surname><given-names>L</given-names></name>, <name><surname>Shan</surname><given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Image expression of time series data of wearable IMU sensor and fusion classification of gymnastics action</article-title>. <source>Expert Syst Appl</source>. <year>2024</year>;<volume>238</volume>:<fpage>121978</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.eswa.2023.121978</pub-id></mixed-citation></ref><ref id="pone.0323689.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Gilmore</surname><given-names>J</given-names></name>, <name><surname>Nasseri</surname><given-names>M</given-names></name>. <article-title>Human activity recognition algorithm with physiological and inertial signals fusion: photoplethysmography, electrodermal activity, and accelerometry</article-title>. <source>Sensors (Basel)</source>. <year>2024</year>;<volume>24</volume>(<issue>10</issue>):<fpage>3005</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/s24103005</pub-id>
<pub-id pub-id-type="pmid">38793858</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Ortega Anderez</surname><given-names>D</given-names></name>, <name><surname>Lotfi</surname><given-names>A</given-names></name>, <name><surname>Pourabdollah</surname><given-names>A</given-names></name>. <article-title>A deep learning based wearable system for food and drink intake recognition</article-title>. <source>J Ambient Intell Human Comput</source>. <year>2020</year>;<volume>12</volume>(<issue>10</issue>):<fpage>9435</fpage>&#x02013;<lpage>47</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s12652-020-02684-7</pub-id></mixed-citation></ref><ref id="pone.0323689.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Walch</surname><given-names>O</given-names></name>, <name><surname>Huang</surname><given-names>Y</given-names></name>, <name><surname>Forger</surname><given-names>D</given-names></name>, <name><surname>Goldstein</surname><given-names>C</given-names></name>. <article-title>Sleep stage prediction with raw acceleration and photoplethysmography heart rate data derived from a consumer wearable device</article-title>. <source>Sleep</source>. <year>2019</year>;<volume>42</volume>(<issue>12</issue>):zsz180. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/sleep/zsz180</pub-id>
<pub-id pub-id-type="pmid">31579900</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Lu</surname><given-names>J</given-names></name>, <name><surname>Tong</surname><given-names>K-Y</given-names></name>. <article-title>Robust single accelerometer-based activity recognition using modified recurrence plot</article-title>. <source>IEEE Sensors J</source>. <year>2019</year>;<volume>19</volume>(<issue>15</issue>):<fpage>6317</fpage>&#x02013;<lpage>24</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/jsen.2019.2911204</pub-id></mixed-citation></ref><ref id="pone.0323689.ref027"><label>27</label><mixed-citation publication-type="confproc"><name><surname>Ederli</surname><given-names>RP</given-names></name>, <name><surname>Vega-Oliveros</surname><given-names>D</given-names></name>, <name><surname>Soriano-Vargas</surname><given-names>A</given-names></name>, <name><surname>Rocha</surname><given-names>A</given-names></name>, <name><surname>Dias</surname><given-names>Z</given-names></name>. <article-title>Sleep-wake classification using recurrence plots from smartwatch accelerometer data.</article-title> In: <conf-name>IEEE Latin American Conference on Computational Intelligence (LA-CCI)</conf-name>. <publisher-name>IEEE</publisher-name>; <year>2023</year>, pp. <fpage>1</fpage>&#x02013;<lpage>6</lpage>.</mixed-citation></ref><ref id="pone.0323689.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Qin</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Meng</surname><given-names>S</given-names></name>, <name><surname>Qin</surname><given-names>Z</given-names></name>, <name><surname>Choo</surname><given-names>K-KR</given-names></name>. <article-title>Imaging and fusing time series for wearable sensor-based human activity recognition</article-title>. <source>Inform Fusion</source>. <year>2020</year>;<volume>53</volume>:<fpage>80</fpage>&#x02013;<lpage>7</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.inffus.2019.06.014</pub-id></mixed-citation></ref><ref id="pone.0323689.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Sarkar</surname><given-names>A</given-names></name>, <name><surname>Hossain</surname><given-names>SKS</given-names></name>, <name><surname>Sarkar</surname><given-names>R</given-names></name>. <article-title>Human activity recognition from sensor data using spatial attention-aided CNN with genetic algorithm</article-title>. <source>Neural Comput Appl</source>. <year>2023</year>;<volume>35</volume>(<issue>7</issue>):<fpage>5165</fpage>&#x02013;<lpage>91</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s00521-022-07911-0</pub-id>
<pub-id pub-id-type="pmid">36311167</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Eckmann</surname><given-names>J-P</given-names></name>, <name><surname>Kamphorst</surname><given-names>SO</given-names></name>, <name><surname>Ruelle</surname><given-names>D</given-names></name>. <article-title>Recurrence plots of dynamical systems</article-title>. <source>Europhys Lett</source>. <year>1987</year>;<volume>4</volume>(<issue>9</issue>):<fpage>973</fpage>&#x02013;<lpage>7</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1209/0295-5075/4/9/004</pub-id></mixed-citation></ref><ref id="pone.0323689.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Marwan</surname><given-names>N</given-names></name>, <name><surname>Carmenromano</surname><given-names>M</given-names></name>, <name><surname>Thiel</surname><given-names>M</given-names></name>, <name><surname>Kurths</surname><given-names>J</given-names></name>. <article-title>Recurrence plots for the analysis of complex systems</article-title>. <source>Phys Rep</source>. <year>2007</year>;<volume>438</volume>(5&#x02013;6):<fpage>237</fpage>&#x02013;<lpage>329</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.physrep.2006.11.001</pub-id></mixed-citation></ref><ref id="pone.0323689.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Iwanski</surname><given-names>JS</given-names></name>, <name><surname>Bradley</surname><given-names>E</given-names></name>. <article-title>Recurrence plots of experimental data: to embed or not to embed?</article-title>
<source>Chaos</source>. <year>1998</year>;<volume>8</volume>(<issue>4</issue>):<fpage>861</fpage>&#x02013;<lpage>71</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1063/1.166372</pub-id>
<pub-id pub-id-type="pmid">12779793</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Le Goallec</surname><given-names>A</given-names></name>, <name><surname>Collin</surname><given-names>S</given-names></name>, <name><surname>Jabri</surname><given-names>M</given-names></name>, <name><surname>Diai</surname><given-names>S</given-names></name>, <name><surname>Vincent</surname><given-names>T</given-names></name>, <name><surname>Patel</surname><given-names>CJ</given-names></name>. <article-title>Machine learning approaches to predict age from accelerometer records of physical activity at biobank scale</article-title>. <source>PLOS Digit Health</source>. <year>2023</year>;<volume>2</volume>(<issue>1</issue>):e0000176. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pdig.0000176</pub-id>
<pub-id pub-id-type="pmid">36812610</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Zeng</surname><given-names>Z</given-names></name>, <name><surname>Amin</surname><given-names>MG</given-names></name>, <name><surname>Shan</surname><given-names>T</given-names></name>. <article-title>Arm motion classification using time-series analysis of the spectrogram frequency envelopes</article-title>. <source>Remote Sensing</source>. <year>2020</year>;<volume>12</volume>(<issue>3</issue>):<fpage>454</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/rs12030454</pub-id></mixed-citation></ref><ref id="pone.0323689.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Roy</surname><given-names>K</given-names></name>, <name><surname>Banik</surname><given-names>D</given-names></name>, <name><surname>Bhattacharjee</surname><given-names>D</given-names></name>, <name><surname>Nasipuri</surname><given-names>M</given-names></name>. <article-title>Patch-based system for classification of breast histology images using deep learning</article-title>. <source>Comput Med Imaging Graph</source>. <year>2019</year>;<volume>71</volume>:<fpage>90</fpage>&#x02013;<lpage>103</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.compmedimag.2018.11.003</pub-id>
<pub-id pub-id-type="pmid">30594745</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Amin-Naji</surname><given-names>M</given-names></name>, <name><surname>Aghagolzadeh</surname><given-names>A</given-names></name>, <name><surname>Ezoji</surname><given-names>M</given-names></name>. <article-title>Ensemble of CNN for multi-focus image fusion</article-title>. <source>Inform Fusion</source>. <year>2019</year>;<volume>51</volume>:<fpage>201</fpage>&#x02013;<lpage>14</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.inffus.2019.02.003</pub-id></mixed-citation></ref><ref id="pone.0323689.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Ganaie</surname><given-names>MA</given-names></name>, <name><surname>Hu</surname><given-names>M</given-names></name>, <name><surname>Malik</surname><given-names>AK</given-names></name>, <name><surname>Tanveer</surname><given-names>M</given-names></name>, <name><surname>Suganthan</surname><given-names>PN</given-names></name>. <article-title>Ensemble deep learning: a review</article-title>. <source>Eng Appl Artif Intell</source>. <year>2022</year>;<volume>115</volume>:<fpage>105151</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.engappai.2022.105151</pub-id></mixed-citation></ref><ref id="pone.0323689.ref038"><label>38</label><mixed-citation publication-type="confproc"><name><surname>Tan</surname><given-names>M</given-names></name>, <name><surname>Le</surname><given-names>Q</given-names></name>. <article-title>EfficientNet: Rethinking model scaling for convolutional neural networks.</article-title> In: <conf-name>Proceedings of the 36th International Conference on Machine Learning, ICML 2019</conf-name>, <conf-loc>Long Beach</conf-loc>, <conf-date>9&#x02013;15 June 2019</conf-date>, pp. <fpage>6105</fpage>&#x02013;<lpage>14</lpage>.<ext-link xlink:href="http://proceedings.mlr.press/v97/tan19a.html" ext-link-type="uri">http://proceedings.mlr.press/v97/tan19a.html</ext-link></mixed-citation></ref><ref id="pone.0323689.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>Russakovsky</surname><given-names>O</given-names></name>, <name><surname>Deng</surname><given-names>J</given-names></name>, <name><surname>Su</surname><given-names>H</given-names></name>, <name><surname>Krause</surname><given-names>J</given-names></name>, <name><surname>Satheesh</surname><given-names>S</given-names></name>, <name><surname>Ma</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>ImageNet large scale visual recognition challenge</article-title>. <source>Int J Comput Vis</source>. <year>2015</year>;<volume>115</volume>(<issue>3</issue>):<fpage>211</fpage>&#x02013;<lpage>52</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></mixed-citation></ref><ref id="pone.0323689.ref040"><label>40</label><mixed-citation publication-type="confproc"><name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Ren</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <article-title>Deep residual learning for image recognition.</article-title> In: <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>. <year>2016</year>, pp. <fpage>770</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="pone.0323689.ref041"><label>41</label><mixed-citation publication-type="confproc"><name><surname>Brodersen</surname><given-names>KH</given-names></name>, <name><surname>Ong</surname><given-names>CS</given-names></name>, <name><surname>Stephan</surname><given-names>KE</given-names></name>, <name><surname>Buhmann</surname><given-names>JM</given-names></name>. <article-title>The balanced accuracy and its posterior distribution.</article-title> In: <conf-name>2010 20th International Conference on Pattern Recognition</conf-name>. <publisher-name>IEEE</publisher-name>; <year>2010</year>, pp. <fpage>3121</fpage>&#x02013;<lpage>4</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/icpr.2010.764</pub-id></mixed-citation></ref><ref id="pone.0323689.ref042"><label>42</label><mixed-citation publication-type="journal"><name><surname>Cohen</surname><given-names>J</given-names></name>. <article-title>A coefficient of agreement for nominal scales</article-title>. <source>Educ Psychol Measure</source>. <year>1960</year>;<volume>20</volume>(<issue>1</issue>):<fpage>37</fpage>&#x02013;<lpage>46</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1177/001316446002000104</pub-id></mixed-citation></ref><ref id="pone.0323689.ref043"><label>43</label><mixed-citation publication-type="journal"><name><surname>Mekruksavanich</surname><given-names>S</given-names></name>, <name><surname>Jitpattanakul</surname><given-names>A</given-names></name>. <article-title>Biometric user identification based on human activity recognition using wearable sensors: an experiment using deep learning models</article-title>. <source>Electronics</source>. <year>2021</year>;<volume>10</volume>(<issue>3</issue>):<fpage>308</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/electronics10030308</pub-id></mixed-citation></ref><ref id="pone.0323689.ref044"><label>44</label><mixed-citation publication-type="book"><name><surname>Dosovitskiy</surname><given-names>A</given-names></name>, <name><surname>Beyer</surname><given-names>L</given-names></name>, <name><surname>Kolesnikov</surname><given-names>A</given-names></name>, <name><surname>Weissenborn</surname><given-names>D</given-names></name>, <name><surname>Zhai</surname><given-names>X</given-names></name>, <name><surname>Unterthiner</surname><given-names>T</given-names></name>, <etal>et al</etal>. <article-title>An image is worth 16 &#x000d7; 16 words: transformers for image recognition at scale.</article-title> In: <source>International Conference on Learning Representations (ICLR)</source>. <year>2021</year>.</mixed-citation></ref><ref id="pone.0323689.ref045"><label>45</label><mixed-citation publication-type="journal"><name><surname>Azad</surname><given-names>R</given-names></name>, <name><surname>Kazerouni</surname><given-names>A</given-names></name>, <name><surname>Heidari</surname><given-names>M</given-names></name>, <name><surname>Aghdam</surname><given-names>EK</given-names></name>, <name><surname>Molaei</surname><given-names>A</given-names></name>, <name><surname>Jia</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Advances in medical image analysis with vision Transformers: a comprehensive review</article-title>. <source>Med Image Anal</source>. <year>2024</year>;<volume>91</volume>:<fpage>103000</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.media.2023.103000</pub-id>
<pub-id pub-id-type="pmid">37883822</pub-id>
</mixed-citation></ref><ref id="pone.0323689.ref046"><label>46</label><mixed-citation publication-type="journal"><name><surname>Bennetot</surname><given-names>A</given-names></name>, <name><surname>Donadello</surname><given-names>I</given-names></name>, <name><surname>El Qadi El Haouari</surname><given-names>A</given-names></name>, <name><surname>Dragoni</surname><given-names>M</given-names></name>, <name><surname>Frossard</surname><given-names>T</given-names></name>, <name><surname>Wagner</surname><given-names>B</given-names></name>, <etal>et al</etal>. <article-title>A practical tutorial on explainable AI techniques</article-title>. <source>ACM Comput Surv</source>. <year>2024</year>;<volume>57</volume>(<issue>2</issue>):<fpage>1</fpage>&#x02013;<lpage>44</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3670685</pub-id></mixed-citation></ref><ref id="pone.0323689.ref047"><label>47</label><mixed-citation publication-type="journal"><name><surname>Shuvo</surname><given-names>MMH</given-names></name>, <name><surname>Islam</surname><given-names>SK</given-names></name>, <name><surname>Cheng</surname><given-names>J</given-names></name>, <name><surname>Morshed</surname><given-names>BI</given-names></name>. <article-title>Efficient acceleration of deep learning inference on resource-constrained edge devices: a review</article-title>. <source>Proc IEEE</source>. <year>2023</year>;<volume>111</volume>(<issue>1</issue>):<fpage>42</fpage>&#x02013;<lpage>91</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/jproc.2022.3226481</pub-id></mixed-citation></ref></ref-list></back><sub-article article-type="author-comment" id="pone.0323689.r001" specific-use="rebutted-decision-letter-unavailable"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323689.r001</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">20 Dec 2024</named-content>
</p></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0323689.r002" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323689.r002</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Xiaohui</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Xiaohui Zhang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Xiaohui Zhang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323689" id="rel-obj002" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">22 Jan 2025</named-content>
</p><p><!--<div>-->PONE-D-24-59060<!--</div>--><!--<div>-->Time-series visual representations for sleep stages classification<!--</div>--><!--<div>-->PLOS ONE</p><p>Dear Dr. Padovani Ederli,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please submit your revised manuscript by Mar 08 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!--</div>--></p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Xiaohui Zhang</p><p>Academic Editor</p><p>PLOS ONE</p><p>Journal requirements:</p><p>When submitting your revision, we need you to address these additional requirements.</p><p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at</p><p><ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and</p><p>
<ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
</p><p>2. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, we expect all author-generated code to be made available without restrictions upon publication of the work. Please review our guidelines at <ext-link xlink:href="https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code" ext-link-type="uri">https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code</ext-link> and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse.</p><p>Additional Editor Comments (if provided):</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!--<font color="black">-->
<bold>Comments to the Author</bold>
</p><p>1. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->2. Has the statistical analysis been performed appropriately and rigorously? <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->3. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->4. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->5. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!--</font>--></p><p>Reviewer #1:&#x000a0;The study explores the application of visual representations of time series data from smartwatches to classify sleep stages. The authors transform data collected from accelerometers and heart rate sensors into visual formats like Gramian Angular Fields (GAF), Recurrence Plots (RP), Markov Transition Fields (MTF), and spectrograms. These visual representations are processed using 2D convolutional neural networks (CNNs), with image patching and ensemble methods enhancing classification performance. The proposed method demonstrates superior accuracy compared to traditional raw data approaches. I have a few points that can help improve the manuscript.</p><p>1. EfficientNet is a class of models. The authors need to specify which specific EfficientNet model was used for the experiment.</p><p>2. The authors should clearly define the classification task they are working on. What exactly is the goal/benefit for different classification task, and what is the distribution of each class? Including a workflow or pipeline demonstration figure would greatly enhance clarity. For each task, the data distribution, including the actual number of cases(and their respective percentages), should be explicitly shown.</p><p>3. The authors mention a data imbalance issue. It would be helpful to know if they took any measures to address this problem or include it in a limitation discussion.</p><p>4. For cross-validation, it is unclear whether the authors used a stratified split based on the label of each sample. Clarifying this would be helpful. It is also unclear if the accuracy in the paper is the average accuracy of the 5-fold cross validation. If so, standard deviation should be included.</p><p>5. For better readability, I suggest placing the figure descriptions together with the figures.</p><p>6. The paper contains a large number of comparison results. The authors might consider highlighting the most significant ones and moving the rest to the supplementary material.</p><p>7. Conclusions and Discussion sections need more work. A summary of the study's goal should be included, along with how the findings support this goal. For example, how why is classifying sleeping stage important and how can the method developed benefit research or improve human well-being.</p><p>Reviewer #2:&#x000a0;The paper utilizes Gramian Angular Fields, Recurrence Plots, Markov models, Transition Fields, and spectrograms to process time series data collected from smartwatches, aiming to enhance the classification of sleep stages. The analyses and experiments are rigorous. However, the methods utilized are largely standard, and the paper lacks a degree of innovation.</p><p>**********</p><p><!--<font color="black">-->6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!--</font>--></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p></body></sub-article><sub-article article-type="author-comment" id="pone.0323689.r003"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323689.r003</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323689" id="rel-obj003" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">26 Feb 2025</named-content>
</p><p>Paper title: Time-series visual representations for sleep stages classification</p><p>Journal: PLOS ONE</p><p>February 21th, 2025</p><p>Dear Editor and Reviewers,</p><p>We would like to thank the editor and reviewers for their comments and suggestions. They were valuable and certainly contributed to improving the quality of our manuscript. In this revised version, we addressed all points raised by the reviewers and made adjustments to enhance clarity, methodology description, and data presentation.</p><p>The comments, along with our continued research during the review process, allowed us to refine and strengthen our manuscript. The revised version has been submitted through the online submission system, with the main changes highlighted.</p><p>We appreciate the time and effort dedicated by the reviewers and the editor and hope that this version meets the journal&#x02019;s criteria for publication. For any further questions, please do not hesitate to contact us.</p><p>Sincerely,</p><p>The authors</p><p>***</p><p>General Comment</p><p>After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Response: We thank the editor and the reviewers for diligently dealing with our submission and for the very constructive feedback.</p><p>In this new version, we addressed the points of both reviewers, and we modified the manuscript according to their insightful and accurate comments. We are grateful to both reviewers and the editor for their positive comments and for the time dedicated to the reviewing process. With our revised manuscript, we send a point-by-point response in which we did our best to address all of them.</p><p>***</p><p>Reviewer #1</p><p>The study explores the application of visual representations of time series data from smartwatches to classify sleep stages. The authors transform data collected from accelerometers and heart rate sensors into visual formats like Gramian Angular Fields (GAF), Recurrence Plots (RP), Markov Transition Fields (MTF), and spectrograms. These visual representations are processed using 2D convolutional neural networks (CNNs), with image patching and ensemble methods enhancing classification performance. The proposed method demonstrates superior accuracy compared to traditional raw data approaches. I have a few points that can help improve the manuscript.</p><p>1. EfficientNet is a class of models. The authors need to specify which specific EfficientNet model was used for the experiment.</p><p>Response: We agree with the reviewer&#x02019;s observation. To clarify, we now explicitly state that we used EfficientNet-B0 for our experiments. This information has been added to the &#x0201c;Materials and methods (Training and validation)&#x0201d; subsection.</p><p>***</p><p>2. The authors should clearly define the classification task they are working on. What exactly is the goal/benefit for different classification tasks, and what is the distribution of each class? Including a workflow or pipeline demonstration figure would greatly enhance clarity. For each task, the data distribution, including the actual number of cases (and their respective percentages), should be explicitly shown.</p><p>Response: Following the reviewer's suggestion, we have revised the manuscript to improve the clarity of our classification tasks. The &#x0201c;Introduction&#x0201d; section now explicitly describes the two classification tasks&#x02014;two-stage (wake/sleep) and three-stage (wake/NREM/REM)&#x02014;along with their practical relevance. The two-stage classification is useful for basic sleep detection, such as in large-scale sleep monitoring or initial sleep disorder screenings, while the three-stage classification enables detailed analysis of sleep architecture, essential for clinical applications like diagnosing sleep disorders or assessing sleep quality.</p><p>In addition, we have included Table 1 in the &#x0201c;Materials and Methods (Dataset)&#x0201d; subsection. This table provides the number of samples and percentages per class, highlighting the dataset imbalance.</p><p>To further enhance clarity, we also added Figure 2, which illustrates the overall workflow of the proposed methodology, including the transformation of time series into visual representations and subsequent classification steps.</p><p>***</p><p>3. The authors mention a data imbalance issue. It would be helpful to know if they took any measures to address this problem or include it in a limitation discussion.</p><p>Response: To mitigate the impact of class imbalance, we applied class weighting, where the loss function assigns higher weights to minority classes, ensuring a more balanced contribution during training. To address this comment, this strategy is now explicitly described in the &#x0201c;Materials and methods (Training and validation)&#x0201d; subsection.</p><p>***</p><p>4. For cross-validation, it is unclear whether the authors used a stratified split based on the label of each sample. Clarifying this would be helpful. It is also unclear if the accuracy in the paper is the average accuracy of the 5-fold cross validation. If so, standard deviation should be included.</p><p>Response: Following the suggestion, we have clarified these points in the revised manuscript. The &#x0201c;Materials and methods (Training and validation)&#x0201d; subsection now explicitly states that we used 5-fold cross-validation, ensuring that data from the same subject were never included in both training and validation within the same fold. Instead of a stratified split, we opted for a random split, allowing the model to capture the natural variability of sleep stage transitions, as now described in the &#x0201c;Materials and Methods (Training and Validation)&#x0201d; subsection.</p><p>Additionally, the &#x0201c;Materials and methods (Performance metrics and model evaluation)&#x0201d; subsection now specifies that the reported results correspond to the average balanced accuracy across the five folds, with the standard deviation included to quantify variability. Standard deviation values are reported in the &#x0201c;Results and discussion&#x0201d; section.</p><p>***</p><p>5. For better readability, I suggest placing the figure descriptions together with the figures.</p><p>Response: In the revised manuscript, figure descriptions have been placed together with their respective figures (figures 7-10 and figures 12-15).</p><p>***</p><p>6. The paper contains a large number of comparison results. The authors might consider highlighting the most significant ones and moving the rest to the supplementary material.</p><p>Response: We agree with the reviewer&#x02019;s suggestion. To streamline the manuscript, we highlighted the most significant results in the main text and moved the full comparison tables (Tables 2 and 3) to the supplementary materials.</p><p>***</p><p>7. Conclusions and Discussion sections need more work. A summary of the study's goal should be included, along with how the findings support this goal. For example, why classifying sleeping stages is important and how can the method developed benefit research or improve human well-being.</p><p>Response: Following the reviewer&#x02019;s suggestion, we revised the &#x0201c;Conclusion&#x0201d; section to better summarize the study's goal and clarify how the findings support it. The revised text now explicitly states that the objective was to enhance sleep stage classification using visual representations and deep learning. We also emphasize the importance of sleep stage classification for assessing sleep quality, detecting disorders, and advancing research.</p><p>***</p><p>Reviewer #2</p><p>The paper utilizes Gramian Angular Fields, Recurrence Plots, Markov models, Transition Fields, and spectrograms to process time series data collected from smartwatches, aiming to enhance the classification of sleep stages. The analyses and experiments are rigorous. However, the methods utilized are largely standard, and the paper lacks a degree of innovation.</p><p>Response: This study introduces a novel application for sleep stage classification using smartwatch data within a comprehensive full pipeline approach. While techniques such as Gramian Angular Fields (GAF), Recurrence Plots (RP), Markov Transition Fields (MTF) and spectrograms are established in other domains, their combined application and evaluation for this task represent a significant advancement. By assessing their effectiveness under identical conditions and comparing them to traditional approaches, this study bridges a critical gap that has not been addressed before in this context.</p><p>This study not only explores the application of these visual transformations but also in the integration of techniques such as image patching and ensemble methods, which significantly enhance classification performance. The combination of GAF with patching and ensemble techniques achieved a balanced accuracy of over 82% for two-stage classification (sleep/wake) and 62% for three-stage classification (wake/NREM/REM).</p><p>A direct comparison with other commonly used representations for sleep stage classification (feature extraction and direct use of raw data) highlights the advantages of visual representations, demonstrating improved classification accuracy and reinforcing their applicability in sleep monitoring, outperforming traditional methods by up to 8.9 percentage points in two-stage classification and 9.1 percentage points in three-stage classification. These results emphasize the ability of visual representations to capture fine-grained temporal structures and enhance prediction robustness.</p><p>Furthermore, this study provides novel insights into the distinct contributions of accelerometer and heart rate data. Accelerometer data proved more effective for distinguishing between sleep and wake states, while heart rate data played a critical role in three-stage classification. This finding underscores the complementary nature of these data sources and their importance for accurate sleep stage classification.</p><p>To address the reviewer's concern regarding innovation, we revised the &#x0201c;Introduction&#x0201d;, &#x0201c;Discussion&#x0201d;, and &#x0201c;Conclusion&#x0201d; sections to better emphasize the study's contributions. These revisions highlight how the proposed methodology advances the field by offering a non-invasive, cost-effective alternative to polysomnography and enabling more reliable health monitoring and early interventions. This study also paves the way for future research by demonstrating the potential of visual representations and deep learning techniques for sleep analysis.</p><supplementary-material id="pone.0323689.s003" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Response to Reviewers.pdf</named-content></p></caption><media xlink:href="pone.0323689.s003.pdf"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0323689.r004" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323689.r004</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Xiaohui</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Xiaohui Zhang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Xiaohui Zhang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323689" id="rel-obj004" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">16 Mar 2025</named-content>
</p><p><!--<div>-->PONE-D-24-59060R1<!--</div>--><!--<div>-->Time-series visual representations for sleep stages classification<!--</div>--><!--<div>-->PLOS ONE</p><p>Dear Dr. Padovani Ederli,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please submit your revised manuscript by Apr 30 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!--</div>--></p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p><!--<div>-->If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Xiaohui Zhang</p><p>Academic Editor</p><p>PLOS ONE</p><p>Journal Requirements:</p><p>Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article&#x02019;s retracted status in the References list and also include a citation and full reference for the retraction notice.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!--<font color="black">-->
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.<!--</font>--></p><p>Reviewer #1:&#x000a0;All comments have been addressed</p><p>Reviewer #2:&#x000a0;All comments have been addressed</p><p>**********</p><p><!--<font color="black">-->2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->3. Has the statistical analysis been performed appropriately and rigorously? <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!--</font>--></p><p>Reviewer #1:&#x000a0;The study explores the application of visual representations of time series data from smartwatches to classify sleep stages. The authors transform data collected from accelerometers and heart rate sensors into visual formats like Gramian Angular Fields (GAF),Recurrence Plots (RP), Markov Transition Fields (MTF), and spectrograms. The visual representations are then feed into 2d neural network for classification. Most of the comments have been address.</p><p>On data split: Given the imbalanced distribution of the dataset (minority class only 7.7%), using random split instead of stratified split is not a good representation of the data distribution. Without stratification, a random split might lead to some folds having very few (or even no samples) from the minority class. Even with the weight adjustment on minority samples in training, this can still lead to unreliable performance estimates as train and test set may not have the same distribution. I suggest replacing the results with stratified split or checking the class distribution for the random split to make sure the data distribution in each fold is reliably representing the overall distribution.</p><p>Reviewer #2:&#x000a0;All the review comments have been addressed, and the paper has been improved. Although I still believe the methods are standard and lack some innovation, the application and execution of these methods demonstrate creativity. There is potential for further development.</p><p>**********</p><p><!--<font color="black">-->7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!--</font>--></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.<!--</div>--></p></body></sub-article><sub-article article-type="author-comment" id="pone.0323689.r005"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323689.r005</article-id><title-group><article-title>Author response to Decision Letter 2</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323689" id="rel-obj005" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">21 Mar 2025</named-content>
</p><p>Paper title: Time-series visual representations for sleep stages classification</p><p>Journal: PLOS ONE</p><p>March 21th, 2025</p><p>Dear Editor and Reviewers,</p><p>We would like to thank the editor and reviewers for their constructive feedback and thoughtful suggestions. Their insights have been invaluable in improving the clarity and rigor of our manuscript.</p><p>In this revised version, we carefully addressed all the reviewers' comments. Specifically, we clarified methodological choices and provided additional justifications where necessary. We also analyzed the data split strategy in more detail to ensure the reliability of our results.</p><p>The revised manuscript has been submitted through the online submission system, with key changes highlighted. Below, we provide a point-by-point response to each reviewer&#x02019;s comments, detailing the revisions made.</p><p>We greatly appreciate the time and effort the reviewers and the editor invested in evaluating our work. We hope that this version meets the high publication standards of PLOS ONE. After this round of revisions, we believe our manuscript is suitable for publication.</p><p>Please feel free to reach out for any further clarifications.</p><p>Sincerely,</p><p>The authors</p><p>General Comment</p><p>After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Response: We thank the editor and the reviewers for diligently dealing with our submission and for the very constructive feedback.</p><p>In this new version, we addressed the points of both reviewers, and we modified the manuscript according to their comments. We are grateful to both the reviewers and the editor for their comments and the time they dedicated to the reviewing process. With our revised manuscript, we send a point-by-point response in which we did our best to address all of them.</p><p>***</p><p>Reviewer #1</p><p>The study explores the application of visual representations of time series data from smartwatches to classify sleep stages. The authors transform data collected from accelerometers and heart rate sensors into visual formats like Gramian Angular Fields (GAF),Recurrence Plots (RP), Markov Transition Fields (MTF), and spectrograms. The visual representations are then fed into a 2d neural network for classification. Most of the comments have been addressed.</p><p>On data split: Given the imbalanced distribution of the dataset (minority class only 7.7%), using random split instead of stratified split is not a good representation of the data distribution. Without stratification, a random split might lead to some folds having very few (or even no samples) from the minority class. Even with the weight adjustment on minority samples in training, this can still lead to unreliable performance estimates as the train and test set may not have the same distribution. I suggest replacing the results with stratified split or checking the class distribution for the random split to make sure the data distribution in each fold is reliably representing the overall distribution.</p><p>Response: We appreciate your insightful comment regarding the impact of using a random split instead of a stratified split in our cross-validation process. We acknowledge the concern that a random split may result in some folds having very few or no samples from the minority class, potentially affecting the reliability of performance estimates in particular scenarios.</p><p>To address this, we analyzed the class distribution across the five splits. We presented the results in the new Table 2 of the revised version (&#x0201c;Materials and methods / Training and validation&#x0201d; subsection). The table shows the percentage of Wake, NREM, and REM samples for each split in the training and validation sets. The training data maintains a stable class distribution across splits, ensuring a balanced representation during model learning.</p><p>While the validation distribution shows some variability&#x02014;particularly in Split 1 and Split 2 for Wake and Split 5 for REM&#x02014;this reflects real-world sleep data, where sleep stages are inherently imbalanced across different nights and individuals. Since the model is evaluated across multiple splits, the impact of these variations is minimized.</p><p>It is important to emphasize that our dataset consists of sleep time-series data, where each sample corresponds to data from a specific subject. To prevent data leakage, we ensured that data from the same subject were not simultaneously used for training and validation. Unlike traditional classification tasks where stratification can be applied at the sample level, our setup requires entire subjects to be assigned exclusively to either a training fold or a validation fold. Our decision to use a random split was also motivated by the goal of maintaining the natural variability of sleep stage transitions, which differ across nights and individuals.</p><p>Given the constraints imposed by subject-level separation, we believe that using a random split does not introduce substantial bias or compromise the reliability of our results in this study. Additionally, the data distribution in each fold reflects the overall distribution, enabling the model to be tested under conditions that closely resemble real sleep patterns.</p><p>While we appreciate the suggestion to replace the results with a stratified split, our analysis indicates that the observed stability in class distributions and the need to maintain subject independence make our current approach the most appropriate for this study.</p><p>We sincerely appreciate your valuable feedback. Please let us know if further clarification is needed.</p><p>***</p><p>Reviewer #2</p><p>All the review comments have been addressed, and the paper has been improved. Although I still believe the methods are standard and lack some innovation, the application and execution of these methods demonstrate creativity. There is potential for further development.</p><p>Response: We sincerely appreciate your time and effort in reviewing our paper. Your feedback has helped us refine our work, and we are grateful for your positive remarks regarding the application and execution of our methods. While we recognize that the techniques employed are well-established, we focused on demonstrating their effectiveness in a novel application. We agree that there is potential for further development, and we see this work as a foundation for future research.</p><p>Thank you again for your valuable insights and constructive evaluation.</p><supplementary-material id="pone.0323689.s004" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Response_to_Reviewers_auresp_2.pdf</named-content></p></caption><media xlink:href="pone.0323689.s004.pdf"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0323689.r006" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323689.r006</article-id><title-group><article-title>Decision Letter 2</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Xiaohui</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Xiaohui Zhang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Xiaohui Zhang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323689" id="rel-obj006" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">13 Apr 2025</named-content>
</p><p>Time-series visual representations for sleep stages classification</p><p>PONE-D-24-59060R2</p><p>Dear Dr. Padovani Ederli,</p><p>We&#x02019;re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p><p>Within one week, you&#x02019;ll receive an e-mail detailing the required amendments. When these have been addressed, you&#x02019;ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p><p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">Editorial Manager&#x000ae;</ext-link>&#x000a0;and clicking the &#x02018;Update My Information' link at the top of the page. If you have any questions relating to publication charges, please contact our Author Billing department directly at authorbilling@plos.org.</p><p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they&#x02019;ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p><p>Kind regards,</p><p>Xiaohui Zhang</p><p>Academic Editor</p><p>PLOS ONE</p><p>Additional Editor Comments (optional):</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!--<font color="black">-->
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.<!--</font>--></p><p>Reviewer #1:&#x000a0;All comments have been addressed</p><p>**********</p><p><!--<font color="black">-->2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->3. Has the statistical analysis been performed appropriately and rigorously? <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!--</font>--></p><p>Reviewer #1:&#x000a0;The study explores the application of visual representation of time series data from smartwatch to classify sleep stages. In the added table, the authors analyzed the class distribution across the splits and show that the training and testing maintain a relatively stable distribution. All the comments have been addressed and the paper has been improved.</p><p>**********</p><p><!--<font color="black">-->7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!--</font>--></p><p>Reviewer #1:&#x000a0;No</p><p>**********</p></body></sub-article><sub-article article-type="editor-report" id="pone.0323689.r007" specific-use="acceptance-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323689.r007</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Xiaohui</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Xiaohui Zhang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Xiaohui Zhang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323689" id="rel-obj007" related-article-type="reviewed-article"/></front-stub><body><p>PONE-D-24-59060R2</p><p>PLOS ONE</p><p>Dear Dr. Padovani Ederli,</p><p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p><p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p><p>* All references, tables, and figures are properly cited</p><p>* All relevant supporting information is included in the manuscript submission,</p><p>* There are no issues that prevent the paper from being properly typeset</p><p>You will receive further&#x000a0;instructions from the production team, including instructions on how to review your proof when it&#x000a0;is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps.</p><p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p><p>If we can help with anything else, please email us at customercare@plos.org.</p><p>Thank you for submitting your work to PLOS ONE and supporting open access.</p><p>Kind regards,</p><p>PLOS ONE Editorial Office Staff</p><p>on behalf of</p><p>Dr. Xiaohui Zhang</p><p>Academic Editor</p><p>PLOS ONE</p></body></sub-article></article>