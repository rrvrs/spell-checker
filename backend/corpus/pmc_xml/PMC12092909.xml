<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">J Imaging Inform Med</journal-id><journal-id journal-id-type="iso-abbrev">J Imaging Inform Med</journal-id><journal-title-group><journal-title>Journal of Imaging Informatics in Medicine</journal-title></journal-title-group><issn pub-type="ppub">2948-2925</issn><issn pub-type="epub">2948-2933</issn><publisher><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39402356</article-id><article-id pub-id-type="pmc">PMC12092909</article-id>
<article-id pub-id-type="publisher-id">1256</article-id><article-id pub-id-type="doi">10.1007/s10278-024-01256-x</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Deep Learning&#x02013;Based Estimation of Radiographic Position to Automatically Set Up the X-Ray Prime Factors</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0112-9709</contrib-id><name><surname>Del Cerro</surname><given-names>C. F.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3090-126X</contrib-id><name><surname>Gim&#x000e9;nez</surname><given-names>R. C.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1452-1918</contrib-id><name><surname>Garc&#x000ed;a-Blas</surname><given-names>J.</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Sosenko</surname><given-names>K.</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0009-0005-1492-3513</contrib-id><name><surname>Ortega</surname><given-names>J. M.</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0989-3231</contrib-id><name><surname>Desco</surname><given-names>M.</given-names></name><address><email>desco@hggm.es</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4847-7233</contrib-id><name><surname>Abella</surname><given-names>M.</given-names></name><address><email>mabella@ing.uc3m.es</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff5">5</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03ths8210</institution-id><institution-id institution-id-type="GRID">grid.7840.b</institution-id><institution-id institution-id-type="ISNI">0000 0001 2168 9183</institution-id><institution>Dept. Bioingenier&#x000ed;a, </institution><institution>Universidad Carlos III de Madrid, </institution></institution-wrap>Legan&#x000e9;s, Madrid, Spain </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0111es613</institution-id><institution-id institution-id-type="GRID">grid.410526.4</institution-id><institution-id institution-id-type="ISNI">0000 0001 0277 7938</institution-id><institution>Instituto de Investigaci&#x000f3;n Sanitaria Gregorio Mara&#x000f1;&#x000f3;n, </institution></institution-wrap>Madrid, Spain </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03ths8210</institution-id><institution-id institution-id-type="GRID">grid.7840.b</institution-id><institution-id institution-id-type="ISNI">0000 0001 2168 9183</institution-id><institution>Dept. de Inform&#x000e1;tica, </institution><institution>Universidad Carlos III de Madrid, </institution></institution-wrap>Madrid, Spain </aff><aff id="Aff4"><label>4</label>Sociedad Espa&#x000f1;ola de Electromedicina y Calidad S.A. (SEDECAL), Madrid, Spain </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02qs1a797</institution-id><institution-id institution-id-type="GRID">grid.467824.b</institution-id><institution-id institution-id-type="ISNI">0000 0001 0125 7682</institution-id><institution>Centro Nacional de Investigaciones Cardiovasculares Carlos III (CNIC), </institution></institution-wrap>Madrid, Spain </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/009byq155</institution-id><institution-id institution-id-type="GRID">grid.469673.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 5901 7501</institution-id><institution>Centro de Investigaci&#x000f3;n en Red en Salud Mental (CIBERSAM), </institution></institution-wrap>Madrid, Spain </aff></contrib-group><pub-date pub-type="epub"><day>14</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>14</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="collection"><month>6</month><year>2025</year></pub-date><volume>38</volume><issue>3</issue><fpage>1661</fpage><lpage>1668</lpage><history><date date-type="received"><day>11</day><month>6</month><year>2024</year></date><date date-type="rev-recd"><day>12</day><month>8</month><year>2024</year></date><date date-type="accepted"><day>30</day><month>8</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Radiation dose and image quality in radiology are influenced by the X-ray prime factors: KVp, mAs, and source-detector distance. These parameters are set by the X-ray technician prior to the acquisition considering the radiographic position. A wrong setting of these parameters may result in exposure errors, forcing the test to be repeated with the increase of the radiation dose delivered to the patient. This work presents a novel approach based on deep learning that automatically estimates the radiographic position from a photograph captured prior to X-ray exposure, which can then be used to select the optimal prime factors. We created a database using 66 radiographic positions commonly used in clinical settings, prospectively obtained during 2022 from 75 volunteers in two different X-ray facilities. The architecture for radiographic position classification was a lightweight version of <italic>ConvNeXt</italic> trained with fine-tuning, discriminative learning rates, and a one-cycle policy scheduler. Our resulting model achieved an accuracy of 93.17% for radiographic position classification and increased to 95.58% when considering the correct selection of prime factors, since half of the errors involved positions with the same KVp and mAs values. Most errors occurred for radiographic positions with similar patient pose in the photograph. Results suggest the feasibility of the method to facilitate the acquisition workflow reducing the occurrence of exposure errors while preventing unnecessary radiation dose delivered to patients.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Radiography</kwd><kwd>Radiographic position</kwd><kwd>Prime factors</kwd><kwd>Deep learning</kwd><kwd>Classification</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004837</institution-id><institution>Ministerio de Ciencia e Innovaci&#x000f3;n</institution></institution-wrap></funding-source><award-id>PDC2021-121656-I00</award-id><principal-award-recipient><name><surname>Abella</surname><given-names>M.</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004587</institution-id><institution>Instituto de Salud Carlos III</institution></institution-wrap></funding-source><award-id>PMPTA22/00121</award-id><award-id>PMPTA22/00118</award-id><principal-award-recipient><name><surname>Desco</surname><given-names>M.</given-names></name><name><surname>Abella</surname><given-names>M.</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100007601</institution-id><institution>Horizon 2020</institution></institution-wrap></funding-source><award-id>801091</award-id><principal-award-recipient><name><surname>Garc&#x000ed;a-Blas</surname><given-names>J.</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Society for Imaging Informatics in Medicine 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">The quality of a radiograph, i.e., contrast resolution and signal-to-noise ratio, depends on the optimal tuning of the so-called prime factors: source voltage (KVp),&#x000a0;source&#x000a0;current and&#x000a0;exposure&#x000a0;time (mAs), and distance from the source to the detector (SDD). These factors that depend on the radiographic position are typically adjusted by a technician prior to acquisition using pre-calculated values based on standards [<xref ref-type="bibr" rid="CR1">1</xref>] and guidelines from the system manufacturer. However, a wrong selection of these factors can result in non-diagnostic images, requiring repetition and exposing the patient to additional ionizing radiation. In fact, studies have shown that the repeat/rejection ratio in clinical practice is between 13% and 14% [<xref ref-type="bibr" rid="CR2">2</xref>]. While most modern systems have an <italic>Automatic Exposure Control</italic> (AEC) that interrupts the radiation when the&#x000a0;exposure&#x000a0;is sufficient at the detector [<xref ref-type="bibr" rid="CR3">3</xref>], it is not effective when&#x000a0;prime factors&#x000a0;are smaller than necessary. Therefore, an automatic mechanism to adjust prime factors to optimal values depending on the patient pose would be highly beneficial.</p><p id="Par3">To the best of our knowledge, no other work has been published that addresses this problem. Several studies have explored the use of <italic>convolutional neural networks</italic> (CNN) to automatically detect radiographic positions in the acquired radiography for quality control [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>]. Other studies have used pretrained networks such as <italic>VGG</italic> [<xref ref-type="bibr" rid="CR6">6</xref>] or <italic>ResNet</italic> [<xref ref-type="bibr" rid="CR7">7</xref>] to classify chest radiographs into anteroposterior or posteroanterior views [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>], or&#x000a0;to detect the radiographic position of hands [<xref ref-type="bibr" rid="CR10">10</xref>]. However, these methods rely on acquired X-ray images for position detection, which does not prevent the patient from receiving an extra radiation dose in case of errors.</p><p id="Par4">On the other hand, the identification of human body parts in photographs has been done with <italic>You Only Look Once</italic> (<italic>YOLO</italic>) [<xref ref-type="bibr" rid="CR11">11</xref>], but this model is not able to distinguish between different human poses [<xref ref-type="bibr" rid="CR12">12</xref>]. Other works described the human pose by a complete representation using key points [<xref ref-type="bibr" rid="CR13">13</xref>], in a variety of clinical applications: estimating the position of clinicians for surgical workflow analysis [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>] or in-bed patients&#x000a0;for monitoring&#x000a0;[<xref ref-type="bibr" rid="CR16">16</xref>], and analyzing infant motion for early detection of cerebral palsy [<xref ref-type="bibr" rid="CR17">17</xref>]. Nevertheless, an accurate estimation of key points involves manual annotation, which is time-consuming and not necessary for the classification problem of radiographic position estimation. In [<xref ref-type="bibr" rid="CR18">18</xref>], the authors optimized a transfer learning methodology for a CNN to classify human activities, but the application of this approach for radiographic position estimation in a clinical setting is yet to be evaluated. In this work, we intend to fill this gap by developing an automatic method for the&#x000a0;detection of the radiographic position of the patient to set up the X-ray prime factors. Our main contributions include the creation of an extensive database containing photographs of patients in a clinical setting for the most common radiographic positions used in clinical practice. We combined transfer learning with a cutting-edge deep-learning architecture to develop a model intended for real-time applications that uses a photograph of the patient positioned in the system to estimate the radiographic position.</p></sec><sec id="Sec2"><title>Material and Methods</title><p id="Par5">The proposed method estimates the radiographic position from a photograph of a patient. This information is used to determine the parameters of the X-ray source (KVp and mAs) by looking up a predefined table&#x000a0;to ensure image quality. The following sections&#x000a0;describe the database, the resulting model, the&#x000a0;evaluation methodology, and the creation of an executable program to be used in a real system.</p><sec id="Sec3"><title>Database</title><p id="Par6">We selected 66 radiographic positions commonly used in clinical settings (see Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>), from 75 volunteers. These positions were chosen in collaboration with radiology technicians from Gregorio Mara&#x000f1;&#x000f3;n University Hospital (Madrid, Spain). Data acquisition was conducted in two locations, one for 50 volunteers and a different one for the remaining 25 volunteers. Although no radiation was involved in the acquisition, all participants signed a legal consent that recognizes the protection of their data as established in national and international regulations (General Data Protection Regulation, EU Regulation 2016/679).
<table-wrap id="Tab1"><label>Table&#x000a0;1</label><caption><p>List of the 66 radiographic positions used in the study, categorized by body region</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Body region</th><th align="left">Positions</th></tr></thead><tbody><tr><td align="left">Head</td><td align="left">Skull PA, Skull LAT, Sinus, Nasal Bones LAT
</td></tr><tr><td align="left">Spine</td><td align="left">Cervical Spine AP, Cervical Spine LAT, Thoracic Spine AP, Thoracic Spine LAT, Lumbar Spine AP, Lumbar Spine LAT</td></tr><tr><td align="left">Shoulder</td><td align="left">Shoulder Left AP, Shoulder Left Axial, Shoulder Right AP, Shoulder Right Axial
</td></tr><tr><td align="left">Upper limb</td><td align="left"> Elbow Left AP, Elbow Left LAT, Elbow Right AP, Elbow Right LAT, Wrist Left AP, Wrist Left LAT, Wrist Right AP, Wrist Right LAT, Hand Left AP, Hand Left OBL, Hand Right AP, Hand Right OBL,&#x000a0;Fingers Left 1st AP, Fingers Left 1st LAT, Fingers Left 3rd AP, Fingers Left 3rd LAT, Fingers Right 1st AP, Fingers Right 1st LAT, Fingers Right 3rd AP, Fingers Right 3rd LAT
</td></tr><tr><td align="left">Torso</td><td align="left"> Thorax&#x000a0;PA, Thorax LAT, Abdomen AP, Ribs Left AP, Ribs Right AP, Pelvis AP, Hip Left AP, Hip Left Axial, Hip Right AP, Hip Right Axial,&#x000a0;Sacro-coccyx AP, Sacro-coccyx LAT&#x000a0;
</td></tr><tr><td align="left">Lower limb</td><td align="left">Femur Left AP, Femur Left LAT, Femur Right AP, Femur Right LAT, Knee Left AP, Knee Left LAT, Knee Right AP, Knee Right LAT, Tibia Left AP, Tibia Left LAT, Tibia Right AP, Tibia Right LAT, Ankle Left AP, Ankle Left LAT, Ankle Right AP, Ankle Right LAT, Foot Left AP, Foot Left OBL, Foot Right AP, Foot Right OBL
</td></tr></tbody></table></table-wrap></p><p id="Par7">The data acquisition setup in both facilities consisted of an X-ray table,&#x000a0;a wall stand, and a ceiling-mounted X-ray source equipped with a YI 4&#x000a0;K Action camera attached to the collimator case using a monopod. To minimize the influence of background elements, we cropped the photographs to a square region with a width of 90&#x000a0;cm, corresponding to the width of the X-ray table&#x000a0;(red square in Fig. <xref rid="Fig1" ref-type="fig">1</xref>), considering the 19&#x000a0;cm offset on the vertical axis of the camera with respect to the collimator light.<fig id="Fig1"><label>Fig.&#x000a0;1</label><caption><p>Example of thorax AP position at both facilities</p></caption><graphic xlink:href="10278_2024_1256_Fig1_HTML" id="MO1"/></fig></p><p id="Par8">The pixel size to calculate the cropped area was determined using Eq.&#x000a0;<xref rid="Equ1" ref-type="disp-formula">1</xref>, as illustrated in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e428">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$PixSize={FOV}_{mm}/{FOV}_{px}=\left({h}_{s}\times {SOD}_{cm}/{LSD}_{cm}\right)/{FOV}_{px}=m\times {SOD}_{cm}=m\times \left({SDD}_{cm}-{PDD}_{cm}\right)$$\end{document}</tex-math><mml:math id="d33e434" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">FOV</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">mm</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">/</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">FOV</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">px</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:msub><mml:mi>h</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">SOD</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">cm</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">/</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">LSD</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">cm</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo stretchy="false">/</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">FOV</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">px</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">SOD</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">cm</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="italic">SDD</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">cm</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">PDD</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">cm</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="10278_2024_1256_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="d33e524">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${h}_{s}$$\end{document}</tex-math><mml:math id="d33e529"><mml:msub><mml:mi>h</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="10278_2024_1256_Article_IEq1.gif"/></alternatives></inline-formula> is the height of the camera sensor, <inline-formula id="IEq2"><alternatives><tex-math id="d33e535">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${LSD}_{cm}$$\end{document}</tex-math><mml:math id="d33e540"><mml:msub><mml:mrow><mml:mi mathvariant="italic">LSD</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">cm</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10278_2024_1256_Article_IEq2.gif"/></alternatives></inline-formula> is the distance between the lens and camera sensor, <inline-formula id="IEq3"><alternatives><tex-math id="d33e551">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${FOV}_{px}$$\end{document}</tex-math><mml:math id="d33e556"><mml:msub><mml:mrow><mml:mi mathvariant="italic">FOV</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">px</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10278_2024_1256_Article_IEq3.gif"/></alternatives></inline-formula> is the field of view in pixels, <inline-formula id="IEq4"><alternatives><tex-math id="d33e566">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m$$\end{document}</tex-math><mml:math id="d33e571"><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href="10278_2024_1256_Article_IEq4.gif"/></alternatives></inline-formula> is a camera-dependent slope,&#x000a0;<inline-formula id="IEq5"><alternatives><tex-math id="d33e575">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${SDD}_{cm}$$\end{document}</tex-math><mml:math id="d33e580"><mml:msub><mml:mrow><mml:mi mathvariant="italic">SDD</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">cm</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10278_2024_1256_Article_IEq5.gif"/></alternatives></inline-formula> is the distance between the source and detector, and <inline-formula id="IEq6"><alternatives><tex-math id="d33e590">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${PDD}_{cm}$$\end{document}</tex-math><mml:math id="d33e595"><mml:msub><mml:mrow><mml:mi mathvariant="italic">PDD</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">cm</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10278_2024_1256_Article_IEq6.gif"/></alternatives></inline-formula> is the distance between the patient and detector (10&#x000a0;cm in our case). The calculated pixel size was 0.0513&#x000a0;cm, 0.0798&#x000a0;cm, and 0.0969&#x000a0;cm for SDD of 100, 150, and 180, respectively.<fig id="Fig2"><label>Fig.&#x000a0;2</label><caption><p>Ray-tracing diagram of the FOV with respect to the camera lens and sensor size</p></caption><graphic xlink:href="10278_2024_1256_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec4"><title>Proposed Model</title><p id="Par9">We adapted the lightweight <italic>atto</italic> version of the <italic>ConvNeXt</italic> architecture (CN) [<xref ref-type="bibr" rid="CR19">19</xref>] from <italic>Pytorch Image Models</italic> (timm) [<xref ref-type="bibr" rid="CR20">20</xref>], to accommodate the 66 radiographic positions considered in our study and enable real-time inference (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>). The loss function was cross-entropy, and we used the Ranger optimizer [<xref ref-type="bibr" rid="CR21">21</xref>].<fig id="Fig3"><label>Fig.&#x000a0;3</label><caption><p>Proposed architecture. Conv2d, convolution layers; LN, normalization layer; k, kernel size; s, stride; and p, padding</p></caption><graphic xlink:href="10278_2024_1256_Fig3_HTML" id="MO3"/></fig></p><p id="Par10">We conducted three experiments, resulting in a total of seven models. In the first experiment, we compared the performance of using color and grayscale images to determine the optimal color mode. In the second experiment, we used the selected color mode and evaluated the impact of three types of data augmentation and their combinations on the training set. This doubled the size of the database for one augmentation and increased it fourfold when combining all the augmentations. Data augmentations&#x000a0;were&#x000a0;horizontal flip, random zoom with a scale factor between 1.1 and 1.3, and random rotation of an angle &#x003b8; drawn from a Gaussian distribution with a mean of 0 and a standard deviation of 10&#x000a0;degrees. We trained end-to-end these six pretrained models pretrained with <italic>ImageNet-1K</italic> [<xref ref-type="bibr" rid="CR22">22</xref>] and obtained an optimal learning rate of 1&#x02009;&#x000d7;&#x02009;10<sup>&#x02212;3</sup> with the Leslie N. Smith test [<xref ref-type="bibr" rid="CR23">23</xref>]. In the third experiment, we added fine-tuning (FT), discriminative learning rates (DLR) [<xref ref-type="bibr" rid="CR24">24</xref>], and a one-cycle policy (OCP) scheduler [<xref ref-type="bibr" rid="CR23">23</xref>]. The Leslie N. Smith indicated an optimal learning rate of 1&#x02009;&#x000d7;&#x02009;10<sup>&#x02212;3</sup> when all layers were frozen except for the last one. When training the model end-to-end, we used discriminative learning rates of 1&#x02009;&#x000d7;&#x02009;10<sup>&#x02212;6</sup>, 1&#x02009;&#x000d7;&#x02009;10<sup>&#x02212;5</sup>, and 1&#x02009;&#x000d7;&#x02009;10<sup>&#x02212;4</sup> for the first, second, and last third of our model, respectively, as determined by the same test.</p><p id="Par11">Input images&#x000a0;were rescaled to 224&#x02009;&#x000d7;&#x02009;224 pixels and normalized based on the mean and standard deviation of <italic>ImageNet-1K</italic>.</p></sec><sec id="Sec5"><title>Implementation</title><p id="Par12">The proposed model was implemented in<italic> Python</italic> [<xref ref-type="bibr" rid="CR20">20</xref>],&#x000a0;using <italic>PyTorch</italic> [<xref ref-type="bibr" rid="CR25">25</xref>]&#x000a0;and&#x000a0;<italic>Fastai</italic> [<xref ref-type="bibr" rid="CR26">26</xref>] libraries. We then developed an executable program using <italic>LibTorch</italic>, a C&#x02009;++&#x02009;library that provides an API for <italic>PyTorch</italic>, and <italic>OpenCV</italic> [<xref ref-type="bibr" rid="CR27">27</xref>] for image preprocessing. To ensure compatibility between <italic>PyTorch</italic> and <italic>LibTorch</italic>, we used <italic>Torch Script</italic>. The program was compiled for Windows&#x000ae; machines with&#x02009;&#x000d7;&#x02009;86&#x02013;64 processor architecture and can perform image inference using only the CPU, without requiring <italic>Python</italic> installation. The workflow involves converting the photograph to grayscale, cropping it to a 90-cm square region, resizing it to 224&#x02009;&#x000d7;&#x02009;224 pixels, and normalizing it based on the mean and standard deviation of <italic>ImageNet-1&#x000a0;K</italic> (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). This preprocessed image is then passed through our model to infer the radiographic position, which is used to automatically determine the appropriate KVp and mAs values based on a predefined table.<fig id="Fig4"><label>Fig.&#x000a0;4</label><caption><p>Workflow of the conversion from trained models to the final executable</p></caption><graphic xlink:href="10278_2024_1256_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec6"><title>Evaluation Methodology</title><p id="Par13">To verify the robustness of the models given the limited size of the database, we performed&#x000a0;5-fold cross-validation with five randomly chosen validation sets for all experiments, splitting the dataset into 60 volunteers for training and 15 for validation.</p><p id="Par14">Since some radiographic positions share the same prime factors, we evaluated the accuracy of the proposed models in terms&#x000a0;of both radiographic position identification and prime factor values. To assess classification errors, we used confusion matrices generated from the validation sets.</p><p id="Par15">We compared our architecture with two lightweight state-of-the-art models with a similar number of parameters (3.7M), EfficientFormerV2 (EF) [<xref ref-type="bibr" rid="CR28">28</xref>] and MobileNetV4 (MN) [<xref ref-type="bibr" rid="CR29">29</xref>], both from the <italic>timm</italic> library. The chi-square test was used to assess differences in the accuracy rates for each model.</p></sec></sec><sec id="Sec7"><title>Results</title><p id="Par16">Table <xref rid="Tab2" ref-type="table">2</xref> presents the mean and standard deviation of the&#x000a0;5-fold cross-validation results for each architecture, considering both grayscale (GS) and color (RGB) models. Our model, CN,&#x000a0;achieves superior performance (<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05) compared to EF and MN in both radiographic position identification and prime factors accuracy. While the performance using GS and RGB models was very similar, with no significant differences between them, GS showed a shorter inference time (using only one channel).
<table-wrap id="Tab2"><label>Table&#x000a0;2</label><caption><p>Results obtained for each color mode with three architectures. The best result is highlighted in bold, and the second-best result is underlined. *<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05 with respect to CN (GS)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Time/epoch (s) &#x02193;</th><th align="left">Best epoch&#x02193;</th><th align="left">Cross-entropy&#x02193;</th><th align="left">Position acc (%)&#x02191;</th><th align="left">Prime factors acc. (%)&#x02191;</th></tr></thead><tbody><tr><td align="left">CN (GS)</td><td align="left">12</td><td char="." align="char"><underline>7&#x02009;&#x000b1;&#x02009;1</underline></td><td char="." align="char"><bold>0.27&#x02009;&#x000b1;&#x02009;0.03</bold></td><td char="." align="char"><bold>91.15&#x02009;&#x000b1;&#x02009;1.06</bold></td><td char="." align="char"><underline>94.10&#x02009;&#x000b1;&#x02009;0.93</underline></td></tr><tr><td align="left">CN (RGB)</td><td align="left">14</td><td char="." align="char"><underline>7&#x02009;&#x000b1;&#x02009;1</underline></td><td char="." align="char"><underline>0.27&#x02009;&#x000b1;&#x02009;0.04</underline></td><td char="." align="char"><underline>91.15&#x02009;&#x000b1;&#x02009;1.28</underline></td><td char="." align="char"><bold>94.57&#x02009;&#x000b1;&#x02009;1.34</bold></td></tr><tr><td align="left">EF (GS)</td><td align="left">12</td><td char="." align="char"><bold>6&#x02009;&#x000b1;&#x02009;0</bold></td><td char="." align="char">0.41&#x02009;&#x000b1;&#x02009;0.03</td><td char="." align="char">86.48&#x02009;&#x000b1;&#x02009;1.24*</td><td char="." align="char">92.01&#x02009;&#x000b1;&#x02009;1.24*</td></tr><tr><td align="left">EF (RGB)</td><td align="left">14</td><td char="." align="char">15&#x02009;&#x000b1;&#x02009;18</td><td char="." align="char">0.37&#x02009;&#x000b1;&#x02009;0.03</td><td char="." align="char">88.77&#x02009;&#x000b1;&#x02009;1.47*</td><td char="." align="char">93.48&#x02009;&#x000b1;&#x02009;0.97*</td></tr><tr><td align="left">MN (GS)</td><td align="left"><bold>8</bold></td><td char="." align="char">26&#x02009;&#x000b1;&#x02009;9</td><td char="." align="char">0.46&#x02009;&#x000b1;&#x02009;0.03</td><td char="." align="char">87.90&#x02009;&#x000b1;&#x02009;1.20*</td><td char="." align="char">91.51&#x02009;&#x000b1;&#x02009;1.15*</td></tr><tr><td align="left">MN (RGB)</td><td align="left"><underline>10</underline></td><td char="." align="char">22&#x02009;&#x000b1;&#x02009;12</td><td char="." align="char">0.43&#x02009;&#x000b1;&#x02009;0.02</td><td char="." align="char">87.82&#x02009;&#x000b1;&#x02009;0.95*</td><td char="." align="char">91.62&#x02009;&#x000b1;&#x02009;0.76*</td></tr></tbody></table></table-wrap></p><p id="Par17">Table <xref rid="Tab3" ref-type="table">3</xref> presents the mean and standard deviation of the 5-fold cross-validation results for different data augmentations and training policies for the&#x000a0;proposed architecture, CN. The use of individual data augmentations or the combination of all of them did not significantly improve the accuracy of the grayscale model. However, the addition of fine-tuning, discriminative learning rates, and OCP reduced cross-entropy and improved both radiographic position and prime factors accuracy by 2.21% and 1.57% respectively (<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05).
<table-wrap id="Tab3"><label>Table&#x000a0;3</label><caption><p>Results obtained for the grayscale models with the proposed architecture&#x000a0;CN(GS), adding&#x000a0;different data augmentations and training policies. The best result is highlighted in bold, and the second-best result is underlined. *<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05 with respect to CN (GS)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Time/epoch (s) &#x02193;</th><th align="left">Best epoch&#x02193;</th><th align="left">Cross-entropy&#x02193;</th><th align="left">Position acc (%)&#x02191;</th><th align="left">Prime factors acc. (%)&#x02191;</th></tr></thead><tbody><tr><td align="left">CN (GS&#x02009;+&#x02009;flip)</td><td align="left"><bold>24</bold></td><td char="." align="char">5&#x02009;&#x000b1;&#x02009;1</td><td char="." align="char">0.27&#x02009;&#x000b1;&#x02009;0.02</td><td char="." align="char">91.00&#x02009;&#x000b1;&#x02009;0.41</td><td char="." align="char">93.85&#x02009;&#x000b1;&#x02009;0.88</td></tr><tr><td align="left">CN (GS&#x02009;+&#x02009;zoom)</td><td align="left"><bold>24</bold></td><td char="." align="char"><underline>4&#x02009;&#x000b1;&#x02009;1</underline></td><td char="." align="char">0.27&#x02009;&#x000b1;&#x02009;0.03</td><td char="." align="char">90.98&#x02009;&#x000b1;&#x02009;0.74</td><td char="." align="char">94.22&#x02009;&#x000b1;&#x02009;0.98</td></tr><tr><td align="left">CN (GS&#x02009;+&#x02009;rotation)</td><td align="left"><bold>24</bold></td><td char="." align="char">5&#x02009;&#x000b1;&#x02009;1</td><td char="." align="char">0.27&#x02009;&#x000b1;&#x02009;0.02</td><td char="." align="char">91.66&#x02009;&#x000b1;&#x02009;1.00</td><td char="." align="char">94.49&#x02009;&#x000b1;&#x02009;1.05</td></tr><tr><td align="left">CN (GS&#x02009;+&#x02009;flip&#x02009;+&#x02009;zoom&#x02009;+&#x02009;rotation)</td><td align="left"><underline>46</underline></td><td char="." align="char"><bold>3&#x02009;&#x000b1;&#x02009;2</bold></td><td char="." align="char"><underline>0.27&#x02009;&#x000b1;&#x02009;0.01</underline></td><td char="." align="char"><underline>91.77&#x02009;&#x000b1;&#x02009;0.88</underline></td><td char="." align="char"><underline>94.54&#x02009;&#x000b1;&#x02009;0.39</underline></td></tr><tr><td align="left">CN (GS&#x02009;+&#x02009;flip&#x02009;+&#x02009;zoom&#x02009;+&#x02009;rotation (FT&#x02009;+&#x02009;DLR&#x02009;+&#x02009;OCP))</td><td align="left"><underline>46</underline></td><td char="." align="char">5&#x02009;&#x000b1;&#x02009;0</td><td char="." align="char"><bold>0.21&#x02009;&#x000b1;&#x02009;0.02</bold></td><td char="." align="char"><bold>93.17&#x02009;&#x000b1;&#x02009;0.80*</bold></td><td char="." align="char"><bold>95.58&#x02009;&#x000b1;&#x02009;0.82*</bold></td></tr></tbody></table></table-wrap></p><p id="Par18">The left panel of&#x000a0;Fig. <xref rid="Fig5" ref-type="fig">5</xref>&#x000a0;displays the confusion matrix, with most predictions falling along the diagonal, indicating correct classification. The right panel of Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> shows that errors in prime factors correspond to similar radiographic positions, such as Skull PA and Sinus (five errors) or Sacro-coccyx AP and Pelvis AP (four errors), which are marked with a red square.<fig id="Fig5"><label>Fig.&#x000a0;5</label><caption><p>Confusion matrix (left) and errors with different prime factors (right) for one of the validation sets</p></caption><graphic xlink:href="10278_2024_1256_Fig5_HTML" id="MO5"/></fig></p><p id="Par19">Errors in radiographic position classification occurred in 6% of validation set cases. These errors can be categorized into four classes: similar patient pose with the same prime factors (35%), similar patient pose with different prime factors (46%), different patient poses with the same prime factors (4%), and different patient poses with different prime factors (15%). Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> provides an example of each class.<fig id="Fig6"><label>Fig.&#x000a0;6</label><caption><p>Scenarios with similar (left) and different (right) patient poses. Errors are highlighted in bold</p></caption><graphic xlink:href="10278_2024_1256_Fig6_HTML" id="MO6"/></fig></p></sec><sec id="Sec8"><title>Discussion and Conclusion</title><p id="Par20">We&#x000a0;have&#x000a0;presented a novel method that estimates the radiographic position from photographs to automatically select the prime factors in real time for X-ray acquisition. To achieve this, we created a database containing photographs of patients in a clinical setting, covering the most common radiographic positions. These photographs were taken with a camera attached to the collimator case, preprocessed, and fed into <italic>ConvNeXt</italic>. This architecture was chosen for its superior balance between efficiency and accuracy, making it well suited for real-time applications.</p><p id="Par21">The comparison of the proposed architecture with previously proposed architectures EfficientFormerV2 and MobileNetV4 showed statistically significant better results. The&#x000a0;combination&#x000a0;of data augmentation techniques and the&#x000a0;application of&#x000a0;training policies resulted in statistically significant improvements in performance, achieving an accuracy of 93.17% for radiographic position classification, despite the limited size of our database. Most errors occurred for positions with a similar patient pose in the photograph. The accuracy increased to 95.88% when evaluating&#x000a0;the correct selection of prime factors, as half of the errors involved positions with the same KVp and mAs values. Only 1% of the total images corresponded to errors with different patient poses and different prime factors. These errors may be attributed to inconsistencies in our database, as some radiographic positions were acquired at different distances in the two facilities. This problem could be minimized by increasing the database with more examples of these radiographic positions. Nevertheless, the remaining errors would not represent a major issue in a real scenario, as the acquisition would still be supervised by an X-ray technician.</p><p id="Par22">Since the prime factors vary slightly depending on the anatomy of the patient, future research will evaluate the use of 3D cameras to complement the pose information with patient thickness data, estimated from the distance between the patient and the camera sensor. This will involve evaluating changes in the prime factors according to the individual morphology of the patient and creating the&#x000a0;corresponding database.</p><p id="Par23">The variation in X-ray room layouts could be a potential challenge that was partly addressed by implementing&#x000a0;the region-of-interest cropping strategy. This could be further enhanced by increasing the database with acquisitions from more facilities or by generating synthetic data with different backgrounds [<xref ref-type="bibr" rid="CR30">30</xref>].</p><p id="Par24">The runtime and performance of our prototype demonstrate the feasibility of our method for real-time use, enabling the reduction of exposure errors and unnecessary radiation dose delivered to patients due to retakes. The implementation of our method in a real system only requires a camera, an executable installed on the control PC, and a new GUI workflow which automatically selects the positions to be confirmed by the X-ray technician. Since radiography is the most common medical imaging modality used in the&#x000a0;clinics [<xref ref-type="bibr" rid="CR31">31</xref>], the incorporation of&#x000a0;this method in radiological systems can significantly impact the healthcare system in terms of patient safety and radiology department workflow optimization.</p></sec></body><back><fn-group><fn><p><bold>Publisher's Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p><bold>Change history</bold></p><p>4/24/2025</p><p>A Correction to this paper has been published: 10.1007/s10278-025-01476-9</p></fn></fn-group><notes notes-type="author-contribution"><title>Author Contribution</title><p>Conceptualization, MA KS, JMO, and MD; methodology, MA, MD, and CFC; software, CFC, RCG, and JGB; validation, CFC, KS, JMO, and MA; formal analysis, MA and MD; investigation, MA, CFC, and RCG; resources, MA, JGB, KS, JMO, and MD; data curation, CFC, KS, and JMO; writing&#x02014;original draft, CFC, RCG, JGB, and MA; writing&#x02014;review and editing, KS, JMO, MA, and MD; supervision, MA and MD; project administration, MA; funding acquisition, MA, JGB, and MD. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This work was supported by Ministerio de Ciencia e Innovaci&#x000f3;n, Agencia Estatal de Investigaci&#x000f3;n, and co-funded by the European Regional Development Fund, &#x0201c;A way of making Europe&#x0201d;: PDC2021-121656-I00 (MULTIRAD), funded by MCIN/AEI/10.13039/501100011033 and by the European Union &#x0201c;NextGenerationEU&#x0201d;/PRTR. This is also funded by Instituto de Salud Carlos III through the projects PMPTA22/00121 and PMPTA22/00118, co-funded by the European Union &#x0201c;NextGenerationEU&#x0201d;/PRTR, and by the ASPIDE Project funded by the European Union&#x02019;s Horizon 2020 Research and Innovation Programme under grant 801091. The CNIC is supported by Instituto de Salud Carlos III, Ministerio de Ciencia e Innovaci&#x000f3;n, and the Pro CNIC Foundation.</p></notes><notes><title>Declarations</title><notes id="FPar1"><title>Ethics Approval</title><p id="Par25">This is an observational study. Approval was granted by the Ethics Committee of the University Carlos III de Madrid.</p></notes><notes id="FPar2"><title>Consent to Participate</title><p id="Par26">All participants signed a legal consent which recognizes the protection of their data as established in the &#x0201c;Ley Org&#x000e1;nica de Protecci&#x000f3;n Jur&#x000ed;dica del Menor&#x0201d; (Ley O. 1/96, of January 15th), &#x0201c;Ley General de Sanidad&#x0201d; (Article 10.3, Ley 14/1986, of April 25th), &#x0201c;Ley B&#x000e1;sica de Autonom&#x000ed;a del Paciente y de Informaci&#x000f3;n y Documentaci&#x000f3;n Cl&#x000ed;nica&#x0201d; (Chapters I and III Ley 41/2002, of November 14th), &#x0201c;Ley Org&#x000e1;nica 3/20 18&#x0201d; (of December 5th regarding the Protection of Personal Data and the Guarantee of Digital Rights), and the General Data Protection Regulation (EU Regulation 2016/679).</p></notes><notes id="FPar3"><title>Consent for Publication</title><p id="Par27">The authors affirm that human research participants provided informed consent for the publication of the images in Figure(s) 1, 4, and 6.</p></notes><notes id="FPar4" notes-type="COI-statement"><title>Competing Interests</title><p id="Par28">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Panzer, W., P. Shrimpton, Jessen K.: European Guidelines on Quality Criteria for Computed Tomography. Office for Official Publications of the European Communities, 2020</mixed-citation></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Foos</surname><given-names>DH</given-names></name><etal/></person-group><article-title>Digital radiography reject analysis: data collection methodology, results, and recommendations from an in-depth investigation at two hospitals</article-title><source>Journal of digital imaging</source><year>2009</year><volume>22</volume><fpage>89</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1007/s10278-008-9112-5</pub-id><pub-id pub-id-type="pmid">18446413</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Foos, D. H., Sehnert, W. J., Reiner, B., Siegel, E. L., Segal, A., Waldman, D. L.: Digital radiography reject analysis: data collection methodology, results, and recommendations from an in-depth investigation at two hospitals.&#x000a0;Journal of digital imaging:&#x000a0;22, 89&#x02013;98, 2009<pub-id pub-id-type="pmid">18446413</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Bushberg, J. T., Boone J. M.: The essential physics of medical imaging, Lippincott Williams &#x00026; Wilkins, 2011</mixed-citation></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>X</given-names></name></person-group><article-title>Generalized radiographic view identification with deep learning</article-title><source>Journal of Digital Imaging</source><year>2021</year><volume>34</volume><issue>1</issue><fpage>66</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1007/s10278-020-00408-z</pub-id><pub-id pub-id-type="pmid">33263143</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Fang, X., Harris, L., Zhou, W., and Huo, D., Generalized radiographic view identification with deep learning. Journal of Digital Imaging: 34(1), 66&#x02013;74, 2021<pub-id pub-id-type="pmid">33263143</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Mairh&#x000f6;fer, D., Laufer, M., Simon, P. M., Sieren, M., Bischof, A., K&#x000e4;ster, T., Barth E., Barkhausen J., Martinetz, T.: An AI-based framework for diagnostic quality assessment of ankle radiographs. International Conference on Medical Imaging with Deep Learning, 2021</mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Simonyan, K., Zisserman A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014</mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition:</italic> 770&#x02013;778, 2016</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Kim, T. K., Yi, P. H., Wei, J., Shin, J. W., Hager, G., Hui, F. K., Sair, H. I., Lin, C. T.: Deep learning method for automated classification of anteroposterior and posteroanterior chest radiographs. Journal of digital imaging: 32, 925&#x02013;930, 2019</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Hosch, R., Kroll, L., Nensa, F., Koitka, S.: Differentiation between anteroposterior and posteroanterior chest X-ray view position with convolutional neural networks. In R&#x000f6;Fo-Fortschritte auf dem Gebiet der R&#x000f6;ntgenstrahlen und der bildgebenden Verfahren: 193(2), 168&#x02013;176, 2021</mixed-citation></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Saun</surname><given-names>TJ</given-names></name></person-group><article-title>Automated classification of radiographic positioning of hand X-rays using a deep neural network</article-title><source>Plastic Surgery</source><year>2021</year><volume>29</volume><issue>2</issue><fpage>75</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1177/2292550321997012</pub-id><pub-id pub-id-type="pmid">34026669</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Saun, T.J.:&#x000a0;Automated classification of radiographic positioning of hand X-rays using a deep neural network. Plastic Surgery: 29(2), 75&#x02013;80, 2021<pub-id pub-id-type="pmid">34026669</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Wang, C. Y., Yeh, I. H., Liao, H. Y. M.: Yolov9: Learning what you want to learn using programmable gradient information. arXiv preprint arXiv:2402.13616, 2024</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Medaramatla, S. C., Samhitha, C. V., Pande, S. D., Vinta, S. R.: Detection of Hand Bone Fractures in X-ray Images using Hybrid YOLO NAS. IEEE Accessed 2024</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Zheng, C., Wu, W., Chen, C., Yang, T., Zhu, S., Shen, J., Kehtarnavaz N., Shah, M.: Deep learning-based human pose estimation: A survey. <italic>ACM Computing Surveys:</italic><italic>56</italic>(1), 1&#x02013;37, 2023</mixed-citation></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Kadkhodamohammadi</surname><given-names>A</given-names></name><etal/></person-group><article-title>Articulated clinician detection using 3D pictorial structures on RGB-D data</article-title><source>Medical image analysis</source><year>2017</year><volume>35</volume><fpage>215</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1016/j.media.2016.07.001</pub-id><pub-id pub-id-type="pmid">27449279</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Kadkhodamohammadi, A., Gangi, A., de Mathelin, M., Padoy, N.: Articulated clinician detection using 3D pictorial structures on RGB-D data. Medical image analysis: 35, 215&#x02013;224, 2017<pub-id pub-id-type="pmid">27449279</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastav</surname><given-names>V</given-names></name><name><surname>Gangi</surname><given-names>A</given-names></name><name><surname>Padoy</surname><given-names>N</given-names></name></person-group><article-title>Unsupervised domain adaptation for clinician pose estimation and instance segmentation in the operating room</article-title><source>Medical Image Analysis</source><year>2022</year><volume>80</volume><fpage>102525</fpage><pub-id pub-id-type="doi">10.1016/j.media.2022.102525</pub-id><pub-id pub-id-type="pmid">35809529</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Srivastav V., Gangi A., and Padoy N.: Unsupervised domain adaptation for clinician pose estimation and instance segmentation in the operating room. Medical Image Analysis: 80,&#x000a0;102525, 2022<pub-id pub-id-type="pmid">35809529</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Bigalke</surname><given-names>A</given-names></name></person-group><article-title>Anatomy-guided domain adaptation for 3D in-bed human pose estimation</article-title><source>Medical Image Analysis</source><year>2023</year><volume>89</volume><fpage>102887</fpage><pub-id pub-id-type="doi">10.1016/j.media.2023.102887</pub-id><pub-id pub-id-type="pmid">37453235</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Bigalke, A., Hansen, L., Diesel, J., Hennigs, C., Rostalski, P., Heinrich, M. P.: Anatomy-guided domain adaptation for 3D in-bed human pose estimation. Medical Image Analysis: 89, 102887, 2023<pub-id pub-id-type="pmid">37453235</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Ni</surname><given-names>H</given-names></name></person-group><article-title>Semi-supervised body parsing and pose estimation for enhancing infant general movement assessment</article-title><source>Medical Image Analysis</source><year>2023</year><volume>83</volume><fpage>102654</fpage><pub-id pub-id-type="doi">10.1016/j.media.2022.102654</pub-id><pub-id pub-id-type="pmid">36327657</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Ni, H., Xue, Y., Ma, L., Zhang, Q., Li, X., Huang, S. X.: Semi-supervised body parsing and pose estimation for enhancing infant general movement assessment. Medical Image Analysis: 83, 102654, 2023<pub-id pub-id-type="pmid">36327657</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Ogundokun</surname><given-names>RO</given-names></name><name><surname>Maskeli&#x0016b;nas</surname><given-names>R</given-names></name><name><surname>Dama&#x00161;evi&#x0010d;ius</surname><given-names>R</given-names></name></person-group><article-title>Human posture detection using image augmentation and hyperparameter-optimized transfer learning algorithms</article-title><source>Applied Sciences</source><year>2022</year><volume>12</volume><issue>19</issue><fpage>10156</fpage><pub-id pub-id-type="doi">10.3390/app121910156</pub-id></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Ogundokun, R. O., Maskeli&#x0016b;nas, R., Dama&#x00161;evi&#x0010d;ius, R.: Human posture detection using image augmentation and hyperparameter-optimized transfer learning algorithms. Applied Sciences: 12(19), 10156, 2022</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Liu, Z., Mao, H., Wu, C. Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition: 11976&#x02013;11986, 2022</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Wightman, R.: PyTorch Image Models. Availble at GitHub <ext-link ext-link-type="uri" xlink:href="https://github.com/huggingface/pytorch-image-models">https://github.com/huggingface/pytorch-image-models</ext-link>. Accessed June 2024</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Wright, L.: New deep learning optimizer, ranger: Synergistic combination of radam+ lookahead for the best of both. Availabl at Github <ext-link ext-link-type="uri" xlink:href="https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer">https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer</ext-link>. Accessed Aug 2023</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition: 248&#x02013;255, 2009</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Smith, L. N.: Cyclical learning rates for training neural networks. In 2017 IEEE winter conference on applications of computer vision (WACV): 464&#x02013;472, 2017</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Howard, J., Ruder, S.: Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Paszke, A., Gross, S., Chintala S., Chanan G., Yang E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., Lerer, A.: Automatic differentiation in pytorch. In proceedings of the Conference on Neural Information Processing Systems (NIPS), 2017</mixed-citation></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>J</given-names></name><name><surname>Gugger</surname><given-names>S</given-names></name></person-group><article-title>Fastai: A layered API for deep learning</article-title><source>Information</source><year>2020</year><volume>11</volume><issue>2</issue><fpage>108</fpage><pub-id pub-id-type="doi">10.3390/info11020108</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Howard, J., Gugger S.: Fastai: A layered API for deep learning. Information: 11(2), 108, 2020</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Bradski, G.: The openCV library. Dr. Dobb's Journal: Software Tools for the Professional Programmer: 25(11), 120&#x02013;123, 2000</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Li, Y., Hu, J., Wen, Y., Evangelidis, G., Salahi, K., Wang, Y., Tulyakov, S., Ren, J.: Rethinking vision transformers for mobilenet size and speed. In Proceedings of the IEEE/CVF International Conference on Computer Vision:16889&#x02013;16900, 2023</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Qin, D., Leichner, C., Delakis, M., Fornoni, M., Luo, S., Yang, F., Wang, W., Banbury, C., Ye, C., Akin, B., Aggarwal, V., Zhu, T., Moro, D., Howard, A.: MobileNetV4-Universal Models for the Mobile Ecosystem. arXiv preprint arXiv:2404.10518, 2024</mixed-citation></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Bouwmans</surname><given-names>T</given-names></name><etal/></person-group><article-title>Deep neural network concepts for background subtraction: A systematic review and comparative evaluation</article-title><source>Neural Networks</source><year>2019</year><volume>117</volume><fpage>8</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2019.04.024</pub-id><pub-id pub-id-type="pmid">31129491</pub-id>
</element-citation><mixed-citation id="mc-CR30" publication-type="journal">Bouwmans, T., Javed, S., Sultana, M., Jung, S. K.: Deep neural network concepts for background subtraction: A systematic review and comparative evaluation. Neural Networks: 117, 8&#x02013;66, 2019<pub-id pub-id-type="pmid">31129491</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">England, N., Improvement N.: Diagnostic imaging dataset statistical release, in London. Department of Health, 2023</mixed-citation></ref></ref-list></back></article>