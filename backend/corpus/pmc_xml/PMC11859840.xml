<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006267</article-id><article-id pub-id-type="pmc">PMC11859840</article-id><article-id pub-id-type="doi">10.3390/s25041038</article-id><article-id pub-id-type="publisher-id">sensors-25-01038</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Enhanced Human Activity Recognition Using Wi-Fi Sensing: Leveraging Phase and Amplitude with Attention Mechanisms</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3991-2037</contrib-id><name><surname>Quy</surname><given-names>Thai Duy</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01038" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0401-8473</contrib-id><name><surname>Lin</surname><given-names>Chih-Yang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af2-sensors-25-01038" ref-type="aff">2</xref><xref rid="c1-sensors-25-01038" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Shih</surname><given-names>Timothy K.</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="af1-sensors-25-01038" ref-type="aff">1</xref><xref rid="c1-sensors-25-01038" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Kang</surname><given-names>Kyungtae</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01038"><label>1</label>Department of Computer Science and Information Engineering, National Central University, Taoyuan City 320317, Taiwan; <email>quytd@dlu.edu.vn</email></aff><aff id="af2-sensors-25-01038"><label>2</label>Department of Mechanical Engineering, National Central University, Taoyuan City 320317, Taiwan</aff><author-notes><corresp id="c1-sensors-25-01038"><label>*</label>Correspondence: <email>andrewlin@ncu.edu.tw</email> (C.-Y.L.); <email>tshih@g.ncu.edu.tw</email> (T.K.S.)</corresp></author-notes><pub-date pub-type="epub"><day>09</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1038</elocation-id><history><date date-type="received"><day>30</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>26</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>07</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Wi-Fi-based human activity recognition (HAR) is a non-intrusive and privacy-preserving method that leverages Channel State Information (CSI) for identifying human activities. However, existing approaches often struggle with robust feature extraction, especially in dynamic and multi-environment scenarios, and fail to effectively integrate amplitude and phase features of CSI. This study proposes a novel model, the Phase&#x02013;Amplitude Channel State Information Network (PA-CSI), to address these challenges. The model introduces two key innovations: (1) a dual-feature approach combining amplitude and phase features for enhanced robustness, and (2) an attention-enhanced feature fusion mechanism incorporating multi-scale convolutional layers and Gated Residual Networks (GRN) to optimize feature extraction. Experimental results demonstrate that the proposed model achieves state-of-the-art performance on three datasets, including StanWiFi (99.9%), MultiEnv (98.0%), and the MINE lab dataset (99.9%). These findings underscore the potential of the PA-CSI model to advance Wi-Fi-based HAR in real-world applications. </p></abstract><kwd-group><kwd>Wi-Fi sensing</kwd><kwd>channel state information (CSI)</kwd><kwd>human activity recognition (HAR)</kwd><kwd>phase and amplitude</kwd><kwd>multi-head attention</kwd><kwd>multi-scale convolutional neural networks</kwd><kwd>gate residual network</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01038"><title>1. Introduction</title><p>Human activity recognition (HAR) [<xref rid="B1-sensors-25-01038" ref-type="bibr">1</xref>] is a significant area of research in computer vision and pattern recognition that focuses on identifying and classifying human activities from multimedia environments. HAR can be achieved using various technologies, including camera-based [<xref rid="B2-sensors-25-01038" ref-type="bibr">2</xref>], sensor-based [<xref rid="B1-sensors-25-01038" ref-type="bibr">1</xref>], and radar-based technologies [<xref rid="B3-sensors-25-01038" ref-type="bibr">3</xref>], as well as Wi-Fi [<xref rid="B1-sensors-25-01038" ref-type="bibr">1</xref>,<xref rid="B4-sensors-25-01038" ref-type="bibr">4</xref>]. Camera-based HAR relies on video data to analyze and classify actions. While it offers high-resolution and detailed visual information, it raises privacy concerns and is sensitive to lighting conditions and occlusions. Sensor-based HAR utilizes wearable devices like accelerometers and gyroscopes to capture movement data, providing accuracy and robustness but requiring user compliance, while also potentially being intrusive [<xref rid="B1-sensors-25-01038" ref-type="bibr">1</xref>,<xref rid="B5-sensors-25-01038" ref-type="bibr">5</xref>]. Radar-based HAR uses radio waves to detect motion, offering privacy advantages and working well in low-light conditions, but it can be affected by interference and requires complex signal processing. In contrast, Wi-Fi-based HAR leverages the analysis of CSI to recognize actions by detecting changes in signal propagation caused by human movement. This method is advantageous for its non-intrusiveness, cost-effectiveness, and ability to work through walls and in various lighting conditions. However, Wi-Fi-based HAR may face challenges in environments with high signal noise and requires sophisticated algorithms to interpret the data accurately [<xref rid="B6-sensors-25-01038" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-01038" ref-type="bibr">7</xref>]. Wi-Fi-based HAR stands out due to its balance of privacy, convenience, and practicality in real-world applications.</p><p>In Wi-Fi sensing, two critical metrics are commonly used to represent the characteristics of a Wi-Fi signal: the Received Signal Strength Indicator (RSSI) and Channel State Information (CSI) [<xref rid="B5-sensors-25-01038" ref-type="bibr">5</xref>,<xref rid="B8-sensors-25-01038" ref-type="bibr">8</xref>]. These signals can propagate and synthesize through either line-of-sight (LOS) or non-line-of-sight (NLOS) scenarios (<xref rid="sensors-25-01038-f001" ref-type="fig">Figure 1</xref>). RSSI is a measure of the power level that a receiver detects from a transmitted signal, often used to estimate the distance between the transmitter and receiver. It provides a coarse measure of signal strength but lacks detailed information about the multipath effects and the fine-grained behavior of the signal [<xref rid="B5-sensors-25-01038" ref-type="bibr">5</xref>]. On the other hand, CSI offers a more comprehensive representation by capturing the amplitude and phase of the signal across multiple subcarriers within a Wi-Fi channel. This allows CSI to provide detailed insights into the propagation environment, including reflections, scattering, and the impact of obstacles [<xref rid="B8-sensors-25-01038" ref-type="bibr">8</xref>]. While RSSI is easier to obtain and requires less computational processing, CSI is preferred in advanced Wi-Fi sensing applications, such as human activity recognition and indoor localization, due to its ability to capture rich spatial and temporal information about the signal&#x02019;s interactions with the environment.</p><p>Channel State Information (CSI) comprises multiple extractable features that reveal characteristics of the wireless environment, such as Time of Flight (ToF), amplitude, phase, Direction of Arrival (DoA), Angle of Arrival (AoA), and phase shift [<xref rid="B8-sensors-25-01038" ref-type="bibr">8</xref>]. Each feature provides unique information; for example, ToF enables precise distance estimation, while DoA and AoA are critical for identifying the signal&#x02019;s spatial direction. In human activity recognition (HAR) applications, amplitude and phase are two particularly important features. Amplitude reflects the magnitude of signal variations, whereas phase provides detailed insights into signal propagation, including phase shifts caused by reflections and scattering. However, Wi-Fi sensing research has largely focused on the amplitude component of CSI, often neglecting phase information in the received signal. Additionally, CSI data are represented as a series of time-varying values across subcarriers. Most recognition models, however, concentrate on the temporal dimension and tend to overlook inter-subcarrier relationships, which could capture variations in body posture or actions, such as standing up and sitting down.</p><p>To address these gaps, this study seeks to utilize relevant features in conjunction with attention mechanisms across both temporal and channel dimensions, enhancing the accuracy and robustness of Wi-Fi-based applications for human activity recognition (HAR). In summary, the primary contributions of this paper to Wi-Fi sensing for HAR are as follows:<list list-type="bullet"><list-item><p>We propose an attention-based model that effectively utilizes both phase and amplitude components to improve HAR performance. While prior studies primarily focused on amplitude, our approach incorporates phase data, which provides complementary insights into human activity. By optimizing feature extraction for both amplitude and phase, our method significantly enhances recognition accuracy and robustness across a wide range of activities and environments.</p></list-item><list-item><p>We introduce an attention-based feature fusion mechanism that integrates spatial and temporal features. This includes the implementation of multi-scale convolutional layers, enabling the network to efficiently capture both local and global patterns. In addition, a Gated Residual Network (GRN) is incorporated into our framework as part of the feature fusion process. GRNs can selectively retain or discard information, improving learning efficiency and reducing unnecessary complexity. This adaptation enhances classification accuracy and addresses the limitations of traditional networks. Our focus lies in how the GRN is specifically adapted and utilized in conjunction with attention mechanisms to maximize the utility of multi-scale features.</p></list-item><list-item><p>Our model is rigorously evaluated using three datasets, including two publicly available datasets and a custom dataset collected for this study. The results demonstrate that our approach achieves superior accuracy and performance compared to existing state-of-the-art (SOTA) models.</p></list-item></list></p></sec><sec id="sec2-sensors-25-01038"><title>2. Literature Review</title><p>The Received Signal Strength Indicator (RSSI), a metric used to measure transmission channel power, has been extensively studied and applied across various domains, including human activity recognition (HAR) [<xref rid="B9-sensors-25-01038" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01038" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01038" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01038" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01038" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01038" ref-type="bibr">14</xref>]. Despite its utility, RSSI&#x02019;s efficiency is constrained. In contrast, Channel State Information (CSI) offers a more comprehensive and detailed representation, providing a four-dimensional matrix that captures the characteristics of transmission and reception channels, subcarriers, and temporal packets. Due to its complexity, numerous studies have explored the application of CSI in HAR using advanced deep learning techniques, such as Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and Transformer models.</p><p>As interest in Wi-Fi CSI methods continues to grow, numerous datasets containing CSI data have been developed for diverse applications. These datasets have become foundational for many researchers, serving as the basis for experimental studies documented in a wide range of published literature. <xref rid="sensors-25-01038-t001" ref-type="table">Table 1</xref> provides an overview of commonly used public datasets and their descriptions, facilitating an understanding of their utility and scope.</p><sec id="sec2dot1-sensors-25-01038"><title>2.1. Wi-Fi CSI Based on CNN and LSTM Approaches</title><p>The intricate features of CSI data, including phase and amplitude, have been effectively utilized in CNN models, yielding significant advancements. Wang et al. [<xref rid="B21-sensors-25-01038" ref-type="bibr">21</xref>] introduced a CSI-based human activity recognition and monitoring system (CARM), a CSI-based method for HAR and monitoring. This model integrated two modules: the CSI-speed module, which captured the relationship between CSI dynamics and human motion, and the CSI-activity module, which correlated movement speed with activity. By leveraging the phase component of CSI, the model was tested on their dataset and achieved 96% accuracy in HAR while demonstrating robustness across different environments. Although the method was accurate, cost-effective, and privacy-preserving when using standard Wi-Fi devices, it was sensitive to noise, impacted Wi-Fi performance, and required extensive training.</p><p>Similarly, Alsaify et al. [<xref rid="B15-sensors-25-01038" ref-type="bibr">15</xref>] proposed the MultiEnv dataset, evaluating system performance in office and hall environments under line-of-sight (LOS) and non-line-of-sight (NLOS) conditions. Their robust five-stage preprocessing and feature extraction pipeline achieved 91.27% accuracy on their dataset [<xref rid="B15-sensors-25-01038" ref-type="bibr">15</xref>,<xref rid="B22-sensors-25-01038" ref-type="bibr">22</xref>]. However, environmental factors such as distance and interference, as well as similar movement patterns (e.g., falling versus sitting), led to higher misclassification rates.</p><p>While CNN-based models demonstrated promising results, they lacked the ability to exploit the temporal characteristics of CSI data. Consequently, temporal-based approaches utilizing LSTM networks have gained attention. For example, Chen et al. [<xref rid="B23-sensors-25-01038" ref-type="bibr">23</xref>] developed the ABLSTM model (attention-based bidirectional LSTM), which was designed to extract representative temporal features from sequential CSI data in both directions. The ABLSTM framework achieved state-of-the-art accuracy (&#x02265;95%) in Wi-Fi CSI-based HAR, evaluated on the StanWiFi dataset and their own dataset, by integrating bidirectional LSTM and attention mechanisms. However, this model required significant computational resources, faced challenges in cross-environment scenarios, and did not address multi-user activity recognition.</p><p>Yadav et al. [<xref rid="B24-sensors-25-01038" ref-type="bibr">24</xref>] proposed CSITime, an enhanced version of the InceptionTime network, which achieved state-of-the-art accuracies across three public datasets: 98.2% on ARIL, 98.88% on StanWiFi, and 99.09% on SignFi. CSITime incorporated data augmentation and advanced optimizations within a streamlined InceptionTime-based architecture. However, the model faced challenges in high-interference environments and multi-user scenarios, while demanding high computational resources.</p><p>To address spatial and temporal aspects of CSI data, several studies combined CNN and LSTM architectures. For instance, Moshiri et al. [<xref rid="B25-sensors-25-01038" ref-type="bibr">25</xref>] employed Raspberry Pi devices to extract amplitude CSI signals for seven activities, converting them into 2D images using pseudo-color plots. Among four evaluated models (1D-CNN, 2D-CNN, LSTM, and bidirectional LSTM), the 2D-CNN achieved 95% accuracy on their collected data. Similarly, Salehinejad and Valaee [<xref rid="B26-sensors-25-01038" ref-type="bibr">26</xref>] introduced LiteHAR, which uses randomly initialized convolution kernels for feature extraction, achieving 93% accuracy on the StanWiFi dataset. Shalaby et al. [<xref rid="B27-sensors-25-01038" ref-type="bibr">27</xref>] compared deep learning models on the StanWiFi dataset, with a CNN-GRU model achieving 99.31% accuracy and its attention-based variant reaching 99.16%. Recently, Islam et al. [<xref rid="B28-sensors-25-01038" ref-type="bibr">28</xref>] proposed the STC-NLSTMNet model (spatio-temporal convolution with nested long short-term memory), which integrated spatial&#x02013;temporal convolution and nested LSTMs, achieved 99.88% and 98.20% accuracy on public datasets (StanWiFi and MultiEnv). This model was a robust and efficient method for HAR, achieving SOTA performance by integrating spatial and temporal features. However, the model had limitations in NLOS environments and multi-user settings.</p></sec><sec id="sec2dot2-sensors-25-01038"><title>2.2. Attention-Based Approaches</title><p>Traditional models such as CNNs and LSTMs were limited in capturing interdependencies within the same data dimension. In recent years, Transformer models have gained prominence due to their multi-head attention mechanism [<xref rid="B29-sensors-25-01038" ref-type="bibr">29</xref>]. Researchers have applied these models to Wi-Fi-based HAR systems with notable success.</p><p>Ding et al. (2022) introduced a Channel&#x02013;Time&#x02013;Subcarrier Attention Mechanism (CTS-AM) to enhance location-independent HAR. The model CTS-AM demonstrated innovation in feature extraction, resource efficiency, and flexibility, outperforming existing methods like CNN and Wi-Hand. However, the system was limited to single-actor scenarios, assumed consistent motion during data collection, and recognized only predefined activities. This model achieved more than 90% average accuracy on their dataset across various locations with limited training samples [<xref rid="B30-sensors-25-01038" ref-type="bibr">30</xref>]. Yang et al. (2023) [<xref rid="B31-sensors-25-01038" ref-type="bibr">31</xref>] proposed WiTransformer, which adapted two Transformer architectures&#x02014;the United Spatiotemporal Transformer (UST) and the Separated Spatiotemporal Transformer (SST)&#x02014;to improve recognition accuracy and robustness in complex environments [<xref rid="B31-sensors-25-01038" ref-type="bibr">31</xref>]. The UST integrated spatial and temporal features through early fusion in a single transformer encoder, resulting in high accuracy, computational efficiency, and robustness across tasks of varying complexity. In contrast, the SST processed spatial and temporal features separately using two transformer encoders, offering flexibility, but at the cost of increased complexity and resource demands. The SST struggled with generalization on smaller datasets, exhibited sensitivity to imbalanced data, and lacked the inductive biases needed for effective cross-domain adaptation. The UST&#x02019;s efficient and unified approach made it more practical and robust compared to the SST, particularly for real-world applications. They experimented with two models on the Widar 3.0 dataset and achieved an overall recognition accuracy of 86.16%.</p><p>Some studies also explored two-way relationships within CSI data. Li et al. (2021) [<xref rid="B32-sensors-25-01038" ref-type="bibr">32</xref>] developed the Two-stream Convolution Augmented Human Activity Transformer (THAT) model, which uses a dual convolution-augmented HAR layer to capture channel and temporal structures. This model effectively accommodated variations in activity speed and blank intervals using Gaussian encoding. Yang et al. (2023) [<xref rid="B19-sensors-25-01038" ref-type="bibr">19</xref>] applied Vision Transformers (ViT) to Wi-Fi sensing tasks, leveraging spatial and temporal features to handle complex data relationships. However, ViT models required substantial training data for optimal performance.</p><p>Although a few studies have utilized phase information for HAR, the majority of existing works predominantly focus on amplitude features, leaving phase information underexplored or insufficiently integrated with amplitude. While existing attention-based methods primarily focused on either spatial or temporal dimensions, our proposed PA-CSI model incorporates attention across both. By leveraging multi-head attention mechanisms and multi-scale convolutional neural networks, the PA-CSI model effectively integrates amplitude and phase features to achieve superior performance. Moreover, the use of Gaussian Range Encoding (GRE) ensures robust temporal feature representation, addressing limitations observed in CTS-AM and THAT models.</p></sec></sec><sec id="sec3-sensors-25-01038"><title>3. Materials and Methods</title><sec id="sec3dot1-sensors-25-01038"><title>3.1. Channel State Information (CSI)</title><p>Channel State Information (CSI) refers to the detailed properties of a communication channel in wireless communication systems. CSI encompasses information about the channel&#x02019;s characteristics, including path loss, fading, delay spread, and interference [<xref rid="B33-sensors-25-01038" ref-type="bibr">33</xref>]. CSI is typically obtained through channel estimation techniques, which involve measuring the channel response to known pilot signals transmitted between the sender and receiver. Mathematically, CSI can be represented by the channel matrix <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:math></inline-formula>, which characterizes the effect of the channel on the transmitted signal. If <italic toggle="yes">x</italic> is the transmitted signal and <italic toggle="yes">y</italic> is the received signal, <italic toggle="yes">N</italic> is the total number of subcarriers, and <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow></mml:math></inline-formula> represents the noise in the system, the relationship between <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> for each <italic toggle="yes">i-th</italic> subcarrier can be expressed as follows (1):<disp-formula id="FD1-sensors-25-01038"><label>(1)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The matrix <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">H</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> presents the relations from transmitter (<italic toggle="yes">T</italic>) and receiver (<italic toggle="yes">R</italic>) as (2):<disp-formula id="FD2-sensors-25-01038"><label>(2)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where each <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> contains complex values, represented as shown in (3):<disp-formula id="FD3-sensors-25-01038"><label>(3)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x0211c;</mml:mo><mml:mfenced><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>&#x02111;</mml:mo><mml:mfenced><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msup><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Each <italic toggle="yes">h<sub>rt</sub></italic> value consists of both the real <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x0211c;</mml:mo><mml:mfenced><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and imaginary <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02111;</mml:mo><mml:mfenced><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> parts, which encapsulate the amplitude and phase changes imposed by the channel. The amplitude (<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and phase (<inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are calculated as given in Equations (4) and (5):<disp-formula id="FD4-sensors-25-01038"><label>(4)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mfenced><mml:mrow><mml:mo>&#x0211c;</mml:mo><mml:mfenced><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced><mml:mrow><mml:mo>&#x02111;</mml:mo><mml:mfenced><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD5-sensors-25-01038"><label>(5)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mfenced><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x02111;</mml:mo><mml:mfenced><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#x0211c;</mml:mo><mml:mfenced><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>An example visualization of falling and sitting down activities, represented by the averaged amplitude and phase values of Wi-Fi Channel State Information (CSI), is shown in <xref rid="sensors-25-01038-f002" ref-type="fig">Figure 2</xref>.</p></sec><sec id="sec3dot2-sensors-25-01038"><title>3.2. Datasets</title><p>In our experiment, we selected and evaluated three datasets: MultiEnv (three environments) [<xref rid="B15-sensors-25-01038" ref-type="bibr">15</xref>], StanWiFi [<xref rid="B16-sensors-25-01038" ref-type="bibr">16</xref>], and a dataset collected by our research team. The rationale for selecting these datasets was their inclusion of both CSI phase and amplitude values, which are critical for the model&#x02019;s ability to effectively utilize these inputs.</p><sec id="sec3dot2dot1-sensors-25-01038"><title>3.2.1. StanWiFi</title><p>The StanWiFi dataset was collected by Yousefi et al. (2017) [<xref rid="B16-sensors-25-01038" ref-type="bibr">16</xref>] in a line-of-sight (LOS) environment. The setup included a single Wi-Fi router with one antenna as the transmitter and three antennas as the receiver, installed on a laptop equipped with an Intel<sup>&#x000ae;</sup> 5300 NIC. The transmitter and receiver were three meters apart. Each session, recorded at a sampling rate of 1000 Hz, lasted 20 s. This dataset consists of six activities&#x02014;fall, run, lie down, walk, sit down, and stand up&#x02014;performed by six participants, each repeating them 20 times. In total, the dataset comprises 577 samples.</p></sec><sec id="sec3dot2dot2-sensors-25-01038"><title>3.2.2. Multiple Environment (MultiEnv)</title><p>The MultiEnv dataset was collected by Alsaify et al. (2020) [<xref rid="B15-sensors-25-01038" ref-type="bibr">15</xref>] across three indoor environments: an office and a hall for line-of-sight (LOS) scenarios, and a non-line-of-sight (NLOS) scenario with a wooden barrier separating the transmitter and receiver. The distance between the transmitter and receiver was set at 3.7 m, with an 8-centimeter wall as the barrier. Data were collected using two computers: one serving as a single-antenna transmitter and the other as a three-antenna receiver, both equipped with an Intel<sup>&#x000ae;</sup> 5300 NIC. As listed in <xref rid="sensors-25-01038-t002" ref-type="table">Table 2</xref>, the dataset includes six activity classes, with a total of 3000 samples (30 subjects &#x000d7; 5 experiments &#x000d7; 20 trials).</p></sec><sec id="sec3dot2dot3-sensors-25-01038"><title>3.2.3. Our Research Team Dataset (MINE Lab Dataset)</title><p>This dataset was collected in an office (LOS) within our laboratory (5.5 &#x000d7; 3 m). The setup utilized Intel<sup>&#x000ae;</sup> 5300 NIC devices, each equipped with three antennas for both the transmitter and receiver, placed 4.0 m apart (<xref rid="sensors-25-01038-f003" ref-type="fig">Figure 3</xref>). The experiment involved five participants performing six activities: standing up and squatting, raising and lowering the right hand, opening and closing the arms, kicking the right and the left leg. In total, the dataset comprises 216 collected samples.</p></sec></sec><sec id="sec3dot3-sensors-25-01038"><title>3.3. Methods</title><p><xref rid="sensors-25-01038-f004" ref-type="fig">Figure 4</xref> illustrates a pipeline of the proposed HAR system in this paper. It begins with raw CSI data, which undergoes preprocessing steps including Kalman filtering [<xref rid="B34-sensors-25-01038" ref-type="bibr">34</xref>], sliding windows (optional), time alignment, and feature extraction. After preprocessing, the CSI data are extracted into two components: amplitude and phase. The amplitude data are normalized, while the phase data are unwrapped, and both features are passed through four Multi-scale Convolution Augmented Transformer (MCAT) layers [<xref rid="B33-sensors-25-01038" ref-type="bibr">33</xref>]. Each MCAT layer processes data along two streams: the data are kept in their original format for the temporal stream, while it is transposed to feed into the channel stream. Gaussian Range Encoding is applied to preserve temporal order. The outputs of the MCAT layers are combined using a CNN with max-pooling layers. The combined outputs are then subsequently fed into a Gated Residual Network (GRN) [<xref rid="B35-sensors-25-01038" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-01038" ref-type="bibr">36</xref>], which includes dense layers with exponential linear unit (ELU) activations, layer normalization, and a final sigmoid activation. The GRN integrates the processed amplitude and phase features to classify human actions. This system effectively enables the classification of human activities from CSI Wi-Fi signals as they interact with their environment.</p><sec id="sec3dot3dot1-sensors-25-01038"><title>3.3.1. Preprocessing: Kalman Filter</title><p>The Kalman filter is a recursive algorithm that efficiently estimates the state of a dynamic system from a series of noisy measurements [<xref rid="B34-sensors-25-01038" ref-type="bibr">34</xref>], to minimize <italic toggle="yes">&#x003b7;</italic>, as described in Equation (1). It operates in two main phases: prediction and update. During the prediction phase, the filter uses the system&#x02019;s previous state and a mathematical model to predict the next state, while also estimating the associated uncertainty. In the update phase, it updates the estimate by incorporating new measurements and reducing uncertainty based on the discrepancy between the predicted and observed data. The Kalman filter assumes that the errors in both the system and the measurements are normally distributed, following a Gaussian distribution, making it optimal for linear systems affected by Gaussian noise.</p></sec><sec id="sec3dot3dot2-sensors-25-01038"><title>3.3.2. Preprocessing: Sliding Windows</title><p>In this phase, appropriate preprocessing techniques are applied to the collected CSI data depending on the characteristics of each dataset. For both the StanWiFi dataset and our own dataset, we use a sliding window approach to generate additional samples, which is also essential for real-time action prediction when the exact point at which the action will occur within the signal is unknown (<xref rid="sensors-25-01038-f005" ref-type="fig">Figure 5</xref>). Specifically, each instance is shifted by 150, and a window size of 1000 is used for each sample. As a result, the data augmentation process leads to an expansion of the StanWiFi dataset from 577 records to 3119 records, while our dataset increases from 216 samples to 2111 samples.</p></sec><sec id="sec3dot3dot3-sensors-25-01038"><title>3.3.3. Preprocessing: Time Alignment</title><p>In the datasets, CSI signal sample lengths vary, and the signals exhibit high density with similar characteristics. To address this issue, we propose a time alignment algorithm designed to standardize the sample sizes using collection times and average signal values. This algorithm processes CSI data based on timestamps, normalizing them to a specified maximum length. For example, in the StanWiFi dataset, sample lengths include 19,995, 19,980, 19,990, and so on. By setting the maximum length to 2000, the algorithm reduces all signal lengths to 2000 (<xref rid="sensors-25-01038-f006" ref-type="fig">Figure 6</xref>).</p></sec><sec id="sec3dot3dot4-sensors-25-01038"><title>3.3.4. Preprocessing: Feature Extraction</title><p>After the Wi-Fi signal is extracted using the sliding window method or through the time alignment technique, the Channel State Information (CSI) data are decomposed into amplitude and phase components as described in Equations (4) and (5).</p></sec><sec id="sec3dot3dot5-sensors-25-01038"><title>3.3.5. Normalization</title><p>CSI data collected from various environments or devices often exhibit variations in size. Normalization ensures that all data samples are standardized to a uniform size, to ensure compatibility with models requiring fixed input dimensions. In this context, normalization is applied to the amplitude across the datasets using the widely utilized min&#x02013;max normalization technique for scaling data within a specific range. The formula for min&#x02013;max normalization is shown in Equation (6).<disp-formula id="FD6-sensors-25-01038"><label>(6)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>min</mml:mi><mml:mfenced><mml:mi>x</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mfenced><mml:mi>x</mml:mi></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:mi>min</mml:mi><mml:mfenced><mml:mi>x</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">x</italic> represents the original data point, min(<italic toggle="yes">x</italic>) is the minimum value in the dataset, max(<italic toggle="yes">x</italic>) is the maximum value in the dataset, and <italic toggle="yes">x&#x02032;</italic> is the normalized value of <italic toggle="yes">x</italic>.</p></sec><sec id="sec3dot3dot6-sensors-25-01038"><title>3.3.6. Phase Unwrapping</title><p>The CSI phase provides critical information about wireless signal propagation, including path delays, multipath effects, and Doppler shifts. However, the phase information derived from CSI is often wrapped, meaning it is confined to a specific range, such as [&#x02212;&#x003c0;, &#x003c0;] or [0, 2&#x003c0;]. This wrapping occurs because phase is typically expressed as an angle, and angles exceeding these bounds are wrapped back into the range.</p><p>Phase unwrapping is the process of resolving these phase ambiguities to create a continuous phase representation. This is achieved by identifying and correcting discontinuities introduced by wrapping, ensuring smooth phase progression over time or frequency. The visualization of phase unwrapping for falling action is illustrated in <xref rid="sensors-25-01038-f007" ref-type="fig">Figure 7</xref>, while the formula for phase unwrapping is provided in Equation (7).<disp-formula id="FD7-sensors-25-01038"><label>(7)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>&#x003a6;</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>&#x003a6;</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003a6;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the unwrapped and raw phases at index <italic toggle="yes">i</italic>, respectively; <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003a6;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the previously unwrapped phase and the floor function <inline-formula><mml:math id="mm13333" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x0230a;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>&#x022c5;<inline-formula><mml:math id="mm13334" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x0230b;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes rounding to the nearest integer.</p></sec><sec id="sec3dot3dot7-sensors-25-01038"><title>3.3.7. Gaussian Range Encoding</title><p>Gaussian Range Encoding (GRE) is introduced in [<xref rid="B32-sensors-25-01038" ref-type="bibr">32</xref>,<xref rid="B37-sensors-25-01038" ref-type="bibr">37</xref>] as a mechanism to preserve order information from CSI data, which is crucial for recognizing sequential activities such as &#x0201c;sitting down&#x0201d; or &#x0201c;standing up&#x0201d;. Unlike traditional positional encoding methods, which assign unique encodings to individual time steps, GRE focuses on encoding ranges. This approach preserves temporal continuity and improves robustness against subtle variations in activity speeds or blank intervals within the sequence. The final representation of GRE is obtained by multiplying the <italic toggle="yes">K</italic>, learnable range of vector embeddings, with the probability density function (PDF) vector. The PDF is normalized across <italic toggle="yes">K</italic> Gaussian distributions. <xref rid="sensors-25-01038-f008" ref-type="fig">Figure 8</xref> shows how data are mapped into Gaussian distributions and how these distributions are transformed into learnable embeddings for integration into the neural network.</p></sec><sec id="sec3dot3dot8-sensors-25-01038"><title>3.3.8. Multi-Scale Convolution Augmented Transformer (MCAT) Layer</title><p>The MCAT layer [<xref rid="B32-sensors-25-01038" ref-type="bibr">32</xref>] integrates two key components: a multi-head self-attention mechanism [<xref rid="B29-sensors-25-01038" ref-type="bibr">29</xref>] and a multi-scale convolutional neural network. These components are connected using residual connections and layer normalization [<xref rid="B38-sensors-25-01038" ref-type="bibr">38</xref>], as shown in <xref rid="sensors-25-01038-f009" ref-type="fig">Figure 9</xref>.</p><p>In the multi-head attention stage, the input <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, representing amplitude or phase&#x02014;is transformed into three distinct vectors: query (<italic toggle="yes">Q</italic>), key (<italic toggle="yes">K</italic>), and value (<italic toggle="yes">V</italic>)&#x02014;via three linear projections. Self-attention computes a weighted sum of the input values, where the weights are determined by the dot product between the query and the corresponding key. The process is defined mathematically as follows:<disp-formula id="FD8-sensors-25-01038"><label>(8)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>Q</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>Q</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-01038"><label>(9)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-01038"><label>(10)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-01038"><label>(11)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mfenced><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
here, <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the dimensionality of both the queries and keys, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> is the scaling factor used to stabilize gradients and enhance computational efficiency.</p><p>To jointly attend to information from different representation subspaces, the vectors <italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic>, and <italic toggle="yes">V</italic> are projected <italic toggle="yes">h</italic> times (referred to as <italic toggle="yes">h-heads</italic>) using distinct projection parameters. The outputs from these projections are then concatenated and passed through a final projection to generate the output:<disp-formula id="FD12-sensors-25-01038"><label>(12)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mfenced><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>;</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mi>W</mml:mi><mml:mi>O</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where each <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mfenced><mml:mrow><mml:mi>X</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>X</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>O</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the final projection matrix.</p><p>Subsequently, the multi-scale CNN stage processes the output features <italic toggle="yes">Y</italic> from the multi-head self-attention module by applying learnable weights <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, <italic toggle="yes">W<sup>ij</sup></italic> consists of <italic toggle="yes">d<sub>o</sub></italic> filters, where each filter sized <italic toggle="yes">j<sub>i</sub></italic> &#x000d7; <italic toggle="yes">d<sub>o</sub></italic>. The output of the multi-cale CNN is presented as <italic toggle="yes">P</italic> = {<italic toggle="yes">P</italic><sub>1</sub>,<italic toggle="yes">P</italic><sub>2</sub>,&#x02026;,<italic toggle="yes">P<sub>j</sub></italic>}, where <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the feature matrix corresponding to the kernel size <italic toggle="yes">j<sub>i</sub></italic> at index <italic toggle="yes">i</italic>. <italic toggle="yes">P<sub>i</sub></italic> is computed as follows:<disp-formula id="FD13-sensors-25-01038"><label>(13)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mfenced><mml:mrow><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mfenced><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Equation (12) involves a series of operations, including convolution, batch normalization (BN), dropout, and a rectified linear unit (ReLU) activation function.</p></sec><sec id="sec3dot3dot9-sensors-25-01038"><title>3.3.9. Gated Residual Network (GRN)</title><p>Gated Residual Networks (GRNs) [<xref rid="B35-sensors-25-01038" ref-type="bibr">35</xref>] are a type of neural network architecture designed to improve information flow via residual connections while incorporating a gating mechanism to regulate the flow of features. The gating mechanism, typically implemented using a sigmoid function, selectively controls which parts of the input are passed through, allowing the network to prioritize the most relevant features. This combination of residual connections and gating mechanisms enables a GRN to more effectively capture complex patterns and improve model performance, especially in tasks that involve high-dimensional data or deep architectures. GRNs have been widely applied in various domains, including time series forecasting [<xref rid="B36-sensors-25-01038" ref-type="bibr">36</xref>] and sequence modeling [<xref rid="B39-sensors-25-01038" ref-type="bibr">39</xref>], which require learning long-term dependencies.</p><p>The structure of the GRN in our model is illustrated in <xref rid="sensors-25-01038-f010" ref-type="fig">Figure 10</xref>. This figure demonstrates how amplitude and phase data are processed through dense layers with exponential linear unit (ELU) activation functions, after which their outputs are summed using a residual connection (denoted by the summation symbol &#x0201c;+&#x0201d;). The resulting feature matrix undergoes layer normalization before being passed through another dense layer and a sigmoid gating mechanism. The final output is computed by integrating the gated outputs through an additional summation operation. The exponential linear unit (ELU) activation functions are defined as shown in Equation (14).<disp-formula id="FD14-sensors-25-01038"><label>(14)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced><mml:mi>x</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="" open="{"><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mfenced><mml:mrow><mml:mi>exp</mml:mi><mml:mfenced><mml:mi>x</mml:mi></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec></sec><sec id="sec4-sensors-25-01038"><title>4. Experimental Evaluation</title><p>In this section, we present the experimental results for the three datasets introduced in <xref rid="sec3dot2-sensors-25-01038" ref-type="sec">Section 3.2</xref> (MultiEnv, StanWiFi, and the MINE lab dataset). The evaluation metrics employed include accuracy (Acc), precision (Pre), recall, and F1-score. Additionally, we provide a comparative analysis of our results against the baseline and the state-of-the-art model.</p><sec id="sec4dot1-sensors-25-01038"><title>4.1. Hyperprameters</title><p>In this subsection, we provide a concise summary of the symbols for the variables and hyperparameters utilized in the experiment, and their corresponding values, chosen to optimize performance (<xref rid="sensors-25-01038-t003" ref-type="table">Table 3</xref>).</p><p>The StanWiFi and MultiEnv datasets were divided into training and validation sets using an 80:20 split ratio, with shuffling applied to ensure randomness. This approach aligns with common practices in the literature for utilizing the StanWiFi and MultiEnv datasets, as referenced in previous studies such as [<xref rid="B16-sensors-25-01038" ref-type="bibr">16</xref>,<xref rid="B22-sensors-25-01038" ref-type="bibr">22</xref>,<xref rid="B24-sensors-25-01038" ref-type="bibr">24</xref>,<xref rid="B26-sensors-25-01038" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01038" ref-type="bibr">27</xref>,<xref rid="B31-sensors-25-01038" ref-type="bibr">31</xref>]. For the MINE lab dataset, a 5-fold cross-validation approach was employed to ensure robust evaluation, given its smaller size. The dataset was divided into five equal parts, with each fold serving as the test set once, while the remaining folds were used for training and validation.</p><p>The loss function deployed in this experiment is sparse categorical cross-entropy, which is defined in Equation (15):<disp-formula id="FD15-sensors-25-01038"><label>(15)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mfenced><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>log</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">N</italic> represents the number of samples, <italic toggle="yes">y<sub>i</sub></italic> is the true label, and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the predicted probability for the true class <italic toggle="yes">y<sub>i</sub></italic> for the <italic toggle="yes">i-th</italic> example.</p></sec><sec id="sec4dot2-sensors-25-01038"><title>4.2. Experimental Results on StanWiFi and MINE Lab Datasets</title><p><xref rid="sensors-25-01038-t004" ref-type="table">Table 4</xref> compares models on the StanWiFi dataset based on accuracy, precision, recall, and F1-score. The models are listed chronologically, with the earliest model being THAT (2021) and CSITime (2022), both of which achieved over 98% accuracy but did not report precision, recall, or F1-score. Several other models, such as LiteHAR (2022) and CNN-GRU (2022), report high precision and recall, though not all models provide complete metric details. Notably, our model achieves the highest reported metrics: 99.93% accuracy, 99.86% precision, 99.95% recall, and 99.95% F1-score. This outperforms all other models in the table, demonstrating a significant improvement over previous works in the field.</p><p><xref rid="sensors-25-01038-t005" ref-type="table">Table 5</xref> compares the performance of two models, THAT (2021), by Bing et al., and our model on the MINE lab dataset. The THAT (2021) model achieves consistent values across all metrics, scoring 97% for accuracy, precision, recall, and F1-score. Our model significantly outperforms the THAT model, achieving 99.24% across all evaluated metrics. This demonstrates a notable improvement on the MINE lab dataset, highlighting the superior capability of our model in accuracy, precision, recall, and F1-score. We selected the THAT model for comparison because it is publicly available online.</p><p><xref rid="sensors-25-01038-f011" ref-type="fig">Figure 11</xref> illustrates the model&#x02019;s performance on six activities from the StanWiFi dataset on the left side: fall, run, lie down, walk, sit down, and stand up. The right side visualizes the model&#x02019;s performance on additional activities: stand up and squat down, raise and lower the right hand, open and close arms, kick the right leg, and kick the left leg. The confusion matrices indicate that the model achieved a near-perfect classification for most activities, demonstrating high true positive rates across both datasets.</p></sec><sec id="sec4dot3-sensors-25-01038"><title>4.3. Experimental Results on MultiEnv Dataset</title><p><xref rid="sensors-25-01038-t006" ref-type="table">Table 6</xref> summarizes the performance of various models on the MultiEnv dataset, divided into three different environments: E1: Office (LOS), E2: Hall (LOS), and E3: Room and hall (NLOS). In the E1 environment, various models such as SVM (2021, 2022), THAT (2021), STC-NLSTMNet (2023), and AAE+RF (2023) are compared. Our model achieves the highest accuracy (99.47%) and outperforms others in all metrics, including precision (99.48%), recall (99.47%), and F1-score (99.47%). In the E2 environment, comparisons include models such as SVM (2021, 2022), THAT (2021), and STC-NLSTMNet (2023). Again, our model achieves the best performance with an accuracy of 98.43%, precision of 98.01%, recall of 97.90%, and F1-score of 97.90%. In the E3 environment, our model once again excels with an accuracy of 98.78%, precision, recall, and F1-score all at 98.78%. This is superior to other models, such as THAT (2021), STC-NLSTMNet (2023), and AAE+RF (2023).</p><p><xref rid="sensors-25-01038-f012" ref-type="fig">Figure 12</xref> visualizes the model&#x02019;s performance across six activities in the MultiEnv dataset running on E1, E2, and E3. These matrices show that the model achieved high accuracy across all activities.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01038"><title>5. Conclusions</title><p>In this study, we presented a novel approach for human activity recognition (HAR) using Wi-Fi sensing, leveraging both the phase and amplitude components of Channel State Information (CSI). Our proposed model, which combines attention-based mechanisms with a Gated Recurrent Network (GRN) architecture, outperformed state-of-the-art models in HAR. The model&#x02019;s efficacy was rigorously tested across multiple datasets, including StanWiFi, MultiEnv, and the MINE lab dataset, showcasing its adaptability and robustness in both line-of-sight (LOS) and non-line-of-sight (NLOS) environments. The source code is available at <uri xlink:href="https://github.com/thai-duy-quy/PA-CSI-HAR">https://github.com/thai-duy-quy/PA-CSI-HAR</uri> (access on 26 January 2025).</p><p>By utilizing both phase and amplitude signals, we addressed the limitations of prior models that predominantly focused on a single feature, leading to improved accuracy in recognizing complex human activities. Our experimental results indicate that the multi-feature approach of combining amplitude and phase information significantly enhances accuracy, precision, recall, and F1-score metrics compared to existing methods.</p><p>Despite these advancements, our method has certain limitations. Experiments conducted in controlled laboratory settings differ from real-world environments, where Wi-Fi signals are influenced by factors such as stationary and moving objects. Moreover, our current study does not include cross-environment testing, even though real-world Wi-Fi signals are significantly affected by the surrounding environment. In the future, we aim to address these challenges by developing solutions to improve the robustness and adaptability of our method for diverse and dynamic conditions.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, T.K.S. and C.-Y.L.; methodology, C.-Y.L. and T.D.Q.; software, T.D.Q. and C.-Y.L.; validation, T.D.Q.; resources, T.K.S.; data curation, T.D.Q.; writing&#x02014;original draft preparation, T.D.Q.; writing&#x02014;review and editing, C.-Y.L. and T.D.Q. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The StanWiFi dataset is available at <uri xlink:href="https://github.com/ermongroup/Wifi_Activity_Recognition">https://github.com/ermongroup/Wifi_Activity_Recognition</uri> (accessed on 6 February 2025). The MultiEnv dataset is available at <uri xlink:href="https://data.mendeley.com/datasets/v38wjmz6f6/1">https://data.mendeley.com/datasets/v38wjmz6f6/1</uri> (accessed on 6 February 2025). The MINE lab dataset is provided upon request by contacting the corresponding author due to author privacy concerns.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01038"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kaseris</surname><given-names>M.</given-names></name>
<name><surname>Kostavelis</surname><given-names>I.</given-names></name>
<name><surname>Malassiotis</surname><given-names>S.</given-names></name>
</person-group><article-title>A Comprehensive Survey on Deep Learning Methods in Human Activity Recognition</article-title><source>Mach. Learn. Knowl. Extr.</source><year>2024</year><volume>6</volume><fpage>842</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.3390/make6020040</pub-id></element-citation></ref><ref id="B2-sensors-25-01038"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aggarwal</surname><given-names>J.K.</given-names></name>
<name><surname>Ryoo</surname><given-names>M.S.</given-names></name>
</person-group><article-title>Human Activity Analysis: A Review</article-title><source>ACM Comput. Surv.</source><year>2011</year><volume>43</volume><fpage>1</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1145/1922649.1922653</pub-id></element-citation></ref><ref id="B3-sensors-25-01038"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
<name><surname>Jing</surname><given-names>X.</given-names></name>
</person-group><article-title>A Survey of Deep Learning-Based Human Activity Recognition in Radar</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><elocation-id>1068</elocation-id><pub-id pub-id-type="doi">10.3390/rs11091068</pub-id></element-citation></ref><ref id="B4-sensors-25-01038"><label>4.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Zhuravchak</surname><given-names>A.</given-names></name>
<name><surname>Kapshii</surname><given-names>O.</given-names></name>
<name><surname>Pournaras</surname><given-names>E.</given-names></name>
</person-group><article-title>Human Activity Recognition Based on Wi-Fi CSI Data -A Deep Neural Network Approach</article-title><source>Proceedings of the Procedia Computer Science</source><publisher-name>Elsevier B.V.</publisher-name><publisher-loc>Amsterdam, The Netherlands</publisher-loc><year>2021</year><volume>Volume 198</volume><fpage>59</fpage><lpage>66</lpage></element-citation></ref><ref id="B5-sensors-25-01038"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jannat</surname><given-names>M.K.A.</given-names></name>
<name><surname>Islam</surname><given-names>M.S.</given-names></name>
<name><surname>Yang</surname><given-names>S.H.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
</person-group><article-title>Efficient Wi-Fi-Based Human Activity Recognition Using Adaptive Antenna Elimination</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>105440</fpage><lpage>105454</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3320069</pub-id></element-citation></ref><ref id="B6-sensors-25-01038"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>C.</given-names></name>
<name><surname>Zhou</surname><given-names>G.</given-names></name>
<name><surname>Lin</surname><given-names>Y.</given-names></name>
</person-group><article-title>Cross-Domain WiFi Sensing with Channel State Information: A Survey</article-title><source>ACM Comput. Surv.</source><year>2023</year><volume>55</volume><fpage>231</fpage><pub-id pub-id-type="doi">10.1145/3570325</pub-id></element-citation></ref><ref id="B7-sensors-25-01038"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>C.-Y.</given-names></name>
<name><surname>Lin</surname><given-names>C.-Y.</given-names></name>
<name><surname>Liu</surname><given-names>Y.-T.</given-names></name>
<name><surname>Chen</surname><given-names>Y.-W.</given-names></name>
<name><surname>Shih</surname><given-names>T.K.</given-names></name>
</person-group><article-title>WiFi-TCN: Temporal Convolution for Human Interaction Recognition Based on WiFi Signal</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>126970</fpage><lpage>126982</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3428550</pub-id></element-citation></ref><ref id="B8-sensors-25-01038"><label>8.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>Z.</given-names></name>
<name><surname>Qian</surname><given-names>K.</given-names></name>
<name><surname>Wu</surname><given-names>C.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
</person-group><source>Smart Wireless Sensing: From IoT to AIoT</source><publisher-name>Springer Nature</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2021</year><isbn>9789811656583</isbn></element-citation></ref><ref id="B9-sensors-25-01038"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sigg</surname><given-names>S.</given-names></name>
<name><surname>Blanke</surname><given-names>U.</given-names></name>
<name><surname>Troster</surname><given-names>G.</given-names></name>
</person-group><article-title>The Telepathic Phone: Frictionless Activity Recognition from WiFi-RSSI</article-title><source>Proceedings of the IEEE International Conference on Pervasive Computing and Communications (PerCom)</source><conf-loc>Budapest, Hungary</conf-loc><conf-date>24&#x02013;28 March 2014</conf-date></element-citation></ref><ref id="B10-sensors-25-01038"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gu</surname><given-names>Y.</given-names></name>
<name><surname>Ren</surname><given-names>F.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>PAWS: Passive Human Activity Recognition Based on WiFi Ambient Signals</article-title><source>IEEE Internet Things J.</source><year>2016</year><volume>3</volume><fpage>796</fpage><lpage>805</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2015.2511805</pub-id></element-citation></ref><ref id="B11-sensors-25-01038"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gu</surname><given-names>Y.</given-names></name>
<name><surname>Quan</surname><given-names>L.</given-names></name>
<name><surname>Ren</surname><given-names>F.</given-names></name>
</person-group><article-title>WiFi-Assisted Human Activity Recognition</article-title><source>Proceedings of the Wireless and Mobile, 2014 IEEE Asia Pacific Conference</source><conf-loc>Bali, Indonesia</conf-loc><conf-date>28&#x02013;30 August 2014</conf-date><fpage>60</fpage><lpage>65</lpage></element-citation></ref><ref id="B12-sensors-25-01038"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sigg</surname><given-names>S.</given-names></name>
<name><surname>Shi</surname><given-names>S.</given-names></name>
<name><surname>Buesching</surname><given-names>F.</given-names></name>
<name><surname>Ji</surname><given-names>Y.</given-names></name>
<name><surname>Wolf</surname><given-names>L.</given-names></name>
</person-group><article-title>Leveraging RF-Channel Fluctuation for Activity Recognition: Active and Passive Systems, Continuous and RSSI-Based Signal Features</article-title><source>Proceedings of the MoMM 2013: The 11th International Conference on Advances in Mobile Computing and Multimedia</source><conf-loc>Vienna, Austria</conf-loc><conf-date>2&#x02013;4 December 2013</conf-date><publisher-name>Association for Computing Machinery</publisher-name><publisher-loc>New York, NY, USA, December</publisher-loc><year>2013</year><fpage>599</fpage></element-citation></ref><ref id="B13-sensors-25-01038"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Youssef</surname><given-names>M.</given-names></name>
<name><surname>Mah</surname><given-names>M.</given-names></name>
<name><surname>Agrawala</surname><given-names>A.</given-names></name>
</person-group><article-title>Challenges: Device-Free Passive Localization for Wireless Environments</article-title><source>Proceedings of the 13th Annual ACM International Conference on Mobile Computing and Networking</source><conf-loc>Montr&#x000e9;al, QC, Canada</conf-loc><conf-date>9&#x02013;14 September 2007</conf-date><publisher-name>ACM Digital Library</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2007</year><fpage>222</fpage><lpage>229</lpage></element-citation></ref><ref id="B14-sensors-25-01038"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rathnayake</surname><given-names>R.M.M.R.</given-names></name>
<name><surname>Maduranga</surname><given-names>M.W.P.</given-names></name>
<name><surname>Tilwari</surname><given-names>V.</given-names></name>
<name><surname>Dissanayake</surname><given-names>M.B.</given-names></name>
</person-group><article-title>RSSI and Machine Learning-Based Indoor Localization Systems for Smart Cities</article-title><source>Eng</source><year>2023</year><volume>4</volume><fpage>1468</fpage><lpage>1494</lpage><pub-id pub-id-type="doi">10.3390/eng4020085</pub-id></element-citation></ref><ref id="B15-sensors-25-01038"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alsaify</surname><given-names>B.A.</given-names></name>
<name><surname>Almazari</surname><given-names>M.M.</given-names></name>
<name><surname>Alazrai</surname><given-names>R.</given-names></name>
<name><surname>Daoud</surname><given-names>M.I.</given-names></name>
</person-group><article-title>A Dataset for Wi-Fi-Based Human Activity Recognition in Line-of-Sight and Non-Line-of-Sight Indoor Environments</article-title><source>Data Brief</source><year>2020</year><volume>33</volume><fpage>106534</fpage><pub-id pub-id-type="doi">10.1016/j.dib.2020.106534</pub-id><pub-id pub-id-type="pmid">33299909</pub-id>
</element-citation></ref><ref id="B16-sensors-25-01038"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yousefi</surname><given-names>S.</given-names></name>
<name><surname>Narui</surname><given-names>H.</given-names></name>
<name><surname>Dayal</surname><given-names>S.</given-names></name>
<name><surname>Ermon</surname><given-names>S.</given-names></name>
<name><surname>Valaee</surname><given-names>S.</given-names></name>
</person-group><article-title>A Survey on Behavior Recognition Using WiFi Channel State Information</article-title><source>IEEE Commun. Mag.</source><year>2017</year><volume>55</volume><fpage>98</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1109/MCOM.2017.1700082</pub-id></element-citation></ref><ref id="B17-sensors-25-01038"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Zheng</surname><given-names>Y.</given-names></name>
<name><surname>Qian</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>G.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>C.</given-names></name>
<name><surname>Yang</surname><given-names>Z.</given-names></name>
</person-group><article-title>Widar3.0: Zero-Effort Cross-Domain Gesture Recognition With Wi-Fi</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>44</volume><fpage>8671</fpage><lpage>8688</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2021.3105387</pub-id><pub-id pub-id-type="pmid">34406937</pub-id>
</element-citation></ref><ref id="B18-sensors-25-01038"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>L.</given-names></name>
<name><surname>Guo</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Lin</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Lu</surname><given-names>B.</given-names></name>
<name><surname>Fang</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Shan</surname><given-names>Z.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
</person-group><article-title>Wiar: A Public Dataset for Wifi-Based Activity Recognition</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>154935</fpage><lpage>154945</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2947024</pub-id></element-citation></ref><ref id="B19-sensors-25-01038"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Zou</surname><given-names>H.</given-names></name>
<name><surname>Lu</surname><given-names>C.X.</given-names></name>
<name><surname>Wang</surname><given-names>D.</given-names></name>
<name><surname>Sun</surname><given-names>S.</given-names></name>
<name><surname>Xie</surname><given-names>L.</given-names></name>
</person-group><article-title>SenseFi: A Library and Benchmark on Deep-Learning-Empowered WiFi Human Sensing</article-title><source>Patterns</source><year>2023</year><volume>4</volume><fpage>100703</fpage><pub-id pub-id-type="doi">10.1016/j.patter.2023.100703</pub-id><pub-id pub-id-type="pmid">36960448</pub-id>
</element-citation></ref><ref id="B20-sensors-25-01038"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Meneghello</surname><given-names>F.</given-names></name>
<name><surname>Fabbro</surname><given-names>N.D.</given-names></name>
<name><surname>Garlisi</surname><given-names>D.</given-names></name>
<name><surname>Tinnirello</surname><given-names>I.</given-names></name>
<name><surname>Rossi</surname><given-names>M.</given-names></name>
</person-group><article-title>A CSI Dataset for Wireless Human Sensing on 80 MHz Wi-Fi Channels</article-title><source>IEEE Commun. Mag.</source><year>2023</year><volume>61</volume><fpage>146</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1109/MCOM.005.2200720</pub-id></element-citation></ref><ref id="B21-sensors-25-01038"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Liu</surname><given-names>A.X.</given-names></name>
<name><surname>Shahzad</surname><given-names>M.</given-names></name>
<name><surname>Ling</surname><given-names>K.</given-names></name>
<name><surname>Lu</surname><given-names>S.</given-names></name>
</person-group><article-title>Device-Free Human Activity Recognition Using Commercial WiFi Devices</article-title><source>IEEE J. Sel. Areas Commun.</source><year>2017</year><volume>35</volume><fpage>1118</fpage><lpage>1131</lpage><pub-id pub-id-type="doi">10.1109/JSAC.2017.2679658</pub-id></element-citation></ref><ref id="B22-sensors-25-01038"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alsaify</surname><given-names>B.A.</given-names></name>
<name><surname>Almazari</surname><given-names>M.</given-names></name>
<name><surname>Alazrai</surname><given-names>R.</given-names></name>
<name><surname>Alouneh</surname><given-names>S.</given-names></name>
<name><surname>Daoud</surname><given-names>M.I.</given-names></name>
</person-group><article-title>A CSI-Based Multi-Environment Human Activity Recognition Framework</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>930</elocation-id><pub-id pub-id-type="doi">10.3390/app12020930</pub-id></element-citation></ref><ref id="B23-sensors-25-01038"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Jiang</surname><given-names>C.</given-names></name>
<name><surname>Cao</surname><given-names>Z.</given-names></name>
<name><surname>Cui</surname><given-names>W.</given-names></name>
</person-group><article-title>WiFi CSI Based Passive Human Activity Recognition Using Attention Based BLSTM</article-title><source>IEEE Trans. Mob. Comput.</source><year>2019</year><volume>18</volume><fpage>2714</fpage><lpage>2724</lpage><pub-id pub-id-type="doi">10.1109/TMC.2018.2878233</pub-id></element-citation></ref><ref id="B24-sensors-25-01038"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yadav</surname><given-names>S.K.</given-names></name>
<name><surname>Sai</surname><given-names>S.</given-names></name>
<name><surname>Gundewar</surname><given-names>A.</given-names></name>
<name><surname>Rathore</surname><given-names>H.</given-names></name>
<name><surname>Tiwari</surname><given-names>K.</given-names></name>
<name><surname>Pandey</surname><given-names>H.M.</given-names></name>
<name><surname>Mathur</surname><given-names>M.</given-names></name>
</person-group><article-title>CSITime: Privacy-Preserving Human Activity Recognition Using WiFi Channel State Information</article-title><source>Neural Netw.</source><year>2022</year><volume>146</volume><fpage>11</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2021.11.011</pub-id><pub-id pub-id-type="pmid">34839089</pub-id>
</element-citation></ref><ref id="B25-sensors-25-01038"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Moshiri</surname><given-names>F.P.</given-names></name>
<name><surname>Shahbazian</surname><given-names>R.</given-names></name>
<name><surname>Nabati</surname><given-names>M.</given-names></name>
<name><surname>Ghorashi</surname><given-names>S.A.</given-names></name>
</person-group><article-title>A CSI-Based Human Activity Recognition Using Deep Learning</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>7225</elocation-id><pub-id pub-id-type="doi">10.3390/s21217225</pub-id><pub-id pub-id-type="pmid">34770532</pub-id>
</element-citation></ref><ref id="B26-sensors-25-01038"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Salehinejad</surname><given-names>H.</given-names></name>
<name><surname>Valaee</surname><given-names>S.</given-names></name>
</person-group><article-title>LiteHAR: Lightweight Human Activity Recognition from WiFi Signals with Random Convolution Kernels</article-title><source>Proceedings of the ICASSP 2022&#x02014;2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Singapore</conf-loc><conf-date>23&#x02013;27 May 2022</conf-date><pub-id pub-id-type="doi">10.1109/ICASSP43922.2022.9746803</pub-id></element-citation></ref><ref id="B27-sensors-25-01038"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shalaby</surname><given-names>E.</given-names></name>
<name><surname>ElShennawy</surname><given-names>N.</given-names></name>
<name><surname>Sarhan</surname><given-names>A.</given-names></name>
</person-group><article-title>Utilizing Deep Learning Models in CSI-Based Human Activity Recognition</article-title><source>Neural Comput. Appl.</source><year>2022</year><volume>34</volume><fpage>5993</fpage><lpage>6010</lpage><pub-id pub-id-type="doi">10.1007/s00521-021-06787-w</pub-id><pub-id pub-id-type="pmid">35017796</pub-id>
</element-citation></ref><ref id="B28-sensors-25-01038"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Islam</surname><given-names>M.S.</given-names></name>
<name><surname>Jannat</surname><given-names>M.K.A.</given-names></name>
<name><surname>Hossain</surname><given-names>M.N.</given-names></name>
<name><surname>Kim</surname><given-names>W.S.</given-names></name>
<name><surname>Lee</surname><given-names>S.W.</given-names></name>
<name><surname>Yang</surname><given-names>S.H.</given-names></name>
</person-group><article-title>STC-NLSTMNet: An Improved Human Activity Recognition Method Using Convolutional Neural Network with NLSTM from WiFi CSI</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>356</elocation-id><pub-id pub-id-type="doi">10.3390/s23010356</pub-id></element-citation></ref><ref id="B29-sensors-25-01038"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vaswani</surname><given-names>A.</given-names></name>
<name><surname>Shazeer</surname><given-names>N.</given-names></name>
<name><surname>Parmar</surname><given-names>N.</given-names></name>
<name><surname>Uszkoreit</surname><given-names>J.</given-names></name>
<name><surname>Jones</surname><given-names>L.</given-names></name>
<name><surname>Gomez</surname><given-names>A.N.</given-names></name>
<name><surname>Kaiser</surname><given-names>L.</given-names></name>
<name><surname>Polosukhin</surname><given-names>I.</given-names></name>
</person-group><article-title>Attention Is All You Need</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1706.03762</pub-id></element-citation></ref><ref id="B30-sensors-25-01038"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ding</surname><given-names>X.</given-names></name>
<name><surname>Jiang</surname><given-names>T.</given-names></name>
<name><surname>Zhong</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>S.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Zeng</surname><given-names>J.</given-names></name>
</person-group><article-title>Wi-Fi-Based Location-Independent Human Activity Recognition with Attention Mechanism Enhanced Method</article-title><source>Electronics</source><year>2022</year><volume>11</volume><elocation-id>642</elocation-id><pub-id pub-id-type="doi">10.3390/electronics11040642</pub-id></element-citation></ref><ref id="B31-sensors-25-01038"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>M.</given-names></name>
<name><surname>Zhu</surname><given-names>H.</given-names></name>
<name><surname>Zhu</surname><given-names>R.</given-names></name>
<name><surname>Wu</surname><given-names>F.</given-names></name>
<name><surname>Yin</surname><given-names>L.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
</person-group><article-title>WiTransformer: A Novel Robust Gesture Recognition Sensing Model with WiFi</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>2612</elocation-id><pub-id pub-id-type="doi">10.3390/s23052612</pub-id><pub-id pub-id-type="pmid">36904814</pub-id>
</element-citation></ref><ref id="B32-sensors-25-01038"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>B.</given-names></name>
<name><surname>Cui</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Wu</surname><given-names>M.</given-names></name>
</person-group><article-title>Two-Stream Convolution Augmented Transformer for Human Activity Recognition</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2021</year><volume>35</volume><fpage>286</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1609/aaai.v35i1.16103</pub-id></element-citation></ref><ref id="B33-sensors-25-01038"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zeng</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>D.</given-names></name>
<name><surname>Gao</surname><given-names>R.</given-names></name>
<name><surname>Gu</surname><given-names>T.</given-names></name>
<name><surname>Zhang</surname><given-names>D.</given-names></name>
</person-group><article-title>FullBreathe</article-title><source>Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</source><year>2018</year><volume>2</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1145/3264958</pub-id></element-citation></ref><ref id="B34-sensors-25-01038"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Brauer</surname><given-names>J.</given-names></name>
<name><surname>Sezgin</surname><given-names>A.</given-names></name>
<name><surname>Zenger</surname><given-names>C.</given-names></name>
</person-group><article-title>Kalman Filter Based MIMO CSI Phase Recovery for COTS WiFi Devices</article-title><source>Proceedings of the ICASSP 2021&#x02014;2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Toronto, ON, Canad</conf-loc><conf-date>6&#x02013;11 June 2021</conf-date><publisher-name>Institute of Electrical and Electronics Engineers Inc.</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2021</year><fpage>4820</fpage><lpage>4824</lpage></element-citation></ref><ref id="B35-sensors-25-01038"><label>35.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Savarese</surname><given-names>P.</given-names></name>
<name><surname>Figueiredo</surname><given-names>D.</given-names></name>
</person-group><article-title>Residual Gates: A Simple Mechanism for Improved Network Optimization</article-title><year>2017</year><comment>Available online: <ext-link xlink:href="https://home.ttic.edu/~savarese/savarese_files/Residual_Gates.pdf" ext-link-type="uri">https://home.ttic.edu/~savarese/savarese_files/Residual_Gates.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-02-06">(accessed on 6 February 2025)</date-in-citation></element-citation></ref><ref id="B36-sensors-25-01038"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lim</surname><given-names>B.</given-names></name>
<name><surname>Arik</surname><given-names>S.O.</given-names></name>
<name><surname>Loeff</surname><given-names>N.</given-names></name>
<name><surname>Pfister</surname><given-names>T.</given-names></name>
</person-group><article-title>Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting</article-title><source>Int. J. Forecast.</source><year>2019</year><volume>37</volume><fpage>1748</fpage><lpage>1764</lpage><pub-id pub-id-type="doi">10.1016/j.ijforecast.2021.03.012</pub-id></element-citation></ref><ref id="B37-sensors-25-01038"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Stragapede</surname><given-names>G.</given-names></name>
<name><surname>Delgado-Santos</surname><given-names>P.</given-names></name>
<name><surname>Tolosana</surname><given-names>R.</given-names></name>
<name><surname>Vera-Rodriguez</surname><given-names>R.</given-names></name>
<name><surname>Guest</surname><given-names>R.</given-names></name>
<name><surname>Morales</surname><given-names>A.</given-names></name>
</person-group><article-title>Mobile Keystroke Biometrics Using Transformers</article-title><source>Proceedings of the 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)</source><conf-loc>Waikoloa Beach, HI, USA</conf-loc><conf-date>5&#x02013;8 January 2022</conf-date></element-citation></ref><ref id="B38-sensors-25-01038"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ba</surname><given-names>J.L.</given-names></name>
<name><surname>Kiros</surname><given-names>J.R.</given-names></name>
<name><surname>Hinton</surname><given-names>G.E.</given-names></name>
</person-group><article-title>Layer Normalization</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="arxiv">1607.06450</pub-id></element-citation></ref><ref id="B39-sensors-25-01038"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qin</surname><given-names>Z.</given-names></name>
<name><surname>Yang</surname><given-names>S.</given-names></name>
<name><surname>Zhong</surname><given-names>Y.</given-names></name>
</person-group><article-title>Hierarchically Gated Recurrent Neural Network for Sequence Modeling</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2311.04823</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01038-f001"><label>Figure 1</label><caption><p>The propagation of Wi-Fi signals from the transmitter to the receiver in LOS/NLOS scenarios.</p></caption><graphic xlink:href="sensors-25-01038-g001" position="float"/></fig><fig position="float" id="sensors-25-01038-f002"><label>Figure 2</label><caption><p>Visualization of Wi-Fi CSI amplitude (above) and phase (below). (<bold>a</bold>) Signal description of the falling action across 3 antennas; (<bold>b</bold>) Signal description of the sitting down action across 3 antennas.</p></caption><graphic xlink:href="sensors-25-01038-g002a" position="float"/><graphic xlink:href="sensors-25-01038-g002b" position="float"/></fig><fig position="float" id="sensors-25-01038-f003"><label>Figure 3</label><caption><p>MINE lab environment.</p></caption><graphic xlink:href="sensors-25-01038-g003" position="float"/></fig><fig position="float" id="sensors-25-01038-f004"><label>Figure 4</label><caption><p>The architecture of the proposed HAR system (CA-CSI).</p></caption><graphic xlink:href="sensors-25-01038-g004" position="float"/></fig><fig position="float" id="sensors-25-01038-f005"><label>Figure 5</label><caption><p>Sliding window applied to the StanWiFi dataset and our dataset.</p></caption><graphic xlink:href="sensors-25-01038-g005" position="float"/></fig><fig position="float" id="sensors-25-01038-f006"><label>Figure 6</label><caption><p>An illustrate of time alignment on three samples in the StanWiFi dataset.</p></caption><graphic xlink:href="sensors-25-01038-g006" position="float"/></fig><fig position="float" id="sensors-25-01038-f007"><label>Figure 7</label><caption><p>An example of phase unwrapping in CSI using for falling action.</p></caption><graphic xlink:href="sensors-25-01038-g007" position="float"/></fig><fig position="float" id="sensors-25-01038-f008"><label>Figure 8</label><caption><p>An example of GRE with K univariate Gaussian distributions.</p></caption><graphic xlink:href="sensors-25-01038-g008" position="float"/></fig><fig position="float" id="sensors-25-01038-f009"><label>Figure 9</label><caption><p>The structure of MCAT layer.</p></caption><graphic xlink:href="sensors-25-01038-g009" position="float"/></fig><fig position="float" id="sensors-25-01038-f010"><label>Figure 10</label><caption><p>The structure of the GRN implemented in our model.</p></caption><graphic xlink:href="sensors-25-01038-g010" position="float"/></fig><fig position="float" id="sensors-25-01038-f011"><label>Figure 11</label><caption><p>Model&#x02019;s accuracy performance on StanWiFi datasets (<bold>left</bold>) and MINE lab dataset (<bold>right</bold>).</p></caption><graphic xlink:href="sensors-25-01038-g011" position="float"/></fig><fig position="float" id="sensors-25-01038-f012"><label>Figure 12</label><caption><p>Confusion matrices for the MultiEnv datasets (E1: <bold>left</bold>; E2: <bold>middle</bold>; E3: <bold>right</bold>).</p></caption><graphic xlink:href="sensors-25-01038-g012" position="float"/></fig><table-wrap position="float" id="sensors-25-01038-t001"><object-id pub-id-type="pii">sensors-25-01038-t001_Table 1</object-id><label>Table 1</label><caption><p>Summary of public datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Source</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Alsaify et al. (2020) [<xref rid="B15-sensors-25-01038" ref-type="bibr">15</xref>] </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MultiEnv</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">This dataset was collected in three scenarios: line-of-sight (LOS) in both the office and hall, and non-line-of-sight (NLOS).</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yousefi et al. (2017) [<xref rid="B16-sensors-25-01038" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">StanWiFi</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">This dataset contains continuous CSI data for six activities without precise segmentation timestamps for each sample.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yang et al. (2022) [<xref rid="B17-sensors-25-01038" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Widar 3.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">This large dataset, collected using Intel 5300 NIC with 30 subcarriers and containing 258 K Wi-Fi-based hand gesture instances spanning 8620 min across 75 domains.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Guo et al. (2019) [<xref rid="B18-sensors-25-01038" ref-type="bibr">18</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WiAR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">This dataset includes 16 activities, comprising coarse-grained activities and gestures, performed 30 times each, by ten volunteers.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yang et al. (2023) [<xref rid="B19-sensors-25-01038" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NTU-Fi</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Collected using the Atheros CSI tool, this dataset features 114 subcarriers per antenna pair, and it includes 6 human activities and 14 gait patterns.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Meneghello et al. (2023) [<xref rid="B20-sensors-25-01038" ref-type="bibr">20</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WiFi-80 MHz</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Collected using two Netgear X4S AC2600 IEEE 802.11ac routers with 256 subcarriers (242 usable), this dataset features ten subjects and three applications</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01038-t002"><object-id pub-id-type="pii">sensors-25-01038-t002_Table 2</object-id><label>Table 2</label><caption><p>Activities and class descriptions in the MultiEnv dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Class</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Activity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No movement</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sitting, standing, or lying on the ground</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Falling</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Falling from a standing position or from a chair</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sitting down or standing up</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sitting down on a chair or standing up from a chair</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Walking</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Walking between the transmitter and receiver</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Turning</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Turning at the transmitter&#x02019;s or receiver&#x02019;s location </td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Picking up</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Picking up an object such as a pen from the ground</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01038-t003"><object-id pub-id-type="pii">sensors-25-01038-t003_Table 3</object-id><label>Table 3</label><caption><p>Hyperparameters and values chosen.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hyperparameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Values</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Window size</td><td align="center" valign="middle" rowspan="1" colspan="1">StanWiFi dataset: 2000; our own dataset: 1000</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Stride size</td><td align="center" valign="middle" rowspan="1" colspan="1">StanWiFi dataset: 200; our own dataset: 100</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">K-Gaussian encoding</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Input data dimensions <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" rowspan="1" colspan="1">StanWiFi dataset: (2000, 90); MultiEnv dataset: (850, 90); MINE lab dataset: (1000, 90)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Filter size in Multi-scale CNN (horizontal, vertical)</td><td align="center" valign="middle" rowspan="1" colspan="1">Horizontal: {10, 40}; Vertical: {2, 4} </td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of heads in the multi-head self-attention mechanism</td><td align="center" valign="middle" rowspan="1" colspan="1">h-head: 9; v-head: 50</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dropout rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of dense layers in GRN</td><td align="center" valign="middle" rowspan="1" colspan="1">256</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="center" valign="middle" rowspan="1" colspan="1">Adam (learning rate = 0.001; decay rate = 0.9)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Batch size</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Epochs </td><td align="center" valign="middle" rowspan="1" colspan="1">200</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Training environment </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NVIDIA GeForce RTX 3060 with CUDA v. 12.4, Python 3.11, TensorFlow 2.16</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01038-t004"><object-id pub-id-type="pii">sensors-25-01038-t004_Table 4</object-id><label>Table 4</label><caption><p>Results of the experiments on the StanWiFi dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Source</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Acc</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pre</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Li et al. [<xref rid="B32-sensors-25-01038" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">THAT (2021)</td><td align="center" valign="middle" rowspan="1" colspan="1">98.20</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Yadav et al. [<xref rid="B24-sensors-25-01038" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">CSITime (2022)</td><td align="center" valign="middle" rowspan="1" colspan="1">98.00</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Salehinejad et al. [<xref rid="B26-sensors-25-01038" ref-type="bibr">26</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">LiteHAR (2022)</td><td align="center" valign="middle" rowspan="1" colspan="1">98.00</td><td align="center" valign="middle" rowspan="1" colspan="1">99.16</td><td align="center" valign="middle" rowspan="1" colspan="1">98.87</td><td align="center" valign="middle" rowspan="1" colspan="1">99.01</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Salaby et al. [<xref rid="B27-sensors-25-01038" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN-GRU (2022)</td><td align="center" valign="middle" rowspan="1" colspan="1">99.31</td><td align="center" valign="middle" rowspan="1" colspan="1">99.5</td><td align="center" valign="middle" rowspan="1" colspan="1">99.43</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Islam et al. [<xref rid="B28-sensors-25-01038" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">STC-NLSTMNet (2023)</td><td align="center" valign="middle" rowspan="1" colspan="1">99.88</td><td align="center" valign="middle" rowspan="1" colspan="1">99.72</td><td align="center" valign="middle" rowspan="1" colspan="1">99.73</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Jannat et al. [<xref rid="B5-sensors-25-01038" ref-type="bibr">5</xref>] </td><td align="center" valign="middle" rowspan="1" colspan="1">AAE+RF (2023)</td><td align="center" valign="middle" rowspan="1" colspan="1">99.84</td><td align="center" valign="middle" rowspan="1" colspan="1">99.82</td><td align="center" valign="middle" rowspan="1" colspan="1">99.83</td><td align="center" valign="middle" rowspan="1" colspan="1">99.81</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PA-CSI (2024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.86</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.95</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01038-t005"><object-id pub-id-type="pii">sensors-25-01038-t005_Table 5</object-id><label>Table 5</label><caption><p>The experiments on MINE lab dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Source</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Acc</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pre</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Li et al. [<xref rid="B32-sensors-25-01038" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">THAT (2021)</td><td align="center" valign="middle" rowspan="1" colspan="1">97.00</td><td align="center" valign="middle" rowspan="1" colspan="1">97.00</td><td align="center" valign="middle" rowspan="1" colspan="1">97.00</td><td align="center" valign="middle" rowspan="1" colspan="1">97.00</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PA-CSI (2024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.24</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01038-t006"><object-id pub-id-type="pii">sensors-25-01038-t006_Table 6</object-id><label>Table 6</label><caption><p>The experiments on MultiEnv dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Environment</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Source</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Acc</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pre</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th></tr></thead><tbody><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">E1: Office<break/>(LOS)</td><td align="center" valign="middle" rowspan="1" colspan="1">Alsaify et al. [<xref rid="B15-sensors-25-01038" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">SVM (2020)</td><td align="center" valign="middle" rowspan="1" colspan="1">94.03</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Li et al. [<xref rid="B32-sensors-25-01038" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">THAT (2021)</td><td align="center" valign="middle" rowspan="1" colspan="1">98.95</td><td align="center" valign="middle" rowspan="1" colspan="1">98.28</td><td align="center" valign="middle" rowspan="1" colspan="1">98.26</td><td align="center" valign="middle" rowspan="1" colspan="1">98.26</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Alsaify et al. [<xref rid="B22-sensors-25-01038" ref-type="bibr">22</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">SVM (2022)</td><td align="center" valign="middle" rowspan="1" colspan="1">91.27</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Islam et al. [<xref rid="B28-sensors-25-01038" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">STC-NLSTMNet (2023)</td><td align="center" valign="middle" rowspan="1" colspan="1">98.20</td><td align="center" valign="middle" rowspan="1" colspan="1">98.10</td><td align="center" valign="middle" rowspan="1" colspan="1">98.08</td><td align="center" valign="middle" rowspan="1" colspan="1">98.09</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Jannat et al. [<xref rid="B5-sensors-25-01038" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">AAE+RF (2023)</td><td align="center" valign="middle" rowspan="1" colspan="1">97.65</td><td align="center" valign="middle" rowspan="1" colspan="1">96.42</td><td align="center" valign="middle" rowspan="1" colspan="1">96.41</td><td align="center" valign="middle" rowspan="1" colspan="1">94.40</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PA-CSI (2024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.47</td></tr><tr><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">E2: Hall<break/>(LOS)</td><td align="center" valign="middle" rowspan="1" colspan="1">Alsaify et al. [<xref rid="B15-sensors-25-01038" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">SVM (2020)</td><td align="center" valign="middle" rowspan="1" colspan="1">94.03</td><td align="center" valign="middle" rowspan="1" colspan="1">- </td><td align="center" valign="middle" rowspan="1" colspan="1">- </td><td align="center" valign="middle" rowspan="1" colspan="1">- </td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Li et al. [<xref rid="B32-sensors-25-01038" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">THAT (2021)</td><td align="center" valign="middle" rowspan="1" colspan="1">97.39</td><td align="center" valign="middle" rowspan="1" colspan="1">97.24</td><td align="center" valign="middle" rowspan="1" colspan="1">97.22</td><td align="center" valign="middle" rowspan="1" colspan="1">97.22</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Alsaify et al. [<xref rid="B22-sensors-25-01038" ref-type="bibr">22</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">SVM (2022)</td><td align="center" valign="middle" rowspan="1" colspan="1">91.27</td><td align="center" valign="middle" rowspan="1" colspan="1"> -</td><td align="center" valign="middle" rowspan="1" colspan="1"> -</td><td align="center" valign="middle" rowspan="1" colspan="1"> -</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Islam et al. [<xref rid="B28-sensors-25-01038" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">STC-NLSTMNet (2023)</td><td align="center" valign="middle" rowspan="1" colspan="1">96.65</td><td align="center" valign="middle" rowspan="1" colspan="1">96.54</td><td align="center" valign="middle" rowspan="1" colspan="1">96.41</td><td align="center" valign="middle" rowspan="1" colspan="1">96.48</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PA-CSI (2024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.90</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.90</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">E3: Room <break/>and hall <break/>(NLOS)</td><td align="center" valign="middle" rowspan="1" colspan="1">Li et al. [<xref rid="B32-sensors-25-01038" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">THAT (2021)</td><td align="center" valign="middle" rowspan="1" colspan="1">97.56</td><td align="center" valign="middle" rowspan="1" colspan="1">97.04</td><td align="center" valign="middle" rowspan="1" colspan="1">97.04</td><td align="center" valign="middle" rowspan="1" colspan="1">97.03</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Islam et al. [<xref rid="B28-sensors-25-01038" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">STC-NLSTMNet (2023)</td><td align="center" valign="middle" rowspan="1" colspan="1">94.68</td><td align="center" valign="middle" rowspan="1" colspan="1">94.57</td><td align="center" valign="middle" rowspan="1" colspan="1">94.55</td><td align="center" valign="middle" rowspan="1" colspan="1">94.56</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Jannat et al. [<xref rid="B5-sensors-25-01038" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">AAE+RF (2023)</td><td align="center" valign="middle" rowspan="1" colspan="1">93.33</td><td align="center" valign="middle" rowspan="1" colspan="1">93.12</td><td align="center" valign="middle" rowspan="1" colspan="1">93.07</td><td align="center" valign="middle" rowspan="1" colspan="1">93.14</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PA-CSI (2024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.78</td></tr></tbody></table></table-wrap></floats-group></article>