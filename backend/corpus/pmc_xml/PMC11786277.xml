<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Iperception</journal-id><journal-id journal-id-type="iso-abbrev">Iperception</journal-id><journal-id journal-id-type="publisher-id">IPE</journal-id><journal-id journal-id-type="hwp">spipe</journal-id><journal-title-group><journal-title>i-Perception</journal-title></journal-title-group><issn pub-type="epub">2041-6695</issn><publisher><publisher-name>SAGE Publications</publisher-name><publisher-loc>Sage UK: London, England</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11786277</article-id><article-id pub-id-type="doi">10.1177/20416695251314182</article-id><article-id pub-id-type="publisher-id">10.1177_20416695251314182</article-id><article-categories><subj-group subj-group-type="heading"><subject>Standard Article</subject></subj-group></article-categories><title-group><article-title>Visualizing the change of &#x0201c;viewpoints&#x0201d; in 3D virtual art exhibition</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0008-4858-8196</contrib-id><name><surname>Matsumoto</surname><given-names>Kazuki</given-names></name><xref rid="corresp1-20416695251314182" ref-type="corresp"/></contrib><aff id="aff1-20416695251314182">Dokkyo University, Japan</aff></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Okada</surname><given-names>Takeshi</given-names></name></contrib><aff id="aff2-20416695251314182"><institution-wrap><institution-id institution-id-type="Ringgold">13143</institution-id><institution content-type="university">The University of Tokyo</institution></institution-wrap>, Japan</aff></contrib-group><author-notes><corresp id="corresp1-20416695251314182">Kazuki Matsumoto, Institute of Informatics, Dokkyo University, 1-1 Gakuencho, Soka-shi, Saitama-ken, Japan. 
Email: <email>k21126@dokkyo.ac.jp</email>
</corresp></author-notes><pub-date pub-type="epub"><day>31</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="collection"><season>Jan-Feb</season><year>2025</year></pub-date><volume>16</volume><issue>1</issue><elocation-id>20416695251314182</elocation-id><history><date date-type="received"><day>19</day><month>6</month><year>2024</year></date><date date-type="accepted"><day>5</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><copyright-holder content-type="sage">SAGE Publications Ltd. Manuscript content on this site is licensed under Creative Commons Licenses</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the Creative Commons Attribution 4.0 License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>) which permits any use, reproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access page (<ext-link ext-link-type="uri" xlink:href="https://us.sagepub.com/en-us/nam/open-access-at-sage">https://us.sagepub.com/en-us/nam/open-access-at-sage</ext-link>).</license-p></license></permissions><abstract><p>This study focuses on the process of viewers&#x02019; engagement with artwork in virtual spaces. Specifically, we propose a methodology that combines two approaches: (a) measuring movements within virtual spaces, and (b) using non-immersive virtual reality. We attempt to demonstrate the effectiveness of this methodology through experimentation. In the experiment, we recorded the exploration of sculptures in virtual spaces of 317 participants using a newly developed system (Virtual Exhibition Space for Tracking and Analyzing system). Results from multiple statistical analyses, including hierarchical Bayesian analysis, indicated differences in view-angles and distances traveled while viewing the artworks under different experimental conditions (where different instructions were given prior to viewing). These findings support the theoretical expectation that the &#x0201c;mode&#x0201d; of viewing artworks is influenced by instructions given before viewing, which is further corroborated by the observed differences in the pattern of psychological measurements between conditions. Based on previous research and these experimental results, we discuss the effectiveness and generalizability of our methodology in capturing variations in the art-viewing process as a tool for studying art-viewing behavior not only in virtual but also in real environments.</p></abstract><kwd-group><kwd>art viewing</kwd><kwd>methodological research</kwd><kwd>non-immersive virtual reality</kwd><kwd>movement</kwd><kwd>Bayesian hierarchical analysis</kwd><kwd>mode of viewing</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>typesetter</meta-name><meta-value>ts19</meta-value></custom-meta><custom-meta><meta-name>cover-date</meta-name><meta-value>January-February 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec><title>How to cite this article</title><p>Matsumoto, K., &#x00026; Okada, T. (2025). Visualizing the change of &#x0201c;viewpoints&#x0201d; in 3D virtual art exhibition. <italic toggle="yes">i-Perception</italic>, <italic toggle="yes">16</italic>(0), 1&#x02013;21. <ext-link xlink:href="https://doi.org/10.1177/20416695251314182" ext-link-type="uri">https://doi.org/10.1177/20416695251314182</ext-link></p></sec><sec sec-type="intro" id="section1-20416695251314182"><p>How do people view artworks in virtual spaces? Art researchers have found value in exploring this question due to the rapid rise in virtual art experiences in recent years (<xref rid="bibr10-20416695251314182" ref-type="bibr">Geismar, 2018</xref>; <xref rid="bibr33-20416695251314182" ref-type="bibr">Schweibenz, 2019</xref>) and because these experiences seem to differ from traditional art encounters. However, previous research on virtual art-viewing has two limitations: (a) it has primarily focused on psychological measures and has not captured viewers&#x02019; movements in virtual spaces; and (b) it has predominantly considered immersive virtual reality (VR) as virtual research and has not sufficiently explored non-immersive VR (desktop VR). In the following sections, we offer an overview of the significance of these two issues. Thereafter, we discuss the research questions that this method is suited to address.</p><p>Through theoretical considerations and experimentation, this study aims to highlight the effectiveness of a research design that simultaneously addresses these two aspects. As later elaborated, this methodology is expected to significantly contribute not only to understanding virtual art experiences but also to the general comprehension of art experiences or other research topics.</p><sec disp-level="2" id="section1A-20416695251314182"><title>Movements in Virtual and Non-Virtual Experience of Art</title><p>The art experience involves physical as well as mental processes, even if it refers to the &#x0201c;passive&#x0201d; recognition of artworks, such as viewing paintings or listening to music. This intuitively makes sense if the same artwork can give a considerably different impression depending on whether one examines it with one's eyes close or views it from a distance. In a non-virtual context, previous studies have already examined how the body affects the artwork's impression. For example, experiments have confirmed that hand movements (<xref rid="bibr19-20416695251314182" ref-type="bibr">Leder et al., 2012</xref>; <xref rid="bibr39-20416695251314182" ref-type="bibr">Ticini et al., 2014</xref>), eye movements (<xref rid="bibr25-20416695251314182" ref-type="bibr">Matsumoto &#x00026; Okada, 2021a</xref>), and hands-on creative activities (<xref rid="bibr26-20416695251314182" ref-type="bibr">Matsumoto &#x00026; Okada, 2021b</xref>) affect viewers&#x02019; esthetic impression. In a broader sense, these findings are closely related to recent theories, such as those of <xref rid="bibr4-20416695251314182" ref-type="bibr">Carbon (2019</xref>, <xref rid="bibr5-20416695251314182" ref-type="bibr">2020</xref>), which emphasize what kind of physical context an artwork is viewed within. This is because the bodily state, also expressed as &#x0201c;bodily context,&#x0201d; can be interpreted as one of the physical contexts that may generally affect one's judgment or feelings (cf. <xref rid="bibr23-20416695251314182" ref-type="bibr">Maglio &#x00026; Trope, 2012</xref>; <xref rid="bibr40-20416695251314182" ref-type="bibr">Troncoso et al., 2023</xref>).</p><p>It is plausible&#x02014;both empirically and intuitively&#x02014;that the body influences mental states during art-viewing as an explanatory variable; at the same time, the body may reflect influences from factors within the process of art-viewing, serving as an response variable. Art-viewing is an information-seeking process, and the viewer actively moves their body to acquire information. Even when facing the same artwork, the flow of processed information can be completely different depending on the spatial trajectory of perspectives. This implies that people must adjust their movements according to what they want to pay attention to. Perceptual input information is shaped by movement, which, in turn, is influenced by the results of cognitive processing. As this microcycle of perception and movement is a constant presence in art-viewing, a comprehensive understanding of art-viewing is incomplete without measuring actual movement.</p><p>The significance of movement does not diminish in the context of art-viewing in virtual spaces. &#x0201c;Virtual space&#x0201d; here refers to an electronic recreation of actual space that includes single or multiple representations of artworks. In a typical virtual space, viewers can navigate within the virtual environment and manipulate a replica in ways similar to those in the actual space. The objective of virtualization is to simulate experiences as realistically as possible, resembling the encounters one would have with actual artworks in the actual space. The most important similarity between the virtual and actual spaces is that the object provides new perceptual information when we interact with it. For example, we can see the hidden other side of the object by rotating it both within the virtual and actual spaces, which is impossible with a static 2D photograph. This capability of virtual spaces creates a sensation akin to being physically in the presence of the actual object&#x02014;in other words, a &#x0201c;virtual&#x0201d; sensation. Therefore, not only does the viewers&#x02019; movement in virtual spaces serve a function equivalent to that of viewers in actual spaces, but it also holds a higher value for consideration due to its direct influence on the quality of virtuality.</p><p>Conducting empirical research on movement in virtual spaces is also a powerful means to examine non-virtual experiences. This is because the behavioral history of individuals in virtual spaces can be digitally recorded, which is much more accurate and convenient than recording the coordinates of people and objects in actual spaces (cf. <xref rid="bibr42-20416695251314182" ref-type="bibr">Yaremych &#x00026; Persky, 2019</xref>). To illustrate this point, we examine the study by <xref rid="bibr18-20416695251314182" ref-type="bibr">K&#x000fc;hnapfel et al. (2024)</xref> as a comprehensive attempt to capture the dynamics of movement during art-viewing. They displayed a copy of Franz Marc's painting in a room arranged for the experiment, resembling a gallery, and ushered each participant into the space to freely view the work. Data were collected synchronously using mobile eye-tracking glasses worn by the participants and movement-tracking cameras installed in the room. They conducted a thorough analysis of the participants&#x02019; trajectories in the room and their gaze patterns on the artwork and surrounding space. They also examined how these related to psychological measurements (e.g., positive emotions toward the work), later measured by subjective reports. Although preparing an actual room and specialized equipment in this study holds unique significance, creating similar situations in a virtual space could be a feasible and realistic alternative for many researchers. With only a computer and a monitor, one can recreate a virtual exhibition space and record the user's camera trajectory (standing for time-varying perspective) within it. In areas other than art-viewing, studies have increasingly focused on measuring movement in virtual spaces (cf. <xref rid="bibr34-20416695251314182" ref-type="bibr">Shamay-Tsoory &#x00026; Mendelsohn, 2019</xref>; <xref rid="bibr42-20416695251314182" ref-type="bibr">Yaremych &#x00026; Persky, 2019</xref>).</p><p>Presumably, the lack of measurement of physical movement in virtual spaces can be attributed to the historical disregard for the relationship between art-viewing and physical states. Given the importance of the aforementioned movement in this context, we propose incorporating movement measurement into a new methodology.</p></sec><sec disp-level="2" id="section1B-20416695251314182"><title>Types of VR and Art-Viewing</title><p>Typically, what is referred to as VR involves the use of specialized equipment, such as head-mounted displays (HMDs), to provide viewers with a heightened sense of immersion. To distinguish this type of VR, the term &#x0201c;immersive VR&#x0201d; is sometimes used. In recent years, the art world has witnessed an increase in VR-based artworks and exhibitions. Consequently, several empirical studies have explored art in immersive VR (e.g., <xref rid="bibr11-20416695251314182" ref-type="bibr">Gotthardt et al., 2025</xref>; <xref rid="bibr16-20416695251314182" ref-type="bibr">Jankovi&#x00107; et al., 2020</xref>; <xref rid="bibr22-20416695251314182" ref-type="bibr">Lin et al., 2020</xref>; <xref rid="bibr24-20416695251314182" ref-type="bibr">Mar&#x000ed;n-Morales et al., 2019</xref>; <xref rid="bibr27-20416695251314182" ref-type="bibr">Miura &#x00026; Kawai, 2019</xref>). However, the virtual art experience is not limited to immersive VR alone. For example, <xref rid="bibr21-20416695251314182" ref-type="bibr">Leopardi et al. (2021)</xref> categorized virtual museums into five groups (also see <xref rid="bibr2-20416695251314182" ref-type="bibr">Beer, 2015</xref>; <xref rid="bibr36-20416695251314182" ref-type="bibr">Styliani et al., 2009</xref>), of which non-immersive VR (or desktop VR) is, in our opinion, of particular importance. This entails using standard flatscreen devices to enable user interaction and partially simulate the subject's virtual presence, as seen in everyday computer programs such as 3D action games or Google Maps. Despite not being commonly labeled as &#x0201c;VR&#x0201d; in ordinary situations, non-immersive VR is used much more frequently than immersive VR. This is due to its ease of adoption for both users and creators&#x02014;particularly, its convenience for users to be able to view it on a regular monitor without requiring an HMD (cf. <xref rid="bibr1-20416695251314182" ref-type="bibr">Barbieri et al., 2017</xref>; <xref rid="bibr13-20416695251314182" ref-type="bibr">He et al., 2023</xref>).</p><p>non-immersive VR is valuable in research not only for its prevalence in the art domain but also for its intermediary position between traditional digital images and immersive VR. As most existing art-viewing research uses non-virtual digital images, non-immersive VR, which serves as an intermediary, is likely to play a unique role in integrating insights from this approach with those obtained from VR. Moreover, from a practical standpoint, non-immersive VR systems provide an advantage by enabling participation in experiments without requiring an HMD. This makes experiments accessible to a wider range of people in the context of remote online studies, where people with only a computer and a monitor can join in. In summary, exploring artistic experiences with non-immersive VR is considered highly effective both theoretically and practically.<sup>
<xref rid="fn1-20416695251314182" ref-type="fn">1</xref>
</sup> The details of the non-immersive VR application are described in the method section.</p></sec><sec disp-level="2" id="section1C-20416695251314182"><title>Mode of Art-Viewing: What the Proposed Methodology Can Reveal</title><p>To demonstrate the effectiveness of our methodology, which focuses on motion in virtual spaces and employs non-immersive VR, we experimentally aim to capture changes in art-viewing processes by tracking movements within the non-immersive virtual space. When people view artwork, different kinds of global patterns are identified in their processes, which we term &#x0201c;mode&#x0201d; of viewing. In this section, we discuss the nature of that and how it aligns with our methodology.</p><p>When we view an object, where we direct out attention to various elements in it sequentially, it is improbable that the processed element at any moment is determined independent of the moment right before it. Rather, our stream of thought and attention is always directed by some temporary topic in a top-down manner. For example, in front of Leonardo da Vinci's &#x0201c;Annunciation,&#x0201d; someone may be interested in the narrative aspect depicted in it. In this situation, this viewer's mental and physical processes will naturally be involved with elements related to the narrative topic, such as gazing upon the characters&#x02019; facial expressions, imagining their conversation and feelings, and so on. We could label this type of process as &#x0201c;narrative mode.&#x0201d; On the other hand, individuals with expertise in the creative process of art may focus on the techniques used and the presence of the artist's originality and imagination, rather than the story, in the painting. This mode of focusing on the artwork's creative process (&#x0201c;process mode&#x0201d;) may involve specific psychological processes not observed in the &#x0201c;narrative mode,&#x0201d; such as the viewer's social comparisons with the artist and associated emotions (cf. <xref rid="bibr14-20416695251314182" ref-type="bibr">Hitsuwari &#x00026; Nomura, 2023</xref>; <xref rid="bibr25-20416695251314182" ref-type="bibr">Matsumoto &#x00026; Okada, 2021a</xref>, <xref rid="bibr26-20416695251314182" ref-type="bibr">2021b</xref>). Conversely, the &#x0201c;narrative mode&#x0201d; should naturally have unique aspects absent in the process mode. Therefore, the &#x0201c;narrative mode&#x0201d; and &#x0201c;process mode&#x0201d; each refer to entirely different psychological processes whose difference comes out of the appreciation of the same artwork. Just as ordinary people have the &#x0201c;narrative mode&#x0201d; and experts the &#x0201c;process mode,&#x0201d; differences in knowledge and experience are crucial for differentiating these modes (cf. <xref rid="bibr3-20416695251314182" ref-type="bibr">Bullot &#x00026; Reber, 2013</xref>; <xref rid="bibr25-20416695251314182" ref-type="bibr">Matsumoto &#x00026; Okada, 2021a</xref>; <xref rid="bibr29-20416695251314182" ref-type="bibr">Parsons, 1987</xref>).</p><p>In addition to shifts in perspective based on individual differences such as knowledge and experience, different situations and goals of viewing artwork can result in mode variations. Viewing art can serve as an autotelic activity for the viewer, offering self-rewarding feelings; in other cases, however, it may also be undertaken as part of studying for an art history exam. Furthermore, viewers may also look at others&#x02019; creations to find inspiration for their own new works. The examples here suggest that the way art is viewed can be influenced not only by individual differences but also by the context; thus, the same person may switch modes depending on the time and circumstances (this point will be revisited when experimentally manipulating modes later).</p><p>Physical movement patterns play an essential role in distinguishing modes of viewing. If viewers have different interests, such as the depicted characters&#x02019; emotions or the painting's creative process, the location of valuable information for them also varies accordingly, such as the characters&#x02019; expressions or the texture of paint and brushstrokes across the entire canvas. Thus, viewers can exhibit the differences in the physical distances and angles they prefer during information seeking and their trajectories over time. This aligns with the results of many studies on gaze behavior during art-viewing, which indicates that eye-movement patterns vary based on the viewers&#x02019; interests (for review, see <xref rid="bibr31-20416695251314182" ref-type="bibr">Rosenberg &#x00026; Klein, 2015</xref>). In short, the difference in modes of viewing is closely tied to the spatial aspects of viewer's behavior. What has been discussed here echoes the earlier discussion about the general importance of movement in the information-seeking process of viewing artworks. This is the reason why focusing the concept of &#x0201c;mode&#x0201d; of viewing is suitable for the current methodology featured by measuring movements.</p></sec><sec disp-level="2" id="section1D-20416695251314182"><title>Comparison of Modes: Current Hypotheses</title><p>In this study, we aim to capture changes in modes of viewing using a new methodology. To achieve this, we first generate variations in modes through experimental manipulation. Hence, we plan to guide individuals to perceive the same artwork differently using instructional cues. What we focus on are the three modes mentioned in the above discussion: the &#x0201c;Narrative Mode,&#x0201d; &#x0201c;Process Mode,&#x0201d; and &#x0201c;Creator Mode.&#x0201d; The &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Process Mode&#x0201d; refer to states where attention is particularly focused on either the narrative aspects represented in the artwork or the creative process aspects, respectively. In this study, we attempt to evoke these two modes by providing viewers with relevant information on the artwork. By contrast, the third condition, &#x0201c;Creator Mode,&#x0201d; may occur independently of knowledge about the artwork. It is included in this comparison with the expectation of supporting the aforementioned prediction that not only individual differences (such as knowledge) but also the mode changes based on the external context. The &#x0201c;Creator Mode&#x0201d; involves viewing art with the goal of creating one's own works, rather than simply viewing the artwork itself.</p><p>As a hypothesis, we anticipate differences in the patterns of scores for psychological variables (e.g., liking, admiration, and interest) in response to the artwork under each condition. Since different modes are likely to influence an entire pattern of the viewing process, we consider using a multidimensional and comprehensive scale and conducting multivariate analysis of variance (MANOVA) suitable for examining the differences. In addition, the appreciation of artworks focusing on the creative process may yield higher scores for both liking and admiration (<xref rid="bibr25-20416695251314182" ref-type="bibr">Matsumoto &#x00026; Okada, 2021a</xref>, <xref rid="bibr26-20416695251314182" ref-type="bibr">2021b</xref>). This suggests the possibility of higher scores of them in the &#x0201c;Process Mode&#x0201d; in this study. More unique to our methodology, we expect differences in information search movements corresponding to each mode&#x02014;specifically in the distance moved and view-angles when viewing artwork in the virtual space.</p></sec></sec><sec sec-type="methods" id="section2-20416695251314182"><title>Method</title><sec id="section2A-20416695251314182"><title>Participants</title><p>A total of 349 participants participated in this study. Due to various issues, such as browser crashes, 32 participants failed to complete the experiment, and data from 317 participants were available for analysis. We recruited the participants online through a crowdsourcing service and gave them approximately JPY 700 after completing the procedure. All methods and procedures were approved by the Ethics Committee of the University of Tokyo.</p></sec><sec id="section2B-20416695251314182"><title>Stimuli</title><p>We utilized a 3D model of the sculptural work &#x0201c;Death and the Mother&#x0201d; created by Niels Hansen Jacobsen (1861&#x02013;1941) as a stimulus (<xref rid="fig1-20416695251314182" ref-type="fig">Figure 1</xref>). This model was downloaded from Sketchfab.com and displayed within the web application developed for this study.<sup>
<xref rid="fn2-20416695251314182" ref-type="fn">2</xref>
</sup> &#x0201c;Death and the Mother&#x0201d; is based on Hans Christian Andersen's fairytale &#x0201c;The Story of a Mother.&#x0201d; The narrative of this fairytale was used as part of the instructions presented to the participants, as described later.</p><fig position="float" id="fig1-20416695251314182"><label>Figure 1.</label><caption><p>A 3D model of the sculptural work &#x0201c;Death and the Mother.&#x0201d; This model is used as a stimulus in the current experiment. Both images are created by the VESTA system. The first figure is seen from the default angle, with the camera in front of the work, reflecting what participants can see when starting the viewing phase. The second figure visualizes the locations of a participant's viewpoints during about 40&#x02005;s, photographed from a position above and to the side. The translucent yellow sphere represents the position of the camera set by the participant at a certain point in time, with brighter yellow indicating that the camera stayed in that location for a longer duration. In this case, from the diagonal line extending downward to the bottom right, it can be observed that this participant approached to or retreated from the work in the front position. The denser concentration of yellow spheres in front of the artwork, with sparser distribution toward the rear, indicates that this participant mainly viewed the artwork from the front. The green text at the top left and yellow spheres are displayed for analysis and set up to be invisible to the participants during the experiment.</p></caption><graphic xlink:href="10.1177_20416695251314182-fig1" position="float"/></fig><p>This sculpture features three characters: &#x0201c;Death&#x0201d; (the Grim Reaper), a mother, and a child. Death is immediately noticeable due to the large scythe it carries, and the mother, who crouches at its feet, is also relatively easy to see. The child, being held against Death's chest, may be slightly difficult to see from the front, but it is likely to be visible when approached or viewed from different angles. Overall, although the artwork's composition can be roughly understood from the front, camera manipulation seems to encourage the examination of the details. Therefore, we believe that this artwork is suitable for observing voluntary behavior in front of it.</p></sec><sec id="section2C-20416695251314182"><title>Web Application: VESTA System</title><p>In the web application of this study (we named this Virtual Exhibition Space for Tracking and Analyzing [VESTA] system), participants had three types of interactions: (a) &#x0201c;Rotation,&#x0201d; (b) &#x0201c;Approach to/Retreat from Target,&#x0201d; and (c) &#x0201c;Shift of Target.&#x0201d; &#x0201c;Rotation&#x0201d; is the most basic action, and it refers to moving the camera's position on a spherical surface centered around the current target while keeping the gaze fixed on the target. In other words, in the &#x0201c;Rotation,&#x0201d; the camera orbits around the target to view the object's different sides hidden from view. While &#x0201c;Rotation&#x0201d; keeps the distance from the target constant, &#x0201c;Approach to/Retreat from Target&#x0201d; changes the distance while keeping the camera angle constant, similar to zooming in and out in a two-dimensional image. As the combination of &#x0201c;Rotation&#x0201d; and &#x0201c;Approach to/Retreat from Target&#x0201d; allows for viewing the entire artwork, &#x0201c;Shift of Target&#x0201d; is less frequently used. &#x0201c;Shift of Target&#x0201d; involves freely shifting the target coordinates that relates to the other two interactions, and it enables the camera to rotate around a point that is different from the original one. The participants were able to freely combine and manipulate these three types at any time.</p><p>The VESTA system recorded both the target coordinates that the participants were looking at and those of the camera's position with a time resolution of up to 60 frames per second (which may decrease depending on the performance of participants&#x02019; web-browsing environment). In other words, we accurately recorded the progression of each participant's operation of the VESTA system as numerical data without any omissions and immediately saved these data in TSV format on our cloud server. All functionalities of the VESTA system were implemented using Three.js, a JavaScript library based on WebGL (online demonstration: <ext-link xlink:href="https://psychologykm.github.io/VESTA/demo/demo_recording_short.html" ext-link-type="uri">https://psychologykm.github.io/VESTA/demo/demo_recording_short.html</ext-link>).</p></sec><sec id="section2D-20416695251314182"><title>Experimental Conditions</title><p>We randomly assigned the participants to one of three conditions: the &#x0201c;Narrative Mode&#x0201d; group (<italic toggle="yes">n</italic>&#x02009;=&#x02009;108); the &#x0201c;Process Mode&#x0201d; group (<italic toggle="yes">n</italic>&#x02009;=&#x02009;110); and the &#x0201c;Creator Mode&#x0201d; group (<italic toggle="yes">n</italic>&#x02009;=&#x02009;99). As the participants needed to complete complex experimental procedures online by themselves, some dropped out of the experimental procedure, which led to differences in sample sizes between conditions. However, as this would not significantly impact the analysis, neither did we replace or replenish the participants.</p><p>All conditions were manipulated solely by differences in the content of a single web page. Each page included a paragraph consisting of 10&#x02013;12 sentences and a 2D photograph of the sculpture from which the stimulus was modeled.</p><p>In the &#x0201c;Narrative Mode&#x0201d; condition, participants were presented with a passage describing the plot of Andersen's &#x0201c;The Story of a Mother,&#x0201d; which had served as the inspiration for the sculpture. The text concludes by noting that the scene depicted in the sculpture corresponds to the final scene of the story, portraying the mother's realization of her child's death and her subsequent sorrow. Additionally, we provided participants with a partially visualized diagram illustrating the relationships between characters and motifs mentioned in the passage. In the &#x0201c;Process Mode&#x0201d; condition, we provided them with a passage explaining the typical creative process of a plaster sculpture, the category to which the current stimuli belong, along with a summarized diagram. In the &#x0201c;Creator Mode&#x0201d; condition, we attempted to induce participants to imagine themselves as sculptors creating a piece rather than providing information about the artwork. Before viewing the stimulus work, participants were asked to freely imagine ideas for a sculpture with the theme of &#x0201c;Death,&#x0201d; making their ideas clear as much as possible. Subsequently, they were shown images of the stimuli work, and then asked to refine their own imaginary creations with a focus on the differences from the presented stimuli work.</p></sec><sec id="section2E-20416695251314182"><title>Procedure</title><p>After reading a broad description of the tasks in an online document, each participant provided informed consent and began the tasks using their favored browser, accessing our web server. We required them to complete the tasks using a computer with a stable connection in a quiet, non-distracting environment.</p><p>On our website, participants first practiced operating the VESTA system by following instructions using a dummy 3D model. Once they felt that they had familiarized themselves with the operation, they proceeded to the aforementioned web page corresponding to the condition assigned to them (see <ext-link xlink:href="https://journals.sagepub.com/doi/suppl/10.1177/20416695251314182" ext-link-type="uri">Supplementary Table S1</ext-link>). We instructed them to read content related to the story if assigned to the &#x0201c;Narrative Mode&#x0201d; condition, read content related to the creative process if assigned to the &#x0201c;Process Mode&#x0201d; condition, or generate their own ideas for art-making if assigned to the &#x0201c;Creator Mode.&#x0201d; Participants were required to stay on the page for three minutes at least, enforced by the system. After this time elapsed and the participants judged that they had completed the page's contents, they proceeded to the VESTA system once again for the main trial. In this main trial, participants interacted with the stimulus for 100 s; shortening or extending this time period was not permitted. After the main trial phase, we asked the participants to freely report their thoughts in written form regarding their observations of the artwork (these data were not investigated further in this study) and answer items on a Likert scale. In the actual procedure, the last two phases&#x02014;the main trial and reporting&#x02014;were repeated for a purpose different from that of the current study. In the current analysis, we only use data collected the first time; thus, this repetition of the two phases has no relation to any of the results of this study.</p></sec><sec id="section2F-20416695251314182"><title>Measurements and Analysis</title><sec id="section2F1-20416695251314182"><title>Likert Scale</title><p>The participants were asked to provide responses that reflected their various impressions of the stimulus. The dimensions measured were (a) liking, (b) admiration, (c) empathy, (d) imagination, (e) inspiration, (f) beauty, (g) boredom, (h) interest, and (i) nostalgia. We adopted (a)&#x02013;(e) from previous studies (<xref rid="bibr25-20416695251314182" ref-type="bibr">Matsumoto &#x00026; Okada, 2021a</xref>, <xref rid="bibr26-20416695251314182" ref-type="bibr">2021b</xref>) on which the current study primarily relies theoretically. To enhance the comprehensiveness of the scale, (f)&#x02013;(i) were adopted and translated from <xref rid="bibr15-20416695251314182" ref-type="bibr">Hosoya (2020)</xref>'s study, which utilized multidimensional items of appropriate length. The only difference from the original scale was that we excluded one of the two items measuring (f) beauty from the current set of items as it used wording that could not be distinguished from (a) liking. Except for (e), which consists of five items (developed by Ishiguro and Okada [2015]), and (g)&#x02013;(i), which consist of two items each, we measured all other dimensions using a single item. All items are shown in the <ext-link xlink:href="https://journals.sagepub.com/doi/suppl/10.1177/20416695251314182" ext-link-type="uri">Supplemental Table S2</ext-link>.</p><p>We conducted a MANOVA on all variables measured using Likert scales. Specifically, we treated the nine items comprising (a)&#x02013;(i) as separate dependent variables and conducted a single test to examine whether the patterns of these variables differed across conditions.</p></sec><sec id="section2F2-20416695251314182"><title>Viewpoint Coordinates</title><p>We analyzed the coordinate information recorded by the VESTA system from two perspectives: the total traveled distance moved by each participant while viewing the stimulus, and the distribution of view-angles toward the artwork. The former analysis is relatively easy to understand; it involved calculating the Euclidean distance between all temporally adjacent pairs of the recorded 3D coordinates of the camera positions for each individual, summing these distances and log-transforming to obtain a score, denoted as &#x0201c;<italic toggle="yes">d<sub>total</sub></italic>.&#x0201d; This score could be highly influenced by an individual's general tendency to move dynamically in 3D space. Therefore, we decided to calculate the same metric for the practice phase, which involved a dummy 3D model and was conducted before the experimental intervention, and include it as a covariate, denoted as &#x0201c;<italic toggle="yes">d<sub>cov</sub></italic>,&#x0201d; in the difference testing model as analysis of covariance (ANCOVA).</p><p>The second analysis regarding view-angles required more complex modeling due to the circular structure of angle data, unlike typical variables that can be arranged on a one-dimensional number line, and the hierarchical structure in which the angle data at each time point is nested within individuals. To overcome this complexity, we adopted Bayesian estimation, which is well-suited for implementing complex hierarchical models due to its flexibility (<xref rid="bibr41-20416695251314182" ref-type="bibr">Veenman et al., 2023</xref>).</p><p>Through Bayesian hierarchical analysis, we examined whether differences existed between conditions in individuals&#x02019; tendencies for view-angles. <xref rid="fig2-20416695251314182" ref-type="fig">Figure 2</xref> shows a graphical model for inferring this difference based on <xref rid="bibr20-20416695251314182" ref-type="bibr">Lee and Wagenmakers (2014)</xref>. The outcome variable is the view-angle at each time point.<sup>
<xref rid="fn3-20416695251314182" ref-type="fn">3</xref>
</sup> We denote the angle at the <italic toggle="yes">j</italic>th data point (or we can also call it <italic toggle="yes">j</italic>th &#x0201c;fixation,&#x0201d; as described below) of the <italic toggle="yes">i</italic>th participant as <italic toggle="yes">&#x003b8;<sub>ij</sub></italic>, which are modeled as draws from a von Mises distribution with mean direction <italic toggle="yes">&#x003bc;<sub>i</sub></italic> and concentration parameter <italic toggle="yes">&#x003ba;<sub>i</sub></italic>. The von Mises distribution is the circular analog to the Gaussian distribution. The <italic toggle="yes">&#x003ba;</italic> in the von Mises distribution corresponds to the inverse of the <italic toggle="yes">&#x003c3;</italic> parameter in the Gaussian distribution. When <italic toggle="yes">&#x003ba;&#x02009;</italic>=&#x02009;0, it is equivalent to a uniform distribution, and conversely, as <italic toggle="yes">&#x003ba;</italic> increases, the probability of occurrence near the mean direction <italic toggle="yes">&#x003bc;</italic> becomes higher. With the hierarchical approach, each participant is assumed to have a different von Mises distribution, the parameters of which further follow Gaussian distributions with parameters <italic toggle="yes">&#x003b1;<sub>&#x003ba;</sub></italic>, <italic toggle="yes">&#x003c3;<sub>&#x003ba;</sub></italic>, <italic toggle="yes">&#x003b1;<sub>&#x003bc;</sub></italic>, and <italic toggle="yes">&#x003c3;<sub>&#x003bc;</sub></italic>. The difference between conditions is considered only for the <italic toggle="yes">&#x003b1;</italic>s, means of Gaussian distributions, in the entire modeling: that is, if a participant is in the &#x0201c;Narrative Mode&#x0201d; condition (coded as <italic toggle="yes">c<sub>i&#x02009;</sub></italic>=&#x02009;0 in <xref rid="fig2-20416695251314182" ref-type="fig">Figure 2</xref>), then <italic toggle="yes">&#x003b1;&#x02009;</italic>=&#x02009;<italic toggle="yes">&#x003b1;<sup>n</sup></italic>; the &#x0201c;Process Mode&#x0201d; condition (<italic toggle="yes">c<sub>i&#x02009;</sub></italic>=&#x02009;1), <italic toggle="yes">&#x003b1;&#x02009;</italic>=&#x02009;<italic toggle="yes">&#x003b1;<sup>p</sup></italic>; and in the &#x0201c;Creator Mode&#x0201d; condition (<italic toggle="yes">c<sub>i&#x02009;</sub></italic>=&#x02009;2), <italic toggle="yes">&#x003b1;&#x02009;</italic>=&#x02009;<italic toggle="yes">&#x003b1;<sup>c</sup></italic>. <italic toggle="yes">&#x003b1;<sup>p</sup></italic> and <italic toggle="yes">&#x003b1;<sup>c</sup></italic> are given by <italic toggle="yes">&#x003b1;<sup>p&#x02009;</sup></italic>=&#x02009;<italic toggle="yes">&#x003b1;<sup>n&#x02009;</sup></italic>+&#x02009;<italic toggle="yes">&#x003b4;<sup>np</sup></italic> and <italic toggle="yes">&#x003b1;<sup>c&#x02009;</sup></italic>=&#x02009;<italic toggle="yes">&#x003b1;<sup>c&#x02009;</sup></italic>+&#x02009;<italic toggle="yes">&#x003b4;<sup>nc</sup></italic>, with <italic toggle="yes">&#x003b1;<sup>n</sup></italic> and <italic toggle="yes">&#x003b4;</italic>s representing the reference value and the differences from it, respectively; the larger the absolute value of <italic toggle="yes">&#x003b4;<sub>&#x003bc;</sub></italic>, the further away each participant's mean view-angle in the given condition (e.g., the &#x0201c;Process Mode&#x0201d; condition) from those of the reference condition (the &#x0201c;Narrative Mode&#x0201d; condition). Conversely, a positive value of <italic toggle="yes">&#x003b4;<sub>&#x003ba;</sub></italic> indicates that in that condition, participants tend to focus on a single location rather than various angles compared to the reference condition (if it is a negative value, the opposite holds true). Priors used for the estimated parameters <italic toggle="yes">&#x003c3;</italic>, <italic toggle="yes">&#x003b1;</italic>, and <italic toggle="yes">&#x003b4;</italic> are only weakly informative.</p><fig position="float" id="fig2-20416695251314182"><label>Figure 2.</label><caption><p>Graphical model for the hierarchical Bayesian analysis. Following <xref rid="bibr20-20416695251314182" ref-type="bibr">Lee and Wagenmakers (2014</xref>, p. 18), each symbol carries the following meanings: circular/square nodes are used for continuous/discontinuous variables; shaded/unshaded nodes are used for observed/unobserved variables, and single-/double-bordered nodes are used for stochastic/deterministic variables.</p></caption><graphic xlink:href="10.1177_20416695251314182-fig2" position="float"/></fig><p>In addition to estimating each parameter, similar to hypothesis testing in frequentist statistics, we compared the H0 and H1 models. The H0 model is the null model where all <italic toggle="yes">&#x003b4;</italic>s in the graphical model in <xref rid="fig2-20416695251314182" ref-type="fig">Figure 2</xref> are fixed to zero. In other words, it can be interpreted as one that completely disregards the differences between the three conditions. H1 imposes no constraints on the model. We calculated the Bayes Factor (BF<sub>10</sub>) to determine which model was preferable; a higher BF<sub>10</sub> provides stronger support for the presence of differences between conditions. The analysis was conducted using Stan (<xref rid="bibr6-20416695251314182" ref-type="bibr">Carpenter et al., 2017</xref>) and R package &#x0201c;bridgesampling&#x0201d; (<xref rid="bibr12-20416695251314182" ref-type="bibr">Gronau et al., 2017</xref>).</p><p>While all the coordinate data for each individual were used to analyze the total traveled distance, for the Bayesian model regarding angles, only the camera coordinate data were included when they remained stationary at one location for 500&#x02005;ms or more (such as fixations of eye movements). The primary reason for this was that in the model, all data collected at a high temporal resolution would result in a particularly high computational load. Second, and perhaps more importantly, we were interested in which angles participants actually continued to view the artwork from, rather than the countless points they passed through while searching for the desired view-angle. This is akin to analyzing the position of fixations rather than the saccadic trajectories in eye-tracking studies.</p><p>Among the 317 participants, those who met any one of the following criteria were excluded from the analysis of coordinates: those who moved the camera too far away from the target at any point during the observation (three participants), those who moved the camera an extremely short distance from the starting point (17 participants), or those whose number of recorded data points was especially low (averaging below 10 records/s) due to issues such as the performance of their personal computers (37 participants). Due to a single participant meeting multiple criteria, 56 individuals were excluded, leaving 261 participants eligible for the coordinate analysis (<italic toggle="yes">n</italic>&#x02009;=&#x02009;84, 97, and 80 for &#x0201c;Narrative Mode&#x0201d; condition, &#x0201c;Process Mode&#x0201d; condition, &#x0201c;Creator Mode&#x0201d; condition, respectively).</p></sec></sec></sec><sec sec-type="results" id="section3-20416695251314182"><title>Results</title><p>All data and codes used in the analyses are available at <ext-link xlink:href="https://github.com/psychologyKM/VESTA" ext-link-type="uri">https://github.com/psychologyKM/VESTA</ext-link>.</p><sec id="section3A-20416695251314182"><title>Likert Scale</title><p><ext-link xlink:href="https://journals.sagepub.com/doi/suppl/10.1177/20416695251314182" ext-link-type="uri">Supplemental Table S3</ext-link> shows the means and SD of the participants&#x02019; scores on all Likert scales for each condition. We conducted a MANOVA to determine whether a significant difference existed in the pattern of those scores across conditions. Using Pillai's trace, the results revealed a significant main effect for the group of psychological measurements, <italic toggle="yes">T&#x02009;</italic>=&#x02009;0.11, <italic toggle="yes">F</italic>(18, 614)&#x02009;=&#x02009;2.01; <italic toggle="yes">p&#x02009;</italic>=&#x02009;.008; partial <italic toggle="yes">&#x003b7;</italic><sup>2&#x02009;</sup>=&#x02009;.06, 95% CI [0.00, 0.07]. We conducted post-hoc comparisons using three separate MANOVAs to examine which pairs of the three conditions contributed to the above results. The analysis revealed significant differences between the &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Process Mode&#x0201d; conditions, <italic toggle="yes">T&#x02009;</italic>=&#x02009;0.10, <italic toggle="yes">F</italic>(9, 208)&#x02009;=&#x02009;2.58; <italic toggle="yes">p&#x02009;</italic>=&#x02009;.008; partial <italic toggle="yes">&#x003b7;</italic><sup>2&#x02009;</sup>=&#x02009;.10, [0.01, 0.15], and between the &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Creator Mode&#x0201d; conditions, <italic toggle="yes">T&#x02009;</italic>=&#x02009;0.10, <italic toggle="yes">F</italic>(9, 197)&#x02009;=&#x02009;2.30; <italic toggle="yes">p&#x02009;</italic>=&#x02009;.018; partial <italic toggle="yes">&#x003b7;</italic><sup>2&#x02009;</sup>=&#x02009;.10, [0.00, 0.14], while no significant difference existed between the &#x0201c;Process Mode&#x0201d; and &#x0201c;Creator Mode&#x0201d; conditions, <italic toggle="yes">T&#x02009;</italic>=&#x02009;0.06, <italic toggle="yes">F</italic>(9, 199)&#x02009;=&#x02009;1.35; <italic toggle="yes">p&#x02009;</italic>=&#x02009;.214.</p><p>Additionally, we conducted separate ANOVAs for each variable to perform post-hoc tests comparing the variances between the &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Process Mode&#x0201d; conditions (denoted as &#x0201c;N-P contrast&#x0201d;) and between the &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Creator Mode&#x0201d; conditions (denoted as &#x0201c;N-C contrast&#x0201d;) as both pairs had showed significant results in the post-hoc MANOVAs. In N-P contrast, ANOVAs revealed only two significant differences: (c) empathy, <italic toggle="yes">F</italic>(1, 216)&#x02009;=&#x02009;11.93; <italic toggle="yes">p&#x02009;</italic>&#x0003c;&#x02009;.001; <italic toggle="yes">&#x003b7;</italic><sup>2&#x02009;</sup>=&#x02009;.05, 95% CI [0.01, 0.12], and (d) imagination, <italic toggle="yes">F</italic>(1, 216)&#x02009;=&#x02009;4.22; <italic toggle="yes">p&#x02009;</italic>=&#x02009;.041; <italic toggle="yes">&#x003b7;</italic><sup>2&#x02009;</sup>=&#x02009;.02, [0.00, 0.07]. For both variables, the &#x0201c;Process Mode&#x0201d; condition showed higher mean value than the &#x0201c;Narrative Mode&#x0201d; condition. In N-C contrast, ANOVAs revealed only two significant differences: (c) empathy, <italic toggle="yes">F</italic>(1, 205)&#x02009;=&#x02009;6.31; <italic toggle="yes">p&#x02009;</italic>=&#x02009;.013; <italic toggle="yes">&#x003b7;</italic><sup>2&#x02009;</sup>=&#x02009;.03, [0.00, 0.09], and (h) interest, <italic toggle="yes">F</italic>(1, 205)&#x02009;=&#x02009;4.59; <italic toggle="yes">p&#x02009;</italic>=&#x02009;.033; <italic toggle="yes">&#x003b7;</italic><sup>2&#x02009;</sup>=&#x02009;.02, [0.00, 0.08]. For both variables, the &#x0201c;Creator Mode&#x0201d; condition showed a higher mean value that the &#x0201c;Narrative Mode&#x0201d; condition. These results were obtained from exploratory post-hoc tests, and it is difficult to draw definitive conclusions from them as, except for (c) empathy, they could be insignificant after correction for multiple comparisons, such as Bonferroni adjustment. Moreover, if we use Bonferroni adjustment for post-hoc MANOVAs, even the entire difference between the &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Creator Mode&#x0201d; conditions could be insignificant. Bearing these limitations in mind, we attempt to interpret these results in the discussion section.</p></sec><sec id="section3B-20416695251314182"><title>Viewpoint Coordinates</title><sec id="section3F3-20416695251314182"><title>Total Traveled Distance</title><p>The mean and SD of <italic toggle="yes">d<sub>total</sub></italic> for each condition were 5.77 (0.49), 5.86 (0.44), and 5.68 (0.59), for the &#x0201c;Narrative Mode,&#x0201d; &#x0201c;Process Mode,&#x0201d; and &#x0201c;Creator Mode&#x0201d; condition, respectively. We conducted an ANCOVA test to determine whether a significant difference in <italic toggle="yes">d<sub>total</sub></italic> existed across conditions. The ANCOVA result revealed a significant main effect for <italic toggle="yes">d<sub>total</sub></italic>, <italic toggle="yes">F</italic>(2, 257)&#x02009;=&#x02009;3.47; <italic toggle="yes">p&#x02009;</italic>=&#x02009;.032; partial <italic toggle="yes">&#x003b7;</italic><sup>2&#x02009;</sup>=&#x02009;.03, 95% CI [0.00, 0.07], with a significant covariate effect, <italic toggle="yes">F</italic>(1, 257)&#x02009;=&#x02009;14.34; <italic toggle="yes">p&#x02009;</italic>&#x0003c;&#x02009;.001; partial <italic toggle="yes">&#x003b7;</italic><sup>2&#x02009;</sup>=&#x02009;.05, [0.01, 0.11]. Post-hoc comparisons were conducted using three separate ANCOVAs to examine which pairs of the three conditions contributed to the above result. The analysis revealed a significant main effect only for the comparison between the &#x0201c;Process Mode&#x0201d; and &#x0201c;Creator Mode&#x0201d; conditions, <italic toggle="yes">F</italic>(1, 174)&#x02009;=&#x02009;7.04; <italic toggle="yes">p&#x02009;</italic>=&#x02009;.009; partial <italic toggle="yes">&#x003b7;</italic><sup>2&#x02009;</sup>=&#x02009;.04, [0.00, 0.11], and insignificant main effects between the &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Creator Mode&#x0201d; conditions, <italic toggle="yes">F</italic>(1, 161)&#x02009;=&#x02009;1.04; <italic toggle="yes">p&#x02009;</italic>=&#x02009;.309), and between the &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Process Mode&#x0201d; conditions, <italic toggle="yes">F</italic>(1, 178)&#x02009;=&#x02009;2.59; <italic toggle="yes">p&#x02009;</italic>=&#x02009;.109). The significance of these results did not change whether Bonferroni correction was applied.</p></sec><sec id="section4F5-20416695251314182"><title>Distribution of View-Angles</title><p>Using the VESTA system, we plotted the coordinates of camera positions adopted by the participants for each condition, overlaying them on the stimulus (<xref rid="fig3-20416695251314182" ref-type="fig">Figure 3A</xref>). In other words, these images visualized the participants&#x02019; &#x0201c;viewpoints&#x0201d; while viewing the artwork. Intuitively, this figure suggests that while in the &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Creator Mode&#x0201d; conditions, the camera's position seemed to show a strong concentration toward the front of the sculpture (rightward in the images); in the &#x0201c;Process Mode&#x0201d; condition, by contrast, the tendency toward the front became more modest, and the frequency of camera movement toward the rear of the sculpture was relatively higher (leftward in the image). In addition, we plotted the areas where participants spent more time viewing (or more precisely capturing on camera) the surface of the stimulus (<xref rid="fig4-20416695251314182" ref-type="fig">Figure 4</xref>). While <xref rid="fig3-20416695251314182" ref-type="fig">Figure 3A</xref> is about &#x0201c;from where they viewed,&#x0201d; <xref rid="fig4-20416695251314182" ref-type="fig">Figure 4</xref> shows &#x0201c;where they looked at.&#x0201d; As we did not have any data on the participants&#x02019; eye movements, our estimation of the amount of time participants spent looking at each area remains approximate. Nevertheless, this visualized result suggests that while participants in the &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Creator Mode&#x0201d; conditions directed their cameras more toward the characters&#x02019; faces, participants in the &#x0201c;Process Mode&#x0201d; condition tended to move their cameras round to the artwork's rear.</p><fig position="float" id="fig3-20416695251314182"><label>Figure 3.</label><caption><p>Recorded data of viewpoint coordinates and estimations of parameters. (A) Images captured from above in the VESTA system, overlaying the stimulus and the recorded coordinates of the cameras distributed around it for each condition. To clearly illustrate the trends, the plotted data points are randomly sampled from the entire dataset at a 1% probability, and the transparency of the yellow spheres indicating camera positions is set higher than in <xref rid="fig1-20416695251314182" ref-type="fig">Figure 1</xref>. (B) Plots of empirical cumulative distribution function curves for Markov Chain Monte Carlo (MCMC) results of <italic toggle="yes">&#x003b1;<sub>&#x003ba;</sub></italic>s and <italic toggle="yes">&#x003ba;<sub>i</sub></italic>s for each condition. Within each plot, the thick and bold line represents <italic toggle="yes">&#x003b1;<sub>&#x003ba;</sub></italic> (group-level parameter), while the thin and faint lines represent <italic toggle="yes">&#x003ba;<sub>i</sub></italic> (individual-level parameter). Consistent with the results in <xref rid="table1-20416695251314182" ref-type="table">Table 1</xref>, the &#x0201c;Process Mode&#x0201d; condition shows that the distributions of <italic toggle="yes">&#x003b1;<sub>&#x003ba;</sub></italic> and <italic toggle="yes">&#x003ba;<sub>i</sub></italic>s are generally concentrated at lower values compared to the other two conditions. (C) Plots of estimated probability density function curves overlayed on the actual data distribution. Within each plot, the gray histogram shows actual <italic toggle="yes">&#x003b8;<sub>ij</sub></italic>s&#x02019; distribution (with gray curve of kernel estimated density). Colored lines show the curves of kernel estimated density of post predictive distributions of <italic toggle="yes">&#x003b8;<sub>ij</sub></italic> (randomly sampled from MCMC results ten times for each condition). The presence of a density peak near 0 indicates that fixations were concentrated toward the front of the stimulus in all conditions. Among all conditions, only the peaks of the red lines in the &#x0201c;Process Mode&#x0201d; condition do not reach 0.4. This means that MCMC results reflected the difference in the degree of concentration of fixations between conditions in the posterior predictive distribution. (D) An integrated plot of every colored line in <xref rid="fig3-20416695251314182" ref-type="fig">Figure 3C</xref>, with them redrawn onto polar coordinates. The direction of these polar coordinates aligns with the orientation of space in <xref rid="fig3-20416695251314182" ref-type="fig">Figure 3A</xref>. To eliminate area distortion, the radius scale is taking the square root. The slight leftward deviation of the red circle from the other circles indicates a tendency similar to that observed in <xref rid="fig3-20416695251314182" ref-type="fig">Figure 3A</xref>, where, in the &#x0201c;Process Mode&#x0201d; condition, fixations tend to concentrate toward the rear of the stimulus compared to the other conditions.</p></caption><graphic xlink:href="10.1177_20416695251314182-fig3" position="float"/></fig><fig position="float" id="fig4-20416695251314182"><label>Figure 4.</label><caption><p>Images captured in the VESTA system, overlaying the stimulus and the cubes representing the differences in the total time observed at each area of the stimulus between conditions (the top left image was taken from a diagonal front perspective and the bottom left one from an upper rear perspective). Each cube is colored based on the following rule: green for more gazes in the &#x0201c;Narrative Mode&#x0201d; condition, red for more gazes in the &#x0201c;Process Mode&#x0201d; condition, and blue for more gazes in the &#x0201c;Creator Mode&#x0201d; condition. The intensity of each color component increases proportionally to the ratio of gazes. For example, a light green cube near the crouching mother's head means that participants in the &#x0201c;Narrative Mode&#x0201d; condition tend to spend much more time looking at that part of the stimuli (more precisely, keeping it within the viewing frame). The average proportion of time participants spent on each area for each condition was calculated as <italic toggle="yes">p<sub>n</sub></italic>, <italic toggle="yes">p<sub>p</sub></italic>, and <italic toggle="yes">p<sub>c</sub></italic> (respectively denoting Narrative, Process, and Creator). The color was determined based on the ratio of these three values, as shown in the color chart on the right. As the differences between all conditions decrease, the color approaches white. Cubes that are closer to white than a certain threshold are removed from the figure for clarity. This figure was created based on the calculation of intersections between lines and 3D objects using the THREE.raycaster class in the Three.js library.</p></caption><graphic xlink:href="10.1177_20416695251314182-fig4" position="float"/></fig><table-wrap position="float" id="table1-20416695251314182"><label>Table 1.</label><caption><p>Posterior summaries of Markov Chain Monte Carlo results for the H0 and H1 model.</p></caption><alternatives><graphic xlink:href="10.1177_20416695251314182-table1" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="char" char="." span="1"/><col align="char" char="." span="1"/><col align="char" char="." span="1"/><col align="char" char="." span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Model</th><th align="left" rowspan="1" colspan="1">Parameter</th><th align="left" rowspan="1" colspan="1">2.5%</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">M</italic>
</th><th align="left" rowspan="1" colspan="1">97.5%</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">SD</italic>
</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">H0</td><td rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mml-inline1" display="inline" overflow="scroll"><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>&#x003ba;</mml:mi></mml:msub></mml:math>
</inline-formula>
</td><td rowspan="1" colspan="1">0.747</td><td rowspan="1" colspan="1">0.847</td><td rowspan="1" colspan="1">0.962</td><td rowspan="1" colspan="1">0.055</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mml-inline2" display="inline" overflow="scroll"><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>&#x003ba;</mml:mi></mml:msub></mml:math>
</inline-formula>
</td><td rowspan="1" colspan="1">1.430</td><td rowspan="1" colspan="1">1.548</td><td rowspan="1" colspan="1">1.670</td><td rowspan="1" colspan="1">0.061</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mml-inline3" display="inline" overflow="scroll"><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>&#x003bc;</mml:mi></mml:msub></mml:math>
</inline-formula>
</td><td rowspan="1" colspan="1">0.336</td><td rowspan="1" colspan="1">0.384</td><td rowspan="1" colspan="1">0.440</td><td rowspan="1" colspan="1">0.026</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mml-inline4" display="inline" overflow="scroll"><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>&#x003bc;</mml:mi></mml:msub></mml:math>
</inline-formula>
</td><td rowspan="1" colspan="1">0.248</td><td rowspan="1" colspan="1">0.307</td><td rowspan="1" colspan="1">0.365</td><td rowspan="1" colspan="1">0.030</td></tr><tr><td rowspan="1" colspan="1">H1</td><td rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mml-inline5" display="inline" overflow="scroll"><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>&#x003ba;</mml:mi></mml:msub></mml:math>
</inline-formula>
</td><td rowspan="1" colspan="1">0.721</td><td rowspan="1" colspan="1">0.820</td><td rowspan="1" colspan="1">0.934</td><td rowspan="1" colspan="1">0.055</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mml-inline6" display="inline" overflow="scroll"><mml:msubsup><mml:mi>&#x003b1;</mml:mi><mml:mi>&#x003ba;</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:math>
</inline-formula>
</td><td rowspan="1" colspan="1">1.519</td><td rowspan="1" colspan="1">1.724</td><td rowspan="1" colspan="1">1.929</td><td rowspan="1" colspan="1">0.105</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mml-inline7" display="inline" overflow="scroll"><mml:msubsup><mml:mi>&#x003b4;</mml:mi><mml:mi>&#x003ba;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:math>
</inline-formula>
</td><td rowspan="1" colspan="1">&#x02212;0.739</td><td rowspan="1" colspan="1">&#x02212;0.466</td><td rowspan="1" colspan="1">&#x02212;0.195</td><td rowspan="1" colspan="1">0.139</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mml-inline8" display="inline" overflow="scroll"><mml:msubsup><mml:mi>&#x003b4;</mml:mi><mml:mi>&#x003ba;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:math>
</inline-formula>
</td><td rowspan="1" colspan="1">&#x02212;0.287</td><td rowspan="1" colspan="1">0.003</td><td rowspan="1" colspan="1">0.300</td><td rowspan="1" colspan="1">0.149</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mml-inline9" display="inline" overflow="scroll"><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>&#x003bc;</mml:mi></mml:msub></mml:math>
</inline-formula>
</td><td rowspan="1" colspan="1">0.336</td><td rowspan="1" colspan="1">0.385</td><td rowspan="1" colspan="1">0.441</td><td rowspan="1" colspan="1">0.027</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mml-inline10" display="inline" overflow="scroll"><mml:msubsup><mml:mi>&#x003b1;</mml:mi><mml:mi>&#x003bc;</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:math>
</inline-formula>
</td><td rowspan="1" colspan="1">0.246</td><td rowspan="1" colspan="1">0.344</td><td rowspan="1" colspan="1">0.444</td><td rowspan="1" colspan="1">0.050</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mml-inline11" display="inline" overflow="scroll"><mml:msubsup><mml:mi>&#x003b4;</mml:mi><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:math>
</inline-formula>
</td><td rowspan="1" colspan="1">&#x02212;0.181</td><td rowspan="1" colspan="1">&#x02212;0.042</td><td rowspan="1" colspan="1">0.100</td><td rowspan="1" colspan="1">0.072</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mml-inline12" display="inline" overflow="scroll"><mml:msubsup><mml:mi>&#x003b4;</mml:mi><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:math>
</inline-formula>
</td><td rowspan="1" colspan="1">&#x02212;0.216</td><td rowspan="1" colspan="1">&#x02212;0.074</td><td rowspan="1" colspan="1">0.066</td><td rowspan="1" colspan="1">0.073</td></tr></tbody></table></alternatives></table-wrap><p>In the Bayesian estimation of the H0 (without any assumption of differences across conditions) and H1 (assuming differences across conditions) models, the convergence of the Markov chain Monte Carlo chains was validated by <italic toggle="yes">R-hat</italic> statistics (all <italic toggle="yes">R-hat</italic> values were less than 1.01). <xref rid="table1-20416695251314182" ref-type="table">Table 1</xref> presents the estimated group-level parameters (for details of Markov Chain Monte Carlo [MCMC] procedures, see <ext-link xlink:href="https://journals.sagepub.com/doi/suppl/10.1177/20416695251314182" ext-link-type="uri">Supplemental Table S4</ext-link>). The BF<sub>10</sub> calculated for 10 iterations ranged from 88.488 to 4189.818, indicating that the H1 model is better at explaining the data compared to the H0 model (this follows the recommendation from <xref rid="bibr41-20416695251314182" ref-type="bibr">Veenman et al. (2023)</xref> to conduct stability analysis when using bridge sampling, and these 10 iterations&#x02014;the process including both MCMC and bridge sampling&#x02014;were run solely for BF calculation). Considering that the model comparison favored the H1 model and that the estimated parameter <italic toggle="yes">&#x003b1;<sub>&#x003ba;</sub></italic> (indicating the concentration of angles in the von Mises distribution) was relatively lower in the &#x0201c;Process Mode&#x0201d; condition, it became apparent from the data that each participant in the &#x0201c;Process Mode&#x0201d; condition viewed the stimulus from a wider range of angles (<xref rid="fig3-20416695251314182" ref-type="fig">Figure 3B</xref>, <xref rid="fig3-20416695251314182" ref-type="fig">3C</xref>, and <xref rid="fig3-20416695251314182" ref-type="fig">3D</xref>).</p></sec></sec></sec><sec sec-type="discussion" id="section4-20416695251314182"><title>Discussion</title><p>Our experiment aimed to capture the changes in virtual art-viewing processes elicited by different contexts. The main results are two. First, the psychometric analysis (MANOVA) shows that the three groups of participants had different patterns of responses to a group of questions about affective and cognitive states. Second, the coordinate analyses (ANCOVA for total traveled distance and hierarchical Bayesian analysis of directional data) show that the three groups of participants had different patterns of viewpoint movement in a virtual space. These main results support our hypotheses, proving the emergence of distinct &#x0201c;modes&#x0201d; of viewing for each condition in the current experimental design.</p><p>As for psychometric analysis, we also found implications from the explorative post-hoc analyses. Participants in the &#x0201c;Process Mode&#x0201d; condition exhibited higher empathy toward the sculptor and more expanded imagination from the artwork than those in the &#x0201c;Narrative Mode&#x0201d; condition. These results align with <xref rid="bibr26-20416695251314182" ref-type="bibr">Matsumoto and Okada's (2021b)</xref> results, where a focus on the creative process significantly facilitated empathy, while imagination increased only with a significant trend. In contrast, participants in the &#x0201c;Creator Mode&#x0201d; condition exhibited higher empathy and interest than those in the &#x0201c;Narrative Mode&#x0201d; condition. Due to the lack of similar previous studies in the &#x0201c;Creator Mode&#x0201d; condition and the inability to definitively determine significance from a multiple comparison perspective, the latter comparison is more challenging to interpret. Nevertheless, these results seem to partially reflect the differentiation in the participants&#x02019; impressions of the artwork induced by the current experimental intervention for three conditions. Interestingly, we found no significant effects for liking and admiration, which were expected to be facilitated by focusing on the creative process. This might be because the &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Creator Mode&#x0201d; conditions, which were compared to the &#x0201c;Process Mode&#x0201d; condition, had their own effects in promoting liking and admiration for the artwork, unlike the contrast group in previous studies (<xref rid="bibr25-20416695251314182" ref-type="bibr">Matsumoto &#x00026; Okada, 2021a</xref>, <xref rid="bibr26-20416695251314182" ref-type="bibr">2021b</xref>).</p><p>Regarding the coordinate analyses, we emphasize that they offer information about changes within each condition that could not be addressed through psychometric analyses. Based on the analysis of viewpoints&#x02019; traveled distance, participants in the &#x0201c;Process Mode&#x0201d; condition moved their perspectives over longer distances within the virtual space, especially compared to the participants in the &#x0201c;Creator Mode&#x0201d; condition. Furthermore, according to the directional analysis, participants in the &#x0201c;Process Mode&#x0201d; condition tended to approach the artwork from a broader perspective rather than focusing on it from a single direction. These characteristics of participants&#x02019; moving patterns in the &#x0201c;Process Mode&#x0201d; condition suggest that they explored the surroundings of the presented artwork more extensively, fueled by a unique interest sparked by the experimental intervention.</p><p>Why did participants in the &#x0201c;Process Mode&#x0201d; condition tend to explore a virtual space more widely? When compared to those in the &#x0201c;Narrative Mode&#x0201d; condition, this can be easily interpreted. In many cases, informative locations for viewers focusing on a narrative aspect in visual art, such as characters&#x02019; facial expression, do not disperse throughout a work but converge in a few areas (although we could have an exceptional setting, such as viewing Gothic sculptures featuring numerous figures). Our setting of the &#x0201c;Narrative Mode&#x0201d; condition is no exception as the priming text was likely to lead viewers to pay attention to the story behind the scenes and they could perceive the relationships or expressions of each character by looking at the work directly from the front. Accordingly, participants in the &#x0201c;Narrative Mode&#x0201d; condition spontaneously stood in front of the artwork and remained there, continuing to observe. In contrast, in the &#x0201c;Process Mode&#x0201d; condition, participants were likely to pay attention to parts of the artwork that evoked its creative process in their minds. Those parts, such as surface texture or unevenness, thickness, curvature, and overall arrangement, are ubiquitously found throughout the work. Therefore, in the current setting, it seems natural that viewers under a &#x0201c;process mode&#x0201d; are motivated to explore a wider space to collect dispersed information.</p><p>Conversely, when comparing &#x0201c;Creator Mode&#x0201d; and &#x0201c;Process Mode,&#x0201d; the difference is more difficult to interpret. Participants in the &#x0201c;Creator Mode&#x0201d; could have yielded results closer to those in the &#x0201c;Process Mode&#x0201d; as both conditions seemed to provide the creator's perspective, but it did not happen. One possible reason is that participants in the &#x0201c;Creator Mode&#x0201d; condition might have been focusing on a narrative aspect of artwork as a &#x0201c;dramatist&#x0201d; rather than as a &#x0201c;sculptor.&#x0201d; If so, participants in the &#x0201c;Creator Mode&#x0201d; condition might not have been interested in what kind of physical creative process was behind the sculpture but might have been more interested in the narrative or mental aspect of the creative process, such as how unique the story of the scene was or how it can be further modified (for segmenting the aspects of the process of creation, see <xref rid="bibr25-20416695251314182" ref-type="bibr">Matsumoto and Okada (2021a)</xref>). Thus, the perspective brought about by the intervention in the &#x0201c;Creator Mode&#x0201d; condition can be closer to that in the &#x0201c;Narrative Mode&#x0201d; than to that of the &#x0201c;Process Mode.&#x0201d; In addition, the physical aspect of creating plaster sculptures is rarely known and thus attracts less attention without being provided with specific knowledge. Considering that novices tend to pay attention to semantic content rather than formal elements (<xref rid="bibr3-20416695251314182" ref-type="bibr">Bullot &#x00026; Reber, 2013</xref>; <xref rid="bibr8-20416695251314182" ref-type="bibr">Cupchik, 1992</xref>; <xref rid="bibr9-20416695251314182" ref-type="bibr">Cupchik &#x00026; Gebotys, 1988</xref>; <xref rid="bibr29-20416695251314182" ref-type="bibr">Parsons, 1987</xref>; <xref rid="bibr32-20416695251314182" ref-type="bibr">Schmidt et al., 1989</xref>), it seems highly plausible that the participants in the &#x0201c;Creator Mode&#x0201d; condition were not led into focusing on the formal features of the artwork related to the physical aspect of the creative process. Furthermore, they might have failed to mimic the thought process of actual creators due to the lack of the knowledge about the sculpture, even though the instruction was to &#x0201c;think as a creator.&#x0201d; This is one of the limitations of this study, and addressing this issue would likely require designing longer and more systematic interventions or recruiting experts with knowledge of creation as participants.</p><p>An important aspect to note here is that the differences between conditions emerged without explicit instructions such as &#x0201c;how to view the work.&#x0201d; Differences in psychological and behavioral indicators can arise due to the instructed viewing methods (e.g., <xref rid="bibr7-20416695251314182" ref-type="bibr">Cotter et al., 2023</xref>; <xref rid="bibr26-20416695251314182" ref-type="bibr">Matsumoto &#x00026; Okada, 2021b</xref>). In contrast, in this study, particularly in the &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Process Mode&#x0201d; conditions, participants were only presented with prior information related to the artwork. The results suggest that the participants in this experiment spontaneously altered their modes of viewing based on their intrinsic interests. Furthermore, considering that changes in modes of viewing can arise depending on nuanced contextual differences, it is predicted that such changes might occur frequently in daily life.</p><p>It should also be noted that the differences between conditions observed in this study are not very large, as can be seen from the plots in <xref rid="fig3-20416695251314182" ref-type="fig">Figure 3</xref>. This is likely attributable to the unobtrusive nature of the instructions used. Since the primary focus of this research is not to identify interventions that produce large effects, but rather to demonstrate the effectiveness of the methodology, the small effect sizes successfully highlight the high sensitivity of this approach.</p><sec id="section4A-20416695251314182"><title>Concept of Mode</title><p>This study focused on the concept of &#x0201c;mode&#x0201d; of viewing, which can effectively explain how an individual's art experience varies depending on the situation. Moreover, it is suitable for capturing changes that encompass multiple aspects (positive or negative) rather than a single-dimensional change. Efforts to examine esthetic experiences through multidimensional measures, which have emerged in recent years (e.g., <xref rid="bibr15-20416695251314182" ref-type="bibr">Hosoya, 2020</xref>; <xref rid="bibr17-20416695251314182" ref-type="bibr">Kenett et al., 2023</xref>; <xref rid="bibr28-20416695251314182" ref-type="bibr">Nummenmaa &#x00026; Hari, 2023</xref>; <xref rid="bibr35-20416695251314182" ref-type="bibr">Specker et al., 2021</xref>), may also provide further benefit by associating individual multidimensional patterns with different modes. Researchers could also consider incorporating physical indicators into this measure, as in this study. One example of the &#x0201c;mode&#x0201d; and its accompanying changes in multiple indicators can be described as follows: when individuals enter a &#x0201c;mode&#x0201d; focusing on the artwork's process (as long as the viewing situation is akin to the current one), they will be engaged in physically extensive exploration and have a specific structure of mental state, such as high empathy toward the artist. Systematically exploring how variables are causally related to each other within a single mode and identifying other conceivable modes remains a challenge for future research.</p><p>It is important to note that these modes may be more constant than those assumed in this study. Although each condition was associated with a single dominant mode in this experiment, at the microlevel, the modes may shift incessantly throughout the viewing period. For example, participants in the &#x0201c;Process Mode&#x0201d; condition may not always be focused on the process of creating the artwork; in some moments, their attention is likely to shift to considering what scene the sculpture represents in a story. Future research is required to elucidate the temporal dynamics of the transition of one mode into another.</p><p>Another point to note is that we can conceive of different levels of &#x0201c;modes.&#x0201d; The terms &#x0201c;Narrative Mode&#x0201d; and &#x0201c;Process Mode&#x0201d; refer to what individuals focus on, whereas the term &#x0201c;Creator Mode&#x0201d; refers to who is viewing or what kind of (or in what state) person is viewing. Two modes defined at different levels refer to the same phenomenon or conceptually overlap despite their apparent difference. Actual creators, when entering the &#x0201c;Creator Mode&#x0201d; to view others&#x02019; works, may process the artwork from multiple perspectives, potentially switching between the Narrative Mode, Process Mode, or even other modes. Therefore, it is necessary to consider the hierarchical relationship of the modes in future research.</p></sec><sec id="section4B-20416695251314182"><title>Scope of the Methodology</title><p>In this study, we implemented a novel methodology for investigating art-viewing behavior in virtual spaces, which are characterized by two elements: (a) measuring and analyzing viewers&#x02019; movements and (b) utilizing non-immersive VR. As a result of this methodology, we were able to capture changes in the participants&#x02019; modes of viewing, which were clearer and more informative than what could be inferred from psychological indicators alone. Therefore, the efficacy of this methodology was sufficiently demonstrated.</p><p>Based on our methodology, we could envision further research endeavors. For example, as non-immersive VR allows numerous participants to participate in an experiment, researchers may use machine learning to accurately predict psychological states (such as interest or boredom) or subsequent behavioral patterns (such as when to stop engaging with the artwork) from movement indicators. Furthermore, it can even be applied to collective viewing behaviors in social groups of two or more individuals. Beyond the framework of scientific research, we believe that it has practical applications in real-world settings. In particular, as an increasing number of virtual viewing situations are created by museums and tourism service providers, a detailed analysis of the individual viewing process using the VESTA system or something similar could be of value both commercially and educationally. In the big picture, the scope of this methodology is not limited to typical art appreciation, but it may also have other applications. Cognitive scientists interested in any cognitive process related to active searching can easily incorporate this methodology into their experiments. If we consider an educational setting, we can gather valuable insights into each learner's attention or identify which parts of a displayed object are overlooked by the majority. Other kinds of virtual spaces such as the metaverse which are already widely used as services would also be included within the scope of this methodology.</p><p>Although this study primarily focused on the art-viewing in virtual spaces, the insights gained from this methodology are expected to contribute to examining esthetic experiences in the real world. This expectation is corroborated by the finding that people's reaction in the &#x0201c;Process Mode&#x0201d; condition of the present experiment closely aligns with theoretical predictions from prior research conducted in real-world settings. Furthermore, as already noted in the introduction, it is worth highlighting that non-immersive VR closely resembles the conventional viewing conditions conducted in traditional research on 2D monitors, as opposed to immersive VR. Nonetheless, it is essential to proceed with caution and carefully verify the extent to which generalizations are possible in future studies. In particular, a significant distinction ought to be made with regard to bodily sensations between walking around an artwork and rotating the camera. This underscores the importance of examining the gaps between virtual and real bodily perceptions.</p><p>We should also note that the current methodology makes differences in movement clearer in some cases; conversely, it lets differences in psychological indicators likely emerge in other cases. This will depend on the type of artwork and mode of viewing. To overcome this issue, it is important to conduct a multifaceted assessment that integrates psychological measures and movement, as has been done in this study.</p></sec><sec id="section4C-20416695251314182"><title>Limitations</title><p>This study has two main limitations. First, it focuses on experiences with artworks in non-immersive VR, which differ from experiences with the real environments, their 2D representations, or immersive VR environments. Specifically, using virtual spaces enables movements that are difficult to achieve in reality, such as viewing large artworks directly from above or below. This likely leads to differences in both movement trajectories and psychological experiences. Moreover, while immersive VR can cause discomfort when moving quickly through space, similar to real-life experiences, non-immersive VR is less likely to produce such effects. This could result in differences in the speed and range of exploratory behaviors between them. Therefore, the differences in exhibition formats should not be overlooked, and this study did not empirically examine these distinctions. Researchers should be careful not to overgeneralize the findings derived from the current methodology.</p><p>Second, the geometric analysis used in this study is just one of the potentially numerous ways to process the data. For example, since the viewpoint coordinates are recorded with time information, a time-series model could also be applied. In future research based on the current methodology, its authors may need to develop analysis methods tailored to their specific topics for themselves. While the richness of the data allows for flexible analysis, it also imposes a burden on researchers to determine the appropriate analytical approach. In the future, a systematic and comprehensive overview of possible analyses for data obtained through the current methodology will be necessary.</p></sec><sec id="section5-20416695251314182"><title>Conclusion</title><p>Through the investigation of changes in people's &#x0201c;modes&#x0201d; of viewing, we successfully demonstrated the effectiveness of integrating measurements of movement with non-immersive VR in examining art-viewing experiences. This methodology not only provides insights into virtual viewing experiences, but it is also likely to offer valuable implications for real-world viewing experiences in a more efficient way. In particular, analyzing how the physical &#x0201c;viewpoint&#x0201d; is positioned in space provides a powerful means to infer what kind of mental &#x0201c;viewpoint,&#x0201d; or what kind of &#x0201c;mode&#x0201d; of viewing, people have in their minds. With the current methodology being both applicable for different areas and sharp for specific research questions, this study paves the way for future research.</p></sec></sec><sec sec-type="supplementary-material" id="section6-20416695251314182" specific-use="figshare"><title>Supplemental Material</title><supplementary-material id="suppl1-20416695251314182" position="float" content-type="local-data"><caption><title>sj-docx-1-ipe-10.1177_20416695251314182 - Supplemental material for Visualizing the change of &#x0201c;viewpoints&#x0201d; in 3D virtual art exhibition</title></caption><media xlink:href="sj-docx-1-ipe-10.1177_20416695251314182.docx"/><p>Supplemental material, sj-docx-1-ipe-10.1177_20416695251314182 for Visualizing the change of &#x0201c;viewpoints&#x0201d; in 3D virtual art exhibition by Kazuki Matsumoto and Takeshi Okada in i-Perception</p></supplementary-material></sec></body><back><ack><title>Acknowledgements</title><p>We thank Editage (<ext-link xlink:href="www.editage.jp" ext-link-type="uri">www.editage.jp</ext-link>) for English language editing.</p></ack><fn-group><fn fn-type="con"><p><bold>Author Contribution(s):</bold>
<bold>Kazuki Matsumoto:</bold> Conceptualization; Data curation; Formal analysis; Investigation; Methodology; Project administration; Resources; Software; Visualization; Writing &#x02013; original draft; Writing &#x02013; review &#x00026; editing.</p><p><bold>Takeshi Okada:</bold> Conceptualization; Project administration; Supervision; Validation; Writing &#x02013; review &#x00026; editing.</p></fn><fn fn-type="COI-statement"><p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></fn><fn fn-type="financial-disclosure"><p><bold>Funding:</bold> The authors received no financial support for the research, authorship, and/or publication of this article.</p></fn><fn fn-type="other"><p><bold>ORCID iD:</bold> Kazuki Matsumoto <ext-link xlink:href="https://orcid.org/0009-0008-4858-8196" ext-link-type="uri">https://orcid.org/0009-0008-4858-8196</ext-link></p></fn><fn fn-type="other"><p><bold>Supplemental Material:</bold> Supplemental material for this article is available online.</p></fn></fn-group><notes><fn-group><fn id="fn1-20416695251314182"><label>1.</label><p>Some previous studies have used nonimmersive VR in the context of art. Most have been conducted as visitor studies in virtual museums or virtual galleries and have succeeded in demonstrating the usefulness of the method (e.g., <xref rid="bibr7-20416695251314182" ref-type="bibr">Cotter et al., 2023</xref>; <xref rid="bibr37-20416695251314182" ref-type="bibr">Sundar et al., 2015</xref>; <xref rid="bibr38-20416695251314182" ref-type="bibr">Sylaiou et al., 2017</xref>). Among them, an exceptional study analyzed movement data while viewing artworks (<xref rid="bibr30-20416695251314182" ref-type="bibr">Rodriguez-Boerwinkle, 2023</xref>), as proposed in the current study. Our methodology could be seen as providing theoretical grounding with new empirical data to this pioneering practice. From another point of view, <xref rid="bibr30-20416695251314182" ref-type="bibr">Rodriguez-Boerwinkle (2023)</xref> focused on implementing a gallery-like space where multiple artworks were exhibited and which was closer to the virtual spaces actually in operation, such as the museums&#x02019; websites. In contrast, the current study conducted experiments focusing on participants&#x02019; responses to a single artwork as traditional empirical aesthetic studies have done, primarily exploring the way to extend existing methodologies adopted by psychologists.</p></fn><fn id="fn2-20416695251314182"><label>2.</label><p>This work is based on &#x0201c;The Death and the Mother&#x0201d; (<ext-link xlink:href="https://sketchfab.com/3d-models/the-death-and-the-mother-793a94b673334484b1bcfbd20c8bae19" ext-link-type="uri">https://sketchfab.com/3d-models/the-death-and-the-mother-793a94b673334484b1bcfbd20c8bae19</ext-link>) by Geoffrey Marchal (<ext-link xlink:href="https://sketchfab.com/geoffreymarchal" ext-link-type="uri">https://sketchfab.com/geoffreymarchal</ext-link>), licensed under CC-BY-NC-4.0 (<ext-link xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>).</p></fn><fn id="fn3-20416695251314182"><label>3.</label><p>In this study, we calculated the view-angle &#x003b8; (&#x02212;&#x003c0; &#x02266; &#x003b8;&#x02009;&#x0003c;&#x02009;&#x003c0;) using two-dimensional coordinates on a plane parallel to the ground where the stimulus was placed, with the origin [0, 0] (the center of the stimulus) as the reference point. The default angle set when entering the VESTA system page, where the camera is directly in front of the sculpture (as the above image in <xref rid="fig1-20416695251314182" ref-type="fig">Figure 1</xref>), was defined as 0. From this starting position, rotating the camera counterclockwise by 90&#x000b0; around the artwork's center (viewed from above) corresponds to &#x003b8;&#x02009;=&#x02009;1/2&#x003c0;, and rotating 90&#x000b0; in the opposite direction corresponds to &#x003b8;&#x02009;=&#x02009;&#x02212;1/2&#x003c0;.</p></fn></fn-group></notes><ref-list><title>References</title><ref id="bibr1-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Barbieri</surname><given-names>L.</given-names></name>
<name><surname>Bruno</surname><given-names>F.</given-names></name>
<name><surname>Muzzupappa</surname><given-names>M.</given-names></name>
</person-group> (<year>2017</year>). <article-title>Virtual museum system evaluation through user studies</article-title>. <source>Journal of Cultural Heritage</source>, <volume>26</volume>, <fpage>101</fpage>&#x02013;<lpage>108</lpage>.
<pub-id pub-id-type="doi">10.1016/j.culher.2017.02.005</pub-id>
</mixed-citation></ref><ref id="bibr2-20416695251314182"><mixed-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Beer</surname><given-names>S.</given-names></name>
</person-group> (<comment>2015, April</comment>). <conf-name>Virtual museums: An innovative kind of museum survey</conf-name>. In <conf-name>Proceedings of the 2015 Virtual Reality International Conference</conf-name>, pp. <fpage>1</fpage>&#x02013;<lpage>6</lpage>.</mixed-citation></ref><ref id="bibr3-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bullot</surname><given-names>N. J.</given-names></name>
<name><surname>Reber</surname><given-names>R.</given-names></name>
</person-group> (<year>2013</year>). <article-title>The artful mind meets art history: Toward a psycho-historical framework for the science of art appreciation</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>36</volume>(<issue>2</issue>), <fpage>123</fpage>&#x02013;<lpage>137</lpage>.
<pub-id pub-id-type="doi">10.1017/S0140525X12000489</pub-id>
<pub-id pub-id-type="pmid">23507091</pub-id>
</mixed-citation></ref><ref id="bibr4-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Carbon</surname><given-names>C. C.</given-names></name>
</person-group> (<year>2019</year>). <article-title>Empirical approaches to studying art experience</article-title>. <source>Journal of Perceptual Imaging: JPI</source>, <volume>2</volume>(<issue>1</issue>), <fpage>10501</fpage>. <pub-id pub-id-type="doi">10.2352/J.Percept.Imaging.2019.2.1.010501</pub-id></mixed-citation></ref><ref id="bibr5-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Carbon</surname><given-names>C. C.</given-names></name>
</person-group> (<year>2020</year>). <article-title>Ecological art experience: How we can gain experimental control while preserving ecologically valid settings and contexts</article-title>. <source>Frontiers in Psychology</source>, <volume>11</volume>, Article <fpage>800</fpage>.
<pub-id pub-id-type="doi">10.3389/fpsyg.2020.00800</pub-id>
<pub-id pub-id-type="pmid">32499736</pub-id>
</mixed-citation></ref><ref id="bibr6-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Carpenter</surname><given-names>B.</given-names></name>
<name><surname>Gelman</surname><given-names>A.</given-names></name>
<name><surname>Hoffman</surname><given-names>M. D.</given-names></name>
<name><surname>Lee</surname><given-names>D.</given-names></name>
<name><surname>Goodrich</surname><given-names>B.</given-names></name>
<name><surname>Betancourt</surname><given-names>M.</given-names></name>
<name><surname>Brubaker</surname><given-names>M. A.</given-names></name>
<name><surname>Guo</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>P.</given-names></name>
<name><surname>Riddell</surname><given-names>A.</given-names></name>
</person-group> (<year>2017</year>). <article-title>Stan: A probabilistic programming language</article-title>. <source>Journal of Statistical Software</source>, <volume>76</volume>(<issue>1</issue>), <fpage>1</fpage>&#x02013;<lpage>32</lpage>. <pub-id pub-id-type="doi">10.18637/jss.v076.i01</pub-id><pub-id pub-id-type="pmid">36568334</pub-id>
</mixed-citation></ref><ref id="bibr7-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cotter</surname><given-names>K. N.</given-names></name>
<name><surname>Harrouche</surname><given-names>M.</given-names></name>
<name><surname>Rodriguez-Boerwinkle</surname><given-names>R. M.</given-names></name>
<name><surname>Boerwinkle</surname><given-names>M.</given-names></name>
<name><surname>Silvia</surname><given-names>P. J.</given-names></name>
<name><surname>Pawelski</surname><given-names>J. O.</given-names></name>
</person-group> (<year>2023</year>). <article-title>Virtual art visits: Examining the effects of slow looking on well-being in an online environment</article-title>. <source>Psychology of Aesthetics, Creativity, and the Arts. Advance online publication</source>.
<pub-id pub-id-type="doi">10.1037/aca0000548</pub-id>
</mixed-citation></ref><ref id="bibr8-20416695251314182"><mixed-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Cupchik</surname><given-names>G. C.</given-names></name>
</person-group> (<year>1992</year>). <part-title>From perception to production: A multilevel analysis of the aesthetic process</part-title>. In <person-group person-group-type="editor">
<name><surname>Cupchik</surname><given-names>G. C.</given-names></name>
<name><surname>L&#x000e1;szl&#x000f3;</surname><given-names>J.</given-names></name>
</person-group> (Eds.), <source>Emerging visions of the aesthetic process: Psychology, semiology, and philosophy</source> (pp. <fpage>61</fpage>&#x02013;<lpage>81</lpage>). <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref><ref id="bibr9-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cupchik</surname><given-names>G. C.</given-names></name>
<name><surname>Gebotys</surname><given-names>R. J.</given-names></name>
</person-group> (<year>1988</year>). <article-title>The search for meaning in art: Interpretive styles and judgments of quality</article-title>. <source>Visual Arts Research</source>, <volume>14</volume>(<issue>2</issue>), <fpage>38</fpage>&#x02013;<lpage>50</lpage>.</mixed-citation></ref><ref id="bibr10-20416695251314182"><mixed-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Geismar</surname><given-names>H.</given-names></name>
</person-group> (<year>2018</year>). <source>Museum object lessons for the digital age</source>. <publisher-name>UCL Press</publisher-name>.</mixed-citation></ref><ref id="bibr11-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gotthardt</surname><given-names>K. A.</given-names></name>
<name><surname>Rakoczy</surname><given-names>K.</given-names></name>
<name><surname>Tallon</surname><given-names>M.</given-names></name>
<name><surname>Seitz</surname><given-names>M.</given-names></name>
<name><surname>Frick</surname><given-names>U.</given-names></name>
</person-group> (<year>2025</year>). <article-title>Can virtual art touch your heart?&#x02014;The impact of virtual reality art on affect considering individual characteristics and aesthetic experiences</article-title>. <source>Empirical Studies of the Arts</source>, <volume>43</volume>(<issue>1</issue>), <fpage>319</fpage>&#x02013;<lpage>354</lpage>. <pub-id pub-id-type="doi">10.1177/02762374231221920</pub-id></mixed-citation></ref><ref id="bibr12-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gronau</surname><given-names>Q.</given-names></name>
<name><surname>Singmann</surname><given-names>H.</given-names></name>
<name><surname>Wagenmakers</surname><given-names>E. J.</given-names></name>
</person-group> (<year>2020</year>). <article-title>bridgesampling: An R package for estimating normalizing constants</article-title>. <source>Journal of Statistical Software</source>, <volume>92</volume>(<issue>10</issue>), <fpage>1</fpage>&#x02013;<lpage>29</lpage>. <pub-id pub-id-type="doi">10.18637/jss.v092.i10</pub-id></mixed-citation></ref><ref id="bibr13-20416695251314182"><mixed-citation publication-type="book"><person-group person-group-type="author">
<name><surname>He</surname><given-names>H.</given-names></name>
<name><surname>Xu</surname><given-names>X.</given-names></name>
<name><surname>Bueno-Vesga</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Gu</surname><given-names>Y.</given-names></name>
</person-group> (<year>2023</year>). <part-title>Outdated or not? A case study of how 3D desktop VR is accepted today</part-title>. In <source>International conference on immersive learning</source> (pp. <fpage>150</fpage>&#x02013;<lpage>160</lpage>). <publisher-name>Springer Nature Switzerland</publisher-name>.</mixed-citation></ref><ref id="bibr14-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hitsuwari</surname><given-names>J.</given-names></name>
<name><surname>Nomura</surname><given-names>M.</given-names></name>
</person-group> (<year>2023</year>). <article-title>Interaction between creation and appreciation: How linguistic art creation impacts the aesthetic evaluation of haiku poetry and ink paintings</article-title>. <source>International Journal of Psychology</source>, <volume>59</volume>(<issue>1</issue>), <fpage>155</fpage>&#x02013;<lpage>162</lpage>.
<pub-id pub-id-type="doi">10.1002/ijop.12959</pub-id>
<pub-id pub-id-type="pmid">37858958</pub-id>
</mixed-citation></ref><ref id="bibr15-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hosoya</surname><given-names>G.</given-names></name>
</person-group> (<year>2020</year>). <article-title>The artwork and the beholder: A probabilistic model for the joint scaling of persons and objects</article-title>. <source>Psychology of Aesthetics, Creativity, and the Arts</source>, <volume>14</volume>(<issue>2</issue>), <fpage>224</fpage>&#x02013;<lpage>236</lpage><pub-id pub-id-type="doi">10.1037/aca0000239</pub-id></mixed-citation></ref><ref id="bibr16-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jankovi&#x00107;</surname><given-names>D.</given-names></name>
<name><surname>Ma&#x00111;arev</surname><given-names>M.</given-names></name>
<name><surname>Dimoski</surname><given-names>J.</given-names></name>
<name><surname>Engler</surname><given-names>M.</given-names></name>
<name><surname>&#x00160;tuli&#x00107;</surname><given-names>V.</given-names></name>
</person-group> (<year>2020</year>). <article-title>Visual art and VR: Experience of art exhibition in VR and real-world setting</article-title>. <source>Proceedings of the XXVI Scientific Conference Empirical Studies in Psychology</source>, <fpage>100</fpage>&#x02013;<lpage>102</lpage>.</mixed-citation></ref><ref id="bibr17-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kenett</surname><given-names>Y. N.</given-names></name>
<name><surname>Cardillo</surname><given-names>E. R.</given-names></name>
<name><surname>Christensen</surname><given-names>A. P.</given-names></name>
<name><surname>Chatterjee</surname><given-names>A.</given-names></name>
</person-group> (<year>2023</year>). <article-title>Aesthetic emotions are affected by context: A psychometric network analysis</article-title>. <source>Scientific Reports</source>, <volume>13</volume>(<issue>1</issue>), Article <fpage>20985</fpage>.
<pub-id pub-id-type="doi">10.1038/s41598-023-48219-w</pub-id>
<pub-id pub-id-type="pmid">38017110</pub-id>
</mixed-citation></ref><ref id="bibr18-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>K&#x000fc;hnapfel</surname><given-names>C.</given-names></name>
<name><surname>Fingerhut</surname><given-names>J.</given-names></name>
<name><surname>Brinkmann</surname><given-names>H.</given-names></name>
<name><surname>Ganster</surname><given-names>V.</given-names></name>
<name><surname>Tanaka</surname><given-names>T.</given-names></name>
<name><surname>Specker</surname><given-names>E.</given-names></name>
<name><surname>Pelowski</surname><given-names>M.</given-names></name>
</person-group> (<year>2024</year>). <article-title>How do we move in front of art? How does this relate to art experience? Linking movement, eye tracking, emotion, and evaluations in a gallery-like setting</article-title>. <source>Empirical Studies of the Arts</source>, <volume>42</volume>(<issue>1</issue>), <fpage>86</fpage>&#x02013;<lpage>146</lpage>.
<pub-id pub-id-type="doi">10.1177/02762374231160000</pub-id>
</mixed-citation></ref><ref id="bibr19-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Leder</surname><given-names>H.</given-names></name>
<name><surname>B&#x000e4;r</surname><given-names>S.</given-names></name>
<name><surname>Topolinski</surname><given-names>S.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Covert painting simulations influence aesthetic appreciation of artworks</article-title>. <source>Psychological Science</source>, <volume>23</volume>(<issue>12</issue>), <fpage>1479</fpage>&#x02013;<lpage>1481</lpage>.
<pub-id pub-id-type="doi">10.1177/0956797612452866</pub-id>
<pub-id pub-id-type="pmid">23137968</pub-id>
</mixed-citation></ref><ref id="bibr20-20416695251314182"><mixed-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>M. D.</given-names></name>
<name><surname>Wagenmakers</surname><given-names>E. J.</given-names></name>
</person-group> (<year>2014</year>). <source>Bayesian cognitive modeling: A practical course</source>. <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref><ref id="bibr21-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Leopardi</surname><given-names>A.</given-names></name>
<name><surname>Ceccacci</surname><given-names>S.</given-names></name>
<name><surname>Mengoni</surname><given-names>M.</given-names></name>
<name><surname>Naspetti</surname><given-names>S.</given-names></name>
<name><surname>Gambelli</surname><given-names>D.</given-names></name>
<name><surname>Ozturk</surname><given-names>E.</given-names></name>
<name><surname>Zanoli</surname><given-names>R.</given-names></name>
</person-group> (<year>2021</year>). <article-title>X-reality technologies for museums: A comparative evaluation based on presence and visitors experience through user studies</article-title>. <source>Journal of Cultural Heritage</source>, <volume>47</volume>, <fpage>188</fpage>&#x02013;<lpage>198</lpage>.
<pub-id pub-id-type="doi">10.1016/j.culher.2020.10.005</pub-id>
</mixed-citation></ref><ref id="bibr22-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>C. L.</given-names></name>
<name><surname>Chen</surname><given-names>S. J.</given-names></name>
<name><surname>Lin</surname><given-names>R.</given-names></name>
</person-group> (<year>2020</year>). <article-title>Efficacy of virtual reality in painting art exhibitions appreciation</article-title>. <source>Applied Sciences</source>, <volume>10</volume>(<issue>9</issue>), Article <fpage>3012</fpage>.
<pub-id pub-id-type="doi">10.3390/app10093012</pub-id>
</mixed-citation></ref><ref id="bibr23-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Maglio</surname><given-names>S. J.</given-names></name>
<name><surname>Trope</surname><given-names>Y.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Disembodiment: Abstract construal attenuates the influence of contextual bodily state in judgment</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>141</volume>(<issue>2</issue>), <fpage>211</fpage><lpage>&#x02013;216</lpage>. <pub-id pub-id-type="doi">10.1037/a0024520</pub-id><pub-id pub-id-type="pmid">21767044</pub-id>
</mixed-citation></ref><ref id="bibr24-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mar&#x000ed;n-Morales</surname><given-names>J.</given-names></name>
<name><surname>Higuera-Trujillo</surname><given-names>J. L.</given-names></name>
<name><surname>Greco</surname><given-names>A.</given-names></name>
<name><surname>Guixeres</surname><given-names>J.</given-names></name>
<name><surname>Llinares</surname><given-names>C.</given-names></name>
<name><surname>Gentili</surname><given-names>C.</given-names></name>
<name><surname>Scilingo</surname><given-names>E. P.</given-names></name>
<name><surname>Alca&#x000f1;iz</surname><given-names>M.</given-names></name>
<name><surname>Valenza</surname><given-names>G.</given-names></name>
</person-group> (<year>2019</year>). <article-title>Real vs. immersive-virtual emotional experience: Analysis of psycho-physiological patterns in a free exploration of an art museum</article-title>. <source>PloS One</source>, <volume>14</volume>(<issue>10</issue>), Article <comment>e0223881</comment>.
<pub-id pub-id-type="doi">10.1371/journal.pone.0223881</pub-id>
</mixed-citation></ref><ref id="bibr25-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Matsumoto</surname><given-names>K.</given-names></name>
<name><surname>Okada</surname><given-names>T.</given-names></name>
</person-group> (<comment>2021a</comment>). <article-title>Viewers recognize the process of creating artworks with admiration: Evidence from experimental manipulation of prior experience</article-title>. <source>Psychology of Aesthetics, Creativity, and the Arts</source>, <volume>15</volume>(<issue>2</issue>), <fpage>352</fpage><lpage>&#x02013;362</lpage>.<pub-id pub-id-type="doi">10.1037/aca0000285</pub-id></mixed-citation></ref><ref id="bibr26-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Matsumoto</surname><given-names>K.</given-names></name>
<name><surname>Okada</surname><given-names>T.</given-names></name>
</person-group> (<comment>2021b</comment>). <article-title>Imagining how lines were drawn: The appreciation of calligraphy and the facilitative factor based on the viewer&#x02019;s rating and heart rate</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>15</volume>, Article <fpage>654610</fpage>.
<pub-id pub-id-type="doi">10.3389/fnhum.2021.654610</pub-id>
<pub-id pub-id-type="pmid">34276322</pub-id>
</mixed-citation></ref><ref id="bibr27-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Miura</surname><given-names>S</given-names></name>
</person-group>., <person-group person-group-type="author">
<name><surname>Kawai</surname><given-names>N</given-names></name>
</person-group>. (<year>2019</year>). <article-title>&#x07a7a;&#x09593;&#x07684;&#x0914d;&#x07f6e;&#x0304c;&#x07f8e;&#x08853;&#x04f5c;&#x054c1;&#x0306e;&#x05370;&#x08c61;&#x08a55;&#x04fa1;&#x0306b;&#x053ca;&#x0307c;&#x03059;&#x05f71;&#x097ff; [influence of spatial arrangement on impression evaluation of works of art]</article-title>. <source>&#x08a8d;&#x077e5;&#x079d1;&#x05b66;<italic toggle="yes">[</italic>Cognitive Studies<italic toggle="yes">]</italic></source>, <volume>26</volume>(<issue>1</issue>), <fpage>179</fpage>&#x02013;<lpage>183</lpage>. <pub-id pub-id-type="doi">10.11225/jcss.26.179</pub-id></mixed-citation></ref><ref id="bibr28-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nummenmaa</surname><given-names>L.</given-names></name>
<name><surname>Hari</surname><given-names>R.</given-names></name>
</person-group> (<year>2023</year>). <article-title>Bodily feelings and aesthetic experience of art</article-title>. <source>Cognition and Emotion</source>, <volume>35</volume>(<issue>3</issue>), <fpage>1</fpage>&#x02013;<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1080/02699931.2023.2183180</pub-id></mixed-citation></ref><ref id="bibr29-20416695251314182"><mixed-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Parsons</surname><given-names>M. J.</given-names></name>
</person-group> (<year>1987</year>). <source>How we understand art: A cognitive developmental account of aesthetic experience</source>. <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref><ref id="bibr30-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rodriguez-Boerwinkle</surname><given-names>R. M.</given-names></name>
<name><surname>Boerwinkle</surname><given-names>M. J.</given-names></name>
<name><surname>Silvia</surname><given-names>P. J.</given-names></name>
</person-group> (<year>2023</year>). <article-title>The open gallery for arts research (OGAR): An open-source tool for studying the psychology of virtual art museum visits</article-title>. <source>Behavior Research Methods</source>, <volume>55</volume>(<issue>2</issue>), <fpage>824</fpage>&#x02013;<lpage>842</lpage>.
<pub-id pub-id-type="doi">10.3758/s13428-022-01857-w</pub-id>
<pub-id pub-id-type="pmid">35469088</pub-id>
</mixed-citation></ref><ref id="bibr31-20416695251314182"><mixed-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Rosenberg</surname><given-names>R.</given-names></name>
<name><surname>Klein</surname><given-names>C.</given-names></name>
</person-group> (<year>2015</year>). <part-title>The moving eye of the beholder: Eye tracking and the perception of paintings</part-title>. In <person-group person-group-type="editor">
<name><surname>Huston</surname><given-names>J. P.</given-names></name>
<name><surname>Nadal</surname><given-names>M.</given-names></name>
<name><surname>Mora</surname><given-names>F.</given-names></name>
<name><surname>Agnati</surname><given-names>L. F.</given-names></name>
<name><surname>Cela-Conde</surname><given-names>C. J.</given-names></name>
</person-group> (Eds.), <source>Art, aesthetics and the brain</source> (pp. <fpage>79</fpage>&#x02013;<lpage>108</lpage>). <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref><ref id="bibr32-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>J. A.</given-names></name>
<name><surname>McLaughlin</surname><given-names>J. P.</given-names></name>
<name><surname>Leighten</surname><given-names>P.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Novice strategies for understanding paintings</article-title>. <source>Applied Cognitive Psychology</source>, <volume>3</volume>(<issue>1</issue>), <fpage>65</fpage>&#x02013;<lpage>72</lpage>.
<pub-id pub-id-type="doi">10.1002/acp.2350030107</pub-id>
</mixed-citation></ref><ref id="bibr33-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schweibenz</surname><given-names>W.</given-names></name>
</person-group> (<year>2019</year>). <article-title>The virtual museum: An overview of its origins, concepts, and terminology</article-title>. <source>The Museum Review</source>, <volume>4</volume>(<issue>1</issue>), <fpage>1</fpage>&#x02013;<lpage>29</lpage>.</mixed-citation></ref><ref id="bibr34-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shamay-Tsoory</surname><given-names>S. G.</given-names></name>
<name><surname>Mendelsohn</surname><given-names>A.</given-names></name>
</person-group> (<year>2019</year>). <article-title>Real-life neuroscience: An ecological approach to brain and behavior research</article-title>. <source>Perspectives on Psychological Science</source>, <volume>14</volume>(<issue>5</issue>), <fpage>841</fpage>&#x02013;<lpage>859</lpage>.
<pub-id pub-id-type="doi">10.1177/1745691619856350</pub-id>
<pub-id pub-id-type="pmid">31408614</pub-id>
</mixed-citation></ref><ref id="bibr35-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Specker</surname><given-names>E.</given-names></name>
<name><surname>Fried</surname><given-names>E. I.</given-names></name>
<name><surname>Rosenberg</surname><given-names>R.</given-names></name>
<name><surname>Leder</surname><given-names>H.</given-names></name>
</person-group> (<year>2021</year>). <article-title>Associating with art: A network model of aesthetic effects</article-title>. <source>Collabra: Psychology</source>, <volume>7</volume>(<issue>1</issue>), Article <fpage>24085</fpage>.
<pub-id pub-id-type="doi">10.1525/collabra.24085</pub-id>
</mixed-citation></ref><ref id="bibr36-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Styliani</surname><given-names>S.</given-names></name>
<name><surname>Fotis</surname><given-names>L.</given-names></name>
<name><surname>Kostas</surname><given-names>K.</given-names></name>
<name><surname>Petros</surname><given-names>P.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Virtual museums, a survey and some issues for consideration</article-title>. <source>Journal of Cultural Heritage</source>, <volume>10</volume>(<issue>4</issue>), <fpage>520</fpage>&#x02013;<lpage>528</lpage>.
<pub-id pub-id-type="doi">10.1016/j.culher.2009.03.003</pub-id>
</mixed-citation></ref><ref id="bibr37-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sundar</surname><given-names>S. S.</given-names></name>
<name><surname>Go</surname><given-names>E.</given-names></name>
<name><surname>Kim</surname><given-names>H. S.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
</person-group> (<year>2015</year>). <article-title>Communicating art, virtually! psychological effects of technological affordances in a virtual museum</article-title>. <source>International Journal of Human-Computer Interaction</source>, <volume>31</volume>(<issue>6</issue>), <fpage>385</fpage>&#x02013;<lpage>401</lpage>.
<pub-id pub-id-type="doi">10.1080/10447318.2015.1033912</pub-id>
</mixed-citation></ref><ref id="bibr38-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sylaiou</surname><given-names>S.</given-names></name>
<name><surname>Mania</surname><given-names>K.</given-names></name>
<name><surname>Paliokas</surname><given-names>I.</given-names></name>
<name><surname>Pujol-Tost</surname><given-names>L.</given-names></name>
<name><surname>Killintzis</surname><given-names>V.</given-names></name>
<name><surname>Liarokapis</surname><given-names>F.</given-names></name>
</person-group> (<year>2017</year>). <article-title>Exploring the educational impact of diverse technologies in online virtual museums</article-title>. <source>International Journal of Arts and Technology</source>, <volume>10</volume>(<issue>1</issue>), <fpage>58</fpage>&#x02013;<lpage>84</lpage>.
<pub-id pub-id-type="doi">10.1504/IJART.2017.083907</pub-id>
</mixed-citation></ref><ref id="bibr39-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ticini</surname><given-names>L. F.</given-names></name>
<name><surname>Rachman</surname><given-names>L.</given-names></name>
<name><surname>Pelletier</surname><given-names>J.</given-names></name>
<name><surname>Dubal</surname><given-names>S.</given-names></name>
</person-group> (<year>2014</year>). <article-title>Enhancing aesthetic appreciation by priming canvases with actions that match the artist's painting style</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>8</volume>, Article <fpage>391</fpage>.
<pub-id pub-id-type="doi">10.3389/fnhum.2014.00391</pub-id>
<pub-id pub-id-type="pmid">24917808</pub-id>
</mixed-citation></ref><ref id="bibr40-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Troncoso</surname><given-names>A.</given-names></name>
<name><surname>Soto</surname><given-names>V.</given-names></name>
<name><surname>Gomila</surname><given-names>A.</given-names></name>
<name><surname>Mart&#x000ed;nez-Pern&#x000ed;a</surname><given-names>D.</given-names></name>
</person-group> (<year>2023</year>). <article-title>Moving beyond the lab: Investigating empathy through the empirical 5E approach</article-title>. <source>Frontiers in Psychology</source>, <volume>14</volume>, <comment>Article 1119469.</comment>
<pub-id pub-id-type="doi">10.3389/fpsyg.2023.1119469</pub-id></mixed-citation></ref><ref id="bibr41-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Veenman</surname><given-names>M.</given-names></name>
<name><surname>Stefan</surname><given-names>A. M.</given-names></name>
<name><surname>Haaf</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2023</year>). <article-title>Bayesian hierarchical modeling: An introduction and reassessment</article-title>. <source>Behavior Research Methods</source>, <volume>56</volume>(<issue>5</issue>), <fpage>1</fpage>&#x02013;<lpage>32</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-023-02204-3</pub-id><pub-id pub-id-type="pmid">36627435</pub-id>
</mixed-citation></ref><ref id="bibr42-20416695251314182"><mixed-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yaremych</surname><given-names>H. E.</given-names></name>
<name><surname>Persky</surname><given-names>S.</given-names></name>
</person-group> (<year>2019</year>). <article-title>Tracing physical behavior in virtual reality: A narrative review of applications to social psychology</article-title>. <source>Journal of Experimental Social Psychology</source>, <volume>85</volume>, Article <fpage>103845</fpage>.
<pub-id pub-id-type="doi">10.1016/j.jesp.2019.103845</pub-id>
<pub-id pub-id-type="pmid">32831397</pub-id>
</mixed-citation></ref></ref-list></back></article>