<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Radiol</journal-id><journal-id journal-id-type="iso-abbrev">Front Radiol</journal-id><journal-id journal-id-type="publisher-id">Front. Radiol.</journal-id><journal-title-group><journal-title>Frontiers in Radiology</journal-title></journal-title-group><issn pub-type="epub">2673-8740</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC12098364</article-id><article-id pub-id-type="doi">10.3389/fradi.2025.1509377</article-id><article-categories><subj-group subj-group-type="heading"><subject>Radiology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Diagnostic precision of a deep learning algorithm for the classification of non-contrast brain CT reports</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>G&#x000fc;zel</surname><given-names>Hamza Eren</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor1" ref-type="corresp">*</xref><uri xlink:href="https://loop.frontiersin.org/people/2825070/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/><role content-type="https://credit.niso.org/contributor-roles/funding-acquisition/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/resources/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>A&#x0015f;c&#x00131;</surname><given-names>G&#x000f6;ktu&#x0011f;</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><xref rid="cor1" ref-type="corresp">*</xref><uri xlink:href="https://loop.frontiersin.org/people/2866367/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Demirbilek</surname><given-names>Oytun</given-names></name><xref rid="aff3" ref-type="aff">
<sup>3</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/3000307/overview"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>&#x000d6;zdemir</surname><given-names>Tu&#x0011f;&#x000e7;e Do&#x0011f;a</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/3053283/overview"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Erekli</surname><given-names>Pelin Berfin</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/3053286/overview"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib></contrib-group><aff id="aff1"><label><sup>1</sup></label><institution>Department of Radiology, &#x00130;zmir City Hospital</institution>, <addr-line>&#x00130;zmir</addr-line>, <country>T&#x000fc;rkiye</country></aff><aff id="aff2"><label><sup>2</sup></label><institution>School of Science and Technology, IE University</institution>, <addr-line>Segovia</addr-line>, <country>Spain</country></aff><aff id="aff3"><label><sup>3</sup></label><institution>School of Management, Technical University of Munich</institution>, <addr-line>Munich</addr-line>, <country>Germany</country></aff><author-notes><fn fn-type="edited-by"><p><bold>Edited by:</bold> Maria Evelina Fantacci, University of Pisa, Italy</p></fn><fn fn-type="edited-by"><p><bold>Reviewed by:</bold> Lei Gao, Wuhan University, China</p><p>Moiz Khan Sherwani, University of Copenhagen, Denmark</p></fn><corresp id="cor1"><label>*</label><bold>Correspondence:</bold> Hamza Eren G&#x000fc;zel <email>hamzaerenguzel@gmail.com</email> G&#x000f6;ktu&#x0011f; A&#x0015f;c&#x00131; <email>drgoktugasci@gmail.com</email></corresp></author-notes><pub-date pub-type="epub"><day>09</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>5</volume><elocation-id>1509377</elocation-id><history><date date-type="received"><day>10</day><month>10</month><year>2024</year></date><date date-type="accepted"><day>24</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 G&#x000fc;zel, A&#x0015f;c&#x00131;, Demirbilek, &#x000d6;zdemir and Erekli.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>G&#x000fc;zel, A&#x0015f;c&#x00131;, Demirbilek, &#x000d6;zdemir and Erekli</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License (CC BY)</ext-link>. The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><sec><title>Objective</title><p>This study aimed to determine the diagnostic precision of a deep learning algorithm for the classificaiton of non-contrast brain CT reports.</p></sec><sec><title>Methods</title><p>A total of 1,861 non-contrast brain CT reports were randomly selected, anonymized, and annotated for urgency level by two radiologists, with review by a senior radiologist. The data, encrypted and stored in Excel format, were securely maintained on a university cloud system. Using Python 3.8.16, the reports were classified into four urgency categories: emergency, not emergency but needs timely attention, clinically non-significant and normal. The dataset was split, with 800 reports used for training and 200 for validation. The DistilBERT model, featuring six transformer layers and 66 million trainable parameters, was employed for text classification. Training utilized the Adam optimizer with a learning rate of 2e-5, a batch size of 32, and a dropout rate of 0.1 to prevent overfitting. The model achieved a mean F1 score of 0.85 through 5-fold cross-validation, demonstrating strong performance in categorizing radiology reports.</p></sec><sec><title>Results</title><p>Of the 1,861 scans, 861 cases were identified as fit for study through the senior radiologist and self-hosted Label Studio interpretations. It was observed that the algorithm achieved a sensitivity of 91% and a specificity of 90% in the measurements made on the test data. The F1 score was measured as 0.89 for the best fold. The algorithm most successfully distinguished emergency results with positive predictive values that were unexpectedly lower than in previously reported studies. Beam hardening artifacts and excessive noise, compromising the quality of CT scan images, were significantly associated with decreased model performance.</p></sec><sec><title>Conclusion</title><p>This study revealed decreased diagnostic accuracy of an AI decision support system (DSS) at our institution. Despite extensive evaluation, we were unable to identify the source of this discrepancy, raising concerns about the generalizability of these tools with indeterminate failure modes. These results further highlight the need for standardized study design to allow for rigorous and reproducible site-to-site comparison of emerging deep learning technologies.</p></sec></abstract><kwd-group><kwd>artificial intelligence</kwd><kwd>report classification</kwd><kwd>computed tomography</kwd><kwd>deep learning</kwd><kwd>noncontrast head CT</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that no financial support was received for the research and/or publication of this article.</funding-statement></funding-group><counts><fig-count count="3"/><table-count count="3"/><equation-count count="0"/><ref-count count="21"/><page-count count="7"/><word-count count="0"/></counts><custom-meta-group><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Artificial Intelligence in Radiology</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>In contemporary medical practice, diagnostic imaging is crucial for expeditious and accurate diagnosis, treatment, and monitoring of diverse diseases. However, the misuse of imaging modalities is prevalent, which can result in adverse outcomes, including increased patient exposure to ionizing radiation, elevated healthcare expenditures, and subsequent cascades of additional imaging examinations (<xref rid="B1" ref-type="bibr">1</xref>, <xref rid="B2" ref-type="bibr">2</xref>). Global utilization of diagnostic imaging modalities, particularly computed tomography (CT) and magnetic resonance imaging (MRI), has undergone a substantial surge, resulting in an upward trend in the incidence of unjustified, low-value examinations (<xref rid="B2" ref-type="bibr">2</xref>, <xref rid="B3" ref-type="bibr">3</xref>). Moreover, it was found that 20%&#x02013;50% of CT scans in the U.S. are unnecessary (<xref rid="B4" ref-type="bibr">4</xref>). This is concerning since CT accounts for less than 10% of all procedures, yet it contributes more than 60% of the total effective dose from imaging examinations (<xref rid="B5" ref-type="bibr">5</xref>). To address the overuse of CT scans, several approaches are currently available. These include establishing clear diagnostic imaging guidelines, offering alternative imaging options, involving specialists, conducting post-imaging reviews, and providing feedback by comparing each referrer's ordering habits with the department's average (<xref rid="B5" ref-type="bibr">5</xref>). Clinical imaging guidelines have a limited impact on reducing unnecessary scans because of several challenges. These challenges include financial issues, limited equipment availability, workplace culture, lack of awareness of guidelines, fear of missing a diagnosis, self-referrals, patient demands, and, most importantly, poor implementation of the guidelines (<xref rid="B1" ref-type="bibr">1</xref>, <xref rid="B6" ref-type="bibr">6</xref>, <xref rid="B7" ref-type="bibr">7</xref>). Regular retrospective audits can help monitor the impact of quality improvement efforts and ensure compliance with guidelines. However, many institutions struggle to perform these audits routinely due to limitations in staffing, funding, and time. Clinical decision support (CDS) systems and more reliable real-time tools for applying standards during routine clinical care could help address these challenges (<xref rid="B8" ref-type="bibr">8</xref>, <xref rid="B9" ref-type="bibr">9</xref>).</p><p>Two studies demonstrated that automated analysis of radiology referrals is possible by using natural language processing (NLP) to interpret unstructured clinical information, along with machine learning (ML) and deep learning (DL) techniques for classifying referrals as either justified or unjustified. In the first study, this approach was tested and shown to be effective (<xref rid="B10" ref-type="bibr">10</xref>), as researchers performed a manual retrospective audit of 375 brain CT referrals using the iGuide system, combining unjustified and potentially justified referrals because of limited data. In a second study, 1,020 lumbar spine MRI referrals from two clinical sites were analyzed for justification, with the data labeled based on clinical expertise (<xref rid="B11" ref-type="bibr">11</xref>).</p><p>A key aspect of our study was creating an AI-based interpreter for the iGuide system (Quality and Safety in Imaging GmbH, Vienna, Austria) using real-world data. This was done to help apply justification standards more effectively in radiology practice, especially with the increasing use of diagnostic imaging, the ongoing issue of inappropriate CT scans, and the limitations of previous research. Thus, we aimed to determine the diagnostic precision of a selected large language model for the classification of brain CT reports. Such a model poses an alternative way to screen the reports and could help alleviate the problem of unnecessary follow-up CT scans.</p></sec><sec id="s2"><title>Methods and materials</title><p>We first introduce how we gathered and maintained the data, and the labeling process. Next, we discuss the large language model we used and its features.</p><sec id="s2a"><title>Data sourcing and labeling</title><p>Randomly selected 1,861 Non-contrast brain CT reports were anonymized and annotated for urgency level by two radiologists and reviewed by one senior radiologist. The research ethics committee at our institution (Ref#161) approved an ethics exemption for the study. Each participating clinical site assessed data privacy concerns and granted an exemption from a full ethical review. The data, which was anonymized, encrypted, and stored in Excel format, was securely kept on a university cloud system. Data analysis was performed using Python version 3.8.16.</p><p>Results were classified into the following groups:</p><sec id="s2a1"><title>Class 1: emergency</title><p>Results that require the patient to apply to the emergency department immediately (intracranial bleeding, stroke, shift, skull fracture, cerebral venous sinus thrombosis, etc.).</p></sec><sec id="s2a2"><title>Class 2: not emergency but needs timely attention</title><p>Reports in which the patient does not need to apply to the emergency room but requires outpatient clinic service in a health institution as soon as possible (intracranial space-occupying lesion, change of lesions during follow-up, metabolic diseases, etc.).</p></sec><sec id="s2a3"><title>Class 3: clinically Non-significant</title><p>Reports in which the patient does not need an outpatient clinical service as soon as possible; the primary physician can read the report anytime (arachnoid cyst, age-related changes, calcifications, variations, etc.).</p></sec><sec id="s2a4"><title>Class 4: normal</title><p>Completely normal findings.</p><p>Brain CT images between November 1, 2023, and June 1, 2024 were anonymized and transmitted to three tertiary referral hospitals: two public and one private. The patient's age, gender, comorbidites and other sociodemographic findings were among the electronically gathered referral data. We noticed 139 reports included mixed cases other than those relevant to brain CTs. Such cases are not in the scope of this study and, therefore, have been excluded from the dataset, leaving us 1,722 reports. Referrals that were incomplete or duplicated. Two radiologists with 2.5 years of experience classified and annotated the cases and all cases were reviewed by a board-certified radiologist with 7 years of experience. To ascertain the quality of our dataset, we curated pertinent reports under the study's objectives, thereby achieving a balanced dataset. We had the assumption that patients with multiple reports would usually have their first report as an emergency. To augment the quantity of emergency reports, we devised novel attributes for the reports, including the number of reports per patient, the character count of the report, and the approximate date. Subsequently, we employed a self-hosted Label Studio (v1.9.0) platform for collaborative data annotation by experts. Following this, we meticulously cleaned and aligned annotations with patient reports, yielding a well-prepared dataset for subsequent machine learning pipelines. This refined dataset provides a robust foundation for developing a precise and clinically significant medical AI model.</p><p>In the data ingestion phase, we utilized SQLalchemy (v2.0.33) to import the patient data into a PostgreSQL database. We indexed and anonymized sensitive patient data using a Fernet Key. All subsequent data transformations were executed within the database via dbt (v1.6.1), an extract-load-Transform (ELT) tool. Additionally, we leveraged the GPT 3.5 API to facilitate the translation of reports from Turkish to English. In order to ensure the precision of the translations, a subset of the reports was manually revised by 2 bilingual medical professionals fluent in Turkish and English language. The reviewers assessed the translations for precision, uniformity, and preservation of clinical sense. The prompt we utilized for this purpose was: Perform the following transformation on the report: &#x0201c;Translate into English&#x0201d;.</p><p>These steps align with the principles and practices recommended in the medical AI field, where high-quality data is of paramount importance for driving breakthroughs and advancing healthcare and medicine. Using a self-hosted Label Studio platform for collaborative data annotation is becoming a common practice in medical AI research and is widely preferred by experts. Therefore, experts could not only manage their annotation cycle but also prepare better-quality datasets. Furthermore, leveraging SQLalchemy and dbt for data ingestion and transformation is a standard procedure in many companies with large databases.</p></sec></sec><sec id="s2b"><title>Model architecture</title><p>We are engrossed in categorizing text-based radiology reports into the four predefined classes. The task includes text classification of radiology reports rather than direct image classification. To implement our text classification approach on the prepared emergency room dataset, we adapted a large language model DistilBERT, a version of BERT with six transformer layers, 768 concealed units, and twelve attention heads. Researchers often use it for text organization by adding a fully connected output layer with soft-max activation to predict the urgency category. It contains around 66 million trainable parameters, in addition to the transformer layers and a fully coupled output layer. DistilBERT is proven to achieve efficiency without sacrificing accuracy (<xref rid="B12" ref-type="bibr">12</xref>). This lightweight model, compared to its counterpart BERT (<xref rid="B13" ref-type="bibr">13</xref>), is trained with a knowledge distillation technique to enable the same information to be learned with much fewer learnable parameters (<xref rid="B14" ref-type="bibr">14</xref>). Knowledge distillation, also known as the teacher-student framework incorporating two models, student network and teacher network, respectively. As shown in <xref rid="F3" ref-type="fig">Figure&#x000a0;3</xref>, in knowledge distillation, DistilBERT is the student network that learns information from BERT as the teacher and becomes a smaller model (DistilBERT) that repeats the teacher's behavior (<xref rid="B12" ref-type="bibr">12</xref>, <xref rid="B15" ref-type="bibr">15</xref>). The BERT (teacher) is already pre-trained; then, we train Distil BERT (student) learning to make predictions in a different context than the teacher. Moreover, we benefit from this feature, since our inputs and expected predictions have a medical context. We first tokenize (i.e., splitting the text into substrings containing multiple words) the text data by using the Distil BERT tokenizer with sequences amplified to a maximum length of up to 512 tokens. Next, we train a text classifier model with the Distil BERT backbone and then evaluate the classification results. All the data was split into three and tested with five-fold cross-validation: 689 reports for training, 172 for validation, and 861 reports reserved for testing (i.e., with a split ratio of 4:1:5). The trained classifier then tested on the remaining 861 annotation reports.</p><fig position="float" id="F3"><label>Figure 3</label><caption><p>Accuracy folds and F1scores.</p></caption><graphic xlink:href="fradi-05-1509377-g003" position="float"/></fig><p>The full labeling methodology, model training pipeline, and evaluation metrics are available in our public GitHub repository (<xref rid="B16" ref-type="bibr">16</xref>, <xref rid="B17" ref-type="bibr">17</xref>).</p></sec><sec id="s2c"><title>Evaluation and results</title><p>We first evaluate the representativeness (i.e., data coverage) of our prepared dataset. As per sociodemographic data, most of the participants had age range 19&#x02013;35 years making 30% of the total population, with male subjects dominating the percentage (58%). Majority of the patients (40%) had no comorbidities and were mostly (80%) of Turkish nationality as tabulated in <xref rid="T1" ref-type="table">Table&#x000a0;1</xref>. To train the Distil BERT text classifier, we use the Adam optimizer with a learning rate of 2e-5 and a batch size of 32 alongside a cross-entropy loss function. A dropout rate of 0.1 was used to avoid overfitting. The training was made for 10 epochs, with early stopping being based on the validation loss. The training was processed on NVIDIA V-100 GPU, and it took about 4&#x02005;h to complete. The model performance was assessed using accuracy, F1 score, and recall features. We performed five-fold cross-validation with a split ratio of 4:1:5 for training, validation, and testing, respectively. The trained classifier attained a mean F1 score of 0.85 across the entire folds, as shown in <xref rid="F1" ref-type="fig">Figure&#x000a0;1</xref>. Out of 861 testing samples, 702 were correctly predicted with an overall accuracy of 81.5%. Class 3 has the highest misclassification rate. The confusion matrix for different classes is provided in <xref rid="F2" ref-type="fig">Figure&#x000a0;2</xref>. We observe that the algorithm produced has a sensitivity of 91% and a specificity of 90% in the measurements made on the test data. The F1 score was measured as 0.89 for the best fold residing within well-performing model F1 scores, i.e., 0.80&#x02013;0.90. We also highlight that in the worst-case scenario, the model can predict decently with approximately 0.81 F1 score. The best-performing model across the folds most successfully distinguished emergency results. However, in medical practice, false-negatives for the emergency class should be strictly avoided. Therefore, the model with the least false negative emergency cases should be preferred. The average recall value of 0.89 (moderate recall, 0.70&#x02013;0.90) indicates that our model is able to indicate most of the actual positive instances, but a moderate number of false negatives indicates a better performance of the model as shown in <xref rid="F1" ref-type="fig">Figure&#x000a0;1</xref>.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Sociodemographic of the participant.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="center" span="1"/></colgroup><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Variable</th><th valign="top" align="center" rowspan="1" colspan="1">Feature</th><th valign="top" align="center" rowspan="1" colspan="1">Percentage (%)</th></tr></thead><tbody><tr><td valign="top" align="left" colspan="3" rowspan="1">Age</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Birth-18 years</td><td valign="top" align="center" rowspan="1" colspan="1">05</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">19&#x02013;35</td><td valign="top" align="center" rowspan="1" colspan="1">30</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">36&#x02013;50</td><td valign="top" align="center" rowspan="1" colspan="1">25</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">51&#x02013;65</td><td valign="top" align="center" rowspan="1" colspan="1">20</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x0003e;65</td><td valign="top" align="center" rowspan="1" colspan="1">20</td></tr><tr><td valign="top" align="left" colspan="3" rowspan="1">Gender</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Male</td><td valign="top" align="center" rowspan="1" colspan="1">58</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Female</td><td valign="top" align="center" rowspan="1" colspan="1">42</td></tr><tr><td valign="top" align="left" colspan="3" rowspan="1">Comorbidities</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">None</td><td valign="top" align="center" rowspan="1" colspan="1">40</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">HTN</td><td valign="top" align="center" rowspan="1" colspan="1">20</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">DM</td><td valign="top" align="center" rowspan="1" colspan="1">15</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">CVD</td><td valign="top" align="center" rowspan="1" colspan="1">10</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Respiratory diseases</td><td valign="top" align="center" rowspan="1" colspan="1">10</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Others</td><td valign="top" align="center" rowspan="1" colspan="1">05</td></tr><tr><td valign="top" align="left" colspan="3" rowspan="1">Ethnicity</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Turkish</td><td valign="top" align="center" rowspan="1" colspan="1">80</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Others</td><td valign="top" align="center" rowspan="1" colspan="1">20</td></tr></tbody></table><table-wrap-foot><fn id="table-fn1"><p>CVD, cardiovascular disease; DM, diabetes mellitus; HTN, hypertension.</p></fn></table-wrap-foot></table-wrap><fig position="float" id="F1"><label>Figure 1</label><caption><p>Test performance in terms of accuracy and F1-score for the models trained in each fold.</p></caption><graphic xlink:href="fradi-05-1509377-g001" position="float"/></fig><fig position="float" id="F2"><label>Figure 2</label><caption><p>Confusion matrix of the best model trained across 5 cross-validation folds.</p></caption><graphic xlink:href="fradi-05-1509377-g002" position="float"/></fig></sec><sec id="s2d"><title>Retrospective justification audit</title><p>A total of 1,861 referrals were initially collected after the annotation; all the data was split in two; the first 1,000 (53.7%) reports were divided, with 800 (42.9%) used for training and 200 (10.7%) used for validation. The DistilBERT model was used for training. The resulting algorithm was tested on the remaining 861 annotated reports, as given in figure one below.</p></sec><sec id="s2e"><title>Prediction of accuracy</title><p>Out of 861, 702 were correctly predicted with an overall accuracy of 81.5%. Class 3 has the highest misclassification rate. The confusion matrix for the best model is provided in <xref rid="F2" ref-type="fig">Figure&#x000a0;2</xref> below.</p><p>Provided below are the expected vs. predicted plot of class 0 in <xref rid="T2" ref-type="table">Table&#x000a0;2</xref>:</p><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Expected vs. Predicted Plot of Class 0.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Class 0</th><th valign="top" align="center" colspan="2" rowspan="1">Expected</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="2" colspan="1">Predicted</td><td valign="top" align="center" rowspan="1" colspan="1">372 (+Ve)</td><td valign="top" align="center" rowspan="1" colspan="1">67 (-Ve)</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">21 (-Ve)</td><td valign="top" align="center" rowspan="1" colspan="1">403 (+Ve)</td></tr></tbody></table></table-wrap><p>It was observed that the algorithm produced sensitivity of 91% and a specificity of 90% in the measurements made on the test data. The F1 score was measured as 0.89 for the best fold. The algorithm most successfully distinguished emergency results. In order to evaluate the performance of the model, we performed K-fold cross-validation, where it was observed that with an average of 0.85 F1 score, residing within well-performing model values, i.e., 0.80&#x02013;0.90. The accuracy folds along with the F1 scores are given in <xref rid="F3" ref-type="fig">Figure&#x000a0;3</xref> below:</p><p>The average recall value of 0.89 (moderate recall, 0.70&#x02013;0.90) indicates that our model is able to indicate most of the actual positive instances, but a moderate number of false negatives indicate a better performance of the model, as given in the <xref rid="T3" ref-type="table">Table&#x000a0;3</xref> below.</p><table-wrap position="float" id="T3"><label>Table 3</label><caption><p>Accuracy, recall, and F1 scores of folds.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Folds</th><th valign="top" align="center" rowspan="1" colspan="1">Accuracy</th><th valign="top" align="center" rowspan="1" colspan="1">Recall</th><th valign="top" align="center" rowspan="1" colspan="1">F 1 score</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">1</td><td valign="top" align="center" rowspan="1" colspan="1">0.82</td><td valign="top" align="center" rowspan="1" colspan="1">0.85</td><td valign="top" align="center" rowspan="1" colspan="1">0.82</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">0.89</td><td valign="top" align="center" rowspan="1" colspan="1">0.93</td><td valign="top" align="center" rowspan="1" colspan="1">0.89</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">0.85</td><td valign="top" align="center" rowspan="1" colspan="1">0.87</td><td valign="top" align="center" rowspan="1" colspan="1">0.84</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">0.87</td><td valign="top" align="center" rowspan="1" colspan="1">0.90</td><td valign="top" align="center" rowspan="1" colspan="1">0.86</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">5</td><td valign="top" align="center" rowspan="1" colspan="1">0.88</td><td valign="top" align="center" rowspan="1" colspan="1">0.91</td><td valign="top" align="center" rowspan="1" colspan="1">087</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Mean of all folds</td><td valign="top" align="center" rowspan="1" colspan="1">0.86</td><td valign="top" align="center" rowspan="1" colspan="1">0.89</td><td valign="top" align="center" rowspan="1" colspan="1">0.85</td></tr></tbody></table></table-wrap></sec></sec><sec sec-type="discussion" id="s3"><title>Discussion</title><p>At our clinical site, we conducted a retrospective analysis to evaluate the performance of a deep learning-based decision support system (DSS) in identifying brain MRIs without contrast. We assessed the diagnostic accuracy of self-hosted Label Studio and identified common causes of false-positive results. We found that the algorithm was able to classify the reports successfully with high performance, similar to previous studies (<xref rid="B18" ref-type="bibr">18</xref>, <xref rid="B19" ref-type="bibr">19</xref>). Our findings indicate that mostly artifacts in the report are associated with reduced algorithm performance. Furthermore, we demonstrate that diagnostic effectiveness is independent of quantitative variations in image quality when evaluated by CT textural studies. Our study demonstrates the effectiveness of AI-based report assessment for predicting clinical reports. With a sensitivity of 91% and a specificity of 90%, the algorithm yielded an overall accuracy of 81.5%. The K-fold cross-validation results in our study indicated that the model performed well across all folds, with an average F1 score of 0.85 and an average recall value of 0.89.</p><p>With an average of 0.85 F1 score and recall value of 0.89, we have found a small number of false-positives that are directly attributable to various apparatuses used for CT scans. These examples, however, do not entirely explain why postsurgical patients perform worse. We cannot conclusively determine the mechanisms underlying this observation because neural networks' mechanisms are not easily examined; however, given these changes in diagnostic performance, we hypothesized that the effectiveness of AI tools may also depend on the characteristics of images. Future deep learning models for the classification may complicate or prevent interpretation by human readers.</p><p>We then investigated whether the false-negative results were due to poor image quality. Poor image quality can impact reporting, as such images are challenging for radiologists to interpret. Regardless of image quality, classifying hundreds of cases by urgency during daily practice can be difficult or even impossible for human experts. Therefore, we hypothesized that misclassification could similarly influence the performance of AI systems. In particular, discrepancies in diagnostic performance may result from subpar picture quality or from notable variations in image quality; also, studies that were mistakenly flagged may have had lower-quality images than studies that were successfully flagged.</p><p>As examples, beam hardening artifacts (<xref rid="B20" ref-type="bibr">20</xref>) and excessive noise (<xref rid="B21" ref-type="bibr">21</xref>) change the textural properties of CT data sets and the reports. Since the reports of these artifacts are vague, their classification can be difficult for the AI system.</p></sec><sec sec-type="conclusions" id="s4"><title>Conclusions</title><p>In this study, we investigated the diagnostic precision of a deep learning algorithm for classifying the non-contrast Brain CT reports. Our results demonstrate that the proposed algorithm achieves high accuracy, sensitivity, and specificity in classifying non-contrast brain CT scan reports. The algorithm's performance was evaluated on a dataset of 861 CT scan reports, which were annotated by expert radiologists to establish a gold standard.</p><p>Our findings highlight the feasibility of automating critical case identification, which could enhance workflow efficiency and expedite patient management in high-volume settings. By accurately distinguishing urgent from non-urgent cases, our approach may contribute to reducing reporting delays and optimizing resource allocation. Future studies should explore the clinical impact of this classification system and its integration into real-world radiology workflows.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors acknowledge the record-keeping and supportive staff of Izmir City Hospital for their help to conduct this study.</p></ack><sec sec-type="data-availability" id="s5"><title>Data availability statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.</p></sec><sec sec-type="ethics-statement" id="s6"><title>Ethics statement</title><p>The studies involving humans were approved by Izmir City Hospital Ethical Review Board. The studies were conducted in accordance with the local legislation and institutional requirements. Written informed consent for participation was not required from the participants or the participants' legal guardians/next of kin in accordance with the national legislation and institutional requirements.</p></sec><sec sec-type="author-contributions" id="s7"><title>Author contributions</title><p>HG: Conceptualization, Formal analysis, Supervision, Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Validation, Visualization, Data curation. GA: Conceptualization, Formal analysis, Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing. OD: Formal analysis, Writing &#x02013; original draft. T&#x000d6;: Data curation, Formal analysis, Writing &#x02013; original draft. PE: Formal analysis, Writing &#x02013; original draft.</p></sec><sec sec-type="COI-statement" id="s9"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="ai-statement" id="s10"><title>Generative AI statement</title><p>The author(s) declare that Generative AI was used in the creation of this manuscript. The study was based on machine learining.</p></sec><sec sec-type="disclaimer" id="s11"><title>Publisher's note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><ref-list><title>References</title><ref id="B1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kjelle</surname><given-names>E</given-names></name><name><surname>Andersen</surname><given-names>ER</given-names></name><name><surname>Krokeide</surname><given-names>AM</given-names></name><name><surname>Soril</surname><given-names>LJJ</given-names></name><name><surname>van Bodegom-Vos</surname><given-names>L</given-names></name><name><surname>Clement</surname><given-names>FM</given-names></name><etal/></person-group>
<article-title>Characterizing and quantifying low-value diagnostic imaging internationally: a scoping review</article-title>. <source>BMC Med Imaging</source>. (<year>2022</year>) <volume>22</volume>(<issue>1</issue>):<fpage>73</fpage>. <pub-id pub-id-type="doi">10.1186/s12880-022-00798-2</pub-id><pub-id pub-id-type="pmid">35448987</pub-id>
</mixed-citation></ref><ref id="B2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith-Bindman</surname><given-names>R</given-names></name><name><surname>Kwan</surname><given-names>ML</given-names></name><name><surname>Marlow</surname><given-names>EC</given-names></name><name><surname>Theis</surname><given-names>MK</given-names></name><name><surname>Bolch</surname><given-names>W</given-names></name><name><surname>Cheng</surname><given-names>SY</given-names></name><etal/></person-group>
<article-title>Trends in use of medical imaging in US health care systems and in Ontario, Canada, 2000-2016</article-title>. <source>Jama</source>. (<year>2019</year>). <volume>322</volume>(<issue>9</issue>):<fpage>843</fpage>&#x02013;<lpage>56</lpage>. <pub-id pub-id-type="doi">10.1001/jama.2019.11456</pub-id><pub-id pub-id-type="pmid">31479136</pub-id>
</mixed-citation></ref><ref id="B3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bou&#x000eb;tt&#x000e9;</surname><given-names>A</given-names></name><name><surname>Karoussou-Schreiner</surname><given-names>A</given-names></name><name><surname>Ducou Le Pointe</surname><given-names>H</given-names></name><name><surname>Grieten</surname><given-names>M</given-names></name><name><surname>de Kerviler</surname><given-names>E</given-names></name><name><surname>Rausin</surname><given-names>L</given-names></name><etal/></person-group>
<article-title>National audit on the appropriateness of CT and MRI examinations in Luxembourg</article-title>. <source>Insights Imaging</source>. (<year>2019</year>) <volume>10</volume>:<fpage>1</fpage>&#x02013;<lpage>2</lpage>. <pub-id pub-id-type="doi">10.1186/s13244-019-0731-9</pub-id><pub-id pub-id-type="pmid">30684056</pub-id>
</mixed-citation></ref><ref id="B4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poto&#x0010d;nik</surname><given-names>J</given-names></name><name><surname>Thomas</surname><given-names>E</given-names></name><name><surname>Lawlor</surname><given-names>A</given-names></name><name><surname>Kearney</surname><given-names>D</given-names></name><name><surname>Heffernan</surname><given-names>EJ</given-names></name><name><surname>Killeen</surname><given-names>RP</given-names></name><etal/></person-group>
<article-title>Machine learning and deep learning for classifying the justification of brain CT referrals</article-title>. <source>Eur Radiol</source>. (<year>2024</year>) <volume>34</volume>(<issue>12</issue>):<fpage>7944</fpage>&#x02013;<lpage>52</lpage>. <pub-id pub-id-type="doi">10.1007/s00330-024-10851-z</pub-id><pub-id pub-id-type="pmid">38913244</pub-id>
</mixed-citation></ref><ref id="B5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunne</surname><given-names>CL</given-names></name><name><surname>Elzinga</surname><given-names>JL</given-names></name><name><surname>Vorobeichik</surname><given-names>A</given-names></name><name><surname>Sudershan</surname><given-names>S</given-names></name><name><surname>Keto-Lambert</surname><given-names>D</given-names></name><name><surname>Lang</surname><given-names>E</given-names></name><etal/></person-group>
<article-title>A systematic review of interventions to reduce computed tomography usage in the emergency department</article-title>. <source>Ann Emerg Med</source>. (<year>2022</year>) <volume>80</volume>(<issue>6</issue>):<fpage>548</fpage>&#x02013;<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1016/j.annemergmed.2022.06.001</pub-id><pub-id pub-id-type="pmid">35927114</pub-id>
</mixed-citation></ref><ref id="B6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chimonas</surname><given-names>SC</given-names></name><name><surname>Diaz-MacInnis</surname><given-names>KL</given-names></name><name><surname>Lipitz-Snyderman</surname><given-names>AN</given-names></name><name><surname>Barrow</surname><given-names>BE</given-names></name><name><surname>Korenstein</surname><given-names>DR</given-names></name></person-group>. <article-title>Why not? Persuading clinicians to reduce overuse</article-title>. <source>Mayo Clin Proc Innov Qual Outcomes</source>. (<year>2020</year>) <volume>4</volume>(<issue>3</issue>):<fpage>266</fpage>&#x02013;<lpage>75</lpage>. <pub-id pub-id-type="doi">10.1016/j.mayocpiqo.2020.01.007</pub-id><pub-id pub-id-type="pmid">32542218</pub-id>
</mixed-citation></ref><ref id="B7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foley</surname><given-names>SJ</given-names></name><name><surname>Bly</surname><given-names>R</given-names></name><name><surname>Brady</surname><given-names>AP</given-names></name><name><surname>Ebdon-Jackson</surname><given-names>S</given-names></name><name><surname>Karoussou-Schreiner</surname><given-names>A</given-names></name><name><surname>Hierath</surname><given-names>M</given-names></name><etal/></person-group>
<article-title>Justification of CT practices across Europe: results of a survey of national competent authorities and radiology societies</article-title>. <source>Insights Imaging</source>. (<year>2022</year>) <volume>13</volume>(<issue>1</issue>):<fpage>177</fpage>. <pub-id pub-id-type="doi">10.1186/s13244-022-01325-1</pub-id><pub-id pub-id-type="pmid">36417017</pub-id>
</mixed-citation></ref><ref id="B8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>F</given-names></name><name><surname>Eberlein-Gonska</surname><given-names>M</given-names></name><name><surname>Hoffmann</surname><given-names>RT</given-names></name><name><surname>Schmitt</surname><given-names>J</given-names></name><name><surname>Blum</surname><given-names>SFU</given-names></name></person-group>. <article-title>Measuring appropriateness of diagnostic imaging: a scoping review</article-title>. <source>Insights Imaging</source>. (<year>2023</year>) <volume>14</volume>(<issue>1</issue>):<fpage>62</fpage>. <pub-id pub-id-type="doi">10.1186/s13244-023-01409-6</pub-id><pub-id pub-id-type="pmid">37052758</pub-id>
</mixed-citation></ref><ref id="B9"><label>9.</label><mixed-citation publication-type="journal"><collab>European Society of Radiology (ESR)</collab>. <article-title>Summary of the proceedings of the international forum 2016: &#x0201c;imaging referral guidelines and clinical decision support&#x02014;how can radiologists implement imaging referral guidelines in clinical routine?&#x0201d;</article-title>. <source>Insights Imaging</source>. (<year>2017</year>) <volume>8</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1007/s13244-016-0523-4</pub-id></mixed-citation></ref><ref id="B10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poto&#x0010d;nik</surname><given-names>J</given-names></name><name><surname>Thomas</surname><given-names>E</given-names></name><name><surname>Killeen</surname><given-names>R</given-names></name><name><surname>Foley</surname><given-names>S</given-names></name><name><surname>Lawlor</surname><given-names>A</given-names></name><name><surname>Stowe</surname><given-names>J</given-names></name></person-group>. <article-title>Automated vetting of radiology referrals: exploring natural language processing and traditional machine learning approaches</article-title>. <source>Insights Imaging</source>. (<year>2022</year>) <volume>13</volume>(<issue>1</issue>):<fpage>127</fpage>. <pub-id pub-id-type="doi">10.1186/s13244-022-01267-8</pub-id><pub-id pub-id-type="pmid">35925429</pub-id>
</mixed-citation></ref><ref id="B11"><label>11.</label><mixed-citation publication-type="other"><article-title>(PDF) Machine learning and deep learning-based Natural Language Processing for auto-vetting the appropriateness of Lumbar Spine Magnetic Resonance Imaging Referrals [Internet]</article-title>. <comment>Available from</comment>: <comment>Available at:</comment>
<ext-link xlink:href="https://www.researchgate.net/publication/360377500_Machine_learning_and_deep_learning-based_Natural_Language_Processing_for_auto-vetting_the_appropriateness_of_Lumbar_Spine_Magnetic_Resonance_Imaging_Referrals" ext-link-type="uri">https://www.researchgate.net/publication/360377500_Machine_learning_and_deep_learning-based_Natural_Language_Processing_for_auto-vetting_the_appropriateness_of_Lumbar_Spine_Magnetic_Resonance_Imaging_Referrals</ext-link>
<comment>(Accessed March 23, 2025)</comment>.</mixed-citation></ref><ref id="B12"><label>12.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Sanh</surname><given-names>V</given-names></name><name><surname>Debut</surname><given-names>L</given-names></name><name><surname>Chaumond</surname><given-names>J</given-names></name><name><surname>Wolf</surname><given-names>T</given-names></name></person-group>. <article-title>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter [Internet]</article-title>. <comment>arXiv; (2020). Available at:</comment>
<ext-link xlink:href="http://arxiv.org/abs/1910.01108" ext-link-type="uri">http://arxiv.org/abs/1910.01108</ext-link>
<comment>(Accessed March 23, 2025)</comment>.</mixed-citation></ref><ref id="B13"><label>13.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>MW</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Toutanova</surname><given-names>K.</given-names></name></person-group>
<article-title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [Internet]</article-title>. <comment>arXiv. (2019). Available at:</comment>
<ext-link xlink:href="http://arxiv.org/abs/1810.04805" ext-link-type="uri">http://arxiv.org/abs/1810.04805</ext-link>
<comment>(Accessed March 23, 2025)</comment>.</mixed-citation></ref><ref id="B14"><label>14.</label><mixed-citation publication-type="other"><article-title>Comprehensive analysis of embeddings and pre-training in NLP | Request PDF</article-title>. <comment>ResearchGate [Internet]. (2024). Available at:</comment>
<ext-link xlink:href="https://www.researchgate.net/publication/355132427_Comprehensive_analysis_of_embeddings_and_pre-training_in_NLP" ext-link-type="uri">https://www.researchgate.net/publication/355132427_Comprehensive_analysis_of_embeddings_and_pre-training_in_NLP</ext-link>
<comment>(Accessed March 23, 2025)</comment>.</mixed-citation></ref><ref id="B15"><label>15.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Dean</surname><given-names>J.</given-names></name></person-group>
<article-title>Distilling the Knowledge in a Neural Network [Internet]</article-title>. <comment>arXiv. (2015). Available at:</comment>
<ext-link xlink:href="http://arxiv.org/abs/1503.02531" ext-link-type="uri">http://arxiv.org/abs/1503.02531</ext-link>
<comment>(Accessed March 23, 2025)</comment>.</mixed-citation></ref><ref id="B16"><label>16.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>G&#x000fc;zel</surname><given-names>HE</given-names></name><name><surname>A&#x0015f;c&#x00131;</surname><given-names>G</given-names></name><name><surname>Demirbilek</surname><given-names>&#x000d6;</given-names></name><name><surname>&#x000d6;zdemir</surname><given-names>TD</given-names></name><name><surname>Erekli</surname><given-names>PB</given-names></name></person-group>. <article-title>MLOps reports</article-title>. (<year>2025</year>). <comment>Available at:</comment>&#x000a0;<ext-link xlink:href="https://github.com/mlops-reports/reports" ext-link-type="uri">https://github.com/mlops-reports/reports</ext-link> (<comment>Accessed May 05, 2025</comment>).</mixed-citation></ref><ref id="B17"><label>17.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>G&#x000fc;zel</surname><given-names>HE</given-names></name><name><surname>A&#x0015f;c&#x00131;</surname><given-names>G</given-names></name><name><surname>Demirbilek</surname><given-names>&#x000d6;</given-names></name><name><surname>&#x000d6;zdemir</surname><given-names>TD</given-names></name><name><surname>Erekli</surname><given-names>PB</given-names></name></person-group>. <article-title>MLOps reports</article-title>. (<year>2025</year>). <comment>Available at</comment>:&#x000a0;<ext-link xlink:href="https://github.com/mlops-reports/reports-dbt" ext-link-type="uri">https://github.com/mlops-reports/reports-dbt</ext-link> (<comment>Accessed May 05, 2025</comment>).</mixed-citation></ref><ref id="B18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>MC</given-names></name><name><surname>Ball</surname><given-names>RL</given-names></name><name><surname>Yang</surname><given-names>L</given-names></name><name><surname>Moradzadeh</surname><given-names>N</given-names></name><name><surname>Chapman</surname><given-names>BE</given-names></name><name><surname>Larson</surname><given-names>DB</given-names></name><etal/></person-group>
<article-title>Deep learning to classify radiology free-text reports</article-title>. <source>Radiology</source>. (<year>2018</year>) <volume>286</volume>(<issue>3</issue>):<fpage>845</fpage>&#x02013;<lpage>52</lpage>. <pub-id pub-id-type="doi">10.1148/radiol.2017171115</pub-id><pub-id pub-id-type="pmid">29135365</pub-id>
</mixed-citation></ref><ref id="B19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugimoto</surname><given-names>K</given-names></name><name><surname>Wada</surname><given-names>S</given-names></name><name><surname>Konishi</surname><given-names>S</given-names></name><name><surname>Okada</surname><given-names>K</given-names></name><name><surname>Manabe</surname><given-names>S</given-names></name><name><surname>Matsumura</surname><given-names>Y</given-names></name><etal/></person-group>
<article-title>Classification of diagnostic certainty in radiology reports with deep learning</article-title>. <source>Stud Health Technol Inform</source>. (<year>2024</year>) <volume>310</volume>:<fpage>569</fpage>&#x02013;<lpage>73</lpage>. <pub-id pub-id-type="doi">10.3233/SHTI231029</pub-id><pub-id pub-id-type="pmid">38269873</pub-id>
</mixed-citation></ref><ref id="B20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ger</surname><given-names>RB</given-names></name><name><surname>Craft</surname><given-names>DF</given-names></name><name><surname>Mackin</surname><given-names>DS</given-names></name><name><surname>Zhou</surname><given-names>S</given-names></name><name><surname>Layman</surname><given-names>RR</given-names></name><name><surname>Jones</surname><given-names>AK</given-names></name><etal/></person-group>
<article-title>Practical guidelines for handling head and neck computed tomography artifacts for quantitative image analysis</article-title>. <source>Comput Med Imaging Graph</source>. (<year>2018</year>) <volume>69</volume>:<fpage>134</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.compmedimag.2018.09.002</pub-id><pub-id pub-id-type="pmid">30268005</pub-id>
</mixed-citation></ref><ref id="B21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bagher-Ebadian</surname><given-names>H</given-names></name><name><surname>Siddiqui</surname><given-names>F</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Movsas</surname><given-names>B</given-names></name><name><surname>Chetty</surname><given-names>IJ</given-names></name></person-group>. <article-title>On the impact of smoothing and noise on robustness of CT and CBCT radiomics features for patients with head and neck cancers</article-title>. <source>Med Phys</source>. (<year>2017</year>) <volume>44</volume>(<issue>5</issue>):<fpage>1755</fpage>&#x02013;<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1002/mp.12188</pub-id><pub-id pub-id-type="pmid">28261818</pub-id>
</mixed-citation></ref></ref-list></back></article>