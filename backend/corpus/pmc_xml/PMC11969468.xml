<!--

File produced by pipelineRunner package (for JATS 2 SCJATS with pipeline SCJATS)
At: 2025-05-22T15:36:08.654Z

Version        : 1.16.1
Last update    : 2024-08-27
Modified by    : dunnm

-->
<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Soc Cogn Affect Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Soc Cogn Affect Neurosci</journal-id><journal-id journal-id-type="publisher-id">scan</journal-id><journal-title-group><journal-title>Social Cognitive and Affective Neuroscience</journal-title></journal-title-group><issn pub-type="ppub">1749-5016</issn><issn pub-type="epub">1749-5024</issn><publisher><publisher-name>Oxford University Press</publisher-name><publisher-loc>UK</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40066991</article-id><article-id pub-id-type="pmc">PMC11969468</article-id>
<article-id pub-id-type="doi">10.1093/scan/nsaf027</article-id><article-id pub-id-type="publisher-id">nsaf027</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Research &#x02013; Neuroscience</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI01880</subject></subj-group></article-categories><title-group><article-title>Neural dynamics of mental state attribution to social robot faces</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-4564-9834</contrib-id><name><surname>Maier</surname><given-names>Martin</given-names></name><!--martin.maier@hu-berlin.de--><aff>
<institution content-type="department">Department of Psychology</institution>, Humboldt-Universit&#x000e4;t zu Berlin, Berlin D-10099, <country country="DE">Germany</country></aff><aff>
<institution content-type="department">Science of Intelligence, Research Cluster of Excellence, Technische Universit&#x000e4;t Berlin</institution>, Berlin D-10587, <country country="DE">Germany</country></aff><xref rid="COR0001" ref-type="corresp"/></contrib><contrib contrib-type="author"><name><surname>Leonhardt</surname><given-names>Alexander</given-names></name><aff>
<institution content-type="department">Department of Psychology</institution>, Humboldt-Universit&#x000e4;t zu Berlin, Berlin D-10099, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7557-1508</contrib-id><name><surname>Blume</surname><given-names>Florian</given-names></name><aff>
<institution content-type="department">Science of Intelligence, Research Cluster of Excellence, Technische Universit&#x000e4;t Berlin</institution>, Berlin D-10587, <country country="DE">Germany</country></aff><aff>
<institution content-type="department">Computer Vision &#x00026; Remote Sensing, Technische Universit&#x000e4;t Berlin</institution>, Berlin D-10587, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8145-1732</contrib-id><name><surname>Bideau</surname><given-names>Pia</given-names></name><aff>
<institution content-type="department">Science of Intelligence, Research Cluster of Excellence, Technische Universit&#x000e4;t Berlin</institution>, Berlin D-10587, <country country="DE">Germany</country></aff><aff>
<institution content-type="department">Inria, CNRS, Univ. Grenoble Alpes</institution>, Montbonnot-Saint-Martin 38330, <country country="FR">France</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2871-9266</contrib-id><name><surname>Hellwich</surname><given-names>Olaf</given-names></name><aff>
<institution content-type="department">Science of Intelligence, Research Cluster of Excellence, Technische Universit&#x000e4;t Berlin</institution>, Berlin D-10587, <country country="DE">Germany</country></aff><aff>
<institution content-type="department">Computer Vision &#x00026; Remote Sensing, Technische Universit&#x000e4;t Berlin</institution>, Berlin D-10587, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-8438-1570</contrib-id><name><surname>Abdel Rahman</surname><given-names>Rasha</given-names></name><aff>
<institution content-type="department">Department of Psychology</institution>, Humboldt-Universit&#x000e4;t zu Berlin, Berlin D-10099, <country country="DE">Germany</country></aff><aff>
<institution content-type="department">Science of Intelligence, Research Cluster of Excellence, Technische Universit&#x000e4;t Berlin</institution>, Berlin D-10587, <country country="DE">Germany</country></aff></contrib></contrib-group><author-notes><corresp id="COR0001">*Corresponding author. Department of Psychology, Humboldt-Universit&#x000e4;t zu Berlin, Rudower Chaussee 18, Berlin D-12489, Germany. E-mail: <email xlink:href="martin.maier@hu-berlin.de">martin.maier@hu-berlin.de</email></corresp></author-notes><pub-date pub-type="collection"><year>2025</year></pub-date><pub-date pub-type="epub" iso-8601-date="2025-03-11"><day>11</day><month>3</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>11</day><month>3</month><year>2025</year></pub-date><volume>20</volume><issue>1</issue><elocation-id>nsaf027</elocation-id><history><date date-type="received"><day>20</day><month>9</month><year>2024</year></date><date date-type="rev-recd"><day>06</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>10</day><month>3</month><year>2025</year></date><date date-type="editorial-decision"><day>28</day><month>2</month><year>2025</year></date><date date-type="corrected-typeset"><day>03</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025. Published by Oxford University Press.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="nsaf027.pdf"/><abstract><title>Abstract</title><p>The interplay of mind attribution and emotional responses is considered crucial in shaping human trust and acceptance of social robots. Understanding this interplay can help us create the right conditions for successful human&#x02013;robot social interaction in alignment with societal goals. Our study shows that affective information about robots describing positive, negative, or neutral behaviour leads participants (<italic toggle="yes">N&#x02009;</italic>=&#x02009;90) to attribute mental states to robot faces, modulating impressions of trustworthiness, facial expression, and intentionality. Electroencephalography recordings from 30 participants revealed that affective information influenced specific processing stages in the brain associated with early face perception (N170 component) and more elaborate stimulus evaluation (late positive potential). However, a modulation of fast emotional brain responses, typically found for human faces (early posterior negativity), was not observed. These findings suggest that neural processing of robot faces alternates between being perceived as mindless machines and intentional agents: people rapidly attribute mental states during perception, literally seeing good or bad intentions in robot faces, but are emotionally less affected than when facing humans. These nuanced insights into the fundamental psychological and neural processes underlying mind attribution can enhance our understanding of human&#x02013;robot social interactions and inform policies surrounding the moral responsibility of artificial agents.</p></abstract><kwd-group><kwd>mind attribution</kwd><kwd>social robots</kwd><kwd>brain activity</kwd><kwd>social cognition</kwd><kwd>affective information</kwd></kwd-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Deutsche Forschungsgemeinschaft</institution><institution-id institution-id-type="DOI">10.13039/501100001659</institution-id></institution-wrap>
</funding-source><award-id>EXC 2002/1 &#x0201c;Science of Intelligence&#x0201d; - projet</award-id></award-group></funding-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Deutsche Forschungsgemeinschaft</institution><institution-id institution-id-type="DOI">10.13039/501100001659</institution-id></institution-wrap>
</funding-source><award-id>EXC 2002/1 &#x0201c;Science of Intelligence&#x0201d; - projet</award-id></award-group></funding-group><counts><page-count count="11"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>The demand for social robots&#x02014;embodied artificial systems designed for various uses such as care, retail, and entertainment&#x02014;is expected to increase in the coming years, leading to more people engaging with them in both private and professional lives (<xref rid="R9" ref-type="bibr">Breazeal et&#x000a0;al. 2016</xref>, <xref rid="R6" ref-type="bibr">Belpaeme et&#x000a0;al. 2018</xref>, <xref rid="R61" ref-type="bibr">Paolillo et&#x000a0;al. 2022</xref>). Yet, key psychological and neurocognitive aspects of interacting with social robots are still under investigation, including the extent to which humans&#x02014;and their brains&#x02014;process robots akin to intentional social agents with mental states (<xref rid="R55" ref-type="bibr">Marchesi et&#x000a0;al. 2019</xref>, <xref rid="R38" ref-type="bibr">Henschel et&#x000a0;al. 2020</xref>, <xref rid="R16" ref-type="bibr">Cross and Ramsey 2021</xref>). This question bears important implications for how we deal with artificial social agents as a society, including moral judgments of their responsibility for negative outcomes (<xref rid="R35" ref-type="bibr">Gray et&#x000a0;al. 2012</xref>).</p><p>Previous theoretical work has proposed that two conflicting intuitions come into play, the &#x02018;intentional&#x02019; stance and the &#x02018;physical&#x02019; (or &#x02018;design&#x02019;) stance (<xref rid="R20" ref-type="bibr">Dennett 1989</xref>, <xref rid="R55" ref-type="bibr">Marchesi et&#x000a0;al. 2019</xref>, <xref rid="R62" ref-type="bibr">Perez-Osorio and Wykowska 2020</xref>, <xref rid="R15" ref-type="bibr">Clark and Fischer 2023</xref>). Taking an intentional stance towards robots means to intuitively treat them as if they had a mind, attributing mental states like motivations, intentions, or emotions. This may enable people to engage processes that typically support social interaction with humans, such as theory of mind, as a basis for social perception, communication, and behavioural coordination (<xref rid="R22" ref-type="bibr">Dryer 1999</xref>, <xref rid="R27" ref-type="bibr">Epley et&#x000a0;al. 2007</xref>, <xref rid="R34" ref-type="bibr">Gray and Wegner 2012</xref>, <xref rid="R18" ref-type="bibr">de Graaf 2016</xref>, <xref rid="R59" ref-type="bibr">Onnasch and Roesler 2019</xref>). On the other hand, people&#x02019;s explicit opinions often reflect a physical stance towards robots, viewing them as machines designed or programmed to behave in specific ways (<xref rid="R19" ref-type="bibr">de Graaf et&#x000a0;al. 2015</xref>, <xref rid="R18" ref-type="bibr">de Graaf 2016</xref>, <xref rid="R44" ref-type="bibr">Kim and Duhachek 2020</xref>, <xref rid="R15" ref-type="bibr">Clark and Fischer 2023</xref>, <xref rid="R42" ref-type="bibr">Jacobs et&#x000a0;al. 2022</xref>).</p><p>How are these seemingly contradictory intuitions&#x02014;robots as intentional beings or mindless machines&#x02014;reflected in people&#x02019;s perception of robots and the underlying neural processes? Most research has focused on how human&#x02013;robot interaction and mind perception are influenced by robot design features (<xref rid="R23" ref-type="bibr">Duffy 2003</xref>, <xref rid="R78" ref-type="bibr">Waytz et&#x000a0;al. 2010</xref>, <xref rid="R10" ref-type="bibr">Broadbent et&#x000a0;al. 2013</xref>, <xref rid="R56" ref-type="bibr">Martini et&#x000a0;al. 2016</xref>, <xref rid="R12" ref-type="bibr">Ceh and Vanman 2018</xref>, <xref rid="R39" ref-type="bibr">Hortensius et&#x000a0;al. 2018</xref>, <xref rid="R59" ref-type="bibr">Onnasch and Roesler 2019</xref>) and perceiver traits, such as attitudes towards robots and artificial intelligence (AI) (<xref rid="R74" ref-type="bibr">Stafford et&#x000a0;al. 2014</xref>, <xref rid="R73" ref-type="bibr">Spatola and Wudarczyk 2021</xref>, <xref rid="R42" ref-type="bibr">Jacobs et&#x000a0;al. 2022</xref>, <xref rid="R76" ref-type="bibr">Thellman et&#x000a0;al. 2022</xref>). However, crucial variables shaping social perception and interaction in real time have been largely overlooked in studies of mind attribution to robots: the human ability to perceive and evaluate others&#x02019; intentions is strongly influenced by context and prior knowledge, such as learned person-related information (<xref rid="R1" ref-type="bibr">Abdel Rahman 2011</xref>, <xref rid="R80" ref-type="bibr">Wieser et&#x000a0;al. 2014</xref>, <xref rid="R60" ref-type="bibr">Otten et&#x000a0;al. 2017</xref>, <xref rid="R5" ref-type="bibr">Baum et&#x000a0;al. 2020</xref>, <xref rid="R53" ref-type="bibr">Maier et&#x000a0;al. 2022</xref>). Our perception of others is not only shaped by what we can read from their faces, but also from what we read into them based on our expectations (<xref rid="R36" ref-type="bibr">Hassin and Trope 2000</xref>). For instance, social-affective information has been shown to influence brain signatures of early perceptual, reflexive emotional, as well as higher-level evaluative processing (<xref rid="R66" ref-type="bibr">Schacht and Sommer 2009</xref>, <xref rid="R1" ref-type="bibr">Abdel Rahman 2011</xref>, <xref rid="R80" ref-type="bibr">Wieser et&#x000a0;al. 2014</xref>, <xref rid="R75" ref-type="bibr">Suess et&#x000a0;al. 2015</xref>, <xref rid="R5" ref-type="bibr">Baum et&#x000a0;al. 2020</xref>, <xref rid="R53" ref-type="bibr">Maier et&#x000a0;al. 2022</xref>), as elaborated on below.</p><sec id="s1-s1"><title>The present study</title><p>In this study, we investigate how brain dynamics reflect mental state attribution in the perception of social robots. Specifically, we test how prior information about robots&#x02019; behaviour influences neural correlates of perception, emotional responses, and evaluation (as illustrated in <xref rid="F1" ref-type="fig">Fig.&#x000a0;1</xref>). In human social perception, prior knowledge or beliefs about others (e.g. &#x02018;she bullied her work colleague&#x02019;) can lead people to perceive emotional expressions in objectively neutral faces, revealing the attribution of mental states that are not necessarily present in the person (<xref rid="R1" ref-type="bibr">Abdel Rahman 2011</xref>, <xref rid="R75" ref-type="bibr">Suess et&#x000a0;al. 2015</xref>, <xref rid="R53" ref-type="bibr">Maier et&#x000a0;al. 2022</xref>). This can be attributed to the interplay between bottom-up and top-down processing, where perceptions are constructed based on combinations of sensory input from the environment and predictions generated from prior knowledge and expectations (<xref rid="R47" ref-type="bibr">Lee and Mumford 2003</xref>, <xref rid="R30" ref-type="bibr">Friston 2009</xref>, <xref rid="R14" ref-type="bibr">Clark 2013</xref>, <xref rid="R63" ref-type="bibr">Press and Yon 2019</xref>, <xref rid="R49" ref-type="bibr">Lupyan et&#x000a0;al. 2020</xref>, <xref rid="R53" ref-type="bibr">Maier et&#x000a0;al. 2022</xref>). Here we investigate this mechanism for the first time with robot faces: does information about a robot&#x02019;s behaviour literally make people see good or bad intentions in its face, indicating mind attribution at an early perceptual stage? And do people react emotionally to neutral robot faces based on whether they are associated with morally good or bad behaviour, as they would with other humans (<xref rid="R1" ref-type="bibr">Abdel Rahman 2011</xref>, <xref rid="R75" ref-type="bibr">Suess et&#x000a0;al. 2015</xref>, <xref rid="R5" ref-type="bibr">Baum et&#x000a0;al. 2020</xref>, <xref rid="R4" ref-type="bibr">Baum and Abdel Rahman 2021</xref>)?</p><fig position="float" id="F1" fig-type="figure"><label>Figure&#x000a0;1.</label><caption><p>Information examples and rating results of Experiment 1. (a). Representative story examples for each affective information condition along with one example of a robot portrait. The pairings of robots and information type were counterbalanced across participants, ensuring each robot was paired equally as often with negative, neutral, and positive information. (b) Trustworthiness ratings after information acquisition and facial expression ratings before and after information acquisition, categorised by information condition. Large dots denote group means with corresponding 95% CIs, while small dots indicate individual participant means. Asterisks highlight statistically significant differences. Photo Romeo the Robot by Softbank Robotics Europe, 2016, Wikimedia Commons (<ext-link xlink:href="https://commons.wikimedia.org/wiki/File:Romeo_the_robot_.jpg" ext-link-type="uri">https://commons.wikimedia.org/wiki/File:Romeo_the_robot_.jpg</ext-link>). CC BY 4.0. Cropped and rotated from original.</p></caption><graphic xlink:href="nsaf027f1" position="float"/></fig><p>In two preregistered experiments, participants were presented with positive, neutral, and negative information about real human-like robots (e.g. &#x02018;teaches social skills to people with autism&#x02019;, &#x02018;assembles orders at a warehouse&#x02019;, &#x02018;reports children to the secret police&#x02019;; see <xref rid="F1" ref-type="fig">Fig.&#x000a0;1</xref>). Subsequently, they rated the valence of the robots&#x02019; facial expressions and their trustworthiness. We hypothesised that, similar to human faces, participants would attribute emotional expressions to the robots&#x02019; faces and assess their trustworthiness influenced by the information they had acquired. Experiment 1 investigated the effect of affective information on ratings of facial expression and trustworthiness, seeking initial evidence that people indeed read good or bad intentions into robot faces. To explore the neurocognitive dynamics associated with the attribution of mental states, Experiment 2 measured evoked brain responses using electroencephalography (EEG) as participants evaluated robots&#x02019; facial expressions.</p></sec></sec><sec id="s2"><title>Materials and methods</title><sec id="s2-s1"><title>Experiment 1</title><p>The preregistration for Experiment 1 can be accessed at <ext-link xlink:href="https://osf.io/qytra" ext-link-type="uri">https://osf.io/qytra</ext-link>.</p><sec id="s2-s1-s1"><title>Participants</title><p>Sixty participants (22 cisgender women, 38 cisgender men; mean age 28&#x02009;years, range 18&#x02013;39) were recruited from Prolific (prolific.com) and received monetary compensation. Thirty participants were German speakers (6 cisgender women, 24 cisgender men; mean age 28&#x02009;years, range 19&#x02013;39), and 30 were English speakers (16 cisgender women, 14 cisgender men; mean age 27&#x02009;years, range 18&#x02013;37). The sample size, a multiple of six ensured counterbalancing across three information conditions and two orders of response button assignments (see <xref rid="s7" ref-type="sec">Supplementary Material</xref> for exclusion criteria). The study adhered to the principles of the Declaration of Helsinki and received approval from the Ethics Committee of the Department of Psychology at Humboldt-Universit&#x000e4;t zu Berlin. Participants provided informed written consent before participation.</p></sec><sec id="s2-s1-s2"><title>Materials</title><p>The picture stimuli comprised 36 full-colour frontal portrait photographs featuring existing humanoid robots, each displaying approximately neutral facial expressions (refer to <xref rid="F1" ref-type="fig">Fig.&#x000a0;1</xref> for an example; further details including names and sources of the robot images are listed in the <xref rid="s7" ref-type="sec">Supplementary Material</xref>). Robot heads were cropped and placed on a grey background and matched in size and eye placement across all images. All robot images had frontal gaze.</p><p>We recorded 36 spoken stories in both English (mean duration: 18.1 s) and German (mean duration: 17.9 s) with affectively positive, neutral, or negative information about the robots (see <xref rid="F1" ref-type="fig">Fig.&#x000a0;1</xref> for examples and <xref rid="s7" ref-type="sec">Supplementary Material</xref> for all stories). To ensure that the stories would be perceived as positive, neutral, or negative according to the respective condition, we pretested them with a separate sample of participants (<italic toggle="yes">N</italic>&#x02009;=&#x02009;15). The valence and arousal ratings of the stories were as expected. Additionally, positive and negative stories were rated as equally realistic, while neutral stories were rated as more realistic compared to both positive and negative stories. Detailed prerating results are provided in the <xref rid="s7" ref-type="sec">Supplementary Material</xref>.</p></sec><sec id="s2-s1-s3"><title>Procedure</title><p>Participants initially rated the facial expressions of 36 robots on a 7-point Likert scale ranging from very negative to very positive. The scale&#x02019;s midpoint was neutral, and the order of anchors counterbalanced across participants. In the main section, participants were presented with six robots per block, each paired with a different story (two of each information condition). They rated each robot&#x02019;s trustworthiness on a 7-point Likert scale immediately after hearing the story and subsequently rated the facial expressions of all robots in the block. Attention was monitored with multiple-choice questions between blocks. After the main experiment, participants completed the Attitude towards Artificial Intelligence Scale (<xref rid="R70" ref-type="bibr">Sindermann et&#x000a0;al. 2021</xref>) and answered questions about the experiment. They reported whether they had researched information about the robots, were distracted, or distrusted any of the presented information, and rated the robots&#x02019; perceived intentionality and deliberateness. Those who distrusted the information estimated the percentage of stories affected. Participants also provided feedback and were debriefed that none of the information was related to the featured robots.</p></sec></sec><sec id="s2-s2"><title>Experiment 2</title><p>Experiment 2 was preregistered under <ext-link xlink:href="https://osf.io/c8va7" ext-link-type="uri">https://osf.io/c8va7</ext-link>. The procedure and materials were similar to Experiment 1, with the following differences due to the EEG setting: (i) Experiment 2 was divided into two main parts: a learning phase (without EEG) in which participants acquired and rehearsed information about all robots, and a subsequent EEG part in which participants performed rating tasks on the robot pictures; (ii) only a subset of 18 robot stimuli was used to reduce the length of the experiment and information to memorise; (iii) rating scales were 5-point instead of 7-point-Likert scales due to the experimental setup in the EEG laboratory; (iv) trustworthiness ratings were collected both before and after learning, whereas facial expression ratings were only collected after learning; and (v) a new task was added at the end to collect perceived intentionality ratings for each robot.</p><sec id="s2-s2-s1"><title>Participants</title><p>Thirty-five participants, all German speakers, were tested to achieve the preregistered final sample of 30 participants (24 cisgender women, 6 cisgender men; mean age 25.4&#x02009;years, range 18&#x02013;36). During debriefing, the five excluded participants had expressed strong doubts about the veracity of the information provided about the robots (see <xref rid="s7" ref-type="sec">Supplementary Material</xref> for power analysis and exclusion criteria). All participants received monetary compensation or course credit. The study adhered to the principles of the Declaration of Helsinki and received approval from the Ethics Committee of the Department of Psychology at Humboldt-Universit&#x000e4;t zu Berlin. Participants provided written consent before participating.</p></sec><sec id="s2-s2-s2"><title>Materials</title><p>A subset of 18 robot pictures with the corresponding stories was selected from the stimulus set of Experiment 1 (see <xref rid="s7" ref-type="sec">Table S2</xref> in the <xref rid="s7" ref-type="sec">Supplementary Material</xref>). Stimuli were presented on a grey background on a 19-inch LCD monitor with a resolution of 1280&#x02009;&#x000d7;&#x02009;1024 pixels and a 75&#x02009;Hz refresh rate. During the rating tasks, robot faces were displayed with a size subtending 6.03&#x000b0; vertical and 6.02&#x000b0; horizontal visual angles (viewing distance: 70&#x02009;cm). We recorded additional short versions of each robot story, focusing on a central part of the robots&#x02019; behaviour. After one presentation of each story&#x02019;s long version, further repetitions used the short versions.</p><p>The added perceived intentionality rating was inspired by the InStance questionnaire (<xref rid="R55" ref-type="bibr">Marchesi et&#x000a0;al. 2019</xref>). Participants rated the robots&#x02019; behaviour on a scale from &#x02212;50 (mechanistic) to 50 (intentional), indicating the extent to which they agreed with mechanistic versus intentional descriptions of the robot&#x02019;s behaviour. Details on the intentionality questionnaire are provided in the <xref rid="s7" ref-type="sec">Supplementary Material</xref>.</p></sec><sec id="s2-s2-s3"><title>Procedure</title><p>The experiment began with participants rating each robot&#x02019;s trustworthiness. Following this, participants entered a 30-min learning phase, during which they acquired and rehearsed information about 18 robots (details provided in the <xref rid="s7" ref-type="sec">Supplementary Material</xref>). EEG electrodes were placed after the learning phase. Participants then rated the trustworthiness of the robots again, followed by facial expression and intentionality ratings. The facial expression task was repeated 12 times for each robot in random order. This task formed the basis for the analysis of event-related potentials (ERPs). As shown in <xref rid="F2" ref-type="fig">Fig.&#x000a0;2</xref>, each trial began with a fixation cross displayed for 500&#x02009;ms, followed by the robot face stimulus, which remained visible until participants responded. To ensure accurate use of the five response buttons corresponding to the rating scale, participants were reminded of the scale at regular intervals during breaks and had a printed reference sheet on the table. The placement of scale anchors (e.g. very positive, very negative) was counterbalanced across participants.</p><fig position="float" id="F2" fig-type="figure"><label>Figure&#x000a0;2.</label><caption><p>EEG study results. (a). Trial sequence of the facial expression rating task. (b) Trustworthiness ratings before and after information acquisition, facial expression and intentionality ratings after information acquisition, categorised by information condition. Large dots denote group means with corresponding 95% CIs, while small dots indicate individual participant means. (c) Grand average ERPs for the N170 and LPP components collected during the facial expression rating task. Grey shading highlights time windows for the N170 and LPP. Scalp topographies illustrate differences between the information conditions, with channels included in the N170 and LPP regions of interest highlighted in white. Asterisks highlight statistically significant differences. Photo Romeo the Robot by Softbank Robotics Europe, 2016, Wikimedia Commons (<ext-link xlink:href="https://commons.wikimedia.org/wiki/File:Romeo_the_robot_.jpg" ext-link-type="uri">https://commons.wikimedia.org/wiki/File:Romeo_the_robot_.jpg</ext-link>). CC BY 4.0. Cropped and rotated from original.</p></caption><graphic xlink:href="nsaf027f2" position="float"/></fig></sec><sec id="s2-s2-s4"><title>EEG recording and analysis</title><p>The EEG data were acquired using Ag/AgCl electrodes placed at 64 scalp sites according to the extended 10&#x02013;20 system, with a sampling rate of 500&#x02009;Hz and all electrodes referenced to the left mastoid. The electrooculogram (EOG) was recorded using a bipolar vertical EOG channel consisting of electrodes Fp1&#x02014;IO1 and a bipolar horizontal EOG channel consisting of electrodes F9&#x02014;F10. During recording, a low-cut-off filter (0.032&#x02009;Hz) was applied, and electrode impedances were maintained below 10 k&#x003a9;. Post-experiment, a calibration procedure was conducted to capture prototypical eye movements for subsequent artefact correction.</p><p>Offline processing for single-trial ERP analysis followed a pipeline detailed in <xref rid="R31" ref-type="bibr">Fr&#x000f6;mer et&#x000a0;al. (2018)</xref>, reimplemented using functions of MNE Python (<xref rid="R33" ref-type="bibr">Gramfort</xref>), available at <ext-link xlink:href="https://github.com/alexenge/hu-neuro-pipeline" ext-link-type="uri">https://github.com/alexenge/hu-neuro-pipeline</ext-link>. Continuous EEG data were rereferenced to a common average reference, and eye movement artefacts were removed using a spatio-temporal dipole modelling procedure with the BESA software (<xref rid="R40" ref-type="bibr">Ille et&#x000a0;al. 2002</xref>). The corrected data were low-pass filtered at 40&#x02009;Hz, segmented into epochs of &#x02212;500 to 1500&#x02009;ms relative to face stimulus onset, and baseline-corrected using the 200&#x02009;ms prestimulus interval. For each participant, 72 segments were created in each information condition (six robots per information condition &#x02715; 12 repetitions), excluding segments containing artefacts (amplitudes over &#x000b1;150&#x02009;&#x000b5;V, or changing by more than 50&#x02009;&#x000b5;V between samples). After artefact rejection, over 99% of segments remained per condition, leaving <italic toggle="yes">M</italic>&#x02009;=&#x02009;71.37 segments per subject in the negative condition (<italic toggle="yes">SD</italic>&#x02009;=&#x02009;1.25; range&#x02009;=&#x02009;67&#x02013;72), <italic toggle="yes">M</italic>&#x02009;=&#x02009;71.33 segments in the neutral condition (<italic toggle="yes">SD</italic>&#x02009;=&#x02009;1.06; range&#x02009;=&#x02009;68&#x02013;72), and M&#x02009;=&#x02009;71.37 segments in the positive condition (<italic toggle="yes">SD</italic>&#x02009;=&#x02009;1.16; range&#x02009;=&#x02009;68&#x02013;72). These represent the trials that were analysed and are reported in the EEG results.</p><p>Single-trial mean amplitudes were obtained for the P1, N170, early posterior negativity (EPN), and late positive potential (LPP) components by averaging across preregistered time windows and electrode sites typical for each component (see <xref rid="F2" ref-type="fig">Fig.&#x000a0;2</xref>) and analysed using linear mixed effects models (LMMs). The P1 was averaged at parieto-occipital electrode sites (O1, O2, Oz, PO7, PO8) in the time window 76&#x02013;116&#x02009;ms centered around the average P1-peak of the ERP collapsed across all conditions. The N170 was averaged at parieto-occipital electrode sites (TP9, TP10, P7, P8, PO9, PO10, O1, O2) centered around its average peak, between 129 and 179&#x02009;ms. The EPN was averaged at posterior electrodes (PO7, PO8, PO9, PO10, TP9, TP10) between 220 and 350&#x02009;ms. The LPP was averaged at centro-parietal sites (Pz, Cz, C1, C2, CP1, CP2) between 408 and 608&#x02009;ms.</p></sec></sec></sec><sec id="s3"><title>RESULTS</title><sec id="s3-s1"><title>Experiment 1</title><p>In Experiment 1, an online study investigated the influence of robot-related information on trustworthiness and facial expression ratings in 60 participants, evenly split between English and German native speakers. After the information manipulation, we anticipated both trustworthiness and facial expression ratings to align with the valence of the learned information. Rating data were analysed using LMMs with fixed effects coded as sliding difference contrasts (see <xref rid="s7" ref-type="sec">Supplementary Material</xref> for details).</p><sec id="s3-s1-s1"><title>Facial expression ratings</title><p>As expected, before participants had learned information about the robots, facial expression ratings were neutral overall and showed no significant differences between conditions (see <xref rid="T1" ref-type="table">Table&#x000a0;1</xref>, <xref rid="F1" ref-type="fig">Fig.&#x000a0;1</xref>). However, after hearing the stories, participants rated the same facial expressions differently depending on the valence of the associated information. Facial expressions in the negative condition were rated as more negative than in the neutral condition and facial expressions in the positive condition were rated as more positive than in the neutral condition. An additional analysis of variance confirmed the significance of the interaction between phase and information, <italic toggle="yes">F</italic>(4, 59.53)&#x02009;=&#x02009;24.16; <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, showing that the overall differences between the information conditions increased significantly between phases. An analysis with phase nested within information showed that shifts in expression ratings from pre- to post-learning were nearly identical in the negative (&#x02206;&#x02009;=&#x02009;&#x02212;0.64) and positive (&#x02206;&#x02009;=&#x02009;0.63) conditions (cf. <xref rid="F2" ref-type="fig">Fig.&#x000a0;2B</xref>). The neutral condition showed a significant positive shift (&#x02206;&#x02009;=&#x02009;0.51). Detailed results are provided in the <xref rid="s7" ref-type="sec">Supplementary Material</xref>. An additional LMM analysis including the variable language indicated no differences in the facial expression ratings provided by English and German speakers (details in <xref rid="s7" ref-type="sec">Supplementary Material</xref>).</p><table-wrap position="float" id="T1"><label>Table&#x000a0;1.</label><caption><p>Facial expression rating results</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Predictors</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</th><th align="left" rowspan="1" colspan="1">95% CI</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>-value</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Intercept</td><td align="left" rowspan="1" colspan="1">&#x02212;0.12</td><td align="left" rowspan="1" colspan="1">[&#x02212;0.35, 0.12]</td><td align="left" rowspan="1" colspan="1">.326</td></tr><tr><td align="left" rowspan="1" colspan="1">Phase(Post-Pre)</td><td align="left" rowspan="1" colspan="1">0.17</td><td align="left" rowspan="1" colspan="1">[0.10, 0.24]</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Phase(Pre):Information(Neu-Neg)</td><td align="left" rowspan="1" colspan="1">&#x02212;0.01</td><td align="left" rowspan="1" colspan="1">[&#x02212;0.13, 0.11]</td><td align="left" rowspan="1" colspan="1">.873</td></tr><tr><td align="left" rowspan="1" colspan="1">Phase(Post):Information(Neu-Neg)</td><td align="left" rowspan="1" colspan="1">1.14</td><td align="left" rowspan="1" colspan="1">[1.02, 1.26]</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Phase(Pre):Information(Pos-Neu)</td><td align="left" rowspan="1" colspan="1">0.05</td><td align="left" rowspan="1" colspan="1">[&#x02212;0.07, 0.17]</td><td align="left" rowspan="1" colspan="1">.437</td></tr><tr><td align="left" rowspan="1" colspan="1">Phase(Post):Information(Pos-Neu)</td><td align="left" rowspan="1" colspan="1">0.17</td><td align="left" rowspan="1" colspan="1">[0.05, 0.29]</td><td align="left" rowspan="1" colspan="1">.<bold>005</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">Random Effects</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">SD</td></tr><tr><td align="left" rowspan="1" colspan="1">Participants</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.19</td></tr><tr><td align="left" rowspan="1" colspan="1">Stimuli</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.37</td></tr><tr><td align="left" rowspan="1" colspan="1">Residual</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">1.21</td></tr><tr><td align="left" rowspan="1" colspan="1">Deviance</td><td align="left" rowspan="1" colspan="1">13&#x02009;764.60</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">Log-Likelihood</td><td align="left" rowspan="1" colspan="1">&#x02212;6882.30</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr></tbody></table><table-wrap-foot><fn id="T0001-fn1"><p>Results of linear mixed model analyses of Facial Expression Ratings in Experiment 1. <italic toggle="yes">Note</italic>. Information Conditions: Neg&#x02009;=&#x02009;Negative, Neu&#x02009;=&#x02009;Neutral, Pos&#x02009;=&#x02009;Positive; Phase Conditions: Pre&#x02009;=&#x02009;Pre-learning, i.e. before information acquisition, Post&#x02009;=&#x02009;Post-learning, i.e. after information acquisition; colons indicate nesting of fixed variables; boldface indicates statistical significance at &#x003b1;&#x02009;=&#x02009;0.05.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s3-s1-s2"><title>Trustworthiness ratings</title><p>Analysis of postlearning trustworthiness ratings yielded a main effect of information. As expected, participants rated robots as significantly more trustworthy when matched with positive information compared to neutral information. Robots matched with negative information were rated as less trustworthy in comparison to robots matched with neutral information (see <xref rid="T2" ref-type="table">Table&#x000a0;2</xref>, <xref rid="F1" ref-type="fig">Fig.&#x000a0;1</xref>). An additional analysis including the independent variable group (English vs. German speakers) revealed a main effect of group on trustworthiness ratings (b&#x02009;=&#x02009;0.30, 95% CI&#x02009;=&#x02009;[0.01, 0.59], <italic toggle="yes">P</italic>&#x02009;=&#x02009;.041). Overall, English-speaking participants rated the robots&#x02019; trustworthiness slightly higher than German-speaking participants (details in <xref rid="s7" ref-type="sec">Supplementary Material</xref>).</p><table-wrap position="float" id="T2"><label>Table&#x000a0;2.</label><caption><p>Trustworthiness rating results</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Predictors</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</th><th align="left" rowspan="1" colspan="1">95% CI</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>-value</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Intercept</td><td align="left" rowspan="1" colspan="1">0.57</td><td align="left" rowspan="1" colspan="1">[0.40, 0.74]</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Information(Neu-Neg)</td><td align="left" rowspan="1" colspan="1">3.42</td><td align="left" rowspan="1" colspan="1">[3.15, 3.70]</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Information(Pos-Neu)</td><td align="left" rowspan="1" colspan="1">0.37</td><td align="left" rowspan="1" colspan="1">[0.22, 0.53]</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Random Effects</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">SD</td></tr><tr><td align="left" rowspan="1" colspan="1">Participants</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.55</td></tr><tr><td align="left" rowspan="1" colspan="1">Information(Neu-Neg)</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.97</td></tr><tr><td align="left" rowspan="1" colspan="1">Information(Pos-Neu)</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.43</td></tr><tr><td align="left" rowspan="1" colspan="1">Stimuli</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.25</td></tr><tr><td align="left" rowspan="1" colspan="1">Residual</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">1.03</td></tr><tr><td align="left" rowspan="1" colspan="1">Deviance</td><td align="left" rowspan="1" colspan="1">6606.81</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">Log-Likelihood</td><td align="left" rowspan="1" colspan="1">&#x02212;3303.41</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr></tbody></table><table-wrap-foot><fn id="T0002-fn1"><p>Results of linear mixed model analyses of Trustworthiness Ratings in Experiment 1. <italic toggle="yes">Note</italic>. Information Conditions: Neg&#x02009;=&#x02009;Negative, Neu&#x02009;=&#x02009;Neutral, Pos&#x02009;=&#x02009;Positive. Boldface indicates statistical significance at &#x003b1;&#x02009;=&#x02009;0.05.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s3-s1-s3"><title>Discussion of Experiment 1</title><p>In Experiment 1, we observed differential effects of affective information on facial expression ratings, providing initial evidence that learned information may lead humans to perceive emotional facial expressions in robot faces. Notably, in the absence of prior information, these faces were initially rated as neutral. These findings imply that participants attributed mental states during robot face perception. Additionally, the affective information manipulation influenced participants&#x02019; trustworthiness ratings, demonstrating that people clearly distinguish between the trustworthiness of robots described as fulfilling negative, neutral, and positive tasks. The additional main effect of participant group on trustworthiness ratings may reflect genuine differences in trust evaluation among English and German speakers. Alternatively, despite our efforts to maintain consistency in meaning across languages, it could stem from nuances in the robots&#x02019; backstories conveyed differently in each language.</p><p>To account for potential task demand effects in facial expression ratings, Experiment 2 used EEG to explore the underlying neurocognitive mechanisms. ERPs were employed to directly assess the impact of information on perception, with early ERP components being less influenced by task demands.</p></sec></sec><sec id="s3-s2"><title>Experiment 2</title><p>In Experiment 2, we used EEG to investigate the neural mechanisms and temporal dynamics underlying the attribution of mental states to robot faces. We tested a new sample of 30 native German speakers, using the same information manipulation for a subset of 18 out of the 36 robots from Experiment 1.</p><p>Using ERPs, we investigated distinct stages of processing in the brain with high temporal precision, aiming to determine which stages support mental state attribution to robots. Our analysis focused on early visual perception (P1 component; <xref rid="R21" ref-type="bibr">Di Russo et&#x000a0;al. 2002</xref>, <xref rid="R37" ref-type="bibr">Haynes et&#x000a0;al. 2003</xref>), visual processing of faces and facial expressions (N170 component; <xref rid="R7" ref-type="bibr">Bentin et&#x000a0;al. 1996</xref>, <xref rid="R24" ref-type="bibr">Eimer et&#x000a0;al. 2010</xref>), fast reflexive emotional responses (EPN; <xref rid="R69" ref-type="bibr">Schupp et&#x000a0;al. 2006</xref>, <xref rid="R66" ref-type="bibr">Schacht and Sommer 2009</xref>), and more elaborate stimulus evaluation (LPP; <xref rid="R69" ref-type="bibr">Schupp et&#x000a0;al. 2006</xref>, <xref rid="R5" ref-type="bibr">Baum et&#x000a0;al. 2020</xref>).</p><p>To better understand the relationship between affective information, ERP components, and intentionality, we added a rating task to assess the intentionality attributed to each robot&#x02019;s behaviour and examine whether affective information enhances perceived intentionality. We also explored the link between specific ERP components and the likelihood of attributing intentionality to robots, considering the intentionality rating as a covariate.</p><sec id="s3-s2-s1"><title>Rating results</title><p>Facial expression ratings were collected after the learning part. Each robot&#x02019;s facial expression was rated 12 times. We report results based on the first rating of each stimulus. Facial expression ratings were again influenced by affective information: expressions of robots associated with negative information were rated as more negative than those associated with neutral information (<italic toggle="yes">b</italic>&#x02009;=&#x02009;0.69, 95% CI&#x02009;=&#x02009;[0.40, 0.97], <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001). Expressions of robots in the positive information condition were rated as slightly more positive compared to the neutral information condition, but this comparison only yielded a statistical trend (<italic toggle="yes">b</italic>&#x02009;=&#x02009;0.18, 95% CI&#x02009;=&#x02009;[&#x02212;0.01, 0.37], <italic toggle="yes">P</italic>&#x02009;=&#x02009;.069).</p><p>Trustworthiness ratings were collected before and after learning. In the baseline ratings before learning, no significant differences were found between the robots assigned to the three information conditions (neutral&#x02014;negative: <italic toggle="yes">b</italic>&#x02009;=&#x02009;0.09, 95% CI&#x02009;=&#x02009;[&#x02212;0.18, 0.35], <italic toggle="yes">P</italic>&#x02009;=&#x02009;.506; positive&#x02014;neutral: <italic toggle="yes">b</italic>&#x02009;=&#x02009;0.03, 95% CI&#x02009;=&#x02009;[&#x02212;0.19, 0.25], <italic toggle="yes">P</italic>&#x02009;=&#x02009;.802). However, after learning, significant differences emerged (neutral&#x02014;negative: <italic toggle="yes">b</italic>&#x02009;=&#x02009;1.83, 95% CI&#x02009;=&#x02009;[1.57, 2.10], <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001; positive&#x02014;neutral: <italic toggle="yes">b</italic>&#x02009;=&#x02009;0.33, 95% CI&#x02009;=&#x02009;[0.11, 0.55], <italic toggle="yes">P</italic>&#x02009;=&#x02009;.003). An additional analysis of variance confirmed the significance of the interaction between phase and information, <italic toggle="yes">F</italic>(2, 973.78)&#x02009;=&#x02009;114.95; <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, showing that the overall differences between trustworthiness ratings in the different information conditions increased significantly between phases. Taken together, the results of Experiment 2 align well with those obtained in Experiment 1.</p><p>Intentionality ratings, assessed on a scale from &#x02212;50 (non-intentional) to 50 (intentional), yielded a mean of &#x02212;7.05 (95% CI&#x02009;=&#x02009;[&#x02212;13.34, &#x02212;0.69]), indicating a tendency towards nonintentional explanations, though not entirely so. Analysis showed that robots associated with negative information were rated as more intentional than those with neutral information (<italic toggle="yes">b</italic>&#x02009;=&#x02009;&#x02212;9.67, 95% CI&#x02009;=&#x02009;[&#x02212;15.62, &#x02212;3.73], <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001). No significant difference was found between neutral and positive information (<italic toggle="yes">b</italic>&#x02009;=&#x02009;3.19, 95% CI&#x02009;=&#x02009;[&#x02212;2.76, 9.14], <italic toggle="yes">P</italic>&#x02009;=&#x02009;.293). This suggests that framing robots with negative behaviour increases perceived intentionality. Overall, affective information influenced ratings of facial expression, trustworthiness, and perceived intentionality.</p></sec><sec id="s3-s2-s2"><title>EEG results</title><p>We tested the effects of negative, neutral, and positive information on the processing of robot faces presented during the facial expression rating task. Specifically, we analysed ERP components associated with early perceptual processing (P1 and N170), reflexive emotional responses to visual input (EPN), and higher-level evaluation (LPP).</p><p>We observed significant influences of affective information on the N170 and LPP components, but not the P1 and EPN components (see <xref rid="T3 T4" ref-type="table">Tables&#x000a0;3 and 4</xref> and <xref rid="F2" ref-type="fig">Fig.&#x000a0;2</xref>). Both N170 and LPP amplitudes were significantly increased in the negative information condition compared to the neutral information condition. There were no significant differences between the neutral and the positive information conditions.</p><table-wrap position="float" id="T3"><label>Table&#x000a0;3.</label><caption><p>Perception-related EEG results</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th valign="bottom" align="left" rowspan="1" colspan="1">&#x000a0;</th><th valign="bottom" colspan="3" align="center" rowspan="1">P1 component</th><th valign="bottom" colspan="3" align="center" rowspan="1">N170 component</th></tr><tr><th align="left" rowspan="1" colspan="1">Predictors</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</th><th align="left" rowspan="1" colspan="1">95% CI</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>-value</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</th><th align="left" rowspan="1" colspan="1">95% CI</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>-value</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Intercept</td><td align="left" rowspan="1" colspan="1">3.11</td><td align="left" rowspan="1" colspan="1">[1.79, 4.43]</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td><td align="left" rowspan="1" colspan="1">&#x02212;5.22</td><td align="left" rowspan="1" colspan="1">[&#x02212;6.53, &#x02212;3.92]</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Information(Neu-Pos)</td><td align="left" rowspan="1" colspan="1">&#x02212;0.12</td><td align="left" rowspan="1" colspan="1">[&#x02212;0.43, 0.19]</td><td align="left" rowspan="1" colspan="1">.455</td><td align="left" rowspan="1" colspan="1">0.21</td><td align="left" rowspan="1" colspan="1">[&#x02212;0.06, 0.47]</td><td align="left" rowspan="1" colspan="1">.127</td></tr><tr><td align="left" rowspan="1" colspan="1">Information(Neg-Neu)</td><td align="left" rowspan="1" colspan="1">0.10</td><td align="left" rowspan="1" colspan="1">[&#x02212;0.21, 0.42]</td><td align="left" rowspan="1" colspan="1">.524</td><td align="left" rowspan="1" colspan="1">&#x02212;0.34</td><td align="left" rowspan="1" colspan="1">[&#x02212;0.60, &#x02212;0.08]</td><td align="left" rowspan="1" colspan="1">.<bold>012</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">Random Effects</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">SD</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">SD</td></tr><tr><td align="left" rowspan="1" colspan="1">Participants</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">3.45</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">3.35</td></tr><tr><td align="left" rowspan="1" colspan="1">Stimuli</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.59</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.82</td></tr><tr><td align="left" rowspan="1" colspan="1">Residual</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">5.24</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">4.42</td></tr><tr><td align="left" rowspan="1" colspan="1">Deviance</td><td align="left" rowspan="1" colspan="1">39&#x02009;670.08</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">37&#x02009;501.78</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">log-Likelihood</td><td align="left" rowspan="1" colspan="1">&#x02212;19&#x02009;835.04</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#x02212;18&#x02009;750.89</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr></tbody></table><table-wrap-foot><fn id="T0003-fn1"><p>Results of linear mixed model analyses of the P1 and N170 components. <italic toggle="yes">Note</italic>. Information Conditions: Neg&#x02009;=&#x02009;Negative, Neu&#x02009;=&#x02009;Neutral, Pos&#x02009;=&#x02009;Positive. Boldface indicates statistical significance at &#x003b1;&#x02009;=&#x02009;0.05.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="T4"><label>Table&#x000a0;4.</label><caption><p>Emotion-related EEG results</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th valign="bottom" align="left" rowspan="1" colspan="1">&#x000a0;</th><th valign="bottom" colspan="3" align="center" rowspan="1">EPN</th><th valign="bottom" colspan="3" align="center" rowspan="1">LPP</th></tr><tr><th align="left" rowspan="1" colspan="1">Predictors</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</th><th align="left" rowspan="1" colspan="1">95% CI</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>-value</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</th><th align="left" rowspan="1" colspan="1">95% CI</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>-value</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Intercept</td><td align="left" rowspan="1" colspan="1">&#x02212;1.02</td><td align="left" rowspan="1" colspan="1">[&#x02212;2.38, 0.33]</td><td align="left" rowspan="1" colspan="1">.135</td><td align="left" rowspan="1" colspan="1">4.35</td><td align="left" rowspan="1" colspan="1">[3.60, 5.10]</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Information(Neu-Pos)</td><td align="left" rowspan="1" colspan="1">&#x02212;0.06</td><td align="left" rowspan="1" colspan="1">[&#x02212;0.34, 0.21]</td><td align="left" rowspan="1" colspan="1">.657</td><td align="left" rowspan="1" colspan="1">&#x02212;0.16</td><td align="left" rowspan="1" colspan="1">[&#x02212;0.48, 0.16]</td><td align="left" rowspan="1" colspan="1">.322</td></tr><tr><td align="left" rowspan="1" colspan="1">Information(Neg-Neu)</td><td align="left" rowspan="1" colspan="1">&#x02212;0.19</td><td align="left" rowspan="1" colspan="1">[&#x02212;0.46, 0.09]</td><td align="left" rowspan="1" colspan="1">.181</td><td align="left" rowspan="1" colspan="1">0.48</td><td align="left" rowspan="1" colspan="1">[0.18, 0.78]</td><td align="left" rowspan="1" colspan="1">.<bold>003</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">Random Effects</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">SD</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">SD</td></tr><tr><td align="left" rowspan="1" colspan="1">Participants</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">3.35</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">1.94</td></tr><tr><td align="left" rowspan="1" colspan="1">Information(Neu-Neg)</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.447</td></tr><tr><td align="left" rowspan="1" colspan="1">Information(Pos-Neu)</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.36</td></tr><tr><td align="left" rowspan="1" colspan="1">Stimuli</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">1.14</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.35</td></tr><tr><td align="left" rowspan="1" colspan="1">Residual</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">4.60</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">4.45</td></tr><tr><td align="left" rowspan="1" colspan="1">Deviance</td><td align="left" rowspan="1" colspan="1">38&#x02009;020.26</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">37&#x02009;543.59</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">log-Likelihood</td><td align="left" rowspan="1" colspan="1">&#x02212;19&#x02009;010.13</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#x02212;18&#x02009;771.79</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr></tbody></table><table-wrap-foot><fn id="T0004-fn1"><p>Results of linear mixed model analyses of the EPN and LPP components. <italic toggle="yes">Note</italic>. Information Conditions: Neg&#x02009;=&#x02009;Negative, Neu&#x02009;=&#x02009;Neutral, Pos&#x02009;=&#x02009;Positive. Boldface indicates statistical significance at &#x003b1;&#x02009;=&#x02009;0.05.</p></fn></table-wrap-foot></table-wrap><p>We examined whether perceived intentionality was linked to the impact of information on the N170 and LPP components by calculating an additional LMM for each, using centered intentionality scores as a covariate. We found an interaction between intentionality and information (negative&#x02014;neutral) in the N170 component (<italic toggle="yes">b</italic>&#x02009;=&#x02009;&#x02212;0.29, 95% CI&#x02009;=&#x02009;[&#x02212;0.57, &#x02212;0.01], <italic toggle="yes">P</italic>&#x02009;=&#x02009;.045), with higher intentionality scores associated with a greater effect of negative information on N170 amplitudes. In the LPP, higher intentionality scores were linked to lower LPP amplitudes (<italic toggle="yes">b</italic>&#x02009;=&#x02009;&#x02212;0.14, 95% CI&#x02009;=&#x02009;[&#x02212;0.28, &#x02212;0.01], <italic toggle="yes">P</italic>&#x02009;=&#x02009;.033).</p><p>We conducted two additional control analyses, detailed in the <xref rid="s7" ref-type="sec">Supplementary Material</xref>. To rule out task demand effects, we included participants&#x02019; scores on the Perceived Awareness of the Research Hypothesis Scale (<xref rid="R65" ref-type="bibr">Rubin 2016</xref>) as a covariate in the analysis of expression ratings from Experiment 2. This analysis confirmed that the main effect of negative knowledge remained robust. A trend was observed for an interaction between hypothesis awareness and the negative knowledge effect, whereby participants with the least awareness of the hypothesis tended to exhibit the strongest knowledge effects. These findings suggest that social desirability or explicit demand characteristics did not significantly influence the results. If anything, participants aware of the hypothesis might have actively avoided giving biased responses, countering concerns about social desirability. Additionally, we tested whether differences in story realism ratings (neutral stories were rated as more realistic than positive and negative ones during prerating) influenced EEG or expression rating results by including each story&#x02019;s realism rating as a covariate. This analysis revealed no main effects of realism or interactions with knowledge effects on ERP components (N170, EPN, LPP) or facial expression ratings (see <xref rid="s7" ref-type="sec">Supplementary Material</xref> for details).</p></sec><sec id="s3-s2-s3"><title>Discussion of Experiment 2</title><p>In line with the findings of Experiment 1, the affective information manipulation significantly influenced trustworthiness and facial expression ratings. We also observed an impact of affective information on perceived intentionality, indicating that robots were judged as more intentional when they were linked to negative backstories. This corresponds with the asymmetry often noted in judgments of human behaviour, where people tend to attribute greater intentionality for actions with negative outcomes compared to positive ones (<xref rid="R45" ref-type="bibr">Knobe 2003</xref>, <xref rid="R28" ref-type="bibr">Feltz 2007</xref>).</p><p>In ERPs, we observed significant information effects on two distinct processing stages: the N170 component, associated with visual perception, and the later LPP component, reflecting more elaborate stimulus evaluation. The N170 is most commonly associated with structural visual encoding of faces, and it has been shown to be sensitive to manipulations of facial expression, as well as the realism of face images (<xref rid="R7" ref-type="bibr">Bentin et&#x000a0;al. 1996</xref>, <xref rid="R68" ref-type="bibr">Schindler et&#x000a0;al. 2017</xref>, <xref rid="R67" ref-type="bibr">Schindler and Bublatzky 2020</xref>). Thus, robot faces associated with negative information may be perceived as displaying an emotional facial expression. Alternatively, since robot faces represent intermediate stimuli between faces and objects (<xref rid="R32" ref-type="bibr">Geiger and Balas 2021</xref>), affective knowledge could shift their perception towards more &#x02018;face-like&#x02019; processing. Both interpretations support the idea that affective information facilitates attribution of mental states to robot faces at an early, potentially automatic perceptual stage. The differentiation of the N170 for identical robot faces based solely on associated affective information represents, to our knowledge, a novel finding. This highlights how top-down influences, such as semantic or affective knowledge, can modulate early visual components (P1 and N1/N170). Such effects have been previously documented for other stimulus categories, including human faces (<xref rid="R64" ref-type="bibr">Righart and de Gelder 2006</xref>, <xref rid="R48" ref-type="bibr">Luo et&#x000a0;al. 2016</xref>), objects (<xref rid="R54" ref-type="bibr">Maier et&#x000a0;al. 2014</xref>, <xref rid="R51" ref-type="bibr">Maier and Abdel Rahman 2019</xref>, <xref rid="R52" ref-type="bibr">2024</xref>, <xref rid="R26" ref-type="bibr">Enge et&#x000a0;al. 2023</xref>), and colours (<xref rid="R77" ref-type="bibr">Thierry et&#x000a0;al. 2009</xref>, <xref rid="R50" ref-type="bibr">Maier and Abdel Rahman 2018</xref>).</p><p>The information effect on the LPP indicates that negatively framed robots are evaluated as more emotionally relevant compared to neutrally framed robots (<xref rid="R3" ref-type="bibr">Abubshait and Wiese</xref>, <xref rid="R69" ref-type="bibr">Schupp et&#x000a0;al. 2006</xref>). Affective information did not influence the P1 component, indicating that low-level visual processing remained unaffected. Since previous studies have demonstrated knowledge effects in the P1 for objects (<xref rid="R2" ref-type="bibr">Abdel Rahman and Sommer 2008</xref>, <xref rid="R54" ref-type="bibr">Maier et&#x000a0;al. 2014</xref>), this suggests that the visual processing of robot faces leaned more towards face perception rather than object perception.</p><p>Notably, contrary to our prediction, affective information did not influence the EPN component. This suggests that the modulation of early emotional responses typically induced by affective information for human faces (e.g. <xref rid="R1" ref-type="bibr">Abdel Rahman 2011</xref>, <xref rid="R75" ref-type="bibr">Suess et&#x000a0;al. 2015</xref>, <xref rid="R48" ref-type="bibr">Luo et&#x000a0;al. 2016</xref>, <xref rid="R4" ref-type="bibr">Baum and Abdel Rahman 2021</xref>, <xref rid="R81" ref-type="bibr">Ziereis and Schacht 2023</xref>) is absent or attenuated for robot faces. We propose this stems from meaningful differences in social perception between human and robot faces, suggesting that processing of robots tends to be more visually driven and less emotionally resonant. This point is elaborated in the general discussion. Still, other factors might explain why N170 but not EPN effects were observed. For example, the N170 is known to be particularly sensitive to the eyes (<xref rid="R41" ref-type="bibr">Itier et&#x000a0;al. 2007</xref>), whereas the EPN appears to be sensitive to the mouth region (<xref rid="R46" ref-type="bibr">Langeslag et&#x000a0;al. 2018</xref>). Given that many robot stimuli in our study featured large, prominent eyes but small or absent mouths, the design characteristics of the robots may have contributed to this dissociation. Future studies should investigate how specific facial features of robots interact with affective context information. Additionally, the highly stylised nature of the robot faces in our study may have played a role. <xref rid="R68" ref-type="bibr">Schindler et&#x000a0;al. (2017)</xref> reported differences in N170 localisation between cartoon and realistic faces, suggesting that cartoon faces rely more on structural analysis associated with the occipital face area, while realistic faces engage holistic processing in the fusiform face area. This may explain why we observed a modulation of the N170, as the processing of stylised faces seems to rely heavily on lower-level visual processes. However, stylisation alone cannot fully account for our findings. Schindler <italic toggle="yes">et&#x000a0;al</italic>. also showed that the effect of emotional expressions on the EPN does not differ between stylised and realistic human faces, indicating that stylisation alone cannot explain the absence of an EPN effect in our study.</p><p>The analysis including perceived intentionality as a covariate allowed us to explore the relationship between affective information, intentionality, and the neurocognitive processes reflected in the N170 and LPP components. The interaction between affective information and perceived intentionality on the N170 further links the perceptual bias caused by negative information to the attribution of intentionality. Two interpretations are plausible: visual face processing enhanced by negative information may increase mental state attribution, or robot faces inherently more predisposed to attribution of intentionality may be more likely to have emotional expressions inferred onto them. This open question warrants further exploration in future research. Moreover, higher perceived intentionality scores were associated with a reduction in LPP amplitudes, independent of affective information. This may indicate that the cognitive effort required for emotional evaluation decreases for robot faces perceived as more intentional (<xref rid="R57" ref-type="bibr">Matsuda and Nittono 2015</xref>, <xref rid="R25" ref-type="bibr">Eiserbeck et&#x000a0;al. 2023</xref>). Robot faces more easily attributed with intentionality might require less deliberation during social-emotional processing, suggesting a more efficient use of theory of mind.</p></sec></sec></sec><sec id="s4"><title>General discussion</title><p>Humanoid social robots present an intriguing puzzle: people often intuitively interact with robots as they would with another human, even though they may be explicitly aware that robots are mechanical artefacts that do not share the same cognitive abilities as humans (<xref rid="R18" ref-type="bibr">de Graaf 2016</xref>, <xref rid="R15" ref-type="bibr">Clark and Fischer 2022</xref>). Thus, people may perceive robots through a &#x02018;physical&#x02019; or &#x02018;intentional&#x02019; lens, sometimes attributing mechanical causation to their behaviour, and at other times, mental causation. In this study, we explored how these shifting perspectives manifest during the perception of robot faces, focusing on the extent to which brain processing reveals the attribution of mental states.</p><p>Experiment 1 established that the valence of information about robots&#x02019; behaviour influenced people&#x02019;s ratings of robots&#x02019; trustworthiness and facial expressions. The same robots were distrusted when associated with negative information (e.g. the robot reports children to the secret police), but were trusted when associated with positive information (e.g. the robot teaches social skills to people with autism). Crucially, participants also rated the objectively neutral facial expressions of robots as more negative when paired with negative information and as more positive when paired with positive information, compared to neutral information. These results provided initial evidence that people in fact read good or bad intentions into robot faces&#x02014;an effect previously observed only during the perception of human faces (<xref rid="R1" ref-type="bibr">Abdel Rahman 2011</xref>, <xref rid="R75" ref-type="bibr">Suess et&#x000a0;al. 2015</xref>).</p><p>Experiment 2 showed that affective information influenced brain dynamics underlying the attribution of mental states to robot faces, affecting both perceptual encoding (N170 ERP component) and more elaborate stimulus evaluation (LPP component). The modulation of the N170 highlights the speed of mental state attribution: changes to perceptual processing occur within 130 to 180&#x02009;ms, at a stage that typically precedes conscious access (<xref rid="R29" ref-type="bibr">F&#x000f6;rster et&#x000a0;al. 2020</xref>). This effect suggests that people not only evaluate robot faces differently, but literally see bad intentions in a robot face associated with negative behaviour. The later effect of information on the LPP indicates that negatively framed robots are also evaluated as more emotionally relevant. Interestingly, contrary to our prediction, affective information did not influence the EPN during robot face processing, indicating the absence of an early, reflexive emotional response typically elicited by similar affective information in the processing of human faces (<xref rid="R1" ref-type="bibr">Abdel Rahman 2011</xref>, <xref rid="R75" ref-type="bibr">Suess et&#x000a0;al. 2015</xref>). The notion that affective responses are less malleable in response to robots compared to humans is supported by fMRI-evidence: <xref rid="R13" ref-type="bibr">Chaminade et&#x000a0;al. (2010)</xref> found that the perception of emotions expressed by a humanoid robot engaged brain regions associated with visual processing (occipital and posterior temporal cortices) more strongly than those associated with emotional resonance, such as the anterior insula and orbitofrontal cortex. Similarly, brain activity linked to social bonding does not increase over prolonged interaction with robots, as it does in human&#x02013;human interaction (<xref rid="R72" ref-type="bibr">Spatola and Chaminade 2022</xref>). Together, these findings suggest partly distinct mechanisms underlying social perception of robots, emphasising visual processing rather than reflexive emotional arousal.</p><p>Our exploratory analyses uncovered further connections between perceived intentionality, affective information, and brain responses. Firstly, robots were perceived as more intentional when paired with negative rather than neutral information, suggesting that framing robots with emotional background information enhances the attribution of mental causes for their actions. Secondly, the perceived intentionality of individual robots was associated with the impact of negative information on the N170 component. This correlation further underscores the idea that the intentionality we attribute to a robot face based on affective information is supported by visual processes, as something we automatically perceive in its facial expression.</p><p>Taken together, it appears that both the intentional and physical stance are reflected in brain dynamics, but at different processing stages. In line with the intentional stance, we rapidly and automatically read mental states into robot faces during visual perception (N170). We also explicitly evaluate robots in the light of acquired information, as shown in ratings and the LPP. However, in our fast emotional reaction (EPN), we are not as affected as we would be by comparable negative information about other humans. This suggests that the brain&#x02019;s emotional response to humanoid robots is influenced more by the physical stance than by social or intentional aspects. In conclusion, while we do engage perceptual and cognitive aspects of social cognition to process social robots, our findings on emotional processing suggest that we do not experience them as fully fledged intentional and social agents.</p><sec id="s4-s1"><title>Implications for social robotics and policy</title><p>Our results emphasise that acceptance and moral judgments of social robots hinge on the interplay of intentionality and emotional valence. Emotional information can increase perceived intentionality, both on the level of explicit ratings and automatic perceptual processing in the brain. Intentionality plays a key role in moral judgment, both in that mindedness may be a prerequisite for moral responsibility and that moral transgressions may necessitate intentional agency (<xref rid="R43" ref-type="bibr">Killen et&#x000a0;al. 2011</xref>, <xref rid="R17" ref-type="bibr">Decety and Cacioppo 2012</xref>, <xref rid="R35" ref-type="bibr">Gray et&#x000a0;al. 2012</xref>, <xref rid="R3" ref-type="bibr">Abubshaitand Wiese</xref> ; <xref rid="R79" ref-type="bibr">Wiese et&#x000a0;al. 2017</xref>).</p><p>Although humanoid robots lack actual intentionality, our results show that contextual semantic and emotional cues can induce perceptions of intentionality, potentially leading to moral judgments based on these attributions. If robots are perceived as more intentional when associated with negative actions, they may be inappropriately judged as morally responsible. This raises concerns about &#x02018;responsibility gaps,&#x02019; where no party is properly held accountable for harmful outcomes caused by artificial agents (<xref rid="R58" ref-type="bibr">Matthias 2004</xref>, <xref rid="R71" ref-type="bibr">Sparrow 2007</xref>, <xref rid="R11" ref-type="bibr">Bryson 2018</xref>, <xref rid="R8" ref-type="bibr">Bigman et&#x000a0;al. 2019</xref>). Thus, the effects of psychological variables like users&#x02019; beliefs and contextual affective information should be considered in policies legislating moral responsibility when deploying artificial social agents.</p></sec></sec><sec id="s5"><title>Conclusion</title><p>We discovered that humans rapidly attribute mental states to humanoid robots following exposure to affective information about the robots&#x02019; behaviour. This was observed in brain dynamics reflecting both perceptual processing and more deliberate evaluation of robot faces, but notably absent during fast emotional processing, which lacked a component seen in the social perception of humans. These findings imply that the processing of social robots oscillates between being perceived as mindless machines and intentional agents, contingent upon the stage of perceptual and emotional processing in the brain. Such nuanced insights into the neural, cognitive, and affective mechanisms underlying the perception of robots have significant implications for social robotics, including policies regarding the moral responsibility of artificial agents. These considerations are especially pertinent given the projected proliferation of such agents in our societies.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="sup1" position="float" content-type="local-data"><label>nsaf027_Supp</label><media xlink:href="nsaf027_supp.zip"/></supplementary-material></sec></body><back><ack id="ack1"><title>Acknowledgements</title><p>The authors would like to express their gratitude to Nora Holtz and Jonathan Buchholz for their support during EEG data collection and Guido Kiecker for task programming and technical assistance.</p></ack><sec id="s6"><title>Author contributions</title><p>Martin Maier (Conceptualisation; Methodology; Investigation; Formal Analysis; Project Administration; Supervision; Visualisation; Writing&#x02014;Original Draft Preparation; Writing&#x02014;Review &#x00026; Editing), Alexander Leonhardt (Methodology; Investigation; Formal Analysis; Writing&#x02014;Original Draft Preparation; Writing&#x02014;Review &#x00026; Editing), Florian Blume (Visualisation; Writing&#x02014;Review &#x00026; Editing), Pia Bideau (Supervision; Writing&#x02014;Review &#x00026; Editing), Olaf Hellwich (Writing&#x02014;Review &#x00026; Editing; Funding Acquisition), Rasha Abdel Rahman (Conceptualisation; Methodology; Funding Acquisition; Project Administration; Resources; Supervision; Writing&#x02014;Review &#x00026; Editing).</p></sec><sec id="s7"><title>Supplementary data</title><p>
<xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> is available at <italic toggle="yes">SCAN</italic> online.</p></sec><sec sec-type="COI-statement" id="s8"><title>Conflict of interest:</title><p>The authors declare no conflicts of interest.</p></sec><sec id="s9"><title>Funding</title><p>This work was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany&#x02019;s Excellence Strategy &#x02013; EXC 2002/1 &#x02018;Science of Intelligence&#x02019; &#x02013; project number 390523135.</p></sec><sec sec-type="data-availability" id="s10"><title>Data availability</title><p>The data and analysis code that support the findings of this study are available upon publication at <ext-link xlink:href="https://osf.io/5bj7x" ext-link-type="uri">https://osf.io/5bj7x</ext-link>.</p></sec><ref-list id="ref1"><title>References</title><ref id="R1"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name>
</person-group>. <article-title>Facing good and evil: early brain signatures of affective biographical knowledge in face recognition</article-title>. <source><italic toggle="yes">Emotion</italic></source> &#x000a0;<year>2011</year>;<volume>11</volume>:<fpage>1397</fpage>&#x02013;<lpage>405</lpage>. doi: <pub-id pub-id-type="doi">10.1037/a0024717</pub-id><pub-id pub-id-type="pmid">21859200</pub-id>
</mixed-citation></ref><ref id="R2"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Sommer</surname> &#x000a0;<given-names>W</given-names></string-name></person-group>. <article-title>Seeing what we know and understand: how knowledge shapes perception</article-title>. <source><italic toggle="yes">Psychon Bull Rev</italic></source> &#x000a0;<year>2008</year>;<volume>15</volume>:<fpage>1055</fpage>&#x02013;<lpage>63</lpage>. doi: <pub-id pub-id-type="doi">10.3758/PBR.15.6.1055</pub-id><pub-id pub-id-type="pmid">19001567</pub-id>
</mixed-citation></ref><ref id="R3"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Abubshait</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Wiese</surname> &#x000a0;<given-names>E</given-names></string-name></person-group>. <article-title>You look human, but act like a machine: agent appearance and behavior modulate different aspects of human&#x02013;robot interaction</article-title>. <source><italic toggle="yes">Front Psychol</italic></source> &#x000a0;<year>2017</year>;<volume>8</volume>:<page-range>1393</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2017.01393</pub-id></mixed-citation></ref><ref id="R4"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Baum</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Emotional news affects social judgments independent of perceived media credibility</article-title>. <source><italic toggle="yes">Soc Cogn Affect Neurosci</italic></source> &#x000a0;<year>2021</year>;<volume>16</volume>:<fpage>280</fpage>&#x02013;<lpage>91</lpage>. doi: <pub-id pub-id-type="doi">10.1093/scan/nsaa164</pub-id><pub-id pub-id-type="pmid">33274748</pub-id>
</mixed-citation></ref><ref id="R5"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Baum</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Rabovsky</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Rose</surname> &#x000a0;<given-names>SB</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Clear judgments based on unclear evidence: person evaluation is strongly influenced by untrustworthy gossip</article-title>. <source><italic toggle="yes">Emotion</italic></source> &#x000a0;<year>2020</year>;<volume>20</volume>:<fpage>248</fpage>&#x02013;<lpage>60</lpage>. doi: <pub-id pub-id-type="doi">10.1037/emo0000545</pub-id><pub-id pub-id-type="pmid">30589302</pub-id>
</mixed-citation></ref><ref id="R6"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Belpaeme</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Kennedy</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Ramachandran</surname> &#x000a0;<given-names>A</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Social robots for education: a review</article-title>. <source><italic toggle="yes">Sci Rob</italic></source> &#x000a0;<year>2018</year>;<volume>3</volume>:<page-range>eaat5954</page-range>. doi: <pub-id pub-id-type="doi">10.1126/scirobotics.aat5954</pub-id></mixed-citation></ref><ref id="R7"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bentin</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Allison</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Puce</surname> &#x000a0;<given-names>A</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Electrophysiological studies of face perception in humans</article-title>. <source><italic toggle="yes">J Cogn Neurosci</italic></source> &#x000a0;<year>1996</year>;<volume>8</volume>:<fpage>551</fpage>&#x02013;<lpage>65</lpage>. doi: <pub-id pub-id-type="doi">10.1162/jocn.1996.8.6.551</pub-id><pub-id pub-id-type="pmid">20740065</pub-id>
</mixed-citation></ref><ref id="R8"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bigman</surname> &#x000a0;<given-names>YE</given-names></string-name>, <string-name><surname>Waytz</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Alterovitz</surname> &#x000a0;<given-names>R</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Holding robots responsible: the elements of machine morality</article-title>. <source><italic toggle="yes">Trends Cogn Sci</italic></source> &#x000a0;<year>2019</year>;<volume>23</volume>:<fpage>365</fpage>&#x02013;<lpage>68</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2019.02.008</pub-id><pub-id pub-id-type="pmid">30962074</pub-id>
</mixed-citation></ref><ref id="R9"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Breazeal</surname> &#x000a0;<given-names>C</given-names></string-name>
<string-name>
<surname>Dautenhahn</surname> &#x000a0;<given-names>K</given-names></string-name>
<string-name>
<surname>Kanda</surname> &#x000a0;<given-names>T</given-names></string-name>
</person-group>. <part-title>Social robotics</part-title>. In: <person-group person-group-type="editor"><string-name><surname>Siciliano</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Khatib</surname> &#x000a0;<given-names>O</given-names></string-name></person-group> (eds), <source><italic toggle="yes">Springer Handbook of Robotics</italic></source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>, <year>2016</year>, <fpage>1935</fpage>&#x02013;<lpage>72</lpage></mixed-citation></ref><ref id="R10"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Broadbent</surname> &#x000a0;<given-names>E</given-names></string-name>, <string-name><surname>Kumar</surname> &#x000a0;<given-names>V</given-names></string-name>, <string-name><surname>Li</surname> &#x000a0;<given-names>X</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Robots with display screens: a robot with a more humanlike face display is perceived to have more mind and a better personality</article-title>. <source><italic toggle="yes">PLoS ONE</italic></source> &#x000a0;<year>2013</year>;<volume>8</volume>:<page-range>e72589</page-range>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0072589</pub-id></mixed-citation></ref><ref id="R11"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bryson</surname> &#x000a0;<given-names>JJ</given-names></string-name>
</person-group>. <article-title>Patiency is not a virtue: the design of intelligent systems and systems of ethics</article-title>. <source><italic toggle="yes">Ethics Inf Technol</italic></source> &#x000a0;<year>2018</year>;<volume>20</volume>:<fpage>15</fpage>&#x02013;<lpage>26</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s10676-018-9448-6</pub-id></mixed-citation></ref><ref id="R12"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ceh</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Vanman</surname> &#x000a0;<given-names>EJ</given-names></string-name></person-group> &#x000a0;<article-title>The robots are coming! The robots are coming! Fear and empathy for human-like entities</article-title>. <source><italic toggle="yes">Preprint</italic></source>, <year>2018</year>. doi: <pub-id pub-id-type="doi">10.31234/osf.io/4cr2u</pub-id></mixed-citation></ref><ref id="R13"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chaminade</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Zecca</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Blakemore</surname> &#x000a0;<given-names>S-J</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Brain response to a humanoid robot in areas implicated in the perception of human emotional gestures</article-title>. <source><italic toggle="yes">PLOS ONE</italic></source> &#x000a0;<year>2010</year>;<volume>5</volume>:<page-range>e11577</page-range>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0011577</pub-id></mixed-citation></ref><ref id="R14"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Clark</surname> &#x000a0;<given-names>A</given-names></string-name>
</person-group>. <article-title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</article-title>. <source><italic toggle="yes">Behav Brain Sci</italic></source> &#x000a0;<year>2013</year>;<volume>36</volume>:<fpage>181</fpage>&#x02013;<lpage>204</lpage>. doi: <pub-id pub-id-type="doi">10.1017/S0140525X12000477</pub-id><pub-id pub-id-type="pmid">23663408</pub-id>
</mixed-citation></ref><ref id="R15"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Clark</surname> &#x000a0;<given-names>HH</given-names></string-name>, <string-name><surname>Fischer</surname> &#x000a0;<given-names>K</given-names></string-name></person-group>. <article-title>Social robots as depictions of social agents</article-title>. <source><italic toggle="yes">Behav Brain Sci</italic></source> &#x000a0;<year>2023</year>;<volume>46</volume>:<page-range>e21</page-range>. doi: <pub-id pub-id-type="doi">10.1017/S0140525X22000668</pub-id></mixed-citation></ref><ref id="R16"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Cross</surname> &#x000a0;<given-names>ES</given-names></string-name>, <string-name><surname>Ramsey</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Mind meets machine: towards a cognitive science of human&#x02013;machine interactions</article-title>. <source><italic toggle="yes">Trends Cogn Sci</italic></source> &#x000a0;<year>2021</year>;<volume>25</volume>:<fpage>200</fpage>&#x02013;<lpage>12</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2020.11.009</pub-id><pub-id pub-id-type="pmid">33384213</pub-id>
</mixed-citation></ref><ref id="R17"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Decety</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Cacioppo</surname> &#x000a0;<given-names>S</given-names></string-name></person-group>. <article-title>The speed of morality: a high-density electrical neuroimaging study</article-title>. <source><italic toggle="yes">J Neurophysiol</italic></source> &#x000a0;<year>2012</year>;<volume>108</volume>:<fpage>3068</fpage>&#x02013;<lpage>72</lpage>. doi: <pub-id pub-id-type="doi">10.1152/jn.00473.2012</pub-id><pub-id pub-id-type="pmid">22956794</pub-id>
</mixed-citation></ref><ref id="R18"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>de Graaf</surname> &#x000a0;<given-names>MMA</given-names></string-name>
</person-group>. <article-title>An ethical evaluation of human&#x02013;robot relationships</article-title>. <source><italic toggle="yes">Int J Soc Robot</italic></source> &#x000a0;<year>2016</year>;<volume>8</volume>:<fpage>589</fpage>&#x02013;<lpage>98</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s12369-016-0368-5</pub-id></mixed-citation></ref><ref id="R19"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>de Graaf</surname> &#x000a0;<given-names>MMA</given-names></string-name>
<string-name>
<surname>Ben Allouch</surname> &#x000a0;<given-names>S</given-names></string-name>
<string-name>
<surname>van Dijk</surname> &#x000a0;<given-names>JAGM</given-names></string-name>
</person-group>. <part-title>What makes robots social? A user&#x02019;s perspective on characteristics for social human-robot interaction</part-title>. In: <person-group person-group-type="editor"><string-name><surname>Tapus</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Andr&#x000e9;</surname> &#x000a0;<given-names>E</given-names></string-name>, <string-name><surname>Martin</surname> &#x000a0;<given-names>J-C</given-names></string-name></person-group>, <etal>et&#x000a0;al.</etal> (eds), <source><italic toggle="yes">Social Robotics</italic></source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>, <year>2015</year>, <fpage>184</fpage>&#x02013;<lpage>93</lpage></mixed-citation></ref><ref id="R20"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Dennett</surname> &#x000a0;<given-names>DC</given-names></string-name>
</person-group>. <source><italic toggle="yes">The Intentional Stance</italic></source>. <publisher-loc>Cambridge, MA, USA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <year>1989</year>.</mixed-citation></ref><ref id="R21"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Di Russo</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Martinez</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Sereno</surname> &#x000a0;<given-names>MI</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Cortical sources of the early components of the visual evoked potential</article-title>. <source><italic toggle="yes">Hum Brain Mapp</italic></source> &#x000a0;<year>2002</year>;<volume>15</volume>:<fpage>95</fpage>&#x02013;<lpage>111</lpage>. doi: <pub-id pub-id-type="doi">10.1002/hbm.10010</pub-id><pub-id pub-id-type="pmid">11835601</pub-id>
</mixed-citation></ref><ref id="R22"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Dryer</surname> &#x000a0;<given-names>DC</given-names></string-name>
</person-group>. <article-title>Getting personal with computers: how to design personalities for agents</article-title>. <source><italic toggle="yes">Appl Artif Intell</italic></source> &#x000a0;<year>1999</year>;<volume>13</volume>:<fpage>273</fpage>&#x02013;<lpage>95</lpage>. doi: <pub-id pub-id-type="doi">10.1080/088395199117423</pub-id></mixed-citation></ref><ref id="R23"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Duffy</surname> &#x000a0;<given-names>BR</given-names></string-name>
</person-group>. <article-title>Anthropomorphism and the social robot</article-title>. <source><italic toggle="yes">Rob Auton Syst</italic></source> &#x000a0;<year>2003</year>;<volume>42</volume>:<fpage>177</fpage>&#x02013;<lpage>90</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0921-8890(02)00374-3</pub-id></mixed-citation></ref><ref id="R24"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Eimer</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Kiss</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Nicholas</surname> &#x000a0;<given-names>S</given-names></string-name></person-group>. <article-title>Response profile of the face-sensitive N170 component: a rapid adaptation study</article-title>. <source><italic toggle="yes">Cereb Cortex</italic></source> &#x000a0;<year>2010</year>;<volume>20</volume>:<fpage>2442</fpage>&#x02013;<lpage>52</lpage>. doi: <pub-id pub-id-type="doi">10.1093/cercor/bhp312</pub-id><pub-id pub-id-type="pmid">20080930</pub-id>
</mixed-citation></ref><ref id="R25"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Eiserbeck</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Maier</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Baum</surname> &#x000a0;<given-names>J</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Deepfake smiles matter less&#x02014;the psychological and neural impact of presumed AI-generated faces</article-title>. <source><italic toggle="yes">Sci Rep</italic></source> &#x000a0;<year>2023</year>;<volume>13</volume>:<page-range>16111</page-range>. doi: <pub-id pub-id-type="doi">10.1038/s41598-023-42802-x</pub-id></mixed-citation></ref><ref id="R26"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Enge</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>S&#x000fc;&#x000df;</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Instant effects of semantic information on visual perception</article-title>. <source><italic toggle="yes">J Neurosci</italic></source> &#x000a0;<year>2023</year>;<volume>43</volume>:<fpage>4896</fpage>&#x02013;<lpage>906</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2038-22.2023</pub-id><pub-id pub-id-type="pmid">37286353</pub-id>
</mixed-citation></ref><ref id="R27"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Epley</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Waytz</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Cacioppo</surname> &#x000a0;<given-names>JT</given-names></string-name></person-group>. <article-title>On seeing human: a three-factor theory of anthropomorphism</article-title>. <source><italic toggle="yes">Psychol Rev</italic></source> &#x000a0;<year>2007</year>;<volume>114</volume>:<fpage>864</fpage>&#x02013;<lpage>86</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0033-295X.114.4.864</pub-id><pub-id pub-id-type="pmid">17907867</pub-id>
</mixed-citation></ref><ref id="R28"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Feltz</surname> &#x000a0;<given-names>A</given-names></string-name>
</person-group>. <article-title>The Knobe effect: a brief overview</article-title>. <source><italic toggle="yes">J Mind Behav</italic></source> &#x000a0;<year>2007</year>;<volume>28</volume>:<fpage>265</fpage>&#x02013;<lpage>77</lpage>.</mixed-citation></ref><ref id="R29"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>F&#x000f6;rster</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Koivisto</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Revonsuo</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>ERP and MEG correlates of visual consciousness: the second decade</article-title>. <source><italic toggle="yes">Conscious Cogn</italic></source> &#x000a0;<year>2020</year>;<volume>80</volume>:<page-range>102917</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.concog.2020.102917</pub-id></mixed-citation></ref><ref id="R30"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Friston</surname> &#x000a0;<given-names>K</given-names></string-name>
</person-group>. <article-title>The free-energy principle: a rough guide to the brain?</article-title> &#x000a0;<source><italic toggle="yes">Trends Cogn Sci</italic></source> &#x000a0;<year>2009</year>;<volume>13</volume>:<fpage>293</fpage>&#x02013;<lpage>301</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2009.04.005</pub-id><pub-id pub-id-type="pmid">19559644</pub-id>
</mixed-citation></ref><ref id="R31"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Fr&#x000f6;mer</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Maier</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Group-level EEG-processing pipeline for flexible single trial-based analyses including linear mixed models</article-title>. <source><italic toggle="yes">Front Neurosci</italic></source> &#x000a0;<year>2018</year>;<volume>12</volume>:<page-range>48</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fnins.2018.00048</pub-id></mixed-citation></ref><ref id="R32"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Geiger</surname> &#x000a0;<given-names>AR</given-names></string-name>, <string-name><surname>Balas</surname> &#x000a0;<given-names>B</given-names></string-name></person-group>. <article-title>Robot faces elicit responses intermediate to human faces and objects at face-sensitive ERP components</article-title>. <source><italic toggle="yes">Sci Rep</italic></source> &#x000a0;<year>2021</year>;<volume>11</volume>:<page-range>17890</page-range>. doi: <pub-id pub-id-type="doi">10.1038/s41598-021-97527-6</pub-id></mixed-citation></ref><ref id="R33"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gramfort</surname> &#x000a0;<given-names>A</given-names></string-name>
</person-group>. <article-title>MEG and EEG data analysis with MNE-Python</article-title>. <source><italic toggle="yes">Front Neurosci</italic></source> &#x000a0;<year>2013</year>;<volume>7</volume>:<page-range>267</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id></mixed-citation></ref><ref id="R34"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gray</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Wegner</surname> &#x000a0;<given-names>DM</given-names></string-name></person-group>. <article-title>Feeling robots and human zombies: mind perception and the uncanny valley</article-title>. <source><italic toggle="yes">Cognition</italic></source> &#x000a0;<year>2012</year>;<volume>125</volume>:<fpage>125</fpage>&#x02013;<lpage>30</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cognition.2012.06.007</pub-id><pub-id pub-id-type="pmid">22784682</pub-id>
</mixed-citation></ref><ref id="R35"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gray</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Young</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Waytz</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>Mind perception is the essence of morality</article-title>. <source><italic toggle="yes">Psychol Inq</italic></source> &#x000a0;<year>2012</year>;<volume>23</volume>:<fpage>101</fpage>&#x02013;<lpage>24</lpage>. doi: <pub-id pub-id-type="doi">10.1080/1047840X.2012.651387</pub-id><pub-id pub-id-type="pmid">22754268</pub-id>
</mixed-citation></ref><ref id="R36"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hassin</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Trope</surname> &#x000a0;<given-names>Y</given-names></string-name></person-group>. <article-title>Facing faces: studies on the cognitive aspects of physiognomy</article-title>. <source><italic toggle="yes">J Pers Soc Psychol</italic></source> &#x000a0;<year>2000</year>;<volume>78</volume>:<fpage>837</fpage>&#x02013;<lpage>52</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0022-3514.78.5.837</pub-id><pub-id pub-id-type="pmid">10821193</pub-id>
</mixed-citation></ref><ref id="R37"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Haynes</surname> &#x000a0;<given-names>JD</given-names></string-name>, <string-name><surname>Roth</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Stadler</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Neuromagnetic correlates of perceived contrast in primary visual cortex</article-title>. <source><italic toggle="yes">J Neurophysiol</italic></source> &#x000a0;<year>2003</year>;<volume>89</volume>:<fpage>2655</fpage>&#x02013;<lpage>66</lpage>. doi: <pub-id pub-id-type="doi">10.1152/jn.00820.2002</pub-id><pub-id pub-id-type="pmid">12612045</pub-id>
</mixed-citation></ref><ref id="R38"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Henschel</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Hortensius</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Cross</surname> &#x000a0;<given-names>ES</given-names></string-name></person-group>. <article-title>Social cognition in the age of human&#x02013;robot interaction</article-title>. <source><italic toggle="yes">Trends Neurosci</italic></source> &#x000a0;<year>2020</year>;<volume>43</volume>:<fpage>373</fpage>&#x02013;<lpage>84</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tins.2020.03.013</pub-id><pub-id pub-id-type="pmid">32362399</pub-id>
</mixed-citation></ref><ref id="R39"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hortensius</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Hekele</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Cross</surname> &#x000a0;<given-names>ES</given-names></string-name></person-group>. <article-title>The perception of emotion in artificial agents</article-title>. <source><italic toggle="yes">IEEE Trans Cogn Dev Syst</italic></source> &#x000a0;<year>2018</year>;<volume>10</volume>:<fpage>852</fpage>&#x02013;<lpage>64</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TCDS.2018.2826921</pub-id></mixed-citation></ref><ref id="R40"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ille</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Berg</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Scherg</surname> &#x000a0;<given-names>M</given-names></string-name></person-group>. <article-title>Artifact correction of the ongoing EEG using spatial filters based on artifact and brain signal topographies</article-title>. <source><italic toggle="yes">J Clin Neurophysiol</italic></source> &#x000a0;<year>2002</year>;<volume>19</volume>:<fpage>113</fpage>&#x02013;<lpage>24</lpage>. doi: <pub-id pub-id-type="doi">10.1097/00004691-200203000-00002</pub-id><pub-id pub-id-type="pmid">11997722</pub-id>
</mixed-citation></ref><ref id="R41"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Itier</surname> &#x000a0;<given-names>RJ</given-names></string-name>, <string-name><surname>Alain</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Sedore</surname> &#x000a0;<given-names>K</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>early face processing specificity: it&#x02019;s in the eyes!</article-title> &#x000a0;<source><italic toggle="yes">J Cogn Neurosci</italic></source> &#x000a0;<year>2007</year>;<volume>19</volume>:<fpage>1815</fpage>&#x02013;<lpage>26</lpage>. doi: <pub-id pub-id-type="doi">10.1162/jocn.2007.19.11.1815</pub-id><pub-id pub-id-type="pmid">17958484</pub-id>
</mixed-citation></ref><ref id="R42"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Jacobs</surname> &#x000a0;<given-names>OL</given-names></string-name>, <string-name><surname>Gazzaz</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Kingstone</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>Mind the robot! Variation in attributions of mind to a wide set of real and fictional robots</article-title>. <source><italic toggle="yes">Int J Soc Robot</italic></source> &#x000a0;<year>2022</year>;<volume>14</volume>:<fpage>529</fpage>&#x02013;<lpage>37</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s12369-021-00807-4</pub-id></mixed-citation></ref><ref id="R43"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Killen</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Mulvey</surname> &#x000a0;<given-names>KL</given-names></string-name>, <string-name><surname>Richardson</surname> &#x000a0;<given-names>C</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The accidental transgressor: morally-relevant theory of mind</article-title>. <source><italic toggle="yes">Cognition</italic></source> &#x000a0;<year>2011</year>;<volume>119</volume>:<fpage>197</fpage>&#x02013;<lpage>215</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cognition.2011.01.006</pub-id><pub-id pub-id-type="pmid">21377148</pub-id>
</mixed-citation></ref><ref id="R44"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kim</surname> &#x000a0;<given-names>TW</given-names></string-name>, <string-name><surname>Duhachek</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>Artificial intelligence and persuasion: a construal-level account</article-title>. <source><italic toggle="yes">Psychol Sci</italic></source> &#x000a0;<year>2020</year>;<volume>31</volume>:<fpage>363</fpage>&#x02013;<lpage>80</lpage>. doi: <pub-id pub-id-type="doi">10.1177/0956797620904985</pub-id><pub-id pub-id-type="pmid">32223692</pub-id>
</mixed-citation></ref><ref id="R45"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Knobe</surname> &#x000a0;<given-names>J</given-names></string-name>
</person-group>. <article-title>Intentional action in folk psychology: an experimental investigation</article-title>. <source><italic toggle="yes">Philos Psychol</italic></source> &#x000a0;<year>2003</year>;<volume>16</volume>:<fpage>309</fpage>&#x02013;<lpage>24</lpage>. doi: <pub-id pub-id-type="doi">10.1080/09515080307771</pub-id></mixed-citation></ref><ref id="R46"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Langeslag</surname> &#x000a0;<given-names>SJE</given-names></string-name>, <string-name><surname>Gootjes</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Van Strien</surname> &#x000a0;<given-names>JW</given-names></string-name></person-group>. <article-title>The effect of mouth opening in emotional faces on subjective experience and the early posterior negativity amplitude</article-title>. <source><italic toggle="yes">Brain Cogn</italic></source> &#x000a0;<year>2018</year>;<volume>127</volume>:<fpage>51</fpage>&#x02013;<lpage>59</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.bandc.2018.10.003</pub-id><pub-id pub-id-type="pmid">30316954</pub-id>
</mixed-citation></ref><ref id="R47"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lee</surname> &#x000a0;<given-names>TS</given-names></string-name>, <string-name><surname>Mumford</surname> &#x000a0;<given-names>D</given-names></string-name></person-group>. <article-title>Hierarchical Bayesian inference in the visual cortex</article-title>. <source><italic toggle="yes">J Opt Soc Am A: Opt Image Sci</italic></source> &#x000a0;<year>2003</year>;<volume>20</volume>:<fpage>1434</fpage>&#x02013;<lpage>48</lpage>. doi: <pub-id pub-id-type="doi">10.1364/JOSAA.20.001434</pub-id></mixed-citation></ref><ref id="R48"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Luo</surname> &#x000a0;<given-names>QL</given-names></string-name>, <string-name><surname>Wang</surname> &#x000a0;<given-names>HL</given-names></string-name>, <string-name><surname>Dzhelyova</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Effect of affective personality information on face processing: evidence from ERPs</article-title>. <source><italic toggle="yes">Front Psychol</italic></source> &#x000a0;<year>2016</year>;<volume>7</volume>:<page-range>810</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2016.00810</pub-id></mixed-citation></ref><ref id="R49"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lupyan</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Boroditsky</surname> &#x000a0;<given-names>L</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Effects of language on visual perception</article-title>. <source><italic toggle="yes">Trends Cogn Sci</italic></source> &#x000a0;<year>2020</year>;<volume>24</volume>:<fpage>930</fpage>&#x02013;<lpage>44</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2020.08.005</pub-id><pub-id pub-id-type="pmid">33012687</pub-id>
</mixed-citation></ref><ref id="R50"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Maier</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Native language promotes access to visual consciousness</article-title>. <source><italic toggle="yes">Psychol Sci</italic></source> &#x000a0;<year>2018</year>;<volume>29</volume>:<fpage>1757</fpage>&#x02013;<lpage>72</lpage>. doi: <pub-id pub-id-type="doi">10.1177/0956797618782181</pub-id><pub-id pub-id-type="pmid">30248272</pub-id>
</mixed-citation></ref><ref id="R51"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Maier</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>No matter how: top-down effects of verbal and semantic category knowledge on early visual perception</article-title>. <source><italic toggle="yes">Cogn Affect Behav Neurosci</italic></source> &#x000a0;<year>2019</year>;<volume>19</volume>:<fpage>859</fpage>&#x02013;<lpage>76</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13415-018-00679-8</pub-id><pub-id pub-id-type="pmid">30607831</pub-id>
</mixed-citation></ref><ref id="R52"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Maier</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Transient and long-term linguistic influences on visual perception: shifting brain dynamics with memory consolidation</article-title>. <source><italic toggle="yes">Lang Learn</italic></source> &#x000a0;<year>2024</year>;<volume>74</volume>:<fpage>157</fpage>&#x02013;<lpage>84</lpage>. doi: <pub-id pub-id-type="doi">10.1111/lang.12631</pub-id></mixed-citation></ref><ref id="R53"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Maier</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Blume</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Bideau</surname> &#x000a0;<given-names>P</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Knowledge-augmented face perception: prospects for the Bayesian brain-framework to align AI and human vision</article-title>. <source><italic toggle="yes">Conscious Cogn</italic></source> &#x000a0;<year>2022</year>;<volume>101</volume>:<page-range>103301</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.concog.2022.103301</pub-id></mixed-citation></ref><ref id="R54"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Maier</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Glage</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Hohlfeld</surname> &#x000a0;<given-names>A</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Does the semantic content of verbal categories influence categorical perception? An ERP study</article-title>. <source><italic toggle="yes">Brain Cogn</italic></source> &#x000a0;<year>2014</year>;<volume>91</volume>:<fpage>1</fpage>&#x02013;<lpage>10</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.bandc.2014.07.008</pub-id><pub-id pub-id-type="pmid">25163810</pub-id>
</mixed-citation></ref><ref id="R55"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Marchesi</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Ghiglino</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Ciardo</surname> &#x000a0;<given-names>F</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Do we adopt the intentional stance toward humanoid robots?</article-title> &#x000a0;<source><italic toggle="yes">Front Psychol</italic></source> &#x000a0;<year>2019</year>;<volume>10</volume>:<page-range>450</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2019.00450</pub-id></mixed-citation></ref><ref id="R56"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Martini</surname> &#x000a0;<given-names>MC</given-names></string-name>, <string-name><surname>Gonzalez</surname> &#x000a0;<given-names>CA</given-names></string-name>, <string-name><surname>Wiese</surname> &#x000a0;<given-names>E</given-names></string-name></person-group>. <article-title>Seeing minds in others&#x02014;can agents with robotic appearance have human-like preferences?</article-title> &#x000a0;<source><italic toggle="yes">Plos one</italic></source> &#x000a0;<year>2016</year>;<volume>11</volume>:<page-range>e0146310</page-range>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0146310</pub-id></mixed-citation></ref><ref id="R57"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Matsuda</surname> &#x000a0;<given-names>I</given-names></string-name>, <string-name><surname>Nittono</surname> &#x000a0;<given-names>H</given-names></string-name></person-group>. <article-title>Motivational significance and cognitive effort elicit different late positive potentials</article-title>. <source><italic toggle="yes">Clin Neurophysiol</italic></source> &#x000a0;<year>2015</year>;<volume>126</volume>:<fpage>304</fpage>&#x02013;<lpage>13</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.clinph.2014.05.030</pub-id><pub-id pub-id-type="pmid">25037654</pub-id>
</mixed-citation></ref><ref id="R58"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Matthias</surname> &#x000a0;<given-names>A</given-names></string-name>
</person-group>. <article-title>The responsibility gap: ascribing responsibility for the actions of learning automata</article-title>. <source><italic toggle="yes">Ethics Inf Technol</italic></source> &#x000a0;<year>2004</year>;<volume>6</volume>:<fpage>175</fpage>&#x02013;<lpage>83</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s10676-004-3422-1</pub-id></mixed-citation></ref><ref id="R59"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Onnasch</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Roesler</surname> &#x000a0;<given-names>E</given-names></string-name></person-group>. <article-title>Anthropomorphizing robots: the effect of framing in human-robot collaboration</article-title>. <source><italic toggle="yes">Proceed Hum Fact Ergon Soc Ann Meet</italic></source> &#x000a0;<year>2019</year>;<volume>63</volume>:<fpage>1311</fpage>&#x02013;<lpage>15</lpage>. doi: <pub-id pub-id-type="doi">10.1177/1071181319631209</pub-id></mixed-citation></ref><ref id="R60"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Otten</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Seth</surname> &#x000a0;<given-names>AK</given-names></string-name>, <string-name><surname>Pinto</surname> &#x000a0;<given-names>Y</given-names></string-name></person-group>. <article-title>A social bayesian brain: how social knowledge can shape visual perception</article-title>. <source><italic toggle="yes">Brain Cogn</italic></source> &#x000a0;<year>2017</year>;<volume>112</volume>:<fpage>69</fpage>&#x02013;<lpage>77</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.bandc.2016.05.002</pub-id><pub-id pub-id-type="pmid">27221986</pub-id>
</mixed-citation></ref><ref id="R61"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Paolillo</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Colella</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Nosengo</surname> &#x000a0;<given-names>N</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>How to compete with robots by assessing job automation risks and resilient alternatives</article-title>. <source><italic toggle="yes">Sci Rob</italic></source> &#x000a0;<year>2022</year>;<volume>7</volume>:<page-range>eabg5561</page-range>. doi: <pub-id pub-id-type="doi">10.1126/scirobotics.abg5561</pub-id></mixed-citation></ref><ref id="R62"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Perez-Osorio</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Wykowska</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>Adopting the intentional stance toward natural and artificial agents</article-title>. <source><italic toggle="yes">Philos Psychol</italic></source> &#x000a0;<year>2020</year>;<volume>33</volume>:<fpage>369</fpage>&#x02013;<lpage>95</lpage>. doi: <pub-id pub-id-type="doi">10.1080/09515089.2019.1688778</pub-id></mixed-citation></ref><ref id="R63"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Press</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Yon</surname> &#x000a0;<given-names>D</given-names></string-name></person-group>. <article-title>Perceptual prediction: rapidly making sense of a noisy world</article-title>. <source><italic toggle="yes">Curr Biol</italic></source> &#x000a0;<year>2019</year>;<volume>29</volume>:<fpage>R751</fpage>&#x02013;<lpage>53</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2019.06.054</pub-id><pub-id pub-id-type="pmid">31386853</pub-id>
</mixed-citation></ref><ref id="R64"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Righart</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>de Gelder</surname> &#x000a0;<given-names>B</given-names></string-name></person-group>. <article-title>Context influences early perceptual analysis of faces&#x02014;an electrophysiological study</article-title>. <source><italic toggle="yes">Cereb Cortex</italic></source> &#x000a0;<year>2006</year>;<volume>16</volume>:<fpage>1249</fpage>&#x02013;<lpage>57</lpage>. doi: <pub-id pub-id-type="doi">10.1093/cercor/bhj066</pub-id><pub-id pub-id-type="pmid">16306325</pub-id>
</mixed-citation></ref><ref id="R65"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Rubin</surname> &#x000a0;<given-names>M</given-names></string-name>
</person-group> (<year>2016</year>). <article-title>The perceived awareness of the research hypothesis scale: assessing the influence of demand characteristics</article-title>. [<comment>Database record</comment>]. APA PsycTests.</mixed-citation></ref><ref id="R66"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schacht</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Sommer</surname> &#x000a0;<given-names>W</given-names></string-name></person-group>. <article-title>Emotions in word and face processing: early and late cortical responses</article-title>. <source><italic toggle="yes">Brain Cogn</italic></source> &#x000a0;<year>2009</year>;<volume>69</volume>:<fpage>538</fpage>&#x02013;<lpage>50</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.bandc.2008.11.005</pub-id><pub-id pub-id-type="pmid">19097677</pub-id>
</mixed-citation></ref><ref id="R67"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schindler</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Bublatzky</surname> &#x000a0;<given-names>F</given-names></string-name></person-group>. <article-title>Attention and emotion: an integrative review of emotional face processing as a function of attention</article-title>. <source><italic toggle="yes">Cortex</italic></source> &#x000a0;<year>2020</year>;<volume>130</volume>:<fpage>362</fpage>&#x02013;<lpage>86</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cortex.2020.06.010</pub-id><pub-id pub-id-type="pmid">32745728</pub-id>
</mixed-citation></ref><ref id="R68"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schindler</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Zell</surname> &#x000a0;<given-names>E</given-names></string-name>, <string-name><surname>Botsch</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Differential effects of face-realism and emotion on event-related brain potentials and their implications for the uncanny valley theory</article-title>. <source><italic toggle="yes">Sci Rep</italic></source> &#x000a0;<year>2017</year>;<volume>7</volume>:<page-range>45003</page-range>. doi: <pub-id pub-id-type="doi">10.1038/srep45003</pub-id></mixed-citation></ref><ref id="R69"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Schupp</surname> &#x000a0;<given-names>HT</given-names></string-name>
<string-name>
<surname>Flaisch</surname> &#x000a0;<given-names>T</given-names></string-name>
<string-name>
<surname>Stockburger</surname> &#x000a0;<given-names>J</given-names></string-name>
</person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<part-title>Emotion and attention: event-related brain potential studies</part-title>. Vol. <volume>156</volume>, In: <person-group person-group-type="editor"><string-name><surname>Anders</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Ende</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Junghofer</surname> &#x000a0;<given-names>M</given-names></string-name></person-group>, <etal>et&#x000a0;al.</etal> (eds), <source><italic toggle="yes">Progress in Brain Research: Understanding Emotions</italic></source> &#x000a0;<publisher-loc>Amsterdam, NL</publisher-loc>: <publisher-name>Elsevier</publisher-name>, <year>2006</year>, <fpage>31</fpage>&#x02013;<lpage>51</lpage>.</mixed-citation></ref><ref id="R70"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sindermann</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Sha</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Zhou</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Assessing the attitude towards artificial intelligence: introduction of a short measure in German, Chinese, and English Language</article-title>. <source><italic toggle="yes">KI - K&#x000fc;nstliche Intelligenz</italic></source> &#x000a0;<year>2021</year>;<volume>35</volume>:<fpage>109</fpage>&#x02013;<lpage>18</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s13218-020-00689-0</pub-id></mixed-citation></ref><ref id="R71"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sparrow</surname> &#x000a0;<given-names>R</given-names></string-name>
</person-group>. <article-title>Killer Robots</article-title>. <source><italic toggle="yes">J Appl Philoso</italic></source> &#x000a0;<year>2007</year>;<volume>24</volume>:<fpage>62</fpage>&#x02013;<lpage>77</lpage>. doi: <pub-id pub-id-type="doi">10.1111/j.1468-5930.2007.00346.x</pub-id></mixed-citation></ref><ref id="R72"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Spatola</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Chaminade</surname> &#x000a0;<given-names>T</given-names></string-name></person-group>. <article-title>Precuneus brain response changes differently during human&#x02013;robot and human&#x02013;human dyadic social interaction</article-title>. <source><italic toggle="yes">Sci Rep</italic></source> &#x000a0;<year>2022</year>;<volume>12</volume>:<page-range>14794</page-range>. doi: <pub-id pub-id-type="doi">10.1038/s41598-022-14207-9</pub-id></mixed-citation></ref><ref id="R73"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Spatola</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Wudarczyk</surname> &#x000a0;<given-names>OA</given-names></string-name></person-group>. <article-title>Ascribing emotions to robots: explicit and implicit attribution of emotions and perceived robot anthropomorphism</article-title>. <source><italic toggle="yes">Comput Hum Behav</italic></source> &#x000a0;<year>2021</year>;<volume>124</volume>:<page-range>106934</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.chb.2021.106934</pub-id></mixed-citation></ref><ref id="R74"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Stafford</surname> &#x000a0;<given-names>RQ</given-names></string-name>, <string-name><surname>MacDonald</surname> &#x000a0;<given-names>BA</given-names></string-name>, <string-name><surname>Jayawardena</surname> &#x000a0;<given-names>C</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Does the robot have a mind? Mind perception and attitudes towards robots predict use of an eldercare robot</article-title>. <source><italic toggle="yes">Int J Soc Robot</italic></source> &#x000a0;<year>2014</year>;<volume>6</volume>:<fpage>17</fpage>&#x02013;<lpage>32</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s12369-013-0186-y</pub-id></mixed-citation></ref><ref id="R75"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Suess</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Rabovsky</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Perceiving emotions in neutral faces: expression processing is biased by affective person knowledge</article-title>. <source><italic toggle="yes">Soc Cogn Affect Neurosci</italic></source> &#x000a0;<year>2015</year>;<volume>10</volume>:<fpage>531</fpage>&#x02013;<lpage>36</lpage>. doi: <pub-id pub-id-type="doi">10.1093/scan/nsu088</pub-id><pub-id pub-id-type="pmid">24948155</pub-id>
</mixed-citation></ref><ref id="R76"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Thellman</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>De Graaf</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Ziemke</surname> &#x000a0;<given-names>T</given-names></string-name></person-group>. <article-title>Mental state attribution to robots: a systematic review of conceptions, methods, and findings</article-title>. <source><italic toggle="yes">ACM Trans Hum-Robot Interact</italic></source> &#x000a0;<year>2022</year>;<volume>11</volume>:<fpage>1</fpage>&#x02013;<lpage>51</lpage>. doi: <pub-id pub-id-type="doi">10.1145/3526112</pub-id></mixed-citation></ref><ref id="R77"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Thierry</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Athanasopoulos</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Wiggett</surname> &#x000a0;<given-names>A</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Unconscious effects of language-specific terminology on preattentive color perception</article-title>. <source><italic toggle="yes">Proc Natl Acad Sci</italic></source> &#x000a0;<year>2009</year>;<volume>106</volume>:<fpage>4567</fpage>&#x02013;<lpage>70</lpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.0811155106</pub-id><pub-id pub-id-type="pmid">19240215</pub-id>
</mixed-citation></ref><ref id="R78"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Waytz</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Gray</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Epley</surname> &#x000a0;<given-names>N</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Causes and consequences of mind perception</article-title>. <source><italic toggle="yes">Trends Cogn Sci</italic></source> &#x000a0;<year>2010</year>;<volume>14</volume>:<fpage>383</fpage>&#x02013;<lpage>88</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2010.05.006</pub-id><pub-id pub-id-type="pmid">20579932</pub-id>
</mixed-citation></ref><ref id="R79"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wiese</surname> &#x000a0;<given-names>E</given-names></string-name>, <string-name><surname>Metta</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Wykowska</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>Robots as intentional agents: using neuroscientific methods to make robots appear more social</article-title>. <source><italic toggle="yes">Front Psychol</italic></source> &#x000a0;<year>2017</year>;<volume>8</volume>:<page-range>1663</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2017.01663</pub-id></mixed-citation></ref><ref id="R80"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wieser</surname> &#x000a0;<given-names>MJ</given-names></string-name>, <string-name><surname>Gerdes</surname> &#x000a0;<given-names>ABM</given-names></string-name>, <string-name><surname>B&#x000fc;ngel</surname> &#x000a0;<given-names>I</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Not so harmless anymore: how context impacts the perception and electrocortical processing of neutral faces</article-title>. <source><italic toggle="yes">NeuroImage</italic></source> &#x000a0;<year>2014</year>;<volume>92</volume>:<fpage>74</fpage>&#x02013;<lpage>82</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.01.022</pub-id><pub-id pub-id-type="pmid">24462933</pub-id>
</mixed-citation></ref><ref id="R81"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ziereis</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Schacht</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>Motivated attention and task relevance in the processing of cross-modally associated faces: behavioral and electrophysiological evidence</article-title>. <source><italic toggle="yes">Cogn Affect Behav Neurosci</italic></source> &#x000a0;<year>2023</year>;<volume>23</volume>:<fpage>1244</fpage>&#x02013;<lpage>66</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13415-023-01112-5</pub-id><pub-id pub-id-type="pmid">37353712</pub-id>
</mixed-citation></ref></ref-list></back></article>