<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Psychon Bull Rev</journal-id><journal-id journal-id-type="iso-abbrev">Psychon Bull Rev</journal-id><journal-title-group><journal-title>Psychonomic Bulletin &#x00026; Review</journal-title></journal-title-group><issn pub-type="ppub">1069-9384</issn><issn pub-type="epub">1531-5320</issn><publisher><publisher-name>Springer US</publisher-name><publisher-loc>New York</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39779657</article-id><article-id pub-id-type="pmc">PMC12092494</article-id>
<article-id pub-id-type="publisher-id">2630</article-id><article-id pub-id-type="doi">10.3758/s13423-024-02630-0</article-id><article-categories><subj-group subj-group-type="heading"><subject>Brief Report</subject></subj-group></article-categories><title-group><article-title>Cracking arbitrariness: A data-driven study of auditory iconicity in spoken English</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0597-9989</contrib-id><name><surname>de Varda</surname><given-names>Andrea Gregor</given-names></name><address><email>a.devarda@campus.unimib.it</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Marelli</surname><given-names>Marco</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01ynf4891</institution-id><institution-id institution-id-type="GRID">grid.7563.7</institution-id><institution-id institution-id-type="ISNI">0000 0001 2174 1754</institution-id><institution>Department of Psychology, </institution><institution>University of Milano &#x02013; Bicocca, </institution></institution-wrap>Piazza dell&#x02019;Ateneo Nuovo 1, Milan, MI 20126 Italy </aff></contrib-group><pub-date pub-type="epub"><day>8</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>8</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="ppub"><year>2025</year></pub-date><volume>32</volume><issue>3</issue><fpage>1425</fpage><lpage>1442</lpage><history><date date-type="accepted"><day>29</day><month>11</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Auditory iconic words display a phonological profile that imitates their referents&#x02019; sounds. Traditionally, those words are thought to constitute a minor portion of the auditory lexicon. In this article, we challenge this assumption by assessing the pervasiveness of onomatopoeia in the English auditory vocabulary through a novel data-driven procedure. We embed spoken words and natural sounds into a shared auditory space through (a) a short-time Fourier transform, (b) a convolutional neural network trained to classify sounds, and (c) a network trained on speech recognition. Then, we employ the obtained vector representations to measure their objective auditory resemblance. These similarity indexes show that imitation is not limited to some circumscribed semantic categories, but instead can be considered as a widespread mechanism underlying the structure of the English auditory vocabulary. We finally empirically validate our similarity indexes as measures of iconicity against human judgments.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Iconicity</kwd><kwd>Onomatopoeia</kwd><kwd>Phonosymbolism</kwd><kwd>Iconicity ratings</kwd><kwd>Deep learning</kwd><kwd>Computational modeling</kwd></kwd-group><funding-group><award-group><funding-source><institution>Universit&#x000e0; degli Studi di Milano - Bicocca</institution></funding-source></award-group><open-access><p>Open access funding provided by Universit&#x000e0; degli Studi di Milano - Bicocca within the CRUI-CARE Agreement.</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Psychonomic Society, Inc. 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Why are objects called the way they are? The traditional perspective on the structure of the lexicon advocates that words are arbitrary labels, and their links to the world are the result of cultural transmission (Bloomfield, <xref ref-type="bibr" rid="CR9">1994</xref>; Firth, <xref ref-type="bibr" rid="CR28">1964</xref>; Hockett, <xref ref-type="bibr" rid="CR41">1960</xref>; Levelt, Roelofs, &#x00026; Meyer, <xref ref-type="bibr" rid="CR55">1999</xref>; Saussure, <xref ref-type="bibr" rid="CR84">1964</xref>). In recent years, however, cognitive science is witnessing a substantial paradigm shift in the way the structure of the vocabulary is conceived. The assumption of an arbitrary relationship between phonology and semantics is being progressively reconsidered in favor of a more complex theoretical panorama, where the correspondence between these two domains is carefully examined and not simply rejected a priori (see for instance A. L. Thompson and Do, <xref ref-type="bibr" rid="CR96">2019</xref>).</p><p id="Par3">A form of non-arbitrariness in language is iconicity, i.e., a relationship between linguistic sounds and their referents that is defined not only by convention, but also by the sounds&#x02019; and the objects&#x02019; inherent qualities. For a word to be iconic, these qualities must be related by means of perceptuomotor analogies (Dingemanse, Blasi, Lupyan, Christiansen, &#x00026; Monaghan, <xref ref-type="bibr" rid="CR22">2015</xref>). Historically, early research has focused on vision-related iconicity (namely the association of certain linguistic sounds with some visual attributes of their referents such as their shape or size; see for instance K&#x000f6;hler, <xref ref-type="bibr" rid="CR47">1929</xref>, <xref ref-type="bibr" rid="CR48">1947</xref>; Maurer, Pathman, &#x00026; Mondloch, <xref ref-type="bibr" rid="CR63">2006</xref>; Sapir, <xref ref-type="bibr" rid="CR83">1929</xref>; Werner, <xref ref-type="bibr" rid="CR101">1948</xref>). However, more recently, several studies have investigated the relationship between word sounds and perceptual information from other sensory modalities (Fontana, <xref ref-type="bibr" rid="CR29">2013</xref>; Fryer, Freeman, &#x00026; Pring, <xref ref-type="bibr" rid="CR30">2014</xref>; Gallace, Boschin, &#x00026; Spence, <xref ref-type="bibr" rid="CR31">2011</xref>; Graven &#x00026; Desebrock, <xref ref-type="bibr" rid="CR32">2018</xref>; Joo, <xref ref-type="bibr" rid="CR45">2020</xref>; Speed, Atkinson, Wnuk, &#x00026; Majid, <xref ref-type="bibr" rid="CR90">2021</xref>).</p><p id="Par4">The fact that vision-related semantic information has drawn a lot of attention in iconicity research is to some extent motivated by the largely undisputed predominance of the visual modality in the human perceptual system (Lynott, Connell, Brysbaert, Brand, &#x00026; Carney, <xref ref-type="bibr" rid="CR59">2020a</xref>; Speed &#x00026; Brybaert, <xref ref-type="bibr" rid="CR91">2022</xref>; Vergallito, Petilli, &#x00026; Marelli, <xref ref-type="bibr" rid="CR100">2020</xref>). However, auditory iconicity displays some peculiar properties that make it a worthy testbed for the study of non-arbitrariness. In contrast to the other modalities, auditory iconicity takes place <italic>within</italic> a perceptual modality, associating verbal and non-verbal sounds. Indeed, the most unmistakable cases of iconicity in oral languages are onomatopoetic words, such as <italic>whisper</italic>, <italic>bubble</italic> or <italic>crack</italic>; these expressions are linked to their meaning by a direct imitative relationship and do not involve any cross-modal correspondence. It is thus not surprising to find that auditory iconic words (i.e., lexical items that resemble in their phonological profile the sound associated with their referents, such as <italic>crack</italic>; henceforth AIWs) hold a privileged role within the iconic lexicon. The most iconic words in English and Spanish often have a dominant auditory meaning (e.g., &#x0201c;trumpet&#x0201d;), as documented by explicit iconicity ratings (Perlman, Little, Thompson, &#x00026; Thompson, <xref ref-type="bibr" rid="CR75">2018</xref>; Winter, Perlman, Perry, &#x00026; Lupyan, <xref ref-type="bibr" rid="CR106">2017</xref>). Furthermore, when participants are asked to associate words with their meaning in languages unknown to them, they achieve the highest accuracy with words with high auditory strength (Dingemanse, Schuerman, Reinisch, Tufvesson, &#x00026; Mitterer, <xref ref-type="bibr" rid="CR23">2016</xref>). AIWs have also been shown to be more phonologically marked than other iconic words, exhibiting less sequential predictability than other iconic words in Cantonese (Thompson, Chan, Yeung, &#x00026; Do, <xref ref-type="bibr" rid="CR95">2022</xref>). Additionally, Edmiston, Perlman, and Lupyan (<xref ref-type="bibr" rid="CR27">2018</xref>) have shown that repeated imitation of environmental sounds can give rise to more word-like forms while retaining a resemblance to the original sounds, highlighting the role of human vocal imitation in the origins of spoken words.</p><p id="Par5">Another piece of evidence for the peculiar status of AIWs comes from linguistic typology, and more precisely from a striking consistency in the cross-lingual distribution of ideophones. Ideophones are a subset of the vocabulary composed of marked words depicting perceptual imagery (Dingemanse, <xref ref-type="bibr" rid="CR20">2012</xref>); they constitute sound-symbolic inventories typical of Sub-Saharan African, East Asian or Native American languages. Auditory iconic words constitute the most prominent class of perceptual terms in the ideophonic lexicon across languages. If a language develops ideophones, auditory ideophones will be part of the ideophonic lexicon &#x02013; in other words, if a language has visual ideophones it therefore must have sound ideophones as well (Dingemanse, <xref ref-type="bibr" rid="CR20">2012</xref>). This regularity has led different researchers to set auditory ideophones at the top of the cross-linguistic implicational hierarchy of the ideophonic lexicon (Dingemanse et&#x000a0;al., <xref ref-type="bibr" rid="CR22">2015</xref>; McLean, <xref ref-type="bibr" rid="CR64">2021</xref>).</p><sec id="Sec2"><title>Iconicity in cognition</title><p id="Par6">Historically, research on iconicity has often focused on circumscribed phenomena (see Magnus, <xref ref-type="bibr" rid="CR61">2013</xref> for an overview). In recent years, however, the topic has been integrated into broader theories of language evolution (Cabrera, <xref ref-type="bibr" rid="CR13">2012</xref>; Dingemanse, Torreira, &#x00026; Enfield, <xref ref-type="bibr" rid="CR25">2013</xref>; Perniss &#x00026; Vigliocco, <xref ref-type="bibr" rid="CR76">2014</xref>; Ramachandran &#x00026; Hubbard, <xref ref-type="bibr" rid="CR80">2001</xref>), acquisition (Asano et al., <xref ref-type="bibr" rid="CR4">2015</xref>; Imai, Kita, Nagumo, &#x00026; Okada, <xref ref-type="bibr" rid="CR43">2008</xref>; Murgiano, Motamedi, &#x00026;&#x000a0;Vigliocco, <xref ref-type="bibr" rid="CR68">2020</xref>; Perniss &#x00026; Vigliocco, <xref ref-type="bibr" rid="CR76">2014</xref>; see Laing <xref ref-type="bibr" rid="CR51">2014</xref>; Laing, <xref ref-type="bibr" rid="CR52">2019</xref> for the role of auditory iconicity in language learning), and processing (Cnudde, Sidhu, &#x00026; Pexman, <xref ref-type="bibr" rid="CR14">2020</xref>; Sidhu, Vigliocco, &#x00026; Pexman, <xref ref-type="bibr" rid="CR88">2020</xref>; Peeters, <xref ref-type="bibr" rid="CR74">2016</xref>; Perniss &#x00026; Vigliocco, <xref ref-type="bibr" rid="CR76">2014</xref>; Van&#x000a0;Hoey, Thompson, Do, &#x00026; Dingemanse, <xref ref-type="bibr" rid="CR99">2023</xref>). Given the increasing recognition of the role played by iconicity in cognition, it is crucial to devise an appropriate measurement for the construct being studied. The studies hereby discussed operationalized iconicity through subjective ratings; however, iconicity ratings have been criticized for having low construct validity. The ability of language users to judge the fit of a sound with respect to its referent has been questioned, as it has been shown that participants have a positive bias when judging whether sounds fit their referents in their native language (Sutherland &#x00026; Cimpian, <xref ref-type="bibr" rid="CR92">2015</xref>). Furthermore, Thompson, Akita, and Do (<xref ref-type="bibr" rid="CR94">2020</xref>) have proposed that, for words that are not obviously iconic, participants might base their responses according to sensory information alone as opposed to form-referent resemblance. The inability of participants to judge the iconicity of non-ideophonic words would call for a different, and possibly objective measure of iconicity. Winter and Perlman (<xref ref-type="bibr" rid="CR105">2021</xref>) provided a response to those criticisms, noting that iconicity ratings have served an important purpose in explaining the distribution of iconic properties in the lexicon; however, they acknowledge that iconicity ratings should be complemented by other tools and methodologies in order to grant a fuller picture on the role of iconicity in language (see also McLean, Dunn, &#x00026; Dingemanse, <xref ref-type="bibr" rid="CR65">2023</xref>).</p><p id="Par7">More generally, the discussion on iconicity ratings can be framed within a broader debate on the role of introspection in psychology and cognitive science. While in the psychological literature it is common practice to employ explicit ratings as independent variables to predict behavioral data (see for instance Sidhu et al., <xref ref-type="bibr" rid="CR88">2020</xref> in the specific case of iconicity ratings), human ratings are themselves an interesting cognitive measure grounded in introspection, that needs to be explained, ideally starting from the objective and measurable properties of the stimuli (see for instance G&#x000fc;nther, Petilli, Vergallito, &#x00026; Marelli, <xref ref-type="bibr" rid="CR36">2020</xref>; G&#x000fc;nther, Marelli, Tureski, &#x00026; Petilli, <xref ref-type="bibr" rid="CR35">2021</xref>; Westbury, <xref ref-type="bibr" rid="CR102">2016</xref>).</p></sec><sec id="Sec3"><title>Data-driven measurements in cognitive science</title><p id="Par8">Computationally specified alternatives to human ratings do exist, and they have been proven successful in predicting human performance across a variety of linguistic tasks (see G&#x000fc;nther, Rinaldi, and Marelli <xref ref-type="bibr" rid="CR37">2019</xref> for a review). Needless to say, text-based data-driven models and measurements have a long-standing tradition in experimental psychology, computational linguistics and natural language processing (see for instance Landauer &#x00026; Dumais, <xref ref-type="bibr" rid="CR53">1997</xref>; Lund &#x00026; Burgess, <xref ref-type="bibr" rid="CR57">1996</xref>; Mikolov, Chen, Corrado, &#x00026; Dean, <xref ref-type="bibr" rid="CR66">2013</xref>), where they have already been employed to study non-arbitrariness in language (Abramova &#x00026; Fern&#x000e1;ndez, <xref ref-type="bibr" rid="CR1">2016</xref>; Abramova, Fern&#x000e1;ndez, &#x00026; Sangati, <xref ref-type="bibr" rid="CR2">2013</xref>; Dautriche, Mahowald, Gibson, &#x00026; Piantadosi, <xref ref-type="bibr" rid="CR16">2017</xref>; de Varda &#x00026; Strapparava, <xref ref-type="bibr" rid="CR18">2021</xref>; Guti&#x000e9;rrez, Levy, &#x00026; Bergen, <xref ref-type="bibr" rid="CR38">2016</xref>; Shillcock, Kirby, McDonald, &#x00026; Brew, <xref ref-type="bibr" rid="CR86">2001</xref>; Tamariz, <xref ref-type="bibr" rid="CR93">2008</xref>). However, language-centered semantic models are not sufficient to capture the phenomenon under scrutiny. The kind of iconic relationship that we study, named in the semiotic literature as &#x0201c;imaginal iconicity&#x0201d; (N&#x000f6;th, <xref ref-type="bibr" rid="CR71">1999</xref>), implies direct similarity between linguistic signs and referred objects. Language-based semantic models, being developed based on linguistic data alone, do not have direct access to the physical properties of the referents. Hence, one of the two semiotic components of the iconic link is not accessible to the model in the first place. For this reason, the objectives of our study required the employment of perceptually grounded models, which have direct access to the physical properties of the referents.</p><p id="Par9">The employment of data-driven models in the study of perceptually-grounded meanings has gained increasing popularity in the last years, with a prominent example being set by the adoption of computer-vision deep-learning architectures in experimental psychology (G&#x000fc;nther et&#x000a0;al., <xref ref-type="bibr" rid="CR36">2020</xref>, <xref ref-type="bibr" rid="CR35">2021</xref>; Petilli, G&#x000fc;nther, Vergallito, Ciapparelli, &#x00026; Marelli, <xref ref-type="bibr" rid="CR79">2021</xref>) and neuroscience (Seeliger et&#x000a0;al., <xref ref-type="bibr" rid="CR85">2018</xref>; Yamins &#x00026; DiCarlo, <xref ref-type="bibr" rid="CR107">2016</xref>). These models have also been employed in iconicity research: de Varda and Strapparava (<xref ref-type="bibr" rid="CR19">2022</xref>) have employed an image-processing neural network to show that word sounds bear a cross-modal resemblance to the visual representations of their referents, with this resemblance being consistent across languages. However, their study was focused on phonovisual iconicity, and thus it implied a cross-modal language-to-perception mapping. This methodology, while overcoming the limitations of iconicity ratings, presents a different shortcoming. Similarly to other data-driven approaches to phonosymbolism (see for instance Blasi, Wichmann, Hammarstr&#x000f6;m, Stadler, &#x00026; Christiansen, <xref ref-type="bibr" rid="CR8">2016</xref>), de Varda and Strapparava &#x02019;s (<xref ref-type="bibr" rid="CR19">2022</xref>) study embraces a <italic>functional</italic> definition of iconicity, characterized as a feature of a signal that allows its meaning to be predicted from its form (Motamedi, Little, Nielsen, &#x00026; Sulik, <xref ref-type="bibr" rid="CR67">2019</xref>). Functional approaches which do not employ direct resemblance as a criterion might be problematic, as they may conflate iconicity and systematicity, a related but distinct phenomenon (Dingemanse et&#x000a0;al., <xref ref-type="bibr" rid="CR22">2015</xref>). Shifting the focus to auditory iconicity grants the possibility of directly projecting linguistic and perceptual representations onto a shared sound space, without the need of any post hoc transformation. Ultimately, this choice allows us to operationalize iconicity as direct sound resemblance.<fig id="Fig1"><label>Fig. 1</label><caption><p>Graphical summary of the experimental pipeline. We embed word sounds (<italic>top</italic>) and natural sounds (<italic>bottom</italic>) into a shared vector format; then, we compute the cosine of the angle subtended by the two vector, and employ this index as a measure of iconicity</p></caption><graphic xlink:href="13423_2024_2630_Fig1_HTML" id="MO1"/></fig></p><p id="Par10">In this article, we follow this promising research line by employing different computational models developed in the field of sound engineering as tools for obtaining data-driven, theory-agnostic iconicity measurements. We then use these derived metrics to assess the pervasiveness of iconicity in the English auditory lexicon, aiming to overcome the limitations of iconicity ratings and other data-driven approaches. Then, we empirically validate our measures in explaining human intuitions against a strong baseline of psycholinguistic variables associated with the construct under scrutiny. Finally, we assess the extent to which our results depend on words that are identified as onomatopoetic by English native speakers.</p></sec></sec><sec id="Sec4"><title>Methods</title><p id="Par11">In this article, we operationalize auditory iconicity as the objective similarity between (i) the sound of a word and (ii) the natural sounds associated to its referent. We consider a word as auditorily iconic if its phonetic profile &#x02013; for instance, the phonetic sequence 
<inline-graphic xlink:href="13423_2024_2630_Figa_HTML.gif" id="d33e467"/>
, &#x0201c;frog&#x0201d; &#x02013; resembles the natural sounds associated with its referent &#x02013; for instance, the croaking of a frog. Both word and natural sounds are transposed in a vector format in one of three ways: <list list-type="order"><list-item><p id="Par12">Short-time Fourier transform (STFT), a procedure that decomposes sound signals into individual frequencies and their amplitudes.</p></list-item><list-item><p id="Par13">Sound classification network, a neural-network model trained to label sounds.</p></list-item><list-item><p id="Par14">Speech recognition network, a neural-network model trained to recognize spoken words.</p></list-item></list>For all these three methods, the obtained sound representations end up embedding natural sounds and word sounds into a shared vector space. This allows to estimate the similarity between the vector representation of the sound of a word (<inline-formula id="IEq1"><alternatives><tex-math id="d33e486">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e491"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula>) and its corresponding natural sound (<inline-formula id="IEq2"><alternatives><tex-math id="d33e500">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e505"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula>), which we employ as a measure of iconicity (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). Note that the three representational alternatives are intended as replications, as the same analyses were carried out independently with vector representations obtained with each of them.</p><p id="Par15">The first option that we consider builds on the spectrotemporal features of vocal and natural sounds. A sound signal is composed of several single-frequency sound waves, which can be decomposed through the application of a STFT (Allen, <xref ref-type="bibr" rid="CR3">1977</xref>). The Fourier transform converts the signal from the time domain to the frequency domain, comparing the sound signal with sinusoids of various frequencies. The STFT performs this operation on various sequential windows of the audio segment, and the result is called a <italic>spectrum</italic>, which can be visualized through a heatmap as a spectrogram. This is the simplest sound representations we consider in our study, and also constitutes a pre-processing stage of the two neural-network models. In this study, we computed sound spectra utilizing the default parameters that were used in the pre-processing stages of SpeechVGG. In particular, we converted the audio files to mono and resampled them to a sample rate of 16,200 Hz. If the length of the resampled audio was shorter than the target sample rate, the file was padded. Otherwise, the central segment of the audio with a length equal to the target sample rate was extracted. We then computed a STFT on the audio segment using a Hamming window with 256 samples per segment and 128 samples overlap. The resulting matrix was then flattened into a one-dimensional array, which was returned as the final feature vector.<fig id="Fig2"><label>Fig. 2</label><caption><p>Graphical summary of the operations carried out by the convolutional and the pooling layer of the HCNNs. During convolution, the kernel (<italic>in red</italic>) slides across the input spectrum (<italic>in blue</italic>). At each position, the kernel is multiplied element-wise with the portion of the spectrum it is currently covering, and the resulting product is summed to produce a single output value for that position in the output feature map (<italic>in green</italic>). Pooling (<italic>in orange</italic>) simply involves taking the average or the maximum value out of a set of adjacent units in a feature map</p></caption><graphic xlink:href="13423_2024_2630_Fig2_HTML" id="MO2"/></fig></p><p id="Par16">While the human auditory system excels in processing spectrotemporal sound features, sound and voice perception show some degree of invariance to contextually irrelevant changes in the acoustic properties of the stimuli (Ley, Vroomen, &#x00026; Formisano, <xref ref-type="bibr" rid="CR56">2014</xref>). Unless a word is uttered twice in the same way and in an identical environment, the raw sound features corresponding to its pronunciation will be different every time it is produced. Similarly, there is some degree of variation in the raw spectrotemporal features associated with natural sounds. For instance, small and large dogs usually produce high- and low-pitch barks, respectively (Edmiston &#x00026; Lupyan, <xref ref-type="bibr" rid="CR26">2015</xref>). Nonetheless, humans are able to correctly recognize both natural and vocal sounds heard in different auditory contexts, showing the ability to map acoustically different sounds onto similar perceptual representations (Ley et al., <xref ref-type="bibr" rid="CR56">2014</xref>). One way to model some degree of invariance with respect to irrelevant low-level features is to employ deep learning architectures and transform the input signal into meaningful representations. Furthermore, audio-processing neural networks have been shown to map onto brain responses to sounds, speaking in favor of their validity as models of human auditory processing (Kell, Yamins, Shook, Norman-Haignere, &#x00026; McDermott, <xref ref-type="bibr" rid="CR46">2018</xref>; see below for a more in-depth discussion). Hence, we complement our spectral baseline with audio representations obtained through two pre-trained hierarchical convolutional neural networks (HCNN) proposed in the field of audio engineering and computer audition.</p><p id="Par17">HCNNs were originally developed for computer vision applications, where they excel at several tasks such as image classification (Krizhevsky, Sutskever, &#x00026; Hinton, <xref ref-type="bibr" rid="CR49">2012</xref>); recently, they have been adapted to process sound data in order to classify environmental sounds (Hershey et&#x000a0;al., <xref ref-type="bibr" rid="CR39">2016</xref>) and human speech (Beckmann, Kegler, Saltini, &#x00026; Cernak, <xref ref-type="bibr" rid="CR7">2019</xref>). HCNNs exploit the hierarchical nature of the visual and auditory data to assemble representations of increasing abstraction using small and simple kernels (or filters) repeated across the input. These models are generally structured as a sequence of convolutional blocks, followed by standard fully connected layers (see for instance Simonyan &#x00026; Zisserman, <xref ref-type="bibr" rid="CR89">2015</xref>). Convolutional blocks are in turn composed of convolutional and pooling layers. Convolutional layers are the core components of the network; they apply relatively small learned kernels to input images and sound <italic>spectra</italic> by sliding them across the input&#x02019;s height and width; the dot product between every element of the filter and the input is then calculated at every spatial position, and the output of these operations is called a <italic>feature map</italic> (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>a). Pooling layers lower the resolution of the obtained feature maps, taking for instance the average or the maximum activation value from a set of adjacent units, thus creating an invariance to small distortions and shifts (LeCun, Bengio, &#x00026; Hinton, <xref ref-type="bibr" rid="CR54">2015</xref>; see Fig. <xref rid="Fig2" ref-type="fig">2</xref>b).</p><p id="Par18">When passing through the sequence of layers, the information in input becomes progressively abstracted away from its superficial features. In auditory HCNNs, the first layers respond to shallow acoustic properties such as loudness and scale, and deeper layers show increased sensitivity to high-level features such as roughness and event categories (Huang, Slaney, &#x00026; Elhilali, <xref ref-type="bibr" rid="CR42">2018</xref>). We employed the output of the convolutional network as an approximation of a representational format proper of the human perceptual system. Sound recognition tasks such as the ones humans perform in everyday life impose a strong functional pressure on the auditory system, and it has been shown that a model trained to perform the same tasks converges to cognitively accurate representational transformations (Kell et&#x000a0;al., <xref ref-type="bibr" rid="CR46">2018</xref>). Indeed, auditory HCNN-based representations can be successfully mapped onto neural responses to auditory stimuli at different processing stages of the auditory cortex, where intermediate model layers provide the best fit for primary auditory cortex, while deeper layers best predict non-primary responses (Kell et&#x000a0;al., <xref ref-type="bibr" rid="CR46">2018</xref>). From a behavioral perspective, auditory HCNNs are able to classify sound data with human-level accuracy, and their error patterns resemble those of humans despite not being optimized to do so (Kell et&#x000a0;al., <xref ref-type="bibr" rid="CR46">2018</xref>). HCNN-based representations have been proposed to be cognitively plausible at least at the computational level of description (Marr, <xref ref-type="bibr" rid="CR62">1982</xref>), being able to predict human behavior and performance in several tasks (although these results were primarily based on image-processing HCNNs, see G&#x000fc;nther et&#x000a0;al. <xref ref-type="bibr" rid="CR36">2020</xref>; G&#x000fc;nther et al. <xref ref-type="bibr" rid="CR35">2021</xref>; Petilli et al. <xref ref-type="bibr" rid="CR79">2021</xref>).<fig id="Fig3"><label>Fig. 3</label><caption><p>Graphical depiction of the SpeechVGG architecture. The image in input represents a spectrogram; the input is passed through a sequence of convolutional (<italic>light blue</italic>), ReLU (<italic>blue</italic>), pooling (<italic>purple</italic>) and fully connected layers (<italic>yellow</italic>) in order to obtain the output representations. Figure drawn with the PlotNeuralNet package (Iqbal, <xref ref-type="bibr" rid="CR44">2020</xref>)</p></caption><graphic xlink:href="13423_2024_2630_Fig3_HTML" id="MO3"/></fig></p><p id="Par19">No auditory HCNN, to our knowledge, has been simultaneously trained to recognize spoken words and natural sounds. However, different architectures have been developed separately to carry out one of the two tasks. Interestingly, the VGG16 model, originally developed for large-scale image recognition (Simonyan &#x00026; Zisserman, <xref ref-type="bibr" rid="CR89">2015</xref>), has been adapted by different research groups to recognize words on the basis of their acoustic properties (SpeechVGG; Beckmann et al., <xref ref-type="bibr" rid="CR7">2019</xref>), and sounds more generally (VGGish; Hershey et al., <xref ref-type="bibr" rid="CR39">2016</xref>). Hence, we chose to employ both architectures in our experimental procedure. SpeechVGG and VGGish are trained to classify sounds, encoded as spectral features, according to their gold-standard labels. In the case of SpeechVGG, these labels are composed by words &#x02013; for instance, a training instance might be formed by the sound spectrum corresponding to someone pronouncing the word &#x0201c;dog&#x0201d;, and the target category label corresponding to the word. In the case of VGGish, the labels correspond to sound event categories; a training instance might thus be composed of the roar of an engine, that the network should classify as the sound of a car. Crucially, after the training phase, these models are able to correctly classify sounds that were not observed during training, and produce quantitatively defined and semantically meaningful representations for a theoretically infinite range of sounds.</p><p id="Par20">The two models are very similar with respect to their structural configuration, the format of the input they are fed with, and the error function they are trained to minimize. They are composed of a stack of convolutional (with small 3 <inline-formula id="IEq3"><alternatives><tex-math id="d33e656">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document}</tex-math><mml:math id="d33e661"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq3.gif"/></alternatives></inline-formula> 3 filter size) and max-pooling (with 2 <inline-formula id="IEq4"><alternatives><tex-math id="d33e665">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document}</tex-math><mml:math id="d33e670"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq3.gif"/></alternatives></inline-formula> 2 window size) layers equipped with ReLU non-linearities; they receive as input log-mel spectrograms and are trained with cross-entropy loss. There are, however, some differences between the two models: while VGGish drops the fifth convolutional block, SpeechVGG maintains the five-block structure of VGG-16. The two fully connected layers at the end of the processing stream have 4,096 and 120 units in VGGish, and a task-dependent shape<xref ref-type="fn" rid="Fn1">1</xref> in SpeechVGG. Regardless of their architectural differences, which are not of interest for the purpose of this study, both networks produce fixed-length vector representations in response to the sound <italic>spectra</italic> they are fed with. A schematic representation of the SpeechVGG architecture is reported in Fig. <xref rid="Fig3" ref-type="fig">3</xref>; for a detailed description, we redirect the reader to the original articles (Beckmann et al., <xref ref-type="bibr" rid="CR7">2019</xref>; Hershey et al., <xref ref-type="bibr" rid="CR39">2016</xref>; see Simonyan and Zisserman <xref ref-type="bibr" rid="CR89">2015</xref> for the details of the original image-processing VGG16).</p><sec id="Sec5"><title>Data</title><p id="Par22">Spoken word data consisted of high-quality recordings released by Tucker et&#x000a0;al. (<xref ref-type="bibr" rid="CR98">2019</xref>) via the Massive Auditory Lexical Decision database (MALD), where they were included as stimuli for a megastudy in auditory lexical decision. Words in the MALD database were selected from different sources, including conversational and written corpora, to ensure generalization across the spoken English lexicon; the recording was performed with professional studio equipment in a sound-isolated room.</p><p id="Par23">For a word to be auditorily iconic, its meaning must be grounded in the auditory modality, as it is necessary for the corresponding concept to be associated with non-linguistic auditory information. A word like &#x0201c;clap&#x0201d; is auditorily iconic since its phonetic realization resembles the sound accompanying a strike of the palms; however, the acoustic information associated with this concept could not be imitated by the phonetics of the corresponding word if that acoustic information was not present in the first place. For this reason, we pre-selected from the dataset the words that exhibited a high auditory perceptual strength (employing the sensory modality ratings released by Lynott, Connell, Brysbaert, Brand, and Carney <xref ref-type="bibr" rid="CR60">2020b</xref>). More precisely, we only included items with auditory strength higher than 3.56 in a five-point scale (1.5 SD above the mean; <italic>N</italic> = 374; 10.34% of the words in the MALD database for which the auditory ratings were available). We then scraped Freesound,<xref ref-type="fn" rid="Fn2">2</xref> a collaborative repository of audio samples where sounds are uploaded to the website by its users, and cover a wide range of subjects, from field recordings and background noises to synthesized sounds. In our scraping procedure, we searched for at least ten sounds tagged with each of the words that we previously selected. In our search, we enforced five constraints: (a) a maximum length of 15 seconds; (b) a maximum of 10 tags per file; (c) a minimum quality rating of 4 out of 5; and (d) a minimum of 500 downloads. These criteria were adopted to automatically select sound events that were sufficiently specific with respect to the tag being considered (a, b) and met a reasonable quality standard, assessed with a sufficient sample size (c, d).</p><p id="Par25">Some of the recordings scraped from Freesound were found to be problematic in relation to our experimental approach.<xref ref-type="fn" rid="Fn3">3</xref> For instance, some audio files for a target word (e.g., &#x0201c;no&#x0201d;) consisted of a recording of a person uttering that word. As in the present paper, we operationalize auditory iconicity as the resemblance between word and natural sounds, if the natural sound is itself a word sound, the iconicity estimate will be artificially inflated. Furthermore, despite our efforts to ensure a high-quality standard for our audio sample, some natural sounds were found not to contain the sound of the referent (for instance, a sound labeled as &#x0201c;bird&#x0201d; containing a synthetic, high-pitch sound). Thus, two annotators (a research assistant and an intern) were asked to listen to all the sounds in our dataset (<italic>N</italic> = 7401) and remove the items that (a) contained a recording of a person uttering the target word, or (b) did not contain the actual sound of the word&#x02019;s referent. In total, 1100 out of 7401 sounds were eliminated from the final version of the dataset (14.86%). Out of these 1100 recordings, 68 were eliminated for reason (a), and 1032 were eliminated for reason (b) (0.91% and 13.94% of the total, respectively). Following an additional check of the first author, 12 other sounds were eliminated. Of the 374 word sounds considered at the beginning of the study, 7 did not contain any sound of their referent after the cleaning procedure was performed, and were thus excluded from the following analyses. The final dataset comprised 367 word sounds and 6,289 natural sounds.</p><p id="Par27">The sound vectors associated with these stimuli were derived in the same way for both linguistic (from MALD) and non-linguistic samples (from Freesound). In the pre-processing stages, we averaged the two channels of stereo audio files and resampled them at 16K Hz (the frequency used during pre-training). Then, we computed the sound spectra using the specific parameters that were employed during the pre-training of each network. We fed each sound item <italic>x</italic> in our stimulus set (both word sounds and natural sounds) to the two VGG16-like models, in order to extract the resulting feature map <inline-formula id="IEq5"><alternatives><tex-math id="d33e734">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varphi (x)$$\end{document}</tex-math><mml:math id="d33e739"><mml:mrow><mml:mi>&#x003c6;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq5.gif"/></alternatives></inline-formula>. Note that the pre-trained models were set in evaluation mode, and their parameters were not updated, as no further training was required. While VGGish has a compact 128-dimensional fully connected layer, the fully connected layers of SpeechVGG are task-dependent, and thus are not included in the general-purpose model release, which only returns the output up to the last max-pooling layer. Hence, in the case of VGGish we directly employed <inline-formula id="IEq6"><alternatives><tex-math id="d33e749">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varphi (x)$$\end{document}</tex-math><mml:math id="d33e754"><mml:mrow><mml:mi>&#x003c6;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq5.gif"/></alternatives></inline-formula> as compact sound representation, whereas in the case of SpeechVGG we extracted the output of block5_pool, flattened it, and reduced it to a 128-dimensional vector applying a PCA transformation.</p><p id="Par28">The outputs of the two models could not be directly compared: the different weights learned during pre-training and the different number of layers cause the sounds in input to be projected onto different sound spaces. Similarly, the sound spectra could not be directly compared with the HCNN-based sound vectors, as the arrays have different shapes and reflect different stages of processing. Hence, we performed all our experiments three times, focusing separately on (i) the spectrotemporal features derived through the STFT, (ii) the distributed representations induced via VGGish, and (iii) the distributed representations induced via SpeechVGG. With considering both a sound- and a speech-processing network we aim at probing the potential advantages of either method: in principle, we would expect the former model (VGGish) to be more sensitive to meaningful properties of natural sounds, and the latter (SpeechVGG) to be optimally receptive to phonetic distinctions. Additionally, the spectral baseline can be informative in terms of the degree of abstraction from low-level features that is necessary to detect iconicity in language.</p></sec><sec id="Sec6"><title>Analyses</title><p id="Par29">Once the sound representations were obtained, we computed the similarity between word-sound vectors (<inline-formula id="IEq7"><alternatives><tex-math id="d33e773">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e778"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula>, e.g., the pronunciation of the word &#x0201c;frog&#x0201d;, 
<inline-graphic xlink:href="13423_2024_2630_Figb_HTML.gif" id="d33e788"/>
) and the corresponding natural-sounds vectors (<inline-formula id="IEq8"><alternatives><tex-math id="d33e790">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e795"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula>, e.g., the croaking of a frog). It is standard practice to employ as a similarity score in high dimensional spaces the cosine similarity between two vectors. The cosine of the angle <inline-formula id="IEq9"><alternatives><tex-math id="d33e804">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\uptheta }$$\end{document}</tex-math><mml:math id="d33e809"><mml:mi mathvariant="normal">&#x003b8;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq9.gif"/></alternatives></inline-formula> subtended by two vectors <inline-formula id="IEq10"><alternatives><tex-math id="d33e814">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e819"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq11"><alternatives><tex-math id="d33e829">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e834"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula> is computed as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e843">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} cos(\uptheta ) = \frac{\overrightarrow{s_w} \cdot \overrightarrow{s_n}}{||\overrightarrow{s_w}|| \cdot ||\overrightarrow{s_n}||} = \frac{\sum _{i=1}^{N}{s_{wi}}{s_{ni}}}{\sqrt{\sum _{i=1}^{N}s_{wi}^{2}}\sqrt{\sum _{i=1}^{N}s_{ni}^{2}}} \end{aligned}$$\end{document}</tex-math><mml:math id="d33e849" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">&#x003b8;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mo>&#x000b7;</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="italic">wi</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="italic">ni</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="italic">wi</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt><mml:msqrt><mml:mrow><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="italic">ni</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13423_2024_2630_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula> Where <inline-formula id="IEq12"><alternatives><tex-math id="d33e968">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_{wi}}$$\end{document}</tex-math><mml:math id="d33e973"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="italic">wi</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq12.gif"/></alternatives></inline-formula> and <inline-formula id="IEq13"><alternatives><tex-math id="d33e984">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_{ni}}$$\end{document}</tex-math><mml:math id="d33e989"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="italic">ni</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq13.gif"/></alternatives></inline-formula> are components of the vectors <inline-formula id="IEq14"><alternatives><tex-math id="d33e1000">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e1005"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq15"><alternatives><tex-math id="d33e1014">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e1019"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula>, respectively, and <italic>N</italic> corresponds to the dimensionality of the vectors (i.e., 128 in our case). The obtained estimates were tested in two analyses.</p><p id="Par30">First, we employed this similarity metric to assess whether auditory words were more similar to the sounds associated with their referent than to the other sounds in the dataset. For each <inline-formula id="IEq16"><alternatives><tex-math id="d33e1034">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e1039"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula>, we computed the cosine similarity with all the <inline-formula id="IEq17"><alternatives><tex-math id="d33e1048">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e1053"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula>, including both sounds that were associated to the given word and sounds that were not. We then evaluated whether <inline-formula id="IEq18"><alternatives><tex-math id="d33e1062">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e1067"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula> were more similar to the associated <inline-formula id="IEq19"><alternatives><tex-math id="d33e1076">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e1081"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula> than to other unrelated <inline-formula id="IEq20"><alternatives><tex-math id="d33e1090">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e1095"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula> by dummy-coding whether they matched (0-1), and employing this value as a regressor, with the cosine similarity as the dependent variable. Due to the hierarchical structure of our data (i.e., multiple natural sounds nested within word sounds), we employed linear mixed-effects models, with random slopes and intercepts for both <inline-formula id="IEq21"><alternatives><tex-math id="d33e1105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e1110"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq22"><alternatives><tex-math id="d33e1119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e1124"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula> (in line with the suggestion of fitting maximal models when possible, see Barr, Levy, Scheepers, and Tily <xref ref-type="bibr" rid="CR6">2013</xref>). We then dropped the random components of the regression when their variance was equal to zero, to avoid singular fit (see for instance Pasch, Bolker, &#x00026; Phelps, <xref ref-type="bibr" rid="CR73">2013</xref>).</p><p id="Par31">Second, we evaluated the degree of association between our data-driven measures of auditory iconicity and explicit ratings of iconicity obtained by English native speakers, as released via two large norming studies (Winter et&#x000a0;al., <xref ref-type="bibr" rid="CR106">2017</xref>, <xref ref-type="bibr" rid="CR104">2023</xref>). We also tested whether our measures were significant predictors of iconicity ratings against a strong baseline of psycholinguistic variables that have been shown to be associated with such a construct. Indeed, it has been shown that iconicity ratings are negatively associated with word length (Perry, Perlman, &#x00026; Lupyan, <xref ref-type="bibr" rid="CR77">2015</xref>; Winter et al., <xref ref-type="bibr" rid="CR106">2017</xref>; Winter et al., <xref ref-type="bibr" rid="CR104">2023</xref>), log frequency (Perry et&#x000a0;al., <xref ref-type="bibr" rid="CR77">2015</xref>; Perry, Perlman, Winter, Massaro, &#x00026; Lupyan, <xref ref-type="bibr" rid="CR78">2018</xref>; Winter et&#x000a0;al., <xref ref-type="bibr" rid="CR106">2017</xref>, <xref ref-type="bibr" rid="CR104">2023</xref>), age of acquisition (Perry et al., <xref ref-type="bibr" rid="CR78">2018</xref>; Winter et al., <xref ref-type="bibr" rid="CR104">2023</xref>), concreteness (Hinojosa, Haro, Magallares, Du&#x000f1;abeitia, &#x00026; Ferr&#x000e9;, <xref ref-type="bibr" rid="CR40">2021</xref>; Winter et al., <xref ref-type="bibr" rid="CR104">2023</xref>), and contextual diversity (Lupyan &#x00026; Winter, <xref ref-type="bibr" rid="CR58">2018</xref>); furthermore, they are positively correlated with perceptual experience (Sidhu &#x00026; Pexman, <xref ref-type="bibr" rid="CR87">2018</xref>; Winter et&#x000a0;al., <xref ref-type="bibr" rid="CR106">2017</xref>, <xref ref-type="bibr" rid="CR104">2023</xref>) and phonological markedness (Dingemanse &#x00026; Thompson, <xref ref-type="bibr" rid="CR24">2020</xref>). In our analyses, we included these measures as covariates to test the predictive power of our metrics while controlling for other possible confounds. Data were obtained from available norms of perceptual strength (Lynott et&#x000a0;al., <xref ref-type="bibr" rid="CR60">2020b</xref>), SUBTLEX-US log frequency and contextual diversity (Brysbaert &#x00026; New, <xref ref-type="bibr" rid="CR11">2009</xref>), concreteness (Brysbaert, Warriner, &#x00026; Kuperman, <xref ref-type="bibr" rid="CR12">2014</xref>), and age of acquisition (Kuperman, Stadthagen-Gonzalez, &#x00026; Brysbaert, <xref ref-type="bibr" rid="CR50">2012</xref>). Phonological markedness was measured with the same word-sound vectors employed to derive our data-driven measurement of iconicity; it was operationalized as the cosine distance of each <inline-formula id="IEq23"><alternatives><tex-math id="d33e1210">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e1215"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula> from the centroid of the word sound space. This choice ensured that the predictive power associated with our iconicity metric was not confounded with phonological markedness.<xref ref-type="fn" rid="Fn4">4</xref></p></sec><sec id="Sec7"><title>Follow-up analysis: removal of onomatopoetic words</title><p id="Par33">As a follow-up analysis, we removed from our dataset all the words that were identified as onomatopoetic forms by two English native speakers, and repeated our analyses on this subset. This follow-up study was aimed at testing whether our results held after excluding the words that were explicitly perceived as imitative.<xref ref-type="fn" rid="Fn5">5</xref> If imitative patterns could be detected with our computational approach even after the exclusion of words judged as onomatopoetic, this would suggest that subtle iconic patterns in the lexicon can elude human explicit intuitions while still contributing to the phonological structure of the auditory vocabulary. Two native English speakers (graduate students with training in psychology, one American, one British) were presented with the original list of 374 words and asked to indicate whether each word was an instance of onomatopoeia or not. They were instructed to indicate as onomatopoetic words those stimuli that satisfied either of these two conditions: <list list-type="order"><list-item><p id="Par35">The word phonetically imitates or resembles the sound it describes. According to this first definition, onomatopoeic words sound like the noises or actions they represent, like &#x0201c;buzz&#x0201d; for the sound a bee makes or &#x0201c;splash&#x0201d; for the sound of something falling in the water.</p></list-item><list-item><p id="Par36">The sound of the word resembles a sound associated with the object or action that the word denotes. One example of this is the word &#x0201c;cuckoo&#x0201d;. <italic>Cuckoo</italic> is the bird&#x02019;s name, but its acoustic resemblance is to the song that it produces, not the bird itself.</p></list-item></list>The two annotators were further asked to indicate the cases where they were unsure about the answer. To be maximally conservative in our follow-up study, we repeated our analyses after removing all the words that at least one participant indicated as onomatopoetic or was unsure.</p><p id="Par37">Our experimental pipeline is summarized in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. All code is publicly available.<xref ref-type="fn" rid="Fn6">6</xref> The code for extracting the embeddings was adapted from the one available on TensorFlow Hub for VGGish<xref ref-type="fn" rid="Fn7">7</xref> and from GitHub for SpeechVGG.<xref ref-type="fn" rid="Fn8">8</xref> Due to copyright constraints, we release aggregated sound vectors for the natural sounds, since the various recordings are protected by different restrictions.</p></sec></sec><sec id="Sec8"><title>Results</title><sec id="Sec9"><title>Iconicity in the auditory lexicon</title><p id="Par41">We were not able to find an effect of conditions (i.e., a higher similarity between matching versus non-matching <inline-formula id="IEq24"><alternatives><tex-math id="d33e1280">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e1285"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq25"><alternatives><tex-math id="d33e1294">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e1299"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula>) employing the raw spectral representations: the average cosine similarity of matching and non-matching <inline-formula id="IEq26"><alternatives><tex-math id="d33e1308">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e1313"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq27"><alternatives><tex-math id="d33e1322">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e1327"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula> were virtually indistinguishable (-0.0497, SD = 0.4532 and -0.0440, SD = 0.4438, respectively; B = 0.0068, SE = 0.0048, <italic>t</italic> = 1.4110, <italic>p</italic> = 0.158, <italic>N</italic> = 2,308,063). However, we detected a significant effect of condition employing the more sophisticated sound processing networks. In the case of VGGish, <inline-formula id="IEq28"><alternatives><tex-math id="d33e1346">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e1351"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq29"><alternatives><tex-math id="d33e1360">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e1365"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula> had a similarity of 0.4180, SD = 0.1498 when they matched, and 0.4111, SD = 0.1516 when they corresponded to different items. The representations obtained from the other network reflected the same distinction: the average similarity between word and natural sounds was -0.0452, SD = 0.1606 when the word was the label of the given sound, and -0.0619, SD = 0.1527 otherwise. The statistical significance of this distinction was confirmed by the results of our mixed-effects models. Indeed, we found a significant effect of condition both in the case of VGGish (B = 0.0033, SE = 0.0015, <italic>t</italic> = 2.176, <italic>p</italic> = 0.0296, <italic>N</italic> = 2,308,063) and SpeechVGG (B = 0.01350, SE = 0.0017, <italic>t</italic> = 7.789, <italic>p</italic>
<inline-formula id="IEq30"><alternatives><tex-math id="d33e1390">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ll $$\end{document}</tex-math><mml:math id="d33e1395"><mml:mo>&#x0226a;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq30.gif"/></alternatives></inline-formula> 0.0001, <italic>N</italic> = 2,308,063). The results were obtained by dropping the random slopes from the models &#x02013; which had variance equal to zero &#x02013;, but aside from convergence issues the random structure of the model did not affect the parameter estimation, as the fixed effects of condition remained significant. It must, however, be acknowledged that, although significant, the observed effects are small, with numerically reduced differences in means and relatively high variability in the data, and obtained by considering a rather large dataset.</p><p id="Par42">Note that the absolute differences between the mean similarities obtained with the spectra, VGGish, and SpeechVGG are not of interest for the purposes of the study. They simply reflect that the auditory space where <inline-formula id="IEq31"><alternatives><tex-math id="d33e1404">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e1409"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq32"><alternatives><tex-math id="d33e1418">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e1423"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula> are projected is characterized by different levels of dispersion. Indeed, the average closeness to the centroid,<xref ref-type="fn" rid="Fn9">9</xref> defined as <inline-formula id="IEq33"><alternatives><tex-math id="d33e1436">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{N}\sum _{i=1}^{N}cos(x_i, \bar{x})$$\end{document}</tex-math><mml:math id="d33e1441"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">&#x000af;</mml:mo></mml:mrow></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq33.gif"/></alternatives></inline-formula>, is 0.4835 employing the sound spectra, 0.6644 in the case of VGGish, and 0.0013 with SpeechVGG. This indicates that the sound spectra and VGGish project the sound vectors relatively close to the centroid, resulting in medium-to-high similarity scores between its generated embeddings. Conversely, the opposite holds for SpeechVGG, where the network generates sound vectors that are, on average, nearly orthogonal. Note that the embedding spaces also contained many <inline-formula id="IEq34"><alternatives><tex-math id="d33e1473">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e1478"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq35"><alternatives><tex-math id="d33e1488">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e1493"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula> pairs that had negative cosine similarity values (VGGish: <inline-formula id="IEq36"><alternatives><tex-math id="d33e1502">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c; 0.01$$\end{document}</tex-math><mml:math id="d33e1507"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq36.gif"/></alternatives></inline-formula>%; SpeechVGG: 70.71%; spectra: 60.44%). In the context of standard, text-based distributional semantic models, it has been argued that negative cosine similarity values carry little useful semantic information (Rotaru, Vigliocco, &#x00026; Frank, <xref ref-type="bibr" rid="CR81">2018</xref>) and it is not clear how to interpret them (G&#x000fc;nther &#x00026; Marelli, <xref ref-type="bibr" rid="CR34">2016</xref>). While it is unclear whether these considerations also apply to auditory vectors, especially since negative cosine similarity values are so common, we repeated the analyses setting the negative cosine similarity values to zero. This methodological choice had no major impact on our results (see Appendix <xref rid="Sec14" ref-type="sec">A</xref>).</p><p id="Par44">As a robustness check, we verified that the reported effects in the mixed effects models analyses remained significant with non-parametric testing. To do so, we randomly reassigned the condition variable (i.e., the independent variable of interest indicating whether <inline-formula id="IEq37"><alternatives><tex-math id="d33e1524">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_w}$$\end{document}</tex-math><mml:math id="d33e1529"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq38"><alternatives><tex-math id="d33e1538">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{s_n}$$\end{document}</tex-math><mml:math id="d33e1543"><mml:mover accent="true"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq2.gif"/></alternatives></inline-formula> corresponded to the same label). For each word sound and natural sound, we randomly selected a pair to be coded as matching, and refitted the mixed effects regression model with the same specification on the new dataset. This procedure was repeated 500 times for each regression model, and empirical <italic>p</italic>&#x000a0;values were calculated as the proportion of the models with random condition assignment that obtained an absolute <italic>t</italic> value at least as extreme as the one observed in the corresponding model with proper condition assignment. Empirical <italic>p</italic>&#x000a0;values were obtained after adding 1 to both the numerator and the denominator (Davison &#x00026; Hinkley, <xref ref-type="bibr" rid="CR17">1997</xref>). The non-parametric analyses yielded similar results to the parametric ones (spectrum: <italic>p</italic> = 0.1537; VGGish: <italic>p</italic> = 0.0100; SpeechVGG: <italic>p</italic> = 0.0020).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Five most and least auditorily iconic words according to the metrics based on the sound spectra, VGGish, and SpeechVGG (<italic>Estimate</italic>)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="3">Spectrum</th><th align="left" colspan="3">VGGish</th><th align="left" colspan="3">SpeechVGG</th></tr><tr><th align="left"/><th align="left">Word</th><th align="left">Estimate</th><th align="left">Rating</th><th align="left">Word</th><th align="left">Estimate</th><th align="left">Rating</th><th align="left">Word</th><th align="left">Estimate</th><th align="left">Rating</th></tr></thead><tbody><tr><td align="left">Most iconic</td><td align="left">thud</td><td align="left">0.9359</td><td align="left">6.3000</td><td align="left">announcer</td><td align="left">0.7722</td><td align="left">3.7272</td><td align="left">drip</td><td align="left">0.5374</td><td align="left">5.6000</td></tr><tr><td align="left"/><td align="left">frog</td><td align="left">0.9218</td><td align="left">3.8000</td><td align="left">media</td><td align="left">0.7683</td><td align="left">2.8000</td><td align="left">pop</td><td align="left">0.4894</td><td align="left">6.4000</td></tr><tr><td align="left"/><td align="left">chat</td><td align="left">0.9061</td><td align="left">4.4000</td><td align="left">mumble</td><td align="left">0.7584</td><td align="left">5.0909</td><td align="left">thud</td><td align="left">0.4875</td><td align="left">6.3000</td></tr><tr><td align="left"/><td align="left">click</td><td align="left">0.9034</td><td align="left">6.7000</td><td align="left">vocal</td><td align="left">0.7578</td><td align="left">3.2727</td><td align="left">clunk</td><td align="left">0.4788</td><td align="left">6.8000</td></tr><tr><td align="left"/><td align="left">drip</td><td align="left">0.8916</td><td align="left">5.6000</td><td align="left">speech</td><td align="left">0.7563</td><td align="left">NA</td><td align="left">click</td><td align="left">0.4477</td><td align="left">6.7000</td></tr><tr><td align="left"/><td align="left"><inline-formula id="IEq39"><alternatives><tex-math id="d33e1730">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdots $$\end{document}</tex-math><mml:math id="d33e1735"><mml:mo>&#x022ef;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq39.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq40"><alternatives><tex-math id="d33e1740">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdots $$\end{document}</tex-math><mml:math id="d33e1745"><mml:mo>&#x022ef;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq39.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq41"><alternatives><tex-math id="d33e1750">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdots $$\end{document}</tex-math><mml:math id="d33e1755"><mml:mo>&#x022ef;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq39.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq42"><alternatives><tex-math id="d33e1760">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdots $$\end{document}</tex-math><mml:math id="d33e1765"><mml:mo>&#x022ef;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq39.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq43"><alternatives><tex-math id="d33e1770">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdots $$\end{document}</tex-math><mml:math id="d33e1775"><mml:mo>&#x022ef;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq39.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq44"><alternatives><tex-math id="d33e1780">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdots $$\end{document}</tex-math><mml:math id="d33e1785"><mml:mo>&#x022ef;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq39.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq45"><alternatives><tex-math id="d33e1790">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdots $$\end{document}</tex-math><mml:math id="d33e1795"><mml:mo>&#x022ef;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq39.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq46"><alternatives><tex-math id="d33e1800">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdots $$\end{document}</tex-math><mml:math id="d33e1805"><mml:mo>&#x022ef;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq39.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq47"><alternatives><tex-math id="d33e1810">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdots $$\end{document}</tex-math><mml:math id="d33e1815"><mml:mo>&#x022ef;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq39.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">Least iconic</td><td align="left">beg</td><td align="left">&#x02013;0.6272</td><td align="left">4.8000</td><td align="left">response</td><td align="left">0.1703</td><td align="left">NA</td><td align="left">trumpet</td><td align="left">&#x02013;0.3778</td><td align="left">4.0000</td></tr><tr><td align="left"/><td align="left">people</td><td align="left">&#x02013;0.6361</td><td align="left">3.4000</td><td align="left">story</td><td align="left">0.1612</td><td align="left">NA</td><td align="left">engine</td><td align="left">&#x02013;0.3786</td><td align="left">NA</td></tr><tr><td align="left"/><td align="left">tune</td><td align="left">&#x02013;0.6495</td><td align="left">4.4000</td><td align="left">beg</td><td align="left">0.1463</td><td align="left">4.8000</td><td align="left">violin</td><td align="left">&#x02013;0.3864</td><td align="left">3.7000</td></tr><tr><td align="left"/><td align="left">jet</td><td align="left">&#x02013;0.6644</td><td align="left">3.9000</td><td align="left">newsflash</td><td align="left">0.1211</td><td align="left">NA</td><td align="left">broadcast</td><td align="left">&#x02013;0.3989</td><td align="left">NA</td></tr><tr><td align="left"/><td align="left">bellow</td><td align="left">&#x02013;0.6722</td><td align="left">4.2000</td><td align="left">soundtrack</td><td align="left">0.0916</td><td align="left">NA</td><td align="left">report</td><td align="left">&#x02013;0.3999</td><td align="left">2.5000</td></tr></tbody></table><table-wrap-foot><p>The column <italic>Rating</italic> reports the average iconicity rating associated to the word (Winter et&#x000a0;al., <xref ref-type="bibr" rid="CR104">2023</xref>). The complete results are available in the online supplementary materials</p></table-wrap-foot></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Results of the linear models with the human ratings as dependent variables, the data-driven iconicity estimates as predictors, and the baseline of psycholinguistic variables listed in the Methods section as covariates</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left" colspan="5">Spectrum</th><th align="left" colspan="5">VGGish</th><th align="left" colspan="5">SpeechVGG</th></tr><tr><th align="left"/><th align="left">Measure</th><th align="left">B</th><th align="left">SE</th><th align="left"><italic>t</italic></th><th align="left"><italic>p</italic></th><th align="left"><inline-formula id="IEq48"><alternatives><tex-math id="d33e1980">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e1985"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup></th><th align="left">B</th><th align="left">SE</th><th align="left"><italic>t</italic></th><th align="left"><italic>p</italic></th><th align="left"><inline-formula id="IEq49"><alternatives><tex-math id="d33e2006">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e2011"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup></th><th align="left">B</th><th align="left">SE</th><th align="left"><italic>t</italic></th><th align="left"><italic>p</italic></th><th align="left"><inline-formula id="IEq50"><alternatives><tex-math id="d33e2031">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e2036"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup></th></tr></thead><tbody><tr><td align="left">Winter et al. (<xref ref-type="bibr" rid="CR106">2017</xref>)</td><td align="left">Markedness</td><td align="left">&#x02013;0.564</td><td align="left">1.592</td><td align="left">&#x02013;0.354</td><td align="left">0.724</td><td align="left">0.003</td><td align="left">&#x02013;3.005</td><td align="left">3.878</td><td align="left">&#x02013;0.775</td><td align="left">0.440</td><td align="left">0.003</td><td align="left">&#x02013;0.673</td><td align="left">1.718</td><td align="left">&#x02013;0.392</td><td align="left">0.696</td><td align="left">0.000</td></tr><tr><td align="left"/><td align="left">CD</td><td align="left">&#x02013;0.580</td><td align="left">0.292</td><td align="left">&#x02013;1.984</td><td align="left">0.049</td><td align="left">0.115</td><td align="left">&#x02013;0.625</td><td align="left">0.294</td><td align="left">&#x02013;2.124</td><td align="left">0.035</td><td align="left">0.121</td><td align="left">&#x02013;0.560</td><td align="left">0.288</td><td align="left">&#x02013;1.946</td><td align="left">0.054</td><td align="left">0.111</td></tr><tr><td align="left"/><td align="left">Auditory</td><td align="left">&#x02013;0.053</td><td align="left">0.247</td><td align="left">&#x02013;0.216</td><td align="left">0.829</td><td align="left">0.000</td><td align="left">&#x02013;0.019</td><td align="left">0.254</td><td align="left">&#x02013;0.075</td><td align="left">0.940</td><td align="left">0.001</td><td align="left">0.008</td><td align="left">0.245</td><td align="left">0.034</td><td align="left">0.973</td><td align="left">0.001</td></tr><tr><td align="left"/><td align="left">Conc.</td><td align="left">&#x02013;0.553</td><td align="left">0.147</td><td align="left">&#x02013;3.769</td><td align="left"><inline-formula id="IEq51"><alternatives><tex-math id="d33e2165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e2170"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.059</td><td align="left">&#x02013;0.557</td><td align="left">0.150</td><td align="left">&#x02013;3.706</td><td align="left"><inline-formula id="IEq52"><alternatives><tex-math id="d33e2186">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e2191"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.058</td><td align="left">&#x02013;0.502</td><td align="left">0.147</td><td align="left">&#x02013;3.419</td><td align="left">0.001</td><td align="left">0.053</td></tr><tr><td align="left"/><td align="left">Freq.</td><td align="left">0.010</td><td align="left">0.560</td><td align="left">0.018</td><td align="left">0.986</td><td align="left">0.107</td><td align="left">0.056</td><td align="left">0.566</td><td align="left">0.099</td><td align="left">0.921</td><td align="left">0.112</td><td align="left">0.020</td><td align="left">0.551</td><td align="left">0.035</td><td align="left">0.972</td><td align="left">0.104</td></tr><tr><td align="left"/><td align="left">AoA</td><td align="left">&#x02013;0.235</td><td align="left">0.072</td><td align="left">&#x02013;3.250</td><td align="left">0.001</td><td align="left">0.036</td><td align="left">&#x02013;0.230</td><td align="left">0.075</td><td align="left">&#x02013;3.075</td><td align="left">0.003</td><td align="left">0.035</td><td align="left">&#x02013;0.218</td><td align="left">0.072</td><td align="left">&#x02013;3.039</td><td align="left">0.003</td><td align="left">0.033</td></tr><tr><td align="left"/><td align="left">Length</td><td align="left">&#x02013;0.074</td><td align="left">0.057</td><td align="left">&#x02013;1.295</td><td align="left">0.197</td><td align="left">0.015</td><td align="left">&#x02013;0.051</td><td align="left">0.057</td><td align="left">&#x02013;0.897</td><td align="left">0.371</td><td align="left">0.012</td><td align="left">&#x02013;0.047</td><td align="left">0.056</td><td align="left">&#x02013;0.849</td><td align="left">0.397</td><td align="left">0.012</td></tr><tr><td align="left"/><td align="left"><inline-formula id="IEq53"><alternatives><tex-math id="d33e2319">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\underline{\textrm{Iconicity}}$$\end{document}</tex-math><mml:math id="d33e2324"><mml:munder><mml:mtext>Iconicity</mml:mtext><mml:mo>&#x0005f;</mml:mo></mml:munder></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq53.gif"/></alternatives></inline-formula></td><td align="left">0.630</td><td align="left">0.208</td><td align="left">3.027</td><td align="left">0.003</td><td align="left">0.066</td><td align="left">1.667</td><td align="left">0.794</td><td align="left">2.100</td><td align="left">0.037</td><td align="left">0.041</td><td align="left">2.164</td><td align="left">0.594</td><td align="left">3.641</td><td align="left"><inline-formula id="IEq54"><alternatives><tex-math id="d33e2358">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e2363"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.102</td></tr><tr><td align="left">Winter et al. (<xref ref-type="bibr" rid="CR104">2023</xref>)</td><td align="left">Markedness</td><td align="left">0.877</td><td align="left">0.895</td><td align="left">0.980</td><td align="left">0.328</td><td align="left">0.003</td><td align="left">&#x02013;0.655</td><td align="left">2.271</td><td align="left">&#x02013;0.288</td><td align="left">0.773</td><td align="left">0.000</td><td align="left">0.609</td><td align="left">1.166</td><td align="left">0.522</td><td align="left">0.602</td><td align="left">0.001</td></tr><tr><td align="left"/><td align="left">CD</td><td align="left">&#x02013;0.079</td><td align="left">0.233</td><td align="left">&#x02013;0.338</td><td align="left">0.736</td><td align="left">0.037</td><td align="left">&#x02013;0.124</td><td align="left">0.232</td><td align="left">&#x02013;0.533</td><td align="left">0.594</td><td align="left">0.040</td><td align="left">&#x02013;0.118</td><td align="left">0.232</td><td align="left">&#x02013;0.509</td><td align="left">0.611</td><td align="left">0.038</td></tr><tr><td align="left"/><td align="left">Auditory</td><td align="left">0.151</td><td align="left">0.175</td><td align="left">0.865</td><td align="left">0.388</td><td align="left">0.005</td><td align="left">0.141</td><td align="left">0.176</td><td align="left">0.806</td><td align="left">0.421</td><td align="left">0.005</td><td align="left">0.131</td><td align="left">0.175</td><td align="left">0.749</td><td align="left">0.454</td><td align="left">0.005</td></tr><tr><td align="left"/><td align="left">Conc.</td><td align="left">&#x02013;0.116</td><td align="left">0.089</td><td align="left">&#x02013;1.303</td><td align="left">0.194</td><td align="left">0.004</td><td align="left">&#x02013;0.095</td><td align="left">0.092</td><td align="left">&#x02013;1.032</td><td align="left">0.303</td><td align="left">0.004</td><td align="left">&#x02013;0.066</td><td align="left">0.093</td><td align="left">&#x02013;0.708</td><td align="left">0.479</td><td align="left">0.004</td></tr><tr><td align="left"/><td align="left">Freq.</td><td align="left">&#x02013;0.497</td><td align="left">0.457</td><td align="left">&#x02013;1.088</td><td align="left">0.278</td><td align="left">0.040</td><td align="left">&#x02013;0.453</td><td align="left">0.457</td><td align="left">&#x02013;0.991</td><td align="left">0.323</td><td align="left">0.043</td><td align="left">&#x02013;0.438</td><td align="left">0.456</td><td align="left">&#x02013;0.960</td><td align="left">0.338</td><td align="left">0.041</td></tr><tr><td align="left"/><td align="left">AoA</td><td align="left">&#x02013;0.178</td><td align="left">0.042</td><td align="left">&#x02013;4.264</td><td align="left"><inline-formula id="IEq55"><alternatives><tex-math id="d33e2564">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e2569"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.065</td><td align="left">&#x02013;0.180</td><td align="left">0.042</td><td align="left">&#x02013;4.295</td><td align="left"><inline-formula id="IEq56"><alternatives><tex-math id="d33e2585">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e2590"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.065</td><td align="left">&#x02013;0.178</td><td align="left">0.042</td><td align="left">&#x02013;4.274</td><td align="left"><inline-formula id="IEq57"><alternatives><tex-math id="d33e2607">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e2612"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.066</td></tr><tr><td align="left"/><td align="left">Length</td><td align="left">&#x02013;0.167</td><td align="left">0.036</td><td align="left">&#x02013;4.693</td><td align="left"><inline-formula id="IEq58"><alternatives><tex-math id="d33e2632">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e2637"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.087</td><td align="left">&#x02013;0.143</td><td align="left">0.034</td><td align="left">&#x02013;4.177</td><td align="left"><inline-formula id="IEq59"><alternatives><tex-math id="d33e2653">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e2658"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.079</td><td align="left">&#x02013;0.151</td><td align="left">0.035</td><td align="left">&#x02013;4.368</td><td align="left"><inline-formula id="IEq60"><alternatives><tex-math id="d33e2675">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e2680"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.081</td></tr><tr><td align="left"/><td align="left"><underline>Iconicity</underline></td><td align="left">0.346</td><td align="left">0.148</td><td align="left">2.340</td><td align="left">0.020</td><td align="left">0.028</td><td align="left">1.030</td><td align="left">0.471</td><td align="left">2.185</td><td align="left">0.030</td><td align="left">0.028</td><td align="left">0.948</td><td align="left">0.393</td><td align="left">2.413</td><td align="left">0.017</td><td align="left">0.033</td></tr></tbody></table><table-wrap-foot><p>The <inline-formula id="IEq61"><alternatives><tex-math id="d33e2728">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e2733"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup> is the R<sup>2</sup> ascribed to the measure of interest, partitioned by averaging over the orders of the predictors (Gr&#x000f6;mping, <xref ref-type="bibr" rid="CR33">2007</xref>)</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec10"><title>Prediction of human ratings</title><p id="Par45">Table <xref rid="Tab1" ref-type="table">1</xref> reports the five most and least iconic words according to the three representational formats we considered in our study. From a qualitative inspection it appears that both the spectrum- and the SpeechVGG-based iconicity estimates correctly identified onomatopoetic words as the most iconic (e.g., &#x0201c;thud&#x0201d;, &#x0201c;click&#x0201d;); on the other hand, the VGGish-based metric seems to be biased in assigning words referring to vocal sounds high iconicity scores (e.g., &#x0201c;announcer&#x0201d;, &#x0201c;speech&#x0201d;). We speculate that VGGish might assign high similarity scores to vocal sounds due to the task it was trained on (aspecific sound classification), which induced a pressure to represent vocal sounds as similar to each other, regardless of their specific content. If vocal sounds are classified as a single or a few classes in the pre-training tag set, they will be represented similarly. This characteristic of the VGGish model might explain the observed bias in the iconicity scores assigned to words referring to vocal sounds. From a quantitative analysis, it emerged that all three of our iconicity measures displayed a significant correlation with the explicit ratings obtained from human participants in norming studies. More precisely, the spectrum-based similarity scores were significantly correlated with both the ratings provided by Winter et&#x000a0;al. (<xref ref-type="bibr" rid="CR106">2017</xref>) (<italic>r</italic> = 0.2634, <italic>p</italic> = 0.0004, <italic>N</italic> = 175) and the ones released by Winter et&#x000a0;al. (<xref ref-type="bibr" rid="CR104">2023</xref>) (<italic>r</italic> = 0.1755, <italic>p</italic> = 0.0056, <italic>N</italic> = 248). Likewise, the SpeechVGG-based iconicity scores were significantly associated with the ratings obtained in the two norming studies (<italic>r</italic> = 0.3348, <italic>p</italic>
<inline-formula id="IEq62"><alternatives><tex-math id="d33e2784">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ll $$\end{document}</tex-math><mml:math id="d33e2789"><mml:mo>&#x0226a;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq30.gif"/></alternatives></inline-formula> 0.0001, <italic>N</italic> = 175; <italic>r</italic> = 0.1672, <italic>p</italic> = 0.0083, <italic>N</italic> = 248), and the same held for the data-driven scores obtained with VGGish (<italic>r</italic> = 0.1885, <italic>p</italic> = 0.0125, <italic>N</italic> = 175, and <italic>r</italic> = 0.1742, <italic>p</italic> = 0.0060, <italic>N</italic> = 248, respectively). Furthermore, the spectrum-based metric was significantly correlated with the similarity scores based on VGGish (<italic>r</italic> = 0.2499, <italic>p</italic>
<inline-formula id="IEq63"><alternatives><tex-math id="d33e2831">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ll $$\end{document}</tex-math><mml:math id="d33e2836"><mml:mo>&#x0226a;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq30.gif"/></alternatives></inline-formula> 0.0001, <italic>N</italic> = 367) and SpeechVGG (<italic>r</italic> = 0.4958, <italic>p</italic>
<inline-formula id="IEq64"><alternatives><tex-math id="d33e2849">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ll $$\end{document}</tex-math><mml:math id="d33e2854"><mml:mo>&#x0226a;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq30.gif"/></alternatives></inline-formula> 0.0001, <italic>N</italic> = 367), and the two measures we obtained from the two neural architectures were significantly associated (<italic>r</italic> = 0.3969, <italic>p</italic>
<inline-formula id="IEq65"><alternatives><tex-math id="d33e2868">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ll $$\end{document}</tex-math><mml:math id="d33e2873"><mml:mo>&#x0226a;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq30.gif"/></alternatives></inline-formula> 0.0001, <italic>N</italic> = 367), indicating that while each method might have been sensitive to different sound properties, they all revealed a coherent pattern of similarity. When the predictive power of our iconicity measurements was assessed against the baseline of psycholinguistic variables described in the previous section, the best predictor of human ratings was the SpeechVGG-based metric, which clearly outperformed the other predictors in both datasets in terms of <inline-formula id="IEq66"><alternatives><tex-math id="d33e2880">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e2885"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup> (see Table <xref rid="Tab2" ref-type="table">2</xref>). Furthermore, all three iconicity estimates were significantly associated with human responses across both rating datasets.</p></sec><sec id="Sec11"><title>Removal of onomatopoetic words</title><p id="Par46">The two annotators displayed a substantial agreement (as per Cohen&#x02019;s guidelines; see Cohen <xref ref-type="bibr" rid="CR15">1960</xref>) in identifying the onomatopoetic words in our original dataset (<inline-formula id="IEq67"><alternatives><tex-math id="d33e2903">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\kappa $$\end{document}</tex-math><mml:math id="d33e2908"><mml:mi>&#x003ba;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq67.gif"/></alternatives></inline-formula> = 0.642, <italic>z</italic> = 12.4, <italic>p</italic> &#x0003c; 0.001, <italic>N</italic> = 374). The words that were identified as onomatopoetic or dubious by at least one annotator were 139 (37.17%).</p><p id="Par47">After the removal of the words judged as onomatopoetic, we were not able to detect a significant effect of condition employing the raw sound spectra (B = 0.0005, SE = 0.0059, <italic>t</italic> = 0.086, <italic>p</italic> = 0.932, <italic>N</italic> = 913,481) nor VGGish (B = 0.0023, SE = 0.0019, <italic>t</italic> = 1.181, <italic>p</italic> = 0.238, <italic>N</italic> = 913,481); however, we did find an effect of condition when employing SpeechVGG-based representations (B = 0.0060, SE = 0.0021, <italic>t</italic> = 2.822, <italic>p</italic> = 0.0048, <italic>N</italic> = 913,481). Non-parametric significance testing led to the same conclusions (spectrum: <italic>p</italic> = 0.9182; VGGish: <italic>p</italic> = 0.2036; SpeechVGG: <italic>p</italic> = 0.0040).</p><p id="Par48">Regarding the prediction of the human ratings, no data-driven iconicity estimate was significantly correlated with the human rating data after onomatopoetic forms were excluded, neither for the ratings provided by Winter et&#x000a0;al. (<xref ref-type="bibr" rid="CR106">2017</xref>) (spectra: <italic>r</italic> = 0.1632, <italic>p</italic> = 0.1181, <italic>N</italic> = 93; VGGish: <italic>r</italic> = -0.0313, <italic>p</italic> = 0.7660, <italic>N</italic> = 93; SpeechVGG: <italic>r</italic> = 0.1129, <italic>p</italic> = 0.2813, <italic>N</italic> = 93) nor for the ones released by Winter et&#x000a0;al. (<xref ref-type="bibr" rid="CR104">2023</xref>) (spectra: <italic>r</italic> = 0.0053, <italic>p</italic> = 0.9456, <italic>N</italic> = 164; VGGish: <italic>r</italic> = -0.0112, <italic>p</italic> = 0.8868, <italic>N</italic> = 164; SpeechVGG: <italic>r</italic> = -0.0976, <italic>p</italic> = 0.214, <italic>N</italic> = 164). The effect did not emerge after controlling for the iconicity-related covariates described in the Methods section, neither in the dataset released by Winter et&#x000a0;al. (<xref ref-type="bibr" rid="CR106">2017</xref>) (spectra: B = 0.197, SE = 0.231, <italic>t</italic> = 0.850, <italic>p</italic> = 0.398, <inline-formula id="IEq68"><alternatives><tex-math id="d33e3036">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e3041"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup> =0.020, <italic>N</italic> = 93; VGGish: B = 0.165, SE = 0.773, <italic>t</italic> = 0.214, <italic>p</italic> = 0.831, <inline-formula id="IEq69"><alternatives><tex-math id="d33e3057">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e3062"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup> = 0.002, <italic>N</italic> = 93; SpeechVGG: B = 0.305, SE = 0.781, <italic>t</italic> = 0.390, <italic>p</italic> = 0.697, <inline-formula id="IEq70"><alternatives><tex-math id="d33e3079">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e3084"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup> = 0.010, <italic>N</italic> = 93) nor Winter et&#x000a0;al. (<xref ref-type="bibr" rid="CR104">2023</xref>) (spectra: B = -0.020, SE = 0.163, <italic>t</italic> = -0.121, <italic>p</italic> = 0.904, <inline-formula id="IEq71"><alternatives><tex-math id="d33e3104">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e3109"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup> = 0.000, <italic>N</italic> = 164; VGGish: B = 0.022, SE = 0.487, <italic>t</italic> = 0.046, <italic>p</italic> = 0.964, <inline-formula id="IEq72"><alternatives><tex-math id="d33e3126">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e3131"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup> = 0.000, <italic>N</italic> = 164; SpeechVGG: B = -0.402, SE = 0.445, <italic>t</italic> = -0.903, <italic>p</italic> = 0.368, <inline-formula id="IEq73"><alternatives><tex-math id="d33e3148">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e3153"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup> = 0.006, <italic>N</italic> = 164).</p></sec></sec><sec id="Sec12"><title>Discussion</title><p id="Par49">The present work provides empirical support for the idea of a widespread phonosymbolic substrate underlying the auditory vocabulary: when transformed into an HCNN-based compact vector format, words with high perceptual strength in the auditory modality resemble the sounds of their referent more than what would be expected by chance. Notably, these auditory phonosymbolic patterns can be detected even in English, a language that is known to be iconically impoverished (Nuckolls, <xref ref-type="bibr" rid="CR72">2003</xref>). Auditory mimicry thus plays a significant role in the phonetic structure of the auditory lexicon, extending beyond the anecdotal cases offered by self-evident onomatopoetic forms (see also Thompson, Van&#x000a0;Hoey, and Do <xref ref-type="bibr" rid="CR97">2021</xref> for similar results in the case of motion ideophones). Indeed, our best-performing model is able to detect signs of imitation in the lexicon even if the words that are perceived as onomatopoetic by human annotators are removed from the analyses. An interesting open question is why the pervasiveness of onomatopoeia has often eluded the intuitions of the philosophers and linguists who studied the nature of linguistic signs. Historically, the role of iconicity in language has been consistently dismissed (Hockett, <xref ref-type="bibr" rid="CR41">1960</xref>; Saussure, <xref ref-type="bibr" rid="CR84">1964</xref>) or restricted to a merely &#x0201c;not wholly insignificant&#x0201d; (Whitney, <xref ref-type="bibr" rid="CR103">1874</xref>,&#x000a0;p. 102) or &#x0201c;vanishingly small&#x0201d; (Newmeyer, <xref ref-type="bibr" rid="CR70">1992</xref>,&#x000a0;p. 758) portion of the lexicon. Even in contemporary iconicity research, most studies have trended towards exploring subjective and indirect characterizations of iconicity. For instance, Winter et&#x000a0;al. (<xref ref-type="bibr" rid="CR104">2023</xref>) defined iconicity as &#x0201c;perceived resemblance&#x0201d; between form and meaning; on a similar vein, Sidhu and Pexman (<xref ref-type="bibr" rid="CR87">2018</xref>) meticulously differentiated sound symbolism from onomatopoeia as different categories possibly supported by different mechanisms, and focused on the latter in their review. These examples illustrate that recent studies in the iconicity literature emphasize instances of indirect iconicity, without recognizing a core role to direct phonetic mimicry. We speculate that the little attention that has been directed towards imitative iconicity, both historically and in recent years, can be ascribed to the natural limitations to the degree of resemblance that vocal sounds can exhibit with respect to natural sounds.</p><p id="Par50">Indeed, the process of compressing noises into sequences of vowels and consonants must abide by both biological and linguistic constraints. First, the vocal sounds must be pronounceable, consistently with the anatomical structure of the human vocal system (Assaneo, Nichols, &#x00026; Trevisan, <xref ref-type="bibr" rid="CR5">2011</xref>). Second, they must be part of the phonological inventory of a given language (Bredin, <xref ref-type="bibr" rid="CR10">1996</xref>). These limitations will inevitably result in an information loss, which might hinder the correspondence between linguistic and non-linguistic sounds, increasing their absolute spectral difference and ultimately making it more difficult to appreciate such correspondences. The employment of objective sound similarity measures can reveal subtle correspondences between natural and vocal sounds that might not be accessible to human intuitions, but are anyway encoded in the phonological profiles of AIWs.</p><p id="Par51">While apparently the sound representations based on the raw spectra were not sufficient to detect the widespread presence of auditory iconicity, both VGGish and SpeechVGG produced sound embeddings that reflected the general correspondence between natural and speech sounds. This is particularly informative since the two networks treat the input sounds in a fundamentally different way: VGGish is accustomed to process words as if they were natural sounds, in line with the type of data it received as input during pre-training; conversely, SpeechVGG handles natural sounds as vocal sounds.<xref ref-type="fn" rid="Fn10">10</xref> The fact that both networks largely detected the general similarity between congruent natural and vocal sound data strengthens the reliability of our findings, providing converging evidence from two independent representational formats.</p><p id="Par53">Besides attesting the pervasiveness of iconicity in the auditory lexicon, our computational measures were significantly predictive of human judgements on form-meaning resemblance. This result shows that participants that produce iconicity ratings sensitize to the similarity between word sounds and their referents, approximating their distance in an auditory space. On the other hand, the predictivity of our measurements with respect to iconicity ratings only holds if onomatopoetic words are included in the sample, suggesting that, for words that are not obviously onomatopoetic, participants do not base their judgements on sound resemblance. These results are compatible with the proposal by Thompson et al. (<xref ref-type="bibr" rid="CR94">2020</xref>), who suggested that, for words that are not obviously iconic, participants may not base their responses on form-meaning resemblance (i.e., what iconicity ratings are supposed to measure), but rather on perceptual information, without any link to phonology. However, an alternative interpretation of our findings is that, for words that are not identified as onomatopoetic, participants might base their iconicity judgements on non-auditory properties. Our study focused on a single dimension of similarity, namely auditory resemblance, whereas iconicity ratings are expected to be produced by taking into account perceptual information from multiple modalities. We acknowledge that, in order to fully capture human intuitions about iconicity, future studies will have to consider other sensory dimensions as well.</p><p id="Par54">In our analyses, SpeechVGG consistently outperformed VGGish and the spectral baseline both in detecting iconic patterns in the lexicon and predicting human iconicity ratings across Winter et&#x000a0;al. &#x02019;s (<xref ref-type="bibr" rid="CR106">2017</xref>) and Winter et&#x000a0;al. &#x02019;s (<xref ref-type="bibr" rid="CR104">2023</xref>) norms. This result suggests that optimizing a network to be maximally sensitive to phonetic distinctions increases its ability to detect iconic patterns in language. We speculate that this difference might be due to the inherent asymmetry in the variability of speech signals and natural sounds. The natural sounds considered in this study covered a wide range of domains, from environmental noises to animal cries and sounds produced by human-created objects; conversely, speech sounds were limited in their variability by the English phonological inventory. A small fluctuation in a speech sound might entail a substantial meaning shift (e.g., &#x0201c;pun&#x0201d; 
<inline-graphic xlink:href="13423_2024_2630_Figc_HTML.gif" id="d33e3220"/>
and &#x0201c;bun&#x0201d; 
<inline-graphic xlink:href="13423_2024_2630_Figd_HTML.gif" id="d33e3223"/>
, which only differ by one subsegmental feature [+/- voiced] in American English); however, it is difficult to conceive two natural sounds that are nearly identical and yet correspond to two radically different objects, which humans are able to distinguish on the basis of their sound. If a model is not optimized to be receptive to speech sounds, small and yet meaningful phonetic differences might be lost in the representations it produces, ultimately affecting the network&#x02019;s accessibility to auditory iconic patterns.</p><p id="Par55">In the present study, we operationalized the construct of auditory iconicity in its most elemental terms as the objective similarity between the sound of spoken words and natural sounds associated to their referents. This choice entails that our study has high construct validity, and indicates that the correspondences we detected are inherently iconic. Conversely, construct validity is often weaker when adopting other experimental designs. Indeed, a common difficulty in iconicity research is to disentangle iconic biases, i.e., analogical and perception-related correspondences, from other forms of non-arbitrariness. For instance, phonosemantic regularities can be an instantiation of systematicity, i.e., statistical regularities in form-meaning relationships that are not meaningful in themselves, but are a property of a linguistic system as a whole (Dingemanse et&#x000a0;al., <xref ref-type="bibr" rid="CR22">2015</xref>; Murgiano, Motamedi, &#x00026; Vigliocco, <xref ref-type="bibr" rid="CR69">2021</xref>; Thompson &#x00026; Do, <xref ref-type="bibr" rid="CR96">2019</xref>). Another alternative to iconicity is indexicality, a form of non-arbitrariness that relates perceptual experience (e.g., a slap) and words (e.g., <italic>ouch</italic>) on account of their co-occurrence, without any imitative link (Dingemanse, <xref ref-type="bibr" rid="CR21">2021</xref>). While discerning between these alternatives is often problematic, our experimental setting measures within-modality direct resemblance, mitigating this theoretical complication. Note, however, that we cannot exclude the possibility that our findings might have been partially driven by indirect iconicity.<xref ref-type="fn" rid="Fn11">11</xref> Consider for instance the case of bird names. Larger birds tend to sing deeper songs (Ryan &#x00026; Brenowitz, <xref ref-type="bibr" rid="CR82">1985</xref>); in this case, if bird names were dependent on their size, the resemblance between bird names and their associated sounds would be mediated by size sound symbolism, and not sound imitation. We believe that the qualitative examples we reported suggest that, at least for SpeechVGG and the sound spectra, the iconic associations we detected with our approach reflect direct sound imitation, as the words that are most iconic according to our estimates are onomatopoetic. However, we leave to future research a thorough analysis of the impact of non-imitative indirect iconicity on data-driven estimates of sound resemblance.</p></sec><sec id="Sec13"><title>Conclusion</title><p id="Par57">Several studies have inspected the relationship between iconicity ratings and different semantic dimensions, such as concreteness (Hinojosa et&#x000a0;al., <xref ref-type="bibr" rid="CR40">2021</xref>; Winter et&#x000a0;al., <xref ref-type="bibr" rid="CR104">2023</xref>) and perceptual experience (Sidhu &#x00026; Pexman, <xref ref-type="bibr" rid="CR87">2018</xref>; Winter et&#x000a0;al., <xref ref-type="bibr" rid="CR106">2017</xref>, <xref ref-type="bibr" rid="CR104">2023</xref>). Perhaps surprisingly, no study to our knowledge has ever assessed the relationship between iconicity ratings and the construct they are expected to measure, namely perceptual resemblance. This is precisely what we empirically tested in this article. With this respect, our study yielded mixed results: iconicity ratings appear to be reliable in identifying onomatopoetic forms, which drive the correlation between data-driven and human-annotated iconicity estimates. When the similarity between word and natural sounds is not evident, iconicity ratings are not approximated by objective measurements of sound resemblance. Future research is needed to understand the factors that explain iconicity ratings in those conditions, taking into account non-auditory semantic features. At the same time, our study demonstrates that data-driven alternatives to human judgments do exist, and can be employed to study subtle and elusive phenomena such as iconicity.</p></sec></body><back><app-group><app id="App1"><sec id="Sec14"><title>Appendix A: Negative cosine similarities</title><p id="Par66">As mentioned in the Results section, it has been argued that negative cosine similarity values between distributional semantic vectors are difficult to interpret (G&#x000fc;nther &#x00026; Marelli, <xref ref-type="bibr" rid="CR34">2016</xref>) and provide little semantic information (Rotaru et&#x000a0;al., <xref ref-type="bibr" rid="CR81">2018</xref>). Thus, we wanted to ensure that the conclusions that we drew from our analyses did not critically depend on negative cosine similarity values. We replicated both the analysis on the pervasiveness of iconicity in the auditory lexicon and the prediction of iconicity ratings after setting the negative cosine similarity values to zero. The choice of whether to consider the negative cosines had little impact on our results.</p><p id="Par67">Concerning our analysis on the pervasiveness of iconicity in the auditory lexicon, we were not able to find an effect of condition when employing the raw sound spectra (B = 0.0026, SE = 0.003, <italic>t</italic> = 0.877, <italic>p</italic> = 0.381, <italic>N</italic> = 2,308,063), but only when considering VGGish- (B = 0.0033, SE = 0.0015, <italic>t</italic> = 2.176, <italic>p</italic> = 0.0296, <italic>N</italic> = 2,308,063) and SpeechVGG-based (B = 0.0069, SE = 0.001, <italic>t</italic> = 6.834, <italic>p</italic>
<inline-formula id="IEq74"><alternatives><tex-math id="d33e3355">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ll $$\end{document}</tex-math><mml:math id="d33e3360"><mml:mo>&#x0226a;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq30.gif"/></alternatives></inline-formula> 0.0001, <italic>N</italic> = 2,308,063) sound representations.</p><p id="Par68">Even with negative cosine values set to zero, our estimates were significantly correlated with the human ratings in the two datasets considered in this study. The spectrum-based similarity scores were significantly correlated with both the ratings provided by Winter et&#x000a0;al. (<xref ref-type="bibr" rid="CR106">2017</xref>) (<italic>r</italic> = 0.2773, <italic>p</italic> = 0.0002, <italic>N</italic> = 175) and the ones released by Winter et&#x000a0;al. (<xref ref-type="bibr" rid="CR104">2023</xref>) (<italic>r</italic> = 0.2069, <italic>p</italic> = 0.001, <italic>N</italic> = 248). Similarly, the SpeechVGG-based iconicity estimates were correlated with the ratings obtained in the two rating datasets (<italic>r</italic> = 0.3256, <italic>p</italic>
<inline-formula id="IEq75"><alternatives><tex-math id="d33e3400">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ll $$\end{document}</tex-math><mml:math id="d33e3405"><mml:mo>&#x0226a;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq30.gif"/></alternatives></inline-formula> 0.0001, <italic>N</italic> = 175; <italic>r</italic> = 0.178, <italic>p</italic> = 0.0049, <italic>N</italic> = 248), and the same held for the VGGish-based scores (<italic>r</italic> = 0.1885, <italic>p</italic> = 0.0125, <italic>N</italic> = 175, and <italic>r</italic> = 0.1742, <italic>p</italic> = 0.0060, <italic>N</italic> = 248). Furthermore, the spectrum-based iconicity measure was correlated with both the similarity scores based on VGGish (<italic>r</italic> = 0.2078, <italic>p</italic>
<inline-formula id="IEq76"><alternatives><tex-math id="d33e3448">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ll $$\end{document}</tex-math><mml:math id="d33e3453"><mml:mo>&#x0226a;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq30.gif"/></alternatives></inline-formula> 0.0001, <italic>N</italic> = 367) and based on SpeechVGG (<italic>r</italic> = 0.4564, <italic>p</italic>
<inline-formula id="IEq77"><alternatives><tex-math id="d33e3466">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ll $$\end{document}</tex-math><mml:math id="d33e3471"><mml:mo>&#x0226a;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq30.gif"/></alternatives></inline-formula> 0.0001, <italic>N</italic> = 367), and the two measures we obtained from the two neural networks were significantly associated (<italic>r</italic> = 0.2466, <italic>p</italic>
<inline-formula id="IEq78"><alternatives><tex-math id="d33e3485">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ll $$\end{document}</tex-math><mml:math id="d33e3490"><mml:mo>&#x0226a;</mml:mo></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq30.gif"/></alternatives></inline-formula> 0.0001, <italic>N</italic> = 367). When controlling for the baseline of psycholinguistic covariates described in the Methods section, all three iconicity estimates were significantly associated with human responses across both rating datasets (see Table <xref rid="Tab3" ref-type="table">3</xref>), and, as in our main analyses, the best predictor of human ratings (in terms of <inline-formula id="IEq79"><alternatives><tex-math id="d33e3500">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e3505"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup>) was the SpeechVGG-based metric.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Results of the linear models predicting the human ratings from the data-driven iconicity estimates as predictors, after setting to zero the negative cosine similarity values</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left" colspan="5">Spectrum</th><th align="left" colspan="5">VGGish</th><th align="left" colspan="5">SpeechVGG</th></tr><tr><th align="left"/><th align="left">Measure</th><th align="left">B</th><th align="left">SE</th><th align="left"><italic>t</italic></th><th align="left"><italic>p</italic></th><th align="left"><inline-formula id="IEq80"><alternatives><tex-math id="d33e3565">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e3570"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup></th><th align="left">B</th><th align="left">SE</th><th align="left"><italic>t</italic></th><th align="left"><italic>p</italic></th><th align="left"><inline-formula id="IEq81"><alternatives><tex-math id="d33e3591">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e3596"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup></th><th align="left">B</th><th align="left">SE</th><th align="left"><italic>t</italic></th><th align="left"><italic>p</italic></th><th align="left"><inline-formula id="IEq82"><alternatives><tex-math id="d33e3616">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}</tex-math><mml:math id="d33e3621"><mml:mi mathvariant="normal">&#x00394;</mml:mi></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq48.gif"/></alternatives></inline-formula>R<sup>2</sup></th></tr></thead><tbody><tr><td align="left">Winter et al. (<xref ref-type="bibr" rid="CR106">2017</xref>)</td><td align="left">Markedness</td><td align="left">&#x02013;0.449</td><td align="left">1.582</td><td align="left">&#x02013;0.284</td><td align="left">0.777</td><td align="left">0.002</td><td align="left">&#x02013;3.005</td><td align="left">3.878</td><td align="left">&#x02013;0.775</td><td align="left">0.440</td><td align="left">0.003</td><td align="left">0.016</td><td align="left">1.702</td><td align="left">0.009</td><td align="left">0.993</td><td align="left">0.000</td></tr><tr><td align="left"/><td align="left">CD</td><td align="left">&#x02013;0.610</td><td align="left">0.291</td><td align="left">&#x02013;2.098</td><td align="left">0.038</td><td align="left">0.116</td><td align="left">&#x02013;0.625</td><td align="left">0.294</td><td align="left">&#x02013;2.124</td><td align="left">0.035</td><td align="left">0.121</td><td align="left">&#x02013;0.571</td><td align="left">0.285</td><td align="left">&#x02013;2.000</td><td align="left">0.047</td><td align="left">0.112</td></tr><tr><td align="left"/><td align="left">Auditory</td><td align="left">&#x02013;0.04</td><td align="left">0.247</td><td align="left">&#x02013;0.161</td><td align="left">0.872</td><td align="left">0.000</td><td align="left">&#x02013;0.019</td><td align="left">0.254</td><td align="left">&#x02013;0.075</td><td align="left">0.940</td><td align="left">0.001</td><td align="left">&#x02013;0.075</td><td align="left">0.242</td><td align="left">&#x02013;0.312</td><td align="left">0.756</td><td align="left">0.000</td></tr><tr><td align="left"/><td align="left">Conc.</td><td align="left">&#x02013;0.544</td><td align="left">0.146</td><td align="left">&#x02013;3.719</td><td align="left"><inline-formula id="IEq83"><alternatives><tex-math id="d33e3750">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e3755"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.058</td><td align="left">&#x02013;0.557</td><td align="left">0.15</td><td align="left">&#x02013;3.706</td><td align="left"><inline-formula id="IEq84"><alternatives><tex-math id="d33e3771">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e3776"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.058</td><td align="left">&#x02013;0.564</td><td align="left">0.141</td><td align="left">&#x02013;3.991</td><td align="left"><inline-formula id="IEq85"><alternatives><tex-math id="d33e3793">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e3798"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.061</td></tr><tr><td align="left"/><td align="left">Freq.</td><td align="left">0.063</td><td align="left">0.558</td><td align="left">0.113</td><td align="left">0.911</td><td align="left">0.108</td><td align="left">0.056</td><td align="left">0.566</td><td align="left">0.099</td><td align="left">0.921</td><td align="left">0.112</td><td align="left">0.015</td><td align="left">0.548</td><td align="left">0.028</td><td align="left">0.978</td><td align="left">0.105</td></tr><tr><td align="left"/><td align="left">AoA</td><td align="left">&#x02013;0.235</td><td align="left">0.072</td><td align="left">&#x02013;3.276</td><td align="left">0.001</td><td align="left">0.036</td><td align="left">&#x02013;0.230</td><td align="left">0.075</td><td align="left">&#x02013;3.075</td><td align="left">0.003</td><td align="left">0.035</td><td align="left">&#x02013;0.242</td><td align="left">0.070</td><td align="left">&#x02013;3.435</td><td align="left">0.001</td><td align="left">0.037</td></tr><tr><td align="left"/><td align="left">Length</td><td align="left">&#x02013;0.065</td><td align="left">0.057</td><td align="left">&#x02013;1.134</td><td align="left">0.259</td><td align="left">0.014</td><td align="left">&#x02013;0.051</td><td align="left">0.057</td><td align="left">&#x02013;0.897</td><td align="left">0.371</td><td align="left">0.012</td><td align="left">&#x02013;0.042</td><td align="left">0.056</td><td align="left">&#x02013;0.763</td><td align="left">0.447</td><td align="left">0.012</td></tr><tr><td align="left"/><td align="left"><inline-formula id="IEq86"><alternatives><tex-math id="d33e3915">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\underline{\textrm{Iconicity}}$$\end{document}</tex-math><mml:math id="d33e3920"><mml:munder><mml:mtext>Iconicity</mml:mtext><mml:mo>&#x0005f;</mml:mo></mml:munder></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq53.gif"/></alternatives></inline-formula></td><td align="left">1.032</td><td align="left">0.323</td><td align="left">3.193</td><td align="left">0.002</td><td align="left">0.071</td><td align="left">1.667</td><td align="left">0.794</td><td align="left">2.100</td><td align="left">0.037</td><td align="left">0.041</td><td align="left">3.965</td><td align="left">1.007</td><td align="left">3.939</td><td align="left"><inline-formula id="IEq87"><alternatives><tex-math id="d33e3954">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e3959"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.097</td></tr><tr><td align="left">Winter et al. (<xref ref-type="bibr" rid="CR105">2021</xref>)</td><td align="left">Markedness</td><td align="left">1.027</td><td align="left">0.896</td><td align="left">1.147</td><td align="left">0.253</td><td align="left">0.003</td><td align="left">&#x02013;0.655</td><td align="left">2.271</td><td align="left">&#x02013;0.288</td><td align="left">0.773</td><td align="left">0.000</td><td align="left">0.858</td><td align="left">1.161</td><td align="left">0.739</td><td align="left">0.461</td><td align="left">0.002</td></tr><tr><td align="left"/><td align="left">CD</td><td align="left">&#x02013;0.089</td><td align="left">0.233</td><td align="left">&#x02013;0.380</td><td align="left">0.704</td><td align="left">0.037</td><td align="left">&#x02013;0.124</td><td align="left">0.232</td><td align="left">&#x02013;0.533</td><td align="left">0.594</td><td align="left">0.040</td><td align="left">&#x02013;0.131</td><td align="left">0.231</td><td align="left">&#x02013;0.567</td><td align="left">0.572</td><td align="left">0.038</td></tr><tr><td align="left"/><td align="left">Auditory</td><td align="left">0.146</td><td align="left">0.175</td><td align="left">0.837</td><td align="left">0.404</td><td align="left">0.005</td><td align="left">0.141</td><td align="left">0.176</td><td align="left">0.806</td><td align="left">0.421</td><td align="left">0.005</td><td align="left">0.102</td><td align="left">0.174</td><td align="left">0.589</td><td align="left">0.557</td><td align="left">0.004</td></tr><tr><td align="left"/><td align="left">Conc.</td><td align="left">&#x02013;0.117</td><td align="left">0.089</td><td align="left">&#x02013;1.313</td><td align="left">0.190</td><td align="left">0.004</td><td align="left">&#x02013;0.095</td><td align="left">0.092</td><td align="left">&#x02013;1.032</td><td align="left">0.303</td><td align="left">0.004</td><td align="left">&#x02013;0.081</td><td align="left">0.090</td><td align="left">&#x02013;0.899</td><td align="left">0.369</td><td align="left">0.004</td></tr><tr><td align="left"/><td align="left">Freq.</td><td align="left">&#x02013;0.478</td><td align="left">0.457</td><td align="left">&#x02013;1.046</td><td align="left">0.297</td><td align="left">0.040</td><td align="left">&#x02013;0.453</td><td align="left">0.457</td><td align="left">&#x02013;0.991</td><td align="left">0.323</td><td align="left">0.043</td><td align="left">&#x02013;0.418</td><td align="left">0.454</td><td align="left">&#x02013;0.920</td><td align="left">0.359</td><td align="left">0.041</td></tr><tr><td align="left"/><td align="left">AoA</td><td align="left">&#x02013;0.176</td><td align="left">0.042</td><td align="left">&#x02013;4.222</td><td align="left"><inline-formula id="IEq88"><alternatives><tex-math id="d33e4160">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e4165"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.064</td><td align="left">&#x02013;0.18</td><td align="left">0.042</td><td align="left">&#x02013;4.295</td><td align="left"><inline-formula id="IEq89"><alternatives><tex-math id="d33e4181">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e4186"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.065</td><td align="left">&#x02013;0.18</td><td align="left">0.041</td><td align="left">&#x02013;4.361</td><td align="left"><inline-formula id="IEq90"><alternatives><tex-math id="d33e4203">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e4208"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.067</td></tr><tr><td align="left"/><td align="left">Length</td><td align="left">&#x02013;0.163</td><td align="left">0.036</td><td align="left">&#x02013;4.594</td><td align="left"><inline-formula id="IEq91"><alternatives><tex-math id="d33e4228">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e4233"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.085</td><td align="left">&#x02013;0.143</td><td align="left">0.034</td><td align="left">&#x02013;4.177</td><td align="left"><inline-formula id="IEq92"><alternatives><tex-math id="d33e4249">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e4254"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.079</td><td align="left">&#x02013;0.157</td><td align="left">0.034</td><td align="left">&#x02013;4.573</td><td align="left"><inline-formula id="IEq93"><alternatives><tex-math id="d33e4271">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c;.001$$\end{document}</tex-math><mml:math id="d33e4276"><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mo>.</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left">0.085</td></tr><tr><td align="left"/><td align="left"><inline-formula id="IEq94"><alternatives><tex-math id="d33e4288">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\underline{\textrm{Iconicity}}$$\end{document}</tex-math><mml:math id="d33e4293"><mml:munder><mml:mtext>Iconicity</mml:mtext><mml:mo>&#x0005f;</mml:mo></mml:munder></mml:math><inline-graphic xlink:href="13423_2024_2630_Article_IEq53.gif"/></alternatives></inline-formula></td><td align="left">0.542</td><td align="left">0.231</td><td align="left">2.350</td><td align="left">0.020</td><td align="left">0.032</td><td align="left">1.030</td><td align="left">0.471</td><td align="left">2.185</td><td align="left">0.030</td><td align="left">0.028</td><td align="left">1.926</td><td align="left">0.687</td><td align="left">2.802</td><td align="left">0.006</td><td align="left">0.035</td></tr></tbody></table></table-wrap></p></sec></app></app-group><fn-group><fn id="Fn1"><label>1</label><p id="Par21">The final fully-connected and output layers of the SpeechVGG model can be altered based on the specific application for which the model is being used, such as language identification or speech inpainting.</p></fn><fn id="Fn2"><label>2</label><p id="Par24">Accessed at <ext-link ext-link-type="uri" xlink:href="https://freesound.org/">https://freesound.org/</ext-link>; for the scraping, we adapted the scripts available at <ext-link ext-link-type="uri" xlink:href="https://github.com/cchyung/freesound-scraper">https://github.com/cchyung/freesound-scraper</ext-link></p></fn><fn id="Fn3"><label>3</label><p id="Par26">We thank an anonymous reviewer for bringing this issue to our attention.</p></fn><fn id="Fn4"><label>4</label><p id="Par32">We thank an anonymous reviewer for bringing to our attention this possible confound.</p></fn><fn id="Fn5"><label>5</label><p id="Par34">We thank Gary Lupyan for suggesting this analysis.</p></fn><fn id="Fn6"><label>6</label><p id="Par38"><ext-link ext-link-type="uri" xlink:href="https://github.com/Andrea-de-Varda/iconicity-datadriven">https://github.com/Andrea-de-Varda/iconicity-datadriven</ext-link></p></fn><fn id="Fn7"><label>7</label><p id="Par39"><ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/vggish/1">https://tfhub.dev/google/vggish/1</ext-link></p></fn><fn id="Fn8"><label>8</label><p id="Par40"><ext-link ext-link-type="uri" xlink:href="https://github.com/bepierre/SpeechVGG/blob/master/examples/speech_music_noise/extract_and_classify.ipynb">https://github.com/bepierre/SpeechVGG/blob/master/examples/speech_music_noise/extract_and_classify.ipynb</ext-link></p></fn><fn id="Fn9"><label>9</label><p id="Par43">We could not employ more common dispersion measures such as the standard deviation since distances in high-dimensional spaces are usually measured with the cosine similarity. This measure is defined in the range (&#x02013;1, 1), and there is a meaningful difference between a positive and a negative cosine similarity between two vectors, which would be lost by squaring.</p></fn><fn id="Fn10"><label>10</label><p id="Par52">Note that while processing natural sounds with a speech network (and vice versa) might increase the average similarity between speech and natural sounds with respect to employing the raw sound spectra, our results do not depend on these similarities alone, but on the similarities of matching versus non-matching natural and word sounds. We thank an anonymous reviewer for giving us the opportunity to clarify this point.</p></fn><fn id="Fn11"><label>11</label><p id="Par56">We thank an anonymous reviewer for raising this problem and suggesting the relevant example that follows.</p></fn><fn><p><bold>Publisher's Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>We thank Giovanni Cassani for his valuable suggestions and feedback during the design of the experiments.</p></ack><notes notes-type="funding-information"><title>Funding</title><p>Open access funding provided by Universit&#x000e0; degli Studi di Milano - Bicocca within the CRUI-CARE Agreement. The contribution of MM was supported by the European Union (ERC-COG-2022, BraveNewWord, 101087053). Views and opinions expressed are, however, those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.</p></notes><notes notes-type="data-availability"><title>Availability of data and materials</title><p>Due to copyright constraints, we release aggregated sound vectors for the natural sounds, since the various recordings are protected by different restrictions.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>All code is publicly available(<ext-link ext-link-type="uri" xlink:href="https://github.com/Andrea-de-Varda/iconicity-datadriven">https://github.com/Andrea-de-Varda/iconicity-datadriven</ext-link>). The code for extracting the embeddings was adapted from the one available on TensorFlow Hub for VGGish(<ext-link ext-link-type="uri" xlink:href="https://tfhub.dev/google/vggish/1">https://tfhub.dev/google/vggish/1</ext-link>) and from GitHub for SpeechVGG(<ext-link ext-link-type="uri" xlink:href="https://github.com/bepierre/SpeechVGG/blob/master/examples/speech_music_noise/extract_and_classify.ipynb">https://github.com/bepierre/SpeechVGG/blob/master/examples/speech_music_noise/extract_and_classify.ipynb</ext-link>).</p></notes><notes><title>Declarations</title><notes id="FPar1"><title>Ethics Approval</title><p id="Par62">Not applicable; the study did not involve human participants.</p></notes><notes id="FPar2"><title>Consent to Participate</title><p id="Par63">Not applicable; the study did not involve human participants.</p></notes><notes id="FPar3"><title>Consent for Publication</title><p id="Par64">All authors provide their consent for the publication of this manuscript.</p></notes><notes id="FPar4" notes-type="COI-statement"><title>Conflict of Interest</title><p id="Par65">The authors declare no conflict of interest.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><mixed-citation publication-type="other">Abramova, E., &#x00026; Fern&#x000e1;ndez, R. (2016). Questioning arbitrariness in language: a data-driven study of conventional iconicity. <italic>Proceedings of the 2016 conference of the north American chapter of the association for computational linguistics: Human language technologies</italic> (pp. 343&#x02013;352). San Diego, California: Association for Computational Linguistics.</mixed-citation></ref><ref id="CR2"><mixed-citation publication-type="other">Abramova, E., Fern&#x000e1;ndez, R., &#x00026; Sangati, F. (2013). Automatic labeling of phonesthemic senses.</mixed-citation></ref><ref id="CR3"><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>J</given-names></name></person-group><article-title>Short term spectral analysis, synthesis, and modification by discrete fourier transform</article-title><source>IEEE Transactions on Acoustics, Speech, and Signal Processing</source><year>1977</year><volume>25</volume><issue>3</issue><fpage>235</fpage><lpage>238</lpage></element-citation><mixed-citation id="mc-CR3" publication-type="journal">Allen, J. (1977). Short term spectral analysis, synthesis, and modification by discrete fourier transform. <italic>IEEE Transactions on Acoustics, Speech, and Signal Processing,</italic><italic>25</italic>(3), 235&#x02013;238.</mixed-citation></citation-alternatives></ref><ref id="CR4"><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Asano</surname><given-names>M</given-names></name><name><surname>Imai</surname><given-names>M</given-names></name><name><surname>Kita</surname><given-names>S</given-names></name><name><surname>Kitajo</surname><given-names>K</given-names></name><name><surname>Okada</surname><given-names>H</given-names></name><name><surname>Thierry</surname><given-names>G</given-names></name></person-group><article-title>Sound symbolism scaffolds language development in preverbal infants</article-title><source>Cortex</source><year>2015</year><volume>63</volume><fpage>196</fpage><lpage>205</lpage><pub-id pub-id-type="pmid">25282057</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Asano, M., Imai, M., Kita, S., Kitajo, K., Okada, H., &#x00026; Thierry, G. (2015). Sound symbolism scaffolds language development in preverbal infants. <italic>Cortex,</italic><italic>63</italic>, 196&#x02013;205.<pub-id pub-id-type="pmid">25282057</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Assaneo</surname><given-names>MF</given-names></name><name><surname>Nichols</surname><given-names>JI</given-names></name><name><surname>Trevisan</surname><given-names>MA</given-names></name></person-group><article-title>The anatomy of onomatopoeia</article-title><source>PloS One</source><year>2011</year><volume>6</volume><issue>12</issue><fpage>e28317</fpage><pub-id pub-id-type="pmid">22194825</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Assaneo, M. F., Nichols, J. I., &#x00026; Trevisan, M. A. (2011). The anatomy of onomatopoeia. <italic>PloS One,</italic><italic>6</italic>(12), e28317.<pub-id pub-id-type="pmid">22194825</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Barr</surname><given-names>DJ</given-names></name><name><surname>Levy</surname><given-names>R</given-names></name><name><surname>Scheepers</surname><given-names>C</given-names></name><name><surname>Tily</surname><given-names>HJ</given-names></name></person-group><article-title>Random effects structure for confirmatory hypothesis testing: Keep it maximal</article-title><source>Journal of memory and language</source><year>2013</year><volume>68</volume><issue>3</issue><fpage>255</fpage><lpage>278</lpage></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Barr, D. J., Levy, R., Scheepers, C., &#x00026; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. <italic>Journal of memory and language,</italic><italic>68</italic>(3), 255&#x02013;278.</mixed-citation></citation-alternatives></ref><ref id="CR7"><mixed-citation publication-type="other">Beckmann, P., Kegler, M., Saltini, H., &#x00026; Cernak, M. (2019). Speech-vgg: A deep feature extractor for speech processing. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1910.09909">arXiv:1910.09909</ext-link></mixed-citation></ref><ref id="CR8"><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Blasi</surname><given-names>DE</given-names></name><name><surname>Wichmann</surname><given-names>S</given-names></name><name><surname>Hammarstr&#x000f6;m</surname><given-names>H</given-names></name><name><surname>Stadler</surname><given-names>PF</given-names></name><name><surname>Christiansen</surname><given-names>MH</given-names></name></person-group><article-title>Sound-meaning association biases evidenced across thousands of languages</article-title><source>Proceedings of the National Academy of Sciences</source><year>2016</year><volume>113</volume><issue>39</issue><fpage>10818</fpage><lpage>10823</lpage><pub-id pub-id-type="doi">10.1073/pnas.1605782113</pub-id></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Blasi, D. E., Wichmann, S., Hammarstr&#x000f6;m, H., Stadler, P. F., &#x00026; Christiansen, M. H. (2016). Sound-meaning association biases evidenced across thousands of languages. <italic>Proceedings of the National Academy of Sciences,</italic><italic>113</italic>(39), 10818&#x02013;10823. 10.1073/pnas.1605782113</mixed-citation></citation-alternatives></ref><ref id="CR9"><mixed-citation publication-type="other">Bloomfield, L. (1994). Language. Motilal Banarsidass Publ. (Google-Books-ID: iqg7hUcRzPIC).</mixed-citation></ref><ref id="CR10"><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Bredin</surname><given-names>H</given-names></name></person-group><article-title>Onomatopoeia as a figure and a linguistic principle</article-title><source>New Literary History</source><year>1996</year><volume>27</volume><issue>3</issue><fpage>555</fpage><lpage>569</lpage></element-citation><mixed-citation id="mc-CR10" publication-type="journal">Bredin, H. (1996). Onomatopoeia as a figure and a linguistic principle. <italic>New Literary History,</italic><italic>27</italic>(3), 555&#x02013;569.</mixed-citation></citation-alternatives></ref><ref id="CR11"><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Brysbaert</surname><given-names>M</given-names></name><name><surname>New</surname><given-names>B</given-names></name></person-group><article-title>Moving beyond ku&#x0010d;era and francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for american english</article-title><source>Behavior research methods</source><year>2009</year><volume>41</volume><issue>4</issue><fpage>977</fpage><lpage>990</lpage><pub-id pub-id-type="pmid">19897807</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Brysbaert, M., &#x00026; New, B. (2009). Moving beyond ku&#x0010d;era and francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for american english. <italic>Behavior research methods,</italic><italic>41</italic>(4), 977&#x02013;990.<pub-id pub-id-type="pmid">19897807</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Brysbaert</surname><given-names>M</given-names></name><name><surname>Warriner</surname><given-names>AB</given-names></name><name><surname>Kuperman</surname><given-names>V</given-names></name></person-group><article-title>Concreteness ratings for 40 thousand generally known English word lemmas</article-title><source>Behavior Research Methods</source><year>2014</year><volume>46</volume><issue>3</issue><fpage>904</fpage><lpage>911</lpage><pub-id pub-id-type="pmid">24142837</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Brysbaert, M., Warriner, A. B., &#x00026; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. <italic>Behavior Research Methods,</italic><italic>46</italic>(3), 904&#x02013;911.<pub-id pub-id-type="pmid">24142837</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Cabrera</surname><given-names>JCM</given-names></name></person-group><article-title>The role of sound symbolism in protolanguage: Some linguistic and archaeological speculations</article-title><source>Theoria et Historia Scientiarum</source><year>2012</year><volume>9</volume><fpage>115</fpage><lpage>130</lpage></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Cabrera, J. C. M. (2012). The role of sound symbolism in protolanguage: Some linguistic and archaeological speculations. <italic>Theoria et Historia Scientiarum,</italic><italic>9</italic>, 115&#x02013;130.</mixed-citation></citation-alternatives></ref><ref id="CR14"><mixed-citation publication-type="other">Cnudde, K., Sidhu, D., &#x00026; Pexman, P. M. (2020). The role of phonology in iconicity effects: Evidence from individual differences.</mixed-citation></ref><ref id="CR15"><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J</given-names></name></person-group><article-title>A coefficient of agreement for nominal scales</article-title><source>Educational and Psychological Measurement</source><year>1960</year><volume>20</volume><issue>1</issue><fpage>37</fpage><lpage>46</lpage></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Cohen, J. (1960). A coefficient of agreement for nominal scales. <italic>Educational and Psychological Measurement,</italic><italic>20</italic>(1), 37&#x02013;46.</mixed-citation></citation-alternatives></ref><ref id="CR16"><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Dautriche</surname><given-names>I</given-names></name><name><surname>Mahowald</surname><given-names>K</given-names></name><name><surname>Gibson</surname><given-names>E</given-names></name><name><surname>Piantadosi</surname><given-names>ST</given-names></name></person-group><article-title>Wordform similarity increases with semantic similarity: An analysis of 100 languages</article-title><source>Cognitive Science</source><year>2017</year><volume>41</volume><issue>8</issue><fpage>2149</fpage><lpage>2169</lpage><pub-id pub-id-type="doi">10.1111/cogs.12453</pub-id><pub-id pub-id-type="pmid">27862241</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Dautriche, I., Mahowald, K., Gibson, E., &#x00026; Piantadosi, S. T. (2017). Wordform similarity increases with semantic similarity: An analysis of 100 languages. <italic>Cognitive Science,</italic><italic>41</italic>(8), 2149&#x02013;2169. 10.1111/cogs.12453<pub-id pub-id-type="pmid">27862241</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><citation-alternatives><element-citation id="ec-CR17" publication-type="book"><person-group person-group-type="author"><name><surname>Davison</surname><given-names>AC</given-names></name><name><surname>Hinkley</surname><given-names>DV</given-names></name></person-group><source>Bootstrap methods and their application</source><year>1997</year><publisher-name>Cambridge University Press</publisher-name></element-citation><mixed-citation id="mc-CR17" publication-type="book">Davison, A. C., &#x00026; Hinkley, D. V. (1997). <italic>Bootstrap methods and their application</italic>. Cambridge University Press.</mixed-citation></citation-alternatives></ref><ref id="CR18"><mixed-citation publication-type="other">de Varda, A. G., &#x00026; Strapparava, C. (2021). A layered bridge from sound to meaning: Investigating cross-linguistic phonosemantic correspondences. <italic>Proceedings of the annual meeting of the cognitive science society</italic> (Vol. 43).</mixed-citation></ref><ref id="CR19"><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>de Varda</surname><given-names>AG</given-names></name><name><surname>Strapparava</surname><given-names>C</given-names></name></person-group><article-title>A cross-modal and cross-lingual study of iconicity in language: Insights from deep learning</article-title><source>Cognitive Science</source><year>2022</year><volume>46</volume><issue>6</issue><fpage>e13147</fpage><pub-id pub-id-type="pmid">35665953</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">de Varda, A. G., &#x00026; Strapparava, C. (2022). A cross-modal and cross-lingual study of iconicity in language: Insights from deep learning. <italic>Cognitive Science,</italic><italic>46</italic>(6), e13147.<pub-id pub-id-type="pmid">35665953</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Dingemanse</surname><given-names>M</given-names></name></person-group><article-title>Advances in the cross-linguistic study of ideophones</article-title><source>Language and Linguistics Compass</source><year>2012</year><volume>6</volume><issue>10</issue><fpage>654</fpage><lpage>672</lpage></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Dingemanse, M. (2012). Advances in the cross-linguistic study of ideophones. <italic>Language and Linguistics Compass,</italic><italic>6</italic>(10), 654&#x02013;672.</mixed-citation></citation-alternatives></ref><ref id="CR21"><mixed-citation publication-type="other">Dingemanse, M. (2021). Ideophones (oxford handbook of word classes). 10.31234/osf.io/u96zt.</mixed-citation></ref><ref id="CR22"><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Dingemanse</surname><given-names>M</given-names></name><name><surname>Blasi</surname><given-names>D</given-names></name><name><surname>Lupyan</surname><given-names>G</given-names></name><name><surname>Christiansen</surname><given-names>M</given-names></name><name><surname>Monaghan</surname><given-names>P</given-names></name></person-group><article-title>Arbitrariness, iconicity, and systematicity in language</article-title><source>Trends in Cognitive Sciences</source><year>2015</year><volume>19</volume><fpage>603</fpage><lpage>615</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.07.013</pub-id><pub-id pub-id-type="pmid">26412098</pub-id>
</element-citation><mixed-citation id="mc-CR22" publication-type="journal">Dingemanse, M., Blasi, D., Lupyan, G., Christiansen, M., &#x00026; Monaghan, P. (2015). Arbitrariness, iconicity, and systematicity in language. <italic>Trends in Cognitive Sciences,</italic><italic>19</italic>, 603&#x02013;615. 10.1016/j.tics.2015.07.013<pub-id pub-id-type="pmid">26412098</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR23"><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Dingemanse</surname><given-names>M</given-names></name><name><surname>Schuerman</surname><given-names>W</given-names></name><name><surname>Reinisch</surname><given-names>E</given-names></name><name><surname>Tufvesson</surname><given-names>S</given-names></name><name><surname>Mitterer</surname><given-names>H</given-names></name></person-group><article-title>What sound symbolism can and cannot do: Testing the iconicity of ideophones from five languages</article-title><source>Language</source><year>2016</year><volume>92</volume><issue>2</issue><fpage>e117</fpage><lpage>e133</lpage></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Dingemanse, M., Schuerman, W., Reinisch, E., Tufvesson, S., &#x00026; Mitterer, H. (2016). What sound symbolism can and cannot do: Testing the iconicity of ideophones from five languages. <italic>Language,</italic><italic>92</italic>(2), e117&#x02013;e133.</mixed-citation></citation-alternatives></ref><ref id="CR24"><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Dingemanse</surname><given-names>M</given-names></name><name><surname>Thompson</surname><given-names>B</given-names></name></person-group><article-title>Playful iconicity: Structural markedness underlies the relation between funniness and iconicity</article-title><source>Language and Cognition</source><year>2020</year><volume>12</volume><issue>1</issue><fpage>203</fpage><lpage>224</lpage></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Dingemanse, M., &#x00026; Thompson, B. (2020). Playful iconicity: Structural markedness underlies the relation between funniness and iconicity. <italic>Language and Cognition,</italic><italic>12</italic>(1), 203&#x02013;224.</mixed-citation></citation-alternatives></ref><ref id="CR25"><mixed-citation publication-type="other">Dingemanse, M., Torreira, F., &#x00026; Enfield, N. J. (2013). Is Huh? a universal word? Conversational infrastructure and the convergent evolution of linguistic items. <italic>PLoS ONE,</italic><italic>8</italic>(11). 10.1371/journal.pone.0078273</mixed-citation></ref><ref id="CR26"><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Edmiston</surname><given-names>P</given-names></name><name><surname>Lupyan</surname><given-names>G</given-names></name></person-group><article-title>What makes words special? words as unmotivated cues</article-title><source>Cognition</source><year>2015</year><volume>143</volume><fpage>93</fpage><lpage>100</lpage><pub-id pub-id-type="pmid">26117488</pub-id>
</element-citation><mixed-citation id="mc-CR26" publication-type="journal">Edmiston, P., &#x00026; Lupyan, G. (2015). What makes words special? words as unmotivated cues. <italic>Cognition,</italic><italic>143</italic>, 93&#x02013;100.<pub-id pub-id-type="pmid">26117488</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR27"><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Edmiston</surname><given-names>P</given-names></name><name><surname>Perlman</surname><given-names>M</given-names></name><name><surname>Lupyan</surname><given-names>G</given-names></name></person-group><article-title>Repeated imitation makes human vocalizations more word-like</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><year>2018</year><volume>285</volume><issue>1874</issue><fpage>20172709</fpage></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Edmiston, P., Perlman, M., &#x00026; Lupyan, G. (2018). Repeated imitation makes human vocalizations more word-like. <italic>Proceedings of the Royal Society B: Biological Sciences,</italic><italic>285</italic>(1874), 20172709.</mixed-citation></citation-alternatives></ref><ref id="CR28"><mixed-citation publication-type="other">Firth, J. R. (1964). On sociological linguistics. <italic>Language in culture and society</italic>, 66&#x02013;70.</mixed-citation></ref><ref id="CR29"><mixed-citation publication-type="other">Fontana, F. (2013). Association of haptic trajectories to takete and maluma. <italic>International workshop on haptic and audio interaction design </italic>(pp. 60&#x02013;68).</mixed-citation></ref><ref id="CR30"><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Fryer</surname><given-names>L</given-names></name><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Pring</surname><given-names>L</given-names></name></person-group><article-title>Touching words is not enough: How visual experience influences haptic-auditory associations in the bouba-kiki effect</article-title><source>Cognition</source><year>2014</year><volume>132</volume><issue>2</issue><fpage>164</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2014.03.015</pub-id><pub-id pub-id-type="pmid">24809744</pub-id>
</element-citation><mixed-citation id="mc-CR30" publication-type="journal">Fryer, L., Freeman, J., &#x00026; Pring, L. (2014). Touching words is not enough: How visual experience influences haptic-auditory associations in the bouba-kiki effect. <italic>Cognition,</italic><italic>132</italic>(2), 164&#x02013;173. 10.1016/j.cognition.2014.03.015<pub-id pub-id-type="pmid">24809744</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR31"><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Gallace</surname><given-names>A</given-names></name><name><surname>Boschin</surname><given-names>E</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name></person-group><article-title>On the taste of bouba and kiki: An exploration of word-food associations in neurologically normal participants</article-title><source>Cognitive neuroscience</source><year>2011</year><volume>2</volume><fpage>34</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1080/17588928.2010.516820</pub-id><pub-id pub-id-type="pmid">24168422</pub-id>
</element-citation><mixed-citation id="mc-CR31" publication-type="journal">Gallace, A., Boschin, E., &#x00026; Spence, C. (2011). On the taste of bouba and kiki: An exploration of word-food associations in neurologically normal participants. <italic>Cognitive neuroscience,</italic><italic>2</italic>, 34&#x02013;46. 10.1080/17588928.2010.516820<pub-id pub-id-type="pmid">24168422</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR32"><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Graven</surname><given-names>T</given-names></name><name><surname>Desebrock</surname><given-names>C</given-names></name></person-group><article-title>Bouba or kiki with and without vision: Shape-audio regularities and mental images</article-title><source>Acta Psychologica</source><year>2018</year><volume>188</volume><fpage>200</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2018.05.011</pub-id><pub-id pub-id-type="pmid">29982038</pub-id>
</element-citation><mixed-citation id="mc-CR32" publication-type="journal">Graven, T., &#x00026; Desebrock, C. (2018). Bouba or kiki with and without vision: Shape-audio regularities and mental images. <italic>Acta Psychologica,</italic><italic>188</italic>, 200&#x02013;212. 10.1016/j.actpsy.2018.05.011<pub-id pub-id-type="pmid">29982038</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR33"><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Gr&#x000f6;mping</surname><given-names>U</given-names></name></person-group><article-title>Relative importance for linear regression in r: the package relaimpo</article-title><source>Journal of Statistical Software</source><year>2007</year><volume>17</volume><fpage>1</fpage><lpage>27</lpage></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Gr&#x000f6;mping, U. (2007). Relative importance for linear regression in r: the package relaimpo. <italic>Journal of Statistical Software,</italic><italic>17</italic>, 1&#x02013;27.</mixed-citation></citation-alternatives></ref><ref id="CR34"><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>G&#x000fc;nther</surname><given-names>F</given-names></name><name><surname>Marelli</surname><given-names>M</given-names></name></person-group><article-title>Understanding karma police: The perceived plausibility of noun compounds as predicted by distributional models of semantic representation</article-title><source>PloS One</source><year>2016</year><volume>11</volume><issue>10</issue><fpage>e0163200</fpage><pub-id pub-id-type="pmid">27732599</pub-id>
</element-citation><mixed-citation id="mc-CR34" publication-type="journal">G&#x000fc;nther, F., &#x00026; Marelli, M. (2016). Understanding karma police: The perceived plausibility of noun compounds as predicted by distributional models of semantic representation. <italic>PloS One,</italic><italic>11</italic>(10), e0163200.<pub-id pub-id-type="pmid">27732599</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR35"><mixed-citation publication-type="other">G&#x000fc;nther, F., Marelli, M., Tureski, S., &#x00026; Petilli, M. A. (2021). Vispa (vision spaces): A computer-vision-based representation system for individual images and concept prototypes, with large-scale evaluation.</mixed-citation></ref><ref id="CR36"><mixed-citation publication-type="other">G&#x000fc;nther, F., Petilli, M. A., Vergallito, A., &#x00026; Marelli, M. (2020). Images of the unseen: Extrapolating visual representations for abstract and concrete words in a data-driven computational model. <italic>Psychological Research,</italic> 1&#x02013;21.</mixed-citation></ref><ref id="CR37"><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name><surname>G&#x000fc;nther</surname><given-names>F</given-names></name><name><surname>Rinaldi</surname><given-names>L</given-names></name><name><surname>Marelli</surname><given-names>M</given-names></name></person-group><article-title>Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions</article-title><source>Perspectives on Psychological Science</source><year>2019</year><volume>14</volume><issue>6</issue><fpage>1006</fpage><lpage>1033</lpage><pub-id pub-id-type="pmid">31505121</pub-id>
</element-citation><mixed-citation id="mc-CR37" publication-type="journal">G&#x000fc;nther, F., Rinaldi, L., &#x00026; Marelli, M. (2019). Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions. <italic>Perspectives on Psychological Science,</italic><italic>14</italic>(6), 1006&#x02013;1033.<pub-id pub-id-type="pmid">31505121</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR38"><mixed-citation publication-type="other">Guti&#x000e9;rrez, E. D., Levy, R., &#x00026; Bergen, B. (2016). Finding non-arbitrary form-meaning systematicity using string-metric learning for kernel regression. In <italic>Proceedings of the 54th annual meeting of the association for computational linguistics</italic> (volume 1: Long papers) (pp. 2379&#x02013;2388). Berlin, Germany: Association for Computational Linguistics.</mixed-citation></ref><ref id="CR39"><mixed-citation publication-type="other">Hershey, S., Chaudhuri, S., Ellis, D. P. W., Gemmeke, J. F., Jansen, A., Moore, R. C., &#x00026; Wilson, K. (2016). Cnn architectures for large-scale audio classification. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.09430">https://arxiv.org/abs/1609.09430</ext-link>.</mixed-citation></ref><ref id="CR40"><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Hinojosa</surname><given-names>JA</given-names></name><name><surname>Haro</surname><given-names>J</given-names></name><name><surname>Magallares</surname><given-names>S</given-names></name><name><surname>Du&#x000f1;abeitia</surname><given-names>JA</given-names></name><name><surname>Ferr&#x000e9;</surname><given-names>P</given-names></name></person-group><article-title>Iconicity ratings for 10,995 Spanish words and their relationship with psycholinguistic variables</article-title><source>Behavior Research Methods</source><year>2021</year><volume>53</volume><issue>3</issue><fpage>1262</fpage><lpage>1275</lpage><pub-id pub-id-type="pmid">33037603</pub-id>
</element-citation><mixed-citation id="mc-CR40" publication-type="journal">Hinojosa, J. A., Haro, J., Magallares, S., Du&#x000f1;abeitia, J. A., &#x00026; Ferr&#x000e9;, P. (2021). Iconicity ratings for 10,995 Spanish words and their relationship with psycholinguistic variables. <italic>Behavior Research Methods,</italic><italic>53</italic>(3), 1262&#x02013;1275.<pub-id pub-id-type="pmid">33037603</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR41"><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name><surname>Hockett</surname><given-names>CF</given-names></name></person-group><article-title>The origin of speech</article-title><source>Scientific American</source><year>1960</year><volume>203</volume><issue>3</issue><fpage>88</fpage><lpage>97</lpage><pub-id pub-id-type="pmid">13683472</pub-id>
</element-citation><mixed-citation id="mc-CR41" publication-type="journal">Hockett, C. F. (1960). The origin of speech. <italic>Scientific American,</italic><italic>203</italic>(3), 88&#x02013;97.<pub-id pub-id-type="pmid">13683472</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR42"><mixed-citation publication-type="other">Huang, N., Slaney, M., &#x00026; Elhilali, M. (2018). Connecting deep neural networks to physical, perceptual, and electrophysiological auditory signals. <italic>Frontiers in Neuroscience,</italic><italic>12</italic>. 10.3389/fnins.2018.00532</mixed-citation></ref><ref id="CR43"><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Imai</surname><given-names>M</given-names></name><name><surname>Kita</surname><given-names>S</given-names></name><name><surname>Nagumo</surname><given-names>M</given-names></name><name><surname>Okada</surname><given-names>H</given-names></name></person-group><article-title>Sound symbolism facilitates early verb learning</article-title><source>Cognition</source><year>2008</year><volume>109</volume><issue>1</issue><fpage>54</fpage><lpage>65</lpage><pub-id pub-id-type="pmid">18835600</pub-id>
</element-citation><mixed-citation id="mc-CR43" publication-type="journal">Imai, M., Kita, S., Nagumo, M., &#x00026; Okada, H. (2008). Sound symbolism facilitates early verb learning. <italic>Cognition,</italic><italic>109</italic>(1), 54&#x02013;65.<pub-id pub-id-type="pmid">18835600</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR44"><mixed-citation publication-type="other">Iqbal, H. (2020). Plotneuralnet.</mixed-citation></ref><ref id="CR45"><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Joo</surname><given-names>I</given-names></name></person-group><article-title>Phonosemantic biases found in leipzig-jakarta lists of 66 languages</article-title><source>Linguistic Typology</source><year>2020</year><volume>24</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1515/lingty-2019-0030</pub-id></element-citation><mixed-citation id="mc-CR45" publication-type="journal">Joo, I. (2020). Phonosemantic biases found in leipzig-jakarta lists of 66 languages. <italic>Linguistic Typology,</italic><italic>24</italic>, 1&#x02013;12. 10.1515/lingty-2019-0030</mixed-citation></citation-alternatives></ref><ref id="CR46"><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJ</given-names></name><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>Shook</surname><given-names>EN</given-names></name><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><article-title>A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title><source>Neuron</source><year>2018</year><volume>98</volume><issue>3</issue><fpage>630</fpage><lpage>644</lpage><pub-id pub-id-type="pmid">29681533</pub-id>
</element-citation><mixed-citation id="mc-CR46" publication-type="journal">Kell, A. J., Yamins, D. L., Shook, E. N., Norman-Haignere, S. V., &#x00026; McDermott, J. H. (2018). A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy. <italic>Neuron,</italic><italic>98</italic>(3), 630&#x02013;644.<pub-id pub-id-type="pmid">29681533</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR47"><citation-alternatives><element-citation id="ec-CR47" publication-type="book"><person-group person-group-type="author"><name><surname>K&#x000f6;hler</surname><given-names>W</given-names></name></person-group><source>Gestalt psychology</source><year>1929</year><publisher-loc>Oxford, England</publisher-loc><publisher-name>Liveright</publisher-name></element-citation><mixed-citation id="mc-CR47" publication-type="book">K&#x000f6;hler, W. (1929). <italic>Gestalt psychology</italic>. Oxford, England: Liveright.</mixed-citation></citation-alternatives></ref><ref id="CR48"><mixed-citation publication-type="other">K&#x000f6;hler, W. (1947). Gestalt psychology: An introduction to new concepts in modern psychology.</mixed-citation></ref><ref id="CR49"><mixed-citation publication-type="other">Krizhevsky, A., Sutskever, I., &#x00026; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. <italic>Advances in Neural Information Processing Systems,</italic><italic>25</italic>.</mixed-citation></ref><ref id="CR50"><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Kuperman</surname><given-names>V</given-names></name><name><surname>Stadthagen-Gonzalez</surname><given-names>H</given-names></name><name><surname>Brysbaert</surname><given-names>M</given-names></name></person-group><article-title>Age-of-acquisition ratings for 30,000 english words</article-title><source>Behavior research methods</source><year>2012</year><volume>44</volume><issue>4</issue><fpage>978</fpage><lpage>990</lpage><pub-id pub-id-type="pmid">22581493</pub-id>
</element-citation><mixed-citation id="mc-CR50" publication-type="journal">Kuperman, V., Stadthagen-Gonzalez, H., &#x00026; Brysbaert, M. (2012). Age-of-acquisition ratings for 30,000 english words. <italic>Behavior research methods,</italic><italic>44</italic>(4), 978&#x02013;990.<pub-id pub-id-type="pmid">22581493</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR51"><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name><surname>Laing</surname><given-names>C</given-names></name></person-group><article-title>A phonological analysis of onomatopoeia in early word production</article-title><source>First Language</source><year>2014</year><volume>34</volume><issue>5</issue><fpage>387</fpage><lpage>405</lpage></element-citation><mixed-citation id="mc-CR51" publication-type="journal">Laing, C. (2014). A phonological analysis of onomatopoeia in early word production. <italic>First Language,</italic><italic>34</italic>(5), 387&#x02013;405.</mixed-citation></citation-alternatives></ref><ref id="CR52"><citation-alternatives><element-citation id="ec-CR52" publication-type="journal"><person-group person-group-type="author"><name><surname>Laing</surname><given-names>C</given-names></name></person-group><article-title>A role for onomatopoeia in early language: Evidence from phonological development</article-title><source>Language and Cognition</source><year>2019</year><volume>11</volume><issue>2</issue><fpage>173</fpage><lpage>187</lpage></element-citation><mixed-citation id="mc-CR52" publication-type="journal">Laing, C. (2019). A role for onomatopoeia in early language: Evidence from phonological development. <italic>Language and Cognition,</italic><italic>11</italic>(2), 173&#x02013;187.</mixed-citation></citation-alternatives></ref><ref id="CR53"><citation-alternatives><element-citation id="ec-CR53" publication-type="journal"><person-group person-group-type="author"><name><surname>Landauer</surname><given-names>TK</given-names></name><name><surname>Dumais</surname><given-names>ST</given-names></name></person-group><article-title>A solution to plato&#x02019;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</article-title><source>Psychological Review</source><year>1997</year><volume>104</volume><issue>2</issue><fpage>211</fpage></element-citation><mixed-citation id="mc-CR53" publication-type="journal">Landauer, T. K., &#x00026; Dumais, S. T. (1997). A solution to plato&#x02019;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. <italic>Psychological Review,</italic><italic>104</italic>(2), 211.</mixed-citation></citation-alternatives></ref><ref id="CR54"><citation-alternatives><element-citation id="ec-CR54" publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><issue>7553</issue><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="pmid">26017442</pub-id>
</element-citation><mixed-citation id="mc-CR54" publication-type="journal">LeCun, Y., Bengio, Y., &#x00026; Hinton, G. (2015). Deep learning. <italic>Nature,</italic><italic>521</italic>(7553), 436&#x02013;444.<pub-id pub-id-type="pmid">26017442</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR55"><citation-alternatives><element-citation id="ec-CR55" publication-type="journal"><person-group person-group-type="author"><name><surname>Levelt</surname><given-names>WJ</given-names></name><name><surname>Roelofs</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>AS</given-names></name></person-group><article-title>A theory of lexical access in speech production</article-title><source>Behavioral and Brain Sciences</source><year>1999</year><volume>22</volume><fpage>1</fpage><lpage>38</lpage><pub-id pub-id-type="pmid">11301520</pub-id>
</element-citation><mixed-citation id="mc-CR55" publication-type="journal">Levelt, W. J., Roelofs, A., &#x00026; Meyer, A. S. (1999). A theory of lexical access in speech production. <italic>Behavioral and Brain Sciences,</italic><italic>22</italic>, 1&#x02013;38.<pub-id pub-id-type="pmid">11301520</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR56"><citation-alternatives><element-citation id="ec-CR56" publication-type="journal"><person-group person-group-type="author"><name><surname>Ley</surname><given-names>A</given-names></name><name><surname>Vroomen</surname><given-names>J</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><article-title>How learning to abstract shapes neural sound representations</article-title><source>Frontiers in Neuroscience</source><year>2014</year><volume>8</volume><fpage>132</fpage><pub-id pub-id-type="pmid">24917783</pub-id>
</element-citation><mixed-citation id="mc-CR56" publication-type="journal">Ley, A., Vroomen, J., &#x00026; Formisano, E. (2014). How learning to abstract shapes neural sound representations. <italic>Frontiers in Neuroscience,</italic><italic>8</italic>, 132.<pub-id pub-id-type="pmid">24917783</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR57"><citation-alternatives><element-citation id="ec-CR57" publication-type="journal"><person-group person-group-type="author"><name><surname>Lund</surname><given-names>K</given-names></name><name><surname>Burgess</surname><given-names>C</given-names></name></person-group><article-title>Producing high-dimensional semantic spaces from lexical co-occurrence</article-title><source>Behavior Research Methods, Instruments, &#x00026; Computers</source><year>1996</year><volume>28</volume><issue>2</issue><fpage>203</fpage><lpage>208</lpage></element-citation><mixed-citation id="mc-CR57" publication-type="journal">Lund, K., &#x00026; Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical co-occurrence. <italic>Behavior Research Methods, Instruments, &#x00026; Computers,</italic><italic>28</italic>(2), 203&#x02013;208.</mixed-citation></citation-alternatives></ref><ref id="CR58"><citation-alternatives><element-citation id="ec-CR58" publication-type="journal"><person-group person-group-type="author"><name><surname>Lupyan</surname><given-names>G</given-names></name><name><surname>Winter</surname><given-names>B</given-names></name></person-group><article-title>Language is more abstract than you think, or, why aren&#x02019;t languages more iconic?</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2018</year><volume>373</volume><issue>1752</issue><fpage>20170137</fpage></element-citation><mixed-citation id="mc-CR58" publication-type="journal">Lupyan, G., &#x00026; Winter, B. (2018). Language is more abstract than you think, or, why aren&#x02019;t languages more iconic? <italic>Philosophical Transactions of the Royal Society B: Biological Sciences,</italic><italic>373</italic>(1752), 20170137.</mixed-citation></citation-alternatives></ref><ref id="CR59"><citation-alternatives><element-citation id="ec-CR59" publication-type="journal"><person-group person-group-type="author"><name><surname>Lynott</surname><given-names>D</given-names></name><name><surname>Connell</surname><given-names>L</given-names></name><name><surname>Brysbaert</surname><given-names>M</given-names></name><name><surname>Brand</surname><given-names>J</given-names></name><name><surname>Carney</surname><given-names>J</given-names></name></person-group><article-title>The lancaster sensorimotor norms: multidimensional measures of perceptual and action strength for 40,000 english words</article-title><source>Behavior Research Methods</source><year>2020</year><volume>52</volume><issue>3</issue><fpage>1271</fpage><lpage>1291</lpage><pub-id pub-id-type="pmid">31832879</pub-id>
</element-citation><mixed-citation id="mc-CR59" publication-type="journal">Lynott, D., Connell, L., Brysbaert, M., Brand, J., &#x00026; Carney, J. (2020). The lancaster sensorimotor norms: multidimensional measures of perceptual and action strength for 40,000 english words. <italic>Behavior Research Methods,</italic><italic>52</italic>(3), 1271&#x02013;1291.<pub-id pub-id-type="pmid">31832879</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR60"><citation-alternatives><element-citation id="ec-CR60" publication-type="journal"><person-group person-group-type="author"><name><surname>Lynott</surname><given-names>D</given-names></name><name><surname>Connell</surname><given-names>L</given-names></name><name><surname>Brysbaert</surname><given-names>M</given-names></name><name><surname>Brand</surname><given-names>J</given-names></name><name><surname>Carney</surname><given-names>J</given-names></name></person-group><article-title>The lancaster sensorimotor norms: multidimensional measures of perceptual and action strength for 40,000 english words</article-title><source>Behavior Research Methods</source><year>2020</year><volume>52</volume><issue>3</issue><fpage>1271</fpage><lpage>1291</lpage><pub-id pub-id-type="pmid">31832879</pub-id>
</element-citation><mixed-citation id="mc-CR60" publication-type="journal">Lynott, D., Connell, L., Brysbaert, M., Brand, J., &#x00026; Carney, J. (2020). The lancaster sensorimotor norms: multidimensional measures of perceptual and action strength for 40,000 english words. <italic>Behavior Research Methods,</italic><italic>52</italic>(3), 1271&#x02013;1291.<pub-id pub-id-type="pmid">31832879</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR61"><mixed-citation publication-type="other">Magnus, M. (2013). A history of sound symbolism. <italic>The Oxford handbook of the history of linguistics</italic>, 191&#x02013;208.</mixed-citation></ref><ref id="CR62"><citation-alternatives><element-citation id="ec-CR62" publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><source>Vision: A computational approach</source><year>1982</year><publisher-loc>San Francisco, CA</publisher-loc><publisher-name>Freeman &#x00026; Co</publisher-name></element-citation><mixed-citation id="mc-CR62" publication-type="book">Marr, D. (1982). <italic>Vision: A computational approach</italic>. San Francisco, CA: Freeman &#x00026; Co.</mixed-citation></citation-alternatives></ref><ref id="CR63"><citation-alternatives><element-citation id="ec-CR63" publication-type="journal"><person-group person-group-type="author"><name><surname>Maurer</surname><given-names>D</given-names></name><name><surname>Pathman</surname><given-names>T</given-names></name><name><surname>Mondloch</surname><given-names>CJ</given-names></name></person-group><article-title>The shape of boubas: Sound-shape correspondences in toddlers and adults</article-title><source>Developmental Science</source><year>2006</year><volume>9</volume><issue>3</issue><fpage>316</fpage><lpage>322</lpage><pub-id pub-id-type="pmid">16669803</pub-id>
</element-citation><mixed-citation id="mc-CR63" publication-type="journal">Maurer, D., Pathman, T., &#x00026; Mondloch, C. J. (2006). The shape of boubas: Sound-shape correspondences in toddlers and adults. <italic>Developmental Science,</italic><italic>9</italic>(3), 316&#x02013;322.<pub-id pub-id-type="pmid">16669803</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR64"><citation-alternatives><element-citation id="ec-CR64" publication-type="journal"><person-group person-group-type="author"><name><surname>McLean</surname><given-names>B</given-names></name></person-group><article-title>Revising an implicational hierarchy for the meanings of ideophones, with special reference to japonic</article-title><source>Linguistic Typology</source><year>2021</year><volume>25</volume><issue>3</issue><fpage>507</fpage><lpage>549</lpage></element-citation><mixed-citation id="mc-CR64" publication-type="journal">McLean, B. (2021). Revising an implicational hierarchy for the meanings of ideophones, with special reference to japonic. <italic>Linguistic Typology,</italic><italic>25</italic>(3), 507&#x02013;549.</mixed-citation></citation-alternatives></ref><ref id="CR65"><citation-alternatives><element-citation id="ec-CR65" publication-type="journal"><person-group person-group-type="author"><name><surname>McLean</surname><given-names>B</given-names></name><name><surname>Dunn</surname><given-names>M</given-names></name><name><surname>Dingemanse</surname><given-names>M</given-names></name></person-group><article-title>Two measures are better than one: Combining iconicity ratings and guessing experiments for a more nuanced picture of iconicity in the lexicon</article-title><source>Language and Cognition</source><year>2023</year><volume>15</volume><issue>4</issue><fpage>716</fpage><lpage>739</lpage></element-citation><mixed-citation id="mc-CR65" publication-type="journal">McLean, B., Dunn, M., &#x00026; Dingemanse, M. (2023). Two measures are better than one: Combining iconicity ratings and guessing experiments for a more nuanced picture of iconicity in the lexicon. <italic>Language and Cognition,</italic><italic>15</italic>(4), 716&#x02013;739.</mixed-citation></citation-alternatives></ref><ref id="CR66"><mixed-citation publication-type="other">Mikolov, T., Chen, K., Corrado, G., &#x00026; Dean, J. (2013). Efficient estimation of word representations in vector space. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1301.3781">arXiv:1301.3781</ext-link>.</mixed-citation></ref><ref id="CR67"><citation-alternatives><element-citation id="ec-CR67" publication-type="journal"><person-group person-group-type="author"><name><surname>Motamedi</surname><given-names>Y</given-names></name><name><surname>Little</surname><given-names>H</given-names></name><name><surname>Nielsen</surname><given-names>A</given-names></name><name><surname>Sulik</surname><given-names>J</given-names></name></person-group><article-title>The iconicity toolbox: empirical approaches to measuring iconicity</article-title><source>Language and Cognition</source><year>2019</year><volume>11</volume><issue>2</issue><fpage>188</fpage><lpage>207</lpage></element-citation><mixed-citation id="mc-CR67" publication-type="journal">Motamedi, Y., Little, H., Nielsen, A., &#x00026; Sulik, J. (2019). The iconicity toolbox: empirical approaches to measuring iconicity. <italic>Language and Cognition,</italic><italic>11</italic>(2), 188&#x02013;207.</mixed-citation></citation-alternatives></ref><ref id="CR68"><mixed-citation publication-type="other">Murgiano, M., Motamedi, Y., &#x00026; Vigliocco, G. (2020). Language is far less arbitrary than one thinks: Iconicity and indexicality in real-world learning and processing. <italic>Journal of Cognition</italic>.</mixed-citation></ref><ref id="CR69"><mixed-citation publication-type="other">Murgiano, M., Motamedi, Y., &#x00026; Vigliocco, G. (2021). Situating language in the real-world: the role of multimodal iconicity and indexicality. <italic>Journal of Cognition,</italic><italic>4</italic>(1).</mixed-citation></ref><ref id="CR70"><mixed-citation publication-type="other">Newmeyer, F. J. (1992). Iconicity and generative grammar. <italic>Language</italic>, 756&#x02013;796.</mixed-citation></ref><ref id="CR71"><mixed-citation publication-type="other">N&#x000f6;th, W. (1999). Peircean semiotics in the study of iconicity in language. <italic>Transactions of the Charles S. Peirce Society,</italic><italic>35</italic>(3), 613&#x02013;619.</mixed-citation></ref><ref id="CR72"><mixed-citation publication-type="other">Nuckolls, J. B. (2003). To be or not to be ideophonically impoverished. <italic>Proceedings of the eleventh annual symposium about language and society-Austin</italic>.</mixed-citation></ref><ref id="CR73"><citation-alternatives><element-citation id="ec-CR73" publication-type="journal"><person-group person-group-type="author"><name><surname>Pasch</surname><given-names>B</given-names></name><name><surname>Bolker</surname><given-names>BM</given-names></name><name><surname>Phelps</surname><given-names>SM</given-names></name></person-group><article-title>Interspecific dominance via vocal interactions mediates altitudinal zonation in neotropical singing mice</article-title><source>The American Naturalist</source><year>2013</year><volume>182</volume><issue>5</issue><fpage>E161</fpage><lpage>E173</lpage><pub-id pub-id-type="pmid">24107377</pub-id>
</element-citation><mixed-citation id="mc-CR73" publication-type="journal">Pasch, B., Bolker, B. M., &#x00026; Phelps, S. M. (2013). Interspecific dominance via vocal interactions mediates altitudinal zonation in neotropical singing mice. <italic>The American Naturalist,</italic><italic>182</italic>(5), E161&#x02013;E173.<pub-id pub-id-type="pmid">24107377</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR74"><mixed-citation publication-type="other">Peeters, D. (2016). Processing consequences of onomatopoeic iconicity in spoken language comprehension. In <italic>38th annual meeting of the cognitive science society (cogsci 2016)</italic> (pp. 1632&#x02013;1647).</mixed-citation></ref><ref id="CR75"><mixed-citation publication-type="other">Perlman, M., Little, H., Thompson, B., &#x00026; Thompson, R. L. (2018). Iconicity in signed and spoken vocabulary: A comparison between American sign language, British sign language, english, and spanish. <italic>Frontiers in Psychology,</italic><italic>9</italic>. 10.3389/fpsyg.2018.01433</mixed-citation></ref><ref id="CR76"><citation-alternatives><element-citation id="ec-CR76" publication-type="journal"><person-group person-group-type="author"><name><surname>Perniss</surname><given-names>P</given-names></name><name><surname>Vigliocco</surname><given-names>G</given-names></name></person-group><article-title>The bridge of iconicity: from a world of experience to the experience of language</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2014</year><volume>369</volume><issue>1651</issue><fpage>20130300</fpage></element-citation><mixed-citation id="mc-CR76" publication-type="journal">Perniss, P., &#x00026; Vigliocco, G. (2014). The bridge of iconicity: from a world of experience to the experience of language. <italic>Philosophical Transactions of the Royal Society B: Biological Sciences,</italic><italic>369</italic>(1651), 20130300.</mixed-citation></citation-alternatives></ref><ref id="CR77"><mixed-citation publication-type="other">Perry, L. K., Perlman, M., &#x00026; Lupyan, G. (2015). Iconicity in English and Spanish and its relation to lexical category and age of acquisition. <italic>PloS One,</italic><italic>10</italic>(9).</mixed-citation></ref><ref id="CR78"><citation-alternatives><element-citation id="ec-CR78" publication-type="journal"><person-group person-group-type="author"><name><surname>Perry</surname><given-names>LK</given-names></name><name><surname>Perlman</surname><given-names>M</given-names></name><name><surname>Winter</surname><given-names>B</given-names></name><name><surname>Massaro</surname><given-names>DW</given-names></name><name><surname>Lupyan</surname><given-names>G</given-names></name></person-group><article-title>Iconicity in the speech of children and adults</article-title><source>Developmental Science</source><year>2018</year><volume>21</volume><issue>3</issue><fpage>e12572</fpage><pub-id pub-id-type="pmid">28523758</pub-id>
</element-citation><mixed-citation id="mc-CR78" publication-type="journal">Perry, L. K., Perlman, M., Winter, B., Massaro, D. W., &#x00026; Lupyan, G. (2018). Iconicity in the speech of children and adults. <italic>Developmental Science,</italic><italic>21</italic>(3), e12572.<pub-id pub-id-type="pmid">28523758</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR79"><citation-alternatives><element-citation id="ec-CR79" publication-type="journal"><person-group person-group-type="author"><name><surname>Petilli</surname><given-names>MA</given-names></name><name><surname>G&#x000fc;nther</surname><given-names>F</given-names></name><name><surname>Vergallito</surname><given-names>A</given-names></name><name><surname>Ciapparelli</surname><given-names>M</given-names></name><name><surname>Marelli</surname><given-names>M</given-names></name></person-group><article-title>Data-driven computational models reveal perceptual simulation in word processing</article-title><source>Journal of Memory and Language</source><year>2021</year><volume>117</volume><fpage>104194</fpage></element-citation><mixed-citation id="mc-CR79" publication-type="journal">Petilli, M. A., G&#x000fc;nther, F., Vergallito, A., Ciapparelli, M., &#x00026; Marelli, M. (2021). Data-driven computational models reveal perceptual simulation in word processing. <italic>Journal of Memory and Language,</italic><italic>117</italic>, 104194.</mixed-citation></citation-alternatives></ref><ref id="CR80"><citation-alternatives><element-citation id="ec-CR80" publication-type="journal"><person-group person-group-type="author"><name><surname>Ramachandran</surname><given-names>VS</given-names></name><name><surname>Hubbard</surname><given-names>EM</given-names></name></person-group><article-title>Synaesthesia-a window into perception, thought and language</article-title><source>Journal of Consciousness Studies</source><year>2001</year><volume>8</volume><issue>12</issue><fpage>3</fpage><lpage>34</lpage></element-citation><mixed-citation id="mc-CR80" publication-type="journal">Ramachandran, V. S., &#x00026; Hubbard, E. M. (2001). Synaesthesia-a window into perception, thought and language. <italic>Journal of Consciousness Studies,</italic><italic>8</italic>(12), 3&#x02013;34.</mixed-citation></citation-alternatives></ref><ref id="CR81"><citation-alternatives><element-citation id="ec-CR81" publication-type="journal"><person-group person-group-type="author"><name><surname>Rotaru</surname><given-names>AS</given-names></name><name><surname>Vigliocco</surname><given-names>G</given-names></name><name><surname>Frank</surname><given-names>SL</given-names></name></person-group><article-title>Modeling the structure and dynamics of semantic processing</article-title><source>Cognitive Science</source><year>2018</year><volume>42</volume><issue>8</issue><fpage>2890</fpage><lpage>2917</lpage><pub-id pub-id-type="pmid">30294932</pub-id>
</element-citation><mixed-citation id="mc-CR81" publication-type="journal">Rotaru, A. S., Vigliocco, G., &#x00026; Frank, S. L. (2018). Modeling the structure and dynamics of semantic processing. <italic>Cognitive Science,</italic><italic>42</italic>(8), 2890&#x02013;2917.<pub-id pub-id-type="pmid">30294932</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR82"><citation-alternatives><element-citation id="ec-CR82" publication-type="journal"><person-group person-group-type="author"><name><surname>Ryan</surname><given-names>MJ</given-names></name><name><surname>Brenowitz</surname><given-names>EA</given-names></name></person-group><article-title>The role of body size, phylogeny, and ambient noise in the evolution of bird song</article-title><source>The American Naturalist</source><year>1985</year><volume>126</volume><issue>1</issue><fpage>87</fpage><lpage>100</lpage></element-citation><mixed-citation id="mc-CR82" publication-type="journal">Ryan, M. J., &#x00026; Brenowitz, E. A. (1985). The role of body size, phylogeny, and ambient noise in the evolution of bird song. <italic>The American Naturalist,</italic><italic>126</italic>(1), 87&#x02013;100.</mixed-citation></citation-alternatives></ref><ref id="CR83"><citation-alternatives><element-citation id="ec-CR83" publication-type="journal"><person-group person-group-type="author"><name><surname>Sapir</surname><given-names>E</given-names></name></person-group><article-title>A study in phonetic symbolism</article-title><source>Journal of Experimental Psychology</source><year>1929</year><volume>12</volume><issue>3</issue><fpage>225</fpage></element-citation><mixed-citation id="mc-CR83" publication-type="journal">Sapir, E. (1929). A study in phonetic symbolism. <italic>Journal of Experimental Psychology,</italic><italic>12</italic>(3), 225.</mixed-citation></citation-alternatives></ref><ref id="CR84"><mixed-citation publication-type="other">Saussure, F. D. (1964). Course of general linguistics (cours de linguistique g&#x000e9;n&#x000e9;rale, 1959). second impression. ed. by Charles Bally and Albert Sechehaye. <italic>Trans. Wade Baskin. London: Peter Owen</italic>.</mixed-citation></ref><ref id="CR85"><citation-alternatives><element-citation id="ec-CR85" publication-type="journal"><person-group person-group-type="author"><name><surname>Seeliger</surname><given-names>K</given-names></name><name><surname>Fritsche</surname><given-names>M</given-names></name><name><surname>G&#x000fc;&#x000e7;l&#x000fc;</surname><given-names>U</given-names></name><name><surname>Schoenmakers</surname><given-names>S</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name><name><surname>Bosch</surname><given-names>SE</given-names></name><name><surname>Van Gerven</surname><given-names>M</given-names></name></person-group><article-title>Convolutional neural network-based encoding and decoding of visual object recognition in space and time</article-title><source>NeuroImage</source><year>2018</year><volume>180</volume><fpage>253</fpage><lpage>266</lpage><pub-id pub-id-type="pmid">28723578</pub-id>
</element-citation><mixed-citation id="mc-CR85" publication-type="journal">Seeliger, K., Fritsche, M., G&#x000fc;&#x000e7;l&#x000fc;, U., Schoenmakers, S., Schoffelen, J.-M., Bosch, S. E., &#x00026; Van Gerven, M. (2018). Convolutional neural network-based encoding and decoding of visual object recognition in space and time. <italic>NeuroImage,</italic><italic>180</italic>, 253&#x02013;266.<pub-id pub-id-type="pmid">28723578</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR86"><mixed-citation publication-type="other">Shillcock, R., Kirby, S., McDonald, S., &#x00026; Brew, C. (2001). Filled pauses and their status in the mental lexicon. <italic>Isca tutorial and research workshop (itrw) on disfluency in spontaneous speech</italic>.</mixed-citation></ref><ref id="CR87"><citation-alternatives><element-citation id="ec-CR87" publication-type="journal"><person-group person-group-type="author"><name><surname>Sidhu</surname><given-names>DM</given-names></name><name><surname>Pexman</surname><given-names>PM</given-names></name></person-group><article-title>Five mechanisms of sound symbolic association</article-title><source>Psychonomic Bulletin &#x00026; Review</source><year>2018</year><volume>25</volume><issue>5</issue><fpage>1619</fpage><lpage>1643</lpage><pub-id pub-id-type="pmid">28840520</pub-id>
</element-citation><mixed-citation id="mc-CR87" publication-type="journal">Sidhu, D. M., &#x00026; Pexman, P. M. (2018). Five mechanisms of sound symbolic association. <italic>Psychonomic Bulletin &#x00026; Review,</italic><italic>25</italic>(5), 1619&#x02013;1643.<pub-id pub-id-type="pmid">28840520</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR88"><citation-alternatives><element-citation id="ec-CR88" publication-type="journal"><person-group person-group-type="author"><name><surname>Sidhu</surname><given-names>DM</given-names></name><name><surname>Vigliocco</surname><given-names>G</given-names></name><name><surname>Pexman</surname><given-names>PM</given-names></name></person-group><article-title>Effects of iconicity in lexical decision</article-title><source>Language and Cognition</source><year>2020</year><volume>12</volume><issue>1</issue><fpage>164</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1017/langcog.2019.36</pub-id></element-citation><mixed-citation id="mc-CR88" publication-type="journal">Sidhu, D. M., Vigliocco, G., &#x00026; Pexman, P. M. (2020). Effects of iconicity in lexical decision. <italic>Language and Cognition,</italic><italic>12</italic>(1), 164&#x02013;181. 10.1017/langcog.2019.36</mixed-citation></citation-alternatives></ref><ref id="CR89"><mixed-citation publication-type="other">Simonyan, K., &#x00026; Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition.</mixed-citation></ref><ref id="CR90"><citation-alternatives><element-citation id="ec-CR90" publication-type="journal"><person-group person-group-type="author"><name><surname>Speed</surname><given-names>LJ</given-names></name><name><surname>Atkinson</surname><given-names>H</given-names></name><name><surname>Wnuk</surname><given-names>E</given-names></name><name><surname>Majid</surname><given-names>A</given-names></name></person-group><article-title>The sound of smell: Associating odor valence with disgust sounds</article-title><source>Cognitive Science</source><year>2021</year><volume>45</volume><issue>5</issue><fpage>e12980</fpage><pub-id pub-id-type="pmid">34018230</pub-id>
</element-citation><mixed-citation id="mc-CR90" publication-type="journal">Speed, L. J., Atkinson, H., Wnuk, E., &#x00026; Majid, A. (2021). The sound of smell: Associating odor valence with disgust sounds. <italic>Cognitive Science,</italic><italic>45</italic>(5), e12980.<pub-id pub-id-type="pmid">34018230</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR91"><citation-alternatives><element-citation id="ec-CR91" publication-type="journal"><person-group person-group-type="author"><name><surname>Speed</surname><given-names>LJ</given-names></name><name><surname>Brybaert</surname><given-names>M</given-names></name></person-group><article-title>Dutch sensory modality norms</article-title><source>Behavior Research Methods</source><year>2022</year><volume>54</volume><issue>3</issue><fpage>1306</fpage><lpage>1318</lpage><pub-id pub-id-type="pmid">34505998</pub-id>
</element-citation><mixed-citation id="mc-CR91" publication-type="journal">Speed, L. J., &#x00026; Brybaert, M. (2022). Dutch sensory modality norms. <italic>Behavior Research Methods,</italic><italic>54</italic>(3), 1306&#x02013;1318.<pub-id pub-id-type="pmid">34505998</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR92"><citation-alternatives><element-citation id="ec-CR92" publication-type="journal"><person-group person-group-type="author"><name><surname>Sutherland</surname><given-names>SL</given-names></name><name><surname>Cimpian</surname><given-names>A</given-names></name></person-group><article-title>An explanatory heuristic gives rise to the belief that words are well suited for their referents</article-title><source>Cognition</source><year>2015</year><volume>143</volume><fpage>228</fpage><lpage>240</lpage><pub-id pub-id-type="pmid">26226428</pub-id>
</element-citation><mixed-citation id="mc-CR92" publication-type="journal">Sutherland, S. L., &#x00026; Cimpian, A. (2015). An explanatory heuristic gives rise to the belief that words are well suited for their referents. <italic>Cognition,</italic><italic>143</italic>, 228&#x02013;240.<pub-id pub-id-type="pmid">26226428</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR93"><citation-alternatives><element-citation id="ec-CR93" publication-type="journal"><person-group person-group-type="author"><name><surname>Tamariz</surname><given-names>M</given-names></name></person-group><article-title>Exploring systematicity between phonological and context-cooccurrence representations of the mental lexicon</article-title><source>The Mental Lexicon</source><year>2008</year><volume>3</volume><fpage>259</fpage><lpage>278</lpage></element-citation><mixed-citation id="mc-CR93" publication-type="journal">Tamariz, M. (2008). Exploring systematicity between phonological and context-cooccurrence representations of the mental lexicon. <italic>The Mental Lexicon,</italic><italic>3</italic>, 259&#x02013;278.</mixed-citation></citation-alternatives></ref><ref id="CR94"><mixed-citation publication-type="other">Thompson, A. L., Akita, K., &#x00026; Do, Y. (2020). Iconicity ratings across the Japanese lexicon: A comparative study with English. <italic>Linguistics Vanguard,</italic><italic>6</italic>(1).</mixed-citation></ref><ref id="CR95"><citation-alternatives><element-citation id="ec-CR95" publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>AL</given-names></name><name><surname>Chan</surname><given-names>MPY</given-names></name><name><surname>Yeung</surname><given-names>PH</given-names></name><name><surname>Do</surname><given-names>Y</given-names></name></person-group><article-title>Structural markedness and depiction: The case of lower sequential predictability in Cantonese ideophones</article-title><source>The Mental Lexicon</source><year>2022</year><volume>17</volume><issue>2</issue><fpage>300</fpage><lpage>324</lpage></element-citation><mixed-citation id="mc-CR95" publication-type="journal">Thompson, A. L., Chan, M. P. Y., Yeung, P. H., &#x00026; Do, Y. (2022). Structural markedness and depiction: The case of lower sequential predictability in Cantonese ideophones. <italic>The Mental Lexicon,</italic><italic>17</italic>(2), 300&#x02013;324.</mixed-citation></citation-alternatives></ref><ref id="CR96"><mixed-citation publication-type="other">Thompson, A. L., &#x00026; Do, Y. (2019). Defining iconicity: An articulation-based methodology for explaining the phonological structure of ideophones. <italic>Glossa: a journal of general linguistics</italic>.</mixed-citation></ref><ref id="CR97"><citation-alternatives><element-citation id="ec-CR97" publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>AL</given-names></name><name><surname>Van Hoey</surname><given-names>T</given-names></name><name><surname>Do</surname><given-names>Y</given-names></name></person-group><article-title>Articulatory features of phonemes pattern to iconic meanings: Evidence from cross-linguistic ideophones</article-title><source>Cognitive Linguistics</source><year>2021</year><volume>32</volume><issue>4</issue><fpage>563</fpage><lpage>608</lpage></element-citation><mixed-citation id="mc-CR97" publication-type="journal">Thompson, A. L., Van Hoey, T., &#x00026; Do, Y. (2021). Articulatory features of phonemes pattern to iconic meanings: Evidence from cross-linguistic ideophones. <italic>Cognitive Linguistics,</italic><italic>32</italic>(4), 563&#x02013;608.</mixed-citation></citation-alternatives></ref><ref id="CR98"><citation-alternatives><element-citation id="ec-CR98" publication-type="journal"><person-group person-group-type="author"><name><surname>Tucker</surname><given-names>BV</given-names></name><name><surname>Brenner</surname><given-names>D</given-names></name><name><surname>Danielson</surname><given-names>DK</given-names></name><name><surname>Kelley</surname><given-names>MC</given-names></name><name><surname>Nenadi&#x00107;</surname><given-names>F</given-names></name><name><surname>Sims</surname><given-names>M</given-names></name></person-group><article-title>The massive auditory lexical decision (mald) database</article-title><source>Behavior Research Methods</source><year>2019</year><volume>51</volume><issue>3</issue><fpage>1187</fpage><lpage>1204</lpage><pub-id pub-id-type="pmid">29916041</pub-id>
</element-citation><mixed-citation id="mc-CR98" publication-type="journal">Tucker, B. V., Brenner, D., Danielson, D. K., Kelley, M. C., Nenadi&#x00107;, F., &#x00026; Sims, M. (2019). The massive auditory lexical decision (mald) database. <italic>Behavior Research Methods,</italic><italic>51</italic>(3), 1187&#x02013;1204.<pub-id pub-id-type="pmid">29916041</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR99"><citation-alternatives><element-citation id="ec-CR99" publication-type="journal"><person-group person-group-type="author"><name><surname>Van Hoey</surname><given-names>T</given-names></name><name><surname>Thompson</surname><given-names>AL</given-names></name><name><surname>Do</surname><given-names>Y</given-names></name><name><surname>Dingemanse</surname><given-names>M</given-names></name></person-group><article-title>Iconicity in ideophones: Guessing, memorizing, and reassessing</article-title><source>Cognitive Science</source><year>2023</year><volume>47</volume><issue>4</issue><fpage>e13268</fpage><pub-id pub-id-type="pmid">37062829</pub-id>
</element-citation><mixed-citation id="mc-CR99" publication-type="journal">Van Hoey, T., Thompson, A. L., Do, Y., &#x00026; Dingemanse, M. (2023). Iconicity in ideophones: Guessing, memorizing, and reassessing. <italic>Cognitive Science,</italic><italic>47</italic>(4), e13268.<pub-id pub-id-type="pmid">37062829</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR100"><mixed-citation publication-type="other">Vergallito, A., Petilli, M. A., &#x00026; Marelli, M. (2020). Perceptual modality norms for 1,121 italian words: A comparison with concreteness and imageability scores and an analysis of their impact in word processing tasks. <italic>Behavior Research Methods,</italic><italic>52</italic>(4), 1599&#x02013;1616.</mixed-citation></ref><ref id="CR101"><mixed-citation publication-type="other">Werner, H. (1948). Comparative psychology of mental development.</mixed-citation></ref><ref id="CR102"><mixed-citation publication-type="other">Westbury, C. (2016). Pay no attention to that man behind the curtain: Explaining semantics without semantics. <italic>The Mental Lexicon,</italic><italic>11</italic>(3), 350&#x02013;374.</mixed-citation></ref><ref id="CR103"><citation-alternatives><element-citation id="ec-CR103" publication-type="journal"><person-group person-group-type="author"><name><surname>Whitney</surname><given-names>WD</given-names></name></person-group><article-title>Fusei or qesei - natural or conventional?</article-title><source>Transactions of the American Philological Association</source><year>1874</year><volume>1869&#x02013;1896</volume><issue>5</issue><fpage>95</fpage><lpage>116</lpage></element-citation><mixed-citation id="mc-CR103" publication-type="journal">Whitney, W. D. (1874). Fusei or qesei - natural or conventional? <italic>Transactions of the American Philological Association,</italic><italic>1869&#x02013;1896</italic>(5), 95&#x02013;116.</mixed-citation></citation-alternatives></ref><ref id="CR104"><mixed-citation publication-type="other">Winter, B., Lupyan, G., Perry, L.K., Dingemanse, M., &#x00026; Perlman, M. (2023). Iconicity ratings for 14,000+ english words. <italic>Behavior research methods</italic>, 1&#x02013;16</mixed-citation></ref><ref id="CR105"><mixed-citation publication-type="other">Winter, B., &#x00026; Perlman, M. (2021). Iconicity ratings really do measure iconicity, and they open a new window onto the nature of language. <italic>Linguistics Vanguard</italic>, <italic>7</italic>(1),</mixed-citation></ref><ref id="CR106"><mixed-citation publication-type="other">Winter, B., Perlman, M., Perry, L. K., &#x00026; Lupyan, G. (2017). Which words are most iconic?: Iconicity in english sensory words. <italic>Interaction Studies,</italic><italic>18</italic>(3), 443&#x02013;464.</mixed-citation></ref><ref id="CR107"><citation-alternatives><element-citation id="ec-CR107" publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><year>2016</year><volume>19</volume><issue>3</issue><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id><pub-id pub-id-type="pmid">26906502</pub-id>
</element-citation><mixed-citation id="mc-CR107" publication-type="journal">Yamins, D. L. K., &#x00026; DiCarlo, J. J. (2016). Using goal-driven deep learning models to understand sensory cortex. <italic>Nature Neuroscience,</italic><italic>19</italic>(3), 356&#x02013;365. 10.1038/nn.4244<pub-id pub-id-type="pmid">26906502</pub-id>
</mixed-citation></citation-alternatives></ref></ref-list></back></article>