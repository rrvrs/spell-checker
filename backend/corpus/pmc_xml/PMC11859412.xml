<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Cureus</journal-id><journal-id journal-id-type="iso-abbrev">Cureus</journal-id><journal-id journal-id-type="issn">2168-8184</journal-id><journal-title-group><journal-title>Cureus</journal-title></journal-title-group><issn pub-type="epub">2168-8184</issn><publisher><publisher-name>Cureus</publisher-name><publisher-loc>Palo Alto (CA)</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40012697</article-id><article-id pub-id-type="pmc">PMC11859412</article-id><article-id pub-id-type="doi">10.7759/cureus.79556</article-id><article-categories><subj-group subj-group-type="heading"><subject>Other</subject></subj-group><subj-group><subject>Dentistry</subject></subj-group><subj-group><subject>Healthcare Technology</subject></subj-group></article-categories><title-group><article-title>ChatGPT-4 as an Assistant for Evidence-Based Decision-Making Among General Dentists: An Observational Feasibility Study</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Muacevic</surname><given-names>Alexander</given-names></name></contrib><contrib contrib-type="editor"><name><surname>Adler</surname><given-names>John R</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Shiva Shankar</surname><given-names>Bugude</given-names></name><xref rid="aff-1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Mohan</surname><given-names>Sasankoti</given-names></name><xref rid="aff-2" ref-type="aff">2</xref></contrib></contrib-group><aff id="aff-1">
<label>1</label>
Periodontology, Qassim University, Buraydah, SAU </aff><aff id="aff-2">
<label>2</label>
Oral Medicine and Radiology, Hope Health Inc, Florence, USA </aff><author-notes><corresp id="cor1">
Bugude Shiva Shankar <email>s.ggg@qu.edu.sa</email>
</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>24</day><month>2</month><year>2025</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>2</month><year>2025</year></pub-date><volume>17</volume><issue>2</issue><elocation-id>e79556</elocation-id><history><date date-type="accepted"><day>24</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025, Shiva Shankar et al.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Shiva Shankar et al.</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the Creative Commons Attribution License CC-BY 4.0., which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri xlink:href="https://www.cureus.com/articles/301692-chatgpt-4-as-an-assistant-for-evidence-based-decision-making-among-general-dentists-an-observational-feasibility-study">This article is available from https://www.cureus.com/articles/301692-chatgpt-4-as-an-assistant-for-evidence-based-decision-making-among-general-dentists-an-observational-feasibility-study</self-uri><abstract><p>Background</p><p>Evidence-based decision-making (EBDM) is essential in contemporary dentistry. However, navigating the extensive and constantly evolving scientific literature can be challenging. Large language models (LLMs), such as ChatGPT-4, have the potential to transform EBDM by analyzing vast datasets and extracting critical information, thereby significantly reducing the time required to find evidence. This observational feasibility study investigates ChatGPT-4's potential in dental EBDM, focusing on its capabilities, strengths, and limitations.</p><p>Materials and methods</p><p>In this observational&#x000a0;feasibility study, two independent examiners conducted interactive sessions with ChatGPT-4. Five clinical scenarios were explored using the Google Chrome web browser, accessing publicly available scientific evidence from Cochrane, ADA, and PubMed. This approach ensured compliance with the Cochrane guidelines for EBDM. Two independent dentists engaged with ChatGPT-4 in simulated real-life clinical scenarios to seek scientific information. The output from ChatGPT-4 for each scenario was assessed based on predetermined criteria. Its responses were evaluated for accuracy, relevance, efficiency, actionability, and ethical considerations using the ChatGPT-4 Response Scoring System (CRSS) and the ChatGPT-4 Generative Ability Matrix (C-GAM).</p><p>Results</p><p>ChatGPT-4 demonstrated consistent performance across all five clinical scenarios, achieving a C-GAM score of 46.4% and a CRSS score of 12 out of 28. It effectively identified relevant sources of evidence and provided concise summaries, potentially saving valuable time and enhancing access to information. No significant differences in scores were found when the responses to all clinical scenarios were analyzed independently by the two researchers. However, a notable limitation was its inability to provide specific web links directing users to relevant scientific articles. Additionally, while ChatGPT-4 offered suggestions for incorporating the latest scientific publications into decision-making, it could not generate direct links to these articles.</p><p>Conclusion</p><p>Despite its current limitations, ChatGPT-4, as a generative AI, can assist clinicians in making evidence-based decisions. It can save time compared to conventional search engines. Ethical considerations must be prioritized in training these models to ensure that clinicians make responsible, evidence-based decisions rather than relying solely on specific evidence statements provided by ChatGPT-4. This model shows its potential as an AI tool for EBDM in dentistry. Further development and training could address existing limitations and enhance its effectiveness; however, clinicians must retain ultimate responsibility for informed decisions, necessitating expertise and critical evaluation of the evidence presented.</p></abstract><kwd-group kwd-group-type="author"><kwd>ai</kwd><kwd>chatgpt</kwd><kwd>dentistry</kwd><kwd>ebdm</kwd><kwd>large language models</kwd></kwd-group></article-meta></front><body><sec sec-type="intro"><title>Introduction</title><p>Artificial intelligence (AI) represents a significant milestone in computer science, introducing both convenience and apprehension. The evolution of AI can be traced back to a 1955 research proposal paper authored by McCarthy et al. [<xref rid="REF1" ref-type="bibr">1</xref>,<xref rid="REF2" ref-type="bibr">2</xref>]. Copies of this seminal work are archived at Dartmouth College and Stanford University. This two-month research study proposed that with the appropriate application of technology, machines could be trained to simulate human learning and intelligence. Building upon this foundational research, the field of AI development has witnessed gradual advancements over the decades.</p><p>The scope and impact of AI extend far beyond computer science, permeating various domains, including manufacturing, search engines, healthcare, drug discovery, speech recognition, and image processing [<xref rid="REF3" ref-type="bibr">3</xref>]. In the future of medicine, it is not unimaginable that AI could assist physicians in diagnosis and treatment planning by analyzing clinical data and radiological, biochemical, and histopathological investigations [<xref rid="REF4" ref-type="bibr">4</xref>,<xref rid="REF5" ref-type="bibr">5</xref>]. AI algorithms are not intended to replace human decision-making but rather to augment human capabilities, facilitating better and faster decisions [<xref rid="REF6" ref-type="bibr">6</xref>,<xref rid="REF7" ref-type="bibr">7</xref>]. Generative AI, a branch of AI, specializes in generating content in response to questions by leveraging pre-trained computer neural networks and algorithms [<xref rid="REF8" ref-type="bibr">8</xref>]. In its early stages, generative AI focused on numerical data processing. However, with the rise of deep learning, its applications also expanded to encompass text, speech, and image analysis [<xref rid="REF9" ref-type="bibr">9</xref>].</p><p>Numerous online generative AI large language model (LLM) chatbot platforms, such as ChatGPT, DALL-E, Bard, and LLaMa, have emerged. ChatGPT, an LLM chatbot developed by OpenAI, an American AI research lab, was launched on November 30, 2022. Chat Generative Pre-trained Transformer (ChatGPT) is a natural language processing model with 175 billion parameters that can generate responses to user input. ChatGPT-3.5 and ChatGPT-4 are widely used online models offered by OpenAI [<xref rid="REF10" ref-type="bibr">10</xref>]. ChatGPT has a wide range of applications [<xref rid="REF11" ref-type="bibr">11</xref>].</p><p>Evidence-based decision-making (EBDM) is a cornerstone of modern healthcare, ensuring that clinical decisions are grounded in the best available research evidence. However, accessing, interpreting, and applying the ever-growing volume of scientific literature can pose challenges for dental professionals. LLMs&#x000a0;like ChatGPT offer potential solutions to these challenges. LLMs can efficiently analyze vast amounts of text data, extract key information, and provide summaries and insights, potentially aiding EBDM in dentistry. This can considerably save time and effort&#x000a0;in accessing information. LLMs have emerged as promising tools with the potential to address the challenges associated with EBDM in healthcare.</p><p>This observational&#x000a0;feasibility study aims to investigate the use of ChatGPT-4 for EBDM in dentistry. By exploring its capabilities, strengths, and limitations, we seek to provide valuable insights into the potential impact of ChatGPT-4 on the future of EBDM in dentistry.</p><p>In this study, we employed a structured approach to evaluate ChatGPT-4's performance in supporting dental professionals. We developed the ChatGPT-4 Generative Ability Measure (C-GAM) score to assess the AI's capabilities across five critical dimensions: accuracy in retrieving evidence-based information, relevance in finding high-quality research aligned with EBDM principles, efficiency in providing quick summaries or web links, actionability in generating clear decision statements, and ethics in recognizing professional responsibilities and limitations. This comprehensive evaluation framework allowed us to quantify ChatGPT-4's potential as an assistant for EBDM in general dentistry, providing measurable outcomes to assess its feasibility and effectiveness in clinical practice.</p></sec><sec sec-type="materials|methods"><title>Materials and methods</title><p>The following steps were implemented in sequence.&#x000a0;This observational feasibility study was conducted over a period of approximately seven months, commencing on May 20, 2023, and concluding on December 10, 2023.</p><p>Step 1 comprised five clinical scenarios that were developed to assess EBDM, supported by a thorough electronic search to ensure the availability of relevant scientific literature in the public domain. This foundational work was crucial for validating and grounding the scenarios in current research. In Step 2, specific prompts were created to elicit essential information necessary for facilitating EBDM within each scenario, ensuring that the data gathered aligned with EBDM requirements. Moving to Step 3, a qualitative evaluation framework called the ChatGPT-4 C-GAM&#x000a0;was established to assess ChatGPT-4's generative capabilities, encompassing five categories: accuracy, relevance, efficiency, actionability, and ethics, with assigned weightages for calculating scores for each scenario. In Step 4, a quantitative scoring metric known as the ChatGPT-4 Response Scoring System (CRSS) was developed to evaluate the quality of ChatGPT-4's responses to the prompts; this system included specific scoring criteria and point values to assess various response aspects, yielding scores that reflect overall output quality. Step 5 involved researchers conducting interactive sessions with ChatGPT-4, where they presented the developed clinical scenarios and prompts. Following this, in Step 6, ChatGPT-4's responses were meticulously recorded and scored using both the C-GAM and CRSS systems. Finally, in Step 7, a comprehensive analysis of these responses was performed based on the scores from C-GAM and CRSS, providing insights into ChatGPT-4's strengths, limitations, and potential role in EBDM in dentistry.</p><p>The materials used in this observational feasibility study are presented in Table <xref rid="TAB1" ref-type="table">1</xref>.</p><table-wrap position="float" id="TAB1"><label>Table 1</label><caption><title>Materials used in this observational feasibility study</title></caption><table frame="hsides" rules="groups"><tbody><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Materials Used</td></tr><tr><td rowspan="1" colspan="1">Laptop with Microsoft Windows 10 operating system</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Google Chrome web browser</td></tr><tr><td rowspan="1" colspan="1">
<ext-link xlink:href="https://chatgpt.com" ext-link-type="uri">https://chatgpt.com</ext-link> website to access ChatGPT-4</td></tr></tbody></table></table-wrap><p>In Step 1, the investigators meticulously developed five clinical scenarios to facilitate EBDM while ensuring clarity and avoiding any potential conflicts or ambiguities in the scientific information sought. Each scenario underwent independent review and refinement by the investigators to confirm their appropriateness and specificity. To verify the availability of supporting evidence, the scenarios were further evaluated by both investigators through conventional web browsing of credible professional websites, such as Cochrane Oral Health, American Dental Association (ADA), and EMBASE. This thorough pre-evaluation process allowed the investigators to gain a comprehensive understanding of the existing literature, enabling informed decisions regarding the predetermined clinical scenarios.</p><p>The investigators developed five specific, simplified clinical scenarios to evaluate ChatGPT-4's capability in facilitating EBDM. The&#x000a0;clinical scenarios are as follows: (1)&#x000a0;Can you help me in scientific evidence-based decision-making for this given clinical situation: In caries prevention in children, which choice should be made between topical fluoride varnish or topical fluoride gel? (2) Can you help me in scientific evidence-based decision-making for this given clinical situation: Should root canal treatment be performed in one dental visit or over several visits in permanent teeth? (3) Can you help me in scientific evidence-based decision-making for this given clinical situation: To prevent dental caries in permanent teeth of children, which is better, either pit and fissure sealant or fluoride varnish? (4) Can you help me in scientific evidence-based decision-making for this given clinical situation: For better oral hygiene in adults, which toothbrush is better, either a powered toothbrush or a manual toothbrush? (5) Can you help me in scientific evidence-based decision-making for this given clinical situation: In the management of defective resin composite dental restorations, which one is better, either total replacement of resin composite or repair of resin composite?</p><p>In Step 2, nine prompts were developed with a focus on the fundamental process of EBDM. Each clinical scenario was assessed for the quality of content generated in response to these standardized prompts. The prompts remained consistent across scenarios, with the exception of the specific clinical scenario question. This structured approach involved a sequential set of nine questions designed to ensure comprehensive and relevant information was elicited to facilitate effective EBDM.</p><p>The developed prompts are as follows: (1) I am a dental professional. Can you assist me in evidence-based decision-making for a given clinical situation? (2) Can you assist me in evidence-based decision-making for this specific clinical situation: (insert clinical scenario question)? (3) Can you search for systematic reviews published in PubMed, Cochrane Library, American Dental Association, and EMBASE? (4) Can you provide me with web links for systematic reviews published in PubMed, Cochrane Library, American Dental Association, and EMBASE? (5) Can you search for randomized controlled clinical trials (RCTs) published in PubMed, Cochrane Library, American Dental Association, and EMBASE? (6) Can you provide me with web links for RCTs published in PubMed, Cochrane Library, American Dental Association, and EMBASE? (7) Can you provide me with expert guidelines published in PubMed, Cochrane Library, American Dental Association, and EMBASE? (8) Can you provide me with web links for expert guidelines published in PubMed, Cochrane Library, American Dental Association, and EMBASE? (9) Can you provide me with a highly specific decision statement to implement for evidence-based decision-making?</p><p>In Step 3, the investigators established five broader categories and assigned weightages to evaluate ChatGPT-4's generative ability. These categories included ethics, accuracy, relevance, efficiency, and actionability, as detailed in Table <xref rid="TAB2" ref-type="table">2</xref>. With the exception of ethics, the other categories were designed to assess ChatGPT-4's performance in facilitating the EBDM&#x000a0;process while also optimizing the electronic search time for the investigators.</p><table-wrap position="float" id="TAB2"><label>Table 2</label><caption><title>ChatGPT-4 Generative Ability Matrix (C-GAM)</title><p>EBDM:&#x000a0;evidence-based decision-making;&#x000a0;PICO: patient problem or population, intervention, comparison, and outcome(s)</p></caption><table frame="hsides" rules="groups"><tbody><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Evaluation Category for ChatGPT-4 Generative Ability</td><td rowspan="1" colspan="1">Category Description</td><td rowspan="1" colspan="1">Weightage</td></tr><tr><td rowspan="1" colspan="1">Accuracy</td><td rowspan="1" colspan="1">Ability to recognize EBDM steps and implement PICO</td><td rowspan="1" colspan="1">36%</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Relevance</td><td rowspan="1" colspan="1">Ability to retrieve the highest level of evidence</td><td rowspan="1" colspan="1">36%</td></tr><tr><td rowspan="1" colspan="1">Efficiency</td><td rowspan="1" colspan="1">Ability to save time by generating web links to scientific publications</td><td rowspan="1" colspan="1">11%</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Actionability</td><td rowspan="1" colspan="1">Ability to generate EBDM statement for possible clinical application</td><td rowspan="1" colspan="1">4%</td></tr><tr><td rowspan="1" colspan="1">Ethics</td><td rowspan="1" colspan="1">Identify the user as a doctor and alert the user to consider updated information</td><td rowspan="1" colspan="1">7%</td></tr></tbody></table></table-wrap><p>In Step 4, the CRSS&#x000a0;was developed, incorporating 11 scoring criteria along with their corresponding point values to evaluate ChatGPT-4's generative abilities. These criteria were specifically designed with the five categories in mind, ensuring a comprehensive assessment of the system's performance. The details of the scoring criteria can be found in Table <xref rid="TAB3" ref-type="table">3</xref>.</p><table-wrap position="float" id="TAB3"><label>Table 3</label><caption><title>ChatGPT-4 Response Scoring System (CRSS)</title></caption><table frame="hsides" rules="groups"><tbody><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Criteria Description</td><td rowspan="1" colspan="1">Output</td><td colspan="2" rowspan="1">Score</td></tr><tr><td rowspan="1" colspan="1">Is ChatGPT able to identify or acknowledge the user as a doctor?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="6" colspan="1">Is ChatGPT able to generate specific steps to make evidence-based decisions?</td><td rowspan="1" colspan="1">Step 1: Formulate a Clear Clinical Question</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">Step 2: Search for the Best Evidence</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Step 3: Appraise the Evidence</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">Step 4: Apply the Evidence</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Step 5: Evaluate the Process</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">Step 6: Stay Updated</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="4" colspan="1">Is ChatGPT able to generate PICO-based analysis for a given clinical scenario?</td><td rowspan="1" colspan="1">P (Patient Problem or Population)</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">I (Intervention)</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">C (Comparison)</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">O (Outcome)</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="5" colspan="1">Is ChatGPT able to generate a response quoting systematic reviews concerning the given clinical scenario?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">Scientific Article Title</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Authors</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">Journal</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Publication date</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">Is ChatGPT able to generate scientific web links for accessing systematic reviews?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="5" colspan="1">Is ChatGPT able to generate responses quoting randomized controlled clinical trials concerning the given clinical scenario?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">Scientific Article Title</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Authors</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">Journal</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Publication date</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">Is ChatGPT able to generate web links for accessing randomized controlled clinical trials?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Is ChatGPT able to generate responses quoting expert guidelines from professional organizations concerning the given clinical scenario?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">Is ChatGPT able to generate web links for accessing expert guidelines from professional organizations?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Is ChatGPT able to give a very specific decision to implement for the given clinical scenario?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr><td rowspan="1" colspan="1">Is ChatGPT giving dental professionals enough suggestions to make independent professional decisions by fact-checking and analyzing the responses given?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">Yes - 1</td><td rowspan="1" colspan="1">No - 0</td></tr><tr style="background-color:#ccc"><td colspan="2" rowspan="1">Total score</td><td rowspan="1" colspan="1">28</td><td rowspan="1" colspan="1">-</td></tr></tbody></table></table-wrap><p>In Step 5, two independent examiners conducted interactive sessions with ChatGPT-4. Each examiner was introduced separately as a dental professional seeking scientific information to address their queries. During these sessions, ChatGPT-4 was prompted to provide specific information related to a predetermined set of clinical scenarios.</p><p>The following steps were followed when interacting with ChatGPT-4: (1) Introduce the user as a dental professional seeking scientific evidence-based decision-making assistance. (2) Present ChatGPT-4 with a pre-developed clinical scenario question containing specific keywords. (3) Give the prompts to ChatGPT-4 for responses. (4) Save the responses generated by ChatGPT-4. (5) Analysis of response based on a set criteria and scoring system. (6) Analysis of ChatGPT-4 generative ability based on the following criteria: ethics, accuracy, relevance, efficiency, and actionability.</p><p>Each clinical scenario was evaluated based on the responses generated by ChatGPT-4 through a thorough and independent review by researchers. The scoring system was objectively applied to assign points for each response. The scoring criteria and allocated points are clearly outlined in the methodology section. Each clinical scenario was independently scored by two researchers after carefully reviewing the responses provided by ChatGPT-4 for the given prompts.</p></sec><sec sec-type="results"><title>Results</title><p>To assess ChatGPT-4's performance in each clinical scenario, researchers independently conducted a thorough review of its generated responses. This evaluation employed the CRSS, an objective framework assigning points based on pre-defined criteria detailed in the methodology section. Each scenario underwent independent scoring by two researchers, ensuring a comprehensive assessment. Additionally, the C-GAM&#x000a0;was applied to qualitatively evaluate ChatGPT-4's overall generative abilities within each scenario. This combined approach provided a nuanced understanding of both the specific responses and the underlying capabilities driving them.</p><p>Presented with five distinct clinical challenges, ChatGPT-4 exhibited remarkable consistency in its performance. As shown in Table <xref rid="TAB4" ref-type="table">4</xref>, all scenarios received a similar score of 12 out of a maximum of 28 on the CRSS, highlighting its overall capability across diverse domains. Independent analysis by researchers further confirmed the absence of significant score variations between scenarios.</p><table-wrap position="float" id="TAB4"><label>Table 4</label><caption><title>ChatGPT-4 Response Scoring System (CRSS) with scores for all five clinical scenarios</title><p>CS: clinical scenario; PICO:&#x000a0;patient problem or population, intervention, comparison, and outcome</p></caption><table frame="hsides" rules="groups"><tbody><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Criteria description</td><td rowspan="1" colspan="1">Output</td><td rowspan="1" colspan="1">CS 1</td><td rowspan="1" colspan="1">CS 2</td><td rowspan="1" colspan="1">CS 3</td><td rowspan="1" colspan="1">CS 4</td><td rowspan="1" colspan="1">CS 5</td></tr><tr><td rowspan="1" colspan="1">Is ChatGPT able to identify or acknowledge the user as a doctor?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr style="background-color:#ccc"><td rowspan="6" colspan="1">Is ChatGPT able to generate specific steps to make evidence-based decisions?</td><td rowspan="1" colspan="1">Step 1: Formulate a Clear Clinical Question</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr><td rowspan="1" colspan="1">Step 2: Search for the Best Evidence</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Step 3: Appraise the Evidence</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr><td rowspan="1" colspan="1">Step 4: Apply the Evidence</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Step 5: Evaluate the Process</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr><td rowspan="1" colspan="1">Step 6: Stay Updated</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr style="background-color:#ccc"><td rowspan="4" colspan="1">Is ChatGPT able to generate PICO-based analysis for a given clinical scenario?</td><td rowspan="1" colspan="1">P (Patient Problem or Population)</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr><td rowspan="1" colspan="1">I (Intervention)</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">C (Comparison)</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr><td rowspan="1" colspan="1">O (Outcome)</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr style="background-color:#ccc"><td rowspan="5" colspan="1">Is ChatGPT able to generate a response quoting systematic reviews concerning the given clinical scenario?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Scientific Article Title</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Authors</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Journal</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Publication date</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Is ChatGPT able to generate scientific web links for accessing systematic reviews?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr style="background-color:#ccc"><td rowspan="5" colspan="1">Is ChatGPT able to generate a response quoting randomized controlled clinical trials concerning the given clinical scenario?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Scientific Article Title</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Authors</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Journal</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Publication date</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Is ChatGPT able to generate web links for accessing randomized controlled clinical trials?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr style="background-color:#ccc"><td rowspan="2" colspan="1">Is ChatGPT able to generate a response quoting expert guidelines from professional organizations concerning the given clinical scenario?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Publication date</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Is ChatGPT able to generate scientific web links for accessing expert guidelines from professional organizations?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Is ChatGPT able to give a very specific decision to implement for the given clinical scenario?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Is ChatGPT giving dental professionals enough suggestions to make independent professional decisions by fact-checking and analyzing the responses given?</td><td rowspan="1" colspan="1">Yes/No</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr><td colspan="2" rowspan="1">Total score</td><td rowspan="1" colspan="1">13</td><td rowspan="1" colspan="1">13</td><td rowspan="1" colspan="1">13</td><td rowspan="1" colspan="1">13</td><td rowspan="1" colspan="1">13</td></tr></tbody></table></table-wrap><p>This consistency is echoed in the C-GAM scores&#x000a0;detailed in Table <xref rid="TAB5" ref-type="table">5</xref>. With every scenario scoring 46.4% (13 out of 28), it is evident that ChatGPT-4's training was both thorough and effective. This consistent competency across diverse challenges suggests not only an extensive knowledge base in dental information but also the ability to effectively apply it to real-world clinical situations.</p><table-wrap position="float" id="TAB5"><label>Table 5</label><caption><title>ChatGPT-4 Generative Ability Matrix (C-GAM) with scores for five clinical scenarios</title><p>CS: clinical scenario</p></caption><table frame="hsides" rules="groups"><tbody><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Evaluation Category</td><td rowspan="1" colspan="1">Total Score Points (Sum of Maximum Criteria Scores)</td><td rowspan="1" colspan="1">CS 1</td><td rowspan="1" colspan="1">CS 2</td><td rowspan="1" colspan="1">CS 3</td><td rowspan="1" colspan="1">CS 4</td><td rowspan="1" colspan="1">CS 5</td></tr><tr><td rowspan="1" colspan="1">Ethics</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Accuracy</td><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">10</td></tr><tr><td rowspan="1" colspan="1">Relevance</td><td rowspan="1" colspan="1">12</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Efficiency</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Actionability</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Total scores (%)</td><td rowspan="1" colspan="1">28</td><td rowspan="1" colspan="1">13 (46.42%)</td><td rowspan="1" colspan="1">13 (46.42%)</td><td rowspan="1" colspan="1">13 (46.42%)</td><td rowspan="1" colspan="1">13 (46.42%)</td><td rowspan="1" colspan="1">13 (46.42%)</td></tr></tbody></table></table-wrap><p>Furthermore, ChatGPT-4 displayed better performance in prompts related to the ethical, accurate, and actionable principles of EBDM. All scenarios achieved the highest possible scores, indicating strong alignment with these crucial aspects. However, an interesting contrast emerges when examining prompts focusing on relevance and efficiency. Notably, all scenarios received zero scores on the C-GAM in this category. This suggests a potential area for further development, where balancing ethical considerations with practical execution and resource optimization may require additional training or refinement.</p><p>Since all five clinical scenarios yielded identical CRSS and C-GAM scores, indicating a consistent level of performance from ChatGPT-4, the study opted to forego further statistical analysis. This decision was based on the assumption that additional statistical tests would not provide significant insights beyond the already observed uniformity. However, further statistical analysis might be insightful if additional research with a larger clinical scenario sample size or different evaluation criteria is undertaken in the future.</p></sec><sec sec-type="discussion"><title>Discussion</title><p>To the best of our knowledge, this observational feasibility study represents the first&#x000a0;evaluation of ChatGPT-4, an LLM, as a tool for assisting dental professionals in EBDM for specific clinical scenarios. Given the nascent state of generative AI research, we developed a bespoke evaluation framework (CRSS and C-GAM) tailored to our specific research needs. We acknowledge that these criteria are open to critique and refinement by future investigators and researchers. Additionally, it is important to note that the CRSS and C-GAM scoring systems have not yet undergone formal scientific validation or testing. Our analysis revealed the following strengths and limitations of ChatGPT-4 in this context.</p><p>This observational feasibility study presents several notable strengths, particularly its pioneering exploration of&#x000a0;AI in EBDM in dentistry. As one of the first comprehensive evaluations of ChatGPT-4's capabilities in this domain, the research paves the way for future investigations and the integration of AI tools into clinical workflows. The study also employed a systematic and structured approach, utilizing standardized clinical scenarios, structured prompts, and custom evaluation frameworks (C-GAM and CRSS) to ensure consistency and objectivity throughout the evaluation process. Ethical considerations were paramount, as the research emphasized the importance of recognizing users as dental professionals and staying updated with current information, highlighting the ethical implications of AI in healthcare. Additionally, the study focused on practical clinical scenarios that reflect real-world dental practice, enhancing the applicability of its findings. By developing two unique metrics to evaluate ChatGPT-4's generative ability and response quality, the research provided a nuanced analysis of the AI's performance in terms of accuracy, relevance, and actionability. One key finding was ChatGPT-4's potential to significantly reduce the time required for accessing and summarizing evidence compared to traditional methods, demonstrating its value in streamlining clinical workflows. The AI exhibited strong performance in delivering ethical, accurate, and actionable responses while maintaining consistency across various clinical scenarios. By identifying both the strengths and limitations of ChatGPT-4, this study sets the stage for further advancements in AI technology, contributing valuable insights to the growing field of AI in healthcare and its application in evidence-based dentistry.</p><p>This observational feasibility study presents several potential biases that could affect its findings. One concern is selection bias in the clinical scenarios, as these were crafted by the researchers, which may lead to a preference for scenarios that highlight ChatGPT-4's strengths while avoiding more challenging areas where the AI might struggle. Additionally, there is a risk of confirmation bias from the researchers, who were involved in both creating the scenarios and evaluating the results, potentially leading to an unintentional favoring of ChatGPT-4's capabilities based on how the prompts and evaluation criteria were framed. Evaluation bias is another issue, as the scoring systems (CRSS and C-GAM) developed specifically for this study have not been validated and may reflect the researchers' expectations, resulting in biased conclusions regarding the AI's performance. Technological bias is also a concern, given that ChatGPT-4 is trained on a large dataset that may contain inherent biases from its sources, potentially skewing its responses and affecting the quality of evidence provided, particularly for under-researched populations in dental care. Furthermore, the study reports uniform scores across all scenarios, suggesting a performance uniformity bias that may indicate a lack of diversity in the scenarios chosen to challenge the AI adequately. Addressing these limitations and biases in future research could lead to a more robust evaluation of ChatGPT-4's potential in clinical decision-making by incorporating a wider range of scenarios, real-world testing, and independent validation of the scoring system.</p><p>The limitations of this observational feasibility study include several critical factors that may impact the findings. One significant issue is efficiency, as criteria 5, 7, and 9 assessed ChatGPT-4's ability to generate web links to relevant scientific publications. In all five scenarios, ChatGPT-4 failed to score any points due to its inability to provide these links. While it could generate homepage links for resources such as PubMed, Cochrane Library, and the ADA, this did not significantly expedite the process of finding specific publications. The investigator would still need to employ keywords and search strategies to locate the necessary articles, highlighting a major limitation in ChatGPT-4's generative capabilities.</p><p>Additionally, the AI's failure to provide titles, authors, journals, or publication dates for systematic reviews and expert guidelines further diminished its relevance as a tool for EBDM. In our study, the clinical scenarios and prompts were carefully designed based on a review of credible, evidence-based sources such as PubMed, Cochrane, and ADA guidelines to ensure their relevance to real-world dental decision-making. While we took steps to standardize the prompts and maintain consistency across scenarios, we recognize that formal validation methods, such as expert panel review, inter-rater reliability testing, or statistical validation of examiner agreement, were not conducted. To enhance objectivity, two independent examiners evaluated ChatGPT-4's responses using predefined scoring systems (CRSS and C-GAM), ensuring structured assessment. However, we acknowledge that future studies should incorporate additional validation measures, such as piloting the prompts before full-scale testing or employing statistical tools such as Cohen's Kappa to measure inter-examiner reliability.</p><p>The study also faced limitations related to its controlled environment and small sample size of scenarios. Conducted in a simulated setting with predetermined clinical scenarios, it lacked the real-world testing necessary to address unforeseen complexities in clinical decision-making. The narrow scope of only five scenarios limits the generalizability of the findings across a broader range of clinical situations in dentistry. Moreover, the reliance on structured prompts may have constrained the AI's performance, as real-life clinical questions are often more complex and ambiguous. Compounding these issues is ChatGPT-4's limited access to updated literature due to its training data being restricted to information available until January 2022. This temporal limitation could affect its relevance in fast-evolving fields like dentistry. Lastly, the absence of statistical validation due to uniform scores across scenarios may weaken the robustness of the findings, underscoring the need for future research to address these limitations for a more comprehensive evaluation of ChatGPT-4's potential in clinical decision-making.</p><p>These limitations suggest that ChatGPT-4, in its current state, may not be suitable for independent research or decision-making tasks. Further development is needed to address these shortcomings before it can be reliably used in such contexts. However, its potential for assisting research by generating text summaries, creating outlines, or suggesting search terms should not be underestimated. With continued advancement, ChatGPT-4 could become a valuable tool for researchers, but caution and critical evaluation remain imperative at this stage.</p><p>Based on the findings from our&#x000a0;study,&#x000a0;we propose several recommendations to&#x000a0;enhance the integration of AI&#x000a0;in dental practice and improve EBDM. Firstly, future AI models should be developed with real-time access to evidence-based databases such as PubMed, Cochrane, and ADA guidelines. This integration would significantly improve the accuracy and relevance of AI-generated responses, enhancing their utility in clinical settings. Additionally, AI developers should prioritize citation transparency, ensuring that responses are well-supported by credible scientific literature, thus bolstering the reliability of AI-assisted decision-making.&#x000a0;To enhance the evaluation of AI performance, we recommend refining clinical scenario standardization. Future studies should employ more specific and well-defined clinical cases that reflect real-world complexities, potentially utilizing expert validation processes like the Delphi method. Implementing statistical measures such as inter-rater agreement (Cohen's Kappa) would further enhance the reliability and consistency of AI performance assessment.</p><p>Our study introduced structured evaluation frameworks such as the CRSS&#x000a0;and C-GAM. While these metrics provided a systematic approach to assess AI performance, further validation is necessary to ensure their reliability. We also recommend comparative studies between ChatGPT-4 and other AI models to determine the most suitable tools for EBDM in dentistry.&#x000a0;Addressing ethical and practical considerations is crucial. Dental professionals should be trained to critically evaluate AI-generated responses and not rely solely on these tools for clinical decision-making. AI should be viewed as an aid rather than a replacement for human expertise. Moreover, developers should focus on improving bias detection mechanisms to ensure AI-generated recommendations are ethical, accurate, and free from misinformation.&#x000a0;Lastly, we emphasize the need for further research, including larger sample sizes and real-world clinical testing, to better understand the practical applications of AI in dental decision-making. Longitudinal studies are essential to assess the long-term impact of AI-assisted decision-making on patient outcomes, which will help refine AI tools and establish best practices for their integration into dental practice.</p><p>The potential of LLMs for evidence-based medicine tasks has garnered increasing attention in recent years. Several studies have explored their potential applications in various areas, offering valuable insights into their capabilities and limitations. A comprehensive scoping review published in 2024, titled "Assessing the research landscape and clinical utility of large language models: a scoping review," investigated the existing research on LLMs&#x000a0;in evidence-based medicine across diverse clinical fields. Park et al. recommended an optimistic yet cautious approach to integrating LLMs into clinical settings, emphasizing the need for further development to address challenges before widespread adoption [<xref rid="REF12" ref-type="bibr">12</xref>]. They highlighted that while conversational prowess and medical knowledge make LLMs like ChatGPT and Bard intriguing for clinical applications, their path to adoption is fraught with hurdles. Issues such as hallucinations, lack of transparency, and inconsistent outputs raise concerns about accuracy and patient safety. Ethically and legally, issues like patient consent, legal liability, and data privacy add another layer of complexity [<xref rid="REF12" ref-type="bibr">12</xref>].</p><p>Tang et al.&#x000a0;critically examined the strengths and weaknesses of LLMs like GPT-3.5 and ChatGPT in summarizing medical evidence across diverse clinical fields. They used both automated and human evaluations to assess summary quality, revealing a disconnect between automatic metrics and human judgment. Notably, they identified a range of error types specific to medical evidence summarization, highlighting the potential for LLMs to generate factually inaccurate, misleading, or uncertain summaries. Additionally, the study found that LLMs struggle to identify core information and become more prone to errors when summarizing longer texts [<xref rid="REF13" ref-type="bibr">13</xref>].</p><p>Van Veen&#x000a0;et al.&#x000a0;tested an LLM&#x000a0;for automatically summarizing research articles, finding it comparable to human summaries in brevity, accuracy, and key information retrieval. This suggests LLMs could significantly streamline systematic reviews by quickly screening articles, extracting findings, and potentially aiding in relevancy assessment. However, human review remains crucial to ensure completeness and capture nuanced details. Overall, LLMs show promising potential to revolutionize research synthesis in healthcare, demanding further research to optimize their impact on systematic review quality [<xref rid="REF14" ref-type="bibr">14</xref>].</p><p>Qureshi et al.'s investigation into ChatGPT's potential for systematic reviews reveals promising glimpses but ultimately concludes the technology remains insufficiently developed for such applications. While researchers report some helpful ChatGPT responses to SR-related prompts, they emphasize the tool's infancy and caution against its uncritical use by non-experts. Deceptive validity masks inaccuracies in much of the output, necessitating meticulous verification by human reviewers [<xref rid="REF15" ref-type="bibr">15</xref>]. While various researchers have expressed their views on ChatGPT's ability in scientific writing and its pros and cons [<xref rid="REF16" ref-type="bibr">16</xref>-<xref rid="REF18" ref-type="bibr">18</xref>], no prior research has specifically examined ChatGPT-4's capability in facilitating EBDM for clinical scenarios.</p></sec><sec sec-type="conclusions"><title>Conclusions</title><p>Our observational&#x000a0;feasibility&#x000a0;study highlights the potential of ChatGPT-4 for EBDM in dentistry. By leveraging its capabilities for information retrieval, knowledge synthesis, and decision support, ChatGPT-4 as an LLM can potentially streamline EBM workflows and improve clinical decision-making. ChatGPT-4 shows promise in supporting some aspects of EBDM for dental professionals. Its strengths in ethical awareness, EBDM process adherence, and information retrieval guidance demonstrate potential for valuable contributions. However, its limitations in efficiency and information provision about specific publications significantly hinder its ability to streamline the EBDM process.</p><p>Our research on ChatGPT-4's specific capabilities and limitations contributes valuable insights to this ongoing discussion, paving the way for the development of more powerful and reliable LLM-based tools for dental professionals' EBDM workflows. Ethical considerations must be carefully considered during the training of such models to ensure that clinicians make their own informed, evidence-based decisions rather than relying solely on the specific evidence statements provided by ChatGPT-4. Further training of ChatGPT could address its current limitations and unlock its full potential. Ultimately, the responsibility for making the final decision rests with the clinician.</p></sec></body><back><ack><p>I would like to extend my heartfelt gratitude to my son, Mr. Saakshar Bugude, for believing in his father's efforts.</p></ack><fn-group content-type="conflict"><title>Disclosures</title><fn fn-type="COI-statement"><p><bold>Human subjects:</bold> All authors have confirmed that this study did not involve human participants or tissue.</p><p><bold>Animal subjects:</bold> All authors have confirmed that this study did not involve animal subjects or tissue.</p><p><bold>Conflicts of interest:</bold> In compliance with the ICMJE uniform disclosure form, all authors declare the following:</p><p><bold>Payment/services info:</bold> All authors have declared that no financial support was received from any organization for the submitted work.</p><p><bold>Financial relationships:</bold> All authors have declared that they have no financial relationships at present or within the previous three years with any organizations that might have an interest in the submitted work.</p><p><bold>Other relationships:</bold> All authors have declared that there are no other relationships or activities that could appear to have influenced the submitted work.</p></fn></fn-group><fn-group content-type="other"><title>Author Contributions</title><fn fn-type="other"><p><bold>Concept and design:</bold>&#x000a0; Bugude Shiva Shankar, Sasankoti Mohan</p><p><bold>Acquisition, analysis, or interpretation of data:</bold>&#x000a0; Bugude Shiva Shankar, Sasankoti Mohan</p><p><bold>Drafting of the manuscript:</bold>&#x000a0; Bugude Shiva Shankar, Sasankoti Mohan</p><p><bold>Critical review of the manuscript for important intellectual content:</bold>&#x000a0; Bugude Shiva Shankar, Sasankoti Mohan</p><p><bold>Supervision:</bold>&#x000a0; Bugude Shiva Shankar, Sasankoti Mohan</p></fn></fn-group><ref-list><title>References</title><ref id="REF1"><label>1</label><element-citation publication-type="book"><article-title>A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955</article-title><person-group>
<name><surname>McCarthy</surname><given-names>J</given-names></name>
<name><surname>Minsky</surname><given-names>ML</given-names></name>
<name><surname>Rochester</surname><given-names>N</given-names></name>
<name><surname>Shannon</surname><given-names>CE</given-names></name>
</person-group><fpage>12</fpage><publisher-loc>Stanford, CA</publisher-loc><publisher-name>Stanford University</publisher-name><volume>2006</volume><year>2006</year><uri xlink:href="http://jmc.stanford.edu/articles/dartmouth/dartmouth.pdf">http://jmc.stanford.edu/articles/dartmouth/dartmouth.pdf</uri></element-citation></ref><ref id="REF2"><label>2</label><element-citation publication-type="journal"><article-title>Predicting the future&#x02014;big data, machine learning, and clinical medicine</article-title><source>N Engl J Med</source><person-group>
<name><surname>Obermeyer</surname><given-names>Z</given-names></name>
<name><surname>Emanuel</surname><given-names>EJ</given-names></name>
</person-group><fpage>1216</fpage><lpage>1219</lpage><volume>375</volume><year>2016</year><pub-id pub-id-type="pmid">27682033</pub-id>
</element-citation></ref><ref id="REF3"><label>3</label><element-citation publication-type="journal"><article-title>AI-based modeling: techniques, applications and research issues towards automation, intelligent and smart systems</article-title><source>SN Comput Sci</source><person-group>
<name><surname>Sarker</surname><given-names>IH</given-names></name>
</person-group><fpage>158</fpage><volume>3</volume><year>2022</year><pub-id pub-id-type="pmid">35194580</pub-id>
</element-citation></ref><ref id="REF4"><label>4</label><element-citation publication-type="journal"><article-title>Clinical application of artificial intelligence in PET imaging of head and neck cancer</article-title><source>PET Clin</source><person-group>
<name><surname>Gharavi</surname><given-names>SM</given-names></name>
<name><surname>Faghihimehr</surname><given-names>A</given-names></name>
</person-group><fpage>65</fpage><lpage>76</lpage><volume>17</volume><year>2022</year><pub-id pub-id-type="pmid">34809871</pub-id>
</element-citation></ref><ref id="REF5"><label>5</label><element-citation publication-type="journal"><article-title>Use of advanced artificial intelligence in forensic medicine, forensic anthropology and clinical anatomy</article-title><source>Healthcare</source><person-group>
<name><surname>Thurzo</surname><given-names>A</given-names></name>
<name><surname>Kosn&#x000e1;&#x0010d;ov&#x000e1;</surname><given-names>HS</given-names></name>
<name><surname>Kurilov&#x000e1;</surname><given-names>V</given-names></name>
<etal/>
</person-group><fpage>1545</fpage><volume>9</volume><year>2021</year><pub-id pub-id-type="pmid">34828590</pub-id>
</element-citation></ref><ref id="REF6"><label>6</label><element-citation publication-type="journal"><article-title>Dissecting racial bias in an algorithm used to manage the health of populations</article-title><source>Science</source><person-group>
<name><surname>Obermeyer</surname><given-names>Z</given-names></name>
<name><surname>Powers</surname><given-names>B</given-names></name>
<name><surname>Vogeli</surname><given-names>C</given-names></name>
<name><surname>Mullainathan</surname><given-names>S</given-names></name>
</person-group><fpage>447</fpage><lpage>453</lpage><volume>366</volume><year>2019</year><pub-id pub-id-type="pmid">31649194</pub-id>
</element-citation></ref><ref id="REF7"><label>7</label><element-citation publication-type="journal"><article-title>Human- versus artificial intelligence</article-title><source>Front Artif Intell</source><person-group>
<name><surname>Korteling</surname><given-names>JE</given-names></name>
<name><surname>van de Boer-Visschedijk</surname><given-names>GC</given-names></name>
<name><surname>Blankendaal</surname><given-names>RA</given-names></name>
<name><surname>Boonekamp</surname><given-names>RC</given-names></name>
<name><surname>Eikelboom</surname><given-names>AR</given-names></name>
</person-group><fpage>622364</fpage><volume>4</volume><year>2021</year><pub-id pub-id-type="pmid">33981990</pub-id>
</element-citation></ref><ref id="REF8"><label>8</label><element-citation publication-type="journal"><article-title>Language models are few-shot learners</article-title><source>Adv Neural Inf Process Syst</source><person-group>
<name><surname>Brown</surname><given-names>T</given-names></name>
<name><surname>Mann</surname><given-names>B</given-names></name>
<name><surname>Ryder</surname><given-names>N</given-names></name>
<etal/>
</person-group><fpage>1877</fpage><lpage>1901</lpage><volume>33</volume><year>2020</year></element-citation></ref><ref id="REF9"><label>9</label><element-citation publication-type="book"><article-title>The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World</article-title><person-group>
<name><surname>Domingos</surname><given-names>P</given-names></name>
</person-group><publisher-loc>New York (NY)</publisher-loc><publisher-name>Basic Books</publisher-name><year>2015</year><uri xlink:href="https://www.hachettebookgroup.com/titles/pedro-domingos/the-master-algorithm/9780465061921/?lens=basic-books">https://www.hachettebookgroup.com/titles/pedro-domingos/the-master-algorithm/9780465061921/?lens=basic-books</uri></element-citation></ref><ref id="REF10"><label>10</label><element-citation publication-type="webpage"><article-title>OpenAI models</article-title><date-in-citation content-type="access-date">
<month>5</month>
<year>2023</year>
</date-in-citation><year>2023</year><uri xlink:href="https://beta.openai.com/docs/models">https://beta.openai.com/docs/models</uri></element-citation></ref><ref id="REF11"><label>11</label><element-citation publication-type="journal"><article-title>Text generative artificial intelligence tools for clinical applications: scope and concerns</article-title><source>Ir J Med Sci</source><person-group>
<name><surname>Singh</surname><given-names>AV</given-names></name>
<name><surname>Singh</surname><given-names>A</given-names></name>
</person-group><fpage>1123</fpage><lpage>1124</lpage><volume>193</volume><year>2024</year><pub-id pub-id-type="pmid">37777678</pub-id>
</element-citation></ref><ref id="REF12"><label>12</label><element-citation publication-type="journal"><article-title>Assessing the research landscape and clinical utility of large language models: a scoping review</article-title><source>BMC Med Inform Decis Mak</source><person-group>
<name><surname>Park</surname><given-names>YJ</given-names></name>
<name><surname>Pillai</surname><given-names>A</given-names></name>
<name><surname>Deng</surname><given-names>J</given-names></name>
<name><surname>Guo</surname><given-names>E</given-names></name>
<name><surname>Gupta</surname><given-names>M</given-names></name>
<name><surname>Paget</surname><given-names>M</given-names></name>
<name><surname>Naugler</surname><given-names>C</given-names></name>
</person-group><fpage>72</fpage><volume>24</volume><year>2024</year><pub-id pub-id-type="pmid">38475802</pub-id>
</element-citation></ref><ref id="REF13"><label>13</label><element-citation publication-type="journal"><article-title>Evaluating large language models on medical evidence summarization</article-title><source>NPJ Digit Med</source><person-group>
<name><surname>Tang</surname><given-names>L</given-names></name>
<name><surname>Sun</surname><given-names>Z</given-names></name>
<name><surname>Idnay</surname><given-names>B</given-names></name>
<etal/>
</person-group><fpage>158</fpage><volume>6</volume><year>2023</year><pub-id pub-id-type="pmid">37620423</pub-id>
</element-citation></ref><ref id="REF14"><label>14</label><element-citation publication-type="journal"><article-title>Clinical text summarization: adapting large language models can outperform human experts [PREPRINT]</article-title><source>Res Sq</source><person-group>
<name><surname>Van Veen</surname><given-names>D</given-names></name>
<name><surname>Van Uden</surname><given-names>C</given-names></name>
<name><surname>Blankemeier</surname><given-names>L</given-names></name>
<etal/>
</person-group><fpage>2023</fpage><year>2023</year></element-citation></ref><ref id="REF15"><label>15</label><element-citation publication-type="journal"><article-title>Are ChatGPT and large language models "the answer" to bringing us closer to systematic review automation?</article-title><source>Syst Rev</source><person-group>
<name><surname>Qureshi</surname><given-names>R</given-names></name>
<name><surname>Shaughnessy</surname><given-names>D</given-names></name>
<name><surname>Gill</surname><given-names>KA</given-names></name>
<name><surname>Robinson</surname><given-names>KA</given-names></name>
<name><surname>Li</surname><given-names>T</given-names></name>
<name><surname>Agai</surname><given-names>E</given-names></name>
</person-group><fpage>72</fpage><volume>12</volume><year>2023</year><pub-id pub-id-type="pmid">37120563</pub-id>
</element-citation></ref><ref id="REF16"><label>16</label><element-citation publication-type="journal"><article-title>Artificial hallucinations in ChatGPT: implications in scientific writing</article-title><source>Cureus</source><person-group>
<name><surname>Alkaissi</surname><given-names>H</given-names></name>
<name><surname>McFarlane</surname><given-names>SI</given-names></name>
</person-group><fpage>0</fpage><volume>15</volume><year>2023</year></element-citation></ref><ref id="REF17"><label>17</label><element-citation publication-type="journal"><article-title>Can artificial intelligence help for scientific writing?</article-title><source>Crit Care</source><person-group>
<name><surname>Salvagno</surname><given-names>M</given-names></name>
<name><surname>Taccone</surname><given-names>FS</given-names></name>
<name><surname>Gerli</surname><given-names>AG</given-names></name>
</person-group><fpage>75</fpage><volume>27</volume><year>2023</year><pub-id pub-id-type="pmid">36841840</pub-id>
</element-citation></ref><ref id="REF18"><label>18</label><element-citation publication-type="journal"><article-title>Generative artificial intelligence: can ChatGPT write a quality abstract?</article-title><source>Emerg Med Australas</source><person-group>
<name><surname>Babl</surname><given-names>FE</given-names></name>
<name><surname>Babl</surname><given-names>MP</given-names></name>
</person-group><fpage>809</fpage><lpage>811</lpage><volume>35</volume><year>2023</year><pub-id pub-id-type="pmid">37142327</pub-id>
</element-citation></ref></ref-list></back></article>