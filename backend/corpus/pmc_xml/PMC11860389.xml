<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006499</article-id><article-id pub-id-type="pmc">PMC11860389</article-id><article-id pub-id-type="doi">10.3390/s25041270</article-id><article-id pub-id-type="publisher-id">sensors-25-01270</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Interpretable Probabilistic Identification of Depression in Speech</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3482-9215</contrib-id><name><surname>Ntalampiras</surname><given-names>Stavros</given-names></name><xref rid="fn1-sensors-25-01270" ref-type="author-notes">&#x02020;</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Friedrich</surname><given-names>Christoph M.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01270">Department of Computer Science, University of Milan, 20133 Milan, Italy; <email>stavros.ntalampiras@unimi.it</email></aff><author-notes><fn id="fn1-sensors-25-01270"><label>&#x02020;</label><p>Current address: via Celoria 18, 20133 Milan, Italy.</p></fn></author-notes><pub-date pub-type="epub"><day>19</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1270</elocation-id><history><date date-type="received"><day>16</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>17</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>17</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the author.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Mental health assessment is typically carried out via a series of conversation sessions with medical professionals, where the overall aim is the diagnosis of mental illnesses and well-being evaluation. Despite its arguable socioeconomic significance, national health systems fail to meet the increased demand for such services that has been observed in recent years. To assist and accelerate the diagnosis process, this work proposes an AI-based tool able to provide interpretable predictions by automatically processing the recorded speech signals. An explainability-by-design approach is followed, where audio descriptors related to the problem at hand form the feature vector (Mel-scaled spectrum summarization, Teager operator and periodicity description), while modeling is based on Hidden Markov Models adapted from an ergodic universal one following a suitably designed data selection scheme. After extensive and thorough experiments adopting a standardized protocol on a publicly available dataset, we report significantly higher results with respect to the state of the art. In addition, an ablation study was carried out, providing a comprehensive analysis of the relevance of each system component. Last but not least, the proposed solution not only provides excellent performance, but its operation and predictions are transparent and interpretable, laying out the path to close the usability gap existing between such systems and medical personnel.</p></abstract><kwd-group><kwd>medical acoustics</kwd><kwd>audio pattern recognition</kwd><kwd>universal background modeling</kwd><kwd>hidden Markov models</kwd><kwd>mental health</kwd><kwd>depression</kwd><kwd>explainable AI</kwd><kwd>interpretable AI</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01270"><title>1. Introduction</title><p>Unfortunately, there is a significant part of the population that suffers from mental health disorders, resulting in a substantial decrease in their quality of life&#x000a0;[<xref rid="B1-sensors-25-01270" ref-type="bibr">1</xref>]. At the same time, such illnesses are associated with a considerable economic impact, which is higher than cancer, cardiovascular, and respiratory diseases&#x000a0;[<xref rid="B2-sensors-25-01270" ref-type="bibr">2</xref>]. However, governmental budgets are typically higher for such diseases with respect to mental illnesses. At present, the diagnosis of such conditions is first and foremost based on reports produced either by the subjects themselves, i.e., self-assessment, or their caregivers. More often than not, such reports are subjective and biased, increasing the probability of an erroneous condition assessment&#x000a0;[<xref rid="B3-sensors-25-01270" ref-type="bibr">3</xref>]. Importantly, there is a great need to objectify such a diagnosis process while making it available over time so that subjects can be treated on a regular basis and receive evidence-based therapy, especially where such psychological services are limited or even nonexistent. Such periodic and methodological assessments would result in improved therapeutics, decreasing the associated negative consequences. The diagnostic procedure clearly depends significantly on the availability of skilled professionals and their specialized knowledge in the particular mental health condition. Consequently, it becomes a time-intensive process accompanied by an excessively high cost.</p><p>A different diagnostic tool that alleviates these constraints involves the development of audio pattern recognition systems that analyze speech signals and draw inferences about the individual&#x02019;s health status. Certainly, the objective is not to substitute for the expertise of a medical professional but rather to create a supportive tool that can offer an initial diagnosis and, potentially, expedite the assessment process. Various modalities have been used for identifying depression, such as visual, EEG, etc.&#x000a0;[<xref rid="B4-sensors-25-01270" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01270" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01270" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-01270" ref-type="bibr">7</xref>], while this work focuses exclusively on the analysis of speech signals.</p><p>Among the various mental disorders, there are several works concentrating on processing speech for detecting depression&#x000a0;[<xref rid="B8-sensors-25-01270" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01270" ref-type="bibr">9</xref>]. There, both handcrafted and features learned automatically in conjunction with generative and discriminative classifiers have been employed&#x000a0;[<xref rid="B10-sensors-25-01270" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01270" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01270" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01270" ref-type="bibr">13</xref>]. A systematic review is out of the scope of this work, while the interested reader may consult the systematic survey presented in&#x000a0;[<xref rid="B3-sensors-25-01270" ref-type="bibr">3</xref>]. By collectively analyzing the related literature, several interesting observations can be made regarding existing gaps: 1.&#x000a0;there is not a standardized experimental protocol permitting reproducible research and reliable comparison of different approaches; 2. temporal modeling approaches have not been systematically considered; 3. interpretation of the model predictions has not been thoroughly considered by the research community; and 4. the majority of the existing approaches consider small-sized datasets, where overfitting may occur and limit their generalization across corpora of diverse characteristics.</p><p>This work argues that, given the increased difficulty of the task&#x02014;where even medical experts need several sessions/weeks to provide a secure assessment&#x02014;along with the limited data availability, automated end-to-end feature learning does not comprise a feasible path yet. As such, and considering the need for fully interpretable predictions, handcrafted features are designed to capture specific properties of the signal&#x02019;s structure. Unfortunately, explainability has not been systematically explored in such a critical scenario, let alone the interaction between the AI system and medical experts. In addition, in detecting speech patterns indicative of healthy/depressed subjects, it is of paramount importance to take into account their evolving structure. This article covers the aforementioned gaps via a pattern modeling scheme explicitly encoding existing temporal dependencies while offering fully interpretable predictions aimed at closing the usability loop present between AI-based systems and medical experts.</p><p>We propose to use a multifaceted feature set combined with a universal data modeling approach. More specifically, the feature set is composed of (a) Mel-Frequency Cepstral Coefficients (MFCC), (b) Teager Energy Operator (TEO), and (c) Fundamental frequency and harmonic ratio.</p><p>The first one is the log Mel-scaled spectrogram summarized using the discrete cosine transform. The second one consists of areas included under the TEO autocorrelation envelope extracted after a critical band-based analysis, as described in&#x000a0;[<xref rid="B14-sensors-25-01270" ref-type="bibr">14</xref>]. Last but not least, the third set captures the periodic nature of the available speech signals. As such, these features capture complementary characteristics of the audio structure and their combined use is be beneficial for identification of depression in speech. These feature sets are juxtaposed and fed to a universal probabilistic modeling scheme based on Hidden Markov Models (HMM).</p><p>Initially, a universal HMM is constructed by employing a balanced subset of the training data, including recordings coming from both classes. At a second stage, using the remaining data of each class, we create adapted versions of the universal model to represent each class. Thus, the class(es) use as a basis a model whose parameters have been learned using a larger amount of data and, as such, comprise finer estimations with respect to using a more limited set. Importantly, we designed a novel data selection scheme, taking into account the perceptual nature of the present problem for determining the model training and adaptation data. This work employs the publicly available Androids dataset&#x000a0;[<xref rid="B15-sensors-25-01270" ref-type="bibr">15</xref>], which has been specifically designed for capturing depression in speech and presents several advantages over existing ones, i.e., it was annotated by professional psychiatrists without considering self-assessment questionnaires, it was recorded at the location where patients are actually treated, and it comes with a standardized experimental protocol.</p><p>Last but not least, the proposed approach was built considering explainability by design, where its operation is fully transparent and takes into account the requirements of such a medical setting while being able to interact with the medical experts. Such a design paradigm examines and addresses problem-related characteristics without seeking to decipher the operation of black-box models at a post-evaluation stage.</p><p>The rest of this work is organized as follows: <xref rid="sec2-sensors-25-01270" ref-type="sec">Section 2</xref> formalizes the problem, while <xref rid="sec3-sensors-25-01270" ref-type="sec">Section 3</xref> describes in detail the proposed pipeline, focusing on the feature extraction and pattern recognition algorithms. Subsequently, <xref rid="sec4-sensors-25-01270" ref-type="sec">Section 4</xref> thoroughly examines the efficacy of the proposed solution and contrasts it with existing solutions. <xref rid="sec5-sensors-25-01270" ref-type="sec">Section 5</xref> underlines the interpretation capabilities of the present solution, while <xref rid="sec6-sensors-25-01270" ref-type="sec">Section 6</xref> reports and comments on the most important findings along with future research directions.</p></sec><sec id="sec2-sensors-25-01270"><title>2. Problem Formalization</title><p>This work assumes the availability of a corpus <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> encompassing single-channel speech recordings belonging to two classes, i.e., class dictionary <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:math></inline-formula> = {<italic toggle="yes">Healthy</italic>, <italic toggle="yes">Depressed</italic>}. Moreover, as per the audio pattern recognition literature&#x000a0;[<xref rid="B16-sensors-25-01270" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01270" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01270" ref-type="bibr">18</xref>], we assume that such mental state associated speech patterns follow consistent, yet unknown probability density functions denoted as <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. We further assume that at each time instance, there is one dominant mental state. Importantly, this work does not assume <italic toggle="yes">a priori</italic> availability of subject-specific knowledge. The final goal is to predict the mental state expressed in novel speech recordings while considering a subject-independent experimental protocol.</p></sec><sec id="sec3-sensors-25-01270"><title>3. The Proposed Solution</title><p>This section presents the pipeline of the proposed methodology, describing the two fundamental processing stages, i.e., feature extraction and audio pattern recognition.</p><sec id="sec3dot1-sensors-25-01270"><title>3.1. Feature Extraction</title><p>Towards capturing diverse aspects of the available speech signals, we employed three feature sets, as described in the following paragraphs.</p><sec id="sec3dot1dot1-sensors-25-01270"><title>3.1.1. Mel-Frequency Cepstral Coefficients</title><p>The initial set of features is sourced from the speech/speaker recognition paradigm&#x000a0;[<xref rid="B19-sensors-25-01270" ref-type="bibr">19</xref>], and it has demonstrated effectiveness in various generalized sound classification tasks, encompassing the medical acoustics field&#x000a0;[<xref rid="B20-sensors-25-01270" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-01270" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01270" ref-type="bibr">22</xref>]. MFCCs serve as a condensed representation of the spectrogram. To derive these coefficients, the spectrogram undergoes a process where it is initially filtered through the Mel-filterbank to align the frequency scale more closely with the human hearing system. Subsequently, the log operator is applied to appropriately distribute the data, and the information is ultimately condensed through the Discrete Cosine Transform. The first 13 coefficients are kept along with their velocity and acceleration coefficients since their evolution over time could be significant when processing speech signals&#x000a0;[<xref rid="B23-sensors-25-01270" ref-type="bibr">23</xref>].</p></sec><sec id="sec3dot1dot2-sensors-25-01270"><title>3.1.2. Teager Energy Operator Autocorrelation Envelope</title><p>The second feature set is derived by processing the speech signals by means of the Teager energy operator&#x000a0;[<xref rid="B14-sensors-25-01270" ref-type="bibr">14</xref>]. It is tailored to handle speech generated in stressful conditions by effectively modeling the related airflow patterns. Consequently, it incorporates information that may not be encompassed by MFCCs and could prove valuable in analyzing speech indicative of a depressive state, where there may be a significant impact from psychological stress&#x000a0;[<xref rid="B24-sensors-25-01270" ref-type="bibr">24</xref>].</p><p>To obtain the autocorrelation envelope using TEO, the audio signal undergoes an initial filtering process using sixteen critical bands (see <xref rid="sensors-25-01270-t001" ref-type="table">Table 1</xref>). Following that, the area covered by the autocorrelation is computed for each frame and then divided by half of the frame length for normalization purposes. The resulting dimensionality is sixteen, corresponding to the number of critical bands.</p></sec><sec id="sec3dot1dot3-sensors-25-01270"><title>3.1.3. Fundamental Frequency/Pitch and Harmonic Ratio</title><p>These features capture periodicity characteristics exhibited in the available speech signals. Their calculation is carried out using the normalized autocorrelation function&#x000a0;[<xref rid="B25-sensors-25-01270" ref-type="bibr">25</xref>], and they have been effective in various speech processing applications, including stress analysis&#x000a0;[<xref rid="B26-sensors-25-01270" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01270" ref-type="bibr">27</xref>], especially in characterizing voiced/unvoiced frames, which may provide useful insights in the present application scenario.</p><p>Overall, the structure of the log-mel spectrogram presents presents notable variations over time, while the respective healthy structure exhibits constant behavior. Moreover, the healthy speech includes considerable energy in the higher frequency bands which is well reflected in the values of the area under the Teager Energy Operator autocorrelation envelope.</p></sec></sec><sec id="sec3dot2-sensors-25-01270"><title>3.2. Universal Probabilistic Model</title><p>This section describes the audio pattern recognition stage, which is based on the universal model denoted as <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula>, trained on a balanced subset of the training set. Once <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula> is available, the adaption process follows, where adapted versions of <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula> are constructed, each one representing a specific class. As such, potential class imbalances have only a minor impact on the final model since <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula>&#x02019;s parameters are estimated on a plethora of data. On top of that, the class-specific models include information representing both the target class and the rest, which otherwise would have remained unseen&#x000a0;[<xref rid="B28-sensors-25-01270" ref-type="bibr">28</xref>].</p><p>In this work, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula> is fed with balanced subpopulations of the training data so that there is no bias favoring one or more classes, thus avoiding overfitting. Thus, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula> estimates the distribution of the entire feature space, including both speech classes. Class-specific versions of <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula> are produced via Maximum-a-posteriori adaptation, as shown in <xref rid="sensors-25-01270-f001" ref-type="fig">Figure 1</xref>. As such, the well-learned parameters are suitably adapted to the characteristics of each speech class. In the following, we describe the HMM, the Maximum-a-posteriori adaptation, the sample selection scheme, and the speech depression classification algorithm.</p></sec><sec id="sec3dot3-sensors-25-01270"><title>3.3. Hidden Markov Model</title><p>Universal and class-specific models are HMMs, which are composed of the following elements:<list list-type="bullet"><list-item><p>the number of states <italic toggle="yes">S</italic>,</p></list-item><list-item><p>the PDF&#x02019;s approximating states&#x02019; distribution via a Gaussian mixture model (GMM), <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are the weights associated with each mixture, <italic toggle="yes">x</italic> is the feature vector, <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> Gaussian mixture, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>the state transition probability matrix <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where entry <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> comprises the probability of moving from state <italic toggle="yes">j</italic> at time <italic toggle="yes">t</italic> to state <italic toggle="yes">i</italic> at time <italic toggle="yes">t + 1</italic>, and</p></list-item><list-item><p>the initial state distribution <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c0;</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is the probability that HMM starts in state <italic toggle="yes">i</italic>, i.e., <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list></p><p>Training is performed via the Baum-Welch algorithm based on an Expectation Maximization logic &#x000a0;[<xref rid="B29-sensors-25-01270" ref-type="bibr">29</xref>]. During testing, the Viterbi algorithm&#x000a0;[<xref rid="B30-sensors-25-01270" ref-type="bibr">30</xref>] identifies the most probable series of HMM states.</p></sec><sec id="sec3dot4-sensors-25-01270"><title>3.4. Correlation-Based <italic toggle="yes">k</italic>-Medoids for Selecting Training and Adaptation Data</title><p>In such universal modeling schemes, the selection of training and adaptation data comprises a relevant aspect that may affect modeling efficiency&#x000a0;[<xref rid="B31-sensors-25-01270" ref-type="bibr">31</xref>]. Thus, this work employs a data organization scheme considering both inter- and intra-class similarities/dissimilarities as assessed by a multidimensional correlation. As such, we used the <italic toggle="yes">k-mediods clustering algorithm</italic>, which unlike the standard <italic toggle="yes">k</italic>-means, is able to incorporate generic pairwise similarity metrics, decreasing the influence of outliers&#x000a0;[<xref rid="B32-sensors-25-01270" ref-type="bibr">32</xref>]. Motivated by the findings in the sound recognition field&#x000a0;[<xref rid="B33-sensors-25-01270" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-01270" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-01270" ref-type="bibr">35</xref>], we employed Canonical Correlation Analysis (CCA) to quantify the distance between feature vectors representing mental health status as captured from speech signals. Unlike GMM-based representations&#x000a0;[<xref rid="B36-sensors-25-01270" ref-type="bibr">36</xref>], correlation-based metrics may be more meaningful from a perceptual point of view.</p><p>More in detail, the distance measure between feature vectors <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, extracted from speech samples <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic>, respectively, is the canonical correlation. CCA provides two&#x000a0;projection planes, i.e., <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="script">R</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="script">R</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">d</italic> is the dimensionality of the feature vectors and where the canonical correlation <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:math></inline-formula> is maximized:<disp-formula id="FD1-sensors-25-01270"><label>(1)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>It is important to note that <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is independent of the scaling of <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> nor <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>; thus, computing <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:math></inline-formula> can be formalized as follows:<disp-formula id="FD2-sensors-25-01270"><label>(2)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
subject to<disp-formula id="FD3-sensors-25-01270"><label>(3)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Vectors <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> can be computed by solving the next optimization problem&#x000a0;[<xref rid="B37-sensors-25-01270" ref-type="bibr">37</xref>]<disp-formula id="FD4-sensors-25-01270"><label>(4)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
subject to<disp-formula id="FD5-sensors-25-01270"><label>(5)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Thus, we get <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which is inversely analogous to the distance between <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> used in the <italic toggle="yes">k-mediods clustering algorithm</italic> implemented using the Partitioning Around Medoids&#x000a0;[<xref rid="B38-sensors-25-01270" ref-type="bibr">38</xref>]. At the same time, such a distance is symmetric. Finally, the most centric speech samples of each class are used for training the Universal HMM; the remaining ones are used during the adaptation process.</p></sec><sec id="sec3dot5-sensors-25-01270"><title>3.5. Maximum A Posteriori Adaptation (MAP)</title><p><italic toggle="yes">MAP</italic> was employed to adapt <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula> using data representing specific classes. Interestingly, <italic toggle="yes">MAP (a)</italic> emphasizes model components that explain the adaptation data, and <italic toggle="yes">(b)</italic> de-emphasizes the components that do not&#x000a0;[<xref rid="B39-sensors-25-01270" ref-type="bibr">39</xref>].</p><p>Such a two-fold process is performed by updating the means of every Gaussian component. Thus, the model is not confused when processing data belonging to other classes since de-emphasized components will be triggered outputting low log-likelihoods. <italic toggle="yes">MAP</italic> is implemented following the mathematically tractable adaptation scheme described in&#x000a0;[<xref rid="B39-sensors-25-01270" ref-type="bibr">39</xref>]. An illustrative example of the way <italic toggle="yes">MAP</italic> updates component <italic toggle="yes">k</italic> of <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula> using data observations <italic toggle="yes">R</italic> is demonstrated in <xref rid="sensors-25-01270-f001" ref-type="fig">Figure 1</xref>.</p><p>During MAP adaptation, we adopted the process described in&#x000a0;[<xref rid="B39-sensors-25-01270" ref-type="bibr">39</xref>], where conjugate informative priors are <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula>&#x02019;s parameters. Accordingly, a mathematically manageable adaptation scheme is developed, where each mean <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math></inline-formula> in <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula> is updated using the following formula:<disp-formula id="FD6-sensors-25-01270"><label>(6)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>&#x003bc;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:msubsup><mml:mover accent="true"><mml:mi>&#x003bc;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:msubsup><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">s</italic> denotes <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula>&#x02019;s state, <italic toggle="yes">k</italic> the Gaussian component, <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:math></inline-formula> a weighting factor, and <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003bc;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> the mean of the parameters of the adaptation data. <italic toggle="yes">N</italic> represents the occupation likelihood with respect to the adaptation data and is calculated as follows:<disp-formula id="FD7-sensors-25-01270"><label>(7)</label><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">R</italic> is a given part of <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> reserved for adaptation purposes with length <italic toggle="yes">T</italic>. The average <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003bc;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> used in Equation (<xref rid="FD6-sensors-25-01270" ref-type="disp-formula">6</xref>) characterizing the adaptation data is calculated as follows:<disp-formula id="FD8-sensors-25-01270"><label>(8)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>&#x003bc;</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:munderover></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover></mml:mstyle><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover></mml:mstyle><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> corresponds to the available adaptation for observation coming from sequence <italic toggle="yes">r</italic> at time <italic toggle="yes">t</italic>.</p><p>As such, if case <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> results in a high value, mixture <italic toggle="yes">k</italic> will be significantly revised via <italic toggle="yes">MAP</italic>. On the contrary, the respective <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math></inline-formula> will not change.</p></sec><sec id="sec3dot6-sensors-25-01270"><title>3.6. Classification of Speech Depression</title><p>Initially, <italic toggle="yes">MAP</italic> is applied in order to obtain the class-specific models corresponding to healthy and patient classes. Subsequently, classification is performed as in Algorithm&#x000a0;1. During the classification phase, the inputs are the class-specific models <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">M</mml:mi><mml:mi>h</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">M</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, and the unknown audio signal <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and the outputs are the probability vector <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, maximum probability <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mi>&#x003b6;</mml:mi></mml:mrow></mml:math></inline-formula>, predicted class, and confidence score <italic toggle="yes">c</italic>. Let us note that <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">M</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the model that was trained on patients, while <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">M</mml:mi><mml:mi>h</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the model representing healthy subjects. Initially, the algorithm extracts the feature set <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> (line 1, Algorithm 1). Next, the probability vector of <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is initialized (line 2, Algorithm 1). In the following, a <italic toggle="yes">for</italic> loop is executed where <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is tested against the class-specific models <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">M</mml:mi><mml:mi>h</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">M</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> outputting the corresponding log-likelihoods to populate <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (lines 3&#x02013;4, Algorithm 1). The maximum log-likelihood <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mi>&#x003b6;</mml:mi></mml:mrow></mml:math></inline-formula> is identified, and the winning class determined (line 5, Algorithm 1). Finally, the confidence score <italic toggle="yes">c</italic> is returned (line 6, Algorithm 1).
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1:</bold> The speech classification algorithm.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-graphic xlink:href="sensors-25-01270-i001.jpg"/></td></tr></tbody></array></p></sec></sec><sec id="sec4-sensors-25-01270"><title>4. Experimental Protocol and Analysis of the Results</title><p>This section extensively analyzes the (a) employed dataset and figures of merit, (b) parameterization of the proposed approach, (c) experimental results and analysis, (d) model generalization capabilities, and (e) ablation study.</p><sec id="sec4dot1-sensors-25-01270"><title>4.1. The Employed Dataset</title><p>This work employs the Androids dataset [<xref rid="B15-sensors-25-01270" ref-type="bibr">15</xref>], which is the only publicly available dataset specifically designed for detecting depression and being thoroughly labeled by medical experts. The corpus includes 228 recordings of 118 native Italian speakers, 64 of whom have been diagnosed with depression. Interestingly, the dataset considers two tasks, i.e., <italic toggle="yes">reading</italic> and <italic toggle="yes">spontaneous</italic> (as captured during an interview) speech. The available recordings last more than 1 h and 30 min for the first task and approximately 7 h and 30 min for the second. The duration associated with female subjects is longer, which reflects the fact that females exhibit depression more frequently than males [<xref rid="B40-sensors-25-01270" ref-type="bibr">40</xref>]. The dataset comprises 228 recordings, with 122 originating from depressed participants and 106 from non-depressed participants. The average participants&#x02019; age is 47 years, and they all have a similar education level. Such variations are equally considered in both healthy and control subjects so that differences exhibited in their speech patterns can be related to their mental state. Last but not least, the corpus comes with a standardized 5-fold experimental protocol facilitating the reproduction and comparison of competing methodologies. Moreover, the following figures of merit are used: <italic toggle="yes">accuracy</italic>, <italic toggle="yes">precision</italic>, <italic toggle="yes">recall</italic>, and F1 score. More detailed information is available in [<xref rid="B15-sensors-25-01270" ref-type="bibr">15</xref>].</p></sec><sec id="sec4dot2-sensors-25-01270"><title>4.2. Parameterization of the Proposed Solution</title><p>The dataset is sampled at 16kHz, and we set the frame and hop size equal to 30 ms and 10 ms, respectively. During the learning of HMMs, the maximum number of iterations with respect to <italic toggle="yes">k</italic>-means initialization, EM, and Baum-Welch algorithms were 50, 25, and 25, respectively [<xref rid="B41-sensors-25-01270" ref-type="bibr">41</xref>]. Sets <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mn>7</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn><mml:mo>,</mml:mo><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mn>64</mml:mn><mml:mo>,</mml:mo><mml:mn>128</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> were exhaustively searched in order to find the combination of the number of states and Gaussian components providing the maximum recognition accuracy on a validation set equal to 10% of the training ones.</p></sec><sec id="sec4dot3-sensors-25-01270"><title>4.3. Experimental Results and Analysis</title><p>This subsection reports the depression identification results, adopting the standardized figures of merit in a subject-independent setting. We first present the experimental results obtained with respect to the <italic toggle="yes">Reading Task</italic> and then proceed to the <italic toggle="yes">Interview Task</italic>.</p><p><xref rid="sensors-25-01270-t002" ref-type="table">Table 2</xref> includes the results with respect to the <italic toggle="yes">Reading Task</italic>, while the highest rates per figure of merit are emboldened. After the optimization phase, the best-performing UBM-HMM encompassed 3 states, each modeled by 16 Gaussian mixtures, while making use of the proposed data selection scheme. Here, the feature set included the MFCCs along with velocity and acceleration coefficients. The remaining features did not offer improved performance. Such a probabilistic scheme reached an excellent F1 score of 96.2 &#x000b1; 1.1%. Importantly, the proposed probabilistic scheme significantly outperforms the existing approaches based on Support Vector Machine (SVM) and Long short-term memory (LSTM, one hidden layer with 32 hidden states) models with respect to every figure of merit. The achieved precision (99.1 &#x000b1; 0.7%) is higher than recall (93.5 &#x000b1; 0.9%). As such, the system&#x02019;s depression identifications are reliable, and, at the same time, actual positives were identified correctly. It should be stressed that such a performance is achieved with a relatively simple model of 3 states reflecting the number of stationarity changes exhibited in the available data.</p><p>The topologies, including the transition probabilities of the constructed HMMs, are illustrated in <xref rid="sensors-25-01270-f002" ref-type="fig">Figure 2</xref>. We see that fully connected HMMs were employed, favoring same-state transitions. It is evident that same-state transition probabilities are higher in speech associated with depression, showing that the respective patterns remain constant for a longer duration compared to healthy speech, as generally expressed by a control subject. In addition, the evolution of the best-performing features does not follow the typical left-right topology in speech recognition, reflecting the different nature of the present problem. We argue that HMM-based modeling of Mel-scaled spectrum summarization, along with its variation over time, comprises a particularly effective way of identifying depression in speech captured during the reading task.</p><p>The obtained results with respect to the <italic toggle="yes">Interview Task</italic> are tabulated in <xref rid="sensors-25-01270-t003" ref-type="table">Table 3</xref>. The optimization based on the validation set yields an ergodic UBM-HMM fed with MFCCs and their velocities, TEO autocorrelation area, fundamental frequency, and harmonic ratio. In this case, the models consist of 7 states and 4 Gaussian components. We observe that the proposed method outperforms the state of the art based on LSTM with respect to all considered figures of merit, reaching an F1-score of 88.9 &#x000b1; 1.7%. In addition, the achieved recall (92.3 &#x000b1; 1.8) is higher than precision (85.8 &#x000b1; 1.9), meaning that the proposed scheme performs well in identifying true positives, while depression identification is not as reliable as during the <italic toggle="yes">Reading Task</italic>. Such a performance behavior is expected to a certain extent since speech recorded during the interviews is spontaneous and exhibits highly non-stationary characteristics when compared to speech recorded during reading. In fact, such a modeling scheme offers important insights into understanding depression classification; additional features and a greater number of states were required to boost the performance, which, nonetheless, did not reach the previous level. The higher number of states (7 vs. 3) implies significantly stronger non-stationary phenomena, clearly demonstrating that the <italic toggle="yes">Interview Task</italic> presents increased difficulty during the modeling phase.</p><p>Furthermore, in <xref rid="sensors-25-01270-f002" ref-type="fig">Figure 2</xref>, we observe the fully connected HMM topology and the associated transition probabilities. We see that same-state transitions are still favored; however, the respective probabilities are lower compared to the reading task, which is expected in spontaneous speech, where the exhibited patterns may change at a higher pace. Overall, we argue that the results offered by combining features that capture complementary information along with the universal background modeling scheme are more than satisfactory and provide valuable insights for the problem at hand.</p><p>The code to reproduce the experiments is available at <uri xlink:href="https://sites.google.com/site/stavrosntalampiras/">https://sites.google.com/site/stavrosntalampiras/</uri>, accessed on 16 February 2025.</p></sec><sec id="sec4dot4-sensors-25-01270"><title>4.4. Testing Model Generalization Capabilities Across Tasks</title><p>During the present experimental phase, we examined the generalization capabilities of the best-performing model in a given task over the remaining one. The respective results are presented in <xref rid="sensors-25-01270-t004" ref-type="table">Table 4</xref>. There, we observe that the model trained on data coming from the <italic toggle="yes">Reading Task</italic> does not generalize as well as the model trained on <italic toggle="yes">Interview Task</italic> data. The rates associated with the <italic toggle="yes">Interview Task</italic> are lower than the respective ones when the same feature set is fed to a larger model. Furthermore, the results obtained in the <italic toggle="yes">Reading Task</italic> are generally higher but present a strong unbalance in precision and recall rates. At the same time, it does not reach the performance level provided by the best-performing model in the <italic toggle="yes">Reading Task</italic>, which may indicate that the additional features capture redundant information while the model includes an unnecessarily large number of states to explain the feature distribution and evolution for the specific task. Overall, the achieved results show that the constructed models are able to generalize up to a certain extent, even though there are relevant differences in the speech data distributions associated with these tasks; as such, transfer learning technologies may be beneficial [<xref rid="B42-sensors-25-01270" ref-type="bibr">42</xref>,<xref rid="B43-sensors-25-01270" ref-type="bibr">43</xref>].</p></sec><sec id="sec4dot5-sensors-25-01270"><title>4.5. Ablation Study</title><p>This section alters certain components of the proposed approach in order to not only understand their effect on performance but also provide meaningful insights regarding the problem at hand. During the first stage, we examine the influence of the considered features and modeling with respect to the <italic toggle="yes">Reading Task</italic>. <xref rid="sensors-25-01270-t005" ref-type="table">Table 5</xref> includes the results achieved with a reduced and augmented feature set, i.e., starting from the sole MFCCs up to MFCCs with their velocity and acceleration, along with the TEO, F0, and HR features. We observe a considerable increase in the reported values since complementary information is captured at each each stage. However, the addition of TEO, F0, and HR to the best-performing feature set (see <xref rid="sensors-25-01270-t002" ref-type="table">Table 2</xref>) reduced the overall scores, which implies that they do not offer further distinctive information with respect to the present problem. Indeed, stress-capturing features are more relevant in the second task, where the presence of such patterns is evident. It is worth noting that the MFCCs alone achieve a rather robust performance (91.3 &#x000b1; 2.1%), which may be attributed to the powerful modeling scheme as well. In fact, class-specific HMMs, i.e., without the universal modeling, are able to reach rates similar to those reported in the related literature, confirming the suitability of such a generative classification scheme. We further assessed the effect of topology and data selection scheme. We see that HMMs restricted to left-right transitions provide considerably poorer performance, which shows that such a connection logic does not reflect well the characteristics of the present problem. Moreover, we examined the effectiveness of the proposed data selection scheme and contrasted it with a clustering-based one [<xref rid="B36-sensors-25-01270" ref-type="bibr">36</xref>]. Interestingly, such a data selection scheme offered improved balance across precision and recall rates. However, it did not surpass the proposed scheme, which takes into account correlation-based distances.</p><p>The second stage of the ablation study is focused on the <italic toggle="yes">Interview Task</italic>. We examined both the composition of the feature set and the parameterization of the HMMs. The respective results are shown in <xref rid="sensors-25-01270-t006" ref-type="table">Table 6</xref>. There, we see that the mere use of MFCCs does not address the problem in an effective way, reaching an F1-score of 66.7 &#x000b1; 5.1%. Considering the velocity coefficient significantly improves the performance (78.3 &#x000b1; 3.8%), while the TEO information boosts it further, confirming the hypothesis that stress-related characteristics play an important role during the specific task. At the same time, it becomes clear that the selected features carry complementary information strongly related to the problem at hand. In addition, we assessed the effect of the universal modeling scheme, topology, as well as the train/adaptation data selection process. We see the class-specific HMMs reach a smaller F1-score (75.8 &#x000b1; 4.3%); the left-right topology is unsuitable for modeling depression in speech, while the clustering-based data selection scheme offers satisfactory performance. Overall, the proposed fully-connected UBM-HMM with correlation-based data selection provides robust performance. Importantly, such a model encodes knowledge characterizing both classes and is able to emphasize and de-emphasize components associated with a specific class, thus reducing the amount of misclassifications with respect to the typical class-specific HMMs.</p><p>Finally, we assessed how the number of states influences performance as expressed in terms of the F1-score. <xref rid="sensors-25-01270-f003" ref-type="fig">Figure 3</xref> illustrates these alterations for both reading and interview tasks. Overall, the number of states per task is the optimal number of stationarity changes from a healthy/control discrimination point of view. In the former task, we observe a relative decrease in performance as the number of states increases beyond 3, showing that the present non-stationarities are captured better with a small number of states. On the contrary, more states are beneficial for the latter task, where performance is maximized when 7 states are considered. This confirms the hypothesis that processing spontaneous speech is generally a harder task than reading speech, as it presents stronger non-stationary characteristics.</p></sec></sec><sec id="sec5-sensors-25-01270"><title>5. Interpretation of the Model&#x02019;s Predictions and Interaction with Medical Experts</title><p>This section highlights the importance of having available clear interpretation of the entire AI pipeline, especially when it comes to critical medical settings. In addition, we describe the ways that the proposed system is able to interact with the medical experts and build trust towards bridging the usability gap existing between AI-based tools and domain experts.</p><p>Importantly, this work applies the explainability-by-design concept in the medical acoustics field. In other words, each system component is designed following the problem specifications and characteristics of the <italic toggle="yes">a priori</italic> available knowledge. As such, the feature set composition was determined along with the model choice and parameterization. The present framework exploits handcrafted features that are easily traced back to the signal&#x02019;s structure, combined with a generative classifier able to provide interpretable predictions. Interestingly, HMMs can point out depression-related information in a speech segment as well as its evolution over time. Unlike NNs, which behave as black boxes, their operation is fully transparent and governed by clear probabilistic rules [<xref rid="B44-sensors-25-01270" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-01270" ref-type="bibr">45</xref>]. They are (a) verifiable, i.e., it can be proved that they process inputs correctly, (b) reliable, i.e., there is an expected operation for inputs located in and out of the known distribution (c) auditable, i.e., we are able to examine their internal state and explain their predictions, and (d) free of bias, since they are trained on correlation-based balanced data subpopulations, thus do not exhibit any type of preferences.</p><p><xref rid="sensors-25-01270-f004" ref-type="fig">Figure 4</xref> illustrates the audio waveforms and output probabilities of the UBM HMM when processing recordings and spectrograms representing both healthy and control subjects with respect to reading and interview tasks. Interestingly, the proposed scheme follows a short-time based processing approach and provides a time series of probabilities characterizing each speech segment. Therefore, the medical expert could monitor the evolution of the subject&#x02019;s status over time and gain valuable insights regarding her/his condition. In the first and third figures, we observe the way the model processes speech coming from healthy subjects, while the second and fourth figures come from control ones. The first pair was recorded during the execution of the reading task, while the second during the interview one. The produced probabilities correspond well with the ground truth, and segments with high confidences are highlighted to the expert.</p><p>Overall, we observe that the probability differences of the two models are higher for the first task, where the model provided higher F1-scores (see <xref rid="sensors-25-01270-t002" ref-type="table">Table 2</xref> and <xref rid="sensors-25-01270-t003" ref-type="table">Table 3</xref>). Importantly, the UBM HMM operates in a transparent way, where one is able to monitor the effective state path along with the emitted probabilities and the resulting prediction based on the maximum a posteriori probability. Moreover, at any given point in time, we can compare the probabilities and assign the winning class. As such, even though the data are only annotated at the recording level, they can be meaningfully segmented into healthy/depressed parts of interest, while the models provide an overall picture of the speech content from the beginning to the end of the session. At the same time, the overall approach is characterized by relatively low computational complexity both during training and testing time.</p><p>Interestingly, the proposed framework not only provides an interpretable probability-based representation of a specific speech segment but may also interact with medical experts to improve familiarization and build trust, thus filling an existing literature gap [<xref rid="B46-sensors-25-01270" ref-type="bibr">46</xref>,<xref rid="B47-sensors-25-01270" ref-type="bibr">47</xref>]. Once the experts become familiar with such an AI-based tool and understand in-depth its operation, they may utilize it effectively and improve and customize its output. The framework may answer to questions, such as (a), which segments contributed most to the prediction, (b) find similar segments from the same same or other patients, etc.</p><p>To respond, the proposed framework isolates segments that maximze the confidence metric so that the medical expert may listen only to those parts, specifically during the diagnoses process. Subsequently, it may provide similarity with segments coming from different subjects with confirmed diagnosis. Similar segments can be easily identified by comparing the effective HMM path and the range of the emitted probabilities. As such, the expert may decide to follow a similar/personalized treatment. In addition, the framework may calculate and store both short- and long-term statistics for browsing at a later stage to assist the diagnosis process. Such a Q&#x00026;A-based interaction scheme may effectively assist the use and acceptance of AI-based tools by the medical personnel. Overall, we argue that the proposed approach not only comprises a highly potent modeling scheme but, at the same time, offers clear interpretations that are strongly required in such a critical applications.</p></sec><sec sec-type="conclusions" id="sec6-sensors-25-01270"><title>6. Conclusions</title><p>This article presents an automatic approach for identifying depression in speech. Two tasks were considered, i.e., <italic toggle="yes">Reading</italic> and <italic toggle="yes">Interview</italic>, for which different features are employed based on the characteristics of the recorded signals. A diverse feature set was formed, capturing complementary aspects of the signals&#x02019; structures. These are fed to a universal modeling scheme able to encode existing temporal dependencies while suitably selecting training and adaptation datasets via a suitably designed correlation-based algorithm. Interestingly, such a scheme incorporates knowledge coming from both healthy and control subjects and outperforms the state of the art with respect to both tasks. At the same time, the present approach can be extended in a straightforward way and operate under non-stationary environments, i.e., it may encompass additional mental states as long as the corresponding data become available without requiring complete retraining. Last but not least, an appropriate Q&#x00026;A-based interaction scheme (see <xref rid="sec5-sensors-25-01270" ref-type="sec">Section 5</xref>) was described, able to meaningfully assist medical professionals during the diagnosis process. The incorporation of such a feature could enhance the adoption of AI-driven solutions within the medical field, as it enables experts to comprehend the rationale behind each prediction. The integration of such features not only aids in closing the usability loop but also supports the education of young psychiatrists and medical personnel.</p><p>The present design choices were motivated by the fact that clear, interpretable decisions are required in the medical field; otherwise, there is a risk that such tools will not be adopted by domain experts. In fact, the proposed system is fully interpretable, i.e., the operation of every component is clear and transparent, unlike black-box systems where an architecture may provide high accuracy without clear reasoning. Actually, such works attempt to explain their operation by employing a reverse logic, where they are trying to understand why an architecture performs well at a post-evaluation phase [<xref rid="B47-sensors-25-01270" ref-type="bibr">47</xref>] by analyzing the relevance/weights that the architecture assigns to specific input regions, and thus interpret them in an opportunistic way, which might not reflect well the characteristics of the problem at hand. As an additional consideration, in many cases such architectures may focus on regions that do not contain expected constituent frequencies but help distinguish between classes, which complicates or renders any interpretation efforts ineffective. That said, there are methodologies targeting the interpretability of the trained weights of the different components in a deep architecture [<xref rid="B48-sensors-25-01270" ref-type="bibr">48</xref>]. However, HMMs usually have much fewer parameters, which significantly eases the interpretability process. We argue that medical AI frameworks [<xref rid="B46-sensors-25-01270" ref-type="bibr">46</xref>], unless accompanied by a rigid etiology framework, have very little chance of being widely adopted in such critical medical applications, especially when considering the recent requirements imposed by the AI Act (REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL).</p><p>In the subsequent phases of this research, our intention is to explore the following facets:<list list-type="bullet"><list-item><p>consider hybrid HMMs, i.e., with emission probabilities based on neural networks [<xref rid="B49-sensors-25-01270" ref-type="bibr">49</xref>],</p></list-item><list-item><p>modify the current framework to suit applications with similar specifications,</p></list-item><list-item><p>integrate additional mental states and develop appropriate sets of features,</p></list-item><list-item><p>explore temporal integration methodologies (statistics, spectral moments, autoregressive models, etc.) at the feature level, given that mental states tend to not change rapidly over time,</p></list-item><list-item><p>examine the efficiency of identification in small data environments, specifically addressing scenarios with limited data availability, such as having few or even just one training sample per class, and</p></list-item><list-item><p>improve the capabilities of the interpretability module, with a particular emphasis on ensuring user-friendliness and garnering acceptance among medical experts.</p></list-item></list></p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The dataset is available in [<xref rid="B15-sensors-25-01270" ref-type="bibr">15</xref>].</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The author declares no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01270"><label>1.</label><element-citation publication-type="webpage"><article-title>Mental Disorders</article-title><comment>Available online: <ext-link xlink:href="https://www.who.int/news-room/fact-sheets/detail/mental-disorders" ext-link-type="uri">https://www.who.int/news-room/fact-sheets/detail/mental-disorders</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-11-25">(accessed on 25 November 2024)</date-in-citation></element-citation></ref><ref id="B2-sensors-25-01270"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Trautmann</surname><given-names>S.</given-names></name>
<name><surname>Rehm</surname><given-names>J.</given-names></name>
<name><surname>Wittchen</surname><given-names>H.</given-names></name>
</person-group><article-title>The economic costs of mental disorders: Do our societies react appropriately to the burden of mental disorders?</article-title><source>EMBO Rep.</source><year>2016</year><volume>17</volume><fpage>1245</fpage><lpage>1249</lpage><pub-id pub-id-type="doi">10.15252/embr.201642951</pub-id><pub-id pub-id-type="pmid">27491723</pub-id>
</element-citation></ref><ref id="B3-sensors-25-01270"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Low</surname><given-names>D.M.</given-names></name>
<name><surname>Bentley</surname><given-names>K.H.</given-names></name>
<name><surname>Ghosh</surname><given-names>S.S.</given-names></name>
</person-group><article-title>Automated assessment of psychiatric disorders using speech: A systematic review</article-title><source>Laryngoscope Investig. Otolaryngol.</source><year>2020</year><volume>5</volume><fpage>96</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1002/lio2.354</pub-id><pub-id pub-id-type="pmid">32128436</pub-id>
</element-citation></ref><ref id="B4-sensors-25-01270"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kapit&#x000e1;ny-F&#x000f6;v&#x000e9;ny</surname><given-names>M.</given-names></name>
<name><surname>Vetr&#x000f3;</surname><given-names>M.</given-names></name>
<name><surname>R&#x000e9;vy</surname><given-names>G.</given-names></name>
<name><surname>Fab&#x000f3;</surname><given-names>D.</given-names></name>
<name><surname>Szirmai</surname><given-names>D.</given-names></name>
<name><surname>Hull&#x000e1;m</surname><given-names>G.</given-names></name>
</person-group><article-title>EEG based depression detection by machine learning: Does inner or overt speech condition provide better biomarkers when using emotion words as experimental cues?</article-title><source>J. Psychiatr. Res.</source><year>2024</year><volume>178</volume><fpage>66</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1016/j.jpsychires.2024.08.002</pub-id><pub-id pub-id-type="pmid">39121709</pub-id>
</element-citation></ref><ref id="B5-sensors-25-01270"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yasin</surname><given-names>S.</given-names></name>
<name><surname>Othmani</surname><given-names>A.</given-names></name>
<name><surname>Raza</surname><given-names>I.</given-names></name>
<name><surname>Hussain</surname><given-names>S.A.</given-names></name>
</person-group><article-title>Machine learning based approaches for clinical and non-clinical depression recognition and depression relapse prediction using audiovisual and EEG modalities: A comprehensive review</article-title><source>Comput. Biol. Med.</source><year>2023</year><volume>159</volume><elocation-id>106741</elocation-id><pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.106741</pub-id><pub-id pub-id-type="pmid">37105109</pub-id>
</element-citation></ref><ref id="B6-sensors-25-01270"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Williamson</surname><given-names>J.R.</given-names></name>
<name><surname>Godoy</surname><given-names>E.</given-names></name>
<name><surname>Cha</surname><given-names>M.</given-names></name>
<name><surname>Schwarzentruber</surname><given-names>A.</given-names></name>
<name><surname>Khorrami</surname><given-names>P.</given-names></name>
<name><surname>Gwon</surname><given-names>Y.</given-names></name>
<name><surname>Kung</surname><given-names>H.T.</given-names></name>
<name><surname>Dagli</surname><given-names>C.</given-names></name>
<name><surname>Quatieri</surname><given-names>T.F.</given-names></name>
</person-group><article-title>Detecting Depression using Vocal, Facial and Semantic Communication Cues</article-title><source>Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>15&#x02013;19 October 2016</conf-date><fpage>11</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1145/2988257.2988263</pub-id></element-citation></ref><ref id="B7-sensors-25-01270"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>H.</given-names></name>
<name><surname>Jiang</surname><given-names>D.</given-names></name>
<name><surname>Oveneke</surname><given-names>M.C.</given-names></name>
<name><surname>Sahli</surname><given-names>H.</given-names></name>
</person-group><article-title>Bipolar Disorder Recognition with Histogram Features of Arousal and Body Gestures</article-title><source>Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>22 October 2018</conf-date><fpage>15</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1145/3266302.3266308</pub-id></element-citation></ref><ref id="B8-sensors-25-01270"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Shen</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>H.</given-names></name>
<name><surname>Lin</surname><given-names>L.</given-names></name>
</person-group><article-title>Automatic Depression Detection: An Emotional Audio-Textual Corpus and A Gru/Bilstm-Based Model</article-title><source>Proceedings of the ICASSP 2022&#x02013;2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Singapore</conf-loc><conf-date>23&#x02013;27 May 2022</conf-date><fpage>6247</fpage><lpage>6251</lpage><pub-id pub-id-type="doi">10.1109/ICASSP43922.2022.9746569</pub-id></element-citation></ref><ref id="B9-sensors-25-01270"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cummins</surname><given-names>N.</given-names></name>
<name><surname>Sethu</surname><given-names>V.</given-names></name>
<name><surname>Epps</surname><given-names>J.</given-names></name>
<name><surname>Williamson</surname><given-names>J.R.</given-names></name>
<name><surname>Quatieri</surname><given-names>T.F.</given-names></name>
<name><surname>Krajewski</surname><given-names>J.</given-names></name>
</person-group><article-title>Generalized Two-Stage Rank Regression Framework for Depression Score Prediction from Speech</article-title><source>IEEE Trans. Affect. Comput.</source><year>2020</year><volume>11</volume><fpage>272</fpage><lpage>283</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2017.2766145</pub-id></element-citation></ref><ref id="B10-sensors-25-01270"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>Z.</given-names></name>
<name><surname>Epps</surname><given-names>J.</given-names></name>
<name><surname>Joachim</surname><given-names>D.</given-names></name>
<name><surname>Chen</surname><given-names>M.</given-names></name>
</person-group><article-title>Depression Detection from Short Utterances via Diverse Smartphones in Natural Environmental Conditions</article-title><source>Proceedings of the Interspeech</source><conf-loc>Hyderabad, India</conf-loc><conf-date>2&#x02013;6 September 2018</conf-date><fpage>3393</fpage><lpage>3397</lpage><pub-id pub-id-type="doi">10.21437/Interspeech.2018-1743</pub-id></element-citation></ref><ref id="B11-sensors-25-01270"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>P.</given-names></name>
<name><surname>Wang</surname><given-names>R.</given-names></name>
<name><surname>Lin</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>F.</given-names></name>
<name><surname>Tu</surname><given-names>J.</given-names></name>
<name><surname>Sun</surname><given-names>M.</given-names></name>
</person-group><article-title>Automatic depression recognition by intelligent speech signal processing: A systematic survey</article-title><source>CAAI Trans. Intell. Technol.</source><year>2022</year><volume>8</volume><fpage>701</fpage><lpage>711</lpage><pub-id pub-id-type="doi">10.1049/cit2.12113</pub-id></element-citation></ref><ref id="B12-sensors-25-01270"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Verde</surname><given-names>L.</given-names></name>
<name><surname>Raimo</surname><given-names>G.</given-names></name>
<name><surname>Vitale</surname><given-names>F.</given-names></name>
<name><surname>Carbonaro</surname><given-names>B.</given-names></name>
<name><surname>Cordasco</surname><given-names>G.</given-names></name>
<name><surname>Marrone</surname><given-names>S.</given-names></name>
<name><surname>Esposito</surname><given-names>A.</given-names></name>
</person-group><article-title>A Lightweight Machine Learning Approach to Detect Depression from Speech Analysis</article-title><source>Proceedings of the 2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)</source><conf-loc>Washington, DC, USA</conf-loc><conf-date>1&#x02013;3 November 2021</conf-date><fpage>330</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1109/ICTAI52525.2021.00054</pub-id></element-citation></ref><ref id="B13-sensors-25-01270"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Lin</surname><given-names>Y.</given-names></name>
<name><surname>Ding</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>C.</given-names></name>
</person-group><article-title>Speech databases for mental disorders: A systematic review</article-title><source>Gen. Psychiatry</source><year>2019</year><volume>32</volume><fpage>e100022</fpage><pub-id pub-id-type="doi">10.1136/gpsych-2018-100022</pub-id></element-citation></ref><ref id="B14-sensors-25-01270"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>G.</given-names></name>
<name><surname>Hansen</surname><given-names>J.</given-names></name>
<name><surname>Kaiser</surname><given-names>J.</given-names></name>
</person-group><article-title>Nonlinear feature based classification of speech under stress</article-title><source>IEEE Trans. Speech Audio Process.</source><year>2001</year><volume>9</volume><fpage>201</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1109/89.905995</pub-id></element-citation></ref><ref id="B15-sensors-25-01270"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Tao</surname><given-names>F.</given-names></name>
<name><surname>Esposito</surname><given-names>A.</given-names></name>
<name><surname>Vinciarelli</surname><given-names>A.</given-names></name>
</person-group><article-title>The Androids Corpus: A New Publicly Available Benchmark for Speech Based Depression Detection</article-title><source>Proceedings of the INTERSPEECH 2023, Convention Centre</source><conf-loc>Dublin, Ireland</conf-loc><conf-date>20&#x02013;24 August 2023</conf-date><fpage>4149</fpage><lpage>4153</lpage><pub-id pub-id-type="doi">10.21437/Interspeech.2023-894</pub-id></element-citation></ref><ref id="B16-sensors-25-01270"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ntalampiras</surname><given-names>S.</given-names></name>
</person-group><article-title>Toward Language-Agnostic Speech Emotion Recognition</article-title><source>J. Audio Eng. Soc.</source><year>2020</year><volume>68</volume><fpage>7</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.17743/jaes.2019.0045</pub-id></element-citation></ref><ref id="B17-sensors-25-01270"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mantegazza</surname><given-names>I.</given-names></name>
<name><surname>Ntalampiras</surname><given-names>S.</given-names></name>
</person-group><article-title>Italian Speech Emotion Recognition</article-title><source>Proceedings of the 2023 24th International Conference on Digital Signal Processing (DSP)</source><conf-loc>Island of Rhodes, Greece</conf-loc><conf-date>11&#x02013;13 June 2023</conf-date><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/DSP58604.2023.10167766</pub-id></element-citation></ref><ref id="B18-sensors-25-01270"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ntalampiras</surname><given-names>S.</given-names></name>
</person-group><article-title>Generalized Sound Recognition in Reverberant Environments</article-title><source>J. Audio Eng. Soc.</source><year>2019</year><volume>67</volume><fpage>772</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.17743/jaes.2019.0030</pub-id></element-citation></ref><ref id="B19-sensors-25-01270"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hidayat</surname><given-names>R.</given-names></name>
<name><surname>Bejo</surname><given-names>A.</given-names></name>
<name><surname>Sumaryono</surname><given-names>S.</given-names></name>
<name><surname>Winursito</surname><given-names>A.</given-names></name>
</person-group><article-title>Denoising Speech for MFCC Feature Extraction Using Wavelet Transformation in Speech Recognition System</article-title><source>Proceedings of the 2018 10th International Conference on Information Technology and Electrical Engineering (ICITEE)</source><conf-loc>Bali, Indonesia</conf-loc><conf-date>24&#x02013;26 July 2018</conf-date><fpage>280</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1109/ICITEED.2018.8534807</pub-id></element-citation></ref><ref id="B20-sensors-25-01270"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ariyanti</surname><given-names>W.</given-names></name>
<name><surname>Liu</surname><given-names>K.C.</given-names></name>
<name><surname>Chen</surname><given-names>K.Y.</given-names></name>
<name><surname>Yu-Tsao</surname></name>
</person-group><article-title>Abnormal Respiratory Sound Identification Using Audio-Spectrogram Vision Transformer</article-title><source>Proceedings of the 2023 45th Annual International Conference of the IEEE Engineering in Medicine &#x00026; Biology Society (EMBC)</source><conf-loc>Sydney, Australia</conf-loc><conf-date>24&#x02013;27 July 2023</conf-date><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/EMBC40787.2023.10341036</pub-id></element-citation></ref><ref id="B21-sensors-25-01270"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ntalampiras</surname><given-names>S.</given-names></name>
</person-group><article-title>Collaborative framework for automatic classification of respiratory sounds</article-title><source>IET Signal Process.</source><year>2020</year><volume>14</volume><fpage>223</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1049/iet-spr.2019.0487</pub-id></element-citation></ref><ref id="B22-sensors-25-01270"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Poir&#x000e8;</surname><given-names>A.M.</given-names></name>
<name><surname>Simonetta</surname><given-names>F.</given-names></name>
<name><surname>Ntalampiras</surname><given-names>S.</given-names></name>
</person-group><article-title>Deep Feature Learning for Medical Acoustics</article-title><source>Artificial Neural Networks and Machine Learning&#x02014;ICANN 2022</source><person-group person-group-type="editor">
<name><surname>Pimenidis</surname><given-names>E.</given-names></name>
<name><surname>Angelov</surname><given-names>P.</given-names></name>
<name><surname>Jayne</surname><given-names>C.</given-names></name>
<name><surname>Papaleonidas</surname><given-names>A.</given-names></name>
<name><surname>Aydin</surname><given-names>M.</given-names></name>
</person-group><publisher-name>Springer Nature Switzerland</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>39</fpage><lpage>50</lpage></element-citation></ref><ref id="B23-sensors-25-01270"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Nosan</surname><given-names>A.</given-names></name>
<name><surname>Sitjongsataporn</surname><given-names>S.</given-names></name>
</person-group><article-title>Speech Recognition Approach using Descend-Delta-Mean and MFCC Algorithm</article-title><source>Proceedings of the 2019 16th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)</source><conf-loc>Chonburi, Thailand</conf-loc><conf-date>10&#x02013;13 July 2019</conf-date><fpage>381</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1109/ECTI-CON47248.2019.8955286</pub-id></element-citation></ref><ref id="B24-sensors-25-01270"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>L.</given-names></name>
<name><surname>Zhao</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>B.</given-names></name>
<name><surname>Cui</surname><given-names>R.</given-names></name>
</person-group><article-title>The Effects of Psychological Stress on Depression</article-title><source>Curr. Neuropharmacol.</source><year>2015</year><volume>13</volume><fpage>494</fpage><lpage>504</lpage><pub-id pub-id-type="doi">10.2174/1570159X1304150831150507</pub-id><pub-id pub-id-type="pmid">26412069</pub-id>
</element-citation></ref><ref id="B25-sensors-25-01270"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Atal</surname><given-names>B.S.</given-names></name>
</person-group><article-title>Automatic Speaker Recognition Based on Pitch Contours</article-title><source>J. Acoust. Soc. Am.</source><year>1972</year><volume>52</volume><fpage>1687</fpage><lpage>1697</lpage><pub-id pub-id-type="doi">10.1121/1.1913303</pub-id></element-citation></ref><ref id="B26-sensors-25-01270"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>McRoberts</surname><given-names>G.W.</given-names></name>
<name><surname>Studdert-Kennedy</surname><given-names>M.</given-names></name>
<name><surname>Shankweiler</surname><given-names>D.P.</given-names></name>
</person-group><article-title>The role of fundamental frequency in signaling linguistic stress and affect: Evidence for a dissociation</article-title><source>Percept. Psychophys.</source><year>1995</year><volume>57</volume><fpage>159</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.3758/BF03206502</pub-id><pub-id pub-id-type="pmid">7885814</pub-id>
</element-citation></ref><ref id="B27-sensors-25-01270"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>France</surname><given-names>D.</given-names></name>
<name><surname>Shiavi</surname><given-names>R.</given-names></name>
<name><surname>Silverman</surname><given-names>S.</given-names></name>
<name><surname>Silverman</surname><given-names>M.</given-names></name>
<name><surname>Wilkes</surname><given-names>M.</given-names></name>
</person-group><article-title>Acoustical properties of speech as indicators of depression and suicidal risk</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2000</year><volume>47</volume><fpage>829</fpage><lpage>837</lpage><pub-id pub-id-type="doi">10.1109/10.846676</pub-id><pub-id pub-id-type="pmid">10916253</pub-id>
</element-citation></ref><ref id="B28-sensors-25-01270"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Reynolds</surname><given-names>D.A.</given-names></name>
<name><surname>Quatieri</surname><given-names>T.F.</given-names></name>
<name><surname>Dunn</surname><given-names>R.B.</given-names></name>
</person-group><article-title>Speaker Verification Using Adapted Gaussian Mixture Models</article-title><source>Digit. Signal Process.</source><year>2000</year><volume>10</volume><fpage>19</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1006/dspr.1999.0361</pub-id></element-citation></ref><ref id="B29-sensors-25-01270"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rabiner</surname><given-names>L.R.</given-names></name>
<name><surname>Juang</surname><given-names>B.H.</given-names></name>
</person-group><article-title>An introduction to hidden Markov models</article-title><source>IEEE ASSP Mag.</source><year>1986</year><volume>3</volume><fpage>4</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1109/MASSP.1986.1165342</pub-id></element-citation></ref><ref id="B30-sensors-25-01270"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Durbin</surname><given-names>R.</given-names></name>
<name><surname>Eddy</surname><given-names>S.R.</given-names></name>
<name><surname>Krogh</surname><given-names>A.</given-names></name>
<name><surname>Mitchison</surname><given-names>G.J.</given-names></name>
</person-group><source>Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids</source><publisher-name>Cambridge University Press</publisher-name><publisher-loc>London, UK</publisher-loc><year>1998</year></element-citation></ref><ref id="B31-sensors-25-01270"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ntalampiras</surname><given-names>S.</given-names></name>
</person-group><article-title>Identification of Anomalous Phonocardiograms Based on Universal Probabilistic Modeling</article-title><source>IEEE Lett. Comput. Soc.</source><year>2020</year><volume>3</volume><fpage>50</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1109/LOCS.2020.3014306</pub-id></element-citation></ref><ref id="B32-sensors-25-01270"><label>32.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Kaufman</surname><given-names>L.</given-names></name>
<name><surname>Rousseeuw</surname><given-names>P.</given-names></name>
</person-group><article-title>Clustering by means of medoids</article-title><source>Statistical Data Analysis Based on the L1-Norm and Related Methods</source><person-group person-group-type="editor">
<name><surname>Dodge</surname><given-names>Y.</given-names></name>
</person-group><comment>North-Holland</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>1987</year><fpage>405</fpage><lpage>416</lpage></element-citation></ref><ref id="B33-sensors-25-01270"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Neto</surname><given-names>A.J.</given-names></name>
<name><surname>Pacheco</surname><given-names>A.G.C.</given-names></name>
<name><surname>Luvizon</surname><given-names>D.C.</given-names></name>
</person-group><article-title>Improving Deep Learning Sound Events Classifiers Using Gram Matrix Feature-Wise Correlations</article-title><source>Proceedings of the ICASSP 2021&#x02013;2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Toronto, ON, Canada</conf-loc><conf-date>6&#x02013;11 June 2021</conf-date><fpage>3780</fpage><lpage>3784</lpage><pub-id pub-id-type="doi">10.1109/ICASSP39728.2021.9414168</pub-id></element-citation></ref><ref id="B34-sensors-25-01270"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ntalampiras</surname><given-names>S.</given-names></name>
</person-group><article-title>Moving Vehicle Classification Using Wireless Acoustic Sensor Networks</article-title><source>IEEE Trans. Emerg. Top. Comput. Intell.</source><year>2018</year><volume>2</volume><fpage>129</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1109/TETCI.2017.2783340</pub-id></element-citation></ref><ref id="B35-sensors-25-01270"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ntalampiras</surname><given-names>S.</given-names></name>
<name><surname>Potamitis</surname><given-names>I.</given-names></name>
</person-group><article-title>Canonical correlation analysis for classifying baby crying sound events</article-title><source>Proceedings of the 22nd International Congress on Sound and Vibration. International Institute of Acoustics and Vibrations</source><conf-loc>Florence, Italy</conf-loc><conf-date>12&#x02013;16 July 2015</conf-date><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="B36-sensors-25-01270"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ntalampiras</surname><given-names>S.</given-names></name>
</person-group><article-title>A Novel Holistic Modeling Approach for Generalized Sound Recognition</article-title><source>IEEE Signal Process. Lett.</source><year>2013</year><volume>20</volume><fpage>185</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1109/LSP.2013.2237902</pub-id></element-citation></ref><ref id="B37-sensors-25-01270"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>L.</given-names></name>
<name><surname>Ji</surname><given-names>S.</given-names></name>
<name><surname>Ye</surname><given-names>J.</given-names></name>
</person-group><article-title>Canonical Correlation Analysis for Multilabel Classification: A Least-Squares Formulation, Extensions, and Analysis</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2011</year><volume>33</volume><fpage>194</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2010.160</pub-id><pub-id pub-id-type="pmid">20733223</pub-id>
</element-citation></ref><ref id="B38-sensors-25-01270"><label>38.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Theodoridis</surname><given-names>S.</given-names></name>
<name><surname>Koutroumbas</surname><given-names>K.</given-names></name>
</person-group><source>Pattern Recognition</source><edition>3rd ed.</edition><publisher-name>Academic Press, Inc.</publisher-name><publisher-loc>Orlando, FL, USA</publisher-loc><year>2006</year></element-citation></ref><ref id="B39-sensors-25-01270"><label>39.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Young</surname><given-names>S.</given-names></name>
<name><surname>Evermann</surname><given-names>G.</given-names></name>
<name><surname>Gales</surname><given-names>M.</given-names></name>
<name><surname>Hain</surname><given-names>T.</given-names></name>
<name><surname>Kershaw</surname><given-names>D.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Moore</surname><given-names>G.</given-names></name>
<name><surname>Odell</surname><given-names>J.</given-names></name>
<name><surname>Ollason</surname><given-names>D.</given-names></name>
<name><surname>Povey</surname><given-names>D.</given-names></name>
<etal/>
</person-group><source>The HTK Book</source><comment>Version 3.4</comment><publisher-name>Cambridge University Engineering Department</publisher-name><publisher-loc>Cambridge, UK</publisher-loc><year>2006</year></element-citation></ref><ref id="B40-sensors-25-01270"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kessler</surname><given-names>R.C.</given-names></name>
<name><surname>Berglund</surname><given-names>P.</given-names></name>
<name><surname>Demler</surname><given-names>O.</given-names></name>
<name><surname>Jin</surname><given-names>R.</given-names></name>
<name><surname>Koretz</surname><given-names>D.</given-names></name>
<name><surname>Merikangas</surname><given-names>K.R.</given-names></name>
<name><surname>Rush</surname><given-names>A.J.</given-names></name>
<name><surname>Walters</surname><given-names>E.E.</given-names></name>
<name><surname>Wang</surname><given-names>P.S.</given-names></name>
</person-group><article-title>The Epidemiology of Major Depressive Disorder: Results From the National Comorbidity Survey Replication (NCS-R)</article-title><source>JAMA</source><year>2003</year><volume>289</volume><fpage>3095</fpage><pub-id pub-id-type="doi">10.1001/jama.289.23.3095</pub-id><pub-id pub-id-type="pmid">12813115</pub-id>
</element-citation></ref><ref id="B41-sensors-25-01270"><label>41.</label><element-citation publication-type="webpage"><article-title>Torch Machine Learning Library</article-title><comment>Available online: <ext-link xlink:href="http://torch.ch/" ext-link-type="uri">http://torch.ch/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-02-16">(accessed on 16 February 2025)</date-in-citation></element-citation></ref><ref id="B42-sensors-25-01270"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gerczuk</surname><given-names>M.</given-names></name>
<name><surname>Amiriparian</surname><given-names>S.</given-names></name>
<name><surname>Ottl</surname><given-names>S.</given-names></name>
<name><surname>Schuller</surname><given-names>B.W.</given-names></name>
</person-group><article-title>EmoNet: A Transfer Learning Framework for Multi-Corpus Speech Emotion Recognition</article-title><source>IEEE Trans. Affect. Comput.</source><year>2023</year><volume>14</volume><fpage>1472</fpage><lpage>1487</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2021.3135152</pub-id></element-citation></ref><ref id="B43-sensors-25-01270"><label>43.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Ntalampiras</surname><given-names>S.</given-names></name>
</person-group><article-title>Transfer Learning for Generalized Audio Signal Processing</article-title><source>Handbook of Artificial Intelligence for Music</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2021</year><fpage>679</fpage><lpage>691</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-72116-9_23</pub-id></element-citation></ref><ref id="B44-sensors-25-01270"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Perikos</surname><given-names>I.</given-names></name>
<name><surname>Kardakis</surname><given-names>S.</given-names></name>
<name><surname>Hatzilygeroudis</surname><given-names>I.</given-names></name>
</person-group><article-title>Sentiment analysis using novel and interpretable architectures of Hidden Markov Models</article-title><source>Knowl. Based Syst.</source><year>2021</year><volume>229</volume><fpage>107332</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2021.107332</pub-id></element-citation></ref><ref id="B45-sensors-25-01270"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ntalampiras</surname><given-names>S.</given-names></name>
<name><surname>Potamitis</surname><given-names>I.</given-names></name>
</person-group><article-title>A Statistical Inference Framework for Understanding Music-Related Brain Activity</article-title><source>IEEE J. Sel. Top. Signal Process.</source><year>2019</year><volume>13</volume><fpage>275</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1109/JSTSP.2019.2905431</pub-id></element-citation></ref><ref id="B46-sensors-25-01270"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tjoa</surname><given-names>E.</given-names></name>
<name><surname>Guan</surname><given-names>C.</given-names></name>
</person-group><article-title>A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2021</year><volume>32</volume><fpage>4793</fpage><lpage>4813</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2020.3027314</pub-id><pub-id pub-id-type="pmid">33079674</pub-id>
</element-citation></ref><ref id="B47-sensors-25-01270"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Tino</surname><given-names>P.</given-names></name>
<name><surname>Leonardis</surname><given-names>A.</given-names></name>
<name><surname>Tang</surname><given-names>K.</given-names></name>
</person-group><article-title>A Survey on Neural Network Interpretability</article-title><source>IEEE Trans. Emerg. Top. Comput. Intell.</source><year>2021</year><volume>5</volume><fpage>726</fpage><lpage>742</lpage><pub-id pub-id-type="doi">10.1109/TETCI.2021.3100641</pub-id></element-citation></ref><ref id="B48-sensors-25-01270"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Akman</surname><given-names>A.</given-names></name>
<name><surname>Schuller</surname><given-names>B.W.</given-names></name>
</person-group><article-title>Audio Explainable Artificial Intelligence: A Review</article-title><source>Intell. Comput.</source><year>2024</year><volume>3</volume><fpage>74</fpage><pub-id pub-id-type="doi">10.34133/icomputing.0074</pub-id></element-citation></ref><ref id="B49-sensors-25-01270"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Razavi</surname><given-names>M.</given-names></name>
<name><surname>Rasipuram</surname><given-names>R.</given-names></name>
<name><surname>Magimai-Doss</surname><given-names>M.</given-names></name>
</person-group><article-title>On modeling context-dependent clustered states: Comparing HMM/GMM, hybrid HMM/ANN and KL-HMM approaches</article-title><source>Proceedings of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Florence, Italy</conf-loc><conf-date>4&#x02013;9 May 2014</conf-date><fpage>7659</fpage><lpage>7663</lpage><pub-id pub-id-type="doi">10.1109/ICASSP.2014.6855090</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01270-f001"><label>Figure 1</label><caption><p>MAP-based adaptation of the <italic toggle="yes">k</italic>-the component of model <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula> using class-specific observations <italic toggle="yes">R</italic>.</p></caption><graphic xlink:href="sensors-25-01270-g001" position="float"/></fig><fig position="float" id="sensors-25-01270-f002"><label>Figure 2</label><caption><p>The topologies including the transition probabilities of the HMMs constructed to address Interview and Reading tasks.</p></caption><graphic xlink:href="sensors-25-01270-g002" position="float"/></fig><fig position="float" id="sensors-25-01270-f003"><label>Figure 3</label><caption><p>Effect of the number of HMM states on F1-score for Reading and Interview tasks.</p></caption><graphic xlink:href="sensors-25-01270-g003" position="float"/></fig><fig position="float" id="sensors-25-01270-f004"><label>Figure 4</label><caption><p>The probabilities output by the UBM-HMM on recordings representing both Healthy and Control subjects with respect to Reading and Interview tasks.</p></caption><graphic xlink:href="sensors-25-01270-g004" position="float"/></fig><table-wrap position="float" id="sensors-25-01270-t001"><object-id pub-id-type="pii">sensors-25-01270-t001_Table 1</object-id><label>Table 1</label><caption><p>The frequency bands used for the extraction of the Teager Energy Operator autocorrelation&#x000a0;envelope.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Band ID</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Lower (Hz)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Center (Hz)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Upper (Hz)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">250</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">400</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">400</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">500</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">600</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">700</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">800</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">800</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">910</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1020</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1020</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1140</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1260</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1260</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1400</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1540</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1540</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1690</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1840</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1840</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2160</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2160</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2350</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2540</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2540</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2750</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2960</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2960</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3200</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3440</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3440</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3720</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4000</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4310</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4620</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4620</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5010</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5400</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5400</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5850</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6300</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6300</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6850</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7400</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01270-t002"><object-id pub-id-type="pii">sensors-25-01270-t002_Table 2</object-id><label>Table 2</label><caption><p>The results of the proposed method versus the state of the art on detecting depression on the Reading Task. The highest rates per figure of merit are emboldened.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="5" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Reading Task</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Approach</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prec.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rec.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Random</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">51.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">51.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">51.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UBM-HMM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>96.2 &#x000b1; 1.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>99.1 &#x000b1; 0.7</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>93.5 &#x000b1; 0.9</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>96.2 &#x000b1; 1.1</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Linear SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.6 &#x000b1; 5.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.6 &#x000b1; 19.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.8 &#x000b1; 12.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.4 &#x000b1; 7.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LSTM&#x000a0;[<xref rid="B15-sensors-25-01270" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.4 &#x000b1; 1.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.5 &#x000b1; 2.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.6 &#x000b1; 2.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.7 &#x000b1; 1.1</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01270-t003"><object-id pub-id-type="pii">sensors-25-01270-t003_Table 3</object-id><label>Table 3</label><caption><p>The results of the proposed method versus the state of the art on detecting depression on the Interview Task. The highest rates per figure of merit are emboldened.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="5" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Interview Task</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Approach</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prec.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rec.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Random</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UBM-HMM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>87.0 &#x000b1; 1.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>85.8 &#x000b1; 1.9</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>92.3 &#x000b1; 1.8</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>88.9 &#x000b1; 1.7</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Linear SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.3 &#x000b1; 10.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.5 &#x000b1; 16.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.5 &#x000b1; 13.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.6 &#x000b1; 13.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LSTM&#x000a0;[<xref rid="B15-sensors-25-01270" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.9 &#x000b1; 1.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.8 &#x000b1; 3.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.1 &#x000b1; 2.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.7 &#x000b1; 0.9</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01270-t004"><object-id pub-id-type="pii">sensors-25-01270-t004_Table 4</object-id><label>Table 4</label><caption><p>Model generalization across tasks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Acc.</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Prec.</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Rec.</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1</th></tr></thead><tbody><tr><td colspan="4" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>Interview Task</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.5 &#x000b1; 5.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.3 &#x000b1; 5.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.9 &#x000b1; 4.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.0 &#x000b1; 6.1</td></tr><tr><td colspan="4" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>Reading Task</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.0 &#x000b1; 3.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.5 &#x000b1; 4.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.2 &#x000b1; 0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.4 &#x000b1; 3.5</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01270-t005"><object-id pub-id-type="pii">sensors-25-01270-t005_Table 5</object-id><label>Table 5</label><caption><p>The results associated with the conducted ablation study with respect to the Reading Task.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="5" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Reading Task</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Approach</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prec.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rec.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UBM-HMM MFCC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.3 &#x000b1; 1.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.0 &#x000b1; 0.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.6 &#x000b1; 3.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.2 &#x000b1; 2.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UBM-HMM MFCC_D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.1 &#x000b1; 2.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.1 &#x000b1; 0.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.7 &#x000b1; 3.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.9 &#x000b1; 2.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UBM-HMM MFCC_D_A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>96.2 &#x000b1; 1.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>99.1 &#x000b1; 0.7</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>93.5 &#x000b1; 0.9</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>96.2 &#x000b1; 1.1</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UBM-HMM MFCC_D_A_TEO</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.4 &#x000b1; 2.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.1 &#x000b1; 0.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.0 &#x000b1; 2.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.6 &#x000b1; 1.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UBM-HMM MFCC_D_A_TEO_F0_HR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.1 &#x000b1; 2.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.1 &#x000b1; 0.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.2 &#x000b1; 1.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.3 &#x000b1; 1.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">class-specific HMM MFCC_D_A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.3 &#x000b1; 3.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.2 &#x000b1; 2.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.1 &#x000b1; 3.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.1 &#x000b1; 2.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">left-right UBM-HMM MFCC_D_A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.8 &#x000b1; 4.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.9 &#x000b1; 3.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.8 &#x000b1; 4.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3 &#x000b1; 3.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">clustering UBM-HMM MFCC_D_A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.0 &#x000b1; 1.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.8 &#x000b1; 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.9 &#x000b1; 2.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.8 &#x000b1; 1.5</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01270-t006"><object-id pub-id-type="pii">sensors-25-01270-t006_Table 6</object-id><label>Table 6</label><caption><p>The results associated to the conducted ablation study with respect to the Interview Task.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="5" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Interview Task</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Approach</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prec.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rec.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UBM-HMM MFCC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.6 &#x000b1; 4.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.5 &#x000b1; 5.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.8 &#x000b1; 4.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.7 &#x000b1; 5.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UBM-HMM MFCC_D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.3 &#x000b1; 4.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.0 &#x000b1; 4.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.2 &#x000b1; 3.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.3 &#x000b1; 3.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UBM-HMM MFCC_D_A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.6 &#x000b1; 4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.4 &#x000b1; 4.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.3 &#x000b1; 3.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.8 &#x000b1; 3.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UBM-HMM MFCC_D_A_TEO</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.6 &#x000b1; 3.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>95.4 &#x000b1; 2.7</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.3 &#x000b1; 2.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.6 &#x000b1; 3.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UBM-HMM MFCC_D_TEO_F0_HR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>87.0 &#x000b1; 1.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.8 &#x000b1; 1.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>92.3 &#x000b1; 1.8</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>88.9 &#x000b1; 1.7</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">class-specific HMM MFCC_D_A_TEO_F0_HR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.4 &#x000b1; 5.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.3 &#x000b1; 4.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.9 &#x000b1; 5.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.8 &#x000b1; 4.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">left-right UBM-HMM MFCC_D_A_TEO_F0_HR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.3 &#x000b1; 2.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.1 &#x000b1; 2.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.9 &#x000b1; 2.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.9 &#x000b1; 2.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">clustering UBM-HMM MFCC_D_A_TEO_F0_HR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.5 &#x000b1; 2.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.1 &#x000b1; 1.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.6 &#x000b1; 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.3 &#x000b1; 2.1</td></tr></tbody></table></table-wrap></floats-group></article>