<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408539</article-id><article-id pub-id-type="pmc">PMC12101739</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0323304</article-id><article-id pub-id-type="publisher-id">PONE-D-24-55463</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Database and Informatics Methods</subject><subj-group><subject>Bioinformatics</subject><subj-group><subject>Sequence Analysis</subject><subj-group><subject>Sequence Alignment</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Nonverbal Communication</subject><subj-group><subject>Facial Expressions</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Nonverbal Communication</subject><subj-group><subject>Facial Expressions</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Music Cognition</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Music Cognition</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Music Cognition</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Sociology</subject><subj-group><subject>Culture</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Bioacoustics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Bioacoustics</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Audio Signal Processing</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Improved generative adversarial networks model for movie dance generation</article-title><alt-title alt-title-type="running-head">Introduction of generative adversarial network model for movie dance generation</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0003-7210-8010</contrib-id><name><surname>Lin</surname><given-names>Zhiqun</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff"/><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Feng</surname><given-names>Kexin</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff"/></contrib></contrib-group><aff id="aff001">
<addr-line>Music College of Capital Normal University, Beijing, China</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Ahmedien</surname><given-names>Diaa Ahmed Mohamed</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Helwan University Faculty of Art Education, EGYPT</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>5779@cnu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0323304</elocation-id><history><date date-type="received"><day>6</day><month>12</month><year>2024</year></date><date date-type="accepted"><day>6</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Lin, Feng</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Lin, Feng</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0323304.pdf"/><abstract><p>To address the challenges of innovation and efficiency in film choreography, this study proposes a dance generation model based on the generative adversarial networks. The model is trained using the AIST++&#x02009;dance motion dataset, incorporating data from multiple dance styles to ensure that the generated dance sequences accurately mimic various stylistic and technical characteristics. The model integrates a music synchronization mechanism and dance structure constraints. These features ensure that the generated dance aligns seamlessly with the background music in terms of rhythm and emotional expression. Additionally, they help maintain a coherent dance structure. Experimental results demonstrate that the proposed model achieves a peak signal-to-noise ratio of 28.5 dB in dance video generation, representing an improvement of 4.2 dB over traditional methods. The structural similarity index reaches 0.83, surpassing the 0.79 achieved by conventional approaches. In a blind evaluation, 85% of professional dancers found the generated dance sequences highly consistent with the original dance styles, marking a 13% improvement over traditional methods. These findings indicate that the model effectively captures the details and fluidity of complex dance movements, providing an innovative and efficient solution for film dance generation.</p></abstract><funding-group><funding-statement>The author(s) received no specific funding for this work.</funding-statement></funding-group><counts><fig-count count="10"/><table-count count="5"/><page-count count="29"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All data generated or analysed during this study are included in this published article and its supplementary information files.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All data generated or analysed during this study are included in this published article and its supplementary information files.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>1. Introduction</title><p>In the realm of filmmaking, dance sequences serve to convey characters&#x02019; inner realms, propel narratives, or evoke specific atmospheres [<xref rid="pone.0323304.ref001" ref-type="bibr">1</xref>&#x02013;<xref rid="pone.0323304.ref003" ref-type="bibr">3</xref>]. Dance in film is not merely a reflection of character or an expression of plot; it is a unique art form that enriches the visual experience of a film and enhances emotional resonance. Through meticulously choreographed dance movements, films can more effectively convey the emotions and conflicts within characters, allowing viewers to experience a deeper sense of connection. Furthermore, film dance serves not only as a form of entertainment but also as a vehicle for cultural heritage and artistic innovation. Chinese folk dance, in particular, emphasizes traditional cultural expression and regional characteristics. Although differences in form and purpose exist between the two, the interaction and fusion between film dance and traditional dance have infused modern dance with renewed vitality.</p><p>Traditional choreography methods typically rely on established principles and experience in dance composition, structuring movements through direct design, pattern layering, and arrangement. These methods emphasize the precision and procedural nature of dancers&#x02019; movements without necessarily incorporating advanced computational techniques or machine learning. In traditional choreography, creation is often driven by a choreographer&#x02019;s deep understanding of dance steps and intuitive arrangement of body language. For instance, in ballet, contemporary dance, or folk dance choreography, choreographers sequentially add dance steps, technical movements, and emotional expressions to construct dance sequences that align with specific stylistic or narrative requirements. The core characteristics of these methods lie in the coordination between the dancer&#x02019;s body and technical movements, as well as the intuitive and manually crafted choreography. From a technical perspective, traditional choreography methods demand high precision and technical proficiency from dancers, relying heavily on the choreographer&#x02019;s in-depth understanding of bodily movement and creative expression. Therefore, technologies for generating dance in film play a significant role in advancing modern dance, fostering artistic innovation, and enhancing creative expression.</p><p>However, amidst the rapid evolution of the film industry and heightened expectations for visual effects, traditional methods of dance choreography confront unprecedented challenges in balancing creativity with production efficiency [<xref rid="pone.0323304.ref004" ref-type="bibr">4</xref>,<xref rid="pone.0323304.ref005" ref-type="bibr">5</xref>]. These methods traditionally rely on choreographers&#x02019; intuition and expertise, necessitating prolonged collaboration and experimentation among dancers and directors to achieve desired performances [<xref rid="pone.0323304.ref006" ref-type="bibr">6</xref>,<xref rid="pone.0323304.ref007" ref-type="bibr">7</xref>]. While fostering highly personalized and perceptibly human artistic expressions, this approach encounters significant constraints in terms of time management and cost containment [<xref rid="pone.0323304.ref008" ref-type="bibr">8</xref>&#x02013;<xref rid="pone.0323304.ref010" ref-type="bibr">10</xref>]. Furthermore, ensuring consistency and replicability in each performance proves challenging, especially within the context of tightly synchronized film shoots [<xref rid="pone.0323304.ref011" ref-type="bibr">11</xref>]. Moreover, traditional dance choreography struggles to faithfully replicate intricate details and explore innovative dance styles, particularly in handling intricate dance movements and diverse expressive demands [<xref rid="pone.0323304.ref012" ref-type="bibr">12</xref>,<xref rid="pone.0323304.ref013" ref-type="bibr">13</xref>].</p><p>Dance generation in films presents a unique set of challenges compared to other dance styles, such as fitness dance, hip-hop, and ballet. First, dance in films is not merely a display of movements; it serves as an artistic expression that integrates with the storyline, character emotions, and cinematic atmosphere. In traditional dance generation, the primary focus is often on the execution of movements. However, in film applications, dance sequences must be closely aligned with the narrative structure to achieve both visual and emotional coherence. Particularly, at different stages of story progression and character emotional shifts, dance movements must adapt to psychological changes and convey the emotional dynamics of the plot. For instance, when a film depicts emotional conflict in a character, the dance must reflect these emotional layers through variations in rhythm, tension in postures, and the subtleties of emotional expression. This requirement makes dance generation more than just imitating movements&#x02014;it necessitates a precise understanding of the film&#x02019;s storyline, character personalities, and emotional fluctuations.</p><p>Moreover, emotional resonance and long-term continuity are two critical factors in film dance generation. Unlike other dance forms, film dance must maintain consistency in movement and emotion across multiple shots. Since dance sequences in films often span several scenes and camera angles, the generated dance must ensure smooth transitions and seamless continuity while aligning with the film&#x02019;s overall emotional tone. For example, in a dance sequence that represents a protagonist&#x02019;s emotional transformation, the changes in movement are not just physical but must also convey the character&#x02019;s inner shift through rhythm, intensity, and form. Additionally, this transformation must remain consistent with the film&#x02019;s narrative progression. The need for sustained emotional expression and synchronized storytelling makes dance generation for films significantly more complex than other dance styles.</p><p>In response to these challenges, artificial intelligence (AI) technologies, notably generative adversarial networks (GANs), have been introduced into dance creation as a transformative solution [<xref rid="pone.0323304.ref014" ref-type="bibr">14</xref>&#x02013;<xref rid="pone.0323304.ref016" ref-type="bibr">16</xref>]. GANs operate as a dual-component machine learning framework comprising a generator and a discriminator that engage in mutual enhancement during the learning process [<xref rid="pone.0323304.ref017" ref-type="bibr">17</xref>,<xref rid="pone.0323304.ref018" ref-type="bibr">18</xref>]. Demonstrating substantial promise in fields such as image and video generation, as well as music composition, GANs facilitate the creation of both novel and authentically inspired artistic works [<xref rid="pone.0323304.ref019" ref-type="bibr">19</xref>].</p><p>Nevertheless, the utilization of GANs in the realm of film dance generation remains relatively limited, particularly in terms of research dedicated to upholding professionalism and stylistic refinement in dance [<xref rid="pone.0323304.ref020" ref-type="bibr">20</xref>]. Existing methodologies primarily focus on generating rudimentary movements rather than intricate, artistically sophisticated professional dances. Furthermore, ensuring precise synchronization of generated dance sequences with the film&#x02019;s music and narrative remains a technical challenge that has yet to be fully resolved [<xref rid="pone.0323304.ref021" ref-type="bibr">21</xref>]. Existing models consider basic movement patterns for generating dance sequences. However, they do not adequately capture important technical details, such as body control, balance, and rhythmic expression, which are essential for achieving high-quality and aesthetically pleasing dance performances.</p><p>This study proposes an enhanced GANs model designed to generate professional-grade dance sequences specifically tailored for film production, referred to as &#x0201c;Movie Dance Generation&#x0201d; hereafter. This term encompasses the generation of stylized dance movements that align with the thematic and artistic requirements of cinematic narratives. By leveraging GANs, the proposed model not only meets the technical demands of professional choreography but also ensures synchronization with musical elements, offering a cohesive and expressive audiovisual experience for cinematic applications. The novel GANs model is developed utilizing publicly available datasets featuring diverse dance movements performed by professional dancers. The primary innovations and contributions of this study are as follows: (1) Design of a Specialized Model for Film Dance Generation: This study develops GANs specifically tailored for film dance, integrated with a Dynamic Convolutional Neural Network (DCNN) to effectively capture intricate spatiotemporal movement patterns and generate high-quality dance sequences. (2) Integration of Music Synchronization and Dance Structure Constraints: The model incorporates music rhythm synchronization and structural constraints to ensure that the generated dance movements align closely with the background music in both rhythm and emotional tone, enhancing logical coherence and artistic appeal. (3) Multimodal Generation Capability: A multimodal generation strategy is proposed to concurrently process dance movements, music, and emotional cues, improving the coordination and artistic expressiveness of generated dance sequences. (4) Self-Attention Mechanism Optimization: The introduction of a self-attention mechanism enhances the modeling of relationships between dance movements, promoting coherence and natural flow in the generated sequences.</p><p>This study comprises five main sections. Section 1 presents the introduction, discussing the significance of dance scenes in filmmaking and the challenges faced by traditional choreography methods, especially in balancing visual impact with innovation. This section also introduces GANs and their potential applications in dance creation. Section 2 provides a literature review, highlighting recent advancements in GANs applications within artistic creation, with a focus on the limitations of current techniques in generating professional-style dance. Section 3 details the methodology, including the design and implementation of the GANs-based film dance generation model, describing the structures of both the generator and discriminator, and outlining how adversarial training enhances the realism and artistic quality of generated dance. Section 4 presents the results and discussion, showing performance evaluation outcomes across multiple metrics and comparing these with traditional choreography and contemporary dance generation techniques. Finally, Section 5 concludes the study, summarizing its primary contributions and exploring the potential of this technology to improve the efficiency and creativity of choreography. The conclusion also addresses the study&#x02019;s limitations and suggests directions for future research.</p></sec><sec id="sec002"><title>2 Literature review</title><p>In recent years, the application of GANs in artistic creation has emerged as a prominent research area, particularly demonstrating unique capabilities in generating image and video content [<xref rid="pone.0323304.ref022" ref-type="bibr">22</xref>]. Within the domain of dance, researchers have explored various applications of GANs aimed at generating sequences of dance movements. For example, in 2023, Cai et al. [<xref rid="pone.0323304.ref023" ref-type="bibr">23</xref>] proposed a model capable of synchronizing dance movements with music rhythm. In 2022, Kritsis et al. [<xref rid="pone.0323304.ref024" ref-type="bibr">24</xref>] investigated the use of deep learning techniques, such as deep belief networks and recurrent neural networks, to generate dance movements aligned with specific musical styles. In 2023, Large et al. [<xref rid="pone.0323304.ref025" ref-type="bibr">25</xref>] analyzed aspects related to musical rhythm generation, perception, attention, perception-motor coordination, and learning, offering insights into the perception and generation of rhythmic patterns in dance.</p><p>In 2021, Wu et al. [<xref rid="pone.0323304.ref026" ref-type="bibr">26</xref>] introduced an innovative framework for creating new clothing patterns and styles using GANs and style transfer algorithms, leveraging Dunhuang clothing datasets to generate novel designs incorporating Dunhuang elements. In 2020, Ahn et al. [<xref rid="pone.0323304.ref027" ref-type="bibr">27</xref>] proposed a framework consisting of a musical feature encoder, pose generator, and music genre classifier, facilitating the generation of three-dimensional human dance poses synchronized with specified music. Additionally, in 2021, Chen et al. [<xref rid="pone.0323304.ref028" ref-type="bibr">28</xref>] presented a fully automated framework based on deep learning for synthesizing realistic upper-body dance animations synchronized with novel guzheng music inputs. This method demonstrated the ability to generate visually plausible guzheng performance animations synchronized with the input music. In 2023, Li et al. [<xref rid="pone.0323304.ref029" ref-type="bibr">29</xref>] investigated the impact of AI on the efficiency of dance innovation.</p><p>The application of GANs in image generation has reached a high level of maturity. In 2022, Zhou et al. [<xref rid="pone.0323304.ref030" ref-type="bibr">30</xref>] with StyleGAN, which significantly enhances image quality and flexibility by separating style and content. In the realm of video generation, in 2022, Brooks et al. [<xref rid="pone.0323304.ref031" ref-type="bibr">31</xref>] introduced a method for generating continuous videos that incorporates short-term and long-term temporal information, achieving high-quality video synthesis. In 2024, Mehmood et al. [<xref rid="pone.0323304.ref032" ref-type="bibr">32</xref>] proposed the VideoGAN model, capable of generating continuous and coherent long-term video segments, offering new approaches for generating complex dynamic scenes. In dance generation, in 2021, Chen et al. [<xref rid="pone.0323304.ref016" ref-type="bibr">16</xref>] introduced ChoreoNet, which integrates GANs with motion capture data to generate dance movements. In 2023, Yin et al. [<xref rid="pone.0323304.ref033" ref-type="bibr">33</xref>] improved dance movement generation accuracy and naturalness by employing conditional GANs (cGANs) and pose estimation algorithms. Additionally, in 2021, Nakatsuka et al. [<xref rid="pone.0323304.ref034" ref-type="bibr">34</xref>] proposed DanceNet, achieving high-fidelity dance movement generation through multi-level spatiotemporal feature extraction. Music synchronization is a crucial aspect of dance generation. In 2022, Zhuang et al. [<xref rid="pone.0323304.ref035" ref-type="bibr">35</xref>] developed the Music2Dance model, which synchronizes dance movements with music rhythms by analyzing the time-frequency features of music signals. In 2024, Kong et al. [<xref rid="pone.0323304.ref036" ref-type="bibr">36</xref>] introduced a Transformer-based model that enhances synchronization by capturing the intricate relationship between music and dance more precisely.</p><p>While significant advancements have been made in GANs and its applications, several limitations persist. Firstly, existing research predominantly focuses on single-modal data generation, posing substantial challenges in integrating cross-modal data such as music and dance [<xref rid="pone.0323304.ref037" ref-type="bibr">37</xref>]. Although many current models generate high-quality dance movements, their effectiveness and synchronization accuracy with complex multimodal data remain areas needing improvement. Secondly, GANs encounter issues of gradient vanishing or exploding when handling long-term sequential data, resulting in unstable and incoherent generation outcomes. Lastly, many studies rely heavily on objective metrics to assess generation quality, lacking comprehensive validation from subjective evaluations within professional domains, thereby challenging the acceptance and professionalism of generated dance movements in practical applications [<xref rid="pone.0323304.ref038" ref-type="bibr">38</xref>,<xref rid="pone.0323304.ref039" ref-type="bibr">39</xref>]. To address these shortcomings, this study proposes the novel GANs model tailored for film dance generation. Specifically, innovations in this work include: firstly, integrating music synchronization mechanisms and dance structural constraints to ensure that generated dance sequences align closely with music rhythms while maintaining logical choreographic arrangements. This cross-modal data fusion strategy addresses the deficiencies in current research concerning music and dance synchronization during generation. Secondly, employing enhanced generator and discriminator architectures to bolster the model&#x02019;s capability in handling long-term sequential data, thereby overcoming traditional GANs limitations in long-term dependency. By incorporating sophisticated temporal information capture mechanisms, the model produces more coherent and natural dance movements. Additionally, this study introduces blind testing involving professional dancers to comprehensively evaluate model generation quality from both subjective and objective perspectives, ensuring the professionalism and naturalness of the generated dance movements. Such evaluation methods not only offer a comprehensive validation mechanism but also establish a robust foundation for the practical feasibility of the generated models. The proposed GANs model significantly enhances the efficiency and quality of film dance generation while pioneering new directions and insights in the field of dance generation research. Future work will continue to optimize algorithms, further enhancing their application value in complex environments, and explore additional cross-modal data fusion methods to expand the widespread application of GANs in artistic creation.</p></sec><sec id="sec003"><title>3 Research methodology</title><p>This section provides a detailed exposition of the design and implementation methodology of GANs models for film dance generation. Initially, the fundamental framework of the model is elucidated, encompassing the construction of both the generator and discriminator, and their mutual enhancement through adversarial training to augment the authenticity and artistic expression of generated dances. Subsequently, the section delineates how the generator utilizes Deep Convolutional Neural Network (DCNN) to learn intricate spatiotemporal movement patterns within dance data and generate new dance action sequences imbued with specific stylistic characteristics. Furthermore, it introduces the integration of music synchronization mechanisms and dance structure constraints. These functionalities ensure that the generated dances precisely synchronize with the soundtrack and maintain reasonable choreographic logic within the film narrative.</p><sec id="sec004"><title>3.1 Design of GAN-based film dance generation model</title><p>In this study, the GANs framework is specifically designed to generate professional dance sequences that meet the demands of film production. The model consists of two main components: a generator and a discriminator, which continuously optimize each other through adversarial training to enhance the realism and artistic expressiveness of the generated dances. The primary objective of the generator is to produce dance motion sequences with a specific style and high coherence. These sequences must not only align with the characteristics of the dance style but also correspond to the film&#x02019;s narrative and emotional tone. To achieve this, the generator employs a DCNN, which excels at learning complex spatiotemporal motion patterns in dance data and generating new dance movements based on noise input.</p><p>The architecture of the generator is designed to accommodate the processing needs of high-dimensional dynamic data. It comprises multiple convolutional layers, batch normalization layers, and ReLU activation layers, forming a robust network capable of generating consistent dance sequences. To ensure that the generated dance movements meet the required artistic standards, the generator continuously adjusts the motion sequences to align with elements such as background music and film narrative. <xref rid="pone.0323304.g001" ref-type="fig">Fig 1</xref> illustrates the detailed architecture of the generator, further demonstrating the working principles of the model.</p><fig position="float" id="pone.0323304.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.g001</object-id><label>Fig 1</label><caption><title>Schematic diagram of generator network structure.</title></caption><graphic xlink:href="pone.0323304.g001" position="float"/></fig><p>In <xref rid="pone.0323304.g001" ref-type="fig">Fig 1</xref>, the input layer receives a random noise vector sampled from a latent distribution, serving as the foundational input for generating novel dance movements. This initial step is followed by multiple convolutional layers, each incorporating deconvolution (also known as transposed convolution) operations. These operations progressively &#x0201c;upsample&#x0201d; the initial low-dimensional noise vector to higher-dimensional features, systematically constructing the spatial structure of the movements. Each convolutional layer is paired with a batch normalization layer, which expedites the training process and stabilizes learning dynamics during generation. ReLU activation layers introduce non-linearity, empowering the network to learn and replicate intricate movement patterns. Finally, the generator&#x02019;s output layer translates the outcomes of transposed convolutions into specific dance movement data, aligning with the dimensions of actual dance movements. This framework enables the generator to transform rudimentary noise inputs into elaborate sequences of dance movements endowed with rich artistic expression through layered processing, meeting the visual and emotional requisites of cinematic contexts.</p><p>In addition, a self-attention mechanism is incorporated into the model design to optimize the complex interrelationships within dance movement sequences. This self-attention mechanism captures intricate dependencies across long sequences, ensuring that the generated dance movements maintain coherence while achieving smooth, natural transitions between actions. This mechanism enables the model to dynamically adjust the synchronization between movements and musical beats at each generation step, resulting in highly precise and refined dance sequences. This approach also enhances the consistency and authenticity of both movement and emotional expression, optimizing the overall presentation of the generated dance.</p><p>The generation process utilizes a multimodal strategy, effectively integrating multiple information dimensions, including dance movements, musical rhythm, and emotional expression. During generation, the model not only analyzes and produces dance movements but also detects and responds to rhythm variations and emotional shifts in the background music, ensuring that the generated dance sequences closely align with the music in terms of rhythm, intensity, and emotional tone. This multimodal coordination greatly enhances the harmony of the generated dance, allowing movements, music, and emotion to merge naturally and further enrich the artistic impact and visual expressiveness of the dance.</p><p>Specifically, the model analyzes the time-frequency features of audio signals, aligning the rhythm and intensity variations in the music with the speed, force, and cadence of the dance movements. This ensures synchronization between dance motions and the accompanying music. This process functions similarly to a &#x0201c;heartbeat&#x0201d; synchronization between music and dance, maintaining consistency with the musical beats and creating fluid, rhythmically cohesive movements.</p><p>Beyond rhythm synchronization, the model also emphasizes variations in the emotional tone of the background music. Musical emotions are typically conveyed through pitch, volume, and chord progressions, while dance expresses emotions through movement intensity, rhythmic variations, and posture. During dance generation, the model employs an emotion recognition algorithm to detect the emotional qualities of the background music in real-time&#x02014;such as joy, sorrow, or tension&#x02014;and adjusts the dance movements accordingly. For example, when the music conveys sadness, the generated dance movements become slower and softer, reflecting a subdued emotional state. Conversely, when the music expresses happiness, the movements become more energetic and dynamic. This emotion-based modulation ensures that the dance aligns with the musical atmosphere, achieving synchronized emotional expression between dance and music.</p><p>By integrating dance movements, musical rhythm, and emotional expression into a unified multimodal generation process, the proposed model not only focuses on the independent generation of each dimension but also ensures their coordination. Dance generation is not merely a matter of technical movement construction; it also requires careful alignment with the emotional tone of the music, rhythm variations, and overall atmosphere. For instance, in certain dance scenes, movements may need to be more dramatic and emotionally intense. In such cases, the model adjusts the fluidity and intensity of dance sequences based on the emotional highs and lows of the background music. In other scenarios, it generates smoother and more relaxed movements to convey emotions such as joy or tranquility.</p><p>This multimodal coordination significantly enhances the harmony of the generated dance, allowing music, movement, and emotion to blend seamlessly. As a result, the artistic impact and visual expressiveness of the dance are greatly enriched. Through this approach, the generated dance not only adheres to the stylistic requirements of choreography but also enhances its ability to convey emotions within a film. This ensures that audiences can perceive the deep interplay and resonance between dance, music, and emotion. In summary, the model&#x02019;s multimodal coordination mechanism ensures that the generated dance movements align with both the musical and emotional context of a film. Additionally, it enhances the overall artistic coherence of choreography within cinematic storytelling. This innovative approach goes beyond traditional dance generation methods, which often focus solely on movement. By fully incorporating emotional and musical dimensions, the model creates a more expressive and immersive dance experience. Consequently, it provides a richer and more efficient solution for dance creation in film production.</p><p>The primary role of the discriminator is to assess input dance sequences and determine whether they originate from the generator. This evaluative process holds critical importance as it directly impacts the generator&#x02019;s training, compelling it to generate dance movement sequences that increasingly resemble authentic ones. Thus, the discriminator aids in refining the generator&#x02019;s capacity to mimic genuine dance styles and dynamics, thereby enhancing the realism and professionalism of the generated dance sequences. The discriminator employs a convolutional neural network architecture tailored for deep learning, specifically engineered to manage and evaluate dance sequences. The detailed structure is depicted in <xref rid="pone.0323304.g002" ref-type="fig">Fig 2</xref>.</p><fig position="float" id="pone.0323304.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.g002</object-id><label>Fig 2</label><caption><title>Network structure of discriminator.</title></caption><graphic xlink:href="pone.0323304.g002" position="float"/></fig><p><xref rid="pone.0323304.g002" ref-type="fig">Fig 2</xref> illustrates that the input layer of the discriminator receives sequences of dance movements, originating from either real dance data or the generator. Multiple convolutional layers are employed to extract features from these dance movements, where each layer applies filters continuously to identify and analyze specific patterns within the movements. Following each convolutional layer, a batch normalization layer is utilized to stabilize and expedite the training process of the deep network. The LeakyReLU activation function is implemented to introduce non-linear processing capabilities, allowing small negative gradients to propagate through the network, thereby enhancing model stability and efficacy. The final layer employs a Sigmoid activation function to convert the discriminator&#x02019;s output into a binary classification, indicating whether the sequence is real (real dance) or synthetic (generator output). This structural framework ensures that the discriminator can effectively evaluate input dance sequences and provide precise authenticity assessments, thereby playing a pivotal role in the adversarial training of the GANs.</p><p>This structural framework ensures that the discriminator effectively evaluates the input dance sequences and provides precise authenticity assessments, playing a crucial role in the training of the GANs. During adversarial training, the generator and discriminator engage in a continuous competition. The generator aims to enhance the realism of the generated dance movements to the point where the discriminator can no longer distinguish them from real dance sequences. Conversely, the discriminator focuses on differentiating between real and generated dance sequences, ensuring the accuracy and reliability of its evaluations. This adversarial training mechanism drives the generator to continuously refine its output, improving the quality and artistic expressiveness of the generated dance sequences. Additionally, it enhances the discriminator&#x02019;s sensitivity to dance movement features, ultimately strengthening the model&#x02019;s generative capability. This dynamic interaction process can be formulated through the following optimization problem, as depicted in Equation (<xref rid="pone.0323304.e001" ref-type="disp-formula">1</xref>):</p><disp-formula id="pone.0323304.e001">
<alternatives><graphic xlink:href="pone.0323304.e001.jpg" id="pone.0323304.e001g" position="anchor"/><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mo>min</mml:mo><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mspace width="2mu"/><mml:msub><mml:mo>max</mml:mo><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mspace width="2mu"/><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x1d53c;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>~</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mi>[</mml:mi><mml:mi>log</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>]</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x1d53c;</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>~</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mi>[</mml:mi><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>]</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(1)</label>
</disp-formula><p>The term <inline-formula id="pone.0323304.e002"><alternatives><graphic xlink:href="pone.0323304.e002.jpg" id="pone.0323304.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> denotes the distribution of real dance data, while <inline-formula id="pone.0323304.e003"><alternatives><graphic xlink:href="pone.0323304.e003.jpg" id="pone.0323304.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> refers to the noise distribution of the generator input. The model&#x02019;s architecture and training mechanism are designed to ensure that the generated dances adhere to the stylistic requirements of professional dance genres while maintaining high consistency with the film&#x02019;s music and narrative scenes.</p><p>This process is illustrated in pseudocode, as shown in Algorithm 1:</p><p>Algorithm 1. Pseudocode for the GAN-based Film Dance Generation Model.</p><p specific-use="line">Step 1: Generator</p><p specific-use="line">Input: noise_vector <italic toggle="yes">z</italic> sampled from latent distribution</p><p specific-use="line">Output: generated_dance_sequence</p><p specific-use="line">1 Initialize random noise_vector z</p><p specific-use="line">2 for each convolutional_layer in generator:</p><p specific-use="line"> Apply transposed convolution (deconvolution) to upsample z</p><p specific-use="line"> Apply batch normalization to stabilize learning</p><p specific-use="line"> Apply ReLU activation to introduce non-linearity</p><p specific-use="line">3 Generate dance_sequence from the final layer of the generator</p><p specific-use="line">4 Return generated_dance_sequence</p><p specific-use="line">Step 2: Discriminator</p><p specific-use="line">Input: dance_sequence (real or generated)</p><p specific-use="line">Output: probability of authenticity</p><p specific-use="line">1 for each convolutional_layer in discriminator:</p><p specific-use="line"> Apply convolution to extract features from dance_sequence</p><p specific-use="line"> Apply batch normalization to stabilize training</p><p specific-use="line"> Apply LeakyReLU activation to allow negative gradient propagation</p><p specific-use="line">2 Pass output through Sigmoid activation to obtain authenticity probability</p><p specific-use="line">3 Return probability of real dance (1) or generated dance (0)</p></sec><sec id="sec005"><title>3.2 Musical synchronization and dance structure constraint</title><p>To enhance the visual and auditory harmony of the generated dance sequences and ensure their synchronization with the music and narrative structure in films, specific functionalities of musical rhythm synchronization and dance structure constraint are incorporated into the generator. These functionalities elevate the artistic expression of the dance movements and uphold their logical coherence. The primary aim of musical rhythm synchronization is to guarantee that the generated dance sequences align closely with the background music in terms of rhythm, intensity, and emotional expression. The process of achieving this objective is illustrated in <xref rid="pone.0323304.g003" ref-type="fig">Fig 3</xref>:</p><fig position="float" id="pone.0323304.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.g003</object-id><label>Fig 3</label><caption><title>The procedure of musical rhythm synchronization.</title></caption><graphic xlink:href="pone.0323304.g003" position="float"/></fig><p><xref rid="pone.0323304.g003" ref-type="fig">Fig 3</xref> illustrates a detailed process involving music analysis, motion adjustments, and assessment of audio-visual consistency. To initiate musical rhythm synchronization, the music analysis process begins by extracting key features from the audio file. Digital signal processing techniques, such as the Fast Fourier Transform, are employed to identify the beat and rhythm of the music. Based on this, the rhythmic patterns of the music are analyzed to distinguish between strong and weak beats and their variations, which help define the rhythm structure of the dance movements.</p><p>Unlike existing studies, the innovation of this study lies in precisely adjusting dance movements based on the rhythm and emotional fluctuations of music. The experiment not only ensured basic synchronization with the beat and intensity of the music but also deeply analyzed how musical emotion influences dance movements. By examining volume variations, the model identifies emotional highs and lows in the music, allowing it to adjust the intensity and amplitude of dance movements accordingly. For instance, during high-intensity segments of the music, the dance movements become more powerful and expressive, whereas in lower-intensity sections, they appear softer and more fluid.</p><p>The core innovation of the synchronization mechanism lies in integrating musical emotion with dynamic dance adjustments. This ensures that the generated dance movements align not only with the rhythm of the music but also resonate with its emotional tone. Specifically, when adjusting dance movements, the generator considers not only musical rhythm and intensity but also the emotional attributes of the music. This multidimensional synchronization mechanism significantly enhances the expressiveness of generated dances, allowing them to synchronize with emotional fluctuations in the music. As a result, the dance and music merge more naturally, creating an emotionally compelling artistic effect. This innovation offers a more refined and sophisticated solution for generating dance in films, advancing the artistic expressiveness of dance generation technology.</p><p>To integrate the music synchronization mechanism into the model, the input layer of the generator includes not only a noise vector but also key features extracted from the music analysis. These features guide the generated dance movements to achieve high alignment with the rhythm and intensity of the music. By calculating a synchronization loss, the model can optimize generated dance movements to ensure temporal alignment with the music beat.</p><p>Finally, audiovisual coherence evaluation is conducted to verify the sensory alignment between visual performance (dance) and auditory experience (music). Through this synchronization mechanism, a strong coherence is established between the dance&#x02019;s visual representation and the music&#x02019;s auditory experience, enriching the overall experience for the audience and enhancing the artistic depth and dimensionality. This effective coordination strengthens the interaction between dance and music, allowing the audience to more deeply experience the emotions and themes conveyed by the artwork.</p><p>To ensure that the generated dance sequences maintain both professional quality and artistic appeal, the experiment incorporates dance structure constraints. These constraints are based on the specific structures and technical requirements of different dance styles. They ensure that the generated dance is both technically accurate and aesthetically engaging. This innovative approach requires a deep understanding of traditional dance techniques and performance arts, effectively integrating these principles into modern computational models. By doing so, it advances the field of dance generation toward a higher level of artistic creation. The specific procedures are depicted in <xref rid="pone.0323304.g004" ref-type="fig">Fig 4</xref>:</p><fig position="float" id="pone.0323304.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.g004</object-id><label>Fig 4</label><caption><title>The process of dance structure constraints.</title></caption><graphic xlink:href="pone.0323304.g004" position="float"/></fig><p><xref rid="pone.0323304.g004" ref-type="fig">Fig 4</xref> delineates this process into three primary steps. Step 1 establishes rules for generating action sequences based on both the traditions and contemporary artistic norms of various dance genres. For example, in classical ballet, movements such as <italic toggle="yes">pointe techniques</italic> and <italic toggle="yes">pirouettes</italic> must adhere to strict technical standards. These standards dictate how movements are executed and how the body is positioned. Choreography is then designed based on these technical requirements to form a structured and fluid sequence. This approach ensures that the dance reflects both artistic expression and creative arrangement. These rules ensure that the generated dance movements maintain stylistic consistency and professionalism, with each movement meeting the specific requirements of the given dance genre. Compared to existing dance generation methods, this approach innovates by deeply integrating the technical requirements of dance styles. It not only emphasizes external accuracy but also focuses on the intrinsic artistic expression of movements, ensuring that each action aligns with the emotional and performative aspects of the specific dance genre.</p><p>Step 2 ensures that the generated dance sequences not only conform to artistic principles but also maintain logical coherence, presenting a dance that appears natural and artistically satisfying. This means that transitions between movements should be smooth, allowing each step to connect logically and forming a complete and expressive dance piece. To achieve this, the model integrates both the temporal sequencing and spatial arrangement of movements, enhancing the continuity of the choreography. This ensures that each action meets technical requirements while also appearing aesthetically fluid from the audience&#x02019;s perspective. This aspect is rarely addressed in existing dance generation models. By focusing on the intrinsic logic of movement sequences, the proposed approach ensures that dance is not merely a collection of independent motions but rather a cohesive and highly artistic performance.</p><p>Step 3 incorporates these structural constraints as additional training parameters and conditions during the generator&#x02019;s training process, guiding and refining dance generation. Here, the generator <italic toggle="yes">G</italic> aims to create action sequences that align with specific dance styles and structures, while the discriminator <italic toggle="yes">D</italic> differentiates between generated sequences and real dance performances. Unlike traditional dance generation methods, the generator in this model does not solely focus on the visual realism of movements. Instead, it emphasizes both artistic and technical conformity. By integrating structural constraints into the training process, the model captures the deeper stylistic characteristics of dance and generates movements that precisely meet artistic and technical standards.</p><p>The dance structure constraints are employed to ensure that the generated dance sequences meet professional standards both technically and artistically. These constraints are defined mathematically as follows. Movement Consistency Constraint: Let A= {a<sub>1</sub>, a<sub>2</sub>, &#x02026;, a<sub>n</sub>} represent the generated sequence of dance movements, where T(a<sub>i</sub>) denotes the technical standards for movement a<sub>i</sub>. The following constraint is defined to ensure that each generated movement conforms to its corresponding technical standard.</p><disp-formula id="pone.0323304.e004">
<alternatives><graphic xlink:href="pone.0323304.e004.jpg" id="pone.0323304.e004g" position="anchor"/><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mi>:</mml:mi><mml:mo>&#x02200;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>(2)</label>
</disp-formula><p>Logical Coherence Constraint: Let P (ai, aj) represent the transition relationship between movements ai and aj. The following constraint is established to ensure that adjacent movements transition smoothly, thereby supporting the flow and coherence of the dance.</p><disp-formula id="pone.0323304.e005">
<alternatives><graphic xlink:href="pone.0323304.e005.jpg" id="pone.0323304.e005g" position="anchor"/><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mi>:</mml:mi><mml:mo>&#x02200;</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></alternatives>
<label>(3)</label>
</disp-formula><p>Structural Integrity Constraint: Let L represent the overall dance structure, encompassing the introduction, development, and conclusion. The constraint, defined in Equation (<xref rid="pone.0323304.e006" ref-type="disp-formula">4</xref>), stipulates that</p><disp-formula id="pone.0323304.e006">
<alternatives><graphic xlink:href="pone.0323304.e006.jpg" id="pone.0323304.e006g" position="anchor"/><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mi>:</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>(4)</label>
</disp-formula><p>f(A) represents the complete dance structure based on the generated sequence A. This constraint ensures that the generated dance sequence aligns with the logical structure of dance art as a whole.</p><p>These constraints are incorporated into the model by introducing penalty terms into the generator&#x02019;s loss function, ensuring that generated dance movements uphold the stylistic, technical, and structural integrity essential to professional dance performance. The generator&#x02019;s behavior is governed by the following optimization objectives, as depicted in Equation (<xref rid="pone.0323304.e007" ref-type="disp-formula">5</xref>):</p><disp-formula id="pone.0323304.e007">
<alternatives><graphic xlink:href="pone.0323304.e007.jpg" id="pone.0323304.e007g" position="anchor"/><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>\arg</mml:mi><mml:msub><mml:mo>min</mml:mo><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mspace width="2mu"/><mml:msub><mml:mo>max</mml:mo><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mspace width="2mu"/><mml:msub><mml:mi>&#x1d53c;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>~</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mi>[</mml:mi><mml:mi>log</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>b</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x1d53c;</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>~</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>~</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mi>[</mml:mi><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>]</mml:mi></mml:mrow></mml:math></alternatives>
<label>(5)</label>
</disp-formula><p>In Equation (<xref rid="pone.0323304.e004" ref-type="disp-formula">2</xref>), <italic toggle="yes">z</italic> refers to the noise vector sampled from the latent space. <italic toggle="yes">c</italic> denotes conditional information pertaining to dance genres, technical requirements for dances, and performance sequences. <inline-formula id="pone.0323304.e008"><alternatives><graphic xlink:href="pone.0323304.e008.jpg" id="pone.0323304.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0323304.e009"><alternatives><graphic xlink:href="pone.0323304.e009.jpg" id="pone.0323304.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> represent the distributions of real dance data and conditional information, respectively. <inline-formula id="pone.0323304.e010"><alternatives><graphic xlink:href="pone.0323304.e010.jpg" id="pone.0323304.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> denotes the prior distribution of the latent space. <inline-formula id="pone.0323304.e011"><alternatives><graphic xlink:href="pone.0323304.e011.jpg" id="pone.0323304.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> represents the distribution of the latent variable, while <inline-formula id="pone.0323304.e012"><alternatives><graphic xlink:href="pone.0323304.e012.jpg" id="pone.0323304.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> represents the true distribution of the training data. The generator not only focuses on the quality of the generated dance movements but also emphasizes the artistic and technical standards of these movements, ensuring that the final generated dance piece can gain professional recognition. Through this approach, the generated dance sequences transcend mere visual representation to authentically convey the essence of dance as an art form.</p><p>The pseudocode illustrating this process, including music synchronization and dance structure constraints, is shown in Algorithm 2:</p><p>Algorithm 2. Pseudocode for Music Synchronization and Dance Structure Constraints</p><p specific-use="line">Step 1: Musical Rhythm Synchronization</p><p specific-use="line">Input: music_file, dance_sequence</p><p specific-use="line">Output: synchronized_dance_sequence</p><p specific-use="line">1 Perform music analysis:</p><p specific-use="line"> Extract beats and tempo using Fast Fourier Transform</p><p specific-use="line"> Analyze rhythm pattern and sound intensity</p><p specific-use="line">2 Adjust dance_sequence based on music analysis:</p><p specific-use="line"> Synchronize movement speed with music tempo</p><p specific-use="line"> Adjust movement intensity according to music dynamics</p><p specific-use="line">3 Assess audio-visual consistency:</p><p specific-use="line"> Ensure dance movements align with music beats</p><p specific-use="line">4 Return synchronized_dance_sequence</p><p specific-use="line">Step 2: Dance Structure Constraints</p><p specific-use="line">Input: dance_genre_rules, generated_dance_sequence</p><p specific-use="line">Output: constrained_dance_sequence</p><p specific-use="line">1 Define dance rules based on genre traditions (e.g., ballet or contemporary dance)</p><p specific-use="line">2 Ensure logical coherence of generated_dance_sequence</p><p specific-use="line">3 Apply dance structure constraints during generator training</p><p specific-use="line">4 Return constrained_dance_sequence adhering to genre-specific standards</p></sec><sec id="sec006"><title>3.3 Parameter configuration</title><p>To ensure optimal performance of the GANs model in the dance video generation task, both model and training parameters are carefully configured. A grid search is conducted over a learning rate range of [0.00001, 0.001]. The results indicate that the video quality, measured by peak signal-to-noise ratio (PSNR) and Structural Similarity Index Measure (SSIM), achieves an optimal balance when the learning rates for the generator and discriminator are set to 0.0002 and 0.0001, respectively. The batch size is set to 32, based on the following considerations: smaller batch sizes (e.g., 16 or 32) are generally more suitable for GANs to avoid mode collapse in either the generator or discriminator. In the experimental environment utilizing an NVIDIA A100 GPU, a batch size of 32 ensures a fast training speed without significantly increasing memory requirements. Comparative experiments with batch sizes of 16, 32, and 64 demonstrate that a batch size of 32 provide the best balance between video generation quality and training efficiency. The Adam optimizer is employed with parameters &#x003b2;&#x02081;&#x02009;=&#x02009;0.5 and &#x003b2;&#x02082;&#x02009;=&#x02009;0.999. These values are selected based on their effectiveness in training GANs models, as they strike a balance between training stability and model convergence speed. Specifically, setting &#x003b2;&#x02081;&#x02009;=&#x02009;0.5 helps reduce gradient oscillations while maintaining dynamic consistency between the generator and discriminator. The training process consists of 200,000 iterations, determined by the convergence speed of the model and the evaluation of video quality. From approximately 150,000 iterations onward, the quality of the generated videos stabilizes, and increasing the training iterations to 250,000 yields no significant performance improvement. Data augmentation techniques, including random cropping and horizontal flipping, are applied during training to enhance the diversity of the training data and improve the model&#x02019;s generalization ability. Rhythm extraction is implemented using the Librosa library, employing the Fast Fourier Transform to extract beat and rhythm features. Librosa is a widely-used, mature toolkit for audio signal processing and is extensively applied in research involving music and dance generation. Synchronization detection is conducted using the cross-correlation method, which effectively measures the temporal alignment between dance sequences and musical signals. The specific parameter settings are detailed in <xref rid="pone.0323304.t001" ref-type="table">Table 1</xref>.</p><table-wrap position="float" id="pone.0323304.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.t001</object-id><label>Table 1</label><caption><title>Parameter settings.</title></caption><alternatives><graphic xlink:href="pone.0323304.t001" id="pone.0323304.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Parameter category</th><th align="left" rowspan="1" colspan="1">Detailed parameters</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Model parameter</td><td align="left" rowspan="1" colspan="1">Learning rate: 0.0002 (Generator), 0.0001 (Discriminator)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Batch size: 32</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Optimizer: Adam, &#x003b2; 1&#x02009;=&#x02009;0.5, &#x003b2; 2&#x02009;=&#x02009;0.999</td></tr><tr><td align="left" rowspan="1" colspan="1">Training parameter</td><td align="left" rowspan="1" colspan="1">Iterations: 200,00</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Data augmentation: random cropping, horizontal flipping</td></tr><tr><td align="left" rowspan="1" colspan="1">Evaluation setting</td><td align="left" rowspan="1" colspan="1">Test set segmentation: 80% training, 20% testing</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Evaluation frequency: Evaluate every 100 iterations</td></tr><tr><td align="left" rowspan="1" colspan="1">Music synchronization</td><td align="left" rowspan="1" colspan="1">Rhythm extraction algorithm: Librosa library</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Synchronous detection algorithm: Cross-correlation</td></tr></tbody></table></alternatives></table-wrap></sec><sec sec-type="materials|methods" id="sec007"><title>3.4 Experimental design</title><p>This study utilizes the AIST++&#x02009;Dance Motion dataset, which includes various dance styles such as street dance, contemporary, and ballet. Known for its high data quality, the dataset encompasses synchronized records of motion capture data, music, and video. The motion capture data consists of three-dimensional coordinates collected through multiple sensors, capturing full-body movements across different dancers. Before usage, the dataset undergoes a thorough preprocessing phase. Initially, raw 3D coordinate data captured by various sensors is transformed into a unified coordinate system. Given the different angles or positions of the sensors, this step eliminates angular deviations caused by sensor arrangement differences, ensuring all dance motion data is described within a consistent spatial reference frame. Following coordinate standardization, joint positions are normalized across dancers. This step addresses variations in body proportions and movement range among dancers, ensuring consistency in scale across the dataset. This normalization allows the model to learn relative motion differences effectively without being affected by absolute dimensions.</p><p>Subsequently, precise alignment is established between the timestamps of dance movements and corresponding music timestamps. This step addresses the potential discrepancies in sampling frequencies or timing records between motion capture data and music, ensuring that dance movements are perfectly synchronized with musical beats. Such alignment enhances the coherence of rhythm and emotional expression in the generated dance sequences. After aligning the timestamps, interpolation techniques are applied to handle irregular sensor sampling rates or missing data points in the collection process. The interpolation process fills in any gaps, generating a continuous time series that ensures temporal coherence and continuity in the dance movements, thereby avoiding abrupt or fragmented motions due to incomplete data. Finally, noise filtering is performed on the preprocessed data to remove any potential noise or errors introduced during motion capture, further enhancing the accuracy and consistency of the dataset. This final step ensures that the input data for the GANs model is sufficiently accurate, enabling the model to learn high-quality features of dance movements effectively.</p><p>To address the limitations of the AIST++&#x02009;Dance Motion Dataset in representing ethnic, regional, and group dance styles, two additional datasets were introduced to enhance the model&#x02019;s diversity and cultural adaptability. The DanceTrack Dataset: This dataset encompasses a wide range of ethnic and regional dance styles from around the world, with a particular focus on culturally distinctive dance forms. It includes various traditional and folk dances, such as Latin dance, Indian dance, and African dance, as well as group dances and regional folk performances, including Chinese folk dance and African tribal dance. These samples not only carry rich cultural backgrounds but also exhibit diverse footwork, movement patterns, and expressive techniques. By incorporating this dataset, the proposed model can learn and generate more diverse dance movements, ensuring its adaptability to different cultural and emotional expressions in film dance generation.</p><p>The DANCE-2-MUSIC Dataset: This dataset focuses on the synchronization between dance and music while also encompassing a variety of dance styles, including culturally significant genres such as Latin and African dance. In addition to synchronizing music rhythm with dance movements, the dataset contains labeled information on dance emotional expressions. This allows the proposed model to more accurately understand and generate dance movements that align with musical rhythm, emotion, and cultural context. By integrating this dataset, the study further enhances the model&#x02019;s cultural adaptability and emotional expressiveness, ensuring that the generated dances exhibit greater artistic diversity and impact.</p><p>Based on these preparations, comprehensive experiments are designed to thoroughly evaluate the GAN-based dance generation model. These experiments not only compare its performance against traditional choreography methods but also against other contemporary dance generation techniques. The experimental design encompasses the following aspects:</p><list list-type="order"><list-item><p>Comparison of generated dances with traditionally choreographed dances.</p></list-item><list-item><p>Comparison with other dance generation technologies, including rule-based systems and alternative learning models.</p></list-item><list-item><p>Evaluation of the diversity and adaptability of dance movements generated by the model across different dance genres and musical conditions.</p></list-item><list-item><p>Assessment of the model&#x02019;s performance in long-term sequence generation and its synchronization capability in complex music and action scenes.</p></list-item><list-item><p>Evaluation of model efficiency and cost usage</p></list-item></list><p>Although this study ensures data diversity and sufficiency, it also accounts for scenarios where training data is limited. Several strategies were implemented to enhance the model&#x02019;s robustness in such cases. When faced with data scarcity, data augmentation techniques&#x02014;including motion rotation, mirroring, and temporal stretching&#x02014;were employed to generate dance variations, thereby increasing data diversity and improving the model&#x02019;s adaptability to limited datasets. Additionally, to further enhance the model&#x02019;s generalization ability, transfer learning strategies were introduced. By pretraining on large-scale dance datasets, the model can quickly adapt and perform well even when trained on smaller datasets. This approach ensures strong performance despite data limitations.</p><p>GANs inherently enhance model performance through adversarial training. Even in data-scarce conditions, the generator and discriminator continue to engage in adversarial learning, allowing the model to capture underlying dance sequence structures and patterns, thereby improving generation quality. Furthermore, integrating few-shot learning techniques enables the model to quickly adapt and generate high-quality dance sequences with only a limited number of samples. By combining these strategies, the model effectively mitigates the challenges of data scarcity while significantly enhancing robustness and generative capability. This ensures that the generated dance movements remain high in quality and diversity, even when data availability is constrained.</p><p>The details of the experimental environment are provided in <xref rid="pone.0323304.t002" ref-type="table">Table 2</xref>:</p><table-wrap position="float" id="pone.0323304.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.t002</object-id><label>Table 2</label><caption><title>Experimental environment.</title></caption><alternatives><graphic xlink:href="pone.0323304.t002" id="pone.0323304.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Environment type</th><th align="left" rowspan="1" colspan="1">Detailed configuration</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Hardware configuration</td><td align="left" rowspan="1" colspan="1">GPU: NVIDIA GeForce RTX 3080 Ti</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">CPU: Intel Core i9-10900K</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">RAM: 64GB DDR4</td></tr><tr><td align="left" rowspan="1" colspan="1">Software configuration</td><td align="left" rowspan="1" colspan="1">Operating System: Ubuntu 20.04 LTS</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Python version: 3.8</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">DL framework: PyTorch 1.7.1</td></tr></tbody></table></alternatives></table-wrap><p>The performance metrics utilized for model evaluation encompass both objective and subjective aspects. PSNR serves to gauge the disparity in image quality between the generated and original videos. A higher PSNR indicates a closer resemblance in image quality to the original data. The calculation is expressed as follows in Equation (<xref rid="pone.0323304.e013" ref-type="disp-formula">6</xref>):</p><disp-formula id="pone.0323304.e013">
<alternatives><graphic xlink:href="pone.0323304.e013.jpg" id="pone.0323304.e013g" position="anchor"/><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:mtext>PSNR</mml:mtext><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mi>&#x000b7;</mml:mi><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mtext>MAX</mml:mtext></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mtext>MSE</mml:mtext></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(6)</label>
</disp-formula><p>In Equation (<xref rid="pone.0323304.e013" ref-type="disp-formula">6</xref>), <inline-formula id="pone.0323304.e014"><alternatives><graphic xlink:href="pone.0323304.e014.jpg" id="pone.0323304.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> represents the maximum possible pixel value in the image, and MSE denotes the mean squared error, as illustrated in Equation (<xref rid="pone.0323304.e015" ref-type="disp-formula">7</xref>):</p><disp-formula id="pone.0323304.e015">
<alternatives><graphic xlink:href="pone.0323304.e015.jpg" id="pone.0323304.e015g" position="anchor"/><mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:mtext>MSE</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="2mu"/><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="2mu"/><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(7)</label>
</disp-formula><p>In Equation (<xref rid="pone.0323304.e015" ref-type="disp-formula">7</xref>), I and <italic toggle="yes">K</italic> denote the original and distorted images, respectively, while <italic toggle="yes">m</italic> and <italic toggle="yes">n</italic> represent the number of rows and columns in the image. SSIM quantifies the fidelity of the visual effect of the video. As the SSIM value approaches 1, it indicates a closer resemblance of the generated video to the original in terms of structure, brightness, and contrast, as depicted in Equation (<xref rid="pone.0323304.e016" ref-type="disp-formula">8</xref>):</p><disp-formula id="pone.0323304.e016">
<alternatives><graphic xlink:href="pone.0323304.e016.jpg" id="pone.0323304.e016g" position="anchor"/><mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:mtext>SSIM</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(8)</label>
</disp-formula><p>In Equation (<xref rid="pone.0323304.e016" ref-type="disp-formula">8</xref>), x and <italic toggle="yes">y</italic> represent the two image windows being compared, <inline-formula id="pone.0323304.e017"><alternatives><graphic xlink:href="pone.0323304.e017.jpg" id="pone.0323304.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0323304.e018"><alternatives><graphic xlink:href="pone.0323304.e018.jpg" id="pone.0323304.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> denote the mean values of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>, respectively, and <inline-formula id="pone.0323304.e019"><alternatives><graphic xlink:href="pone.0323304.e019.jpg" id="pone.0323304.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0323304.e020"><alternatives><graphic xlink:href="pone.0323304.e020.jpg" id="pone.0323304.e020g" position="anchor"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pone.0323304.e021"><alternatives><graphic xlink:href="pone.0323304.e021.jpg" id="pone.0323304.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> refer to the covariance and variances of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>.</p><disp-formula id="pone.0323304.e022">
<alternatives><graphic xlink:href="pone.0323304.e022.jpg" id="pone.0323304.e022g" position="anchor"/><mml:math id="M22" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives>
<label>(9)</label>
</disp-formula><disp-formula id="pone.0323304.e023">
<alternatives><graphic xlink:href="pone.0323304.e023.jpg" id="pone.0323304.e023g" position="anchor"/><mml:math id="M23" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives>
<label>(10)</label>
</disp-formula><p><italic toggle="yes">L</italic> represents the dynamic range of the data (maximum pixel value), while <italic toggle="yes">k</italic><sub><italic toggle="yes">1</italic></sub>&#x02009;=&#x02009;0.01 and <italic toggle="yes">k</italic><sub><italic toggle="yes">2</italic></sub>&#x02009;=&#x02009;0.03 are small constants employed for stability. Blind testing entails inviting professional dancers and dance enthusiasts to evaluate the generated dances without disclosing whether they are choreographed by humans or generated by machines. This approach assesses the professionalism and stylization of the dances. Additionally, audience feedback is solicited regarding their overall impression of the dance, encompassing its expressiveness, emotional conveyance, and artistic appeal.</p><p>To comprehensively assess the practical applicability of the proposed model, an experiment was conducted involving 20 experts with extensive experience in dance creation, performance, and education. These experts were carefully selected through a rigorous screening process, drawing from renowned domestic and international dance competitions and performances to ensure their strong professional background and hands-on expertise. Their expertise spans various domains of dance, from choreography to teaching, with the group consisting of 10 dance performance specialists and 10 senior choreographers and dance educators.</p><p>To ensure objectivity and accuracy in the evaluation process, a blind test was implemented. The experts were unaware of the origin of each dance video, preventing them from distinguishing between sequences generated by the proposed model, traditional methods, or other models. Each expert reviewed a series of dance videos generated by different models and scored them based on predefined evaluation criteria. The evaluation criteria consisted of six dimensions: recognizability as professional dance, motion smoothness, color fidelity, emotional expression, technical complexity, and synchronization between dancers. Each criterion is rated on a five-point scale, with scores ranging from 1 to 10, where 10 is the highest score. Experts record their scores on evaluation sheets, and subsequently, all results are aggregated and subjected to statistical analysis.</p></sec></sec><sec sec-type="conclusions" id="sec008"><title>4 Results and discussion</title><p>This section presents the performance evaluation results of the proposed GAN-based model for generating film dances, comparing it with traditional choreography methods and other modern dance generation techniques. Through quantitative analysis of the quality of generated dance videos, including metrics such as PSNR and SSIM, the experimental findings demonstrate the model&#x02019;s advantages in visual quality and structural fidelity. Furthermore, blind testing conducted by professional dancers corroborates the superior performance of the model in terms of professional recognition, motion fluidity, and music synchronization accuracy. Lastly, this section explores the model&#x02019;s diversity and adaptability across different dance styles and musical conditions, as well as its capability in long-term sequence generation and synchronization in complex music and action scenarios.</p><sec id="sec009"><title>4.1 Performance quantitative analysis of generated dance videos</title><p>To further highlight the innovation and advantages of the proposed model, this study compares the generated dance sequences with the baseline model, and selects four dance styles to demonstrate the model&#x02019;s ability to generate diverse dance movements under different dance styles. One of the images depicts a moment of classical ballet, with dancers dressed in traditional ballet costumes, graceful movements, delicate expressions, and postures that complement each other, creating a harmonious visual experience that is highly compatible with the background music. In contrast, the dance movements generated by the baseline model are more rigid in posture, lacking delicate facial expressions and delicate expression of stage effects. The other image depicts a modern dance performance, where dancers express emotions through large and creative body movements, complemented by vivid lighting effects, enhancing visual impact and making the dance more infectious and emotionally charged. However, the baseline model is difficult to accurately reproduce large-scale movements and the free flow of the body when generating modern dance, resulting in a lack of vitality and emotional depth in the generated results.</p><p>The third image depicts the performance of street dance dancers in a city background, showcasing the freedom and individuality of street dance culture through powerful movements. The dancers cooperate seamlessly with each other, and the rhythm of their movements is precisely synchronized with the background music, making the performance full of dynamism. The baseline model has significant errors in handling fast and intense movements, making it difficult to reflect the kinetic energy and rhythm required for street dance. The last image presents a comprehensive performance that combines ballet, modern dance, and street dance elements, forming a unique artistic style. This fusion not only showcases the technical diversity of dance movements, but also reflects the integration and innovation of culture. When the baseline model generates dance sequences with multi style fusion, it is difficult to achieve a good balance between different dance features, making the synthesized movements unnatural.</p><p>To assess the efficacy of the proposed GAN-based film dance generation model, it is compared against traditional dance choreography, the Vector Quantized - Variational Autoencoder (VQ-VAE) model, and the Transformer model in the same dance generation task. Traditional dance choreography involves manually designing dance motion sequences without utilizing AI techniques directly. The VQ-VAE integrates autoencoder-based representation learning with discretization methods, facilitating efficient representation learning of dance movements and generating new dance sequences that aim to synchronize with music and convey specific emotions. On the other hand, the Transformer model employs self-attention mechanisms to capture dependencies in lengthy sequences, enabling dance movement sequences to synchronize more accurately with music rhythms and emotional nuances, thereby producing high-quality dance videos rich in emotional expression.</p><p>To visually demonstrate the video generation quality across diverse dance styles, <xref rid="pone.0323304.g005" ref-type="fig">Fig 5</xref> illustrates a comparative analysis of average PSNR performance between traditional dance choreography methods and the three advanced generation models.</p><fig position="float" id="pone.0323304.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.g005</object-id><label>Fig 5</label><caption><title>Comparison of average PSNR of generated dance videos of various dance styles.</title></caption><graphic xlink:href="pone.0323304.g005" position="float"/></fig><p><xref rid="pone.0323304.g005" ref-type="fig">Fig 5</xref> illustrates that the proposed model achieves an average PSNR of 28.5 dB in the generated dance videos, notably surpassing the traditional dance choreography method at 24.3 dB. Specifically, in classical ballet style, the proposed GAN model achieves a PSNR of 30.2 dB compared to 25.8 dB for traditional methods, indicating a significant improvement of 4.4 dB and highlighting superior visual quality in the generated videos compared to traditional approaches. Similarly, in modern dance and street dance styles, the GAN model outperforms traditional methods by 4.7 dB and 3.9 dB, respectively, demonstrating its effectiveness in preserving details and textures. The increase in PSNR directly correlates with enhanced video quality, particularly crucial in high-demand film production settings where image clarity and visual fidelity are paramount for audience engagement. This notable advancement underscores the capability of the GAN model to analyze and produce high-quality dance videos. This notable advancement underscores the capability of the GAN model to analyze and produce high-quality dance videos. The significant improvement in PSNR can be attributed to the structure and training mechanism of the GAN model. Unlike traditional methods, the GAN optimizes the generator through adversarial training, enabling it to continuously learn how to more accurately reproduce the dynamic features of dance movements during the generation process. As a result, the model captures more complex motion patterns and expressions during training, thereby enhancing the overall quality of the generated videos.</p><p><xref rid="pone.0323304.g006" ref-type="fig">Fig 6</xref> presents the SSIM performance of videos across different dance styles:</p><fig position="float" id="pone.0323304.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.g006</object-id><label>Fig 6</label><caption><title>SSIM Comparison of Diverse Dance Styles.</title></caption><graphic xlink:href="pone.0323304.g006" position="float"/></fig><p><xref rid="pone.0323304.g006" ref-type="fig">Fig 6</xref> illustrates that the proposed GAN model surpasses other models in terms of SSIM across all dance styles. The overall SSIM performance is 0.83, compared to 0.79 for traditional methods, indicating superior capability in preserving structural integrity. Specifically, in classical ballet, the GAN model achieves an SSIM value of 0.86, which is 0.06 higher than the 0.80 achieved by traditional methods, highlighting its exceptional ability to preserve structural details. This improvement stems from the GAN model&#x02019;s superior ability to capture the complex dynamics and emotional expressions inherent in dance. Visually, the higher SSIM values indicate a strong similarity between the generated videos and the original videos, ensuring the authenticity and coherence of the dance movements. This is particularly crucial for visually complex dance styles, such as classical ballet, as it significantly enhances audience acceptance and satisfaction. By achieving higher SSIM and PSNR values, the model not only improves the visual quality of the videos but also establishes a stronger connection between artistic expression and audience experience, further promoting the interaction between dance and music.</p><p>To further assess the effectiveness of the proposed GAN model in dance video generation, it is compared with three other models: traditional choreography methods, the VQ-VAE model, and the Transformer model. <xref rid="pone.0323304.t003" ref-type="table">Table 3</xref> presents the 95% confidence intervals for the different models&#x02019; generated dance videos, reflecting the consistency and stability of each model&#x02019;s output.</p><table-wrap position="float" id="pone.0323304.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.t003</object-id><label>Table 3</label><caption><title>Statistical Results of 95% Confidence Intervals.</title></caption><alternatives><graphic xlink:href="pone.0323304.t003" id="pone.0323304.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Dance Style</th><th align="left" rowspan="1" colspan="1">Model</th><th align="left" rowspan="1" colspan="1">95% Confidence Interval (PSNR)</th><th align="left" rowspan="1" colspan="1">95% Confidence Interval (SSIM)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Classical Ballet</td><td align="left" rowspan="1" colspan="1">Proposed GAN Model</td><td align="left" rowspan="1" colspan="1">[29.5, 30.9]</td><td align="left" rowspan="1" colspan="1">[0.84, 0.88]</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Traditional Choreography Method</td><td align="left" rowspan="1" colspan="1">[25.1, 26.5]</td><td align="left" rowspan="1" colspan="1">[0.78, 0.82]</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">VQ-VAE Model</td><td align="left" rowspan="1" colspan="1">[27.0, 28.0]</td><td align="left" rowspan="1" colspan="1">[0.80, 0.82]</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Transformer Model</td><td align="left" rowspan="1" colspan="1">[28.0, 29.0]</td><td align="left" rowspan="1" colspan="1">[0.81, 0.83]</td></tr><tr><td align="left" rowspan="1" colspan="1">Modern Dance</td><td align="left" rowspan="1" colspan="1">Proposed GAN Model</td><td align="left" rowspan="1" colspan="1">[28.2, 29.6]</td><td align="left" rowspan="1" colspan="1">[0.81, 0.85]</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Traditional Choreography Method</td><td align="left" rowspan="1" colspan="1">[23.5, 24.9]</td><td align="left" rowspan="1" colspan="1">[0.76, 0.80]</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">VQ-VAE Model</td><td align="left" rowspan="1" colspan="1">[26.0, 27.0]</td><td align="left" rowspan="1" colspan="1">[0.79, 0.81]</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Transformer Model</td><td align="left" rowspan="1" colspan="1">[27.5, 28.5]</td><td align="left" rowspan="1" colspan="1">[0.80, 0.82]</td></tr><tr><td align="left" rowspan="1" colspan="1">Street Dance</td><td align="left" rowspan="1" colspan="1">Proposed GAN Model</td><td align="left" rowspan="1" colspan="1">[28.5, 29.9]</td><td align="left" rowspan="1" colspan="1">[0.80, 0.84]</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Traditional Choreography Method</td><td align="left" rowspan="1" colspan="1">[24.6, 26.0]</td><td align="left" rowspan="1" colspan="1">[0.77, 0.81]</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">VQ-VAE Model</td><td align="left" rowspan="1" colspan="1">[26.5, 27.5]</td><td align="left" rowspan="1" colspan="1">[0.78, 0.80]</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Transformer Model</td><td align="left" rowspan="1" colspan="1">[27.0, 28.0]</td><td align="left" rowspan="1" colspan="1">[0.79, 0.81]</td></tr></tbody></table></alternatives></table-wrap><p>The 95% confidence intervals presented in <xref rid="pone.0323304.t003" ref-type="table">Table 3</xref> demonstrate that the proposed GAN model exhibits higher consistency in generating effects across various dance styles. Notably, for classical ballet and modern dance, the PSNR and SSIM values show relatively narrow confidence intervals, indicating the model&#x02019;s reliability. In contrast, the confidence intervals for traditional choreography methods and other comparative models exhibit greater variability, further validating the advantages of the GAN model in generating high-quality dance videos.</p></sec><sec id="sec010"><title>4.2 Qualitative analysis of generated dance video performance</title><p>To comprehensively evaluate the practical effectiveness of the proposed model, <xref rid="pone.0323304.g007" ref-type="fig">Fig 7</xref> provides a thorough comparison of traditional dance choreography and emerging generative models across key evaluation metrics such as professional dancer recognition, motion fluency, and color fidelity. These metrics directly influence the aesthetic value and professional perception of dance videos.</p><fig position="float" id="pone.0323304.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.g007</object-id><label>Fig 7</label><caption><title>Comprehensive Evaluation and Comparison of Different Models.</title></caption><graphic xlink:href="pone.0323304.g007" position="float"/></fig><p><xref rid="pone.0323304.g007" ref-type="fig">Fig 7</xref> illustrates that the proposed GAN model achieves a high acceptance rate among professional dancers at 85%, surpassing traditional methods by 13%. This indicates the satisfaction of professional dancers with the professionalism and authenticity of the dance movements generated by the GAN model. The smoothness of movements has also improved from a traditional rating of 8.8/10 to 9.2/10, reflecting enhanced naturalness and fluency in movement portrayal. The strong endorsement from professional dancers and the evaluation of movement smoothness validate the model&#x02019;s accuracy and efficiency in capturing and generating intricate dance movements. Such capabilities are particularly beneficial for film productions that demand precise and high-quality dance content, significantly elevating the professionalism and appeal of the final product.</p><p>The evaluation results of the other metrics are illustrated in <xref rid="pone.0323304.g008" ref-type="fig">Fig 8</xref>:</p><fig position="float" id="pone.0323304.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.g008</object-id><label>Fig 8</label><caption><title>Expert Ratings for Dance Videos Generated by Different Models.</title></caption><graphic xlink:href="pone.0323304.g008" position="float"/></fig><p>In <xref rid="pone.0323304.g008" ref-type="fig">Fig 8</xref>, the proposed GAN model demonstrates exceptional performance across all metrics, particularly in terms of professional dancer recognition, emotional expression, and the coordination among dancers, with scores significantly surpassing those of the other models. This indicates that the proposed GAN model not only captures the technical and artistic performances of the dancers more effectively but also communicates the emotional essence of the dance, thereby enhancing the overall viewing experience. Additionally, the model shows advantages in technical complexity and color fidelity compared to traditional choreography methods and other generative models, further confirming its innovativeness and effectiveness in the realm of cinematic dance generation.</p></sec><sec id="sec011"><title>4.3 Other evaluation results</title><p>The accuracy of music synchronization is a critical criterion for evaluating the success of dance video generation. <xref rid="pone.0323304.g009" ref-type="fig">Fig 9</xref> illustrates the effectiveness of different models in ensuring synchronization between dance movements and the rhythm of background music, categorized by tempo:</p><fig position="float" id="pone.0323304.g009"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.g009</object-id><label>Fig 9</label><caption><title>Music Synchronization Accuracy (by Rhythm).</title></caption><graphic xlink:href="pone.0323304.g009" position="float"/></fig><p><xref rid="pone.0323304.g009" ref-type="fig">Fig 9</xref> illustrates that the GAN model achieves an accuracy of 96% in synchronizing with fast-paced music, highlighting its exceptional ability to synchronize with rapid movements and dynamic music changes. The model maintains high accuracy levels for moderate and slow-paced music at 97% and 95%, respectively, underscoring its versatility and responsiveness across different rhythmic patterns. These high synchronization accuracy values indicate the model&#x02019;s proficiency in aligning dance movements precisely with background music, ensuring seamless integration of visual and auditory elements. This synchronization capability is crucial for enhancing the natural flow of dance sequences in films and optimizing the overall audience experience. Particularly in cinematic contexts, precise synchronization between music and movements enhances emotional expression and narrative depth, providing viewers with a more immersive and emotionally engaging cinematic experience.</p><p><xref rid="pone.0323304.g010" ref-type="fig">Fig 10</xref> further compares the accuracy of music synchronization based on the emotional cues conveyed by the background music.</p><fig position="float" id="pone.0323304.g010"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.g010</object-id><label>Fig 10</label><caption><title>Music synchronization accuracy (by emotion).</title></caption><graphic xlink:href="pone.0323304.g010" position="float"/></fig><p><xref rid="pone.0323304.g010" ref-type="fig">Fig 10</xref> illustrates the outstanding performance of the GAN model, particularly in handling dance sequences imbued with intense emotions, achieving synchronization accuracies of up to 95%. Even in contexts portraying melancholic or serene emotions, synchronization accuracies of 92% and 94% are achieved, respectively, highlighting the model&#x02019;s effectiveness in capturing and expressing diverse emotional nuances within dance sequences. The accurate portrayal of emotions is crucial in dance artistry, especially in narrative-driven mediums such as cinema. By adjusting dance movements based on the emotional characteristics of the accompanying music, the GAN model enhances not only the expressiveness of the dance but also deepens emotional resonance with the audience. This capability is pivotal in creating compelling visual narratives and reinforcing the narrative atmosphere, leveraging dance as a powerful medium for emotion and storytelling.</p><p>Through meticulous analysis of the data presented in each figure and subsequent comparisons, the proposed GAN model demonstrates significant advantages across various critical metrics. It surpasses conventional methods not only in terms of visual quality (PSNR and SSIM) but also in setting new benchmarks for acceptance by professional dancers, movement smoothness, and music synchronization accuracy. These achievements underscore the potential of the proposed technology to enhance both the efficiency and creative freedom of dance choreography in film production, while simultaneously elevating the artistic quality of the final product and enriching the emotional experience of the audience. Moreover, these findings underscore the adaptability and versatility of the GAN model, capable of effectively accommodating diverse dance styles and musical genres, which holds substantial promise for meeting the multifaceted demands of the film industry.</p></sec><sec id="sec012"><title>4.4 Ablation study and results</title><p>The ablation study systematically evaluates the impact of various components on model performance to validate their effectiveness. The study begins with a baseline model that incorporates no enhancements, relying solely on the conventional structure of a GAN. Subsequently, new components are progressively introduced, and each version is assessed independently. Each time a new component is added, the model is trained and tested using the same dataset and evaluation criteria. Baseline Model (No Enhancements): This model serves as a benchmark against which subsequent improvements can be compared. The evaluation records scores related to visual quality, motion fluidity, music synchronization, and emotional expression. Addition of Self-Attention Mechanism: Building upon the baseline model, a self-attention mechanism is introduced. This mechanism enhances the model&#x02019;s ability to capture the relationships between dance movements, thereby improving the coherence and naturalness of the generated outputs. The model is reassessed, and its performance is documented. Addition of Multimodal Generation Strategy: On the foundation of the previous version, a multimodal generation strategy is incorporated, enabling the model to process dance movements, music, and emotional information concurrently. This step aims to enhance the coherence and artistic expressiveness of the generated sequences. Complete Model (With All Enhancements): All improvements are integrated to form the complete model, which undergoes a final evaluation. The results of the ablation experiments are summarized in <xref rid="pone.0323304.t004" ref-type="table">Table 4</xref>.</p><table-wrap position="float" id="pone.0323304.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.t004</object-id><label>Table 4</label><caption><title>Ablation Experiment Results.</title></caption><alternatives><graphic xlink:href="pone.0323304.t004" id="pone.0323304.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Model Version</th><th align="left" rowspan="1" colspan="1">Visual Quality Score (Out of 10)</th><th align="left" rowspan="1" colspan="1">Motion Fluidity Score (Out of 10)</th><th align="left" rowspan="1" colspan="1">Music Synchronization Score (Out of 10)</th><th align="left" rowspan="1" colspan="1">Emotional Expression Score (Out of 10)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Baseline Model (No Enhancements)</td><td align="left" rowspan="1" colspan="1">6.0</td><td align="left" rowspan="1" colspan="1">6.5</td><td align="left" rowspan="1" colspan="1">6.0</td><td align="left" rowspan="1" colspan="1">6.2</td></tr><tr><td align="left" rowspan="1" colspan="1">Addition of Self-Attention Mechanism</td><td align="left" rowspan="1" colspan="1">7.5</td><td align="left" rowspan="1" colspan="1">8.0</td><td align="left" rowspan="1" colspan="1">7.5</td><td align="left" rowspan="1" colspan="1">7.8</td></tr><tr><td align="left" rowspan="1" colspan="1">Addition of Multimodal Generation Strategy</td><td align="left" rowspan="1" colspan="1">8.0</td><td align="left" rowspan="1" colspan="1">8.5</td><td align="left" rowspan="1" colspan="1">8.0</td><td align="left" rowspan="1" colspan="1">8.5</td></tr><tr><td align="left" rowspan="1" colspan="1">Complete Model (With All Enhancements)</td><td align="left" rowspan="1" colspan="1">8.5</td><td align="left" rowspan="1" colspan="1">9.0</td><td align="left" rowspan="1" colspan="1">8.8</td><td align="left" rowspan="1" colspan="1">9.2</td></tr></tbody></table></alternatives></table-wrap><p><xref rid="pone.0323304.t004" ref-type="table">Table 4</xref> compares the scores of each version, demonstrating significant improvements across all metrics with the introduction of new components. For instance, the complete model achieves the highest scores for visual quality and motion fluidity, reaching 8.5 and 9.0, respectively, while the corresponding scores for the baseline model are only 6.0 and 6.5. These results indicate that the self-attention mechanism and multimodal generation strategy play crucial roles in enhancing model performance. The self-attention mechanism not only improves the relevance among dance movements but also enhances the natural fluidity of the generated actions. The introduction of the multimodal generation strategy ensures a tight coordination between dance movements and music, thereby elevating the overall artistic expressiveness and the viewer&#x02019;s visual experience. The findings from these ablation experiments validate the innovative design and effectiveness of the proposed model and its components, laying a solid foundation for further research and practical applications.</p><p>Next, a comparative analysis was conducted between the proposed method and traditional choreography methods, the VQ-VAE model, and the Transformer model. The analysis focused on differences in time efficiency, cost savings, and computational complexity. The details are presented in <xref rid="pone.0323304.t005" ref-type="table">Table 5</xref>.</p><table-wrap position="float" id="pone.0323304.t005"><object-id pub-id-type="doi">10.1371/journal.pone.0323304.t005</object-id><label>Table 5</label><caption><title>Comparison of Time Efficiency, Cost, and Computational Complexity.</title></caption><alternatives><graphic xlink:href="pone.0323304.t005" id="pone.0323304.t005g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">Time Efficiency (per dance segment)</th><th align="left" rowspan="1" colspan="1">Cost Savings (%)</th><th align="left" rowspan="1" colspan="1">Cost Analysis</th><th align="left" rowspan="1" colspan="1">Computational Complexity</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Traditional Choreography</td><td align="left" rowspan="1" colspan="1">4-6 hours</td><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">$ 450-700</td><td align="left" rowspan="1" colspan="1">O(n)</td></tr><tr><td align="left" rowspan="1" colspan="1">VQ-VAE Model</td><td align="left" rowspan="1" colspan="1">30-40 minutes</td><td align="left" rowspan="1" colspan="1">30%</td><td align="left" rowspan="1" colspan="1">$ 30</td><td align="left" rowspan="1" colspan="1">O(n)</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer Model</td><td align="left" rowspan="1" colspan="1">15-25 minutes</td><td align="left" rowspan="1" colspan="1">50%</td><td align="left" rowspan="1" colspan="1">$ 40</td><td align="left" rowspan="1" colspan="1">O(log n)</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed GAN Model</td><td align="left" rowspan="1" colspan="1">10-15 minutes</td><td align="left" rowspan="1" colspan="1">55%</td><td align="left" rowspan="1" colspan="1">$ 20</td><td align="left" rowspan="1" colspan="1">O(log n)</td></tr></tbody></table></alternatives></table-wrap><p>As shown in <xref rid="pone.0323304.t005" ref-type="table">Table 5</xref>, the proposed GAN model demonstrates significant advantages in time efficiency, cost savings, and computational complexity compared to traditional choreography methods, the VQ-VAE model, and the Transformer model. First, traditional choreography requires 4&#x02013;6 hours per dance segment, with most of the time spent on choreography design and performer rehearsals, which heavily rely on manual effort. The VQ-VAE model, while capable of generating dance segments in 30&#x02013;40 minutes, still has relatively low efficiency due to the encoding and decoding process. In contrast, the Transformer model reduces generation time to approximately 15&#x02013;25 minutes by leveraging self-attention mechanisms to improve efficiency. However, it remains constrained by high computational resource requirements. Compared to these approaches, the proposed GAN model generates dance segments in just 10&#x02013;15 minutes, significantly improving time efficiency through optimized data processing and generation phases.</p><p>In terms of cost savings, traditional choreography incurs high production costs, averaging $450&#x02013;700 per dance segment, primarily due to expenses related to manual choreography and performer rehearsals. The VQ-VAE model reduces costs by approximately 30%, bringing the cost down to $30 per segment, though it still requires significant computational resources. The Transformer model achieves around 50% cost savings, lowering the cost to $40 per segment, but remains computationally expensive. In contrast, the proposed GAN model generates dance segments in a fully automated manner, eliminating manual choreography costs. Each segment costs only $20, representing a 55% reduction compared to traditional methods, without requiring substantial computational resources.</p><p>Finally, in terms of computational complexity, traditional choreography methods exhibit a complexity of O(n), where n represents the complexity of the dance segment, primarily due to manual design and rehearsals. The VQ-VAE model also operates at O(n) complexity, constrained by the encoding and decoding process, making it inefficient for handling complex dance movements. The Transformer model improves efficiency with a complexity of O(log n), effectively processing long-sequence data. Similarly, the proposed GAN model achieves O(log n) complexity during the generation phase, enabling rapid dance segment generation. Its optimized design makes it suitable for large-scale applications, demonstrating higher computational efficiency and broader applicability. Overall, the proposed GAN model offers clear advantages over the VQ-VAE and Transformer models in terms of time efficiency, cost savings, and computational complexity. In particular, its ability to reduce costs and enhance generation efficiency makes it well-suited for large-scale dance generation tasks.</p></sec><sec id="sec013"><title>4.5 Comprehensive discussion</title><p>This study proposed a dance generation model based on GANs, with a particular focus on synchronization between music and dance as well as structural constraints. The experimental results demonstrated significant improvements. To comprehensively evaluate the contributions of this model, a comparison was conducted with existing studies that have also made significant advancements in music-dance synchronization. For instance, Wang et al. (2024) [<xref rid="pone.0323304.ref040" ref-type="bibr">40</xref>] introduced an improved video action recognition method incorporating a soft-associated feature aggregation module to enhance the accuracy of key action recognition in videos. Their approach primarily relied on synchronizing dance movements with the rhythm and emotions of the music. The core innovation of this study lay in the in-depth exploration of spatiotemporal feature extraction from video data. However, its limitations included handling diverse dance styles and complex movement variations. The generated dance sequences lacked diversity and creativity in intricate dance movements.</p><p>Similarly, Li et al. (2024) [<xref rid="pone.0323304.ref041" ref-type="bibr">41</xref>] proposed a framework based on a Cross-Modal Transformer aimed at enabling style transfer between different dance genres through cross-modal learning. This method focused on style migration by integrating dance video and music features, achieving relatively effective dance style transformation. However, challenges remained in maintaining movement continuity and synchronizing with musical rhythms. The generated dances were less precise in rhythm alignment compared to the model proposed in this study.</p><p>Additionally, Wang &#x00026; Yi (2024) [<xref rid="pone.0323304.ref042" ref-type="bibr">42</xref>] introduced an innovative framework based on a soft correlation strategy. This approach enhanced feature representation by aggregating multi-level, multi-dimensional features and incorporating temporal characteristics generated by the network. While it improved the network&#x02019;s ability to distinguish similar video actions, the generated movements lacked expressive emotions and detailed visual depictions.</p><p>In contrast, the model proposed in this study addressed these limitations by introducing dance structural constraints and a music synchronization mechanism. This innovation effectively improved dance style diversity, rhythm synchronization, and structural coherence. Specifically, the proposed model not only generated motion sequences consistent with various dance styles but also precisely adjusted movement rhythm and intensity to align with musical emotional fluctuations. Compared to existing models, it excelled in preserving dance style diversity and ensuring fluidity in generated movements. Additionally, it achieved higher accuracy in synchronizing with musical rhythm and emotion. By incorporating structural constraints, the proposed model ensured that generated dance movements maintained both technical accuracy and artistic appeal, resulting in more natural and logically coherent sequences. Furthermore, evaluation results based on structural similarity and PSNR indicated that the dance videos generated by the proposed model surpassed traditional dance generation methods in both visual quality and technical performance. In particular, the model demonstrated strong advantages in movement fluidity and detail refinement. Blind tests involving professional dancers further validated the model&#x02019;s effectiveness in maintaining stylistic consistency and emotional expression, with 85% of participants acknowledging its superior performance.</p><p>In summary, the proposed GAN-based model achieves significant breakthroughs in dance generation by integrating music synchronization mechanisms with structural constraints. Unlike previous methods, this model not only addresses issues related to synchronization and style diversity but also enhances the creativity and expressiveness of generated dance sequences. Ultimately, it provides innovative technological support and practical value for cinematic dance production.</p></sec></sec><sec sec-type="conclusions" id="sec014"><title>5 Conclusion</title><p>This study proposes a GAN-based model for dance video generation, whose superiority is validated through multiple experiments. By comparing the proposed model with traditional choreography methods, the VQ-VAE model, and the Transformer model, the study demonstrates that the GAN-based approach excels in visual quality, structural similarity, and music synchronization accuracy. Notably, the model exhibits remarkable generation capabilities in complex dance styles, including ballet, contemporary dance, and street dance. It achieves the highest average PSNR and SSIM values across all dance styles, with scores of 28.5 dB and 0.83, respectively, significantly outperforming traditional methods and alternative generation models. These results indicate the model&#x02019;s exceptional performance in generating high-fidelity and structurally consistent videos. In terms of visual quality and motion fluidity, the model receives an 85% approval rate from professional dancers, reflecting the authenticity and artistic merit of the generated movements. Furthermore, the model demonstrates music synchronization accuracy of 96%, 97%, and 95% for fast, medium, and slow-paced music, respectively. These findings confirm the model&#x02019;s ability to generate natural and harmonious dance motions across varying musical tempos and emotional contexts. Additionally, the integration of a self-attention mechanism and multimodal generation strategies significantly enhances motion coherence and artistic expression. This provides robust technological support for the deep fusion of dance and music, with extensive potential applications in filmmaking and artistic creation.</p><p>However, despite its outstanding performance across multiple metrics, the proposed model has several limitations. First, its performance heavily relies on the quality and diversity of the training data. Although this study utilizes datasets spanning various dance styles, the model&#x02019;s effectiveness in rare or hybrid dance styles requires further verification. Second, due to the complexity of GANs, the model&#x02019;s training and inference times are relatively long, limiting its potential for real-time applications. Third, while the model performs well in emotional synchronization, there is room for improvement in capturing more nuanced emotional expressions and dynamic transitions.</p><p>Future research could focus on expanding the training dataset to include a broader range of dance styles, choreography techniques, and complex music types to enhance the model&#x02019;s generalization capabilities. Lightweight model architectures or optimized training algorithms, such as knowledge distillation, could be explored to reduce model complexity and enable real-time dance generation. Additionally, incorporating computational emotion models could refine the emotional control mechanisms, enabling the generation of more intricate and expressive dance movements. Lastly, integrating GAN models with human-computer interaction technologies could allow users to adjust dance motions and synchronize music in real-time, addressing personalized needs effectively. Additionally, future work could incorporate transfer learning and domain adaptation techniques to enhance the model&#x02019;s generalization ability across new dance styles or scenarios. Transfer learning would allow the model to be pre-trained on existing datasets and then fine-tuned to adapt to new dance styles, improving its performance on novel data. Meanwhile, domain adaptation techniques could act as a bridge between the source and target domains. They help transfer dance styles from the source domain to the target domain. This approach is especially useful when target data is limited.</p></sec><sec id="sec015" sec-type="supplementary-material"><title>Supporting information</title><supplementary-material id="pone.0323304.s001" position="float" content-type="local-data"><label>S1 Data</label><caption><title>Data.</title><p>(XLSX)</p></caption><media xlink:href="pone.0323304.s001.xlsx"/></supplementary-material></sec></body><back><ref-list><title>References</title><ref id="pone.0323304.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Kun</surname><given-names>Y</given-names></name>, <name><surname>Yaakup</surname><given-names>HS</given-names></name>, <name><surname>Syed Zainuddin</surname><given-names>SS</given-names></name>, <name><surname>Mohammad Razi</surname><given-names>SA</given-names></name>. <article-title>Dance Images in the Documentary Films: Its History and Development</article-title>. <source>IJMCR</source>. <year>2022</year>;<volume>3</volume>(<issue>1</issue>):<fpage>33</fpage>&#x02013;<lpage>44</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.25299/ijmcr.v3i1.8462</pub-id></mixed-citation></ref><ref id="pone.0323304.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Harris</surname><given-names>A</given-names></name>. <article-title>Blanche Evan&#x02019;sFilm Studies of the Dance: The &#x0201c;Technique Problem&#x0201d; and the Creation of New Forms in 1930s Revolutionary Dance</article-title>. <source>Dance Res J</source>. <year>2022</year>;<volume>54</volume>(<issue>1</issue>):<fpage>70</fpage>&#x02013;<lpage>91</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1017/s014976772200002x</pub-id></mixed-citation></ref><ref id="pone.0323304.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>E</given-names></name>. <article-title>Cultural thought and philosophical elements of singing and dancing in Indian films</article-title>. <source>Trans/Form/A&#x000e7;&#x000e3;o</source>. <year>2023</year>;<volume>46</volume>(<issue>4</issue>):<fpage>315</fpage>&#x02013;<lpage>28</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1590/0101-3173.2023.v46n4.p315</pub-id></mixed-citation></ref><ref id="pone.0323304.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Nair</surname><given-names>K</given-names></name>. <article-title>Toward a Phenomenology of Film Production</article-title>. <source>Discourse</source>. <year>2022</year>;<volume>44</volume>(<issue>2</issue>):<fpage>158</fpage>&#x02013;<lpage>80</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1353/dis.2022.0016</pub-id></mixed-citation></ref><ref id="pone.0323304.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>LomasMart&#x000ed;nez</surname><given-names>S</given-names></name>. <article-title>Dancing across European and Latin-American borders: Pili and Mili&#x02019;s transnational musical films of the 1960s</article-title>. <source>Transnatl Screens</source>. <year>2022</year>;<volume>13</volume>(<issue>1</issue>):<fpage>64</fpage>&#x02013;<lpage>79</lpage>.</mixed-citation></ref><ref id="pone.0323304.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Georgios</surname><given-names>L</given-names></name>. <article-title>The Transformation of Traditional Dance from Its First to Its Second Existence: The Effectiveness of Music - Movement Education and Creative Dance in the Preservation of Our Cultural Heritage</article-title>. <source>JETS</source>. <year>2017</year>;<volume>6</volume>(<issue>1</issue>):<fpage>104</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.11114/jets.v6i1.2879</pub-id></mixed-citation></ref><ref id="pone.0323304.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Mabingo</surname><given-names>A</given-names></name>. <article-title>Decolonizing Dance Pedagogy: Application of Pedagogies of Ugandan Traditional Dances in Formal Dance Education</article-title>. <source>Journal of Dance Education</source>. <year>2015</year>;<volume>15</volume>(<issue>4</issue>):<fpage>131</fpage>&#x02013;<lpage>41</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/15290824.2015.1023953</pub-id></mixed-citation></ref><ref id="pone.0323304.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Malarsih</surname><given-names>M</given-names></name>, <name><surname>Rohidi</surname><given-names>TR</given-names></name>, <name><surname>Sumaryanto</surname><given-names>T</given-names></name>, <name><surname>Hartono</surname><given-names>H</given-names></name>. <article-title>Mangkunegaran dance style in the custom and tradition of Pura Mangkunegaran</article-title>. <source>Harmonia</source>. <year>2017</year>;<volume>17</volume>(<issue>2</issue>):<fpage>136</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.15294/harmonia.v17i2.12128</pub-id></mixed-citation></ref><ref id="pone.0323304.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Kunej</surname><given-names>D</given-names></name>. <article-title>The changing nature of instrumental music and musicians in folk dance ensembles</article-title>. <source>Traditiones</source>. <year>2023</year>;<volume>52</volume>(<issue>2</issue>):<fpage>69</fpage>&#x02013;<lpage>90</lpage>.</mixed-citation></ref><ref id="pone.0323304.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Juwariyah</surname><given-names>A</given-names></name>, <name><surname>Trisakti</surname><given-names>T</given-names></name>, <name><surname>Abida</surname><given-names>FIN</given-names></name>. <article-title>Conserving the traditional Indonesian performance art &#x0201c;langen tayub&#x0201d; through &#x0201c;waranggana&#x0201d; creativities</article-title>. <source>Cogent Arts &#x00026; Humanities</source>. <year>2023</year>;<volume>10</volume>(<issue>1</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/23311983.2023.2247672</pub-id></mixed-citation></ref><ref id="pone.0323304.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Petkovski</surname><given-names>F</given-names></name>. <article-title>Choreography as Ideology: Dance Heritage, Performance Politics, and the Former Yugoslavia</article-title>. <source>Dance Res J</source>. <year>2023</year>;<volume>55</volume>(<issue>1</issue>):<fpage>98</fpage>&#x02013;<lpage>119</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1017/s0149767723000013</pub-id></mixed-citation></ref><ref id="pone.0323304.ref012"><label>12</label><mixed-citation publication-type="other">Mart&#x000ed;nez RP, Moll&#x000e1; AFA, Naranjo FJR. Traditional dances in Spain: Bibliometric study based on high impact search engines[J]. Retos: nuevas tendencias en educaci&#x000f3;n f&#x000ed;sica, deporte y recreaci&#x000f3;n, 2024 (51): 18&#x02013;31.</mixed-citation></ref><ref id="pone.0323304.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Shahriar</surname><given-names>S</given-names></name>. <article-title>GAN computers generate arts? A survey on visual arts, music, and literary text generation using generative adversarial network</article-title>. <source>Displays</source>. <year>2022</year>;<volume>73</volume>:<fpage>102237</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.displa.2022.102237</pub-id></mixed-citation></ref><ref id="pone.0323304.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>M</given-names></name>, <name><surname>Huang</surname><given-names>X</given-names></name>, <name><surname>Yu</surname><given-names>J</given-names></name>. <article-title>Generative adversarial networks for image and video synthesis: algorithms and applications</article-title>. <source>Proc IEEE</source>. <year>2021</year>;<volume>109</volume>(<issue>5</issue>):<fpage>839</fpage>&#x02013;<lpage>62</lpage>.</mixed-citation></ref><ref id="pone.0323304.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Gui</surname><given-names>J</given-names></name>, <name><surname>Sun</surname><given-names>Z</given-names></name>, <name><surname>Wen</surname><given-names>Y</given-names></name>. <article-title>A review on generative adversarial networks: algorithms, theory, and applications</article-title>. <source>IEEE Trans Knowl Data Eng</source>. <year>2021</year>;<volume>35</volume>(<issue>4</issue>):<fpage>3313</fpage>&#x02013;<lpage>32</lpage>.</mixed-citation></ref><ref id="pone.0323304.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>CChen</surname><given-names>K</given-names></name>, <name><surname>Tan</surname><given-names>Z</given-names></name>, <name><surname>Lei</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>S-H</given-names></name>, <name><surname>Guo</surname><given-names>Y-C</given-names></name>, <name><surname>Zhang</surname><given-names>W</given-names></name>, <etal>et al</etal>. <article-title>ChoreoMaster</article-title>. <source>ACM Trans Graph</source>. <year>2021</year>;<volume>40</volume>(<issue>4</issue>):<fpage>1</fpage>&#x02013;<lpage>13</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3450626.3459932</pub-id></mixed-citation></ref><ref id="pone.0323304.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Aldausari</surname><given-names>N</given-names></name>, <name><surname>Sowmya</surname><given-names>A</given-names></name>, <name><surname>Marcus</surname><given-names>N</given-names></name>, <name><surname>Mohammadi</surname><given-names>G</given-names></name>. <article-title>Video Generative Adversarial Networks: A Review</article-title>. <source>ACM Comput Surv</source>. <year>2022</year>;<volume>55</volume>(<issue>2</issue>):<fpage>1</fpage>&#x02013;<lpage>25</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3487891</pub-id></mixed-citation></ref><ref id="pone.0323304.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Kumar</surname><given-names>L</given-names></name>, <name><surname>Singh</surname><given-names>D</given-names></name>. <article-title>A comprehensive survey on generative adversarial networks used for synthesizing multimedia content</article-title>. <source>Multimedia Tools Appl</source>. <year>2023</year>;<volume>82</volume>(<issue>26</issue>):<fpage>40585</fpage>&#x02013;<lpage>624</lpage>.</mixed-citation></ref><ref id="pone.0323304.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>Y-F</given-names></name>, <name><surname>Liu</surname><given-names>W-D</given-names></name>. <article-title>Choreography cGAN: generating dances with music beats using conditional generative adversarial networks</article-title>. <source>Neural Comput &#x00026; Applic</source>. <year>2021</year>;<volume>33</volume>(<issue>16</issue>):<fpage>9817</fpage>&#x02013;<lpage>33</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s00521-021-05752-x</pub-id></mixed-citation></ref><ref id="pone.0323304.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Ferreira</surname><given-names>JP</given-names></name>, <name><surname>Coutinho</surname><given-names>TM</given-names></name>, <name><surname>Gomes</surname><given-names>TL</given-names></name>, <name><surname>Neto</surname><given-names>JF</given-names></name>, <name><surname>Azevedo</surname><given-names>R</given-names></name>, <name><surname>Martins</surname><given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Learning to dance: A graph convolutional adversarial network to generate realistic dance motions from audio</article-title>. <source>Computers &#x00026; Graphics</source>. <year>2021</year>;<volume>94</volume>:<fpage>11</fpage>&#x02013;<lpage>21</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cag.2020.09.009</pub-id></mixed-citation></ref><ref id="pone.0323304.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Sun</surname><given-names>G</given-names></name>, <name><surname>Wong</surname><given-names>Y</given-names></name>, <name><surname>Cheng</surname><given-names>Z</given-names></name>, <name><surname>Kankanhalli</surname><given-names>MS</given-names></name>, <name><surname>Geng</surname><given-names>W</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>. <article-title>DeepDance: Music-to-Dance Motion Choreography With Adversarial Learning</article-title>. <source>IEEE Trans Multimedia</source>. <year>2021</year>;<volume>23</volume>:<fpage>497</fpage>&#x02013;<lpage>509</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tmm.2020.2981989</pub-id></mixed-citation></ref><ref id="pone.0323304.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Tomaz Neves</surname><given-names>PL</given-names></name>, <name><surname>Fornari</surname><given-names>J</given-names></name>, <name><surname>Batista Florindo</surname><given-names>J</given-names></name>. <article-title>Self-attention generative adversarial networks applied to conditional music generation</article-title>. <source>Multimed Tools Appl</source>. <year>2022</year>;<volume>81</volume>(<issue>17</issue>):<fpage>24419</fpage>&#x02013;<lpage>30</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11042-022-12116-7</pub-id></mixed-citation></ref><ref id="pone.0323304.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Cai</surname><given-names>X</given-names></name>, <name><surname>Xi</surname><given-names>M</given-names></name>, <name><surname>Jia</surname><given-names>S</given-names></name>, <name><surname>Xu</surname><given-names>X</given-names></name>, <name><surname>Wu</surname><given-names>Y</given-names></name>, <name><surname>Sun</surname><given-names>H</given-names></name>. <article-title>An Automatic Music-Driven Folk Dance Movements Generation Method Based on Sequence-To-Sequence Network</article-title>. <source>Int J Patt Recogn Artif Intell</source>. <year>2023</year>;<volume>37</volume>(<issue>05</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1142/s021800142358003x</pub-id></mixed-citation></ref><ref id="pone.0323304.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Kritsis</surname><given-names>K</given-names></name>, <name><surname>Gkiokas</surname><given-names>A</given-names></name>, <name><surname>Pikrakis</surname><given-names>A</given-names></name>. <article-title>Danceconv: dance motion generation with convolutional networks</article-title>. <source>IEEE Access</source>. <year>2022</year>;<volume>10</volume>:<fpage>44982</fpage>&#x02013;<lpage>5000</lpage>.</mixed-citation></ref><ref id="pone.0323304.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Large</surname><given-names>EW</given-names></name>, <name><surname>Roman</surname><given-names>I</given-names></name>, <name><surname>Kim</surname><given-names>JC</given-names></name>, <name><surname>Cannon</surname><given-names>J</given-names></name>, <name><surname>Pazdera</surname><given-names>JK</given-names></name>, <name><surname>Trainor</surname><given-names>LJ</given-names></name>, <etal>et al</etal>. <article-title>Dynamic models for musical rhythm perception and coordination</article-title>. <source>Front Comput Neurosci</source>. <year>2023</year>;<volume>17</volume>:<fpage>1151895</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fncom.2023.1151895</pub-id>
<pub-id pub-id-type="pmid">37265781</pub-id>
</mixed-citation></ref><ref id="pone.0323304.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>Q</given-names></name>, <name><surname>Zhu</surname><given-names>B</given-names></name>, <name><surname>Yong</surname><given-names>B</given-names></name>, <name><surname>Wei</surname><given-names>Y</given-names></name>, <name><surname>Jiang</surname><given-names>X</given-names></name>, <name><surname>Zhou</surname><given-names>R</given-names></name>, <etal>et al</etal>. <article-title>ClothGAN: generation of fashionable Dunhuang clothes using generative adversarial networks</article-title>. <source>Connection Science</source>. <year>2020</year>;<volume>33</volume>(<issue>2</issue>):<fpage>341</fpage>&#x02013;<lpage>58</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/09540091.2020.1822780</pub-id></mixed-citation></ref><ref id="pone.0323304.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Ahn</surname><given-names>H</given-names></name>, <name><surname>Kim</surname><given-names>J</given-names></name>, <name><surname>Kim</surname><given-names>K</given-names></name>, <name><surname>Oh</surname><given-names>S</given-names></name>. <article-title>Generative Autoregressive Networks for 3D Dancing Move Synthesis From Music</article-title>. <source>IEEE Robot Autom Lett</source>. <year>2020</year>;<volume>5</volume>(<issue>2</issue>):<fpage>3501</fpage>&#x02013;<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/lra.2020.2977333</pub-id></mixed-citation></ref><ref id="pone.0323304.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>J</given-names></name>, <name><surname>Fan</surname><given-names>C</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Li</surname><given-names>G</given-names></name>, <name><surname>Zhao</surname><given-names>Z</given-names></name>, <name><surname>Deng</surname><given-names>Z</given-names></name>, <etal>et al</etal>. <article-title>A Music-Driven Deep Generative Adversarial Model for Guzheng Playing Animation</article-title>. <source>IEEE Trans Vis Comput Graph</source>. <year>2023</year>;<volume>29</volume>(<issue>2</issue>):<fpage>1400</fpage>&#x02013;<lpage>14</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TVCG.2021.3115902</pub-id>
<pub-id pub-id-type="pmid">34582351</pub-id>
</mixed-citation></ref><ref id="pone.0323304.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>C</given-names></name>, <name><surname>Xu</surname><given-names>Y</given-names></name>, <name><surname>Zheng</surname><given-names>H</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Han</surname><given-names>H</given-names></name>, <name><surname>Zeng</surname><given-names>L</given-names></name>. <article-title>Artificial intelligence, resource reallocation, and corporate innovation efficiency: Evidence from China&#x02019;s listed companies</article-title>. <source>Resources Policy</source>. <year>2023</year>;<volume>81</volume>:<fpage>103324</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.resourpol.2023.103324</pub-id></mixed-citation></ref><ref id="pone.0323304.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>Y</given-names></name>, <name><surname>Yang</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Chang</surname><given-names>EI-C</given-names></name>, <name><surname>Fan</surname><given-names>Y</given-names></name>, <name><surname>Xu</surname><given-names>Y</given-names></name>. <article-title>3D Segmentation Guided Style-Based Generative Adversarial Networks for PET Synthesis</article-title>. <source>IEEE Trans Med Imaging</source>. <year>2022</year>;<volume>41</volume>(<issue>8</issue>):<fpage>2092</fpage>&#x02013;<lpage>104</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TMI.2022.3156614</pub-id>
<pub-id pub-id-type="pmid">35239478</pub-id>
</mixed-citation></ref><ref id="pone.0323304.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Brooks</surname><given-names>T</given-names></name>, <name><surname>Hellsten</surname><given-names>J</given-names></name>, <name><surname>Aittala</surname><given-names>M</given-names></name>. <article-title>Generating long videos of dynamic scenes</article-title>. <source>Adv Neural Inf Process Syst</source>. <year>2022</year>;<volume>35</volume>:<fpage>31769</fpage>&#x02013;<lpage>81</lpage>.</mixed-citation></ref><ref id="pone.0323304.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Mehmood</surname><given-names>R</given-names></name>, <name><surname>Bashir</surname><given-names>R</given-names></name>, <name><surname>Giri</surname><given-names>KJ</given-names></name>. <article-title>VTM-GAN: video-text matcher based generative adversarial network for generating videos from textual description</article-title>. <source>Int j inf tecnol</source>. <year>2023</year>;<volume>16</volume>(<issue>1</issue>):<fpage>221</fpage>&#x02013;<lpage>36</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s41870-023-01468-4</pub-id></mixed-citation></ref><ref id="pone.0323304.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Yin</surname><given-names>W</given-names></name>, <name><surname>Yin</surname><given-names>H</given-names></name>, <name><surname>Baraka</surname><given-names>K</given-names></name>, <name><surname>Kragic</surname><given-names>D</given-names></name>, <name><surname>Bj&#x000f6;rkman</surname><given-names>M</given-names></name>. <article-title>Multimodal dance style transfer</article-title>. <source>Machine Vision and Applications</source>. <year>2023</year>;<volume>34</volume>(<issue>4</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s00138-023-01399-x</pub-id></mixed-citation></ref><ref id="pone.0323304.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Nakatsuka</surname><given-names>T</given-names></name>, <name><surname>Tsuchiya</surname><given-names>Y</given-names></name>, <name><surname>Hamanaka</surname><given-names>M</given-names></name>, <name><surname>Morishima</surname><given-names>S</given-names></name>. <article-title>Audio-Oriented Video Interpolation Using Key Pose</article-title>. <source>Int J Patt Recogn Artif Intell</source>. <year>2021</year>;<volume>35</volume>(<issue>16</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1142/s0218001421600168</pub-id></mixed-citation></ref><ref id="pone.0323304.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Zhuang</surname><given-names>W</given-names></name>, <name><surname>Wang</surname><given-names>C</given-names></name>, <name><surname>Chai</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Shao</surname><given-names>M</given-names></name>, <name><surname>Xia</surname><given-names>S</given-names></name>. <article-title>Music2Dance: DanceNet for Music-Driven Dance Generation</article-title>. <source>ACM Trans Multimedia Comput Commun Appl</source>. <year>2022</year>;<volume>18</volume>(<issue>2</issue>):<fpage>1</fpage>&#x02013;<lpage>21</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3485664</pub-id></mixed-citation></ref><ref id="pone.0323304.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Kong</surname><given-names>S</given-names></name>. <article-title>A deep learning model of dance generation for young children based on music rhythm and beat</article-title>. <source>Concurrency Comput Pract Exp</source>. <year>2024</year>;<volume>36</volume>(<issue>13</issue>):e8053.</mixed-citation></ref><ref id="pone.0323304.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Hou</surname><given-names>Y</given-names></name>, <name><surname>Yao</surname><given-names>H</given-names></name>, <name><surname>Sun</surname><given-names>X</given-names></name>. <article-title>Soul dancer: emotion-based human action generation</article-title>. <source>ACM Trans Multim Comput Commun Appl</source>. <year>2020</year>;<volume>15</volume>(3s):<fpage>1</fpage>&#x02013;<lpage>19</lpage>.</mixed-citation></ref><ref id="pone.0323304.ref038"><label>38</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>X</given-names></name>, <name><surname>Ko</surname><given-names>YC</given-names></name>. <article-title>The use of deep learning technology in dance movement generation</article-title>. <source>Front Neurorobot</source>. <year>2022</year>;<volume>16</volume>:<fpage>911469</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fnbot.2022.911469</pub-id>
<pub-id pub-id-type="pmid">35990883</pub-id>
</mixed-citation></ref><ref id="pone.0323304.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>Cai</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>T</given-names></name>, <name><surname>Lu</surname><given-names>R</given-names></name>, <name><surname>Jia</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>H</given-names></name>. <article-title>Automatic generation of Labanotation based on human pose estimation in folk dance videos</article-title>. <source>Neural Comput &#x00026; Applic</source>. <year>2023</year>;<volume>35</volume>(<issue>35</issue>):<fpage>24755</fpage>&#x02013;<lpage>71</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s00521-023-08206-8</pub-id></mixed-citation></ref><ref id="pone.0323304.ref040"><label>40</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>F</given-names></name>, <name><surname>Jin</surname><given-names>X</given-names></name>, <name><surname>Yi</surname><given-names>S</given-names></name>. <article-title>LI3D-BiLSTM: A Lightweight Inception-3D Networks with BiLSTM for Video Action Recognition</article-title>. <source>IECE Transactions on Emerging Topics in Artificial Intelligence</source>. <year>2024</year>;<volume>1</volume>(<issue>1</issue>):<fpage>58</fpage>&#x02013;<lpage>70</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.62762/tetai.2024.628205</pub-id></mixed-citation></ref><ref id="pone.0323304.ref041"><label>41</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>T</given-names></name>, <name><surname>Kong</surname><given-names>L</given-names></name>, <name><surname>Yang</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>B</given-names></name>, <name><surname>Xu</surname><given-names>J</given-names></name>. <article-title>Bridging Modalities: A Survey of Cross-Modal Image-Text Retrieval</article-title>. <source>Chin j inf fusion</source>. <year>2024</year>;<volume>1</volume>(<issue>1</issue>):<fpage>79</fpage>&#x02013;<lpage>92</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.62762/cjif.2024.361895</pub-id></mixed-citation></ref><ref id="pone.0323304.ref042"><label>42</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>F</given-names></name>, <name><surname>Yi</surname><given-names>S</given-names></name>. <article-title>Spatio-temporal Feature Soft Correlation Concatenation Aggregation Structure for Video Action Recognition Networks</article-title>. <source>IECE Transactions on Sensing, Communication, and Control</source>. <year>2024</year>;<volume>1</volume>(<issue>1</issue>):<fpage>60</fpage>&#x02013;<lpage>71</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.62762/tscc.2024.212751</pub-id></mixed-citation></ref></ref-list></back></article>