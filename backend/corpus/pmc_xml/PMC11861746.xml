<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006473</article-id><article-id pub-id-type="pmc">PMC11861746</article-id><article-id pub-id-type="doi">10.3390/s25041245</article-id><article-id pub-id-type="publisher-id">sensors-25-01245</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Cloud Detection in Remote Sensing Images Based on a Novel Adaptive Feature Aggregation Method</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-0891-4445</contrib-id><name><surname>Zhou</surname><given-names>Wanting</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01245" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1346-6216</contrib-id><name><surname>Mo</surname><given-names>Yan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-01245" ref-type="aff">1</xref><xref rid="af2-sensors-25-01245" ref-type="aff">2</xref><xref rid="c1-sensors-25-01245" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Ou</surname><given-names>Qiaofeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="af1-sensors-25-01245" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Bai</surname><given-names>Shaowei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-01245" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Girard</surname><given-names>Sylvain</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01245"><label>1</label>School of Information Engineering, Nanchang Hangkong University, Nanchang 330063, China; <email>2304085401015@stu.nchu.edu.cn</email> (W.Z.); <email>ou.qiaofeng@nchu.edu.cn</email> (Q.O.); <email>2404085404317@stu.nchu.edu.cn</email> (S.B.)</aff><aff id="af2-sensors-25-01245"><label>2</label>College of Aeronautics Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China</aff><author-notes><corresp id="c1-sensors-25-01245"><label>*</label>Correspondence: <email>moyan@nchu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>18</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1245</elocation-id><history><date date-type="received"><day>14</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>15</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>17</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Cloud detection constitutes a pivotal task in remote sensing preprocessing, yet detecting cloud boundaries and identifying thin clouds under complex scenarios remain formidable challenges. In response to this challenge, we designed a network model, named NFCNet. The network comprises three submodules: the Hybrid Convolutional Attention Module (HCAM), the Spatial Pyramid Fusion Attention (SPFA) module, and the Dual-Stream Convolutional Aggregation (DCA) module. The HCAM extracts multi-scale features to enhance global representation while matching channel importance weights to focus on features that are more critical to the detection task. The SPFA module employs a novel adaptive feature aggregation method that simultaneously compensates for detailed information lost in the downsampling process and reinforces critical information in upsampling to achieve more accurate discrimination between cloud and non-cloud pixels. The DCA module integrates high-level features with low-level features to ensure that the network maintains its sensitivity to detailed information. Experimental results using the HRC_WHU, CHLandsat8, and 95-Cloud datasets demonstrate that the proposed algorithm surpasses existing optimal methods, achieving finer segmentation of cloud boundaries and more precise localization of subtle thin clouds.</p></abstract><kwd-group><kwd>cloud detection</kwd><kwd>adaptive feature aggregation</kwd><kwd>multi-scale</kwd><kwd>feature fusion</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62261038</award-id></award-group><funding-statement>This research was funded by the National Natural Science Foundation of China, grant number 62261038.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01245"><title>1. Introduction</title><p>Remote sensing technology has played an increasingly important role in the past decade, providing technical support for monitoring critical fields such as agriculture [<xref rid="B1-sensors-25-01245" ref-type="bibr">1</xref>], forestry [<xref rid="B2-sensors-25-01245" ref-type="bibr">2</xref>], oceans [<xref rid="B3-sensors-25-01245" ref-type="bibr">3</xref>], soil [<xref rid="B4-sensors-25-01245" ref-type="bibr">4</xref>], environmental protection [<xref rid="B5-sensors-25-01245" ref-type="bibr">5</xref>], and meteorology [<xref rid="B6-sensors-25-01245" ref-type="bibr">6</xref>]. Accurate cloud detection enables the acquisition of detailed cloud data, which facilitate in-depth analysis of weather systems. This has a profound impact on the formulation of disaster prevention and mitigation strategies, the planning of agricultural production, and ensuring aviation transport safety, among other areas. However, cloud cover can obscure ground information, resulting in varying degrees of blurriness or information loss in the collected data, posing significant challenges to research in these critical fields. To enhance the clarity and imaging quality of remote sensing images, accurate cloud detection is imperative. Traditional cloud detection methods can achieve better results under specific conditions but often show greater limitations when faced with complex and variable cloud morphology, changing lighting conditions, and differences in ground cover.</p><p>Deep neural networks, endowed with robust feature extraction and representation learning capabilities, possess the ability to accurately identify distinct regions within images and extract objects of interest. As a result, deep learning has become widely popular in numerous application areas. Among them, the convolutional neural network (CNN), as a representative model of deep learning in the field of image processing, is able to automatically learn and extract effective feature representations from a large amount of image data. Johnston et al. [<xref rid="B7-sensors-25-01245" ref-type="bibr">7</xref>] applied the CNN to cloud detection, and the algorithm had an amazing ability to apply context information classification, which greatly improved the detection performance. Matsunobu et al. [<xref rid="B8-sensors-25-01245" ref-type="bibr">8</xref>] demonstrated that the CNN could directly identify the presence of clouds in remote sensing imagery at a relatively high temporal resolution without the need for auxiliary data. Chai et al. [<xref rid="B9-sensors-25-01245" ref-type="bibr">9</xref>] designed a shallow CNN, compared to a deep convolutional neural network, which not only achieved more stable training but also significantly reduced detection costs. Liu et al. [<xref rid="B10-sensors-25-01245" ref-type="bibr">10</xref>] proposed a deformable convolutional cloud detection network with an encoder&#x02013;decoder architecture to address the challenge of CNNs lacking an internal mechanism to handle geometric transformations of clouds. However, the performance of this model may still be constrained when faced with extreme weather conditions or complex surface features.</p><p>Attention mechanisms can dynamically adjust the level of attention paid to different regions, and many cloud detection methods use attention mechanisms to improve detection accuracy. Zhang et al. [<xref rid="B11-sensors-25-01245" ref-type="bibr">11</xref>] introduced a CNN based on cascaded feature attention and channel attention, enhancing the focus on crucial image features to improve the accuracy of cloud detection. Zhang et al. [<xref rid="B12-sensors-25-01245" ref-type="bibr">12</xref>] presented a multi-scale-attention convolutional neural network to improve the accuracy of cloud detection by realizing the acquisition of different receptive fields and the learning of pixel importance through a multi-scale module and an attention module, respectively. Zhang et al. [<xref rid="B13-sensors-25-01245" ref-type="bibr">13</xref>] designed integrated color and texture feature attention modules to enhance information extraction and facilitate the web learning of richer details by utilizing dual attention modules. However, this feature extraction method may fail to capture some subtle or complex cloud features, especially when the contrast between clouds and the background is low or when cloud types and morphologies are diverse. Tan et al. [<xref rid="B14-sensors-25-01245" ref-type="bibr">14</xref>] proposed an automatic cloud identification method that combines multi-feature fusion with a channel attention mechanism. This method employs MobileNetV2 as the backbone network on the basis of DeepLabV3+ to effectively distinguish between snow and clouds. Nevertheless, the attention mechanisms in these algorithms primarily focus on the extraction of local features, often neglecting global contextual information, which may affect the accuracy and completeness of overall cloud detection.</p><p>UNet [<xref rid="B15-sensors-25-01245" ref-type="bibr">15</xref>] is a classical network structure in deep learning, which was proposed to solve the problem of biomedical image segmentation. Due to its high scalability and customizability, researchers have improved and optimized it for use in the field of cloud detection according to the needs of specific tasks. Yanan et al. [<xref rid="B16-sensors-25-01245" ref-type="bibr">16</xref>] and Guo et al. [<xref rid="B17-sensors-25-01245" ref-type="bibr">17</xref>] attempted to incorporate attention mechanisms into symmetrical encoder&#x02013;decoder network architectures to more accurately differentiate between cloud and non-cloud pixels. Kanu et al. [<xref rid="B18-sensors-25-01245" ref-type="bibr">18</xref>] proposed a model based on encoder&#x02013;decoder architecture and combined Atrous Spatial Pyramid Pooling and separable convolution to optimize the network and improve the accuracy of cloud detection. Hu et al. [<xref rid="B19-sensors-25-01245" ref-type="bibr">19</xref>] presented Cloud Detection UNet, an encoder&#x02013;decoder network architecture specifically designed for cloud detection in remote sensing images, which is capable of delicately segmenting cloud edges and precisely identifying thin clouds against complex ground backgrounds. Yin et al. [<xref rid="B20-sensors-25-01245" ref-type="bibr">20</xref>] employed Resnet 50 instead of the UNet3+ feature extraction network and added an attention module to extract deep features for the accurate detection of clouds and snow. Li et al. [<xref rid="B21-sensors-25-01245" ref-type="bibr">21</xref>] aimed to embed a dense global context block into the UNet framework for thin cloud detection. Li et al. [<xref rid="B22-sensors-25-01245" ref-type="bibr">22</xref>] enhanced the information flow by cascading two U-shaped networks and utilizing residual connections, and their improved hopping connections facilitate multi-scale feature exploitation, which aids in the identification of thin clouds. Yet, along with the performance improvement, this algorithm also results in an increase in parameter volume and computational complexity, posing key challenges that urgently need addressing in practical applications.</p><p>Originally designed for natural language processing tasks, Transformer&#x02019;s powerful sequence processing capabilities and flexibility have led to corresponding extended applications in cloud inspection. Wen et al. [<xref rid="B23-sensors-25-01245" ref-type="bibr">23</xref>] proposed an improved cloud detection model that integrates Transformer with a multi-scale feature extraction module to efficiently capture global and local multi-scale features in cloud regions. Ma et al. [<xref rid="B24-sensors-25-01245" ref-type="bibr">24</xref>] combined the advantages of both Transformer and the CNN to design a hybrid CNN&#x02013;Transformer network with differential feature enhancement to enhance finer detail extraction and establish long-range dependencies. Singh et al. [<xref rid="B25-sensors-25-01245" ref-type="bibr">25</xref>] presented a spatial&#x02013;spectral attention transformer network that introduces a spatial&#x02013;spectral attention module at its core, forgoing traditional convolutional methods and directly utilizing image patches to generate enhanced feature maps, thereby enhancing cloud detection performance. Tan et al. [<xref rid="B26-sensors-25-01245" ref-type="bibr">26</xref>] proposed an innovative Transformer-based cloud detection method based on Transformer, which optimizes cloud detection by introducing Cyclic Refinement Architecture to enhance the resolution and quality of feature extraction and to effectively retain key details.</p><p>Cloud shadows are areas of darkness that appear on the Earth&#x02019;s surface when sunlight is obstructed by clouds. The presence of these shadowed areas can interfere with the acquisition of surface information. Therefore, a large number of methods for cloud and cloud shadow detection have also been investigated in recent years. Wieland et al. [<xref rid="B27-sensors-25-01245" ref-type="bibr">27</xref>] designed a CNN based on UNet improvement to segment clouds and cloud shadows in remote sensing images. Qu et al. [<xref rid="B28-sensors-25-01245" ref-type="bibr">28</xref>] established a strip pooling channel spatial attention network to accurately extract the local position information of clouds and their shadows and improve the accuracy of edge segmentation. Mohajerani et al. [<xref rid="B29-sensors-25-01245" ref-type="bibr">29</xref>] used a new filtered Jaccard loss based on Cloud-Net [<xref rid="B30-sensors-25-01245" ref-type="bibr">30</xref>] for training and developed a sunlight direction-aware data augmentation technique for cloud shadow detection. Li et al. [<xref rid="B31-sensors-25-01245" ref-type="bibr">31</xref>] proposed a novel Transformer-based cloud shadow detection algorithm, which uses a hierarchical Transformer structure in the encoder to extract cloud shadow features and a multilayer perceptron in the decoder for the fusion of features and pixel classification.</p><p>Although deep learning has shown great potential for application in the field of cloud detection in remote sensing imagery, many existing methods are still not robust enough to deal with the fine definition of cloud boundaries and the accurate identification of thin clouds. Specifically, thick clouds in remote sensing images often occupy prominent positions in stacked patterns, contrasting sharply with background pixels, and are relatively intuitive to detect; however, cloud boundaries are intricate and complex, and accurately depicting complete cloud boundaries has become a major challenge. What is even more difficult is that thin cloud regions scattered in images are difficult to distinguish from the background pixels, which undoubtedly increases the detection difficulty. Therefore, how to perfectly outline the cloud boundaries and accurately capture thin cloud regions has become a key challenge to be solved. To address this challenge, we propose the NFCNet algorithm model based on coding and decoding structures. The innovative contributions of this study are summarized as follows:<list list-type="bullet"><list-item><p>The HCAM possesses the capability to capture multi-scale information, offering a more extensive understanding of the intricate morphology and hierarchical organization of clouds. Furthermore, it expertly assigns differentiated weights to feature channels, thereby enhancing the crucial feature details essential for cloud detection.</p></list-item><list-item><p>The SPFA module employs a novel adaptive feature aggregation method, which not only comprehensively reveals the spatial distribution characteristics of cloud layers but also precisely focuses on and meticulously captures local details, transcending the limitations of merely acquiring local features. Consequently, this method excels in achieving both the precise identification of delicate thin clouds and intricate delineation of cloud boundaries simultaneously.</p></list-item><list-item><p>The use of the DCA module fuses the hierarchical details of encoding and decoding to guide the detail recovery of the feature maps, enabling more accurate localization of clouds and finer outlining of cloud boundaries.</p></list-item><list-item><p>In the encoding stage, the model skillfully fuses high-resolution and low-resolution information to achieve the complementary enhancement of fine details and extensive contextual information. This complementary information enables it to flexibly adapt to diverse application scenarios and improve its recognition of thin clouds.</p></list-item></list></p><p>The layout of the subsequent sections is as follows. <xref rid="sec2dot1-sensors-25-01245" ref-type="sec">Section 2.1</xref> describes our proposed NFCNet in detail. The dataset used and the validity of our algorithm through extensive experiments are presented in <xref rid="sec3-sensors-25-01245" ref-type="sec">Section 3</xref>. Finally, we give conclusions in <xref rid="sec4-sensors-25-01245" ref-type="sec">Section 4</xref>.</p></sec><sec sec-type="methods" id="sec2-sensors-25-01245"><title>2. Methodology</title><p>In this section, we will detail the proposed deep learning cloud detection network architecture based on deep learning. Firstly, we will introduce the overall framework of the network. Subsequently, we will elaborate on the three pivotal submodules that constitute the network. Lastly, we will introduce the network training loss function employed.</p><sec id="sec2dot1-sensors-25-01245"><title>2.1. Overview</title><p>As depicted in <xref rid="sensors-25-01245-f001" ref-type="fig">Figure 1</xref>, the network architecture of the proposed model is improved by the use of encoders, decoders, and hopping connection operations designed to achieve efficient differentiation between clouds and background. The cloud map undergoes four levels of encoder operations to gradually extract and compress the features of the input image to form a coded feature map. In the decoding stage, the feature map is upsampled level by level to recover the spatial dimension of the image to match the input feature map size.</p><p>The overall model consists of the HCAM, SPFA module, and DCA module, which can effectively capture semantic information and detailed features in an image and cope with the challenges of the complex shapes of cloud maps and highly similar regions to the background. In the encoding stage, the HCAM is used to extract different scale features in the image, which cover all aspects from local details to global structure, and pays special attention to the critical detailed information in the cloud image, thus greatly improving the detection of small-scale objects. Meanwhile, the SPFA module is introduced to enhance the feature representation in the channel and spatial dimensions, effectively compensating for the spatial details lost due to the downsampling operation. In the decoding stage, the SPFA module further enhances the key information in the upsampled feature map to achieve the optimization of the segmentation effect. The high-resolution feature maps generated by the encoder are combined with the same-resolution feature maps obtained after the upsampling process to achieve complementary information, and this combination constitutes one end of the input to the DCA module. The other input is derived from the output of the decoder after upsampling. The DCA module skillfully integrates the shallow details from the encoder with the deep semantic features of the decoder, aiming to balance the semantic and detailed information, enhance the richness of the cloud map features in the decoding stage, and then improve the efficiency and feasibility of decoding.</p></sec><sec id="sec2dot2-sensors-25-01245"><title>2.2. HCAM</title><p>In deep neural networks, the receptive field is the range of influence of the output neurons of a layer in the neural network on the input data, and the magnitude of its value indicates the amount of contextual information that can be accessed and determines the extent to which the neural network understands the input data, which has a direct impact on the accuracy of the target recognition.</p><p>To improve the network&#x02019;s ability to perceive local features and capture more contextual information, it is necessary to increase the receptive field. Nevertheless, the study of Zhou et al. [<xref rid="B32-sensors-25-01245" ref-type="bibr">32</xref>] showed that the receptive field of convolution is limited; for deeper networks, the perceptual domain is much smaller than the theoretical one, making it difficult to capture long-range feature correlations. The receptive field can be expanded layer by layer by increasing the number of convolutional layers, but Sun et al. [<xref rid="B33-sensors-25-01245" ref-type="bibr">33</xref>] showed that for DNNs with a limited number of hidden units, increasing the depth is not always good, and too deep a network may lead to problems such as gradient vanishing or gradient explosion. He et al. [<xref rid="B34-sensors-25-01245" ref-type="bibr">34</xref>] proposed Spatial Pyramid Pooling; although it can extend the receptive fields by pooling operations at different scales, the sizes of these receptive fields are fixed and cannot fully adapt to changes in the sizes and shapes of all targets in the input image. In addition, the parallel use of multiple pooling of different sizes to fuse multi-scale features inevitably produces some degree of information loss. To overcome these limitations, we propose the use of dilated convolutions with varying dilation rates [<xref rid="B35-sensors-25-01245" ref-type="bibr">35</xref>] to increase the receptive field, thereby enhancing the semantic information of deep features. This is beneficial for improving the detection performance of small cloud targets. The advantage of dilated convolution lies in preserving the original network&#x02019;s receptive field while not losing spatial resolution. This feature enables it to effectively address the issue of reduced spatial resolution associated with the subsampling process, and maintaining spatial resolution is crucial for cloud detection tasks involving multi-detail scenes.</p><p>Each channel of the feature map carries different feature information, and the degree of contribution of these channels in cloud detection tasks varies as well. The SE Block [<xref rid="B36-sensors-25-01245" ref-type="bibr">36</xref>] dynamically allocates feature weights based on the learning loss to match higher weights for key feature channels and reduce the interference of irrelevant regions on the prediction results. Firstly, the features are compressed spatially through global average pooling, transforming each two-dimensional feature channel into a real number with a global receptive field. Secondly, after obtaining the global information, the generation and selection of feature channel weights are achieved in a parameterized manner. Finally, the adjusted importance weights are multiplied element-wise with the original feature map, effectively recalibrating the original features along the channel dimension. This further enhances the expressive power and specificity of the features.</p><p>Combining these two points, we propose the HCAM, as shown in <xref rid="sensors-25-01245-f002" ref-type="fig">Figure 2</xref>. Firstly, a 3 &#x000d7; 3 convolution with a stride of 1 is used to reduce the number of channels, thereby not only alleviating the computational burden but also adeptly capturing local features. Secondly, dilated convolutions with different resolutions are employed to extract features at various scales, with resolutions set to 1, 6, 12, and 18, respectively. Thirdly, a 1 &#x000d7; 1 convolution is utilized to adjust the dimensions of the multi-scale features fused in the previous step. Fourthly, the SE Block mechanism is integrated to dynamically assign importance weights to the channels, focusing on the key features in the cloud detection task. Fifthly, the calibrated channel importance weights are multiplied element-wise with the output feature map from step three to achieve feature reweighting. Finally, a 3 &#x000d7; 3 convolution is applied to obtain the refined feature map output, where the SE Block part and the final output feature maps are given by the following equations:<disp-formula id="FD1-sensors-25-01245"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:munderover><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-01245"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mfrac></mml:mstyle><mml:mo>&#x000d7;</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-sensors-25-01245"><label>(3)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="" open="{" close="}"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>F</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the output feature map after 1 &#x000d7; 1 convolution for dimension adjustment, <italic toggle="yes">F</italic> denotes the local feature map extracted by the first 3 &#x000d7; 3 convolution, <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the global average pooling, <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is BatchNormal, <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula> is the Relu activation function, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mi>&#x003b4;</mml:mi></mml:mrow></mml:math></inline-formula> is the sigmoid function, <italic toggle="yes">H</italic> and <italic toggle="yes">W</italic> denote the height and width of the feature maps, and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> refer to the two fully connected operations, where the first fully connected layer achieves feature dimensionality reduction and fusion by decreasing the number of channels, while the second fully connected layer recalibrates the reduced-dimensionality features.</p></sec><sec id="sec2dot3-sensors-25-01245"><title>2.3. SPFA</title><p>When an image undergoes downsampling, it loses contextual information and details, which makes it difficult to recover the lost boundaries and texture-rich region features during the upsampling process to restore the image resolution. Therefore, it is particularly important to enhance the network&#x02019;s ability to capture long-range semantic relationships to achieve more accurate boundary detail predictions. Guo et al. [<xref rid="B37-sensors-25-01245" ref-type="bibr">37</xref>] used multiple strip convolutions in a pyramid structure to obtain multi-scale global contextual information, enhancing the detection of strip-shaped targets. Liao et al. [<xref rid="B38-sensors-25-01245" ref-type="bibr">38</xref>] used a parallel strip convolution enhancement network to improve the perception ability of the target area. To enhance the semantic information of deep features, we use grouped strip convolution [<xref rid="B39-sensors-25-01245" ref-type="bibr">39</xref>] to extract multi-scale features. Strip convolution increases the model&#x02019;s receptive field in that dimensional direction by dividing the regular convolution into long strips of convolution kernels in the horizontal and vertical directions, allowing the model to capture long-distance spatial dependencies more efficiently.</p><p>In order to enhance the model&#x02019;s attention to detailed features during downsampling and at the same time realize the effective recovery of key features in the upsampling stage, we not only need to pay attention to the extraction of channel features but also need to strengthen the capture and utilization of spatial information. The SE Block used in the HCAM can dynamically assign channel importance weights but lacks access to spatial information, whereas the cloud detection task requires not only identifying the presence of clouds but also accurately locating their positions. Inspired by the CBAM&#x02019;s [<xref rid="B40-sensors-25-01245" ref-type="bibr">40</xref>] ability to focus on both channel and spatial information, several excellent algorithms improve the use of the CBAM to improve target detection accuracy. For example, Cheng et al. [<xref rid="B41-sensors-25-01245" ref-type="bibr">41</xref>] designed multi-scale fused attention modules for more accurate depth prediction. Meng et al. [<xref rid="B42-sensors-25-01245" ref-type="bibr">42</xref>] proposed a multifunctional fused attention gate module to effectively suppress irrelevant background interference. The channel attention module relies on pooling operations when extracting channel features, but this process inevitably leads to the loss of some information [<xref rid="B34-sensors-25-01245" ref-type="bibr">34</xref>]. Furthermore, due to the limitation of the receptive field, the module has constraints in capturing global information, making it difficult to fully obtain the comprehensive features of the entire input data. To overcome this limitation, the self-attention [<xref rid="B43-sensors-25-01245" ref-type="bibr">43</xref>] mechanism is introduced. Self-attention improves the detection of fine thin clouds in cloud maps by capturing the correlation between two elements to obtain dense pixel-level contextual information. Specifically, this method calculates the similarity between Query and Key and then uses these similarities as weights to perform a weighted summation on Value. This allows for the effective capture and fusion of important information within the input sequence, thereby generating a comprehensive representation that contains both the original information and contextual information.</p><p>Combining the above analyses, we introduce the SPFA module, which aims to efficiently compensate for the loss of information details in the downsampling stage and significantly enhance detail expressiveness during the upsampling process. The detailed structure of the SPFA module is shown in <xref rid="sensors-25-01245-f003" ref-type="fig">Figure 3</xref>. First, the framework incorporates a parallel multi-scale stripe convolution mechanism to accurately extract and fuse multi-level features. The expression for this process is as follows:<disp-formula id="FD4-sensors-25-01245"><label>(4)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x0223c;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD5-sensors-25-01245"><label>(5)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-01245"><label>(6)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>[</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the input feature map, with stripe convolution groups corresponding to 2, 4, and 8, respectively, from smaller to larger scales.</p><p>Second, to broaden the model&#x02019;s global perspective, we cleverly integrate a self-attention mechanism to apply global attention weighting to the feature maps while simultaneously combining a spatial attention module to achieve the precise localization of cloud pixel positions. Additionally, we deploy an MLP in parallel, with a dimensionality reduction rate of 16 set through experimental testing. The first layer of the MLP reduces the input feature dimension to 1/16 of its original size, while the second layer is responsible for restoring it to the original dimension. Subsequently, we perform a meticulous pixel-level concatenation of the output feature maps from the MLP with the feature maps processed by the attention mechanism to further enrich the feature representation, where the mathematical equation of the detailed process of self-attention combined with spatial attention can be expressed as follows:<disp-formula id="FD7-sensors-25-01245"><label>(7)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-25-01245"><label>(8)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:mrow><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mi>V</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mi>X</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-01245"><label>(9)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:msub><mml:mi>F</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-01245"><label>(10)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced separators="" open="{" close="}"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-01245"><label>(11)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-01245"><label>(12)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot4-sensors-25-01245"><title>2.4. DCA</title><p>The encoder part goes through the HCAM, SPFA module, and downsampling operations to gradually extract high-level semantic features of an image. Still, at the same time, the spatial resolution is gradually reduced, causing the decoder to reconstruct the target data without sufficient spatial location information and local details to affect the generation of a high-quality output. Therefore, the introduction of the DCA module realizes the effective integration of shallow and deep feature maps, which play a key role in the process of information feature fusion at different scales and semantic levels.</p><p>The DCA module is designed as an efficient feature fusion mechanism. It first performs multi-scale modeling on the input features at the decoding end to obtain multi-scale features that contain rich semantic information to enhance the detection performance of cloud boundaries with different morphologies. Then, the DCA module skillfully fuses the features at the encoding end with the multi-scale features at the decoding end to realize feature complementation and enhancement. Subsequently, the fused features are injected into the decoding phase, ensuring that during the gradual restoration of the image&#x02019;s spatial resolution, the image details can be effectively and finely recovered, thereby enabling more accurate segmentation decisions to be made.</p><p>The proposed skip connection DCA module is shown in <xref rid="sensors-25-01245-f004" ref-type="fig">Figure 4</xref>; input1 is the shallow feature map and input2 is the deep feature map. First, we let the shallow features reduce the spatial dimensions by global average pooling, with the channel dimensions still retained. Since the channel averages of the global average pooling output features directly reflect the confidence level of the corresponding category of the channel, a 1 &#x000d7; 1 convolution is considered to learn the weights between different channels to adjust the importance of the features more flexibly. The mathematical calculation equation for the process is as follows:<disp-formula id="FD13-sensors-25-01245"><label>(13)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="" open="{" close="}"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="" open="[" close="]"><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Next, the output of the decoder is used as an input to the input2 port of the DCA module in the following steps. In step 1, the deep features are convolved 1 &#x000d7; 1 to obtain <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, reducing the number of channels in the feature map. In step 2, the feature map <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is executed as strip convolution operations with four different-sized convolution kernels to capture different scale features and increase the diversity of feature representations, after which the four outputs are combined in channel dimensions to obtain <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. In the third step, a 3 &#x000d7; 3 convolution operation is performed on the output feature <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for further feature extraction and smoothing process. The equation is expressed as follows:<disp-formula id="FD14-sensors-25-01245"><label>(14)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced separators="" open="{" close="}"><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD15-sensors-25-01245"><label>(15)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>[</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD16-sensors-25-01245"><label>(16)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Third, <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are multiplied element by element, and a 1 &#x000d7; 1 convolution outputs the shallow and deep fused feature maps.<disp-formula id="FD17-sensors-25-01245"><label>(17)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="" open="{" close="}"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot5-sensors-25-01245"><title>2.5. Loss Function</title><p>In deep learning image segmentation, the output prediction results hinge on the design of the network architecture and the choice of the loss function, which is a pivotal component in the model optimization process and defines how the difference is measured between the model prediction and the true label. Different loss functions will steer the model towards learning in distinct manners, thereby influencing the performance and behavior of the model. By taking into account the types of datasets and the characteristics of cloud pixels, we chose the cross-entropy loss function to train the network model and employed the Adam optimizer to optimize the loss function. The cross-entropy loss quantified the discrepancy between the output probability distribution of the model and the probability distribution of the real labels, effectively distinguishing the difficulty of samples during the training process and facilitating faster convergence. The mathematical expression of the cross-entropy loss function is defined as follows:<disp-formula id="FD18-sensors-25-01245"><label>(18)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munder><mml:mfenced separators="" open="{" close="}"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">N</italic> denotes the number of pixels in each image, <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the true value of the cloud label, and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the prediction result.</p></sec></sec><sec id="sec3-sensors-25-01245"><title>3. Experimental Results</title><sec id="sec3dot1-sensors-25-01245"><title>3.1. Dataset</title><list list-type="order"><list-item><p>The CHLandsat8 dataset is designed for cloud detection in high-resolution remote sensing images of Chinese regions. The dataset contains 64 full-scene images from different regions, covering diverse weather conditions, land cover types, and cloud morphology. In the experiments, the original 8000 &#x000d7; 8000 pixel images were cropped to 352 &#x000d7; 352 pixel blocks to reduce the computational complexity; finally, 22,616 images were obtained for training and 10,080 images were obtained for testing and validation. During the training process, we resized the images to 256 &#x000d7; 256 and randomized and flipped the images to increase the diversity of the data.</p></list-item><list-item><p>The HRC_WHU dataset [<xref rid="B44-sensors-25-01245" ref-type="bibr">44</xref>] is a high-resolution cloud cover validation dataset established by Wuhan University. It comprises 150 high-quality images with resolutions ranging from 0.5 to 15 meters, comprehensively covering five major land surface types: water, vegetation, urban areas, snow and ice, and barren land. The images possess a pixel size of 1280 &#x000d7; 720. During preprocessing, each image was randomly cropped into 80 images of 256 &#x000d7; 256 pixels, with blurry and duplicate images being eliminated. Consequently, a total of 8800 images were utilized for training, while 3150 images were designated for testing and validation. Throughout the training process, the images underwent random permutations and flips.</p></list-item><list-item><p>The 95-Cloud dataset [<xref rid="B29-sensors-25-01245" ref-type="bibr">29</xref>] represents an expansion of the 38-Cloud dataset [<xref rid="B30-sensors-25-01245" ref-type="bibr">30</xref>], which inherits the characteristics of the 38-Cloud dataset and adds more data and samples. This expanded dataset aims to support more sophisticated cloud detection and segmentation tasks. The training patches for the dataset were extracted from 75 Landsat 8 Collection 1 Level-1 scenes. Given the presence of black areas around the Landsat 8 image scenes, empty pixels occupying over 80% of the training patches were excluded to enhance training efficiency. In the training process of the model, we merged the three RGB channels of the dataset to form a three-channel data image and resized the original pixel size of the training images from 384 &#x000d7; 384 to 256 &#x000d7; 256, applied random permutations, and flipped the images. To further improve the model&#x02019;s generalization capability, the 38-Cloud and 95-Cloud datasets were combined, with 80% of the combined data used for training and the remaining 20% reserved for testing and validation.</p></list-item></list></sec><sec id="sec3dot2-sensors-25-01245"><title>3.2. Evaluation Metrics</title><p>We assessed the accuracy and reliability of the segmentation results from multiple dimensions by employing some common evaluation metrics commonly used in cloud detection tasks.<disp-formula id="FD19-sensors-25-01245"><label>(19)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD20-sensors-25-01245"><label>(20)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD21-sensors-25-01245"><label>(21)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD22-sensors-25-01245"><label>(22)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD23-sensors-25-01245"><label>(23)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD24-sensors-25-01245"><label>(24)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>In the above equation, <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is True Positive, <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is True Negative, <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is False Positive, and <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is False Negative.</p></sec><sec id="sec3dot3-sensors-25-01245"><title>3.3. Ablation Experiment</title><p>In this section, in order to evaluate the contribution of each module to the overall model, we detail how we conducted per-module ablation experiments on the HRC_WHU dataset. Among other things, to present the advantages of the HCAM, we chose to alternate the basic two-time 3 &#x000d7; 3 convolution instead of using the HCAM as a baseline on the codec.</p><p>As shown in the above <xref rid="sensors-25-01245-t001" ref-type="table">Table 1</xref>, compared to the baseline model, the introduction of the HCAM achieved a significant improvement in model performance, showing an all-round optimization effect. Specifically, Dice, IoU, Re, and F1 were improved by 2.44%, 4.03%, 2.54%, and 2.39%, respectively, presenting the advantages of HCAM&#x02019;s ability to capture key features and especially perform well in enhancing the completeness of the model&#x02019;s ability to recognize all relevant instances. The SPFA module was further fused to realize the deep mining of multi-scale contextual information and effective compensation of detailed features, which significantly enhanced the detection accuracy of the model. It is worth noting that when the output of the SPFA module was directly used as the input of the DCA module; the comprehensive evaluation of Pre and Re was degraded, whereas combination with high resolution through upsampling as the input of the DCA module could produce substantial improvements in the model. The superiority of the fusion of high-resolution and low-resolution strategies at the coding end of the DCA module could be demonstrated, providing strong support for achieving finer detail recovery in the decoding stage.</p></sec><sec id="sec3dot4-sensors-25-01245"><title>3.4. Comparison Experiment</title><p>To verify the performance of our algorithm on cloud detection, we compared the proposed model with other excellent cloud detection models on three datasets: HRC_WHU, CHLandsat8, and 95-Cloud. Specifically, we compared it to UNet [<xref rid="B15-sensors-25-01245" ref-type="bibr">15</xref>], Deeplabv3+ [<xref rid="B45-sensors-25-01245" ref-type="bibr">45</xref>], SAtt-UNet [<xref rid="B16-sensors-25-01245" ref-type="bibr">16</xref>], Cloud-AttU [<xref rid="B17-sensors-25-01245" ref-type="bibr">17</xref>], DCNet [<xref rid="B10-sensors-25-01245" ref-type="bibr">10</xref>], CRSNet [<xref rid="B46-sensors-25-01245" ref-type="bibr">46</xref>], BABFNet [<xref rid="B47-sensors-25-01245" ref-type="bibr">47</xref>], MCDNet [<xref rid="B48-sensors-25-01245" ref-type="bibr">48</xref>], and AFMUNet [<xref rid="B49-sensors-25-01245" ref-type="bibr">49</xref>]. Among them, UNet and Deeplabv3+ are classical network models in semantic segmentation, which have achieved remarkable results in the field of cloud detection. Since our algorithm is based on an encoder&#x02013;decoder architecture, we chose the SAtt-UNet, Cloud-AttU, and DCNet models for comparison. These three models, along with CRSNet, BABFNet, MCDNet, and AFMUNet, are all specifically designed for cloud detection tasks and have achieved high accuracy.</p><sec id="sec3dot4dot1-sensors-25-01245"><title>3.4.1. Cloud Detection Results on HRC_WHU Dataset</title><p><xref rid="sensors-25-01245-t002" ref-type="table">Table 2</xref> shows the results of the experimental data metrics&#x02019; output on the HRC_WHU dataset for the proposed algorithm and the nine compared models. It is clearly seen that our proposed algorithm was optimal in the five metrics of PA, Dice, IoU, Pre, and F1, and although Re was not the highest, there was only a small difference compared to the best-performing models. The excellent performance of these metrics reflects the advantages of our proposed algorithm in terms of comprehensive performance, especially in the handling of complex backgrounds and diverse cloud shapes, which shows high stability and adaptability.</p><p>The HRC_WHU dataset contained scenarios with five different land cover types. Among them, the cloud detection task with snow/ice and water as the background was particularly challenging and highly susceptible to false detection and missed detection problems. In order to further validate the performance of the NFCCNet algorithm in different scenarios, we selected six models that performed well in the overall dataset and evaluated them in exhaustive experiments. As shown in <xref rid="sensors-25-01245-f005" ref-type="fig">Figure 5</xref>, our algorithm exhibited optimal performance under snow/ice and water backgrounds, almost comprehensively outperforming other algorithms. Meanwhile, NFCNet also presented the highest ACC and IoU in all other scenarios, which fully proves that our algorithm was able to show better stability and anti-interference ability when facing highly disturbed backgrounds.</p><p>To more intuitively demonstrate the improvement in the proposed algorithm in cloud boundary identification and thin cloud detection capabilities, we screened six sets of images from the test set for the comparative demonstration of model prediction effects. <xref rid="sensors-25-01245-f006" ref-type="fig">Figure 6</xref> displays the segmentation results of various algorithms on six sets of images, with red marks indicating missed detection pixels, green marks indicating falsely detected pixels, white representing cloud areas, and black representing the background. From the results, it is easy to see that all the algorithms showed high accuracy in localizing thick cloud regions, but there were different degrees of leakage and misdetection when dealing with cloud boundaries and thin cloud regions. Among them, Cloud-AttU, DCNet, and AFMUNet were unable to detect the fine thin cloud pixels in the third set of images, and although other comparison algorithms made some improvements in this regard, there were many falsely detected pixels for the cloud boundary region. For the background area where cloud boundaries and thin clouds intertwined in the second set of images, most of the comparison algorithms almost predicted this area as cloud. And, for the thin cloud boundary regions in the fourth and fifth set of images, all the algorithms had a hard time outlining them completely. In contrast, our proposed algorithm performed optimally in depicting cloud boundaries and detecting thin cloud areas, with the minimum number of falsely detected and missed pixels. Furthermore, for the last set of images, our algorithm could still detect the cloud outline well even in the presence of a highly disturbed background, demonstrating its superior anti-interference capability. This outstanding performance is attributed to the innovative design of our algorithm: by utilizing dilated convolutions with different dilation rates to extract features, the receptive field was effectively expanded, enhancing the detection capability for fine and thin clouds. Additionally, we introduced the channel attention mechanism, which enabled the algorithm to focus on the most critical information by differentially weighting the features of each channel, thus further enhancing the algorithm&#x02019;s ability to discriminate clouds in complex backgrounds.</p></sec><sec id="sec3dot4dot2-sensors-25-01245"><title>3.4.2. Cloud Detection Results on CHLandsat8 Dataset</title><p><xref rid="sensors-25-01245-t003" ref-type="table">Table 3</xref> displays the experimental data indicator results outputted by all algorithms on the CHLandsat8 dataset. From the data analysis, it can be observed that our algorithm exhibited superior performance compared to other algorithms in terms of the indicators PA, Dice, IoU, and F1. Although the Pre and Re data did not attain the highest values, the comprehensive evaluation indicator F1 achieved the best result, which effectively demonstrates that our model possessed good stability and robustness.</p><p>To more intuitively demonstrate the performance advantages of our algorithm, we also selected six sets of images from the test set for the predictive comparison of the models. These images were chosen with a focus on scenarios where cloud boundaries were intricate and the detection of thin cloud areas was particularly challenging. As shown in <xref rid="sensors-25-01245-f007" ref-type="fig">Figure 7</xref>, in the first, fourth, and fifth sets of images, the comparison algorithms suffered from severely missed detections when capturing thin cloud regions that were highly similar to the background, indicating a lack of sufficient context extraction capability. In the second set of images, detecting thin cloud boundaries was extremely challenging, and apart from BABFNet, the other algorithms handled the details roughly, failing to finely depict the boundaries of the thin clouds. In the third and sixth sets of images, the comparison algorithms performed unsatisfactorily in detecting small cloud targets, mainly due to the insufficient processing of spatial location information, making precise localization difficult. In contrast, our proposed algorithm demonstrated significant advantages in thin cloud localization and boundary detection. This was primarily attributed to the use of parallel multi-scale stripe convolution technology in our algorithm, which significantly enhanced the network&#x02019;s ability to capture long-distance semantic relationships, thereby improving the detection accuracy for thin clouds. Furthermore, we cleverly combined the self-attention mechanism and spatial attention mechanism to precisely guide the model in compensating for boundary details, effectively mitigating the significant loss of detailed information caused by the downsampling process. Going further, the fusion of feature information by the DCA module was injected into the decoding stage, which provided strong support for detail recovery and enabled the model to depict the cloud boundary morphology in a more delicate and precise manner.</p><p>It is noteworthy that a major challenge our algorithm faced in the cloud detection task of this dataset was the high number of falsely detected pixels. The root of this issue lay in certain defects in the design of the feature extraction algorithm. These deficiencies resulted in the algorithm mistakenly classifying more non-cloud areas as clouds during the recognition process, thereby affecting the overall detection accuracy.</p></sec><sec id="sec3dot4dot3-sensors-25-01245"><title>3.4.3. Cloud Detection Results on 95-Cloud Dataset</title><p>To more comprehensively validate the performance of the proposed method in cloud detection, we also conducted comparative experiments on the 95-Cloud dataset. <xref rid="sensors-25-01245-t004" ref-type="table">Table 4</xref> presents the experimental data and performance metrics of various algorithms on the 95-Cloud dataset. As can be seen from the data in the table, our algorithm outperformed the comparison model in all evaluation metrics except for the Re data. Specifically, the BABFNet model achieved the best performance on the comparison algorithm, while our algorithm was improved by 0.51%, 1.38%, 1.83%, 1.09%, 1.58%, and 1.42% in PA, Dice, IoU, Pre, Re, and F1, respectively. This fully demonstrates the feasibility of the innovative model functions of our algorithm, as well as the superiority of its integration strategy, enabling more accurate cloud detection.</p><p>We also selected six sets of images from the test set for the visual comparison of the cloud detection performance of each model. These images all met the criteria of having complex cloud boundaries and difficult-to-identify, thin cloud areas. As shown in <xref rid="sensors-25-01245-f008" ref-type="fig">Figure 8</xref>, from an overall perspective, the segmentation results of UNet and DeeplabV3+ were relatively rough, with a large number of false detections. Especially in the first three sets of images, these two algorithms tended to mistakenly classify background areas with similar textures to clouds as clouds. In comparison, CRSNet, BABFNet, and AFMUNet exhibited relatively better detection performance, demonstrating stronger anti-interference capabilities when processing background pixels. However, they still suffered from a certain degree of false detections and missed detections. In the middle background area of the fourth set of images, there was high similarity between the background&#x02019;s pixels and those of thin clouds. Due to the lack of effective feature differentiation and detail capture capabilities, UNet, DeeplabV3+, CRSNet, and DCNet mistakenly identified a large number of background pixels as clouds when processing this area. The last two sets of images similarly highlighted the deficiencies of the comparison algorithms in localizing thin cloud regions.</p><p>Our proposed algorithm achieved certain results in reducing false detections and missed detections, particularly in depicting fine thin cloud regions and boundary details with higher precision. This was attributed to our use of parallel dilated convolutions to capture multi-scale information, enabling the model to better understand the morphology and distribution of clouds. Meanwhile, the use of the attention mechanism effectively reduced the loss of detailed information and realized the complementary enhancement of fine details and extensive contextual information by efficiently fusing the information features of different scales and semantic levels in the process of recovering the image resolution, which further improved the quality of recovering the boundary pixels.</p><p>However, our algorithm also had deficiencies in restoring the details of thin cloud boundaries. Specifically, the boundaries of the thin cloud regions were often not clear enough in the detection process, and there was a loss of some detailed information, which affected the accuracy and credibility of the cloud detection results to a certain extent.</p></sec></sec><sec id="sec3dot5-sensors-25-01245"><title>3.5. Experimental Analysis and Prospects</title><p>In this study, we conducted comprehensive cloud detection experiments on three representative remote sensing image datasets. The experimental results indicate that our algorithm performed excellently across all three datasets in terms of comprehensive evaluation metrics, outperforming other advanced algorithms for comparison. Notably, the CHLandsat8 dataset, due to its wide range of geographical and climatic conditions, significantly increased the complexity of the cloud detection task, leading to the inferior performance of various algorithms on this dataset compared to the other two. In contrast, although the 95-cloud dataset also exhibited diversity, its data distribution was relatively more concentrated, which to some extent reduced the detection difficulty. Meanwhile, the use of the HRC_WHU dataset focused more on testing the models&#x02019; ability to accurately capture detailed information, which was not the primary challenge posed by the CHLandsat8 dataset.</p><p>By visualizing and analyzing the results of our experiments, we drew the conclusion that the proposed algorithm demonstrated superior performance on the HRC_WHU dataset. Specifically, it exhibited a low count of falsely detected and missed detected pixels while possessing notable advantages in capturing boundary details and identifying thin clouds. However, on the CHLandsat8 dataset, the algorithm&#x02019;s performance in handling boundary details was somewhat lacking, with a relatively high number of missed detected pixels, and was particularly prone to misclassifying similar background areas near cloud boundaries as clouds. As for the 95-Cloud dataset, our algorithm performed adequately in accurately locating thin cloud regions, but there were still minor flaws, specifically errors in pixel classification within some thin cloud boundary areas. This situation could be attributed to two main factors: firstly, the diversity in cloud shapes and background complexity across different datasets posed challenges for the algorithm in adapting to new environments; secondly, the generalization ability of the algorithm itself needs to be improved, especially in terms of feature extraction, where there are design deficiencies that make it difficult to fully capture and represent detailed features. Therefore, to further enhance the algorithm&#x02019;s performance across different datasets, we need to conduct in-depth optimizations and improvements in these areas.</p><p>Based on the above analysis, in order to further enhance the performance of the algorithm across various datasets, especially in terms of improving its ability to handle boundary details and recognize complex cloud patterns, we plan to actively introduce and integrate advanced diffusion models into the algorithm framework in our subsequent research. These models have demonstrated remarkable potential in the fields of image processing and computer vision, particularly in capturing fine-grained features of images and enhancing the generalization performance of models. For instance, Rsdiff [<xref rid="B50-sensors-25-01245" ref-type="bibr">50</xref>] is able to capture local and global features in images more efficiently by introducing a random wandering and diffusion process. Crs-diff [<xref rid="B51-sensors-25-01245" ref-type="bibr">51</xref>] is a method based on cross-diffusion and adaptive feature extraction, which is capable of effectively removing noises and interfering with information while preserving the details of an image. Based on the introduction of the diffusion model, we will also carry out the in-depth optimization and adjustment of our algorithm. This will include fine-tuning the model parameters to adapt to the characteristics of different datasets, optimizing the feature selection strategy to improve the efficiency and accuracy of the algorithm, and introducing more diverse regularization methods to enhance the stability and generalization performance of the model. The aim will be to ensure that the algorithm can show excellent and stable performance on all types of datasets, especially in handling complex cloud morphology and fine boundary details.</p></sec></sec><sec sec-type="conclusions" id="sec4-sensors-25-01245"><title>4. Conclusions</title><p>With the advancement of remote sensing technology, it has become easier to acquire remote sensing images. The accuracy of cloud detection, as a preprocessing step to extract key information from images, is crucial to ensure the effective utilization of remotely sensed data. In this study, we propose a deep neural network based on a multi-feature fusion attention mechanism for cloud detection in remote sensing images by considering the advantages of codec structures and cloud image features for design. Firstly, we cleverly utilized dilated convolutions with varying dilation rates to capture multi-scale information, effectively enlarging the receptive field and enhancing the model&#x02019;s ability to perceive global features. At the same time, by assigning differentiated weights to the channels, the model can focus more on important feature information. Secondly, to further improve model performance, we designed the SPFA module. This module ingeniously employs multi-scale strip convolution techniques to capture the long-distance spatial dependencies of cloud layers, thereby enabling a deeper understanding of their spatial distribution patterns. Additionally, the SPFA module integrates a self-attention mechanism to explore the intrinsic relationships between pixels. In addition, the DCA module is used to fuse low-level and high-level information in skip connections to improve the efficiency of semantic information propagation, enabling more feature details to be acquired while recovering the spatial resolution of the image. Our results from a series of experiments on the HRC_WHU, CHLandsat8, and 95-Cloud datasets showed a significant improvement in the cloud detection accuracy of our model compared to previous cloud detection methods. The visualization experiments demonstrated that our algorithm was able to generate smooth and accurate boundary predictions for complex cloud maps and also showed excellent localization ability for hard-to-see thin clouds. Nevertheless, we recognize that the algorithm may still lose some details when detecting extremely thin cloud regions. Therefore, we will continue to work on optimizing the algorithm to capture the details of debris and thin cloud features more accurately to further improve the comprehensiveness and accuracy of cloud detection. </p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, W.Z. and Y.M.; Methodology, W.Z.; Software, W.Z.; validation, W.Z., Y.M., and S.B.; Investigation, W.Z.; Resources, Q.O.; Data curation, W.Z.; writing&#x02014;original draft preparation, W.Z.; writing&#x02014;review and editing, Y.M.; visualization, W.Z.; supervision, Y.M.; Project administration, S.B.; Funding acquisition, Y.M. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data provided in this study are available from the first author on request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest. The funders had no role in the design of this study.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01245"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>do Nascimento Bendini</surname><given-names>H.</given-names></name>
<name><surname>Fonseca</surname><given-names>L.M.G.</given-names></name>
<name><surname>Schwieder</surname><given-names>M.</given-names></name>
<name><surname>K&#x000f6;rting</surname><given-names>T.S.</given-names></name>
<name><surname>Rufin</surname><given-names>P.</given-names></name>
<name><surname>Sanches</surname><given-names>I.D.A.</given-names></name>
<name><surname>Leitao</surname><given-names>P.J.</given-names></name>
<name><surname>Hostert</surname><given-names>P.</given-names></name>
</person-group><article-title>Detailed agricultural land classification in the Brazilian cerrado based on phenological information from dense satellite image time series</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2019</year><volume>82</volume><fpage>101872</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2019.05.005</pub-id></element-citation></ref><ref id="B2-sensors-25-01245"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>H&#x000e4;rk&#x000f6;nen</surname><given-names>S.</given-names></name>
<name><surname>Lehtonen</surname><given-names>A.</given-names></name>
<name><surname>Eerik&#x000e4;inen</surname><given-names>K.</given-names></name>
<name><surname>Peltoniemi</surname><given-names>M.</given-names></name>
<name><surname>M&#x000e4;kel&#x000e4;</surname><given-names>A.</given-names></name>
</person-group><article-title>The title of the cited contribution. Estimating forest carbon fluxes for large regions based on process-based modelling, NFI data and Landsat satellite images</article-title><source>For. Ecol. Manag.</source><year>2011</year><volume>262</volume><fpage>2364</fpage><lpage>2377</lpage><pub-id pub-id-type="doi">10.1016/j.foreco.2011.08.035</pub-id></element-citation></ref><ref id="B3-sensors-25-01245"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hopkins</surname><given-names>J.</given-names></name>
<name><surname>Lucas</surname><given-names>M.</given-names></name>
<name><surname>Dufau</surname><given-names>C.</given-names></name>
<name><surname>Sutton</surname><given-names>M.</given-names></name>
<name><surname>Stum</surname><given-names>J.</given-names></name>
<name><surname>Lauret</surname><given-names>O.</given-names></name>
<name><surname>Channelliere</surname><given-names>C.</given-names></name>
</person-group><article-title>Detection and variability of the Congo River plume from satellite derived sea surface temperature, salinity, ocean colour and sea level</article-title><source>Remote Sens. Environ.</source><year>2013</year><volume>139</volume><fpage>365</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2013.08.015</pub-id></element-citation></ref><ref id="B4-sensors-25-01245"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dematt&#x000ea;</surname><given-names>J.A.M.</given-names></name>
<name><surname>Fongaro</surname><given-names>C.T.</given-names></name>
<name><surname>Rizzo</surname><given-names>R.</given-names></name>
<name><surname>Safanelli</surname><given-names>J.L.</given-names></name>
</person-group><article-title>Geospatial Soil Sensing System (GEOS3): A powerful data mining procedure to retrieve soil spectral reflectance from satellite images</article-title><source>Remote Sens. Environ.</source><year>2018</year><volume>212</volume><fpage>161</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2018.04.047</pub-id></element-citation></ref><ref id="B5-sensors-25-01245"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bosquilia</surname><given-names>R.W.</given-names></name>
<name><surname>Muller-Karger</surname><given-names>F.E.</given-names></name>
</person-group><article-title>Analysis of the wetland classification using optical satellite imagery in the environmental protection area of Guaraque&#x000e7;aba, PR, Brazil</article-title><source>J. S. Am. Earth Sci.</source><year>2011</year><volume>112</volume><fpage>103615</fpage><pub-id pub-id-type="doi">10.1016/j.jsames.2021.103615</pub-id></element-citation></ref><ref id="B6-sensors-25-01245"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>de Brito</surname><given-names>C.S.</given-names></name>
<name><surname>da Silva</surname><given-names>R.M.</given-names></name>
<name><surname>Santos</surname><given-names>C.A.G.</given-names></name>
<name><surname>Neto</surname><given-names>R.M.B.</given-names></name>
<name><surname>Coelho</surname><given-names>V.H.R.</given-names></name>
</person-group><article-title>Monitoring meteorological drought in a semiarid region using two long-term satellite-estimated rainfall datasets: A case study of the Piranhas River basin, northeastern Brazil</article-title><source>Atmos. Res.</source><year>2021</year><volume>250</volume><fpage>105380</fpage><pub-id pub-id-type="doi">10.1016/j.atmosres.2020.105380</pub-id></element-citation></ref><ref id="B7-sensors-25-01245"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Johnston</surname><given-names>T.</given-names></name>
<name><surname>Young</surname><given-names>S.R.</given-names></name>
<name><surname>Hughes</surname><given-names>D.</given-names></name>
<name><surname>Patton</surname><given-names>R.M.</given-names></name>
<name><surname>White</surname><given-names>D.</given-names></name>
</person-group><article-title>Optimizing convolutional neural networks for cloud detection</article-title><source>Proceedings of the Machine Learning on HPC Environments</source><conf-loc>Denver, CO, USA</conf-loc><conf-date>12&#x02013;17 November 2017</conf-date><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="B8-sensors-25-01245"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Matsunobu</surname><given-names>L.M.</given-names></name>
<name><surname>Pedro</surname><given-names>H.T.</given-names></name>
<name><surname>Coimbra</surname><given-names>C.F.</given-names></name>
</person-group><article-title>Cloud detection using convolutional neural networks on remote sensing images</article-title><source>Sol. Energy</source><year>2021</year><volume>230</volume><fpage>1020</fpage><lpage>1032</lpage><pub-id pub-id-type="doi">10.1016/j.solener.2021.10.065</pub-id></element-citation></ref><ref id="B9-sensors-25-01245"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chai</surname><given-names>D.</given-names></name>
<name><surname>Huang</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>M.</given-names></name>
<name><surname>Yang</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>R.</given-names></name>
</person-group><article-title>Remote sensing image cloud detection using a shallow convolutional neural network</article-title><source>ISPRS J. Photogramm. Remote Sens.</source><year>2024</year><volume>209</volume><fpage>66</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2024.01.026</pub-id></element-citation></ref><ref id="B10-sensors-25-01245"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Li</surname><given-names>Q.</given-names></name>
<name><surname>Min</surname><given-names>M.</given-names></name>
<name><surname>Yao</surname><given-names>Z.</given-names></name>
</person-group><article-title>DCNet: A deformable convolutional cloud detection network for remote sensing imagery</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2022</year><volume>19</volume><fpage>8013305</fpage><pub-id pub-id-type="doi">10.1109/LGRS.2021.3086584</pub-id></element-citation></ref><ref id="B11-sensors-25-01245"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
</person-group><article-title>Cloud detection method using CNN based on cascaded feature attention and channel attention</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2021</year><volume>60</volume><fpage>4104717</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2021.3120752</pub-id></element-citation></ref><ref id="B12-sensors-25-01245"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Yang</surname><given-names>S.</given-names></name>
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Xiao</surname><given-names>B.</given-names></name>
<name><surname>Cao</surname><given-names>X.</given-names></name>
</person-group><article-title>Ground-based cloud detection using multiscale attention convolutional neural network</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2021</year><volume>19</volume><fpage>8019605</fpage><pub-id pub-id-type="doi">10.1109/LGRS.2021.3106337</pub-id></element-citation></ref><ref id="B13-sensors-25-01245"><label>13.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
</person-group><article-title>Cloud Detection Method Using Convolutional Neural Network Based on Cascaded Color and Texture Feature Attention</article-title><source>Proceedings of the 2021 26th International Conference on Automation and Computing (ICAC), Portsmouth, UK, 2&#x02013;4 September 2021</source><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2021</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B14-sensors-25-01245"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
</person-group><article-title>Automatic cloud detection method for optical remote sensing images combining channel attention mechanism and multi-feature fusion</article-title><source>J. Phys. Conf. Ser.</source><year>2023</year><volume>2493</volume><fpage>012006</fpage><pub-id pub-id-type="doi">10.1088/1742-6596/2493/1/012006</pub-id></element-citation></ref><ref id="B15-sensors-25-01245"><label>15.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Ronneberger</surname><given-names>O.</given-names></name>
<name><surname>Fischer</surname><given-names>P.</given-names></name>
<name><surname>Brox</surname><given-names>T.</given-names></name>
</person-group><article-title>U-net: Convolutional networks for biomedical image segmentation</article-title><source>Medical Image Computing and Computer-Assisted Intervention, Proceedings of the MICCAI 2015: 18th International Conference, Munich, Germany, 5&#x02013;9 October 2015</source><comment>proceedings, part III 18</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2015</year><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id="B16-sensors-25-01245"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yanan</surname><given-names>G.</given-names></name>
<name><surname>Xiaoqun</surname><given-names>C.</given-names></name>
<name><surname>Bainian</surname><given-names>L.</given-names></name>
<name><surname>Kecheng</surname><given-names>P.</given-names></name>
</person-group><article-title>Cloud detection for satellite imagery using deep learning</article-title><source>J. Phys. Conf. Ser.</source><year>2020</year><volume>1617</volume><fpage>012089</fpage><pub-id pub-id-type="doi">10.1088/1742-6596/1617/1/012089</pub-id></element-citation></ref><ref id="B17-sensors-25-01245"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>Y.</given-names></name>
<name><surname>Cao</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>B.</given-names></name>
<name><surname>Gao</surname><given-names>M.</given-names></name>
</person-group><article-title>Cloud detection for satellite imagery using attention-based U-Net convolutional neural network</article-title><source>Symmetry</source><year>2020</year><volume>12</volume><elocation-id>1056</elocation-id><pub-id pub-id-type="doi">10.3390/sym12061056</pub-id></element-citation></ref><ref id="B18-sensors-25-01245"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kanu</surname><given-names>S.</given-names></name>
<name><surname>Khoja</surname><given-names>R.</given-names></name>
<name><surname>Lal</surname><given-names>S.</given-names></name>
<name><surname>Raghavendra</surname><given-names>B.</given-names></name>
<name><surname>Asha</surname><given-names>C.</given-names></name>
</person-group><article-title>CloudX-net: A robust encoder-decoder architecture for cloud detection from satellite remote sensing images</article-title><source>Remote Sens. Appl. Soc. Environ.</source><year>2020</year><volume>20</volume><fpage>100417</fpage><pub-id pub-id-type="doi">10.1016/j.rsase.2020.100417</pub-id></element-citation></ref><ref id="B19-sensors-25-01245"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>D.</given-names></name>
<name><surname>Xia</surname><given-names>M.</given-names></name>
</person-group><article-title>CDUNet: Cloud detection UNet for remote sensing imagery</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>4533</elocation-id><pub-id pub-id-type="doi">10.3390/rs13224533</pub-id></element-citation></ref><ref id="B20-sensors-25-01245"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yin</surname><given-names>M.</given-names></name>
<name><surname>Wang</surname><given-names>P.</given-names></name>
<name><surname>Ni</surname><given-names>C.</given-names></name>
<name><surname>Hao</surname><given-names>W.</given-names></name>
</person-group><article-title>Cloud and snow detection of remote sensing images based on improved Unet3+</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><elocation-id>14415</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-18812-6</pub-id><pub-id pub-id-type="pmid">36002645</pub-id>
</element-citation></ref><ref id="B21-sensors-25-01245"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Yang</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Lu</surname><given-names>S.</given-names></name>
<name><surname>Ye</surname><given-names>Y.</given-names></name>
<name><surname>Ban</surname><given-names>Y.</given-names></name>
</person-group><article-title>GCDB-UNet: A novel robust cloud detection approach for remote sensing images</article-title><source>Knowl.-Based Syst.</source><year>2022</year><volume>238</volume><fpage>107890</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2021.107890</pub-id></element-citation></ref><ref id="B22-sensors-25-01245"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>A.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Ma</surname><given-names>X.</given-names></name>
</person-group><article-title>Residual dual u-shape networks with improved skip connections for cloud detection</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2023</year><volume>21</volume><fpage>5000205</fpage><pub-id pub-id-type="doi">10.1109/LGRS.2023.3337860</pub-id></element-citation></ref><ref id="B23-sensors-25-01245"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wen</surname><given-names>X.</given-names></name>
<name><surname>Ye</surname><given-names>W.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
</person-group><article-title>Cloud detection of remote sensing image based on transformer and multi scale attention blocks</article-title><source>Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence</source><conf-loc>Dongguan, China</conf-loc><conf-date>16&#x02013;18 December 2022</conf-date><fpage>592</fpage><lpage>596</lpage></element-citation></ref><ref id="B24-sensors-25-01245"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ma</surname><given-names>N.</given-names></name>
<name><surname>Sun</surname><given-names>L.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
<name><surname>Zhou</surname><given-names>C.</given-names></name>
<name><surname>Dong</surname><given-names>C.</given-names></name>
</person-group><article-title>CNN-TransNet: A hybrid CNN-transformer network with differential feature enhancement for cloud detection</article-title><source>IEEE Geosci. Remote. Sens. Lett.</source><year>2023</year><volume>20</volume><fpage>1001705</fpage><pub-id pub-id-type="doi">10.1109/LGRS.2023.3288742</pub-id></element-citation></ref><ref id="B25-sensors-25-01245"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Singh</surname><given-names>R.</given-names></name>
<name><surname>Biswas</surname><given-names>M.</given-names></name>
<name><surname>Pal</surname><given-names>M.</given-names></name>
</person-group><article-title>A transformer-based cloud detection approach using Sentinel 2 imageries</article-title><source>Int. J. Remote Sens.</source><year>2023</year><volume>44</volume><fpage>3194</fpage><lpage>3208</lpage><pub-id pub-id-type="doi">10.1080/01431161.2023.2216850</pub-id></element-citation></ref><ref id="B26-sensors-25-01245"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>H.</given-names></name>
<name><surname>Sun</surname><given-names>S.</given-names></name>
<name><surname>Cheng</surname><given-names>T.</given-names></name>
<name><surname>Shu</surname><given-names>X.</given-names></name>
</person-group><article-title>Transformer-Based Cloud Detection Method for High-Resolution Remote Sensing Imagery</article-title><source>Comput. Mater. Contin.</source><year>2024</year><volume>80</volume><fpage>661</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.32604/cmc.2024.052208</pub-id></element-citation></ref><ref id="B27-sensors-25-01245"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wieland</surname><given-names>M.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Martinis</surname><given-names>S.</given-names></name>
</person-group><article-title>Multi-sensor cloud and cloud shadow segmentation with a convolutional neural network</article-title><source>Remote Sens. Environ.</source><year>2019</year><volume>230</volume><fpage>111203</fpage><pub-id pub-id-type="doi">10.1016/j.rse.2019.05.022</pub-id></element-citation></ref><ref id="B28-sensors-25-01245"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qu</surname><given-names>Y.</given-names></name>
<name><surname>Xia</surname><given-names>M.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Strip pooling channel spatial attention network for the segmentation of cloud and cloud shadow</article-title><source>Comput. Geosci.</source><year>2021</year><volume>157</volume><fpage>104940</fpage><pub-id pub-id-type="doi">10.1016/j.cageo.2021.104940</pub-id></element-citation></ref><ref id="B29-sensors-25-01245"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mohajerani</surname><given-names>S.</given-names></name>
<name><surname>Saeedi</surname><given-names>P.</given-names></name>
</person-group><article-title>Cloud and cloud shadow segmentation for remote sensing imagery via filtered jaccard loss function and parametric augmentation</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2021</year><volume>14</volume><fpage>4254</fpage><lpage>4266</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2021.3070786</pub-id></element-citation></ref><ref id="B30-sensors-25-01245"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mohajerani</surname><given-names>S.</given-names></name>
<name><surname>Saeedi</surname><given-names>P.</given-names></name>
</person-group><article-title>Cloud-Net: An end-to-end cloud detection algorithm for Landsat 8 imagery</article-title><source>Proceedings of the IGARSS 2019&#x02013;2019 IEEE International Geoscience and Remote Sensing Symposium</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>28 July&#x02013;2 August 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2019</year><fpage>1029</fpage><lpage>1032</lpage></element-citation></ref><ref id="B31-sensors-25-01245"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>Q.</given-names></name>
</person-group><article-title>CSDFormer: A cloud and shadow detection method for landsat images based on transformer</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2024</year><volume>129</volume><fpage>103799</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2024.103799</pub-id></element-citation></ref><ref id="B32-sensors-25-01245"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>B.</given-names></name>
<name><surname>Khosla</surname><given-names>A.</given-names></name>
<name><surname>Lapedriza</surname><given-names>A.</given-names></name>
<name><surname>Oliva</surname><given-names>A.</given-names></name>
<name><surname>Torralba</surname><given-names>A.</given-names></name>
</person-group><article-title>Object detectors emerge in deep scene cnns</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1412.6856</pub-id></element-citation></ref><ref id="B33-sensors-25-01245"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>S.</given-names></name>
<name><surname>Chen</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>T.-Y.</given-names></name>
</person-group><article-title>On the depth of deep neural networks: A theoretical view</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Phoenix, AZ, USA</conf-loc><conf-date>12&#x02013;17 February 2016</conf-date></element-citation></ref><ref id="B34-sensors-25-01245"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Spatial pyramid pooling in deep convolutional networks for visual recognition</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2015</year><volume>37</volume><fpage>1904</fpage><lpage>1916</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2015.2389824</pub-id><pub-id pub-id-type="pmid">26353135</pub-id>
</element-citation></ref><ref id="B35-sensors-25-01245"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yu</surname><given-names>F.</given-names></name>
<name><surname>Koltun</surname><given-names>V.</given-names></name>
<name><surname>Funkhouser</surname><given-names>T.</given-names></name>
</person-group><article-title>Dilated residual network</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;16 July 2017</conf-date><fpage>472</fpage><lpage>480</lpage></element-citation></ref><ref id="B36-sensors-25-01245"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>J.</given-names></name>
<name><surname>Shen</surname><given-names>L.</given-names></name>
<name><surname>Sun</surname><given-names>G.</given-names></name>
</person-group><article-title>Squeeze-and-excitation networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#x02013;23 June 2018</conf-date><fpage>7132</fpage><lpage>7141</lpage></element-citation></ref><ref id="B37-sensors-25-01245"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>M.-H.</given-names></name>
<name><surname>Lu</surname><given-names>C.-Z.</given-names></name>
<name><surname>Hou</surname><given-names>Q.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Cheng</surname><given-names>M.-M.</given-names></name>
<name><surname>Hu</surname><given-names>S.-M.</given-names></name>
</person-group><article-title>Segnext: Rethinking convolutional attention design for semantic segmentation</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>1140</fpage><lpage>1156</lpage></element-citation></ref><ref id="B38-sensors-25-01245"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liao</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>M.</given-names></name>
<name><surname>Zhang</surname><given-names>K.</given-names></name>
<name><surname>Zhou</surname><given-names>H.</given-names></name>
<name><surname>Zou</surname><given-names>Y.</given-names></name>
<name><surname>Xiong</surname><given-names>W.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Kuang</surname><given-names>F.</given-names></name>
<name><surname>Zhu</surname><given-names>D.</given-names></name>
</person-group><article-title>SC-Net: A new strip convolutional network model for rice seedling and weed segmentation in paddy field</article-title><source>Comput. Electron. Agric.</source><year>2024</year><volume>220</volume><fpage>108862</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2024.108862</pub-id></element-citation></ref><ref id="B39-sensors-25-01245"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hou</surname><given-names>Q.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Cheng</surname><given-names>M.-M.</given-names></name>
<name><surname>Feng</surname><given-names>J.</given-names></name>
</person-group><article-title>Strip pooling: Rethinking spatial pooling for scene parsing</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;19 June 2020</conf-date><fpage>4003</fpage><lpage>4012</lpage></element-citation></ref><ref id="B40-sensors-25-01245"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Woo</surname><given-names>S.</given-names></name>
<name><surname>Park</surname><given-names>J.</given-names></name>
<name><surname>Lee</surname><given-names>J.-Y.</given-names></name>
<name><surname>Kweon</surname><given-names>I.S.</given-names></name>
</person-group><article-title>Cbam: Convolutional block attention module</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#x02013;14 September 2018</conf-date><fpage>3</fpage><lpage>19</lpage></element-citation></ref><ref id="B41-sensors-25-01245"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cheng</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Yu</surname><given-names>Y.</given-names></name>
<name><surname>Song</surname><given-names>Z.</given-names></name>
<name><surname>Tang</surname><given-names>C.</given-names></name>
</person-group><article-title>TinyDepth: Lightweight self-supervised monocular depth estimation based on transformer</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>138</volume><fpage>109313</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.109313</pub-id></element-citation></ref><ref id="B42-sensors-25-01245"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Meng</surname><given-names>W.</given-names></name>
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
</person-group><article-title>AFC-Unet: Attention-fused full-scale CNN-transformer unet for medical image segmentation</article-title><source>Biomed. Signal Process. Control</source><year>2025</year><volume>99</volume><elocation-id>106839</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2024.106839</pub-id></element-citation></ref><ref id="B43-sensors-25-01245"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vaswani</surname><given-names>A.</given-names></name>
<name><surname>Shazeer</surname><given-names>N.</given-names></name>
<name><surname>Parmar</surname><given-names>N.</given-names></name>
<name><surname>Uszkoreit</surname><given-names>J.</given-names></name>
<name><surname>Jones</surname><given-names>L.</given-names></name>
<name><surname>Gomez</surname><given-names>A.N.</given-names></name>
<name><surname>Kaiser</surname><given-names>L.</given-names></name>
<name><surname>Polosukhin</surname><given-names>I.</given-names></name>
</person-group><article-title>Attention is all you need</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><pub-id pub-id-type="doi">10.48550/arXiv.1706.03762</pub-id></element-citation></ref><ref id="B44-sensors-25-01245"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Shen</surname><given-names>H.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
</person-group><article-title>HRC_WHU: High-resolution cloud cover validation data. ISPRS J. Photogramm</article-title><source>Remote Sens.</source><year>2019</year><volume>150</volume><fpage>197</fpage><lpage>212</lpage></element-citation></ref><ref id="B45-sensors-25-01245"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>L.-C.</given-names></name>
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
<name><surname>Papandreou</surname><given-names>G.</given-names></name>
<name><surname>Schroff</surname><given-names>F.</given-names></name>
<name><surname>Adam</surname><given-names>H.</given-names></name>
</person-group><article-title>Encoder-decoder with atrous separable convolution for semantic image segmentation</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#x02013;14 September 2018</conf-date><fpage>801</fpage><lpage>818</lpage></element-citation></ref><ref id="B46-sensors-25-01245"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Weng</surname><given-names>L.</given-names></name>
<name><surname>Ding</surname><given-names>L.</given-names></name>
<name><surname>Xia</surname><given-names>M.</given-names></name>
<name><surname>Lin</surname><given-names>H.</given-names></name>
</person-group><article-title>CRSNet: Cloud and cloud shadow refinement segmentation networks for remote sensing imagery</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>1664</elocation-id><pub-id pub-id-type="doi">10.3390/rs15061664</pub-id></element-citation></ref><ref id="B47-sensors-25-01245"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>C.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Kuang</surname><given-names>N.</given-names></name>
<name><surname>Luo</surname><given-names>H.</given-names></name>
<name><surname>Zhong</surname><given-names>S.</given-names></name>
<name><surname>Fan</surname><given-names>J.</given-names></name>
</person-group><article-title>Boundary-aware bilateral fusion network for cloud detection</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2023</year><volume>61</volume><fpage>5403014</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2023.3276750</pub-id></element-citation></ref><ref id="B48-sensors-25-01245"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dong</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>M.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
</person-group><article-title>MCDNet: Multilevel cloud detection network for remote sensing images based on dual-perspective change-guided and multi-scale feature fusion</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2024</year><volume>129</volume><fpage>103820</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2024.103820</pub-id></element-citation></ref><ref id="B49-sensors-25-01245"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Du</surname><given-names>W.</given-names></name>
<name><surname>Fan</surname><given-names>Z.</given-names></name>
<name><surname>Yan</surname><given-names>Y.</given-names></name>
<name><surname>Yu</surname><given-names>R.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
</person-group><article-title>AFMUNet: Attention Feature Fusion Network Based on a U-Shaped Structure for Cloud and Cloud Shadow Detection</article-title><source>Remote Sens.</source><year>2024</year><volume>16</volume><elocation-id>1574</elocation-id><pub-id pub-id-type="doi">10.3390/rs16091574</pub-id></element-citation></ref><ref id="B50-sensors-25-01245"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sebaq</surname><given-names>A.</given-names></name>
<name><surname>ElHelw</surname><given-names>M.</given-names></name>
</person-group><article-title>Rsdiff: Remote sensing image generation from text using diffusion model</article-title><source>Neural Comput. Appl.</source><year>2024</year><volume>36</volume><fpage>23103</fpage><lpage>23111</lpage><pub-id pub-id-type="doi">10.1007/s00521-024-10363-3</pub-id></element-citation></ref><ref id="B51-sensors-25-01245"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tang</surname><given-names>D.</given-names></name>
<name><surname>Cao</surname><given-names>X.</given-names></name>
<name><surname>Hou</surname><given-names>X.</given-names></name>
<name><surname>Jiang</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Meng</surname><given-names>D.</given-names></name>
</person-group><article-title>CRS-Diff: Controllable Remote Sensing Image Generation with Diffusion Model</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2024</year><volume>62</volume><fpage>5638714</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2024.3453414</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01245-f001"><label>Figure 1</label><caption><p>The structure of NFCNet.</p></caption><graphic xlink:href="sensors-25-01245-g001" position="float"/></fig><fig position="float" id="sensors-25-01245-f002"><label>Figure 2</label><caption><p>Diagram of HCAM structure.</p></caption><graphic xlink:href="sensors-25-01245-g002" position="float"/></fig><fig position="float" id="sensors-25-01245-f003"><label>Figure 3</label><caption><p>Diagram of SPFA structure.</p></caption><graphic xlink:href="sensors-25-01245-g003" position="float"/></fig><fig position="float" id="sensors-25-01245-f004"><label>Figure 4</label><caption><p>Diagram of DCA structure.</p></caption><graphic xlink:href="sensors-25-01245-g004" position="float"/></fig><fig position="float" id="sensors-25-01245-f005"><label>Figure 5</label><caption><p>Comparison of accuracy of different methods with five land cover types.</p></caption><graphic xlink:href="sensors-25-01245-g005" position="float"/></fig><fig position="float" id="sensors-25-01245-f006"><label>Figure 6</label><caption><p>The visual output of each network on the HRC_WHU dataset: (<bold>a</bold>) the original image; (<bold>b</bold>) the corresponding label; (<bold>c</bold>) the prediction of UNet; (<bold>d</bold>) the prediction of deeplabv3+; (<bold>e</bold>) the prediction of SAtt-UNet; (<bold>f</bold>) the prediction of Cloud-Attu; (<bold>g</bold>) the prediction of DCNet; (<bold>h</bold>) the prediction of CRSNet; (<bold>i</bold>) the prediction of BABFNet; (<bold>j</bold>) the prediction of MCDNet; (<bold>k</bold>) the prediction of AFMUNet; and (<bold>l</bold>) the prediction of NFCNet.</p></caption><graphic xlink:href="sensors-25-01245-g006" position="float"/></fig><fig position="float" id="sensors-25-01245-f007"><label>Figure 7</label><caption><p>The visual output of each network on the CHLandsat8 dataset: (<bold>a</bold>) the original image; (<bold>b</bold>) the corresponding label; (<bold>c</bold>) the prediction of UNet; (<bold>d</bold>) the prediction of deeplabv3+; (<bold>e</bold>) the prediction of SAtt-UNet; (<bold>f</bold>) the prediction of Cloud-Attu; (<bold>g</bold>) the prediction of DCNet; (<bold>h</bold>) the prediction of CRSNet; (<bold>i</bold>) the prediction of BABFNet; (<bold>j</bold>) the prediction of MCDNet; (<bold>k</bold>) the prediction of AFMUNet; and (<bold>l</bold>) the prediction of NFCNet.</p></caption><graphic xlink:href="sensors-25-01245-g007" position="float"/></fig><fig position="float" id="sensors-25-01245-f008"><label>Figure 8</label><caption><p>The visual output of each network on the 95-Cloud dataset: (<bold>a</bold>) the original image; (<bold>b</bold>) the corresponding label; (<bold>c</bold>) the prediction of UNet; (<bold>d</bold>) the prediction of deeplabv3+; (<bold>e</bold>) the prediction of SAtt-UNet; (<bold>f</bold>) the prediction of Cloud-Attu; (<bold>g</bold>) the prediction of DCNet; (<bold>h</bold>) the prediction of CRSNet; (<bold>i</bold>) the prediction of BABFNet; (<bold>j</bold>) the prediction of AFMUNet; and (<bold>k</bold>) the prediction of NFCNet.</p></caption><graphic xlink:href="sensors-25-01245-g008" position="float"/></fig><table-wrap position="float" id="sensors-25-01245-t001"><object-id pub-id-type="pii">sensors-25-01245-t001_Table 1</object-id><label>Table 1</label><caption><p>Quantitative comparison of different methods on HRC_WHU dataset (IN %).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Acc</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dice</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mIoU</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Re</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Pre</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Param Size (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPs (G)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Baseline</td><td align="left" valign="middle" rowspan="1" colspan="1">92.94</td><td align="left" valign="middle" rowspan="1" colspan="1">91.00</td><td align="left" valign="middle" rowspan="1" colspan="1">83.87</td><td align="left" valign="middle" rowspan="1" colspan="1">90.01</td><td align="left" valign="middle" rowspan="1" colspan="1">92.40</td><td align="left" valign="middle" rowspan="1" colspan="1">91.19</td><td align="center" valign="middle" rowspan="1" colspan="1">3.4692</td><td align="center" valign="middle" rowspan="1" colspan="1">4.1324</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HCAM</td><td align="left" valign="middle" rowspan="1" colspan="1">94.36</td><td align="left" valign="middle" rowspan="1" colspan="1">92.97</td><td align="left" valign="middle" rowspan="1" colspan="1">87.11</td><td align="left" valign="middle" rowspan="1" colspan="1">92.95</td><td align="left" valign="middle" rowspan="1" colspan="1">93.28</td><td align="left" valign="middle" rowspan="1" colspan="1">93.11</td><td align="center" valign="middle" rowspan="1" colspan="1">5.6700</td><td align="center" valign="middle" rowspan="1" colspan="1">8.4720</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HCAM + SPFA</td><td align="left" valign="middle" rowspan="1" colspan="1">95.16</td><td align="left" valign="middle" rowspan="1" colspan="1">94.21</td><td align="left" valign="middle" rowspan="1" colspan="1">89.15</td><td align="left" valign="middle" rowspan="1" colspan="1">94.21</td><td align="left" valign="middle" rowspan="1" colspan="1">95.15</td><td align="left" valign="middle" rowspan="1" colspan="1">94.67</td><td align="center" valign="middle" rowspan="1" colspan="1">14.7034</td><td align="center" valign="middle" rowspan="1" colspan="1">18.3244</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HCAM + SPFA + DCA</td><td align="left" valign="middle" rowspan="1" colspan="1">95.20</td><td align="left" valign="middle" rowspan="1" colspan="1">94.58</td><td align="left" valign="middle" rowspan="1" colspan="1">89.85</td><td align="left" valign="middle" rowspan="1" colspan="1">94.15</td><td align="left" valign="middle" rowspan="1" colspan="1">94.80</td><td align="left" valign="middle" rowspan="1" colspan="1">94.47</td><td align="center" valign="middle" rowspan="1" colspan="1">14.7759</td><td align="center" valign="middle" rowspan="1" colspan="1">18.5641</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HCAM + SPFA + HCAM + DCA (ours)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.44</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.86</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.25</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.03</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.74</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.0874</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.5490</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01245-t002"><object-id pub-id-type="pii">sensors-25-01245-t002_Table 2</object-id><label>Table 2</label><caption><p>Quantitative comparison of different methods on HRC_WHU dataset (IN %).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Acc</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dice</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mIoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Re</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Pre</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">UNet</td><td align="center" valign="middle" rowspan="1" colspan="1">92.94</td><td align="center" valign="middle" rowspan="1" colspan="1">91.00</td><td align="center" valign="middle" rowspan="1" colspan="1">83.87</td><td align="center" valign="middle" rowspan="1" colspan="1">90.01</td><td align="center" valign="middle" rowspan="1" colspan="1">92.40</td><td align="center" valign="middle" rowspan="1" colspan="1">91.19</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DeeplabV3+</td><td align="center" valign="middle" rowspan="1" colspan="1">93.62</td><td align="center" valign="middle" rowspan="1" colspan="1">92.35</td><td align="center" valign="middle" rowspan="1" colspan="1">86.29</td><td align="center" valign="middle" rowspan="1" colspan="1">94.05</td><td align="center" valign="middle" rowspan="1" colspan="1">91.20</td><td align="center" valign="middle" rowspan="1" colspan="1">92.60</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SAtt-UNet</td><td align="center" valign="middle" rowspan="1" colspan="1">94.95</td><td align="center" valign="middle" rowspan="1" colspan="1">94.08</td><td align="center" valign="middle" rowspan="1" colspan="1">88.97</td><td align="center" valign="middle" rowspan="1" colspan="1">93.88</td><td align="center" valign="middle" rowspan="1" colspan="1">94.45</td><td align="center" valign="middle" rowspan="1" colspan="1">94.16</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Cloud-AttU</td><td align="center" valign="middle" rowspan="1" colspan="1">93.86</td><td align="center" valign="middle" rowspan="1" colspan="1">92.79</td><td align="center" valign="middle" rowspan="1" colspan="1">86.77</td><td align="center" valign="middle" rowspan="1" colspan="1">93.31</td><td align="center" valign="middle" rowspan="1" colspan="1">92.54</td><td align="center" valign="middle" rowspan="1" colspan="1">92.92</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DCNet</td><td align="center" valign="middle" rowspan="1" colspan="1">93.81</td><td align="center" valign="middle" rowspan="1" colspan="1">92.63</td><td align="center" valign="middle" rowspan="1" colspan="1">86.49</td><td align="center" valign="middle" rowspan="1" colspan="1">92.86</td><td align="center" valign="middle" rowspan="1" colspan="1">92.63</td><td align="center" valign="middle" rowspan="1" colspan="1">92.74</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CRSNet</td><td align="center" valign="middle" rowspan="1" colspan="1">93.12</td><td align="center" valign="middle" rowspan="1" colspan="1">91.74</td><td align="center" valign="middle" rowspan="1" colspan="1">85.07</td><td align="center" valign="middle" rowspan="1" colspan="1">92.30</td><td align="center" valign="middle" rowspan="1" colspan="1">91.59</td><td align="center" valign="middle" rowspan="1" colspan="1">91.95</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BABFNet</td><td align="center" valign="middle" rowspan="1" colspan="1">94.31</td><td align="center" valign="middle" rowspan="1" colspan="1">93.25</td><td align="center" valign="middle" rowspan="1" colspan="1">87.56</td><td align="center" valign="middle" rowspan="1" colspan="1">93.00</td><td align="center" valign="middle" rowspan="1" colspan="1">93.68</td><td align="center" valign="middle" rowspan="1" colspan="1">93.36</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MCDNet</td><td align="center" valign="middle" rowspan="1" colspan="1">94.16</td><td align="center" valign="middle" rowspan="1" colspan="1">93.36</td><td align="center" valign="middle" rowspan="1" colspan="1">87.60</td><td align="center" valign="middle" rowspan="1" colspan="1">93.21</td><td align="center" valign="middle" rowspan="1" colspan="1">93.61</td><td align="center" valign="middle" rowspan="1" colspan="1">93.41</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AFMUNet</td><td align="center" valign="middle" rowspan="1" colspan="1">93.11</td><td align="center" valign="middle" rowspan="1" colspan="1">91.39</td><td align="center" valign="middle" rowspan="1" colspan="1">84.47</td><td align="center" valign="middle" rowspan="1" colspan="1">91.05</td><td align="center" valign="middle" rowspan="1" colspan="1">92.11</td><td align="center" valign="middle" rowspan="1" colspan="1">91.58</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NFCNet (ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.86</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.74</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.88</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01245-t003"><object-id pub-id-type="pii">sensors-25-01245-t003_Table 3</object-id><label>Table 3</label><caption><p>Quantitative comparison of different methods on CHLandsat8 dataset (IN %).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Acc</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dice</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mIoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Re</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Pre</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">UNet</td><td align="center" valign="middle" rowspan="1" colspan="1">91.21</td><td align="center" valign="middle" rowspan="1" colspan="1">86.10</td><td align="center" valign="middle" rowspan="1" colspan="1">76.86</td><td align="center" valign="middle" rowspan="1" colspan="1">92.98</td><td align="center" valign="middle" rowspan="1" colspan="1">81.52</td><td align="center" valign="middle" rowspan="1" colspan="1">86.87</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DeeplabV3+</td><td align="center" valign="middle" rowspan="1" colspan="1">91.75</td><td align="center" valign="middle" rowspan="1" colspan="1">86.13</td><td align="center" valign="middle" rowspan="1" colspan="1">77.76</td><td align="center" valign="middle" rowspan="1" colspan="1">88.23</td><td align="center" valign="middle" rowspan="1" colspan="1">85.76</td><td align="center" valign="middle" rowspan="1" colspan="1">86.97</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SAtt-UNet</td><td align="center" valign="middle" rowspan="1" colspan="1">91.90</td><td align="center" valign="middle" rowspan="1" colspan="1">86.80</td><td align="center" valign="middle" rowspan="1" colspan="1">78.01</td><td align="center" valign="middle" rowspan="1" colspan="1">90.91</td><td align="center" valign="middle" rowspan="1" colspan="1">84.54</td><td align="center" valign="middle" rowspan="1" colspan="1">87.60</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Cloud-AttU</td><td align="center" valign="middle" rowspan="1" colspan="1">92.58</td><td align="center" valign="middle" rowspan="1" colspan="1">86.83</td><td align="center" valign="middle" rowspan="1" colspan="1">78.08</td><td align="center" valign="middle" rowspan="1" colspan="1">85.29</td><td align="center" valign="middle" rowspan="1" colspan="1">90.13</td><td align="center" valign="middle" rowspan="1" colspan="1">87.64</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DCNet</td><td align="center" valign="middle" rowspan="1" colspan="1">91.86</td><td align="center" valign="middle" rowspan="1" colspan="1">86.49</td><td align="center" valign="middle" rowspan="1" colspan="1">77.47</td><td align="center" valign="middle" rowspan="1" colspan="1">89.33</td><td align="center" valign="middle" rowspan="1" colspan="1">85.29</td><td align="center" valign="middle" rowspan="1" colspan="1">87.26</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CRSNet</td><td align="center" valign="middle" rowspan="1" colspan="1">90.40</td><td align="center" valign="middle" rowspan="1" colspan="1">85.40</td><td align="center" valign="middle" rowspan="1" colspan="1">75.88</td><td align="center" valign="middle" rowspan="1" colspan="1">94.08</td><td align="center" valign="middle" rowspan="1" colspan="1">79.68</td><td align="center" valign="middle" rowspan="1" colspan="1">86.28</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BABFNet</td><td align="center" valign="middle" rowspan="1" colspan="1">92.55</td><td align="center" valign="middle" rowspan="1" colspan="1">87.04</td><td align="center" valign="middle" rowspan="1" colspan="1">79.11</td><td align="center" valign="middle" rowspan="1" colspan="1">90.89</td><td align="center" valign="middle" rowspan="1" colspan="1">86.19</td><td align="center" valign="middle" rowspan="1" colspan="1">88.48</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MCDNet</td><td align="center" valign="middle" rowspan="1" colspan="1">92.49</td><td align="center" valign="middle" rowspan="1" colspan="1">86.72</td><td align="center" valign="middle" rowspan="1" colspan="1">77.84</td><td align="center" valign="middle" rowspan="1" colspan="1">84.68</td><td align="center" valign="middle" rowspan="1" colspan="1">90.35</td><td align="center" valign="middle" rowspan="1" colspan="1">87.42</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AFMUNet</td><td align="center" valign="middle" rowspan="1" colspan="1">92.17</td><td align="center" valign="middle" rowspan="1" colspan="1">86.89</td><td align="center" valign="middle" rowspan="1" colspan="1">78.14</td><td align="center" valign="middle" rowspan="1" colspan="1">88.44</td><td align="center" valign="middle" rowspan="1" colspan="1">86.95</td><td align="center" valign="middle" rowspan="1" colspan="1">87.68</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NFCNet (ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.02</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01245-t004"><object-id pub-id-type="pii">sensors-25-01245-t004_Table 4</object-id><label>Table 4</label><caption><p>Quantitative comparison of different methods on 95-Cloud dataset (IN %).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Acc</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dice</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mIoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Re</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Pre</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">UNet</td><td align="center" valign="middle" rowspan="1" colspan="1">92.45</td><td align="center" valign="middle" rowspan="1" colspan="1">84.10</td><td align="center" valign="middle" rowspan="1" colspan="1">75.53</td><td align="center" valign="middle" rowspan="1" colspan="1">86.85</td><td align="center" valign="middle" rowspan="1" colspan="1">84.36</td><td align="center" valign="middle" rowspan="1" colspan="1">85.58</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DeeplabV3+</td><td align="center" valign="middle" rowspan="1" colspan="1">92.43</td><td align="center" valign="middle" rowspan="1" colspan="1">84.95</td><td align="center" valign="middle" rowspan="1" colspan="1">76.55</td><td align="center" valign="middle" rowspan="1" colspan="1">92.72</td><td align="center" valign="middle" rowspan="1" colspan="1">80.98</td><td align="center" valign="middle" rowspan="1" colspan="1">86.45</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SAtt-UNet</td><td align="center" valign="middle" rowspan="1" colspan="1">93.15</td><td align="center" valign="middle" rowspan="1" colspan="1">84.51</td><td align="center" valign="middle" rowspan="1" colspan="1">76.38</td><td align="center" valign="middle" rowspan="1" colspan="1">85.95</td><td align="center" valign="middle" rowspan="1" colspan="1">86.35</td><td align="center" valign="middle" rowspan="1" colspan="1">86.14</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Cloud-AttU</td><td align="center" valign="middle" rowspan="1" colspan="1">94.14</td><td align="center" valign="middle" rowspan="1" colspan="1">85.82</td><td align="center" valign="middle" rowspan="1" colspan="1">78.02</td><td align="center" valign="middle" rowspan="1" colspan="1">86.83</td><td align="center" valign="middle" rowspan="1" colspan="1">87.07</td><td align="center" valign="middle" rowspan="1" colspan="1">86.94</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DCNet</td><td align="center" valign="middle" rowspan="1" colspan="1">92.73</td><td align="center" valign="middle" rowspan="1" colspan="1">84.59</td><td align="center" valign="middle" rowspan="1" colspan="1">76.11</td><td align="center" valign="middle" rowspan="1" colspan="1">89.33</td><td align="center" valign="middle" rowspan="1" colspan="1">82.86</td><td align="center" valign="middle" rowspan="1" colspan="1">85.97</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CRSNet</td><td align="center" valign="middle" rowspan="1" colspan="1">91.03</td><td align="center" valign="middle" rowspan="1" colspan="1">82.19</td><td align="center" valign="middle" rowspan="1" colspan="1">73.01</td><td align="center" valign="middle" rowspan="1" colspan="1">88.91</td><td align="center" valign="middle" rowspan="1" colspan="1">79.46</td><td align="center" valign="middle" rowspan="1" colspan="1">83.91</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BABFNet</td><td align="center" valign="middle" rowspan="1" colspan="1">94.94</td><td align="center" valign="middle" rowspan="1" colspan="1">88.15</td><td align="center" valign="middle" rowspan="1" colspan="1">81.37</td><td align="center" valign="middle" rowspan="1" colspan="1">89.01</td><td align="center" valign="middle" rowspan="1" colspan="1">88.88</td><td align="center" valign="middle" rowspan="1" colspan="1">88.94</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AFMUNet</td><td align="center" valign="middle" rowspan="1" colspan="1">93.93</td><td align="center" valign="middle" rowspan="1" colspan="1">86.29</td><td align="center" valign="middle" rowspan="1" colspan="1">78.99</td><td align="center" valign="middle" rowspan="1" colspan="1">88.17</td><td align="center" valign="middle" rowspan="1" colspan="1">86.59</td><td align="center" valign="middle" rowspan="1" colspan="1">87.37</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NFCNet (ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.64</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.36</td></tr></tbody></table></table-wrap></floats-group></article>