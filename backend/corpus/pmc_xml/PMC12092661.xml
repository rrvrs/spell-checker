<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="data-paper" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Data</journal-id><journal-id journal-id-type="iso-abbrev">Sci Data</journal-id><journal-title-group><journal-title>Scientific Data</journal-title></journal-title-group><issn pub-type="epub">2052-4463</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40394033</article-id><article-id pub-id-type="pmc">PMC12092661</article-id>
<article-id pub-id-type="publisher-id">5112</article-id><article-id pub-id-type="doi">10.1038/s41597-025-05112-7</article-id><article-categories><subj-group subj-group-type="heading"><subject>Data Descriptor</subject></subj-group></article-categories><title-group><article-title><italic>Galar</italic> - a large multi-label video capsule endoscopy dataset</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0009-0001-8953-3887</contrib-id><name><surname>Le Floch</surname><given-names>Maxime</given-names></name><address><email>Maxime.LeFloch@ukdd.de</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Wolf</surname><given-names>Fabian</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>McIntyre</surname><given-names>Lucian</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Weinert</surname><given-names>Christoph</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Palm</surname><given-names>Albrecht</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Volk</surname><given-names>Konrad</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Herzog</surname><given-names>Paul</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><name><surname>Kirk</surname><given-names>Sophie Helene</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Steinh&#x000e4;user</surname><given-names>Jonas L.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Stopp</surname><given-names>Catrein</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Geissler</surname><given-names>Mark Enrik</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0676-6926</contrib-id><name><surname>Herzog</surname><given-names>Moritz</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Sulk</surname><given-names>Stefan</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3730-5348</contrib-id><name><surname>Kather</surname><given-names>Jakob Nikolas</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Meining</surname><given-names>Alexander</given-names></name><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author"><name><surname>Hann</surname><given-names>Alexander</given-names></name><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author"><name><surname>Palm</surname><given-names>Steffen</given-names></name><xref ref-type="aff" rid="Aff8">8</xref></contrib><contrib contrib-type="author"><name><surname>Hampe</surname><given-names>Jochen</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Herzog</surname><given-names>Nora</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Brinkmann</surname><given-names>Franz</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/042aqky30</institution-id><institution-id institution-id-type="GRID">grid.4488.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2111 7257</institution-id><institution>Else Kr&#x000f6;ner Fresenius Center for Digital Health, </institution><institution>Technische Universit&#x000e4;t Dresden (TU Dresden), </institution></institution-wrap>Dresden, Germany </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/042aqky30</institution-id><institution-id institution-id-type="GRID">grid.4488.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2111 7257</institution-id><institution>Department of Medicine I, University Hospital Dresden, </institution><institution>Technische Universit&#x000e4;t Dresden (TU Dresden), </institution></institution-wrap>Dresden, Germany </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/042aqky30</institution-id><institution-id institution-id-type="GRID">grid.4488.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2111 7257</institution-id><institution>Institute of Computer Science, </institution><institution>Technische Universit&#x000e4;t Dresden (TU Dresden), </institution></institution-wrap>Dresden, Germany </aff><aff id="Aff4"><label>4</label>Diakonissen Krankenhaus Dresden, Gastroenterology, Dresden, Germany </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/032000t02</institution-id><institution-id institution-id-type="GRID">grid.6582.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9748</institution-id><institution>Institute of Human Genetics, </institution><institution>Ulm University and Ulm University Medical Center, </institution></institution-wrap>Ulm, Germany </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/013czdx64</institution-id><institution-id institution-id-type="GRID">grid.5253.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0328 4908</institution-id><institution>Medical Oncology, National Center for Tumor Diseases (NCT), </institution><institution>University Hospital Heidelberg, </institution></institution-wrap>Heidelberg, Germany </aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03pvr2g57</institution-id><institution-id institution-id-type="GRID">grid.411760.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 1378 7891</institution-id><institution>Interventional and Experimental Endoscopy (InExEn), Department of Internal Medicine II, </institution><institution>University Hospital W&#x000fc;rzburg, </institution></institution-wrap>W&#x000fc;rzburg, Germany </aff><aff id="Aff8"><label>8</label>Medical Office for Gastroenterology and Internal Medicine, Dippoldiswalde, Germany </aff></contrib-group><pub-date pub-type="epub"><day>20</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>12</volume><elocation-id>828</elocation-id><history><date date-type="received"><day>27</day><month>3</month><year>2024</year></date><date date-type="accepted"><day>29</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Video capsule endoscopy (VCE) is an important technology with many advantages (non-invasive, representation of small bowel), but faces many limitations as well (time-consuming analysis, short battery lifetime, and poor image quality). Artificial intelligence (AI) holds potential to address every one of these challenges, however the progression of machine learning methods is limited by the avaibility of extensive data. We propose <italic>Galar</italic>, the most comprehensive dataset of VCE to date. <italic>Galar</italic> consists of 80 videos, culminating in 3,513,539 annotated frames covering functional, anatomical, and pathological aspects and introducing a selection of 29 distinct labels. The multisystem and multicenter VCE data from two centers in Saxony (Germany), was annotated framewise and cross-validated by five annotators. The vast scope of annotation and size of <italic>Galar</italic> make the dataset a valuable resource for the use of AI models in VCE, thereby facilitating research in diagnostic methods, patient care workflow, and the development of predictive analytics in the field.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Medical research</kwd><kwd>Scientific data</kwd><kwd>Gastrointestinal diseases</kwd><kwd>Pathology</kwd></kwd-group><funding-group><award-group><funding-source><institution>This research was funded by the German Federal Ministry of Education and Research (BMBF) within the project SEMECO-A4 (03ZU1210HB).</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background &#x00026; Summary</title><p id="Par2">Video Capsule Endoscopy (VCE) is a minimally invasive gastroenterological imaging procedure used to capture video footage of a patient&#x02019;s digestive tract. This is especially relevant for the small intestine, which is not readily accessible through conventional endoscopic procedures like colonoscopy and esophagogastroduodenoscopy. However, this comes with limitations such as a time-consuming manual analysis<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, technical restrictions (e.g., battery runtime<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> or a lack of active locomotion), and heterogeneous image quality. In 16.5% of cases, the capsule does not pass through the ileocecal valve, resulting in incomplete small intestine examinations<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>.</p><p id="Par3">VCE is currently primarily employed for the detection of internal bleeding<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>. However, the potential use cases for VCE are far more expansive. The indications for capsule endoscopy are evolving alongside technological advancements, such as the introduction of colon capsule endoscopy<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, thereby expanding its use e.g. in pediatric populations and for inflammatory bowel disease<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>.</p><p id="Par4">The major drawback of VCE is the large amount of video footage generated, as medical staff are required to watch hours of recorded video. In these recordings, the section of interest is a tiny subset of the total video, and fluctuating image quality renders large parts unusable for diagnostic purposes. The use of Artificial Intelligence (AI) in VCE is already reducing the diagnostic evaluation time needed to interpret the large amount of VCE footage. With the rise of AI in VCE, the procedure has the potential to become more widely used and thereby more cost-efficient, as observed in other modalities, such as AI-assisted X-Ray evaluation<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. Lately, the integration of Edge AI emphasizes the growing need for efficient, miniaturized algorithms for low-power devices<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>, which opens up new possibilities for real-time analysis within VCE systems.</p><p id="Par5">The successful development of AI models requires substantial quantities of high-quality data, as well as precise and rigorous annotations<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. However, the availability of large datasets is scarce; the VCE-datasets thus far publicly available are either relatively small<sup><xref ref-type="bibr" rid="CR12">12</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref></sup>, or are limited to specific questions (e.g., quality, ulcers, bleeding, polyps, anatomy)<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR15">15</xref>&#x02013; <xref ref-type="bibr" rid="CR18">18</xref></sup>. To further drive the progress of AI in VCE, the creation of large, preprocessed, and annotated datasets is necessary<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>. Most academic research projects process their own data, which is tailored to their specific tasks<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup> and do not make their datasets publicly available. The drawback of such an individualistic approach is that it necessitates a disproportionate amount of resources, limiting the progress of research. The existence of large, high-quality datasets could reduce the cost and effort involved in developing research for VCE and other medical technologies<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>.</p><p id="Par6">In this publication, we introduce a dataset that marks a considerable advancement in the field of capsule endoscopic research. In the domain of VCE there are a few openly accessible datasets, an overview of these is given in Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref>. Galar positions itself to be one of the largest datasets in the field. With 29 distinct labels, incorporating a broad range of functional, anatomical, and pathological annotations across 3,513,539 frames, Galar is primed for application in the field of machine learning.</p><p id="Par7">Furthermore, the <italic>Galar</italic> dataset consists of VCE data from two endoscopy centers in Germany, with two different capsule systems (Olympus<sup>TM</sup> Endocapsule 10 System, PillCam<sup>TM</sup> SB2, SB3, and Colon2 Capsule Endoscopy Systems<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>). As multidisciplinary and multicenter VCE research is needed for the clinical use of AI in patient diagnosis<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>, this further elevates Galar in its usefulness.</p><p id="Par8">In summary, we provide a multicentric, multisystem dataset with high frame count and the most diverse and detailed annotations to date. These characteristics establish <italic>Galar</italic> as a robust resource for training machine learning models in video capsule endoscopy.</p></sec><sec id="Sec2"><title>Methods</title><p id="Par9">Videos were collected from the University Hospital Carl Gustav Carus (Dresden, Germany) and from an outpatient practice for gastroenterology (Dippoldiswalde, Germany). VCE recordings were obtained from August 2011 to March 2023 using the Olympus<sup>TM</sup> Endocapsule 10 System (Hamburg, Germany) as well as the PillCam<sup>TM</sup> SB2, SB3, and Colon Capsule Endoscopy Systems (Meerbusch, Germany)<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>. The videos were initially generated in proprietary data formats and were converted to the Moving Picture Experts Group (MPEG) format. The video resolution ranged from 336 &#x000d7; 336 pixels (Olympus<sup>TM</sup>) to 576&#x000a0;&#x000d7;&#x000a0;576 pixels (PillCam<sup>TM</sup>). Out of the 449 recordings, 80 videos were pre-selected for annotation based on the related findings by selecting only pathological videos for annotation. To de-identify VCE recordings, randomly generated study IDs were assigned, and the videos were cut. Afterwards, videos were transferred to university servers. There each video in the dataset was labeled framewise, resulting in 3,513,539 labeled frames.</p><p id="Par10">This study was approved by the Ethics Committee of the University Hospital Carl Gustav Carus at the Technical University of Dresden on December 16, 2022 (Ethics ID: BO-EK-534122022), confirming adherence to the ethical principles of the Declaration of Helsinki. Due to the retrospective anonymization of the data and their collection during clinically indicated routine interventions, explicit consent was not required. This is additionally supported by the Ethics Committee&#x02019;s approval, a consultation with the data privacy officer, and local law. Section 34, Paragraph 1 of the Saxon Hospital Act (S&#x000e4;chsKHG) explicitly allows the collection and analysis of this type of data.</p><sec id="Sec3"><title>Data preparation</title><p id="Par11">CVAT<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> is a web-based, open-source image- and video annotation tool. Using CVAT, five annotators (a team of experienced gastroenterologists and trained medical students) labelled the data. The labels were categorized into three main groups: The <bold>technical</bold> group consists of labels concerning the image quality, where <italic>good view</italic> indicates a reduction of the view by less than 50%, <italic>reduced view</italic> indicates a reduction of the view by over 50%, and <italic>no view</italic> indicates a reduction of the view by over 95%. Furthermore, a distinction is made between <italic>bubbles</italic> and <italic>dirt</italic> as factors contributing to the degradation of image quality. The <bold>anatomical</bold> group consists of typical landmarks: <italic>z-line, pylorus, papilla of Vater, ileocecal valve</italic> and the different sections of the gastrointestinal tract: <italic>mouth, esophagus, stomach, small intestine, colon</italic>. The final group is the <bold>pathological</bold> group, which consists of the most frequent pathologies found in VCE and some less frequent findings: <italic>ulcer, polyp, active bleeding, blood, erythema, erosion, angiectasia, inflammatory bowel disease, foreign body, esophagitis, varices, hematin, celiac, cancer, lymphangiectasis</italic>. The pathologies <italic>esophagitis, varices and celiac</italic> did not occur in any of the videos. Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> gives an overview of example images of the 26 labels in the dataset, Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> displays the number of annotated frames per label.<fig id="Fig1"><label>Fig. 1</label><caption><p>Example images of the 26 labels in the dataset. The figure does not contain images of the labels <italic>esophagitis</italic>, <italic>varices</italic> and <italic>celiac</italic>, as there were no instances of these pathologies present in the set of VCE studies.</p></caption><graphic xlink:href="41597_2025_5112_Fig1_HTML" id="d33e532"/></fig><fig id="Fig2"><label>Fig. 2</label><caption><p>Overall frames per label count of the <italic>Galar</italic> dataset. Image occurrences per labels are displayed across the three main groups (technical, sections and anatomical). The y-axis is scaled logarithmically. Legend: Orange - Anatomical Green - Pathologies Red - Technical.</p></caption><graphic xlink:href="41597_2025_5112_Fig2_HTML" id="d33e543"/></fig></p></sec><sec id="Sec4"><title>Annotation Process</title><p id="Par12">An early decision was made to label each frame in the dataset individually, with the annotation process occurring in multiple stages. From each of the videos, every unique frame was extracted using Python (v3.9.8)<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> and FFMPEG (v4.0.6)<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Frames originating from the PillCam<sup>TM</sup> capsule system were cropped to remove black borders. A timestamp, visible in the top right corner, was also removed. No further pre-processing was done for the videos from the Olympus<sup>TM</sup> capsule system. Subsequently, the frames were uploaded to CVAT, where frames were annotated by our team. Frames containing unrecognizable features were given the label <italic>unknown</italic>. Then, all frames labeled with a pathology were cross-validated with the confirmation of a secondary annotator. Any frames still possessing the <italic>unknown</italic> label were reviewed by a gastroenterologist with 10 years of experience in endoscopy and were relabeled accordingly.</p></sec></sec><sec id="Sec5"><title>Data Records</title><p id="Par13">The <italic>Galar</italic> VCE dataset can be found in the open access repository <italic>figshare</italic><sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. It consists of 3,513,539 frames, each labeled with 29 labels, and has a total size of &#x000a0;~ 580GB. A detailed overview of the structure of the dataset is shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>. Each video in the dataset was labeled framewise. The dataset contains the folders <italic>Frames</italic> and <italic>Labels</italic>. The <italic>Labels</italic> folder contains CSV files, where each file has a header starting with the <italic>index</italic> column, followed by the columns of the 29 possible labels described in Data preparation and ending with the <italic>frame</italic> column, which refers to the corresponding frame the labels belong to. The <italic>Frames</italic> folder consists of 80 sub-folders, each containing the frames associated with a study. Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> shows the number of videos, resolution, and distribution of frames per capsule system. Additionally, a metadata file is provided, containing patient age, gender, and capsule system used.<fig id="Fig3"><label>Fig. 3</label><caption><p>The file structure of the <italic>Galar</italic> dataset. Frames are stored chronologically in subfolders of the <italic>Frames</italic> folder. Labels are stored in a single CSV file, per study. The metadata file further contains data on a per study basis.</p></caption><graphic xlink:href="41597_2025_5112_Fig3_HTML" id="d33e618"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>Overview of the data records in the <italic>Galar</italic> dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Capsule System</th><th align="left">Resolution (Pixels)</th><th align="left">No. of frames</th><th align="left">No. of videos</th></tr></thead><tbody><tr><td>PillCam<sup>TM</sup> University Hospital Dresden</td><td>512&#x000a0;&#x000d7;&#x000a0;512</td><td>528,470</td><td>38</td></tr><tr><td>Olympus<sup>TM</sup> University Hospital Dresden</td><td>336&#x000a0;&#x000d7;&#x000a0;336</td><td>2,750,514</td><td>22</td></tr><tr><td>PillCam<sup>TM</sup> Dippoldiswalde</td><td>512&#x000a0;&#x000d7;&#x000a0;512</td><td>234,555</td><td>20</td></tr></tbody></table><table-wrap-foot><p>Description of the resolution, number of frames, and number of videos per capsule system.</p></table-wrap-foot></table-wrap></p><p id="Par14">Six videos contain technical annotations: <italic>5, 8, 9, 13, 14, 22</italic>. A total of 35,733 frames were annotated with this label category. The creation of technical annotations was found to be more resource intensive compared to the other categories, as the visibility is more volatile and prone to sudden change. As the category is highly relevant for machine learning (ML) applications in VCE, the labels were included for completeness.</p></sec><sec id="Sec6"><title>Technical Validation</title><p id="Par15">The dataset was used to train multiple ResNet-50 models<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. The data was split into a set for training and validation consisting of 60 videos, and a test set comprised of the remaining 20 videos. K-fold cross-validation was performed on the data in the training and validation set. The videos from the test set all originate from the Dippoldiswalde practice and there is no overlap of these videos with those from the train set.</p><p id="Par16">The labels <italic>dirt</italic> and <italic>bubbles</italic> form a multi-label classification problem, while the labels <italic>good view</italic>, <italic>reduced view</italic>, and <italic>no view</italic> as well as the section labels <italic>mouth, esophagus, stomach, small intestine</italic>, and <italic>colon</italic> require multi-class classification. Some of the other more frequently occurring pathological (e.g., <italic>blood</italic> or <italic>polyp</italic>) labels are trained on separately.</p><p id="Par17">For the classification of the multi-label and the multi-class models, 5-fold cross-validation was employed. The binary classification of the pathologic labels was done using 2-fold cross-validation, as some labels were not contained in a sufficient number of videos. To ensure that the frames of one patient are not spread over the training and test set and to get the best possible distribution of the labels over the folds, sklearn&#x02019;s StratifiedGroupKFold<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> method was applied.</p><p id="Par18">The ResNet-50 model pre-trained on ImageNet<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> was fine-tuned for 10 epochs using PyTorch (v2.0.1)<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Following this, fine-tuning was done for each of the target tasks. These models were trained over 100 epochs (with early stopping), with a 128 Batch size and a 0.001 learning rate. For each image, a Resize transform was applied, to scale the image down to 224&#x000a0;&#x000d7;&#x000a0;224. Additionally, the transforms ShiftScaleRotate, RGBShift, GaussNoise and RandomBrightnessContrast were each applied with a 30% likelihood to each image. The small subset of images which contain the technical annotation were fine-tuned similarly, excepting the epochs, which were capped at 50 with early stopping.</p><p id="Par19">As measurements for the classification performance, the F-1 score, the Area under the Receiver Operating Characteristic Curve (AUROC) as well as the accuracy were calculated using the TorchMetrics (v1.0.3)<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> Python library.</p><p id="Par20">Tables&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab4" ref-type="table">4</xref>, and <xref rid="Tab5" ref-type="table">5</xref> show results for the classification models. The model fine-tuned for <italic>dirt</italic> and <italic>bubbles</italic>, along with the two multi-class models, performed decently with accuracy value up to 93% for the labels <italic>stomach</italic> and 92% for <italic>small intestine</italic>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Classification results for a ResNet-50 fine-tuned on <italic>bubbles</italic> and <italic>dirt</italic>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Label</th><th align="left">F1</th><th align="left">AUROC</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td>bubbles</td><td>0.89</td><td>0.87</td><td>0.82</td></tr><tr><td>dirt</td><td>0.88</td><td>0.94</td><td>0.88</td></tr><tr><td>macro average</td><td>0.88</td><td>0.90</td><td>0.85</td></tr><tr><td>micro average</td><td>0.89</td><td>&#x02014;</td><td>0.85</td></tr></tbody></table><table-wrap-foot><p>The metrics were computed individually for each label, and both macro- and micro-averaged scores are calculated across all labels. The outcomes are averaged across the 5 cross-validation folds.</p></table-wrap-foot></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>Classification results for a ResNet-50 fine-tuned on <italic>good view</italic>, <italic>reduced view</italic>, and <italic>no view</italic>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Label</th><th align="left">F1</th><th align="left">AUROC</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td>good view</td><td>0.88</td><td>0.88</td><td>0.87</td></tr><tr><td>reduced view</td><td>0.41</td><td>0.85</td><td>0.47</td></tr><tr><td>no view</td><td>0.33</td><td>0.91</td><td>0.29</td></tr><tr><td>macro average</td><td>0.54</td><td>0.88</td><td>0.54</td></tr><tr><td>micro average</td><td>0.79</td><td>&#x02014;</td><td>0.79</td></tr></tbody></table><table-wrap-foot><p>The metrics are computed individually for each label, and both macro- and micro-averaged scores are calculated across all labels. The outcomes are averaged across the 5 cross-validation folds.</p></table-wrap-foot></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Classification results for a ResNet-50 fine-tuned on <italic>mouth</italic>, <italic>esophagus</italic>, <italic>stomach</italic>, <italic>small intestine</italic>, and <italic>colon</italic>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Label</th><th align="left">F1</th><th align="left">AUROC</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td>mouth</td><td>0.42</td><td>1.00</td><td>0.75</td></tr><tr><td>esophagus</td><td>0.65</td><td>1.00</td><td>0.73</td></tr><tr><td>stomach</td><td>0.78</td><td>0.93</td><td>0.93</td></tr><tr><td>small intestine</td><td>0.93</td><td>0.95</td><td>0.92</td></tr><tr><td>colon</td><td>0.75</td><td>0.96</td><td>0.72</td></tr><tr><td>macro average</td><td>0.71</td><td>0.96</td><td>0.81</td></tr><tr><td>micro average</td><td>0.89</td><td>&#x02014;</td><td>0.89</td></tr></tbody></table><table-wrap-foot><p>The metrics are computed individually for each label, and both macro- and micro-averaged scores are calculated across all labels. The outcomes are averaged across the 5 cross-validation folds.</p></table-wrap-foot></table-wrap><table-wrap id="Tab5"><label>Table 5</label><caption><p>Classification results for multiple ResNet-50 models, each fine-tuned, on points of interest (e.g. <italic>blood)</italic>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Label</th><th align="left">F1</th><th align="left">AUROC</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td>blood</td><td>0.14</td><td>0.87</td><td>0.98</td></tr><tr><td>pylorus</td><td>0.01</td><td>0.74</td><td>0.99</td></tr><tr><td>z-line</td><td>0.02</td><td>0.87</td><td>1.00</td></tr><tr><td>ulcer</td><td>0.00</td><td>0.44</td><td>0.99</td></tr><tr><td>polyp</td><td>0.05</td><td>0.73</td><td>1.00</td></tr><tr><td>erythema</td><td>0.02</td><td>0.70</td><td>1.00</td></tr></tbody></table><table-wrap-foot><p>The metrics are computed individually for each label. The outcomes are averaged across the 2 cross-validation folds.</p></table-wrap-foot></table-wrap></p><p id="Par21">The binary models for pathological labels encountered challenges to accurately identify positive samples. To improve performance, weighted sampling as well as weighted loss was explored. For weighted sampling, the probability for an image to be sampled was based on the occurrence of its class, as a fraction of the total dataset. For the more complex multi-label problem, each unique combination of labels was assigned a weight, again as a portion of the total dataset. This made the weights dynamic, based on the target the model is trained on.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Overview of openly accessible VCE datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Size</th><th align="left">No. of Labels</th></tr></thead><tbody><tr><td>Kvasir-Capsule<sup><xref ref-type="bibr" rid="CR12">12</xref></sup></td><td>47,238 labelled images</td><td>3 Landmarks, 2 Technical, 9 Pathological</td></tr><tr><td>Rhode Island<sup><xref ref-type="bibr" rid="CR15">15</xref></sup></td><td>5,247,588 labelled images</td><td>4 Sections</td></tr><tr><td>AI-KODA<sup><xref ref-type="bibr" rid="CR17">17</xref></sup></td><td>2,173 labelled images from 28 patients</td><td>8 Technical</td></tr><tr><td>VCE-AnomalyNet<sup><xref ref-type="bibr" rid="CR18">18</xref></sup></td><td>108,832 labelled images</td><td>17 Pathological</td></tr><tr><td>ERS Dataset<sup><xref ref-type="bibr" rid="CR14">14</xref></sup></td><td>5,970 labelled images (982,041 imprecisely labelled *)</td><td>123 labels</td></tr><tr><td>SEE-AI<sup><xref ref-type="bibr" rid="CR13">13</xref></sup></td><td>18,481 labelled images</td><td>12 Total</td></tr></tbody></table><table-wrap-foot><p>*Imprecisely labelled images inherit labels from those that are labelled by an expert, and where the image appears chronologically close.</p></table-wrap-foot></table-wrap></p><p id="Par22">Although these strategies helped to improve performance on some labels, other required heavy parameter optimization. This underscores the difficulty and necessity of improving and developing AI methods to address the challenges of imbalanced label distrubution and multi-source data. Consequently, it highlights the importance of a multicentric, multisystem dataset with extensive annotations of pathologies.</p></sec><sec id="Sec7"><title>Usage Notes</title><p id="Par23">With <italic>Galar</italic> we provide the largest public VCE dataset, both in terms of the number of features labeled per image and the total number of annotated images. The large number of ground truth labeled images allows for supervised training of machine learning models and is a significant contribution to the landscape of publicly available VCE datasets.</p><p id="Par24">If the dataset is to be employed for machine learning applications, it is essential to carefully partition the data into training and validation sets. The comparative rarity of select labels, especially over others in the same class, must be respected. Additionally, the data originates from two different VCE systems and was collected at two different study sites. Patients of varying age and gender are also present in the dataset. This information must be considered when generating splits. The metadata file, found in the figshare repository, provides information regarding the capsule system and patient age and gender, per individual study.</p><p id="Par25">The dataset is provided compressed, in the 7-Zip (.7z) format. The data must be uncompressed before it may be viewed and modified; common operating systems (Windows, Linux, MacOs) by default provide archive utility which enables this.</p><p id="Par26">By licensing the dataset under a Creative Commons Attribution 4.0 International (CC BY 4.0) License which allows sharing, copying, and redistribution, as well as adaptation and transformation, we hope to advance research in the field. For more details about Creative Commons licensing, please refer to <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org">https://creativecommons.org</ext-link>.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Maxime Le Floch, Fabian Wolf, Lucian McIntyre, Nora Herzog, Franz Brinkmann.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This research was funded by the BMBF (German Federal Ministry of Education and Research) as part of the SEMECO cluster4future FKZ 03ZU1210GA and 03ZU121HB.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>M.L.F.: methodology, investigation, data curation, writing original draft, visualization. F.W.: methodology, software, technical validation, writing original draft, visualization. L.M.: methodology, software, data curation, writing original draft, visualization. C.W.: methodology, data annotation. A.P.: methodology, data annotation. K.V.: methodology, data annotation. P.H.: data curation, investigation. S.H.K.: methodology, investigation, clinical supervision. J.L.S.: methodology, investigation, review and editing. C.S.: data curation. M.E.G.: methodology, data curation, data annotation. M.H.: review and editing. S.S.: data curation, technical and clinical supervision. A.M.: methodology, review and editing. A.H.: methodology, review and editing. J.N.K.: review and editing, supervision. S.P.: conceptualization, investigation, data curation. J.H.: conceptualization, review and editing, supervision, N.H.: conceptualization, investigation, data curation, writing original draft, review, and editing, visualization, supervision, project administration, funding acquisition. F.B.: conceptualization, investigation, data curation, review, and editing, visualization, supervision, project administration, funding acquisition.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>Open Access funding enabled and organized by Projekt DEAL.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The code employed for the technical validation can be accessed via our public GitHub Repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/EKFZ-AI-Endoscopy/GalarCapsuleML">https://github.com/EKFZ-AI-Endoscopy/GalarCapsuleML</ext-link>. The repository contains a full guide on running the code, tuning hyperparameters, and generating statistics.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par27">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmed</surname><given-names>M</given-names></name><etal/></person-group><article-title>Video Capsule Endoscopy in Gastroenterology</article-title><source>Gastroenterology Research</source><year>2022</year><volume>15</volume><fpage>47</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.14740/gr1487</pub-id><pub-id pub-id-type="pmid">35572472</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Ahmed, M. <italic>et al</italic>. Video Capsule Endoscopy in Gastroenterology. <italic>Gastroenterology Research</italic><bold>15</bold>, 47&#x02013;55, 10.14740/gr1487 (2022).<pub-id pub-id-type="pmid">35572472</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Kwack</surname><given-names>WG</given-names></name><etal/></person-group><article-title>Current Status and Research into Overcoming Limitations of Capsule Endoscopy</article-title><source>Clinical endoscopy</source><year>2016</year><volume>49</volume><fpage>8</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.5946/ce.2016.49.1.8</pub-id><pub-id pub-id-type="pmid">26855917</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Kwack, W. G. <italic>et al</italic>. Current Status and Research into Overcoming Limitations of Capsule Endoscopy. <italic>Clinical endoscopy</italic><bold>49</bold>, 8&#x02013;15, 10.5946/ce.2016.49.1.8 (2016).<pub-id pub-id-type="pmid">26855917</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Indications and detection, completion, and retention rates of small-bowel capsule endoscopy: a systematic review</article-title><source>Gastrointestinal Endoscopy</source><year>2010</year><volume>71</volume><fpage>280</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1016/j.gie.2009.09.031</pub-id><pub-id pub-id-type="pmid">20152309</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Liao, Z. <italic>et al</italic>. Indications and detection, completion, and retention rates of small-bowel capsule endoscopy: a systematic review. <italic>Gastrointestinal Endoscopy</italic><bold>71</bold>, 280&#x02013;286, 10.1016/j.gie.2009.09.031 (2010).<pub-id pub-id-type="pmid">20152309</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Goenka</surname><given-names>MK</given-names></name><etal/></person-group><article-title>Capsule endoscopy: Present status and future expectation</article-title><source>World Journal of Gastroenterology</source><year>2014</year><volume>20</volume><fpage>10024</fpage><lpage>10037</lpage><pub-id pub-id-type="doi">10.3748/wjg.v20.i29.10024</pub-id><pub-id pub-id-type="pmid">25110430</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Goenka, M. K. <italic>et al</italic>. Capsule endoscopy: Present status and future expectation. <italic>World Journal of Gastroenterology</italic><bold>20</bold>, 10024&#x02013;10037, 10.3748/wjg.v20.i29.10024 (2014).<pub-id pub-id-type="pmid">25110430</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Iddan</surname><given-names>G</given-names></name><etal/></person-group><article-title>Wireless capsule endoscopy</article-title><source>Nature</source><year>2000</year><volume>405</volume><fpage>417</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1038/35013140</pub-id><pub-id pub-id-type="pmid">10839527</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Iddan, G. <italic>et al</italic>. Wireless capsule endoscopy. <italic>Nature</italic><bold>405</bold>, 417&#x02013;417, 10.1038/35013140 (2000).<pub-id pub-id-type="pmid">10839527</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Spada</surname><given-names>C</given-names></name><etal/></person-group><article-title>Colon capsule endoscopy: What we know and what we would like to know</article-title><source>World Journal of Gastroenterology : WJG</source><year>2014</year><volume>20</volume><fpage>16948</fpage><lpage>16955</lpage><pub-id pub-id-type="doi">10.3748/wjg.v20.i45.16948</pub-id><pub-id pub-id-type="pmid">25493007</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Spada, C. <italic>et al</italic>. Colon capsule endoscopy: What we know and what we would like to know. <italic>World Journal of Gastroenterology : WJG</italic><bold>20</bold>, 16948&#x02013;16955, 10.3748/wjg.v20.i45.16948 (2014).<pub-id pub-id-type="pmid">25493007</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Nemeth</surname><given-names>A</given-names></name><etal/></person-group><article-title>Video capsule endoscopy in pediatric patients with Crohn&#x02019;s disease: a single-center experience of 180 procedures</article-title><source>Therapeutic Advances in Gastroenterology</source><year>2018</year><volume>11</volume><fpage>1756284818758929</fpage><pub-id pub-id-type="doi">10.1177/1756284818758929</pub-id><pub-id pub-id-type="pmid">29531578</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Nemeth, A. <italic>et al</italic>. Video capsule endoscopy in pediatric patients with Crohn&#x02019;s disease: a single-center experience of 180 procedures. <italic>Therapeutic Advances in Gastroenterology</italic><bold>11</bold>, 1756284818758929, 10.1177/1756284818758929 (2018).<pub-id pub-id-type="pmid">29531578</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Mun, S. K. <italic>et al</italic>. Artificial Intelligence for the Future Radiology Diagnostic Service. <italic>Frontiers in Molecular Biosciences</italic>, <bold>7</bold>, <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fmolb.2020.614258">https://www.frontiersin.org/articles/10.3389/fmolb.2020.614258</ext-link> (2021).</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Werner, J. <italic>et al</italic>. Precise Localization Within the GI Tract by Combining Classification of CNNs and Time-Series Analysis of HMMs., 174&#x02013;183, 10.1007/978-3-031-45676-3_18 (2024).</mixed-citation></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>A locally-processed light-weight deep neural network for detecting colorectal polyps in wireless capsule endoscopes</article-title><source>J. Real-Time Image Process.</source><year>2021</year><volume>18(4)</volume><fpage>1183</fpage><lpage>1194</lpage><pub-id pub-id-type="doi">10.1007/s11554-021-01126-7</pub-id></element-citation><mixed-citation id="mc-CR10" publication-type="journal">Wang, Y. <italic>et al</italic>. A locally-processed light-weight deep neural network for detecting colorectal polyps in wireless capsule endoscopes. <italic>J. Real-Time Image Process.</italic><bold>18</bold>(4), 1183&#x02013;1194, 10.1007/s11554-021-01126-7 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><etal/></person-group><article-title>Annotation-efficient deep learning for automatic medical image segmentation</article-title><source>Nature Communications</source><year>2021</year><volume>12</volume><fpage>5915</fpage><pub-id pub-id-type="doi">10.1038/s41467-021-26216-9</pub-id><pub-id pub-id-type="pmid">34625565</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Wang, S. <italic>et al</italic>. Annotation-efficient deep learning for automatic medical image segmentation. <italic>Nature Communications</italic><bold>12</bold>, 5915, 10.1038/s41467-021-26216-9 (2021).<pub-id pub-id-type="pmid">34625565</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Smedsrud</surname><given-names>PH</given-names></name><etal/></person-group><article-title>Kvasir-Capsule, a video capsule endoscopy dataset</article-title><source>Scientific Data</source><year>2021</year><volume>8</volume><fpage>142</fpage><pub-id pub-id-type="doi">10.1038/s41597-021-00920-z</pub-id><pub-id pub-id-type="pmid">34045470</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Smedsrud, P. H. <italic>et al</italic>. Kvasir-Capsule, a video capsule endoscopy dataset. <italic>Scientific Data</italic><bold>8</bold>, 142, 10.1038/s41597-021-00920-z (2021).<pub-id pub-id-type="pmid">34045470</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Akihito, Y. <italic>et al</italic>. The SEE-AI Project Dataset, 10.34740/KAGGLE/DS/1516536 (2022).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Cychnerski, J. <italic>et al</italic>. ERS: a novel comprehensive endoscopy image dataset for machine learning, compliant with the MST 3.0 specification. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2201.08746">https://arxiv.org/abs/2201.08746</ext-link> (2022).</mixed-citation></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Charoen</surname><given-names>A</given-names></name><etal/></person-group><article-title>Rhode Island gastroenterology video capsule endoscopy data set</article-title><source>Scientific Data</source><year>2022</year><volume>9</volume><fpage>602</fpage><pub-id pub-id-type="doi">10.1038/s41597-022-01726-3</pub-id><pub-id pub-id-type="pmid">36202840</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Charoen, A. <italic>et al</italic>. Rhode Island gastroenterology video capsule endoscopy data set. <italic>Scientific Data</italic><bold>9</bold>, 602, 10.1038/s41597-022-01726-3 (2022).<pub-id pub-id-type="pmid">36202840</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Iakovidis</surname><given-names>DK</given-names></name><etal/></person-group><article-title>Software for enhanced video capsule endoscopy: challenges for essential progress</article-title><source>Nature Reviews Gastroenterology &#x00026; Hepatology</source><year>2015</year><volume>12</volume><fpage>172</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1038/nrgastro.2015.13</pub-id><pub-id pub-id-type="pmid">25688052</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Iakovidis, D. K. <italic>et al</italic>. Software for enhanced video capsule endoscopy: challenges for essential progress. <italic>Nature Reviews Gastroenterology &#x00026; Hepatology</italic><bold>12</bold>, 172&#x02013;186, 10.1038/nrgastro.2015.13 (2015).<pub-id pub-id-type="pmid">25688052</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Palak, H. <italic>et al</italic>. Deepak Gunjan, Prof. Nidhi Goel, Prof. S. Indu. AI-KODA Dataset: An AI-Image Dataset for Automatic Assessment of Cleanliness in Video Capsule Endoscopy as per Korea-Canada Scores., 10.6084/m9.figshare.25807915.v1 (2024).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Thakur, A. <italic>et al</italic>. VCE-AnomalyNet: A New Dataset Fueling AI Precision in Anomaly Detection for Video Capsule Endoscopy, 10.22541/au.171387106.63353485/v1 (2024).</mixed-citation></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>J</given-names></name><etal/></person-group><article-title>Recent Development of Computer Vision Technology to Improve Capsule Endoscopy</article-title><source>Clinical Endoscopy</source><year>2019</year><volume>52</volume><fpage>328</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.5946/ce.2018.172</pub-id><pub-id pub-id-type="pmid">30786704</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Park, J. <italic>et al</italic>. Recent Development of Computer Vision Technology to Improve Capsule Endoscopy. <italic>Clinical Endoscopy</italic><bold>52</bold>, 328&#x02013;333, 10.5946/ce.2018.172 (2019).<pub-id pub-id-type="pmid">30786704</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Hwang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Improved classification and localization approach to small bowel capsule endoscopy using convolutional neural network</article-title><source>Digestive Endoscopy</source><year>2021</year><volume>33</volume><fpage>598</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1111/den.13787</pub-id><pub-id pub-id-type="pmid">32640059</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">Hwang, Y. <italic>et al</italic>. Improved classification and localization approach to small bowel capsule endoscopy using convolutional neural network. <italic>Digestive Endoscopy</italic><bold>33</bold>, 598&#x02013;607, 10.1111/den.13787 (2021).<pub-id pub-id-type="pmid">32640059</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Mascarenhas</surname><given-names>S</given-names></name><etal/></person-group><article-title>Artificial Intelligence and Capsule Endoscopy: Automatic Detection of Small Bowel Blood Content Using a Convolutional Neural Network</article-title><source>GE - Portuguese Journal of Gastroenterology</source><year>2021</year><volume>29</volume><fpage>331</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1159/000518901</pub-id><pub-id pub-id-type="pmid">36159196</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Mascarenhas, S. <italic>et al</italic>. Artificial Intelligence and Capsule Endoscopy: Automatic Detection of Small Bowel Blood Content Using a Convolutional Neural Network. <italic>GE - Portuguese Journal of Gastroenterology</italic><bold>29</bold>, 331&#x02013;338, 10.1159/000518901 (2021).<pub-id pub-id-type="pmid">36159196</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>A</given-names></name><etal/></person-group><article-title>Shifting machine learning for healthcare from development to deployment and from models to data</article-title><source>Nature Biomedical Engineering</source><year>2022</year><volume>6</volume><fpage>1330</fpage><lpage>1345</lpage><pub-id pub-id-type="doi">10.1038/s41551-022-00898-y</pub-id><pub-id pub-id-type="pmid">35788685</pub-id>
</element-citation><mixed-citation id="mc-CR22" publication-type="journal">Zhang, A. <italic>et al</italic>. Shifting machine learning for healthcare from development to deployment and from models to data. <italic>Nature Biomedical Engineering</italic><bold>6</bold>, 1330&#x02013;1345, 10.1038/s41551-022-00898-y (2022).<pub-id pub-id-type="pmid">35788685</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">PillCam SB 3 Capsule &#x02223; Medtronic (UK). <ext-link ext-link-type="uri" xlink:href="https://www.medtronic.com/covidien/en-gb/products/capsule-endoscopy/pillcam-capsules/pillcam-sb-3-capsule.html">https://www.medtronic.com/covidien/en-gb/products/capsule-endoscopy/pillcam-capsules/pillcam-sb-3-capsule.html</ext-link>.</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Kapselendoskopie - Gastroenterologie - Olympus Medizintechnik. <ext-link ext-link-type="uri" xlink:href="https://www.olympus.de/medical/de/Produkte-und-L">https://www.olympus.de/medical/de/Produkte-und-L</ext-link>.</mixed-citation></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>YJ</given-names></name><etal/></person-group><article-title>The Future of Capsule Endoscopy: The Role of Artificial Intelligence and Other Technical Advancements</article-title><source>Clinical Endoscopy</source><year>2020</year><volume>53</volume><fpage>387</fpage><lpage>394</lpage><pub-id pub-id-type="doi">10.5946/ce.2020.133</pub-id><pub-id pub-id-type="pmid">32668529</pub-id>
</element-citation><mixed-citation id="mc-CR25" publication-type="journal">Yang, Y. J. <italic>et al</italic>. The Future of Capsule Endoscopy: The Role of Artificial Intelligence and Other Technical Advancements. <italic>Clinical Endoscopy</italic><bold>53</bold>, 387&#x02013;394, 10.5946/ce.2020.133 (2020).<pub-id pub-id-type="pmid">32668529</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">CVAT. <ext-link ext-link-type="uri" xlink:href="https://www.cvat.ai/">https://www.cvat.ai/</ext-link>.</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Python. <ext-link ext-link-type="uri" xlink:href="https://www.python.org/">https://www.python.org/</ext-link>.</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">FFMPEG, <italic>et al</italic>. <ext-link ext-link-type="uri" xlink:href="https://ffmpeg.org/">https://ffmpeg.org/</ext-link>.</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Le Floch, M. <italic>et al</italic>. Galar - a large multi-label video capsule endoscopy dataset. <italic>Figshare</italic>+. 10.25452/figshare.plus.25304616.v1 (2025) .</mixed-citation></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>W</given-names></name><etal/></person-group><article-title>ResNet and its application to medical image processing: Research progress and challenges</article-title><source>Computer Methods and Programs in Biomedicine</source><year>2023</year><volume>240</volume><fpage>107660</fpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2023.107660</pub-id><pub-id pub-id-type="pmid">37320940</pub-id>
</element-citation><mixed-citation id="mc-CR30" publication-type="journal">Xu, W. <italic>et al</italic>. ResNet and its application to medical image processing: Research progress and challenges. <italic>Computer Methods and Programs in Biomedicine</italic><bold>240</bold>, 107660, 10.1016/j.cmpb.2023.107660 (2023).<pub-id pub-id-type="pmid">37320940</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Sklearn&#x02019;s StratifiedGroupKFold. <italic>scikit-learn</italic>, <ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedGroupKFold.html">sklearn.model_selection.StratifiedGroupKFold.html</ext-link>.</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">ImageNet. <ext-link ext-link-type="uri" xlink:href="https://www.image-net.org/">https://www.image-net.org/</ext-link>.</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">PyTorch. <ext-link ext-link-type="uri" xlink:href="https://pytorch.org/">https://pytorch.org/</ext-link>.</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Detlefsen. TorchMetrics - Measuring Reproducibility in PyTorch. <italic>Journal of Open Source Software</italic><bold>7</bold>, 4101, 10.21105/joss.04101 (2022).</mixed-citation></ref></ref-list></back></article>