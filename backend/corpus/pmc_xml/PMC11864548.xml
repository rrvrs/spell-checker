<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40009590</article-id><article-id pub-id-type="pmc">PMC11864548</article-id><article-id pub-id-type="publisher-id">PONE-D-24-03883</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0310992</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Information Technology</subject><subj-group><subject>Natural Language Processing</subject><subj-group><subject>Named Entity Recognition</subject><subj-group><subject>Entity Disambiguation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Mathematical and Statistical Techniques</subject><subj-group><subject>Cluster Analysis</subject><subj-group><subject>Hierarchical Clustering</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Clustering Algorithms</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Clustering Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Network Analysis</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject><subj-group><subject>Recurrent Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject><subj-group><subject>Recurrent Neural Networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Semantics</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Author name disambiguation based on heterogeneous graph neural network</article-title><alt-title alt-title-type="running-head">Author name disambiguation based on heterogeneous graph neural network</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Ge</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="aff001" ref-type="aff"/></contrib><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0004-4925-3495</contrib-id><name><surname>Sun</surname><given-names>Zikai</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="aff001" ref-type="aff"/><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>HU</surname><given-names>Weiyang</given-names></name><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="aff001" ref-type="aff"/></contrib><contrib contrib-type="author"><name><surname>Cai</surname><given-names>MengHuan</given-names></name><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><xref rid="aff001" ref-type="aff"/></contrib></contrib-group><aff id="aff001">
<addr-line>College of Intelligent Equipment, Shandong University of Science and Technology, Taian, Shandong, China</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Silva</surname><given-names>Filipi N.</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Indiana University Bloomington, UNITED STATES OF AMERICA</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>s571018868@163.com</email></corresp></author-notes><pub-date pub-type="collection"><year>2025</year></pub-date><pub-date pub-type="epub"><day>26</day><month>2</month><year>2025</year></pub-date><volume>20</volume><issue>2</issue><elocation-id>e0310992</elocation-id><history><date date-type="received"><day>29</day><month>1</month><year>2024</year></date><date date-type="accepted"><day>22</day><month>8</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Wang et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Wang et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0310992.pdf"/><abstract><p>With the dramatic increase in the number of published papers and the continuous progress of deep learning technology, the research on name disambiguation is at a historic peak, the number of paper authors is increasing every year, and the situation of authors with the same name is intensifying, therefore, it is a great challenge to accurately assign the newly published papers to their respective authors. The current mainstream methods for author disambiguation are mainly divided into two methods: feature-based clustering and connection-based clustering, but none of the current mainstream methods can efficiently deal with the author name disambiguation problem, For this reason, this paper proposes the author name ablation method based on the relational graph heterogeneous attention neural network, first extract the semantic and relational information of the paper, use the constructed graph convolutional embedding module to train the splicing to get a better feature representation, and input the constructed network to get the vector representation. As the existing graph heterogeneous neural network can not learn different types of nodes and edge interaction, add multiple attention, design ablation experiments to verify its impact on the network. Finally improve the traditional hierarchical clustering method, combined with the graph relationship and topology, using training vectors instead of distance calculation, can automatically determine the optimal k-value, improve the accuracy and efficiency of clustering. The experimental results show that the average F1 value of this paper&#x02019;s method on the Aminer dataset is 0.834, which is higher than other mainstream methods.</p></abstract><funding-group><funding-statement>The author(s) received no specific funding for this work.</funding-statement></funding-group><counts><fig-count count="10"/><table-count count="5"/><page-count count="22"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>The third-party data (v1, v2, and whoiswho) used in this study are publicly available from AMiner (<ext-link xlink:href="https://www.aminer.cn/whoiswho" ext-link-type="uri">https://www.aminer.cn/whoiswho</ext-link>). The authors did not have any special access privileges to these data that others would not have. All interested researchers can access the required data for this study by following the standard procedures on the AMiner platform.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>The third-party data (v1, v2, and whoiswho) used in this study are publicly available from AMiner (<ext-link xlink:href="https://www.aminer.cn/whoiswho" ext-link-type="uri">https://www.aminer.cn/whoiswho</ext-link>). The authors did not have any special access privileges to these data that others would not have. All interested researchers can access the required data for this study by following the standard procedures on the AMiner platform.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>With the dramatic increase in the number of published papers and the continuous advancement of deep learning techniques, research on name disambiguation is at an all-time high, the number of authors of papers is increasing every year, and the situation of authors with the same name is intensifying, therefore, it is a great challenge to accurately assign the newly published papers to their respective authors. In real life, the phenomenon of same name is common. And when a digital library user searches for a particular author&#x02019;s name, he or she may see a mixture of results by different authors with the same author&#x02019;s name between these results, and distinguishing between the two is an important prerequisite for improving the quality of digital library services and content quality. The general task of author disambiguation is to associate publications with the same name or highly similarly spelled names to different human entities. However, current methods suffer from the following shortcomings: (1) Existing methods suffer from insufficient feature utilisation. Typically, these methods only single-handedly utilise paper text or relational attributes, failing to effectively mine and fuse multiple types of features that are potentially valuable for the author name disambiguation task. (2) Using only local information. Existing methods, when using the network embedding-based approach for author name disambiguation, tend to construct homogeneous networks only for the local paper data corresponding to the name that generates the ambiguity, without effectively utilizing the rich semantic information in the global heterogeneous information network, which leads to inefficient use of global information and limits the performance of the disambiguation model. In response to the above mentioned deficiencies, in this paper, we improve the above deficiencies of the existing author name disambiguation methods, design the extraction of fusion of multiple types of features, the use of our graph convolution module to train the information splicing, which is able to get a better representation of the features, the construction of a multi-attention-head heterogeneous graph convolutional attention network way to learn not to use the type of nodes and edges between the rich interaction relationship to address the characteristics of global heterogeneous information network, Finally we improved the traditional hierarchical clustering method by combining the relationships and topologies between graphs such as relational semantics with hierarchical clustering for disambiguation of homonymous authors. And the experimental comparison with several baseline methods was conducted, and the experimental results show that the average F1 value of this paper&#x02019;s method with only three paper features on the Aminer dataset is 0.834, which is higher than that of other mainstream methods.</p></sec><sec id="sec002"><title>Related work</title><p>Author name ambiguity is a common challenge in academic literature databases and digital libraries. Some authors in the real world have the same name (Aman, 2018; Kim, 2020; Shoaib et al., 2020) [<xref rid="pone.0310992.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0310992.ref002" ref-type="bibr">2</xref>]. This problem is very challenging in literature databases because, for example, as of March 2023, there are about 40K articles in Microsoft Academic Graph (MAG) that cite the work of &#x0201c;Wang Wei&#x0201d;, and the problem of acronym name ambiguity is even more significant. Although databases such as MAG and AMiner provide disambiguating author identifiers (IDs), the performance of author identifier systems created based on the author name disambiguation (And) method is far from satisfactory for databases of millions in size (Zhang et al., 2020) [<xref rid="pone.0310992.ref003" ref-type="bibr">3</xref>]. After years of research, the authors have introduced a variety of different methods within the field of name elimination. For example, a variety of classification models are used to learn the two-by-two similarity function, including Naive Bayes [<xref rid="pone.0310992.ref004" ref-type="bibr">4</xref>], Logistic Regression [<xref rid="pone.0310992.ref005" ref-type="bibr">5</xref>], Support Vector Machines, Decision Trees, Random Forests (RF) [<xref rid="pone.0310992.ref006" ref-type="bibr">6</xref>], Deep Neural Networks (DNNs) [<xref rid="pone.0310992.ref007" ref-type="bibr">7</xref>] and Gradient Augmented Trees (GBTs), etc. Qiao et al [<xref rid="pone.0310992.ref008" ref-type="bibr">8</xref>] use DNNs with hand-crafted features, whereas zhang et al [<xref rid="pone.0310992.ref009" ref-type="bibr">9</xref>] utilise DNNs to learn feature representations from bag-of-words vectors to learn feature representations from bag-of-words vectors. Bertrand M et al [<xref rid="pone.0310992.ref010" ref-type="bibr">10</xref>] first learn the representation of each record using DNN and then improve it by an autoencoder, where the encoder is constructed based on the similarity between records. Meanwhile, some studies want to use unsupervised methods [<xref rid="pone.0310992.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0310992.ref011" ref-type="bibr">11</xref>] to solve the problem of author name disambiguation, and algorithms such as DBSCAN [<xref rid="pone.0310992.ref011" ref-type="bibr">11</xref>] and hierarchical clustering [<xref rid="pone.0310992.ref012" ref-type="bibr">12</xref>] have been used for this purpose. For example, Liu et al. [<xref rid="pone.0310992.ref013" ref-type="bibr">13</xref>] and Kim et al. [<xref rid="pone.0310992.ref014" ref-type="bibr">14</xref>] used the similarity between a pair of records with the same name to disambiguate author names on the PubMed dataset. Zhang et al. [<xref rid="pone.0310992.ref015" ref-type="bibr">15</xref>] used Recurrent Neural Networks (RNN) to predict the number of authors who published only one paper in the Aminer dataset, and in this direction they proposed a two-stage approach applied to the DBLP dataset. stage approach, in which the first stage is to divide the author records into different clusters and then disambiguate each cluster separately. Wu et al. [<xref rid="pone.0310992.ref013" ref-type="bibr">13</xref>] used Shannon entropy to fuse features such as the affiliation and content of the papers to generate a matrix that represents the two-by-two correlation of the papers, while Hierarchical Agglomerative Clustering (HAC) was used to predict the number of authors who have published only one paper in the Aminer dataset. HAC) uses this matrix to disambiguate author names in the dataset. In addition, supervised methods [<xref rid="pone.0310992.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0310992.ref016" ref-type="bibr">16</xref>] have been widely used in research, mainly applied after clustering together blocks of authors sharing the same name. Han et al. [<xref rid="pone.0310992.ref004" ref-type="bibr">4</xref>] proposed two supervised learning methods to disambiguate authors in the literature. For example, in the case of a selected reference, the first method uses a plain Bayesian model to find the author block with the maximum posterior probability. The second approach uses a support vector machine approach to classify references from DBLP to the original author. Sun et al. [<xref rid="pone.0310992.ref017" ref-type="bibr">17</xref>] used heuristic features such as the percentage of citations collected by the author&#x02019;s name variable and made this to remove ambiguities in common author names. Additionally, neural networks were used to verify that two citations were close enough to determine if the two citations were published by the same target author. In [<xref rid="pone.0310992.ref018" ref-type="bibr">18</xref>], they propose an entity resolution system called &#x0201c;Deeper Levels&#x0201d;. This system uses a combination of Bidirectional Recurrent Neural Networks (BRNN) and Long Short-Term Memory (LSTM) as hidden units to generate distributed representations of each tuple in order to capture the connections between them. In summary, although there are some feature-based and connection-based author name disambiguation methods have been proposed but they tend to have problems such as insufficient use of the features and using only local information, this paper addresses the shortcomings of the existing author name disambiguation methods, and designs an author name disambiguation method that can fuse multiple types of features and use the construction of a heterogeneous graph convolutional network to solve the characteristics of the global heterogeneous information network. Specifically, our work as follows:</p><list list-type="order"><list-item><p>Proposing an author name disambiguation method based on the fusion embedding of semantic and relational features, we fuse and embed the extracted semantics and relations, which further condenses the aggregated information, transforms the high-dimensional sparse data in the graphs into the low-dimensional sparse data, and preserves the topology between individual graphs.</p></list-item><list-item><p>A global heterogeneous graph convolutional attention neural network (RA-HNet) is proposed, which uses a meta-path correlation weight sampling strategy in the network to fully fuse local and global information, and improves the structure of the graph neural network by adding a graph attention mechanism to the network in order to improve the network&#x02019;s ability to learn the expression of the features, and lightens the network so that the network is able to learn the global variables while speeding up the training speed of the network, we also experimented on the sensitivity of the newly added multi-head attention parameter, and finally used the improved hierarchical clustering (RHAC) for clustering, our clustering algorithm can not specify the number of clusters, and through the iterative updating of the algorithm to determine the optimal k value automatically, and at the same time, can retain the topology of the graph, the experiments show that our method has a certain degree of superiority.</p></list-item><list-item><p>In this paper, a large number of comparative and ablation experiments have been conducted to demonstrate the superiority of this paper&#x02019;s method in terms of modeling effect, clustering algorithm prediction accuracy, and clustering algorithm efficiency.</p></list-item></list></sec><sec sec-type="materials|methods" id="sec003"><title>Methods</title><p>In our work, thesis data is regarded as node information. A network linking the node information is constructed, and the network is used to save and express the relationship information between databases. Due to the diversity of the attributes of the relational information of thesis data, the general isomorphic network cannot save the information at the same time. We solve this problem by constructing a heterogeneous graph convolutional network.</p><sec id="sec004"><title>Definition 1 (Heterogeneous network)</title><p>A heterogeneous network refers to a graph <italic toggle="yes">G</italic> = (<italic toggle="yes">V</italic>, <italic toggle="yes">L</italic>, <italic toggle="yes">N</italic>, <italic toggle="yes">R</italic>) in which there is more than one class of nodes and more than one class of edges, and the sum of their classes is greater than 2. i.e., in a heterogeneous network, a heterogeneous graph is represented by a set of nodes <italic toggle="yes">V</italic> = (<italic toggle="yes">v</italic><sub>1</sub>, <italic toggle="yes">v</italic><sub>2</sub>&#x02026;&#x02026;<italic toggle="yes">v</italic><sub><italic toggle="yes">n</italic></sub>) and a set of edges <italic toggle="yes">L</italic> = (<italic toggle="yes">l</italic><sub>1</sub>, <italic toggle="yes">l</italic><sub>2</sub>&#x02026;&#x02026;<italic toggle="yes">l</italic><sub><italic toggle="yes">n</italic></sub>), where nodes <italic toggle="yes">v</italic> &#x02208; <italic toggle="yes">V</italic>, edges <italic toggle="yes">l</italic> &#x02208; <italic toggle="yes">L</italic>, and there are two mapping functions in a heterogeneous graph using &#x00393;<sub><italic toggle="yes">n</italic></sub> to denote the set of nodes, &#x00393;<sub><italic toggle="yes">i</italic></sub> to denote the set of edges, and each node corresponds to the node mapping function &#x003a6;<sub><italic toggle="yes">n</italic></sub> : <italic toggle="yes">L</italic> &#x02192; &#x00393;<sub><italic toggle="yes">n</italic></sub> and each edge corresponds to the edge mapping function &#x003a6;<sub><italic toggle="yes">j</italic></sub> : <italic toggle="yes">L</italic> &#x02192; &#x00393;<sub><italic toggle="yes">j</italic></sub>, i.e., |<italic toggle="yes">N</italic>| + |<italic toggle="yes">L</italic>| &#x02265; 2.</p></sec><sec id="sec005"><title>Definition 2 (Relational heterogeneous network)</title><p>In a heterogeneous network, each relation within the network has a weight, and for the relation <italic toggle="yes">m</italic> &#x02208; <italic toggle="yes">M</italic>, its value can be represented by |<italic toggle="yes">m</italic>|. In the Relational Heterogeneous Graph Convolutional Network (RA-HGCN) constructed in this paper, One type of node and three types of relations to construct this network are used, In the later part, we use RA-HGCN to represent our relational heterogeneous network. We take the published papers as the nodes of the heterogeneous network, and the three kinds of relations include Co-Reliable Author, Co-pub place and ReTitle, which will be explained in the following: Co-reliable Author Each published paper will have one Each published paper will have one or several authors, so when we want to disambiguate the author&#x02019;s name is n, the author may be related to more than one paper, in order to eliminate the ambiguity of the author&#x02019;s n, assume that two of the published papers <inline-formula id="pone.0310992.e001"><alternatives><graphic xlink:href="pone.0310992.e001.jpg" id="pone.0310992.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:msubsup><mml:mi>P</mml:mi><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> are <inline-formula id="pone.0310992.e002"><alternatives><graphic xlink:href="pone.0310992.e002.jpg" id="pone.0310992.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:msubsup><mml:mi>P</mml:mi><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> and, and the set of authors of these published papers are expressed as <italic toggle="yes">N</italic><sub>1</sub> and <italic toggle="yes">N</italic><sub>2</sub> respectively, so that <inline-formula id="pone.0310992.e003"><alternatives><graphic xlink:href="pone.0310992.e003.jpg" id="pone.0310992.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:msubsup><mml:mi>N</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> means that <italic toggle="yes">N</italic><sub>1</sub> does not include the set of authors n; if the intersection set of and is null <inline-formula id="pone.0310992.e004"><alternatives><graphic xlink:href="pone.0310992.e004.jpg" id="pone.0310992.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02229;</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02260;</mml:mo><mml:mi>&#x02300;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, then it means that in addition to the N, there are other co-authors between the two published papers, co-author weight is the weight of the co-authors. If the intersection of <inline-formula id="pone.0310992.e005"><alternatives><graphic xlink:href="pone.0310992.e005.jpg" id="pone.0310992.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:msubsup><mml:mi>N</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0310992.e006"><alternatives><graphic xlink:href="pone.0310992.e006.jpg" id="pone.0310992.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:msubsup><mml:mi>N</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> is empty then it means that there are other co-authors between these two published papers besides n. The weight of the co-authors is <inline-formula id="pone.0310992.e007"><alternatives><graphic xlink:href="pone.0310992.e007.jpg" id="pone.0310992.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02260;</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. In this paper, we set a threshold value for the weights, and those exceeding this threshold value are defined as reliable co-authors.</p><sec id="sec006"><title>Co-pub place</title><p>If two papers are published in the same conference or journal, it creates a co-pub place relationship between the two papers, generally the same paper will only be published in one journal or conference, so if there is a co-pub place relationship between the two papers then we define their value as 1.</p></sec><sec id="sec007"><title>ReTitle</title><p>A paper usually contains many keywords to summarise the main content of the paper, then for these keywords c, we first remove the stop words between, and then each keyword is converted to its stem, which forms a key phrase <italic toggle="yes">T</italic><sub><italic toggle="yes">p</italic></sub>, then for two papers <italic toggle="yes">A</italic><sub>1</sub> and <italic toggle="yes">A</italic><sub>2</sub>, if their key phrases have an intersection |<italic toggle="yes">T</italic><sub><italic toggle="yes">p</italic></sub>1 &#x02229; <italic toggle="yes">T</italic><sub><italic toggle="yes">p</italic></sub>2| &#x02260; &#x02300;, there is a Relevant Title relationship between them, with the weight of |<italic toggle="yes">T</italic><sub><italic toggle="yes">p</italic></sub>1 &#x02229; <italic toggle="yes">T</italic><sub><italic toggle="yes">p</italic></sub>2|.</p><p>In our work, we use these three common attributes to construct a heterogeneous graph convolutional network, in which two nodes are connected by multiple undirected relations, through which we can find the connecting path between two nodes. For example, in <xref rid="pone.0310992.g001" ref-type="fig">Fig 1</xref>, there is a path <inline-formula id="pone.0310992.e008"><alternatives><graphic xlink:href="pone.0310992.e008.jpg" id="pone.0310992.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mover accent="true"><mml:mo>&#x02192;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>-</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mover><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mover accent="true"><mml:mo>&#x02192;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>-</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mover><mml:msub><mml:mi>n</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> between nodes <italic toggle="yes">N</italic><sub>1</sub> and <italic toggle="yes">N</italic><sub>5</sub>, which can be interpreted as <italic toggle="yes">N</italic><sub>1</sub> and <italic toggle="yes">N</italic><sub>4</sub> between another paper by an author and <italic toggle="yes">N</italic><sub>5</sub> published in the same place.</p><fig position="float" id="pone.0310992.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.g001</object-id><label>Fig 1</label><caption><title>Random walk sampling methodology (in the figure C-R stands for co-reliable authors, C-P stands for co-publication sites and RT stands for relevant titles).</title></caption><graphic xlink:href="pone.0310992.g001" position="float"/></fig></sec></sec><sec id="sec008"><title>Definition 3 (Meta-path)</title><p>In our heterogeneous network G=(V,L,N,R), a meta-path can be denoted as <inline-formula id="pone.0310992.e009"><alternatives><graphic xlink:href="pone.0310992.e009.jpg" id="pone.0310992.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mover accent="true"><mml:mo>&#x02192;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mover><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mover accent="true"><mml:mo>&#x02192;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mover><mml:mover accent="true"><mml:mo>&#x02192;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:mover><mml:mo>&#x02026;</mml:mo><mml:mover accent="true"><mml:mo>&#x02192;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mover><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> where <italic toggle="yes">n</italic><sub>1</sub>, <italic toggle="yes">n</italic><sub>2</sub>, &#x02026;<italic toggle="yes">n</italic><sub><italic toggle="yes">m</italic>+1</sub> stands for the nodes and <italic toggle="yes">r</italic><sub>1</sub>, <italic toggle="yes">r</italic><sub>2</sub>, &#x02026;<italic toggle="yes">r</italic><sub><italic toggle="yes">m</italic>+1</sub> stands for the relationship between the nodes.</p></sec><sec id="sec009"><title>Introduction of 3 PROPOSED MODEL</title><p>In this section, we present our heterogeneous graph convolutional network embedding method, which encodes the content and relationships of the paper separately and then clusters them using a graph-based clustering algorithm.</p></sec><sec id="sec010"><title>Thesis network embedding section</title><p>In this section, we present the graph convolutional embedding module (RA-HNet), it gets the nodes by running on the neighborhood of a local graph, fuses semantics and relations between nodes using the fusion embedding module, we embed the fused features into the network layer, In the following part, we use RA-HNet to represent our graph volume product embedding module.</p><p>Our fusion embedding module is shown below in <xref rid="pone.0310992.g002" ref-type="fig">Fig 2</xref>, this module includes an Encoder and a Decoder and an error function in between, the core idea of the encoder is to use the encoder to encode the data into mean and variance vectors, train the vectors to reconstruct the error so that the distribution in the latent space is close to the standard normal distribution, and use the decoder to decode it back to the original space. The output vector after training is the spliced feature vector. Given a graph G=(V,L,R), where V denotes the set of nodes, L denotes the set of edges, and R denotes the set of relations. Each node in the graph represents a published paper with keywords, authors and other information. Firstly node initialisation is performed, for the nodes of the published paper <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub> &#x02208; <italic toggle="yes">V</italic>, we use Word2vec method to encode their information into a fixed length feature vector <inline-formula id="pone.0310992.e010"><alternatives><graphic xlink:href="pone.0310992.e010.jpg" id="pone.0310992.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>, As shown in the following <xref rid="pone.0310992.e011" ref-type="disp-formula">(1)</xref>:
<disp-formula id="pone.0310992.e011"><alternatives><graphic xlink:href="pone.0310992.e011.jpg" id="pone.0310992.e011g" position="anchor"/><mml:math id="M11" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>LeakyReLU</mml:mtext><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msubsup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mi>j</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mfrac><mml:msubsup><mml:mi>u</mml:mi><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:msubsup><mml:mi>w</mml:mi><mml:mi>r</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(1)</label></disp-formula>
In the formula <inline-formula id="pone.0310992.e012"><alternatives><graphic xlink:href="pone.0310992.e012.jpg" id="pone.0310992.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msubsup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is the hidden state of node <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub> in the layer <italic toggle="yes">t</italic> of the relational network, and <italic toggle="yes">m</italic><sup><italic toggle="yes">t</italic></sup> is the dimension represented by this layer. In our experiments, we set <italic toggle="yes">a</italic> = 0.001 and LeakyReLU(&#x022c5;) = max(0, &#x022c5;), <inline-formula id="pone.0310992.e013"><alternatives><graphic xlink:href="pone.0310992.e013.jpg" id="pone.0310992.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:msubsup><mml:mi>N</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> to represent the set of neighbours cues of node <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub> when the relation <italic toggle="yes">r</italic> &#x02208; <italic toggle="yes">R</italic> is in place, <inline-formula id="pone.0310992.e014"><alternatives><graphic xlink:href="pone.0310992.e014.jpg" id="pone.0310992.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:msubsup><mml:mi>k</mml:mi><mml:mi>j</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> represents the normalisation constant of the edge (<italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">p</italic><sub><italic toggle="yes">j</italic></sub>), while <italic toggle="yes">&#x003d5;</italic>(<italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">p</italic><sub><italic toggle="yes">j</italic></sub>) = <italic toggle="yes">r</italic> represents the type of the edge is r, and <inline-formula id="pone.0310992.e015"><alternatives><graphic xlink:href="pone.0310992.e015.jpg" id="pone.0310992.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:msubsup><mml:mi>K</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> represents the sum of the weights of the relations that are connected to <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub> and have a relation type of <inline-formula id="pone.0310992.e016"><alternatives><graphic xlink:href="pone.0310992.e016.jpg" id="pone.0310992.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>r</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. It is the relationship and layer specific trainable weight matrix that is used to ensure that the representation of a node can be mapped to the next layer. In order to ensure that each layer can have an effect on the nodes in the next layer, we assume that each node has a single connection to their neighbouring nodes with a weight of 1 to ensure that information can be passed between layers.</p><fig position="float" id="pone.0310992.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.g002</object-id><label>Fig 2</label><caption><title>Fusion embedded modules.</title></caption><graphic xlink:href="pone.0310992.g002" position="float"/></fig><p>Then we define L convolutional layers in the RA-HGCN model, in each convolutional layer, the output of the previous layer is the input of the next layer, <inline-formula id="pone.0310992.e017"><alternatives><graphic xlink:href="pone.0310992.e017.jpg" id="pone.0310992.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> represents the initial features of the input nodes of the first layer, and for each thesis node <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>, the RA-HGCN model embeds its textual information into <inline-formula id="pone.0310992.e018"><alternatives><graphic xlink:href="pone.0310992.e018.jpg" id="pone.0310992.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> and encodes it with its local neighbourhood on the graph G into <italic toggle="yes">u</italic><sub><italic toggle="yes">i</italic></sub>, As shown in the following <xref rid="pone.0310992.e019" ref-type="disp-formula">(2)</xref>:
<disp-formula id="pone.0310992.e019"><alternatives><graphic xlink:href="pone.0310992.e019.jpg" id="pone.0310992.e019g" position="anchor"/><mml:math id="M19" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>L</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003c6;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(2)</label></disp-formula>
<italic toggle="yes">&#x003c6;</italic> represents the parameters of RA-HGCN, in each convolutional layer of RA-HGCN, each node receives the information of its one-hop neighbours to update the representation, and the neighbour information of each node under the same relation type shares the same transformation parameters. When multiple convolutional layers are stacked, the one-hop neighbour information received by each node already includes the neighbour information of the previous convolutional layer, and through the L-layer, a node can receive information from at most L-hop distances of its multi-relationship neighbours.</p></sec><sec id="sec011"><title>Graph multiple attention mechanisms</title><p>Since the transformed information is sparse and not conducive to our distinction between authors with the same name, in order to enable our network to better aggregate information between neighboring nodes, combine the semantics and relationships between paper nodes, and improve the embedding ability of the nodes to efficiently transform high-dimensional data from published papers. In order to enable our network to better aggregate information between neighbouring nodes, combine the semantics and relationships between paper nodes, and improve the embedding ability of nodes to efficiently transform high-dimensional data in published papers. We adds the attention mechanism into the RA-HGCN model for information aggregation between publication nodes, as well as to learn the magnitude of the influence between each different neighbouring nodes. We firstly splice the first-order neighbor information of the current paper node and obtain the importance of the current neighbor node by transforming it as, As shown in the following <xref rid="pone.0310992.e020" ref-type="disp-formula">Eq (3)</xref>:
<disp-formula id="pone.0310992.e020"><alternatives><graphic xlink:href="pone.0310992.e020.jpg" id="pone.0310992.e020g" position="anchor"/><mml:math id="M20" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>&#x000b7;</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(3)</label></disp-formula>
Where, <italic toggle="yes">&#x003b2;</italic><sub><italic toggle="yes">ij</italic></sub> represents the degree of influence of the neighbour node <italic toggle="yes">j</italic> on the current node <italic toggle="yes">i</italic>, <italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub><italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub> &#x02208; <italic toggle="yes">wD</italic>. <italic toggle="yes">&#x003c3;</italic> is the activation function, <italic toggle="yes">a</italic> is the weight vector, and concat denotes the splice operation, using the splice operation can join the first and last of the two vectors to form a new vector, and use <xref rid="pone.0310992.e021" ref-type="disp-formula">Eq (4)</xref> to aggregate the weighted features between neighbours and update the node
<disp-formula id="pone.0310992.e021"><alternatives><graphic xlink:href="pone.0310992.e021.jpg" id="pone.0310992.e021g" position="anchor"/><mml:math id="M21" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="normal">&#x003a9;</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(4)</label></disp-formula>
Formula <italic toggle="yes">Omega</italic> denotes the neighbouring nodes in the convolutional graph that are directly connected to the node <italic toggle="yes">i</italic>. In addition, since a single graph attention head does not aggregate node information enough and may not be able to take into account the influence relation of the whole network, for this reason we added multiple attention heads to the network in order to enhance the learning of node information in the graph, As shown in the following <xref rid="pone.0310992.e022" ref-type="disp-formula">Eq (5)</xref>.
<disp-formula id="pone.0310992.e022"><alternatives><graphic xlink:href="pone.0310992.e022.jpg" id="pone.0310992.e022g" position="anchor"/><mml:math id="M22" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="normal">&#x003a9;</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(5)</label></disp-formula>
The k in the formula <italic toggle="yes">a</italic><sup><italic toggle="yes">k</italic></sup> represents the k attention head. In order to further condense the information of published papers, this paper designs a new fully-connected layer that uses the fully-connected layer to learn different weights as well as to classify co-reliable authors.
<disp-formula id="pone.0310992.e023"><alternatives><graphic xlink:href="pone.0310992.e023.jpg" id="pone.0310992.e023g" position="anchor"/><mml:math id="M23" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(6)</label></disp-formula>
Where <italic toggle="yes">W</italic><sub><italic toggle="yes">i</italic></sub> represents the parameters of the fully connected layer and <italic toggle="yes">b</italic><sub><italic toggle="yes">i</italic></sub> is the bias term of the connected layer. In simple terms, if a path in the convolutional network has a positive effect on the disambiguation, then the parameter value of this path will become larger, and finally the generating vector <italic toggle="yes">y</italic> is calculated by <xref rid="pone.0310992.e023" ref-type="disp-formula">Eq (6)</xref>, and the <italic toggle="yes">y</italic> vector contains the characteristics of the nodes in this path as well as the path information, etc., and finally the cross-entropy loss function is used to get the optimal embedding of the literature nodes through cross-entropy, and the loss function is shown in <xref rid="pone.0310992.e024" ref-type="disp-formula">Eq (7)</xref>:
<disp-formula id="pone.0310992.e024"><alternatives><graphic xlink:href="pone.0310992.e024.jpg" id="pone.0310992.e024g" position="anchor"/><mml:math id="M24" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(7)</label></disp-formula></p></sec><sec id="sec012"><title>Random sampling of association weights for meta-paths</title><p>The goal of this paper is to train a convolutional network (RA-HGCN) in such a way that the network is able to encode a high-quality representation of the nodes of each published paper. Inspired by the classical approaches of network embedding methods DeepWalk [<xref rid="pone.0310992.ref019" ref-type="bibr">19</xref>] and Metapath2Vec [<xref rid="pone.0310992.ref020" ref-type="bibr">20</xref>], which both employ a random wandering strategy and a skip-gram model, to learn and represent the nodes in the network. We propose an associative weight random sampling strategy for metapaths for sampling paths on weighted heterogeneous networks.</p><p>Two nodes <italic toggle="yes">n</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">n</italic><sub><italic toggle="yes">j</italic></sub> can be connected by multiple undirected relations between them, and a sequence of connected nodes based on these relations can be seen as a path to the between <italic toggle="yes">n</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">n</italic><sub><italic toggle="yes">j</italic></sub>. A path <italic toggle="yes">n</italic><sub>1</sub> &#x02192; <italic toggle="yes">n</italic><sub>4</sub> &#x02192; <italic toggle="yes">n</italic><sub>5</sub> exists between <italic toggle="yes">n</italic><sub>1</sub> and <italic toggle="yes">n</italic><sub>5</sub> as shown in <xref rid="pone.0310992.g003" ref-type="fig">Fig 3</xref> as an example of partial random sampling in a network. The correlation of nodes in heterogeneous relationships can be captured using our method, while in sampling the paths we consider the relationship weights in the network. Intuitively it means that the higher the relationship value between two nodes, the higher the similarity; and in each step, as the executor moves towards its neighbours, the higher the relationship value between the current node and the neighbouring node, and the more likely it is to be sampled.</p><fig position="float" id="pone.0310992.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.g003</object-id><label>Fig 3</label><caption><title>Example of random sampling.</title></caption><graphic xlink:href="pone.0310992.g003" position="float"/></fig><p>As an example, given G=(V,L,R), the path consists of <inline-formula id="pone.0310992.e025"><alternatives><graphic xlink:href="pone.0310992.e025.jpg" id="pone.0310992.e025g" position="anchor"/><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mover accent="true"><mml:mo>&#x02192;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mover><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mover accent="true"><mml:mo>&#x02192;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mover><mml:mo>&#x02026;</mml:mo><mml:mover accent="true"><mml:mo>&#x02192;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mover><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <italic toggle="yes">p</italic><sup><italic toggle="yes">t</italic></sup> &#x02208; <italic toggle="yes">V</italic>, <italic toggle="yes">r</italic><sup><italic toggle="yes">i</italic></sup> &#x02208; <italic toggle="yes">R</italic>, represents the length of the relationship between <italic toggle="yes">p</italic><sup><italic toggle="yes">t</italic></sup>, <italic toggle="yes">p</italic><sup><italic toggle="yes">t</italic>+<italic toggle="yes">m</italic>+1</sup>, the probability of transfer at step length t, As shown in the following <xref rid="pone.0310992.e026" ref-type="disp-formula">Eq (8)</xref>:
<disp-formula id="pone.0310992.e026"><alternatives><graphic xlink:href="pone.0310992.e026.jpg" id="pone.0310992.e026g" position="anchor"/><mml:math id="M26" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold">P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02260;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02209;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(8)</label></disp-formula>
where <italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic></sub> &#x02208; <italic toggle="yes">R</italic>, is the next relation of <italic toggle="yes">&#x003d5;</italic>(<italic toggle="yes">p</italic><sup><italic toggle="yes">t</italic>&#x02212;1</sup>, <italic toggle="yes">p</italic><sup><italic toggle="yes">t</italic></sup>), and if it is the first node, then <italic toggle="yes">i</italic> = 1, |<italic toggle="yes">p</italic><sup><italic toggle="yes">t</italic></sup>, <italic toggle="yes">p</italic><sup><italic toggle="yes">t</italic>+1</sup>| denotes the weight of the relation between the two, and <italic toggle="yes">L</italic><sup><italic toggle="yes">t</italic></sup>(<italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic></sub>) denotes the set of relations between <italic toggle="yes">p</italic><sup><italic toggle="yes">t</italic></sup>, <italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic></sub>. Specifically, we first select a node in the RA-HGCN as the start node in the path and generate a meta-path of length l, and then use the last node in the network as the start node of another path. In this way, each random walk recursively samples nodes in the network and finally generates a long path of fixed length guided by p. Compared to the random walk strategy, this strategy is able to effectively avoid the influence of bias caused by the central node and the bias of the number of different relations. In addition to this the strategy in this paper also considers the possibility of diverse permutations between paths, as well as the multiple relationships between individual nodes in the path and the weights between individual relationships, which makes the strategy in this paper able to retain the heterogeneous relationships between nodes in the generated paths very well.</p></sec><sec id="sec013"><title>A weighted heterogeneous jump graph model for semantics and relations</title><p>For the paths generated in the previous section, in this paper we use a weighted heterogeneous jump model (RAHG-skip gram) to learn published papers in the network. Given a G=(V,L,R), our goal is to efficiently learn node representations by maximising the probability that any node <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub> has its neighbour <italic toggle="yes">p</italic><sub><italic toggle="yes">c</italic></sub>, As shown in the following <xref rid="pone.0310992.e027" ref-type="disp-formula">Eq (9)</xref>:
<disp-formula id="pone.0310992.e027"><alternatives><graphic xlink:href="pone.0310992.e027.jpg" id="pone.0310992.e027g" position="anchor"/><mml:math id="M27" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>arg</mml:mtext><mml:mspace width="2pt"/><mml:munder><mml:mtext>max</mml:mtext><mml:mi>&#x003b8;</mml:mi></mml:munder><mml:mspace width="2pt"/><mml:mo>&#x02211;</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:munder><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mspace width="2pt"/><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(9)</label></disp-formula>
Where <italic toggle="yes">N</italic><sub><italic toggle="yes">r</italic></sub>(<italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>) denotes the set of neighbours of <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>, &#x02200;<italic toggle="yes">p</italic><sub><italic toggle="yes">c</italic></sub> &#x02208; <italic toggle="yes">N</italic><sub><italic toggle="yes">r</italic></sub>(<italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>), (<italic toggle="yes">p</italic><sub><italic toggle="yes">c</italic></sub>, <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>) &#x02208; <italic toggle="yes">r</italic>, <italic toggle="yes">&#x003b8;</italic> denotes the RA-HGCN model of this paper, and the relation weight of the function |<italic toggle="yes">p</italic><sub><italic toggle="yes">c</italic></sub>, <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>| represents a weight which ensures that neighbours with correlation with have higher values as well as higher output probabilities, while our softmax function is defined As shown in the following <xref rid="pone.0310992.e028" ref-type="disp-formula">Eq (10)</xref>:
<disp-formula id="pone.0310992.e028"><alternatives><graphic xlink:href="pone.0310992.e028.jpg" id="pone.0310992.e028g" position="anchor"/><mml:math id="M28" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext><mml:mspace width="2pt"/><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup><mml:mspace width="2pt"/><mml:mtext>exp</mml:mtext><mml:mspace width="2pt"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(10)</label></disp-formula>
Where <italic toggle="yes">u</italic><sub><italic toggle="yes">i</italic></sub> represents the embedded network vector after the initial features of <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub> are encoded by RA-HGCN in <xref rid="pone.0310992.e019" ref-type="disp-formula">Eq (2)</xref>, and then this paper adopts the negative sampling method which is popular in current research to improve the efficiency of optimisation, and the probability can be approximated and defined as in this paper&#x02019;s treatment, As shown in the following <xref rid="pone.0310992.e029" ref-type="disp-formula">Eq (11)</xref>:
<disp-formula id="pone.0310992.e029"><alternatives><graphic xlink:href="pone.0310992.e029.jpg" id="pone.0310992.e029g" position="anchor"/><mml:math id="M29" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02248;</mml:mo><mml:mspace width="2pt"/><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mspace width="2pt"/><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(11)</label></disp-formula>
where <inline-formula id="pone.0310992.e030"><alternatives><graphic xlink:href="pone.0310992.e030.jpg" id="pone.0310992.e030g" position="anchor"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> represents the sigmoid function and <inline-formula id="pone.0310992.e031"><alternatives><graphic xlink:href="pone.0310992.e031.jpg" id="pone.0310992.e031g" position="anchor"/><mml:math id="M31" display="inline" overflow="scroll"><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> represents the set of negative samples in the path calculated by probability.</p><p>In the method of this paper, by selecting different initial nodes, a set of random path sets can be obtained, which is denoted by RW in this paper. And the frequency of their appearances is proportional to the frequency of their appearances in the context of the target node and the degree of relationship. Based on this situation, the maximisation of the objective can be approximated as equal to the minimisation of the loss function, As shown in the following <xref rid="pone.0310992.e032" ref-type="disp-formula">Eq (12)</xref>:
<disp-formula id="pone.0310992.e032"><alternatives><graphic xlink:href="pone.0310992.e032.jpg" id="pone.0310992.e032g" position="anchor"/><mml:math id="M32" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>R</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>&#x02211;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:munder><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mspace width="2pt"/><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">&#x003bb;</mml:mi><mml:msup><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x02016;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(12)</label></disp-formula>
Where <italic toggle="yes">k</italic> represents the size of the context association, <inline-formula id="pone.0310992.e033"><alternatives><graphic xlink:href="pone.0310992.e033.jpg" id="pone.0310992.e033g" position="anchor"/><mml:math id="M33" display="inline" overflow="scroll"><mml:msubsup><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is the set of <italic toggle="yes">k</italic> context nodes before and after in the path set. In a random traversal of the set, different sets of association sizes represent different neighbouring nodes. &#x003bb; Parameters control overfitting. The general framework of the RA-HGCN network model in this paper is shown in <xref rid="pone.0310992.g004" ref-type="fig">Fig 4</xref>, and the model mainly differs from the classical network embedding methods in the following ways: (1) The model in this paper can preserve various heterogeneous relationships between nodes as well as relationship weights. (2) Using the weighted heterogeneous model in combination with the network, it is able to fuse the semantic and relational features of the published papers, and then embedded into the network afterwards, which is able to obtain better disambiguation effect.</p><fig position="float" id="pone.0310992.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.g004</object-id><label>Fig 4</label><caption><title>RA-HGCN network model.</title></caption><graphic xlink:href="pone.0310992.g004" position="float"/></fig></sec><sec id="sec014"><title>Graph-based hierarchical clustering</title><p>Through the investigation, this paper found that the published papers have a few authors published a large number of papers of the situation, and the hierarchical clustering (HAC) is applied in many references, but there are also the following shortcomings: HAC needs to pre-set the number of K clusters, and thus the algorithm&#x02019;s time complexity is high, and how to determine the value of K is also a difficult problem, therefore, we improved the original hierarchical clustering (RHAC) Therefore, we improved the original hierarchical clustering (RHAC) by introducing the topology of the graph and the relationship between the graph nodes to reduce the computational complexity in hierarchical clustering and improve the efficiency.</p><p>For a graph G=(V,L,R), the weights of <italic toggle="yes">l</italic><sub><italic toggle="yes">ij</italic></sub> &#x02208; <italic toggle="yes">L</italic> of the edges between node and node <italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub> are denoted as, As shown in the following <xref rid="pone.0310992.e034" ref-type="disp-formula">Eq (13)</xref>:
<disp-formula id="pone.0310992.e034"><alternatives><graphic xlink:href="pone.0310992.e034.jpg" id="pone.0310992.e034g" position="anchor"/><mml:math id="M34" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(13)</label></disp-formula>
Where <italic toggle="yes">&#x003c3;</italic>(&#x022c5;) is the function with the range (0, 1) and <italic toggle="yes">&#x003b4;</italic>(<italic toggle="yes">a</italic>) is a truth value function which is 1 when a is true and 0 otherwise.In RHAC, we first consider each sample as a separate cluster and then use the idea of hierarchical clustering to merge the two clusters which have the largest similarity at each step until they are merged to the specified K-value, and when the K-value is not determinable, we use the optimal module partitioning mechanism to determine it automatically partition of the paper, the node partitioning module is as follows, As shown in the following <xref rid="pone.0310992.e035" ref-type="disp-formula">Eq (14)</xref>:
<disp-formula id="pone.0310992.e035"><alternatives><graphic xlink:href="pone.0310992.e035.jpg" id="pone.0310992.e035g" position="anchor"/><mml:math id="M35" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(14)</label></disp-formula>
Where <italic toggle="yes">m</italic> represents the sum of edge weights in the graph G, <italic toggle="yes">w</italic><sub><italic toggle="yes">i</italic></sub> represents the sum of weights of all edges of the node <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>, and <italic toggle="yes">b</italic><sub><italic toggle="yes">i</italic></sub> represents the cluster <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub> where is located; In brief, in our RHAC, after each merging process, the current module division degree <italic toggle="yes">M</italic>, is calculated until the final calculation is completed, and the largest <italic toggle="yes">M</italic> value is selected as the K value, after which the clustering result is calculated. In RHAC, the use of vector operations instead of the original distance calculation improves the efficiency and accuracy of clustering, and we have conducted sufficient comparative experiments in our experiments.</p></sec></sec><sec id="sec015"><title>Experimental results</title><p>In this section, this paper demonstrates the effectiveness of the newly proposed framework in the task of author name disambiguation, and we conduct experiments using the Aminer-v1, Aminer-v2, and WhoISWho datasets, and the results show that the framework approach in our work has a significant advantage over the other classical approaches.</p><sec id="sec016"><title>Dataset</title><p>In order to test the method proposed in this paper, we used the three Aminer [<xref rid="pone.0310992.ref001" ref-type="bibr">1</xref>] dataset datasets mentioned above. The Aminer dataset is widely used in the field of author name disambiguation, and the names of the people in the dataset are fully labeled with information about the titles, abstracts, authors, and institutions of the literature, including 8505 published papers, 1703 authors, 1977 conferences (journals) and 110 fuzzy names, etc. AMiner-v2 includes 70285 published papers, 12798 authors, and 100 fuzzy names. whoIsWho includes 399255 published papers, 45187 authors, and 421 fuzzy names, and the following <xref rid="pone.0310992.g005" ref-type="fig">Fig 5</xref> shows an example of the data.</p><fig position="float" id="pone.0310992.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.g005</object-id><label>Fig 5</label><caption><title>Examples of data sets.</title></caption><graphic xlink:href="pone.0310992.g005" position="float"/></fig></sec><sec id="sec017"><title>Baseline methodology</title><p>In order to test the effectiveness of the RA-HGCN proposed in this paper for the task of author name disambiguation, the following classical methods are used in this paper to compare with our method. These methods are briefly explained:</p><p>RA-HGCN: Our approach considers the relations and semantics of published papers and constructs a heterogeneous graph convolutional neural network based on relations and semantics, and finally clusters using RHAC to obtain results.</p><p>Node2Vec [<xref rid="pone.0310992.ref021" ref-type="bibr">21</xref>]: This approach samples the sequence of neighbouring nodes of each node through a random wandering algorithm, and then processes the node sequence using a Word2Vec-like approach for the final network node representation.</p><p>Struct2Vec [<xref rid="pone.0310992.ref022" ref-type="bibr">22</xref>]: This approach considers the degree of structural similarity of two nodes in the relationship graph, even if these two nodes are far apart, if the structures are more similar, their similarity is correspondingly higher.</p><p>GHOST [<xref rid="pone.0310992.ref023" ref-type="bibr">23</xref>]: This approach constructs a collaborative network graph for each author name with ambiguity and calculates the similarity using a path-based approach, and then uses affinity propagation clustering to obtain the disambiguation results. The proposed affinity propagation algorithm is a clustering algorithm that does not require the number of clusters to be specified similar to the clustering in this paper.</p><p>Zhang et al [<xref rid="pone.0310992.ref003" ref-type="bibr">3</xref>]: This approach is based on automatic coding of graphs and embedding of publication paper nodes with local connections learnt globally with metrics, after which an end-to-end model is constructed and trained using neural networks, and finally hierarchical clustering is used to determine the allocation of publications.</p><p>Xu et al [<xref rid="pone.0310992.ref024" ref-type="bibr">24</xref>]: This method mainly deals with the part of authors with ambiguous names, builds multiple publishing networks for multiple relations respectively, after that uses a merging strategy for coarsening on each network separately, learns and embeds them by employing a newly proposed network embedding method, and finally determines the final clustering results using the (Density Clustering) and (Graph Based Clustering) algorithms.</p><p>Zhang et al [<xref rid="pone.0310992.ref018" ref-type="bibr">18</xref>]: In order to facilitate the processing of the relationships between the data, this method constructs three independent networks for each published paper with dichotomies, the processed relationships are learnt using network embedding, and finally hierarchical clustering is used to determine the final result.</p><p>DeepWalk [<xref rid="pone.0310992.ref025" ref-type="bibr">25</xref>]: This method is an approach to learn potential relationships between nodes by using a random walk strategy and use it for network embedding, this method is mainly applied in homogeneous unweighted networks.</p><p>LINE [<xref rid="pone.0310992.ref026" ref-type="bibr">26</xref>]: This method is widely used in isomorphic weighted networks and is able to keep the first-order and second-order information of the nodes similar.</p><p>SDNE [<xref rid="pone.0310992.ref027" ref-type="bibr">27</xref>]: This method can be viewed as an extension of the LINE method, but unlike the LINE algorithm, SDNE optimises both the first-order and second-order similarities, where the first-order similarity is the similarity between vertices directly adjacent to each other, and the second-order similarity is the similarity between the neighbouring nodes of the vertices, and the node embeddings obtained from the neighbouring nodes are close to each other in the space by optimising the two similarities at the same time.</p><p>Metapath2Vec [<xref rid="pone.0310992.ref028" ref-type="bibr">28</xref>]: This approach is widely used in heterogeneous networks in the presence of binary edges, where a neighbourhood is constructed for the current node by using random wandering of metapaths, followed by embedding using a heterogeneous jump model.</p><p>Hin2Vec [<xref rid="pone.0310992.ref020" ref-type="bibr">20</xref>]: This method is also an unweighted heterogeneous network embedding method that preserves the semantics of the relationships between nodes and the details of the network structure, and in this way enhances the learning between the nodes.</p><p>GraphSAGE [<xref rid="pone.0310992.ref019" ref-type="bibr">19</xref>]: This method performs node embedding by using different aggregation functions to form node neighbourhoods, but this method does not use unsupervised loss functions and thus can only be applied with isomorphic networks.</p><p>Qiao [<xref rid="pone.0310992.ref008" ref-type="bibr">8</xref>]: This approach is done by constructing a heterogeneous graph convolutional network and using a clustering algorithm for disambiguation, but this approach does not take into account the treatment of relationships between nodes.</p></sec><sec id="sec018"><title>Experimental setup</title><p>In our experiments, we only used three features of the published paper as the presentation, and thus may have been less effective than the presentation of the original paper. in order to facilitate the comparison of experiments, We set the number of clusters of clustering set to a real number, and use the accuracy (Pairwise Precision), recall (Pairwise Recall) and <italic toggle="yes">F</italic><sub>1</sub> &#x02212; <italic toggle="yes">Score</italic> [<xref rid="pone.0310992.ref011" ref-type="bibr">11</xref>] as the main evaluation metrics for the effectiveness of our experiments As well as the baseline methodology experiments, we also computed the <italic toggle="yes">Macro</italic><sub>1</sub> &#x02212; <italic toggle="yes">Score</italic> (representing the average <italic toggle="yes">F</italic><sub>1</sub> value of all ambiguous names) metric for the dataset for comparison, As shown in the following Eqs <xref rid="pone.0310992.e036" ref-type="disp-formula">(15)</xref>, <xref rid="pone.0310992.e037" ref-type="disp-formula">(16)</xref> and <xref rid="pone.0310992.e038" ref-type="disp-formula">(17)</xref>.
<disp-formula id="pone.0310992.e036"><alternatives><graphic xlink:href="pone.0310992.e036.jpg" id="pone.0310992.e036g" position="anchor"/><mml:math id="M36" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>precision</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(15)</label></disp-formula>
<disp-formula id="pone.0310992.e037"><alternatives><graphic xlink:href="pone.0310992.e037.jpg" id="pone.0310992.e037g" position="anchor"/><mml:math id="M37" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Re</mml:mtext><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(16)</label></disp-formula>
<disp-formula id="pone.0310992.e038"><alternatives><graphic xlink:href="pone.0310992.e038.jpg" id="pone.0310992.e038g" position="anchor"/><mml:math id="M38" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(17)</label></disp-formula>
Where <italic toggle="yes">TP</italic> denotes the number of papers predicted to be by the same author and actually by the same author, <italic toggle="yes">FP</italic> denotes the number of papers predicted to be by the same author but actually not by the same author. <italic toggle="yes">FN</italic> denotes the number of papers predicted to be by different authors but actually by the same author. In calculating the value of <italic toggle="yes">F</italic><sub>1</sub>, we choose the harmonic mean of precision and recall, and the value ranges between (0, 1), with higher values indicating a more effective model.</p><p>We use one RTX A5000 GPU to train the model and use the pre-trained wod2vec model to handle the embedding of thesis node relationships, in terms of parameters we use one RTX A5000 GPU to train the model and use the pre-trained wod2vec model to handle the embedding of thesis node relationships, in terms of parameters we set the initial learning rate to, the regularization factor, linked between above, because of the difference in dataset sizes, by experimenting with the embedding dimensions we chose 128 embedding dimensions on the V1 dataset, and 256 dimensions as well as 512 dimensions on V2 as well as WHOISWHO datasets, respectively., we set the initial learning rate to, the regularization coefficient, the link between the above because of the difference in dataset sizes, by experimenting with the embedding dimensions we choose an embedding dimension of 128 on the V1 dataset, and 256 dimensions as well as 512 dimensions on the V2 as well as WHOISWHO datasets, respectively.</p></sec><sec id="sec019"><title>Results</title><p>
<xref rid="pone.0310992.t001" ref-type="table">Table 1</xref> shows the average of the results for each metric, which is obtained by averaging all the different disambiguation names by summing them up, and it can be seen from the table that the method in this paper is basically superior to the other methods, e.g., it outperforms the other baseline methods on the Avg Pre metrics by 1.42 to 31.28 (SDNE+31.28, Struct2Vec+6.95, Aminer+5.08, Node2Vec+12.38, Zhang et al+12.41, GHOST+1.42, and GHOST+1.42). 5.08, Node2Vec+12.38, Zhang et al+12.41, GHOST+1.42). In addition to the metrics comparison of several classical methods, we also performed experimental comparisons of several other baseline methods mentioned, the results of which are shown in <xref rid="pone.0310992.t002" ref-type="table">Table 2</xref> below. Our method achieved the best results in both Pre metrics and F1 metrics, Line uses a homogeneous network that takes into account first-order and second-order relationships, PHNet uses a heterogeneous network for inter-local learning, BOND&#x02019;s method works by combining semantic features with heterogeneous graphs, and our method takes into account the relationship between semantics and relations. And the combination of the two greatly improves the expressiveness of the features, which makes our method achieve better results in all three types of metrics. In the metrics of Rec, our method also achieved the second best result, while the first one, Xu&#x02019;s method, was able to achieve better recall more because it mainly focuses on the gap between positive and negative edges and learns the global graph attributes in a coarsened network, about our method achieved a high recall as well as F1 score, but the accuracy is relatively low, we analyze that it may be due to the fact that We capture the complex and diverse relationships between authors by using a heterogeneous graph convolutional attention network, which contributes to the recall rate. Meanwhile, as the multi-head attention mechanism can improve the model&#x02019;s ability to learn to understand complex graph structures, which helps the F1 score. And regarding the relatively low accuracy rate may be due to the fact that there is a category imbalance among the unused authors, i.e., some authors have more samples while others have fewer samples.</p><table-wrap position="float" id="pone.0310992.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.t001</object-id><label>Table 1</label><caption><title>Comparison of the methodology of this paper with several classical methods for the three types of indicators.</title></caption><alternatives><graphic xlink:href="pone.0310992.t001" id="pone.0310992.t001g" position="float"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" style="border-bottom-width:thick;border-right-width:thick" rowspan="1" colspan="1">Method</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Our Method</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">SDNE</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Struct2Vec</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Aminer</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Node2Vec</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Zhang et al</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">GHOST</th></tr></thead><tbody><tr><td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Pre</td><td align="char" char="." rowspan="1" colspan="1">
<bold>83.04</bold>
</td><td align="char" char="." rowspan="1" colspan="1">51.76</td><td align="char" char="." rowspan="1" colspan="1">76.09</td><td align="char" char="." rowspan="1" colspan="1">77.96</td><td align="char" char="." rowspan="1" colspan="1">70.66</td><td align="char" char="." rowspan="1" colspan="1">70.63</td><td align="char" char="." rowspan="1" colspan="1">81.62</td></tr><tr><td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Rec</td><td align="char" char="." rowspan="1" colspan="1">81.43</td><td align="char" char="." rowspan="1" colspan="1">
<bold>90.74</bold>
</td><td align="char" char="." rowspan="1" colspan="1">30.33</td><td align="char" char="." rowspan="1" colspan="1">63.03</td><td align="char" char="." rowspan="1" colspan="1">65.39</td><td align="char" char="." rowspan="1" colspan="1">59.53</td><td align="char" char="." rowspan="1" colspan="1">40.43</td></tr><tr><td align="left" style="border-right-width:thick" rowspan="1" colspan="1">F1</td><td align="char" char="." rowspan="1" colspan="1">
<bold>81.34</bold>
</td><td align="char" char="." rowspan="1" colspan="1">59.95</td><td align="char" char="." rowspan="1" colspan="1">39.76</td><td align="char" char="." rowspan="1" colspan="1">69.70</td><td align="char" char="." rowspan="1" colspan="1">61.93</td><td align="char" char="." rowspan="1" colspan="1">64.61</td><td align="char" char="." rowspan="1" colspan="1">54.07</td></tr></tbody></table></alternatives></table-wrap><table-wrap position="float" id="pone.0310992.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.t002</object-id><label>Table 2</label><caption><title>Comparison of this paper&#x02019;s method with ten baseline methods on the Aminer dataset for three types of metrics.</title></caption><alternatives><graphic xlink:href="pone.0310992.t002" id="pone.0310992.t002g" position="float"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" style="border-bottom-width:thick;border-right-width:thick" rowspan="1" colspan="1">Name</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Our Method</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">PHNet</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">BOND</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Xu</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Hin2Vec</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Component</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">DeepWalk</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">LINE</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Metapth2Vec</th><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">GraphSAGE</th></tr></thead><tbody><tr><td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Pre</td><td align="char" char="." rowspan="1" colspan="1">
<bold>83.04</bold>
</td><td align="char" char="." rowspan="1" colspan="1">65.91</td><td align="char" char="." rowspan="1" colspan="1">70.21</td><td align="char" char="." rowspan="1" colspan="1">74.5</td><td align="char" char="." rowspan="1" colspan="1">61.6</td><td align="char" char="." rowspan="1" colspan="1">58.2</td><td align="char" char="." rowspan="1" colspan="1">77.8</td><td align="char" char="." rowspan="1" colspan="1">73.7</td><td align="char" char="." rowspan="1" colspan="1">63.0</td><td align="char" char="." rowspan="1" colspan="1">79.03</td></tr><tr><td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Rec</td><td align="char" char="." rowspan="1" colspan="1">81.43</td><td align="char" char="." rowspan="1" colspan="1">68.32</td><td align="char" char="." rowspan="1" colspan="1">72.78</td><td align="char" char="." rowspan="1" colspan="1">
<bold>83.2</bold>
</td><td align="char" char="." rowspan="1" colspan="1">74.3</td><td align="char" char="." rowspan="1" colspan="1">73.4</td><td align="char" char="." rowspan="1" colspan="1">69.5</td><td align="char" char="." rowspan="1" colspan="1">59.7</td><td align="char" char="." rowspan="1" colspan="1">60.3</td><td align="char" char="." rowspan="1" colspan="1">70.19</td></tr><tr><td align="left" style="border-right-width:thick" rowspan="1" colspan="1">F1</td><td align="char" char="." rowspan="1" colspan="1">
<bold>81.34</bold>
</td><td align="char" char="." rowspan="1" colspan="1">67.09</td><td align="char" char="." rowspan="1" colspan="1">71.4</td><td align="char" char="." rowspan="1" colspan="1">63.5</td><td align="char" char="." rowspan="1" colspan="1">56.2</td><td align="char" char="." rowspan="1" colspan="1">48.2</td><td align="char" char="." rowspan="1" colspan="1">73.4</td><td align="char" char="." rowspan="1" colspan="1">66.1</td><td align="char" char="." rowspan="1" colspan="1">61.6</td><td align="char" char="." rowspan="1" colspan="1">74.34</td></tr></tbody></table></alternatives></table-wrap><p>In order to more visually demonstrate the superiority of the methodology of this paper and the effectiveness of the comparative methodology on specific names, for individual names, these names are often listed as classic examples for citation illustration in previous studies. we give the name disambiguation results of our model and several other baseline models for some specific authors on the Aminer dataset in <xref rid="pone.0310992.t003" ref-type="table">Table 3</xref> below. In the <xref rid="pone.0310992.t003" ref-type="table">Table 3</xref>, we have selected ten of the names and computed the average scores; for example, Bin Yu belongs to six different authors in the dataset, and five of them have published only one paper; similarly, the names of each of them in the figure represent multiple different authors, and a portion of the authors (about 46%) have published only one paper in their entire study career, and similarly, there are there is a proportion of authors who have published a large number of papers during their study career. The table below shows the <italic toggle="yes">F</italic><sub>1</sub> scores for the names shown, with the last row (Avg) representing the scores for all the names on the macro level, being the average of the scores for all the different names.</p><table-wrap position="float" id="pone.0310992.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.t003</object-id><label>Table 3</label><caption><title>Comparison of the effectiveness of this paper&#x02019;s method with ten baseline methods for disambiguation on the Aminer dataset.</title></caption><alternatives><graphic xlink:href="pone.0310992.t003" id="pone.0310992.t003g" position="float"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Name</th><th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Our Method</th><th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Qiao</th><th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Zhang</th><th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Zhang</th><th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Xu</th><th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Hin2Vec</th><th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Component</th><th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">DeepWalk</th><th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">LINE</th><th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Metapth2Vec</th><th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">GraphSAGE</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Ajay Gupta</td><td align="char" char="." rowspan="1" colspan="1">0.697</td><td align="char" char="." rowspan="1" colspan="1">
<bold>0.750</bold>
</td><td align="char" char="." rowspan="1" colspan="1">0.618</td><td align="char" char="." rowspan="1" colspan="1">0.568</td><td align="char" char="." rowspan="1" colspan="1">0.552</td><td align="char" char="." rowspan="1" colspan="1">0.684</td><td align="char" char="." rowspan="1" colspan="1">0.329</td><td align="char" char="." rowspan="1" colspan="1">0.370</td><td align="char" char="." rowspan="1" colspan="1">0.578</td><td align="char" char="." rowspan="1" colspan="1">0.298</td><td align="char" char="." rowspan="1" colspan="1">0.654</td></tr><tr><td align="left" rowspan="1" colspan="1">Alok Gupta</td><td align="char" char="." rowspan="1" colspan="1">
<bold>1.000</bold>
</td><td align="char" char="." rowspan="1" colspan="1">0.942</td><td align="char" char="." rowspan="1" colspan="1">0.590</td><td align="char" char="." rowspan="1" colspan="1">0.689</td><td align="char" char="." rowspan="1" colspan="1">0.892</td><td align="char" char="." rowspan="1" colspan="1">0.734</td><td align="char" char="." rowspan="1" colspan="1">0.690</td><td align="char" char="." rowspan="1" colspan="1">0.582</td><td align="char" char="." rowspan="1" colspan="1">0.835</td><td align="char" char="." rowspan="1" colspan="1">0.663</td><td align="char" char="." rowspan="1" colspan="1">0.651</td></tr><tr><td align="left" rowspan="1" colspan="1">Bin Yu</td><td align="char" char="." rowspan="1" colspan="1">
<bold>0.987</bold>
</td><td align="char" char="." rowspan="1" colspan="1">0.696</td><td align="char" char="." rowspan="1" colspan="1">0.614</td><td align="char" char="." rowspan="1" colspan="1">0.431</td><td align="char" char="." rowspan="1" colspan="1">0.585</td><td align="char" char="." rowspan="1" colspan="1">0.490</td><td align="char" char="." rowspan="1" colspan="1">0.292</td><td align="char" char="." rowspan="1" colspan="1">0.490</td><td align="char" char="." rowspan="1" colspan="1">0.475</td><td align="char" char="." rowspan="1" colspan="1">0.354</td><td align="char" char="." rowspan="1" colspan="1">0.441</td></tr><tr><td align="left" rowspan="1" colspan="1">David Cooper</td><td align="char" char="." rowspan="1" colspan="1">0.900</td><td align="char" char="." rowspan="1" colspan="1">0.900</td><td align="char" char="." rowspan="1" colspan="1">
<bold>0.931</bold>
</td><td align="char" char="." rowspan="1" colspan="1">0.737</td><td align="char" char="." rowspan="1" colspan="1">0.884</td><td align="char" char="." rowspan="1" colspan="1">0.931</td><td align="char" char="." rowspan="1" colspan="1">0.327</td><td align="char" char="." rowspan="1" colspan="1">0.737</td><td align="char" char="." rowspan="1" colspan="1">0.833</td><td align="char" char="." rowspan="1" colspan="1">0.833</td><td align="char" char="." rowspan="1" colspan="1">0.862</td></tr><tr><td align="left" rowspan="1" colspan="1">David Nelson</td><td align="char" char="." rowspan="1" colspan="1">
<bold>0.944</bold>
</td><td align="char" char="." rowspan="1" colspan="1">
<bold>0.944</bold>
</td><td align="char" char="." rowspan="1" colspan="1">0.556</td><td align="char" char="." rowspan="1" colspan="1">0.750</td><td align="char" char="." rowspan="1" colspan="1">0.735</td><td align="char" char="." rowspan="1" colspan="1">0.635</td><td align="char" char="." rowspan="1" colspan="1">0.219</td><td align="char" char="." rowspan="1" colspan="1">0.353</td><td align="char" char="." rowspan="1" colspan="1">0.523</td><td align="char" char="." rowspan="1" colspan="1">0.788</td><td align="char" char="." rowspan="1" colspan="1">0.710</td></tr><tr><td align="left" rowspan="1" colspan="1">Fei Su</td><td align="char" char="." rowspan="1" colspan="1">
<bold>1.000</bold>
</td><td align="char" char="." rowspan="1" colspan="1">
<bold>1.000</bold>
</td><td align="char" char="." rowspan="1" colspan="1">0.941</td><td align="char" char="." rowspan="1" colspan="1">0.933</td><td align="char" char="." rowspan="1" colspan="1">0.630</td><td align="char" char="." rowspan="1" colspan="1">0.917</td><td align="char" char="." rowspan="1" colspan="1">0.648</td><td align="char" char="." rowspan="1" colspan="1">0.684</td><td align="char" char="." rowspan="1" colspan="1">0.721</td><td align="char" char="." rowspan="1" colspan="1">0.930</td><td align="char" char="." rowspan="1" colspan="1">0.948</td></tr><tr><td align="left" rowspan="1" colspan="1">Hao Wang</td><td align="char" char="." rowspan="1" colspan="1">0.687</td><td align="char" char="." rowspan="1" colspan="1">0.604</td><td align="char" char="." rowspan="1" colspan="1">0.543</td><td align="char" char="." rowspan="1" colspan="1">0.403</td><td align="char" char="." rowspan="1" colspan="1">0.557</td><td align="char" char="." rowspan="1" colspan="1">
<bold>0.624</bold>
</td><td align="char" char="." rowspan="1" colspan="1">0.086</td><td align="char" char="." rowspan="1" colspan="1">0.382</td><td align="char" char="." rowspan="1" colspan="1">0.400</td><td align="char" char="." rowspan="1" colspan="1">0.420</td><td align="char" char="." rowspan="1" colspan="1">0.192</td></tr><tr><td align="left" rowspan="1" colspan="1">Jie Tang</td><td align="char" char="." rowspan="1" colspan="1">
<bold>0.989</bold>
</td><td align="char" char="." rowspan="1" colspan="1">0.982</td><td align="char" char="." rowspan="1" colspan="1">0.910</td><td align="char" char="." rowspan="1" colspan="1">0.657</td><td align="char" char="." rowspan="1" colspan="1">0.522</td><td align="char" char="." rowspan="1" colspan="1">0.825</td><td align="char" char="." rowspan="1" colspan="1">0.883</td><td align="char" char="." rowspan="1" colspan="1">0.738</td><td align="char" char="." rowspan="1" colspan="1">0.432</td><td align="char" char="." rowspan="1" colspan="1">0.902</td><td align="char" char="." rowspan="1" colspan="1">0.741</td></tr><tr><td align="left" rowspan="1" colspan="1">Thomas Wolf</td><td align="char" char="." rowspan="1" colspan="1">0.720</td><td align="char" char="." rowspan="1" colspan="1">
<bold>0.860</bold>
</td><td align="char" char="." rowspan="1" colspan="1">0.352</td><td align="char" char="." rowspan="1" colspan="1">0.703</td><td align="char" char="." rowspan="1" colspan="1">0.522</td><td align="char" char="." rowspan="1" colspan="1">0.516</td><td align="char" char="." rowspan="1" colspan="1">0.502</td><td align="char" char="." rowspan="1" colspan="1">0.320</td><td align="char" char="." rowspan="1" colspan="1">0.357</td><td align="char" char="." rowspan="1" colspan="1">0.390</td><td align="char" char="." rowspan="1" colspan="1">0.710</td></tr><tr><td align="left" rowspan="1" colspan="1">Yang Wang</td><td align="char" char="." rowspan="1" colspan="1">
<bold>0.989</bold>
</td><td align="char" char="." rowspan="1" colspan="1">0.548</td><td align="char" char="." rowspan="1" colspan="1">0.409</td><td align="char" char="." rowspan="1" colspan="1">0.273</td><td align="char" char="." rowspan="1" colspan="1">0.574</td><td align="char" char="." rowspan="1" colspan="1">0.443</td><td align="char" char="." rowspan="1" colspan="1">0.118</td><td align="char" char="." rowspan="1" colspan="1">0.171</td><td align="char" char="." rowspan="1" colspan="1">0.211</td><td align="char" char="." rowspan="1" colspan="1">0.310</td><td align="char" char="." rowspan="1" colspan="1">0.204</td></tr><tr><td align="left" rowspan="1" colspan="1">Avg</td><td align="char" char="." rowspan="1" colspan="1">
<bold>0.813</bold>
</td><td align="char" char="." rowspan="1" colspan="1">0.786</td><td align="char" char="." rowspan="1" colspan="1">0.680</td><td align="char" char="." rowspan="1" colspan="1">0.715</td><td align="char" char="." rowspan="1" colspan="1">0.681</td><td align="char" char="." rowspan="1" colspan="1">0.629</td><td align="char" char="." rowspan="1" colspan="1">0.507</td><td align="char" char="." rowspan="1" colspan="1">0.563</td><td align="char" char="." rowspan="1" colspan="1">0.606</td><td align="char" char="." rowspan="1" colspan="1">0.643</td><td align="char" char="." rowspan="1" colspan="1">0.678</td></tr></tbody></table></alternatives></table-wrap><p>As shown by the experimental results in the table above, the method proposed in this paper is significantly better than all the baseline methods, e.g., the average score (Avg) is higher than the other baseline methods by 0.048 to 0.327 (Component+0.048, Zhang+0.119, DeepWalk+0.271, LINE+0.228, Qiao+0.048, etc.).</p><p>In individual names, other baseline methods also slightly outperform this paper&#x02019;s method, e.g., in the results for the name David Cooper, Zhang et al.&#x02019;s method achieves a score of 0.931, which may be attributed to the fact that the baseline method&#x02019;s end-to-end local joins are better at disambiguating the name David Cooper, whereas in the results for the name Thomas Wolf, the Qiao et al. achieved a score of 0.860 better than the method of this paper 0.72.</p><p>It has been proved experimentally that the method proposed in this paper is significantly better than all the baseline methods, which may be due to the fact that, in this paper, the use of a combination of semantics and relations for embedding is able to improve the learning ability of the network. In terms of the network, the model in this paper has a significant advantage over the DeepWalk model in terms of embedding due to the use of random sampling of association weights of meta-paths as well as a semantic and relationship-based jump model; similarly, the model in this paper takes into account the relationships and node types in the network as well as the different weights of the relationships, which makes it stronger than the LINE, in terms of learning. Metapth2Vec model.</p><p>In order to evaluate the effectiveness of this paper&#x02019;s method on different datasets, this paper also uses three datasets for comparison experiments, and the results of the experiments are shown in <xref rid="pone.0310992.t004" ref-type="table">Table 4</xref>. From <xref rid="pone.0310992.t005" ref-type="table">Table 5</xref>, we can see that the results of this paper&#x02019;s method on different datasets are also basically superior to the baseline method. GHOST constructs a partnership network for each author with ambiguities, which may have made the prediction accuracy of their method higher, whereas this paper&#x02019;s method basically takes all three metrics into account, and collectively, this paper&#x02019;s method outperforms the other baseline methods.</p><table-wrap position="float" id="pone.0310992.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.t004</object-id><label>Table 4</label><caption><title>Comparison of ablation experiments.</title></caption><alternatives><graphic xlink:href="pone.0310992.t004" id="pone.0310992.t004g" position="float"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="center" rowspan="1" colspan="1">HAC</th><th align="center" rowspan="1" colspan="1">K unknown</th><th align="center" rowspan="1" colspan="1">K known</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="char" char="." rowspan="1" colspan="1">85.95</td><td align="char" char="." rowspan="1" colspan="1">51.01</td><td align="char" char="." rowspan="1" colspan="1">83.04</td></tr><tr><td align="left" rowspan="1" colspan="1">Rec</td><td align="char" char="." rowspan="1" colspan="1">76.46</td><td align="char" char="." rowspan="1" colspan="1">95.46</td><td align="char" char="." rowspan="1" colspan="1">81.43</td></tr><tr><td align="left" rowspan="1" colspan="1">F1</td><td align="char" char="." rowspan="1" colspan="1">79.48</td><td align="char" char="." rowspan="1" colspan="1">58.78</td><td align="char" char="." rowspan="1" colspan="1">81.34</td></tr></tbody></table></alternatives></table-wrap><table-wrap position="float" id="pone.0310992.t005"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.t005</object-id><label>Table 5</label><caption><title>Comparison of the methods in this paper on three different datasets.</title></caption><alternatives><graphic xlink:href="pone.0310992.t005" id="pone.0310992.t005g" position="float"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Method</th><th align="center" colspan="3" style="border-bottom-width:thick" rowspan="1">AMiner-v1</th><th align="center" colspan="3" style="border-bottom-width:thick" rowspan="1">AMiner-v2</th><th align="center" colspan="3" style="border-bottom-width:thick" rowspan="1">WhoISWho</th></tr><tr><th align="left" rowspan="1" colspan="1"/><th align="center" rowspan="1" colspan="1">P</th><th align="center" rowspan="1" colspan="1">R</th><th align="center" rowspan="1" colspan="1">F1</th><th align="center" rowspan="1" colspan="1">P</th><th align="center" rowspan="1" colspan="1">R</th><th align="center" rowspan="1" colspan="1">F1</th><th align="center" rowspan="1" colspan="1">P</th><th align="center" rowspan="1" colspan="1">R</th><th align="center" rowspan="1" colspan="1">F1</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">AMiner</td><td align="char" char="." rowspan="1" colspan="1">82.36</td><td align="char" char="." rowspan="1" colspan="1">79.23</td><td align="char" char="." rowspan="1" colspan="1">80.76</td><td align="char" char="." rowspan="1" colspan="1">77.56</td><td align="char" char="." rowspan="1" colspan="1">63.44</td><td align="char" char="." rowspan="1" colspan="1">69.79</td><td align="char" char="." rowspan="1" colspan="1">56.29</td><td align="char" char="." rowspan="1" colspan="1">48.03</td><td align="char" char="." rowspan="1" colspan="1">51.83</td></tr><tr><td align="left" rowspan="1" colspan="1">GHOST</td><td align="char" char="." rowspan="1" colspan="1">
<bold>93.46</bold>
</td><td align="char" char="." rowspan="1" colspan="1">67.49</td><td align="char" char="." rowspan="1" colspan="1">78.38</td><td align="char" char="." rowspan="1" colspan="1">
<bold>85.13</bold>
</td><td align="char" char="." rowspan="1" colspan="1">43.28</td><td align="char" char="." rowspan="1" colspan="1">57.39</td><td align="char" char="." rowspan="1" colspan="1">
<bold>76.16</bold>
</td><td align="char" char="." rowspan="1" colspan="1">24.59</td><td align="char" char="." rowspan="1" colspan="1">37.18</td></tr><tr><td align="left" rowspan="1" colspan="1">DeepWalk</td><td align="char" char="." rowspan="1" colspan="1">80.54</td><td align="char" char="." rowspan="1" colspan="1">74.67</td><td align="char" char="." rowspan="1" colspan="1">77.49</td><td align="char" char="." rowspan="1" colspan="1">57.92</td><td align="char" char="." rowspan="1" colspan="1">62.31</td><td align="char" char="." rowspan="1" colspan="1">60.03</td><td align="char" char="." rowspan="1" colspan="1">24.37</td><td align="char" char="." rowspan="1" colspan="1">
<bold>68.18</bold>
</td><td align="char" char="." rowspan="1" colspan="1">35.91</td></tr><tr><td align="left" rowspan="1" colspan="1">Node2Vec</td><td align="char" char="." rowspan="1" colspan="1">64.51</td><td align="char" char="." rowspan="1" colspan="1">
<bold>73.48</bold>
</td><td align="char" char="." rowspan="1" colspan="1">68.70</td><td align="char" char="." rowspan="1" colspan="1">28.45</td><td align="char" char="." rowspan="1" colspan="1">
<bold>57.96</bold>
</td><td align="char" char="." rowspan="1" colspan="1">38.17</td><td align="char" char="." rowspan="1" colspan="1">31.64</td><td align="char" char="." rowspan="1" colspan="1">62.21</td><td align="char" char="." rowspan="1" colspan="1">41.95</td></tr><tr><td align="left" rowspan="1" colspan="1">Our method</td><td align="char" char="." rowspan="1" colspan="1">83.04</td><td align="char" char="." rowspan="1" colspan="1">81.43</td><td align="char" char="." rowspan="1" colspan="1">
<bold>81.34</bold>
</td><td align="char" char="." rowspan="1" colspan="1">75.64</td><td align="char" char="." rowspan="1" colspan="1">67.90</td><td align="char" char="." rowspan="1" colspan="1">
<bold>71.56</bold>
</td><td align="char" char="." rowspan="1" colspan="1">58.19</td><td align="char" char="." rowspan="1" colspan="1">69.89</td><td align="char" char="." rowspan="1" colspan="1">
<bold>63.47</bold>
</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec020"><title>Experimental setup</title><p>In this paper, we have used Improved Hierarchical Clustering (RHAC) and shown the predicted classification of some of the names in the Aminer dataset in <xref rid="pone.0310992.g006" ref-type="fig">Fig 6</xref>. As can be seen from the figure below, most of the nodes are accurately predicted in the clustering results of RHAC, which proves that the method of this paper is more superior when compared to the classical clustering methods.</p><fig position="float" id="pone.0310992.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.g006</object-id><label>Fig 6</label><caption><title>Comparison of actual and predicted clustering of RHAC in six different names.</title><p>The origin of the figure shows the actual clustering effect, and the &#x000d7; sign shows the predicted clustering effect.</p></caption><graphic xlink:href="pone.0310992.g006" position="float"/></fig><p>In order to further validate the superiority of the clustering algorithm (RHAC) in this paper, this paper also compares the time consumed by clustering with four classical clustering algorithms, namely hierarchical agglomerative clustering (HAC) K-means, Gaussian Mixture (GMM), and Spectral Clustering (SC), respectively, in terms of the K-value is known, and the K-value is unknown. The results of the comparison of the K-value is known in the following <xref rid="pone.0310992.g007" ref-type="fig">Fig 7</xref>. From the experimental results in Figs <xref rid="pone.0310992.g007" ref-type="fig">7</xref> and <xref rid="pone.0310992.g008" ref-type="fig">8</xref>, it can be seen that the clustering efficiency as well as the clustering results of this paper&#x02019;s clustering algorithm, RHAC, are better than other classical clustering algorithms on the Aminer dataset, which suggests that this paper&#x02019;s introduction of the topology of the graph and the relationships between nodes into hierarchical clustering to improve the efficiency of traditional clustering is effective.</p><fig position="float" id="pone.0310992.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.g007</object-id><label>Fig 7</label><caption><title>Comparison of time consumed by different clustering algorithms when K value is known and the score.</title></caption><graphic xlink:href="pone.0310992.g007" position="float"/></fig><fig position="float" id="pone.0310992.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.g008</object-id><label>Fig 8</label><caption><title>Comparison of time consumed by different clustering algorithms when K value is known and the score.</title></caption><graphic xlink:href="pone.0310992.g008" position="float"/></fig><p>Ablation experiments:</p><p>In this paper, ablation experiments were conducted on the improved graph clustering algorithm by comparing it without specifying k-value using graph convolution module, specifying k-value using graph module and with classical hierarchical clustering, and the results are shown in <xref rid="pone.0310992.t004" ref-type="table">Table 4</xref> below: As can be seen from the experimental results in <xref rid="pone.0310992.t004" ref-type="table">Table 4</xref>, the improvement of classical hierarchical clustering using the graph convolution module in this paper achieves better results in a comprehensive way, the recall of this paper&#x02019;s method reaches 95.46, higher than that of the classical method by 19.00 when the value of k is not specified, and when the value of k is specified, the value of f1 of this paper&#x02019;s method reaches 81.34, higher than that of the classical method by 1.86, and the accuracy and recall scores are both higher than 80, which proves that the improvement strategy of this paper is effective. In this paper, we also conducted ablation experiments on the newly added multi-head attention mechanism. The settings of the number of attention heads are (2, 4, 6, 8, 10, 12, 14), the results are shown in <xref rid="pone.0310992.g009" ref-type="fig">Fig 9</xref>, through the experiment on the number of attention heads, it is concluded that the F1 value gradually improves with the increase of the number of attention heads and gradually stabilizes when the number of attention heads reaches 10, and the subsequent fluctuation of the F1 value with the increase of the number of attention heads may be due to the fact that, with the increase of the number of attention heads, the too many attention heads cannot have a significant effect on the improvement of the experimental effect, and also greatly improves the structural complexity of the network and improves the time of training, therefore, in this paper, we choose to use 10 attention heads as the embedding of multi-head attention. In this paper, we also conducted experiments on the embedding dimension of the parameters, the embedding dimension is set as (4, 8, 16, 32, 64, 128, 256, 512) The experimental results are shown in <xref rid="pone.0310992.g010" ref-type="fig">Fig 10</xref>, and the F1 value is gradually improved with the increase of the embedding dimension; in the V1 dataset, the model reaches a relative stability in the 128-dimension, and there is a slight decrease on the experimental results after the 128-dimension, which may be due to the embedding of the dimension is too large that leads to too much information embedded, which has an effect on the experimental results. In the V2 dataset because the amount of data is much larger than V1, the V2 dataset reaches relative stability at 256 dimensions. In the WHOISWHO dataset, the model reaches relative stability at 512 dimensions. Therefore, this paper uses 128 dimensions for the V1 dataset, 256 dimensions for the V2 dataset, and the same 256 dimensions for the WHOISWHO dataset as the dimension embedding.</p><fig position="float" id="pone.0310992.g009"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.g009</object-id><label>Fig 9</label><caption><title>The effects of the number of attentional heads on the experimental results.</title></caption><graphic xlink:href="pone.0310992.g009" position="float"/></fig><fig position="float" id="pone.0310992.g010"><object-id pub-id-type="doi">10.1371/journal.pone.0310992.g010</object-id><label>Fig 10</label><caption><title>The effects of the number of attentional heads on the experimental results.</title></caption><graphic xlink:href="pone.0310992.g010" position="float"/></fig></sec></sec><sec sec-type="conclusions" id="sec021"><title>Conclusion</title><p>In this paper, we propose a new Heterogeneous Graph Convolutional Attention Network (RA-HGCN) embedding and using Improved Hierarchical Clustering Disambiguation (RHAC) for author name disambiguation. Our network not only solves the problem that isomorphic graphs do not fully utilize the node information of the graph, but also considers the co-authorship problem. We give a threshold value between the authors of an article, and the authors who co-authored a paper that exceeds the threshold value are defined as reliable co-authors, and use the author name disambiguation method based on the relational graph heterogeneous neural network, firstly, the semantic and relational information of the published papers will be extracted and the information will be trained to be spliced using our graph convolution module, and a better representation of the features can be obtained by processing the information, and then it is inputted into our constructed relational heterogeneous graph attention neural network for training to get the vector representation of semantic and relational information. Finally, we use the improved hierarchical clustering algorithm (RHAC) to combine the relationship and topology between the graphs such as relational semantics and hierarchical clustering, and use the vector representations obtained from the training to replace the distance calculation of the original hierarchical clustering, so that our clustering method can determine the optimal k-value automatically during the clustering process, and improve the accuracy and efficiency of the clustering. After experiments, the average F1 value of this paper&#x02019;s method on the Aminer dataset using only three thesis features is 0.834, which is higher than other mainstream methods. Comparisons with several baseline methods and ablation experiments on the proposed method show that the method proposed in this paper significantly outperforms the baseline method in terms of accuracy as well as consumption time. However, the method proposed in this paper is not considered separately for newly published papers i.e., for incremental ablation, which also makes it necessary to reconstruct the network for training for newly published papers, which greatly increases the time required for ablation. In our future work, we will try to improve our method so that it can effectively disambiguate the increments.</p></sec></body><back><ref-list><title>References</title><ref id="pone.0310992.ref001"><label>1</label><mixed-citation publication-type="journal">
<name><surname>Aman</surname><given-names>V.</given-names></name> (<year>2018</year>). <article-title>A new bibliometric approach to measure knowledge transfer of internationally mobile scientists</article-title>. <source><italic toggle="yes">Scientometrics</italic></source>, <volume>117</volume>(<issue>1</issue>), <fpage>227</fpage>&#x02013;<lpage>247</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11192-018-2864-x</pub-id></mixed-citation></ref><ref id="pone.0310992.ref002"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Shoaib</surname><given-names>M.</given-names></name>, <name><surname>Daud</surname><given-names>A.</given-names></name>, &#x00026; <name><surname>Amjad</surname><given-names>T.</given-names></name> (<year>2020</year>). <article-title>Author name disambiguation in bibliographic databases: A survey</article-title>. <source><italic toggle="yes">CoRR</italic></source>, abs/2004.06391.</mixed-citation></ref><ref id="pone.0310992.ref003"><label>3</label><mixed-citation publication-type="other">Zhang, L., Huang, Y., Cheng, Q., &#x00026; Lu, W. (2020). Mining Author Identifiers for PubMed by Linking to Open Bibliographic Databases. In <italic toggle="yes">Proceedings&#x02014;Companion of the 2020 IEEE 20th International Conference on Software Quality</italic>, <italic toggle="yes">Reliability</italic>, <italic toggle="yes">and Security</italic>, <italic toggle="yes">QRS-C 2020</italic> (pp. 209&#x02013;212).</mixed-citation></ref><ref id="pone.0310992.ref004"><label>4</label><mixed-citation publication-type="other">Han, H., Giles, L., Zha, H., Li, C., &#x00026; Tsioutsiouliklis, K. (2004). Two supervised learning approaches for name disambiguation in author citations. In <italic toggle="yes">Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries</italic> (pp. 296&#x02013;305). IEEE.</mixed-citation></ref><ref id="pone.0310992.ref005"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Levin</surname><given-names>M.</given-names></name>, <name><surname>Krawczyk</surname><given-names>S.</given-names></name>, <name><surname>Bethard</surname><given-names>S.</given-names></name>, &#x00026; <name><surname>Jurafsky</surname><given-names>D.</given-names></name> (<year>2012</year>). <article-title>Citation-based bootstrapping for large-scale author disambiguation</article-title>. <source><italic toggle="yes">Journal of the American Society for Information Science and Technology</italic></source>, <volume>63</volume>(<issue>5</issue>), <fpage>1030</fpage>&#x02013;<lpage>1047</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/asi.22621</pub-id></mixed-citation></ref><ref id="pone.0310992.ref006"><label>6</label><mixed-citation publication-type="other">Jhawar, K., Sanyal, D. K., Chattopadhyay, S., Bhowmick, P. K., &#x00026; Das, P. P. (2020). Author name disambiguation in PubMed using ensemble-based classification algorithms. In <italic toggle="yes">Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020</italic> (pp. 469&#x02013;470). ACM.</mixed-citation></ref><ref id="pone.0310992.ref007"><label>7</label><mixed-citation publication-type="other">Tran, H. N., Huynh, T., &#x00026; Do, T. (2014). Author name disambiguation by using deep neural network. In <italic toggle="yes">Asian Conference on Intelligent Information and Database Systems</italic> (pp. 123&#x02013;132). Springer.</mixed-citation></ref><ref id="pone.0310992.ref008"><label>8</label><mixed-citation publication-type="journal">
<name><surname>Qiao</surname><given-names>Y.</given-names></name>, <name><surname>Zheng</surname><given-names>Q.</given-names></name>, <name><surname>Sakai</surname><given-names>T.</given-names></name>, <name><surname>Ye</surname><given-names>J.</given-names></name>, &#x00026; <name><surname>Liu</surname><given-names>J.</given-names></name> (<year>2015</year>). <article-title>Dynamic author name disambiguation for growing digital libraries</article-title>. <source><italic toggle="yes">Information Retrieval Journal</italic></source>, <volume>18</volume>(<issue>5</issue>), <fpage>379</fpage>&#x02013;<lpage>412</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10791-015-9261-3</pub-id></mixed-citation></ref><ref id="pone.0310992.ref009"><label>9</label><mixed-citation publication-type="other">Zhang, Y., Zhang, F., Yao, P., Tang, J. (2018). Name disambiguation in Aminer: Clustering, maintenance, and human in the loop. In <italic toggle="yes">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &#x00026; Data Mining</italic> (pp. 1002&#x02013;1011).</mixed-citation></ref><ref id="pone.0310992.ref010"><label>10</label><mixed-citation publication-type="journal">
<name><surname>Bertrand</surname><given-names>M.</given-names></name>, &#x00026; <name><surname>Mullainathan</surname><given-names>S.</given-names></name> (<year>2004</year>). <article-title>Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination</article-title>. <source><italic toggle="yes">American Economic Review</italic></source>, <volume>94</volume>(<issue>4</issue>), <fpage>991</fpage>&#x02013;<lpage>1013</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1257/0002828042002561</pub-id></mixed-citation></ref><ref id="pone.0310992.ref011"><label>11</label><mixed-citation publication-type="other">Khabsa, M., Treeratpituk, P., &#x00026; Giles, C. L. (2015). Online person name disambiguation with constraints. In <italic toggle="yes">Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries</italic> (pp. 37&#x02013;46).</mixed-citation></ref><ref id="pone.0310992.ref012"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Wu</surname><given-names>H.</given-names></name>, <name><surname>Li</surname><given-names>B.</given-names></name>, <name><surname>Pei</surname><given-names>Y.</given-names></name>, &#x00026; <name><surname>He</surname><given-names>J.</given-names></name> (<year>2014</year>). <article-title>Unsupervised author disambiguation using Dempster&#x02013;Shafer theory</article-title>. <source><italic toggle="yes">Scientometrics</italic></source>, <volume>101</volume>(<issue>3</issue>), <fpage>1955</fpage>&#x02013;<lpage>1972</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11192-014-1283-x</pub-id></mixed-citation></ref><ref id="pone.0310992.ref013"><label>13</label><mixed-citation publication-type="journal">
<name><surname>Liu</surname><given-names>W.</given-names></name>, <name><surname>Islamaj Do&#x0011f;an</surname><given-names>R.</given-names></name>, <name><surname>Kim</surname><given-names>S.</given-names></name>, <name><surname>Comeau</surname><given-names>D. C.</given-names></name>, <name><surname>Kim</surname><given-names>W.</given-names></name>, <name><surname>Yeganova</surname><given-names>L.</given-names></name>, <etal>et al</etal>. (<year>2014</year>). <article-title>Author name disambiguation for PubMed</article-title>. <source><italic toggle="yes">Journal of the Association for Information Science and Technology</italic></source>, <volume>65</volume>(<issue>4</issue>), <fpage>765</fpage>&#x02013;<lpage>781</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/asi.23063</pub-id>
<pub-id pub-id-type="pmid">28758138</pub-id>
</mixed-citation></ref><ref id="pone.0310992.ref014"><label>14</label><mixed-citation publication-type="journal">
<name><surname>Kim</surname><given-names>K.</given-names></name>, <name><surname>Sefid</surname><given-names>A.</given-names></name>, &#x00026; <name><surname>Giles</surname><given-names>C. L.</given-names></name> (<year>2020</year>). <article-title>Learning CNF blocking for large-scale author name disambiguation. In</article-title>
<source><italic toggle="yes">Proceedings of the First Workshop on Scholarly Document Processing</italic></source> (pp. <fpage>72</fpage>&#x02013;<lpage>80</lpage>). <comment>doi: </comment><pub-id pub-id-type="doi">10.18653/v1/2020.sdp-1.8</pub-id></mixed-citation></ref><ref id="pone.0310992.ref015"><label>15</label><mixed-citation publication-type="other">Zhang, Y., Zhang, F., Yao, P., &#x00026; Tang, J. (2018). Name disambiguation in Aminer: Clustering, maintenance, and human in the loop. In <italic toggle="yes">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &#x00026; Data Mining</italic> (pp. 1002&#x02013;1011).</mixed-citation></ref><ref id="pone.0310992.ref016"><label>16</label><mixed-citation publication-type="other">Zhang, B., Dundar, M., &#x00026; Al Hasan, M. (2016). Bayesian non-exhaustive classification: A case study on online name disambiguation using temporal record streams. In <italic toggle="yes">Proceedings of the 25th ACM International Conference on Information and Knowledge Management</italic> (pp. 1341&#x02013;1350).</mixed-citation></ref><ref id="pone.0310992.ref017"><label>17</label><mixed-citation publication-type="other">Sun, X., Kaur, J., Possamai, L., &#x00026; Menczer, F. (2011). Detecting ambiguous author names in crowdsourced scholarly data. In <italic toggle="yes">2011 IEEE Third International Conference on Privacy</italic>, <italic toggle="yes">Security</italic>, <italic toggle="yes">Risk and Trust and 2011 IEEE Third International Conference on Social Computing</italic> (pp. 568&#x02013;571). IEEE.</mixed-citation></ref><ref id="pone.0310992.ref018"><label>18</label><mixed-citation publication-type="other">Zhang, B., &#x00026; Al Hasan, M. (2017). Name disambiguation in anonymized graphs using network embedding. In <italic toggle="yes">Proceedings of the 2017 ACM Conference on Information and Knowledge Management</italic> (pp. 1239&#x02013;1248). Singapore.</mixed-citation></ref><ref id="pone.0310992.ref019"><label>19</label><mixed-citation publication-type="other">Hamilton, W., Ying, Z., &#x00026; Leskovec, J. (2017). Inductive representation learning on large graphs. In <italic toggle="yes">Advances in Neural Information Processing Systems</italic> (pp. 1024&#x02013;1034).</mixed-citation></ref><ref id="pone.0310992.ref020"><label>20</label><mixed-citation publication-type="other">Fu, T.-Y., Lee, W.-C., &#x00026; Lei, Z. (2017). Hin2vec: Explore meta-paths in heterogeneous information networks for representation learning. In <italic toggle="yes">Proceedings of the 2017 ACM Conference on Information and Knowledge Management</italic> (pp. 1797&#x02013;1806). ACM.</mixed-citation></ref><ref id="pone.0310992.ref021"><label>21</label><mixed-citation publication-type="other">Grover, A., &#x00026; Leskovec, J. (2016). node2vec: Scalable feature learning for networks. In <italic toggle="yes">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic> (pp. 855&#x02013;864). San Francisco, United States.</mixed-citation></ref><ref id="pone.0310992.ref022"><label>22</label><mixed-citation publication-type="other">Ribeiro, L. F., Saverese, P. H., &#x00026; Figueiredo, D. R. (2017). struc2vec: Learning Node Representations from Structural Identity. In <italic toggle="yes">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic> (pp. 385&#x02013;394). Halifax, Canada.</mixed-citation></ref><ref id="pone.0310992.ref023"><label>23</label><mixed-citation publication-type="journal">
<name><surname>Fan</surname><given-names>X.</given-names></name>, <name><surname>Wang</surname><given-names>J.</given-names></name>, <name><surname>Pu</surname><given-names>X.</given-names></name>, <name><surname>Zhou</surname><given-names>L.</given-names></name>, <name><surname>Lv</surname><given-names>B.</given-names></name> (<year>2011</year>). <article-title>On Graph-Based Name Disambiguation</article-title>. <source><italic toggle="yes">Journal of Data &#x00026; Information Quality</italic></source>, <volume>2</volume>(<issue>2</issue>), <fpage>1</fpage>&#x02013;<lpage>23</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/1891879.1891883</pub-id></mixed-citation></ref><ref id="pone.0310992.ref024"><label>24</label><mixed-citation publication-type="other">Xu, J., Shen, S., Li, D., &#x00026; et al. (2018). A network-embedding based method for author disambiguation. In <italic toggle="yes">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</italic> (pp. 1735&#x02013;1738).</mixed-citation></ref><ref id="pone.0310992.ref025"><label>25</label><mixed-citation publication-type="book">
<name><surname>Perozzi</surname><given-names>B.</given-names></name>, <name><surname>Al-Rfou</surname><given-names>R</given-names></name>., &#x00026; <name><surname>Skiena</surname><given-names>S.</given-names></name> (<year>2014</year>). <part-title>DeepWalk: Online learning of social representations</part-title>. In <source><italic toggle="yes">SIGKDD</italic></source> (pp. <fpage>701</fpage>&#x02013;<lpage>710</lpage>). <publisher-name>ACM</publisher-name>.</mixed-citation></ref><ref id="pone.0310992.ref026"><label>26</label><mixed-citation publication-type="other">Tang, J., Qu, M., Wang, M., Zhang, M., &#x00026; et al. (2015). LINE: Large-scale information network embedding. In <italic toggle="yes">WWW</italic> (pp. 1067&#x02013;1077). International World Wide Web Conferences Steering Committee.</mixed-citation></ref><ref id="pone.0310992.ref027"><label>27</label><mixed-citation publication-type="other">Wang, D., Cui, P., &#x00026; Zhu, W. (2016). Structural Deep Network Embedding. In <italic toggle="yes">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic> (pp. 1225&#x02013;1234). San Francisco, United States.</mixed-citation></ref><ref id="pone.0310992.ref028"><label>28</label><mixed-citation publication-type="book">
<name><surname>Dong</surname><given-names>Y.</given-names></name>, <name><surname>Chawla</surname><given-names>N. V.</given-names></name>, &#x00026; <name><surname>Swami</surname><given-names>A.</given-names></name> (<year>2017</year>). <part-title>metapath2vec: Scalable representation learning for heterogeneous networks</part-title>. In <source><italic toggle="yes">SIGKDD</italic></source> (pp. <fpage>135</fpage>&#x02013;<lpage>144</lpage>). <publisher-name>ACM</publisher-name>.</mixed-citation></ref></ref-list></back></article>