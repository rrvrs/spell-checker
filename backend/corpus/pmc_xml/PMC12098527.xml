<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Bioeng Biotechnol</journal-id><journal-id journal-id-type="iso-abbrev">Front Bioeng Biotechnol</journal-id><journal-id journal-id-type="publisher-id">Front. Bioeng. Biotechnol.</journal-id><journal-title-group><journal-title>Frontiers in Bioengineering and Biotechnology</journal-title></journal-title-group><issn pub-type="epub">2296-4185</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC12098527</article-id><article-id pub-id-type="publisher-id">1550875</article-id><article-id pub-id-type="doi">10.3389/fbioe.2025.1550875</article-id><article-categories><subj-group subj-group-type="heading"><subject>Bioengineering and Biotechnology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Research on target localization and adaptive scrubbing of intelligent bathing assistance system</article-title><alt-title alt-title-type="left-running-head">Li et al.</alt-title><alt-title alt-title-type="right-running-head">
<ext-link xlink:href="https://doi.org/10.3389/fbioe.2025.1550875" ext-link-type="uri">10.3389/fbioe.2025.1550875</ext-link>
</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Li</surname><given-names>Ping</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/2891043/overview"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Feng</surname><given-names>Shikai</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/2892344/overview"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Yu</surname><given-names>Hongliu</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><xref rid="c001" ref-type="corresp">*</xref><uri xlink:href="https://loop.frontiersin.org/people/1380736/overview"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/Writing - review &#x00026; editing/"/></contrib></contrib-group><aff id="aff1">
<sup>1</sup>
<institution>Department of Biomedical Engineering</institution>, <institution>Changzhi Medical College</institution>, <addr-line>Changzhi</addr-line>, <country>China</country>
</aff><aff id="aff2">
<sup>2</sup>
<institution>Institute of Rehabilitation Engineering and Technology</institution>, <institution>University of Shanghai for Science and Technology</institution>, <addr-line>Shanghai</addr-line>, <country>China</country>
</aff><author-notes><fn fn-type="edited-by"><p>
<bold>Edited by:</bold>
<ext-link xlink:href="https://loop.frontiersin.org/people/699671/overview" ext-link-type="uri">Changsheng Li</ext-link>, Beijing Institute of Technology, China</p></fn><fn fn-type="edited-by"><p>
<bold>Reviewed by:</bold>
<ext-link xlink:href="https://loop.frontiersin.org/people/1114399/overview" ext-link-type="uri">Lin Wang</ext-link>, Chinese Academy of Sciences (CAS), China</p><p>
<ext-link xlink:href="https://loop.frontiersin.org/people/2978085/overview" ext-link-type="uri">Xiaoli Gao</ext-link>, Taiyuan University of Technology, China</p><p>
<ext-link xlink:href="https://loop.frontiersin.org/people/2994141/overview" ext-link-type="uri">Jing Zhang</ext-link>, Siemens Healthineers, China</p></fn><corresp id="c001">*Correspondence: Hongliu Yu, <email>yhl98@hotmail.com</email>
</corresp></author-notes><pub-date pub-type="epub"><day>09</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>13</volume><elocation-id>1550875</elocation-id><history><date date-type="received"><day>24</day><month>12</month><year>2024</year></date><date date-type="accepted"><day>23</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025 Li, Feng and Yu.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Li, Feng and Yu</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><sec><title>Introduction</title><p>Bathing is a primary daily activity. Existing bathing systems are limited by their lack of intelligence and adaptability, reliance on caregivers, and the complexity of their control algorithms. Although visual sensors are widely used in intelligent systems, current intelligent bathing systems do not effectively process depth information from these sensors.</p></sec><sec><title>Methods</title><p>The scrubbing task of the intelligent bath assist system can be divided into a pre-contact localization phase and a post-contact adaptive scrubbing phase. YOLOv5s, known for its ease of deployment and high accuracy, is utilized for multi-region skin detection to identify different body parts. The depth correction algorithm is designed to improve the depth accuracy of RGB-D vision sensors. The 3D position and pose of the target point in the RGB camera coordinate system are modeled and then transformed to the robot base coordinate system by hand-eye calibration. The system localization accuracy is measured when the collaborative robot runs into contact with the target. The self-rotating end scrubber head has flexible bristles with an adjustable length of 10&#x000a0;mm. After the end is in contact with the target, the point cloud scrubbing trajectory is optimized using cubic B-spline interpolation. Normal vectors are estimated based on approximate triangular dissected dyadic relations. Segmented interpolation is proposed to achieve real-time planning and to address the potential effects of possible unexpected movements of the target. The position and pose updating strategy of the end scrubber head is established.</p></sec><sec><title>Results</title><p>YOLOv5s enables real-time detection, tolerating variations in skin color, water vapor, occlusion, light, and scene. The localization error is relatively small, with a maximum value of 2.421&#x000a0;mm, a minimum value of 2.081&#x000a0;mm, and an average of 2.186&#x000a0;mm. Sampling the scrubbing curve every 2&#x000a0;mm along the x-axis and comparing actual to desired trajectories, the y-axis shows a maximum deviation of 2.23&#x000a0;mm, which still allows the scrubbing head to conform to the human skin surface.</p></sec><sec><title>Discussion</title><p>The study does not focus on developing complex control algorithms but instead emphasizes improving the accuracy of depth data to enhance localization precision.</p></sec></abstract><kwd-group><kwd>intelligent bathing assistance</kwd><kwd>deep learning</kwd><kwd>multi-region skin detection</kwd><kwd>depth correction</kwd><kwd>hand-eye calibration</kwd><kwd>B-spline curve</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that financial support was received for the research and/or publication of this article. This research was funded by the National Key R&#x00026;D Program of China under Grant 2022YFC3601400, and the Research Start-up Fund Project for PhD Scholars of Changzhi Medical College, China (No. 2024BS12).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Biosensors and Biomolecular Electronics</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><title>1 Introduction</title><p>The aging population trend places a substantial economic burden on families and insurance systems, creating an increased demand for specialized care, especially for bathing. Although various bathing aids are available, such as anti-slip mats, grab bars, and bath mats, most lack personalization, intelligent functions, and integrated functionality. The intelligent bathing assistance system offers a solution to caregiver shortages and the high demands on caregivers by minimizing awkward interactions and reducing potential risks for both users and caregivers. The system supports older adults&#x02019; independence and quality of life while advancing the intelligence and effectiveness of bathing assistance technologies (<xref rid="B11" ref-type="bibr">He et al., 2019</xref>).</p><p>With the advancement of artificial intelligence, visual perception has become one of the most prominent research areas, widely applied in fields such as drones (<xref rid="B33" ref-type="bibr">Shi et al., 2023</xref>), industrial robots (<xref rid="B3" ref-type="bibr">D'Avella et al., 2023</xref>), and service robots (<xref rid="B17" ref-type="bibr">Juang and Zhang, 2019</xref>). Lin et al. employed an RGB-D camera and utilized local feature descriptors of point clouds to achieve object recognition (<xref rid="B26" ref-type="bibr">Lin et al., 2019</xref>). Mart&#x000ed;nez et al. constructed composite feature vectors by integrating local and global features and employed feature fusion techniques to achieve clothing classification and perception based on an RGB-D camera (<xref rid="B29" ref-type="bibr">Mart&#x000ed;nez et al., 2019</xref>). Fu et al. developed a machine vision system based on an RGB-D camera, employing depth thresholding to remove background noise and utilizing a convolutional neural network to identify apples from RGB images (<xref rid="B7" ref-type="bibr">Fu et al., 2020</xref>). Luo et al. designed a vision perception system based on deep learning, capable of rapidly identifying wooden blocks within the field of view in industrial environments (<xref rid="B28" ref-type="bibr">Luo et al., 2020</xref>). Jia et al. employed a template-matching approach to automatically detect and segment cows from RGB and depth images (<xref rid="B16" ref-type="bibr">Jia et al., 2021</xref>). Yu et al. eliminated redundant information using depth images based on an RGB-D camera and trained a random forest binary classification model based on color and texture features to achieve lychee recognition (<xref rid="B37" ref-type="bibr">Yu L. et al., 2021</xref>). Huang et al. investigated the visual perception technology for assisting micro aerial vehicles in navigating stairs, employing a front camera to detect stairway entrances and a bottom camera to extract features of the stairs, walls, and railings (<xref rid="B14" ref-type="bibr">Huang and Lin, 2021</xref>). Weng et al. developed a dual-arm mobile robot visual perception system for human-robot interaction, using YOLO for target recognition and localization (<xref rid="B35" ref-type="bibr">Weng et al., 2022</xref>). Li et al. developed a tea-picking robot for field-based tea leaf recognition and localization based on an RGB-D camera, detecting tea bud regions with YOLO and employing point cloud data for 3D localization (<xref rid="B25" ref-type="bibr">Li Y. et al., 2021</xref>). Common methods for calculating the three-dimensional coordinates of targets often involve monocular vision, binocular vision, and RGB-D cameras, with typical data sources including RGB images, depth maps, and point clouds. The advantages and limitations of these data sources, as well as the sensors used, are summarized in <xref rid="T1" ref-type="table">Table 1</xref>. Bathing target localization requires three-dimensional information about the target, and visual sensors capable of generating depth information are the preferred choice for this task. Vision-based target localization offers advantages such as accessibility, universality, and the ability to provide rich scene information. However, in the context of bathing assistance, no template can represent all users, and manual labeling is not feasible. Moreover, the high safety requirements during scrubbing demand precise distance information. Additionally, variations in environmental factors, such as humidity and lighting conditions, pose challenges for skin detection.</p><table-wrap position="float" id="T1"><label>TABLE 1</label><caption><p>Advantages, limitations, and sensor types associated with various data sources.</p></caption><table frame="hsides" rules="groups"><thead valign="top"><tr><th align="left" rowspan="1" colspan="1">Sensors</th><th align="left" rowspan="1" colspan="1">Data sources</th><th align="left" rowspan="1" colspan="1">Advantages</th><th align="left" rowspan="1" colspan="1">Limitations</th></tr></thead><tbody valign="top"><tr><td align="left" rowspan="1" colspan="1">Monocular vision</td><td align="left" rowspan="1" colspan="1">RGB images</td><td align="left" rowspan="1" colspan="1">Simple structure and low cost</td><td align="left" rowspan="1" colspan="1">Highly affected by lighting and unable to personally obtain depth information</td></tr><tr><td align="left" rowspan="1" colspan="1">Binocular vision</td><td align="left" rowspan="1" colspan="1">RGB images, depth images, and point cloud information</td><td align="left" rowspan="1" colspan="1">It can obtain deep information</td><td align="left" rowspan="1" colspan="1">Binocular matching is influenced by various factors, particularly its ineffectiveness in textureless scenes</td></tr><tr><td align="left" rowspan="1" colspan="1">RGB-D sensors</td><td align="left" rowspan="1" colspan="1">RGB images, depth images, and point cloud information</td><td align="left" rowspan="1" colspan="1">Capable of obtaining depth information with a variety of imaging principles available</td><td align="left" rowspan="1" colspan="1">Depth accuracy is closely related to the distance between the object and the sensor, with significantly reduced precision for transparent objects and reflective surfaces</td></tr></tbody></table></table-wrap><p>Zlatintsi et al. developed the I-Support, which primarily consists of cameras, automatic scrubbers, and an electric shower chair (<xref rid="B41" ref-type="bibr">Zlatintsi et al., 2020</xref>). This system uses point cloud data for visual perception and employs predefined cleaning paths to simplify the problem. However, it uses multiple RGB-D cameras, leading to system redundancy, and involves complex control algorithms. Furthermore, it fails to meet personalized needs and cannot provide user-adaptive scrubbing capabilities.</p><p>Although previous studies have explored bathing assistance systems, they have not fully addressed the potential negative impact of depth value errors on bathing operations. To address this issue, this study designs a bathing assistance system and proposes a method to enhance the depth accuracy of visual sensors. Utilizing high-precision depth data, the study achieves accurate target localization and adaptive scrubbing functions. The structure of this paper is organized as follows: In the &#x0201c;Materials and Methods&#x0201d; section, the study provides a detailed discussion on target localization before contact and adaptive scrubbing after contact, with a focus on deep learning-based multi-region skin detection, depth value correction, and adaptive scrubbing. The &#x0201c;Results&#x0201d; section presents the outcomes of multi-region skin detection, localization experiments, and scrubbing experiments. The &#x0201c;Discussion&#x0201d; section offers an analysis of the experimental results. Finally, the &#x0201c;Conclusion&#x0201d; section summarizes the main findings and contributions of this research.</p></sec><sec sec-type="materials|methods" id="s2"><title>2 Materials and methods</title><p>The bathing tasks of the intelligent bathing assistance system can be divided into two main phases: the pre-contact and post-contact tasks. The goal of the pre-contact phase is to achieve high-precision localization, ensuring that the end-effector accurately reaches the target area. The post-contact phase involves planning the adaptive scrubbing, ensuring that the end-effector conforms precisely to the surface of the human skin for scrubbing. As shown in <xref rid="F1" ref-type="fig">Figure 1</xref>, after completing tasks such as skin detection, depth correction, 3D position and pose modeling, and hand-eye calibration, the system can execute the target localization task before contact. Once the end-effector reaches the target area, the system extracts and plans the scrubbing trajectory while updating the end-effector&#x02019;s pose to perform adaptive scrubbing.</p><fig position="float" id="F1"><label>FIGURE 1</label><caption><p>High-precision positioning before end-effector contact with the target, followed by adaptive scrubbing post-contact.</p></caption><graphic xlink:href="fbioe-13-1550875-g001" position="float"/></fig><p>The experimental equipment is shown in <xref rid="F2" ref-type="fig">Figure 2</xref>. TX2 has a low price, low power consumption, and small size, making it suitable for situations such as cost control and limited workspace. The scheme of predicting depth by RGB increases the need for computational power and raises the cost. An RGB-D camera capable of generating depth information is preferred for this task. Due to the presence of water vapor and liquid droplets, the RGB-D camera based on the passive imaging principle is more suitable for applications in the assisted bathing scenario. The Intel RealSense D455 visual sensor demonstrates cost-effectiveness while maintaining superior imaging quality compared to its counterparts within the same product series. This device preserves identical field of view (FOV) specifications for both RGB and depth modalities, thereby enabling synchronized spatial registration that facilitates precise depth value extraction corresponding to individual RGB pixels. Considering the safety requirements and the special bathing environment, the bathing robotic arm needs to have force sensing and waterproof capabilities. Therefore, a lightweight six-degree-of-freedom collaborative robot arm RM65-6&#x000a0;F that meets the requirements is selected, with a six-degree-of-freedom force sensor installed at the end. The scrubbing head at the end-effector is capable of autonomous rotation and is fitted with flexible bristles.</p><fig position="float" id="F2"><label>FIGURE 2</label><caption><p>The experimental equipment.</p></caption><graphic xlink:href="fbioe-13-1550875-g002" position="float"/></fig><sec id="s2-1"><title>2.1 Vision-based high-precision 3D localization before contact</title><sec id="s2-1-1"><title>2.1.1 Deep learning-based multi-region skin detection</title><p>Skin detection plays a crucial role in diagnosing conditions such as melanoma and skin cancer (<xref rid="B18" ref-type="bibr">Khan et al., 2021a</xref>). In a robotic bathing system, the realization of different bathing modes necessitates the precise positioning of body parts to activate the appropriate mode for a specific body region. When using object detection for multi-region skin detection, the focus is on distinguishing between skin regions as opposed to classifying each pixel individually. This approach reduces both the annotation burden and computational complexity, thereby facilitating real-time robotic control (<xref rid="B23" ref-type="bibr">Li et al., 2023</xref>).</p><p>In recent years, object detection algorithms based on the deep convolutional neural network (DCNN) have advanced rapidly (<xref rid="B2" ref-type="bibr">Chen et al., 2024</xref>), utilizing large datasets to automatically learn features, demonstrating strong robustness to challenges such as steam, lighting, and target variations in bathing environments. DCNN-based object detection algorithms can be categorized into two-stage/single-stage approaches and anchor-based/anchor-free methods (<xref rid="B24" ref-type="bibr">Li P. et al., 2021</xref>). Among these, single-stage algorithms, which directly perform classification and regression, are particularly notable for their excellent real-time performance and have gained widespread attention in practical applications (<xref rid="B5" ref-type="bibr">Fang et al., 2022</xref>). The YOLOv5s model is distinguished by its minimalistic design, which is optimized for deployment on hardware platforms with restricted computational and memory footprints. It boasts an expedited inference rate, making it particularly well-suited for time-sensitive object detection scenarios (<xref rid="B22" ref-type="bibr">Lawal et al., 2023</xref>). YOLOv5s, with its ease of deployment (<xref rid="B37" ref-type="bibr">Yu Y. et al., 2021</xref>), real-time capabilities, and high accuracy, was selected as the preferred deep learning model for multi-region skin detection in this study.</p><sec id="s2-1-1-1"><title>2.1.1.1 Construction of the datasets</title><p>To build diverse skin detection datasets, we collected images containing skin regions, considering factors such as skin tone, lighting conditions, and humidity. Using the annotation tool LabelImg (<xref rid="B8" ref-type="bibr">Geng et al., 2024</xref>; <xref rid="B13" ref-type="bibr">Hu et al., 2024</xref>), we manually delineated rectangular bounding boxes around each target and labeled their categories and positions, generating XML files in PASCAL VOC format. Each XML file includes the image filename, the coordinates of the ground truth (GT) bounding box, and the associated category labels. The datasets consisted of seven categories: (1) Face_skin, (2) Trunk_skin, (3) Upperlimb_skin, (4) Lowerlimb_skin, (5) Hand_skin, (6) Foot_skin, and (7) Background, representing the skin of various body parts and the background region. To address the issue of class imbalance and improve skin detection performance, we applied offline data augmentation techniques to ensure a more balanced distribution of samples across categories (<xref rid="B15" ref-type="bibr">Hussain et al., 2024</xref>). More than 20 methods were implemented, including adding random pixels, Gaussian noise, random rectangular occlusion, random pixel zeroing, salt-and-pepper noise, Gaussian blur, motion blur, adaptive histogram equalization, horizontal flipping, vertical flipping, proportional scaling, non-proportional scaling, random translation, HSV transformation, perspective transformation, random contrast adjustment, edge enhancement, random brightness adjustment, max pooling, average pooling, random cropping and padding, etc. These techniques were applied to images with limited sample sizes to generate new samples. In total, the datasets included 2,266 images, as shown in <xref rid="F3" ref-type="fig">Figure 3</xref>.</p><fig position="float" id="F3"><label>FIGURE 3</label><caption><p>Example images of the object detection datasets.</p></caption><graphic xlink:href="fbioe-13-1550875-g003" position="float"/></fig></sec><sec id="s2-1-1-2"><title>2.1.1.2 Model training settings and evaluation metrics</title><p>Training a network from scratch typically requires a substantial amount of annotated data (<xref rid="B31" ref-type="bibr">Pattnaik et al., 2020</xref>). However, collecting images containing skin is challenging, making it difficult to construct large-scale datasets. Training on small datasets poses a significant risk of model overfitting. To address this issue, we employed a transfer learning approach (<xref rid="B31" ref-type="bibr">Pratondo and Bramantoro, 2022</xref>), which enabled efficient training of a model on small datasets with performance comparable to training from scratch (<xref rid="B18" ref-type="bibr">Khan et al., 2021b</xref>; <xref rid="B30" ref-type="bibr">Mayya et al., 2024</xref>). Specifically, the model was pre-trained on the ImageNet dataset, and the resulting weight file served as the initial weight for the model.</p><p>The model was trained using the PyTorch framework at the Supercomputing Center of the University of Shanghai for Science and Technology. The datasets were split into training, validation, and test sets, with proportions of 60%, 20%, and 20%, respectively. The initial learning rate was set to 0.001, with a decay rate of 0.01, and the optimizer used was stochastic gradient descent (SGD). During training, the parameters of the backbone network were initially frozen, with only the remaining parameters updated. Afterward, the backbone network was unfrozen, and all parameters underwent trained. This strategy effectively enhanced the convergence speed and training efficiency of the network.</p><p>Recall and Precision are critical metrics for assessing the network performance, as shown in <xref rid="e1" ref-type="disp-formula">Equations 1</xref>, <xref rid="e2" ref-type="disp-formula">2</xref>. Specifically, TP denotes the number of true positives (correctly detected targets), FP refers to the number of false positives, and FN represents the number of undetected targets (<xref rid="B27" ref-type="bibr">Lu and Hong, 2024</xref>). By plotting Recall on the x-axis and Precision on the y-axis, a Precision-Recall (P-R) curve can be generated. The area enclosed by the curve and the axes corresponds to the average precision (AP). The mean of the AP across all categories is defined as the mean average precision (mAP), which serves as an important evaluation metric for multi-class object detection.<disp-formula id="e1">
<mml:math id="m1" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math>
<label>(1)</label>
</disp-formula>
<disp-formula id="e2">
<mml:math id="m2" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math>
<label>(2)</label>
</disp-formula>
</p></sec></sec><sec id="s2-1-2"><title>2.1.2 Depth correction algorithm</title><p>Upon localizing the region of the body part, acquiring distance measurements becomes imperative to facilitate accurate three-dimensional pose estimation. However, depth data from RGB-D cameras often exhibit issues such as high noise, low accuracy, and outliers (<xref rid="B1" ref-type="bibr">Alenya et al., 2014</xref>). Additionally, geometric distortions and system biases may affect these measurements (<xref rid="B36" ref-type="bibr">Yang et al., 2020</xref>), which can degrade localization accuracy. To enhance the precision of distance data, our study presents a depth correction algorithm to achieve high-accuracy target localization.</p><p>The depth quality evaluation metrics are essential for analyzing depth accuracy, developing depth correction algorithms, and evaluating the effectiveness of the corrections. <italic>Z</italic>
<sub>
<italic>accuracy</italic>
</sub> is used to assess the precision of depth data. The filling rate is represented by the ratio of valid pixels to total pixels. The root mean square error (RMSE) quantifies spatial noise. The filling rate, a hardware-dependent parameter that cannot be modified through algorithms, can be manually adjusted in Intel<sup>&#x000ae;</sup> RealSense&#x02122; Viewer software. The <italic>Z</italic>
<sub>
<italic>accuracy</italic>
</sub> and RMSE are used as depth quality evaluation indicators in depth correction research. Their calculation methods are provided in <xref rid="e3" ref-type="disp-formula">Equations 3</xref>, <xref rid="e4" ref-type="disp-formula">and 4</xref>, where <italic>i</italic> denotes the index of a point in the point cloud, <italic>n</italic>
<sub>
<italic>0</italic>
</sub> represents the total number of points in the point cloud, <italic>j</italic>
<sub>
<italic>i</italic>
</sub> is the distance value at each point, GT denotes the ground truth distance, and <italic>d</italic>
<sub>
<italic>0i</italic>
</sub> is the distance from the <italic>i</italic>th point to the fitted plane. <italic>Z</italic>
<sub>
<italic>accuracy</italic>
</sub> quantifies the proximity between the measured depth values and the GT. It is calculated as the difference between the average depth value of all pixels and the GT. RMSE quantifies the intrinsic variation in depth values, calculated by measuring the deviation of all valid pixels from the best-fit plane (<xref rid="B10" ref-type="bibr">Gupta et al., 2023</xref>).<disp-formula id="e3">
<mml:math id="m3" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mn mathvariant="bold">0</mml:mn></mml:msub></mml:msubsup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">j</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>G</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mn mathvariant="bold">0</mml:mn></mml:msub></mml:mfrac></mml:mrow></mml:math>
<label>(3)</label>
</disp-formula>
<disp-formula id="e4">
<mml:math id="m4" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mn mathvariant="bold">0</mml:mn></mml:msub></mml:msubsup><mml:msup><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn mathvariant="bold">0</mml:mn></mml:msub></mml:mfrac></mml:msqrt></mml:mrow></mml:math>
<label>(4)</label>
</disp-formula>
</p><p>The D455 not only captures RGB and depth images of the scene but also outputs point cloud data simultaneously. Moreover, the point cloud and depth images can be converted interchangeably. The depth quality analysis of D455 reveals that the RMSE and <italic>Z</italic>
<sub>
<italic>accuracy</italic>
</sub> increase gradually with distance, exhibiting a nonlinear relationship. The depth measurement at each pixel deviates from the GT, while the average depth measurement across a planar surface also exhibits deviation from the GT. When capturing a flat wall, the error is reflected in two aspects: the deviation of individual 3D points from the true plane and the misalignment between the fitted plane and the actual plane. To correct the errors, each 3D point must be adjusted to align with the true plane, and the discrepancy between the fitted and actual planes requires rectification. A chessboard pattern is introduced for correction, leveraging its mature corner detection algorithms to facilitate accurate identification of the chessboard and its corresponding plane. As the error increases with distance, an iterative approach is employed. The correction begins at short distances, using parameters derived from these distances as initial values to estimate the error correction function for greater distances.</p><p>The depth correction algorithm is implemented in two stages: (1) The function <italic>f</italic>
<sub>
<italic>l</italic>
</sub> is utilized to correct depth errors at different pixels. A specific function <italic>f</italic>
<sub>
<italic>l</italic>
</sub> should be calculated for each pixel to correct the 3D points of a flat wall surface, ensuring they align onto a single plane. (2) The function <italic>f</italic>
<sub>
<italic>g</italic>
</sub> is employed to correct the average depth value, ensuring the plane from the previous stage is aligned to its true position. In theory, the chessboard pattern should form a planar surface (<xref rid="B6" ref-type="bibr">Fryskowska, 2019</xref>). The chessboard plane observed within the RGB camera&#x02019;s field of view is aligned with the corrected point cloud plane from the first stage to estimate the errors. Since the RGB image and the point cloud are in different reference frames, there exists a rigid-body transformation between the two. To perform function fitting, the transformation matrix <sup>
<bold>
<italic>C</italic>
</bold>
</sup>
<sub>
<bold>
<italic>D</italic>
</bold>
</sub>
<bold>
<italic>M</italic>
</bold> between the RGB image and the point cloud must be obtained. Additionally, the process of calculating the extrinsic parameters relies on the accuracy of the intrinsic parameters (<xref rid="B39" ref-type="bibr">Zhang et al., 2020</xref>). To address the issue of interdependence, a solution that simultaneously optimizes both <italic>f</italic>
<sub>
<italic>g</italic>
</sub> and <sup>
<bold>
<italic>C</italic>
</bold>
</sup>
<sub>
<bold>
<italic>D</italic>
</bold>
</sub>
<bold>
<italic>M</italic>
</bold> is adopted.</p><p>The depth correction algorithm is illustrated in <xref rid="F4" ref-type="fig">Figure 4A</xref>. The chessboard corner points are first extracted from the RGB image, and a point cloud is generated from the depth image. Function <italic>f</italic>
<sub>
<italic>l</italic>
</sub> is estimated using the chessboard corner points, the corresponding point cloud, and the factory-calibrated transformation matrix <sup>
<italic>C</italic>
</sup>
<sub>
<italic>D</italic>
</sub>
<italic>M</italic>
<sub>
<italic>0</italic>
</sub>. Once <italic>f</italic>
<sub>
<italic>l</italic>
</sub> is determined, it is applied to correct the point cloud. The corrected point cloud, chessboard corner points, and a subset of the wall point cloud data are then combined to calculate function <italic>f</italic>
<sub>
<italic>g</italic>
</sub> and <sup>
<bold>
<italic>C</italic>
</bold>
</sup>
<sub>
<bold>
<italic>D</italic>
</bold>
</sub>
<bold>
<italic>M</italic>
</bold>.</p><fig position="float" id="F4"><label>FIGURE 4</label><caption><p>Depth correction <bold>(A)</bold> Two-stage depth correction algorithm <bold>(B)</bold>
<italic>f</italic>
<sub>
<italic>l</italic>
</sub> estimation methods <bold>(C)</bold>
<italic>f</italic>
<sub>
<italic>g</italic>
</sub> estimation methods.</p></caption><graphic xlink:href="fbioe-13-1550875-g004" position="float"/></fig><p>The process of estimating <italic>f</italic>
<sub>
<italic>l</italic>
</sub> is illustrated in <xref rid="F4" ref-type="fig">Figure 4B</xref>. A chessboard is pasted on a flat wall, and the camera position is progressively adjusted to increase the distance from the wall. Each collected point cloud is processed iteratively. The current estimate of <italic>f</italic>
<sub>
<italic>l</italic>
</sub> is applied to the initial point cloud to correct the errors. Wall points are extracted from the corrected point cloud, and a plane is fitted to the subset to improve computational efficiency (<xref rid="B40" ref-type="bibr">Zhu et al., 2021</xref>). The plane is then used to update <italic>f</italic>
<sub>
<italic>l</italic>
</sub>. Data for fitting <italic>f</italic>
<sub>
<italic>l</italic>
</sub> are extracted through distance-based projection method, with each processed corrected point cloud generating a set of data for fitting <italic>f</italic>
<sub>
<italic>l</italic>
</sub>. The iterative process continues for all point clouds, resulting in the progressive refinement of <italic>f</italic>
<sub>
<italic>l</italic>
</sub>.</p><p>The process of estimating <italic>f</italic>
<sub>
<italic>g</italic>
</sub> is illustrated in <xref rid="F4" ref-type="fig">Figure 4C</xref>. In the same scene, the plane in the RGB image and the corresponding point cloud can be aligned through specific rotational and translational transformations. The rotation matrix and translation vector represent the rigid-body transformation between the RGB camera coordinate system and the depth camera coordinate system (<xref rid="B36" ref-type="bibr">Yang et al., 2020</xref>). The transformation is estimated using the plane equations derived from the RGB image and the corresponding point cloud. This transformation is subsequently applied to the plane equation in the RGB camera coordinate system to derive the plane equation in the depth camera coordinate system, thereby updating <italic>f</italic>
<sub>
<italic>g</italic>
</sub>.</p><p>The D455 camera was used to collect data from a flat wall equipped with a chessboard pattern at varying distances and orientations. RGB images, depth images, and point clouds were captured for estimating <italic>f</italic>
<sub>
<italic>l</italic>
</sub> and <italic>f</italic>
<sub>
<italic>g</italic>
</sub>.</p></sec><sec id="s2-1-3"><title>2.1.3 3D position and pose modeling of target points in the RGB camera coordinate system</title><p>According to the camera model, if the distance <italic>Z</italic>
<sub>
<italic>c</italic>
</sub> of the target point and the camera intrinsic matrix <bold>
<italic>K</italic>
</bold>
<sub>
<bold>
<italic>int</italic>
</bold>
</sub> are known, the 3D coordinates of the target point in the RGB camera coordinate system can be derived from its pixel coordinates [u,v] in the RGB image, as shown in <xref rid="e5" ref-type="disp-formula">Equation 5</xref> (<xref rid="B20" ref-type="bibr">Konecny et al., 2024</xref>). This enables 3D positional modeling of the target point in the RGB camera coordinate system, denoted as [<italic>X</italic>
<sub>
<italic>c</italic>
</sub>,<italic>Y</italic>
<sub>
<italic>c</italic>
</sub>,<italic>Z</italic>
<sub>
<italic>c</italic>
</sub>].<disp-formula id="e5">
<mml:math id="m5" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi></mml:msub><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="bold">1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mi mathvariant="bold-italic">int</mml:mi></mml:msub><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
<label>(5)</label>
</disp-formula>
</p><p>As shown in <xref rid="F5" ref-type="fig">Figure 5</xref>, <italic>O</italic>
<sub>
<italic>0</italic>
</sub>-<italic>uv</italic> represents the pixel coordinate system in the camera model. A circle is drawn with the target point as the center and a predefined radius. By utilizing the pixel coordinates and corresponding depth values, the 3D positions of all pixels within the circle in the RGB camera coordinate system are computed. These points define a surface on the target surface, and the pose of the target point can be modeled through surface normal vector analysis. The issue of visual occlusion can be addressed by adjusting the circle&#x02019;s radius.</p><fig position="float" id="F5"><label>FIGURE 5</label><caption><p>Attitude modeling at the target point.</p></caption><graphic xlink:href="fbioe-13-1550875-g005" position="float"/></fig></sec><sec id="s2-1-4"><title>2.1.4 Hand-eye calibration</title><p>The camera-robot calibration paradigm is conventionally categorized into Eye-in-Hand and Eye-to-hand configurations. In the Eye-in-hand system, the sensor is mounted on the robotic end-effector, enabling complete observation of the target workspace and facilitating precise visual servo control. This configuration, however, presents inherent challenges including motion-induced image degradation and compounded error propagation from kinematic inaccuracies during calibration procedures. Conversely, the Eye-to-hand system positions the camera in a stationary configuration external to the robotic manipulator, thereby eliminating motion artifacts while achieving extended spatial coverage encompassing both the target domain and manipulator workspace. This arrangement introduces potential visual occlusion risks.</p><p>Considering operational reliability and environmental factors such as humidity resistance requirements, the solution adopted an Eye-to-hand configuration. During hand-eye calibration, the D455 was mounted on the robot&#x02019;s base, and the calibration was performed using the ROS package <italic>easy_handeye</italic> and an ArUco board attached to the robot&#x02019;s end-effector. The ArUco board was generated by the ArUco markers generator with Marker ID 582 and Marker size 100&#x000a0;mm. The process provided the translation vector components <italic>t</italic>
<sub>
<italic>x</italic>
</sub>&#x03001;<italic>t</italic>
<sub>
<italic>y</italic>
</sub>, and <italic>t</italic>
<sub>
<italic>z</italic>
</sub>, along with the quaternion components <italic>x</italic>&#x03001;<italic>y</italic>&#x03001;<italic>z</italic>, and <italic>w</italic> representing the orientation. The 3D position and pose of the target point were then transformed from the RGB camera coordinate system to the robot&#x02019;s base coordinate system (<xref rid="B4" ref-type="bibr">Ding et al., 2024</xref>). Subsequently, inverse kinematics was applied to compute the joint angles, enabling precise positioning and contact with the skin.</p></sec></sec><sec id="s2-2"><title>2.2 Post-contact adaptive wiping</title><p>When the end effector establishes contact with the target skin area, we must plan a wiping trajectory to accommodate the curvature of the human skin.</p><sec id="s2-2-1"><title>2.2.1 Point cloud-based optimization of scrubbing trajectories and normal vector calculation</title><p>Cubic B-spline interpolation is utilized to optimize the point cloud trajectory, ensuring a smooth wiping path. B-spline curves have been extensively applied in trajectory fitting and discretization. For a given degree k, a k-degree B-spline curve <italic>p</italic>(<italic>&#x003bc;</italic>) is defined by <xref rid="e6" ref-type="disp-formula">Equation 6</xref>, where <italic>U</italic> = {<italic>&#x003bc;</italic>
<sub>
<italic>0</italic>
</sub>, <italic>&#x003bc;</italic>
<sub>
<italic>1</italic>
</sub>, &#x02026; , <italic>&#x003bc;</italic>
<sub>
<italic>k</italic>
</sub>, <italic>&#x003bc;</italic>
<sub>
<italic>k+1</italic>
</sub>, &#x02026; , <italic>&#x003bc;</italic>
<sub>
<italic>m-k-1</italic>
</sub>, <italic>&#x003bc;</italic>
<sub>
<italic>m-k</italic>
</sub>, &#x02026; , <italic>&#x003bc;</italic>
<sub>
<italic>m</italic>
</sub>} represents the knot vector, and <italic>N</italic>
<sub>
<italic>j,k</italic>
</sub>(<italic>&#x003bc;</italic>) denotes the j-th k-degree B-spline basis function, as expressed in <xref rid="e7" ref-type="disp-formula">Equation 7</xref> (<xref rid="B9" ref-type="bibr">Guo et al., 2024</xref>). Using the deBoor algorithm (<xref rid="B21" ref-type="bibr">Kong et al., 2016</xref>; <xref rid="B34" ref-type="bibr">Wang et al., 2019</xref>), we derive <xref rid="e8" ref-type="disp-formula">Equation 8</xref>. The arc length is approximated through step size, yielding <xref rid="e9" ref-type="disp-formula">Equation 9</xref> where &#x02206;<italic>&#x003bc;</italic>
<sub>
<italic>i</italic>
</sub>
<sup>
<italic>P</italic>
</sup> represents the parameter increment for the <italic>i</italic>-th interpolation step (<italic>i</italic> = 1, 2, 3, &#x02026; , <italic>n</italic>), and <italic>l</italic> denotes the interpolation step length. The <italic>&#x003bc;</italic>-value corresponding to the <italic>i</italic>-th interpolation point can be calculated through <xref rid="e10" ref-type="disp-formula">Equation 10</xref>.<disp-formula id="e6">
<mml:math id="m6" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>,</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math>
<label>(6)</label>
</disp-formula>
<disp-formula id="e7">
<mml:math id="m7" overflow="scroll"><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>,</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mn mathvariant="bold">1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="bold">1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mfrac><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math>
<label>(7)</label>
</disp-formula>
<disp-formula id="e8">
<mml:math id="m8" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfrac><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mfrac><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="bold">1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
<label>(8)</label>
</disp-formula>
<disp-formula id="e9">
<mml:math id="m9" overflow="scroll"><mml:mrow><mml:mo>&#x02206;</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mfrac><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mn mathvariant="bold">2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mfrac><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mn mathvariant="bold">2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mfrac><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mn mathvariant="bold">2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math>
<label>(9)</label>
</disp-formula>
<disp-formula id="e10">
<mml:math id="m10" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:munderover></mml:mstyle><mml:mo>&#x02206;</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi></mml:msubsup></mml:mrow></mml:math>
<label>(10)</label>
</disp-formula>
</p><p>To establish topological relationships between points in the cloud, we employ approximate triangular tessellation to generate a triangular mesh, as illustrated in <xref rid="F6" ref-type="fig">Figure 6</xref>. Let {<italic>l</italic>
<sub>
<italic>1</italic>
</sub>,<italic>l</italic>
<sub>
<italic>2</italic>
</sub>, &#x02026; ,<italic>l</italic>
<sub>
<italic>n</italic>
</sub>} represent the set of wiping paths, where <italic>P</italic>
<sub>
<italic>i,j</italic>
</sub> denotes a wiping point. <italic>P</italic>
<sub>
<italic>i,j-1</italic>
</sub> and <italic>P</italic>
<sub>
<italic>i,j+1</italic>
</sub>, located on the same scan line as <italic>P</italic>
<sub>
<italic>i,j</italic>
</sub>, and <italic>P</italic>
<sub>
<italic>i-1,j</italic>
</sub> and <italic>P</italic>
<sub>
<italic>i+1,j</italic>
</sub>, which are the closest points to <italic>P</italic>
<sub>
<italic>i,j</italic>
</sub> on adjacent scan lines, are identified. Using these points, four triangles are constructed with the following vertices: {<italic>P</italic>
<sub>
<italic>i,j</italic>
</sub>,<italic>P</italic>
<sub>
<italic>i,j+1</italic>
</sub>,<italic>P</italic>
<sub>
<italic>i-1,j</italic>
</sub>}, {<italic>P</italic>
<sub>
<italic>i,j</italic>
</sub>,<italic>P</italic>
<sub>
<italic>i-1,j</italic>
</sub>,<italic>P</italic>
<sub>
<italic>i,j-1</italic>
</sub>}, and {<italic>P</italic>
<sub>
<italic>i,j</italic>
</sub>,<italic>P</italic>
<sub>
<italic>i+1,j</italic>
</sub>,<italic>P</italic>
<sub>
<italic>i,j+1</italic>
</sub>}. The coordinates of the triangle vertices are denoted as (<italic>P</italic>
<sub>
<italic>x1</italic>
</sub>, <italic>P</italic>
<sub>
<italic>y1</italic>
</sub>, <italic>P</italic>
<sub>
<italic>z1</italic>
</sub>), (<italic>P</italic>
<sub>
<italic>x2</italic>
</sub>, <italic>P</italic>
<sub>
<italic>y2</italic>
</sub>, <italic>P</italic>
<sub>
<italic>z2</italic>
</sub>), and (<italic>P</italic>
<sub>
<italic>x3</italic>
</sub>, <italic>P</italic>
<sub>
<italic>y3</italic>
</sub>, <italic>P</italic>
<sub>
<italic>z3</italic>
</sub>), and the method for calculating the normal vector is given by <xref rid="e11" ref-type="disp-formula">Equations 11</xref>, <xref rid="e12" ref-type="disp-formula">12</xref>, <xref rid="e13" ref-type="disp-formula">13</xref>. For the shared vertex <italic>P</italic>
<sub>
<italic>i,j</italic>
</sub>, the normal vector <italic>n</italic>
<sub>
<italic>i,j</italic>
</sub> is approximated as the weighted average of the normal vectors of the four triangles, with the area of each triangle serving as the weight. For boundary points, the normal vector is calculated by taking the weighted average of the normal vectors of the two adjacent triangles that share the vertex.<disp-formula id="e11">
<mml:math id="m11" overflow="scroll"><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>&#x000b4;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mn mathvariant="bold">3</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn mathvariant="bold">3</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">y</mml:mi></mml:msub><mml:mo>&#x000b4;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn mathvariant="bold">3</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mn mathvariant="bold">3</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi></mml:msub><mml:mo>&#x000b4;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn mathvariant="bold">3</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn mathvariant="bold">3</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math>
<label>(11)</label>
</disp-formula>
<disp-formula id="e12">
<mml:math id="m12" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b4;</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>&#x000b4;</mml:mo></mml:mover><mml:mn mathvariant="bold">2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">y</mml:mi></mml:msub><mml:mo>&#x000b4;</mml:mo></mml:mover><mml:mn mathvariant="bold">2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi></mml:msub><mml:mo>&#x000b4;</mml:mo></mml:mover><mml:mn mathvariant="bold">2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math>
<label>(12)</label>
</disp-formula>
<disp-formula id="e13">
<mml:math id="m13" overflow="scroll"><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mo>&#x000b4;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b4;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">y</mml:mi></mml:msub><mml:mo>&#x000b4;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b4;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi></mml:msub><mml:mo>&#x000b4;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b4;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math>
<label>(13)</label>
</disp-formula>
</p><fig position="float" id="F6"><label>FIGURE 6</label><caption><p>Topological relationships between points.</p></caption><graphic xlink:href="fbioe-13-1550875-g006" position="float"/></fig></sec><sec id="s2-2-2"><title>2.2.2 Real-time trajectory planning</title><p>When the wiping path becomes lengthy, the increasing number of interpolation and fitting points elevates computational load, thereby degrading real-time performance. Furthermore, unexpected human movements during operation may alter the wiping path. To resolve these issues, we propose a segmented interpolation real-time planning method as illustrated in <xref rid="F7" ref-type="fig">Figure 7</xref>. This method divides the wiping trajectory into several segments and performs interpolation for each segment sequentially. The interpolation points are stored in a First-In-First-Out (FIFO) queue, where the control function retrieves points to guide the robot&#x02019;s motion. With this approach, the robot can initiate movement after first-segment interpolation. Both B-spline interpolation and motion guidance processes are executed in separate threads for concurrent execution. This method not only facilitates real-time trajectory planning but also enhances adaptability to unexpected movements. If unforeseen movements occur during the robot&#x02019;s wiping process, real-time adjustments can be made during the fitting of the next trajectory segment. Increased segmentation granularity enhances robustness against unexpected human motions.</p><fig position="float" id="F7"><label>FIGURE 7</label><caption><p>First-in-first-out queue for interpolated points.</p></caption><graphic xlink:href="fbioe-13-1550875-g007" position="float"/></fig></sec><sec id="s2-2-3"><title>2.2.3 Updating the pose of the scrubbing head</title><p>The pose of the wiping head must be updated in real-time according to the dynamic changes in the wiping trajectory, which ensures proper contact with the target surface, thereby improving wiping efficiency and maintaining stable contact. As shown in <xref rid="F8" ref-type="fig">Figure 8</xref>, the pose at the scrubbing point P is [<italic>p</italic>
<sub>
<italic>x</italic>
</sub>, <italic>p</italic>
<sub>
<italic>y</italic>
</sub>, <italic>p</italic>
<sub>
<italic>z</italic>
</sub>, <italic>&#x003c6;</italic>
<sub>
<italic>x</italic>
</sub>, <italic>&#x003c6;</italic>
<sub>
<italic>y</italic>
</sub>, <italic>&#x003c6;</italic>
<sub>
<italic>z</italic>
</sub>], and the scrubbing head vector is K = [-<italic>a</italic>
<sub>
<italic>x</italic>
</sub>, -<italic>a</italic>
<sub>
<italic>y</italic>
</sub>, -<italic>a</italic>
<sub>
<italic>z</italic>
</sub>]. Let the positive direction of the I-axis represent the tangential direction at P, with the tangential vector defined as <italic>I</italic> = [<italic>o</italic>
<sub>
<italic>x</italic>
</sub>, <italic>o</italic>
<sub>
<italic>y</italic>
</sub>, <italic>o</italic>
<sub>
<italic>z</italic>
</sub>]. The J-axis is the cross product of K and I, expressed as <italic>J</italic> = [<italic>m</italic>
<sub>
<italic>x</italic>
</sub>, <italic>m</italic>
<sub>
<italic>y</italic>
</sub>, <italic>m</italic>
<sub>
<italic>z</italic>
</sub>]. The transformation matrix <sup>
<italic>B</italic>
</sup>
<italic>P</italic>
<sub>
<italic>H</italic>
</sub> from the tool coordinate frame (defined by I, J, K) to the robot base coordinate frame is given by <xref rid="e14" ref-type="disp-formula">Equation 14</xref>. The scrubbing pose is transformed into the end-effector pose of the robot, as shown in <xref rid="e15" ref-type="disp-formula">Equation 15</xref>. Here, <sup>
<italic>B</italic>
</sup>
<sub>
<italic>E</italic>
</sub>
<italic>T</italic>
<sub>
<italic>P</italic>
</sub> is the pose matrix of the robot during scrubbing, <sup>
<italic>H</italic>
</sup>
<sub>
<italic>P</italic>
</sub>
<italic>T</italic> denotes the transformation matrix from the scrubbing tool coordinate frame to the local coordinate frame of the scrubbing point, and <sup>
<italic>E</italic>
</sup>
<sub>
<italic>H</italic>
</sub>
<italic>T</italic> is the transformation matrix from the tool coordinate frame to the end flange coordinate frame. During scrubbing, the tool coordinate frame coincides with the local coordinate frame of the scrubbing point, i.e., <sup>
<italic>H</italic>
</sup>
<sub>
<italic>P</italic>
</sub>
<italic>T</italic> = <italic>I</italic>, which is the identity matrix. The length of the scrubbing head d = 10&#x000a0;cm; thus, <sup>
<italic>E</italic>
</sup>
<sub>
<italic>H</italic>
</sub>
<italic>T</italic> is defined as shown in <xref rid="e16" ref-type="disp-formula">Equation 16</xref>. Consequently, the robot&#x02019;s pose matrix is expressed in <xref rid="e17" ref-type="disp-formula">Equation 17</xref>. By integrating the inverse kinematics model, the joint angles for the robot are calculated.<disp-formula id="e14">
<mml:math id="m14" overflow="scroll"><mml:mrow><mml:mmultiscripts><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">H</mml:mi><mml:none/><mml:mprescripts/><mml:none/><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mmultiscripts><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">y</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mi mathvariant="bold-italic">y</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">y</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">y</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd><mml:mtd><mml:mn mathvariant="bold">1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
<label>(14)</label>
</disp-formula>
<disp-formula id="e15">
<mml:math id="m15" overflow="scroll"><mml:mrow><mml:mmultiscripts><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi><mml:none/><mml:mprescripts/><mml:mi mathvariant="bold-italic">E</mml:mi><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mmultiscripts><mml:mmultiscripts><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mprescripts/><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mmultiscripts><mml:mmultiscripts><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mprescripts/><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mmultiscripts><mml:mo>=</mml:mo><mml:mmultiscripts><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">H</mml:mi><mml:none/><mml:mprescripts/><mml:none/><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mmultiscripts></mml:mrow></mml:math>
<label>(15)</label>
</disp-formula>
<disp-formula id="e16">
<mml:math id="m16" overflow="scroll"><mml:mrow><mml:mmultiscripts><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mprescripts/><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mmultiscripts><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mn mathvariant="bold">1</mml:mn></mml:mtd><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd><mml:mtd><mml:mn mathvariant="bold">1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mn mathvariant="bold">1</mml:mn></mml:mtd><mml:mtd><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd><mml:mtd><mml:mn mathvariant="bold">1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
<label>(16)</label>
</disp-formula>
<disp-formula id="e17">
<mml:math id="m17" overflow="scroll"><mml:mrow><mml:mmultiscripts><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi><mml:none/><mml:mprescripts/><mml:mi mathvariant="bold-italic">E</mml:mi><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mmultiscripts><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mmultiscripts><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">H</mml:mi><mml:none/><mml:mprescripts/><mml:none/><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mmultiscripts><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mmultiscripts><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mprescripts/><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mmultiscripts></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>
<label>(17)</label>
</disp-formula>
</p><fig position="float" id="F8"><label>FIGURE 8</label><caption><p>Scrubbing schematic.</p></caption><graphic xlink:href="fbioe-13-1550875-g008" position="float"/></fig><p>The angles <italic>&#x003b8;</italic>
<sub>
<italic>i</italic>
</sub>, formed by adjacent vectors <bold>
<italic>P</italic>
</bold>
<sub>
<bold>
<italic>i-1</italic>
</bold>
</sub>
<bold>
<italic>P</italic>
</bold>
<sub>
<bold>
<italic>i</italic>
</bold>
</sub> and <bold>
<italic>P</italic>
</bold>
<sub>
<bold>
<italic>i</italic>
</bold>
</sub>
<bold>
<italic>P</italic>
</bold>
<sub>
<bold>
<italic>i+1</italic>
</bold>
</sub> within the trajectory point sets {<italic>P</italic>
<sub>
<italic>i,1</italic>
</sub>,<italic>P</italic>
<sub>
<italic>i,2</italic>
</sub>, &#x02026; , <italic>P</italic>
<sub>
<italic>i,n</italic>
</sub>}, are calculated. These angles represent the rotation angle of the scrubbing head in the base coordinate system, denoted as <italic>&#x003b8;</italic>
<sub>
<italic>z</italic>
</sub> = <italic>&#x003b8;</italic>
<sub>
<italic>i</italic>
</sub>. The calculation method for <italic>&#x003b8;</italic>
<sub>
<italic>i</italic>
</sub> is shown in <xref rid="e18" ref-type="disp-formula">Equation 18</xref>. To reduce computational complexity, a threshold angle <italic>&#x003b8;</italic> = 6&#x000b0;is set: if <italic>&#x003b8;</italic>
<sub>
<italic>z</italic>
</sub>&#x0003c;<italic>&#x003b8;</italic>, no adjustment is made to the scrubbing head's pose. The threshold value is determined through extensive scrubbing experiments. When <italic>&#x003b8;</italic>
<sub>
<italic>i</italic>
</sub> is below 6&#x000b0;, indicating minimal contour variation, no pose adjustment is required. This approach reduces computation while ensuring task completion.<disp-formula id="e18">
<mml:math id="m18" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">cos</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold">1</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="bold">2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
<label>(18)</label>
</disp-formula>
</p></sec></sec></sec><sec sec-type="results" id="s3"><title>3 Results</title><sec id="s3-1"><title>3.1 Multi-region skin detection</title><p>Upon completion of training, YOLOv5s had a compact model size of 27&#x000a0;MB, yet achieved a mAP of 90%. When deployed on the TX2 platform, YOLOv5s demonstrated a frame rate of 42 frames per second (FPS), enabling real-time detection performance.</p><p>The robustness test results are shown in <xref rid="F9" ref-type="fig">Figure 9</xref>. YOLOv5s demonstrates a certain level of robustness to variations in skin tone, humidity, lighting, occlusion, and scene conditions. As shown in panel (A), the model accurately detects skin across fair, medium, and dark tones. Panel (B) illustrates the application of image processing techniques on bathing scene images containing human models, simulating the humid environment commonly encountered during bathing. In such conditions, YOLOv5s still accurately detects the skin across various body regions. Panels (C) (D), and (E) further demonstrate the model&#x02019;s robustness to occlusion, lighting variations, and different scene contexts.</p><fig position="float" id="F9"><label>FIGURE 9</label><caption><p>Robustness testing of the model <bold>(A)</bold> Detection results across different skin tones <bold>(B)</bold> Detection results in different humidity scenarios <bold>(C)</bold> Inference results under occlusion <bold>(D)</bold> Inference results under different lighting conditions <bold>(E)</bold> Inference results across different scenarios.</p></caption><graphic xlink:href="fbioe-13-1550875-g009" position="float"/></fig></sec><sec id="s3-2"><title>3.2 Localization experiments</title><p>Considering that the center of the detection box was most likely to be located within the skin region, the center point was selected as the target point. The actual position in the base coordinate system was determined using skin detection, depth information, camera intrinsic parameters, and the hand-eye matrix. The ideal position was defined as the location data displayed on the teach pendant when the six-dimensional force sensor was adjusted to zero. The human model&#x02019;s position was adjusted, and eight localization tests were conducted. Localization errors were computed based on <xref rid="e19" ref-type="disp-formula">Equation 19</xref>, where the subscript <italic>l</italic> represents the theoretical position and <italic>s</italic> represents the actual position. The error along each of the three axes was taken as the absolute difference between the actual and theoretical positions. The impact of the depth correction algorithm on localization performance was evaluated by comparing the differences between the actual and ideal depth data. The actual data was obtained from the teach pendant reading after the robot&#x02019;s localization, while the ideal data corresponded to the teach pendant reading when the six-dimensional force sensor was set to zero. The error was calculated as the absolute difference between these two readings.<disp-formula id="e19">
<mml:math id="m19" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b5;</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mn mathvariant="bold">2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mn mathvariant="bold">2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mn mathvariant="bold">2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math>
<label>(19)</label>
</disp-formula>
</p><p>The results were presented in <xref rid="T2" ref-type="table">Table 2</xref>. The robotic arm&#x02019;s localization error was relatively small, with a maximum value of 2.421&#x000a0;mm, a minimum value of 2.081&#x000a0;mm, and an average of 2.186&#x000a0;mm. The average errors along the x, y, and z-axes were 1.144&#x000a0;mm, 1.368&#x000a0;mm, and 1.111&#x000a0;mm, respectively. Before and after depth correction, the mean errors were 18.329&#x000a0;mm and 1.111&#x000a0;mm, respectively. These results indicated that the localization error and the errors along the three axes were minimal, demonstrating high localization accuracy. Moreover, depth correction was critical for achieving high-precision localization.</p><table-wrap position="float" id="T2"><label>TABLE 2</label><caption><p>Results of the positioning experiment.</p></caption><table frame="hsides" rules="groups"><thead valign="top"><tr><th align="center" rowspan="1" colspan="1">No.</th><th align="center" rowspan="1" colspan="1">Theoretical position (mm)</th><th align="center" rowspan="1" colspan="1">Actual position (mm)</th><th align="center" rowspan="1" colspan="1">Positioning error (mm)</th><th align="center" rowspan="1" colspan="1">
<italic>X</italic>-axis error (mm)</th><th align="center" rowspan="1" colspan="1">
<italic>Y</italic>-axis error (mm)</th><th align="center" rowspan="1" colspan="1">
<italic>Z</italic>-axis error (mm)</th><th align="center" rowspan="1" colspan="1">Pre-correction depth error (mm)</th></tr></thead><tbody valign="top"><tr><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">(251.04, 375.06, 763.69)</td><td align="center" rowspan="1" colspan="1">(252.15, 376.14, 765.15)</td><td align="center" rowspan="1" colspan="1">2.128</td><td align="center" rowspan="1" colspan="1">1.11</td><td align="center" rowspan="1" colspan="1">1.08</td><td align="center" rowspan="1" colspan="1">1.46</td><td align="center" rowspan="1" colspan="1">21.02</td></tr><tr><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">(292.02, 272.92, 833.79)</td><td align="center" rowspan="1" colspan="1">(292.91, 271.60, 832.45)</td><td align="center" rowspan="1" colspan="1">2.081</td><td align="center" rowspan="1" colspan="1">0.89</td><td align="center" rowspan="1" colspan="1">1.32</td><td align="center" rowspan="1" colspan="1">1.34</td><td align="center" rowspan="1" colspan="1">19.35</td></tr><tr><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">(242.42, 162.88, 725.62)</td><td align="center" rowspan="1" colspan="1">(241.38, 164.74, 725.55)</td><td align="center" rowspan="1" colspan="1">2.132</td><td align="center" rowspan="1" colspan="1">1.04</td><td align="center" rowspan="1" colspan="1">1.86</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">14.87</td></tr><tr><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">(246.15, 344.01, 631.70)</td><td align="center" rowspan="1" colspan="1">(247.87, 345.25, 632.40)</td><td align="center" rowspan="1" colspan="1">2.233</td><td align="center" rowspan="1" colspan="1">1.72</td><td align="center" rowspan="1" colspan="1">1.24</td><td align="center" rowspan="1" colspan="1">0.7</td><td align="center" rowspan="1" colspan="1">14.45</td></tr><tr><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">(246.56, 344.50, 631.16)</td><td align="center" rowspan="1" colspan="1">(247.62, 343.40, 632.66)</td><td align="center" rowspan="1" colspan="1">2.141</td><td align="center" rowspan="1" colspan="1">1.06</td><td align="center" rowspan="1" colspan="1">1.1</td><td align="center" rowspan="1" colspan="1">1.5</td><td align="center" rowspan="1" colspan="1">16.34</td></tr><tr><td align="center" rowspan="1" colspan="1">6</td><td align="center" rowspan="1" colspan="1">(328.02, 148.08, 855.10)</td><td align="center" rowspan="1" colspan="1">(329.41, 149.25, 856.70)</td><td align="center" rowspan="1" colspan="1">2.421</td><td align="center" rowspan="1" colspan="1">1.39</td><td align="center" rowspan="1" colspan="1">1.17</td><td align="center" rowspan="1" colspan="1">1.6</td><td align="center" rowspan="1" colspan="1">24.34</td></tr><tr><td align="center" rowspan="1" colspan="1">7</td><td align="center" rowspan="1" colspan="1">(253.3, 373.52, 762.91)</td><td align="center" rowspan="1" colspan="1">(254.10, 375.28, 764.01)</td><td align="center" rowspan="1" colspan="1">2.224</td><td align="center" rowspan="1" colspan="1">0.8</td><td align="center" rowspan="1" colspan="1">1.76</td><td align="center" rowspan="1" colspan="1">1.1</td><td align="center" rowspan="1" colspan="1">17.28</td></tr><tr><td align="center" rowspan="1" colspan="1">8</td><td align="center" rowspan="1" colspan="1">(327.11, 147.31, 854.44)</td><td align="center" rowspan="1" colspan="1">(328.25, 148.72, 853.32)</td><td align="center" rowspan="1" colspan="1">2.131</td><td align="center" rowspan="1" colspan="1">1.14</td><td align="center" rowspan="1" colspan="1">1.41</td><td align="center" rowspan="1" colspan="1">1.12</td><td align="center" rowspan="1" colspan="1">18.98</td></tr></tbody></table></table-wrap><p>The proposed depth correction methodology utilized a multi-modal dataset comprising synchronized RGB images, depth maps, and 3D point clouds. A comparative analysis was conducted against the framework presented in the literature (<xref rid="B12" ref-type="bibr">Herrera et al., 2012</xref>), which similarly employed a checkerboard-based correction paradigm and the same dataset. The experimental results demonstrated comparable correction accuracy between both methods at proximal ranges (&#x0003c;0.3m). However, the proposed algorithm exhibited superior performance at distal ranges (&#x0003e;0.9m). This enhanced long-range accuracy was attributed to the iterative optimization framework, which provided improved initial parameter estimation through successive approximation, thereby reducing error propagation in depth correction.</p></sec><sec id="s3-3"><title>3.3 Scrubbing experiment</title><p>The experiment was conducted to quantitatively evaluate the system&#x02019;s performance during simulated and executed scrubbing tasks. It focused on the system&#x02019;s ability to ensure the scrubbing head maintained the correct trajectory while moving along the target area.</p><p>The scrubbing trajectory was sampled along the end-effector&#x02019;s x-axis with a point spacing of 2&#x000a0;mm to complete the adaptive scrubbing task. By comparing the actual movement of the scrubbing head with the desired trajectory, the trajectory error in the y-axis was obtained, as shown in <xref rid="F10" ref-type="fig">Figure 10</xref>. The maximum error was 2.23&#x000a0;mm. Given that the adjustable length of the scrubbing head&#x02019;s brush was 10&#x000a0;mm, the error did not affect the fitting of the scrubbing head to the human skin surface.</p><fig position="float" id="F10"><label>FIGURE 10</label><caption><p>The trajectory error in the <italic>y</italic>-axis.</p></caption><graphic xlink:href="fbioe-13-1550875-g010" position="float"/></fig></sec></sec><sec sec-type="discussion" id="s4"><title>4 Discussion</title><p>Visual perception was frequently employed in robotic perception systems; however, the involvement of skin detection could raise privacy concerns among users. To address these concerns, the following privacy protection strategies were implemented based on practical requirements: 1) A color image encryption scheme based on vector representation was adopted; 2) Noise, such as Gaussian noise and salt-and-pepper noise, was added to the image to obfuscate private information; 3) Data processing occurred exclusively on the local embedded platform, ensuring that no data was transmitted to the cloud for processing; 4) During operation, the visual scenes captured by the sensor were not displayed, and only topics published via ROS were output; 5) Due to the limited storage capacity of the embedded platform, visual data in the bathing scenario was not stored; 6) The system was not connected to the internet, eliminating data transmission risks.</p><p>In this study, the six-dimensional force sensor played a crucial role in ensuring safety during emergencies. When the detected force exceeded a predefined threshold, the robot retracted the end-effector and returned to its initial pose. Achieving precise force control typically required more complex control algorithms, particularly when adjustments to all three forces and moments were necessary. However, by utilizing the high-precision distance information fromdepth correction in this study, combined with the safety guarantees provided by the force sensor, the adaptive scrubbing task could be completed without relying on complex control algorithms. The innovation of this work lay primarily in developing an algorithm for enhancing depth quality, which demonstrated excellent performance in both localization and scrubbing. Additionally, the study contributed by creating skin detection datasets, providing valuable data support for related research.</p><p>The biological characteristics of the human model differed from the skin properties of real human subjects. Future research will involve experiments on real human participants under ethically approved conditions. Additionally, the current system employed a seated posture with a collaborative robot for bathing tasks, which did not allow effective cleaning of the buttocks region. In future developments, a stand-to-sit posture transition mechanism could be designed to enable buttocks cleaning in a standing position. Furthermore, a user intent recognition module could be incorporated to fully account for the preferences of users, thereby enhancing the system&#x02019;s intelligence and human-robot interaction experience.</p><p>The proposed system demonstrated significant advancements over conventional bathing systems through three key innovations. First, it incorporated a visual perception module to enhance operational intelligence. Second, compared with existing vision-enabled systems, it achieved superior depth measurement accuracy and utilized precise depth information for reliable positioning and scrubbing. Third, the implementation of a motorized rotating seat mechanism reduced the number of required vision sensors while ensuring complete body coverage.</p></sec><sec sec-type="conclusion" id="s5"><title>5 Conclusion</title><p>Acquiring depth information was a critical step for enabling accurate robotic target localization. The intelligent bathing assistance system employed an RGB-D camera to collect depth data and utilized deep learning techniques to detect different skin regions in RGB images, enabling an intelligent selection of bathing modes. The system demonstrated robustness against variations in skin tone, lighting, and humidity. This study proposed a depth correction algorithm to achieve high-precision target localization. During contact, the system did not rely on constant-force control; instead, it leveraged high-accuracy visual data and an adjustable-length scrubbing head to implement strategies including scrubbing trajectory optimization, normal vector estimation, segmented interpolation for real-time planning, and end-effector pose updates. The system&#x02019;s high-precision localization and adaptive scrubbing capabilities were primarily attributed to the precise visual information obtained through depth correction, eliminating the need for complex force control algorithms.</p></sec></body><back><ack><p>The authors appreciatively acknowledge the support from the high-performance computing center of the University of Shanghai for Science and Technology.</p></ack><sec sec-type="data-availability" id="s6"><title>Data availability statement</title><p>The datasets for this study can be found in <ext-link xlink:href="https://doi.org/10.6084/m9.figshare.21282396.v1" ext-link-type="uri">https://doi.org/10.6084/m9.figshare.21282396.v1</ext-link> [FigShare].</p></sec><sec sec-type="ethics-statement" id="s7"><title>Ethics statement</title><p>Written informed consent was not obtained from the individual(s) for the publication of any potentially identifiable images or data included in this article because The images are all collected from the web and are publicly available data on the web.</p></sec><sec sec-type="author-contributions" id="s8"><title>Author contributions</title><p>PL: Formal Analysis, Methodology, Project administration, Writing &#x02013; original draft. SF: Data curation, Validation, Writing &#x02013; original draft. HY: Project administration, Supervision, Writing &#x02013; review and editing.</p></sec><sec sec-type="COI-statement" id="s10"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="ai-statement" id="s11"><title>Generative AI statement</title><p>The author(s) declare that no Generative AI was used in the creation of this manuscript.</p></sec><sec sec-type="disclaimer" id="s12"><title>Publisher&#x02019;s note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Alenya</surname><given-names>G.</given-names></name><name><surname>Foix</surname><given-names>S.</given-names></name><name><surname>Torras</surname><given-names>C.</given-names></name></person-group> (<year>2014</year>). <article-title>Using ToF and RGBD cameras for 3D robot perception and manipulation in human environments</article-title>. <source>Intel. Serv. Robot.</source>
<volume>7</volume> (<issue>4</issue>), <fpage>211</fpage>&#x02013;<lpage>220</lpage>. <pub-id pub-id-type="doi">10.1007/s11370-014-0159-5</pub-id>
</mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Luo</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>F.</given-names></name><name><surname>Tian</surname><given-names>Z.</given-names></name></person-group> (<year>2024</year>). <article-title>A review of object detection: datasets, performance evaluation, architecture, applications and current trends</article-title>. <source>Multimed. Tools Appl.</source>
<volume>83</volume>, <fpage>65603</fpage>&#x02013;<lpage>65661</lpage>. <pub-id pub-id-type="doi">10.1007/s11042-023-17949-4</pub-id>
</mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>D'Avella</surname><given-names>S.</given-names></name><name><surname>Avizzano</surname><given-names>C. A.</given-names></name><name><surname>Tripiccho</surname><given-names>P.</given-names></name></person-group> (<year>2023</year>). <article-title>ROS-industrial based robotic cell for industry 4.0: eye-in-hand stereo camera and visual servoing for flexible, fast, and accurate picking and hooking in the line</article-title>. <source>Robot. Cim-Int Manuf.</source>
<volume>80</volume>, <fpage>102453</fpage>. <pub-id pub-id-type="doi">10.1016/j.rcim.2022.102453</pub-id>
</mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ding</surname><given-names>Z.</given-names></name><name><surname>Lv</surname><given-names>Y.</given-names></name><name><surname>Kong</surname><given-names>L.</given-names></name><name><surname>Dong</surname><given-names>Z.</given-names></name><name><surname>Zheng</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2024</year>). <article-title>Flexible calibration method and application of passenger car HUD detection based on collaborative robot</article-title>. <source>IEEE Access</source>
<volume>12</volume>, <fpage>35399</fpage>&#x02013;<lpage>35409</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2024.3350631</pub-id>
</mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Fang</surname><given-names>J. Y.</given-names></name><name><surname>Meng</surname><given-names>J. B.</given-names></name><name><surname>Liu</surname><given-names>X. S.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Qi</surname><given-names>P.</given-names></name><name><surname>Wei</surname><given-names>C. C.</given-names></name></person-group> (<year>2022</year>). <article-title>Single-target detection of oncomelania hupensis based on improved YOLOv5s</article-title>. <source>Front. Bioeng. Biotech.</source>
<volume>10</volume>, <fpage>861079</fpage>. <pub-id pub-id-type="doi">10.3389/fbioe.2022.861079</pub-id>
</mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Fryskowska</surname><given-names>A.</given-names></name></person-group> (<year>2019</year>). <article-title>An improvement in the identification of the centres of checkerboard targets in point clouds using terrestrial laser scanning</article-title>. <source>Sensors-Basel</source>
<volume>19</volume> (<issue>4</issue>), <fpage>938</fpage>. <pub-id pub-id-type="doi">10.3390/s19040938</pub-id>
<pub-id pub-id-type="pmid">30813356</pub-id>
</mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Fu</surname><given-names>L. S.</given-names></name><name><surname>Majeed</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Karkee</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>Q.</given-names></name></person-group> (<year>2020</year>). <article-title>Faster R-CNN-based apple detection in dense-foliage fruiting-wall trees using RGB and depth features for robotic harvesting</article-title>. <source>Biosyst. Eng.</source>
<volume>197</volume>, <fpage>245</fpage>&#x02013;<lpage>256</lpage>. <pub-id pub-id-type="doi">10.1016/j.biosystemseng.2020.07.007</pub-id>
</mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Geng</surname><given-names>X.</given-names></name><name><surname>Su</surname><given-names>Y.</given-names></name><name><surname>Cao</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name></person-group> (<year>2024</year>). <article-title>YOLOFM: an improved fire and smoke object detection algorithm based on YOLOv5n</article-title>. <source>Sci. Rep-Uk</source>
<volume>14</volume> (<issue>1</issue>), <fpage>4543</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-024-55232-0</pub-id>
</mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Guo</surname><given-names>W.</given-names></name><name><surname>Huang</surname><given-names>X.</given-names></name><name><surname>Qi</surname><given-names>B.</given-names></name><name><surname>Ren</surname><given-names>X.</given-names></name><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Chen</surname><given-names>X.</given-names></name></person-group> (<year>2024</year>). <article-title>Vision-guided path planning and joint configuration optimization for robot grinding of spatial surface weld beads via point cloud</article-title>. <source>Adv. Eng. Inf.</source>
<volume>61</volume>, <fpage>102465</fpage>. <pub-id pub-id-type="doi">10.1016/j.aei.2024.102465</pub-id>
</mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Gupta</surname><given-names>H.</given-names></name><name><surname>Lilienthal</surname><given-names>A. J.</given-names></name><name><surname>Andreasson</surname><given-names>H.</given-names></name><name><surname>Kurtser</surname><given-names>P.</given-names></name></person-group> (<year>2023</year>). <article-title>NDT-6D for color registration in agri-robotic applications</article-title>. <source>J. Field Robot.</source>
<volume>40</volume> (<issue>6</issue>), <fpage>1603</fpage>&#x02013;<lpage>1619</lpage>. <pub-id pub-id-type="doi">10.1002/rob.22194</pub-id>
</mixed-citation></ref><ref id="B11"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>He</surname><given-names>Z.</given-names></name><name><surname>Yuan</surname><given-names>F.</given-names></name><name><surname>Chen</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>M.</given-names></name></person-group> (<year>2019</year>). &#x0201c;<article-title>Ergonomic design of multi-functional bathing robot</article-title>,&#x0201d; in <conf-name>IEEE International Conference on Real-time Computing and Robotics (RCAR)</conf-name>, <conf-loc>Irkutsk, Russia</conf-loc>, <conf-date>August, 2019</conf-date>, <fpage>580</fpage>&#x02013;<lpage>585</lpage>. <pub-id pub-id-type="doi">10.1109/RCAR47638.2019.9043939</pub-id>
</mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Herrera</surname><given-names>D. C.</given-names></name><name><surname>Kannala</surname><given-names>J.</given-names></name><name><surname>Heikkila</surname><given-names>J.</given-names></name></person-group> (<year>2012</year>). <article-title>Joint depth and color camera calibration with distortion correction</article-title>. <source>IEEE T Pattern Anal.</source>
<volume>34</volume> (<issue>10</issue>), <fpage>2058</fpage>&#x02013;<lpage>2064</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2012.125</pub-id>
</mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Hu</surname><given-names>D.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>Xu</surname><given-names>J.</given-names></name></person-group> (<year>2024</year>). <article-title>Improved YOLOv5-based image detection of cotton impurities</article-title>. <source>Text. Res. J.</source>
<volume>94</volume> (<issue>7-8</issue>), <fpage>906</fpage>&#x02013;<lpage>917</lpage>. <pub-id pub-id-type="doi">10.1177/00405175231221296</pub-id>
</mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Huang</surname><given-names>C. M.</given-names></name><name><surname>Lin</surname><given-names>T. W.</given-names></name></person-group> (<year>2021</year>). <article-title>Visual perception with servoing for assisting micro aerial vehicle through a staircase</article-title>. <source>IEEE Sens. J.</source>
<volume>21</volume> (<issue>10</issue>), <fpage>11834</fpage>&#x02013;<lpage>11843</lpage>. <pub-id pub-id-type="doi">10.1109/JSEN.2020.3020404</pub-id>
</mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Hussain</surname><given-names>T.</given-names></name><name><surname>Hong</surname><given-names>J.</given-names></name><name><surname>Seok</surname><given-names>J.</given-names></name></person-group> (<year>2024</year>). <article-title>A hybrid deep learning and machine learning-based approach to classify defects in hot rolled steel strips for smart manufacturing</article-title>. <source>Cmc-Comput Mater Con.</source>
<volume>80</volume> (<issue>2</issue>), <fpage>2099</fpage>&#x02013;<lpage>2119</lpage>. <pub-id pub-id-type="doi">10.32604/cmc.2024.050884</pub-id>
</mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Jia</surname><given-names>N.</given-names></name><name><surname>Kootstra</surname><given-names>G.</given-names></name><name><surname>Koerkamp</surname><given-names>P. G.</given-names></name><name><surname>Shi</surname><given-names>Z. X.</given-names></name><name><surname>Du</surname><given-names>S. H.</given-names></name></person-group> (<year>2021</year>). <article-title>Segmentation of body parts of cows in RGB-depth images based on template matching</article-title>. <source>Comput. Electron Agr.</source>
<volume>180</volume>, <fpage>105897</fpage>. <pub-id pub-id-type="doi">10.1016/j.compag.2020.105897</pub-id>
</mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Juang</surname><given-names>L.</given-names></name><name><surname>Zhang</surname><given-names>S.</given-names></name></person-group> (<year>2019</year>). <article-title>Intelligent service robot vision control using embedded system</article-title>. <source>Intell. Autom. Soft Co.</source>
<volume>25</volume> (<issue>3</issue>), <fpage>451</fpage>&#x02013;<lpage>459</lpage>. <pub-id pub-id-type="doi">10.31209/2019.100000126</pub-id>
</mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Khan</surname><given-names>M. A.</given-names></name><name><surname>Muhammad</surname><given-names>K.</given-names></name><name><surname>Sharif</surname><given-names>M.</given-names></name><name><surname>Akram</surname><given-names>T.</given-names></name><name><surname>de Albuquerque</surname><given-names>V. H. C.</given-names></name></person-group> (<year>2021a</year>). <article-title>Multi-class skin lesion detection and classification via teledermatology</article-title>. <source>IEEE J. Biomed. Health</source>
<volume>25</volume> (<issue>12</issue>), <fpage>4267</fpage>&#x02013;<lpage>4275</lpage>. <pub-id pub-id-type="doi">10.1109/JBHI.2021.3067789</pub-id>
</mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Khan</surname><given-names>M. A.</given-names></name><name><surname>Zhang</surname><given-names>Y. D.</given-names></name><name><surname>Sharif</surname><given-names>M.</given-names></name><name><surname>Akram</surname><given-names>T.</given-names></name></person-group> (<year>2021b</year>). <article-title>Pixels to classes: intelligent learning framework for multiclass skin lesion localization and classification</article-title>. <source>Comput. Electr. Eng.</source>
<volume>90</volume>, <fpage>106956</fpage>. <pub-id pub-id-type="doi">10.1016/j.compeleceng.2020.106956</pub-id>
</mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Konecny</surname><given-names>J.</given-names></name><name><surname>Beremlijski</surname><given-names>P.</given-names></name><name><surname>Bailova</surname><given-names>M.</given-names></name><name><surname>Machacek</surname><given-names>Z.</given-names></name><name><surname>Koziorek</surname><given-names>J.</given-names></name><name><surname>Prauzek</surname><given-names>M.</given-names></name></person-group> (<year>2024</year>). <article-title>Industrial camera model positioned on an effector for automated tool center point calibration</article-title>. <source>Sci. Rep-Uk</source>
<volume>14</volume> (<issue>1</issue>), <fpage>323</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-023-51011-5</pub-id>
</mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kong</surname><given-names>X. H.</given-names></name><name><surname>Li</surname><given-names>D.</given-names></name><name><surname>Jiao</surname><given-names>Q. S.</given-names></name></person-group> (<year>2016</year>). <article-title>An efficient evaluation algorithm for NURBS interpolation</article-title>. <source>J. South China Univ. Technol. Nat. Sci. Ed.</source>
<volume>44</volume> (<issue>1</issue>), <fpage>85</fpage>&#x02013;<lpage>92</lpage>. <pub-id pub-id-type="doi">10.3969/j.issn.1000-565X.2016.01.013</pub-id>
</mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lawal</surname><given-names>O. M.</given-names></name><name><surname>Zhu</surname><given-names>S. Y.</given-names></name><name><surname>Cheng</surname><given-names>K.</given-names></name></person-group> (<year>2023</year>). <article-title>An improved YOLOv5s model using feature concatenation with attention mechanism for real-time fruit detection and counting</article-title>. <source>Front. Plant Sci.</source>
<volume>14</volume>, <fpage>1153505</fpage>. <pub-id pub-id-type="doi">10.3389/fpls.2023.1153505</pub-id>
<pub-id pub-id-type="pmid">37434602</pub-id>
</mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Li</surname><given-names>P.</given-names></name><name><surname>Han</surname><given-names>T.</given-names></name><name><surname>Ren</surname><given-names>Y.</given-names></name><name><surname>Xu</surname><given-names>P.</given-names></name><name><surname>Yu</surname><given-names>H.</given-names></name></person-group> (<year>2023</year>). <article-title>Improved YOLOv4-tiny based on attention mechanism for skin detection</article-title>. <source>Peerj Comput. Sci.</source>
<volume>9</volume>, <fpage>1288</fpage>. <pub-id pub-id-type="doi">10.7717/peerj-cs.1288</pub-id>
</mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Li</surname><given-names>P.</given-names></name><name><surname>Yu</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Xu</surname><given-names>P.</given-names></name></person-group> (<year>2021</year>). <article-title>Comparative study of human skin detection using object detection based on transfer learning</article-title>. <source>Appl. Artif. Intell.</source>
<volume>35</volume> (<issue>15</issue>), <fpage>2370</fpage>&#x02013;<lpage>2388</lpage>. <pub-id pub-id-type="doi">10.1080/08839514.2021.1997215</pub-id>
</mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>He</surname><given-names>L.</given-names></name><name><surname>Jia</surname><given-names>J.</given-names></name><name><surname>Lv</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Qiao</surname><given-names>X.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>In-field tea shoot detection and 3D localization using an RGB-D camera</article-title>. <source>Comput. Electron Agr.</source>
<volume>185</volume>, <fpage>106149</fpage>. <pub-id pub-id-type="doi">10.1016/j.compag.2021.106149</pub-id>
</mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lin</surname><given-names>Y. L.</given-names></name><name><surname>Zhou</surname><given-names>H. T.</given-names></name><name><surname>Chen</surname><given-names>M. Y.</given-names></name><name><surname>Min</surname><given-names>H. S.</given-names></name></person-group> (<year>2019</year>). <article-title>Automatic sorting system for industrial robot with 3D visual perception and natural language interaction</article-title>. <source>Meas. Control-Uk</source>
<volume>52</volume> (<issue>1-2</issue>), <fpage>100</fpage>&#x02013;<lpage>115</lpage>. <pub-id pub-id-type="doi">10.1177/0020294018819552</pub-id>
</mixed-citation></ref><ref id="B27"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Lu</surname><given-names>L.</given-names></name><name><surname>Hong</surname><given-names>W.</given-names></name></person-group> (<year>2024</year>). &#x0201c;<article-title>WG-YOLOv5: improved YOLOv5 based on wavelet transform and GSConv for real-time wildfire detection</article-title>,&#x0201d; in <conf-name>Third International Conference on Electronic Information Engineering, Big Data, and Computer Technology (EIBDCT 2024)</conf-name>, <conf-loc>Beijing, China</conf-loc>, <conf-date>January, 2024</conf-date>. <pub-id pub-id-type="doi">10.1117/12.3031068</pub-id>
</mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Luo</surname><given-names>Y. C.</given-names></name><name><surname>Li</surname><given-names>S. P.</given-names></name><name><surname>Li</surname><given-names>D.</given-names></name></person-group> (<year>2020</year>). <article-title>Intelligent perception system of robot visual servo for complex industrial environment</article-title>. <source>Sensors-Basel</source>
<volume>20</volume> (<issue>24</issue>), <fpage>7121</fpage>. <pub-id pub-id-type="doi">10.3390/s20247121</pub-id>
<pub-id pub-id-type="pmid">33322548</pub-id>
</mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Martinez</surname><given-names>L.</given-names></name><name><surname>Ruiz-Del-Solar</surname><given-names>J.</given-names></name><name><surname>Sun</surname><given-names>L.</given-names></name><name><surname>Siebert</surname><given-names>J. P.</given-names></name><name><surname>Aragon-Camarasa</surname><given-names>G.</given-names></name></person-group> (<year>2019</year>). <article-title>Continuous perception for deformable objects understanding</article-title>. <source>Robot. Auton. Syst.</source>
<volume>118</volume>, <fpage>220</fpage>&#x02013;<lpage>230</lpage>. <pub-id pub-id-type="doi">10.1016/j.robot.2019.05.010</pub-id>
</mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mayya</surname><given-names>A.</given-names></name><name><surname>Alkayem</surname><given-names>N. F.</given-names></name><name><surname>Shen</surname><given-names>L.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Fu</surname><given-names>R. H.</given-names></name><name><surname>Wang</surname><given-names>Q.</given-names></name><etal/></person-group> (<year>2024</year>). <article-title>Efficient hybrid ensembles of CNNs and transfer learning models for bridge deck image-based crack detection</article-title>. <source>Structures</source>
<volume>64</volume>, <fpage>106538</fpage>. <pub-id pub-id-type="doi">10.1016/j.istruc.2024.106538</pub-id>
</mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Pattnaik</surname><given-names>G.</given-names></name><name><surname>Shrivastava</surname><given-names>V. K.</given-names></name><name><surname>Parvathi</surname><given-names>K.</given-names></name></person-group> (<year>2020</year>). <article-title>Transfer learning-based framework for classification of pest in tomato plants</article-title>. <source>Appl. Artif. Intell.</source>
<volume>34</volume> (<issue>13</issue>), <fpage>981</fpage>&#x02013;<lpage>993</lpage>. <pub-id pub-id-type="doi">10.1080/08839514.2020.1792034</pub-id>
</mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Pratondo</surname><given-names>A.</given-names></name><name><surname>Bramantoro</surname><given-names>A.</given-names></name></person-group> (<year>2022</year>). <article-title>Classification of zophobas morio and tenebrio molitor using transfer learning</article-title>. <source>Peerj Comput. Sci.</source>
<volume>8</volume>, <fpage>e884</fpage>. <pub-id pub-id-type="doi">10.7717/peerj-cs.884</pub-id>
<pub-id pub-id-type="pmid">35494845</pub-id>
</mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Shi</surname><given-names>L.</given-names></name><name><surname>Li</surname><given-names>B.</given-names></name><name><surname>Shi</surname><given-names>W.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name></person-group> (<year>2023</year>). <article-title>Visual servoing of quadrotor UAVs for slant targets with autonomous object search</article-title>. <source>P I Mech. Eng. I-J Sys</source>
<volume>237</volume> (<issue>5</issue>), <fpage>791</fpage>&#x02013;<lpage>804</lpage>. <pub-id pub-id-type="doi">10.1177/09596518221144490</pub-id>
</mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Liu</surname><given-names>C.</given-names></name><name><surname>Wu</surname><given-names>J. H.</given-names></name><name><surname>Xiong</surname><given-names>Z. H.</given-names></name></person-group> (<year>2019</year>). <article-title>Research of the real-time interpolation based on piecewise power basis functions</article-title>. <source>P I Mech. Eng. B-J Eng.</source>
<volume>233</volume> (<issue>3</issue>), <fpage>889</fpage>&#x02013;<lpage>899</lpage>. <pub-id pub-id-type="doi">10.1177/0954405418759703</pub-id>
</mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Weng</surname><given-names>W.</given-names></name><name><surname>Huang</surname><given-names>H.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name><name><surname>Lin</surname><given-names>C.</given-names></name></person-group> (<year>2022</year>). <article-title>Development of a visual perception system on a dual-arm mobile robot for human-robot interaction</article-title>. <source>Sensors-Basel</source>
<volume>22</volume> (<issue>23</issue>), <fpage>9545</fpage>. <pub-id pub-id-type="doi">10.3390/s22239545</pub-id>
<pub-id pub-id-type="pmid">36502245</pub-id>
</mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Yang</surname><given-names>L.</given-names></name><name><surname>Dryanovski</surname><given-names>I.</given-names></name><name><surname>Valenti</surname><given-names>R. G.</given-names></name><name><surname>Wolberg</surname><given-names>G.</given-names></name><name><surname>Xiao</surname><given-names>J.</given-names></name></person-group> (<year>2020</year>). <article-title>RGB-D camera calibration and trajectory estimation for indoor mapping</article-title>. <source>Auton. Robot.</source>
<volume>44</volume> (<issue>8</issue>), <fpage>1485</fpage>&#x02013;<lpage>1503</lpage>. <pub-id pub-id-type="doi">10.1007/s10514-020-09941-w</pub-id>
</mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Yu</surname><given-names>L. Y.</given-names></name><name><surname>Xiong</surname><given-names>J. T.</given-names></name><name><surname>Fang</surname><given-names>X. Q.</given-names></name><name><surname>Yang</surname><given-names>Z. G.</given-names></name><name><surname>Chen</surname><given-names>Y. Q.</given-names></name><name><surname>Lin</surname><given-names>X. Y.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>A litchi fruit recognition method in a natural environment using RGB-D images</article-title>. <source>Biosyst. Eng.</source>
<volume>204</volume>, <fpage>50</fpage>&#x02013;<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1016/j.biosystemseng.2021.01.015</pub-id>
</mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Yu</surname><given-names>Y.</given-names></name><name><surname>Zhao</surname><given-names>J.</given-names></name><name><surname>Gong</surname><given-names>Q.</given-names></name><name><surname>Huang</surname><given-names>C.</given-names></name><name><surname>Zheng</surname><given-names>G.</given-names></name><name><surname>Ma</surname><given-names>J.</given-names></name></person-group> (<year>2021</year>). <article-title>Real-time underwater maritime object detection in side-scan sonar images based on transformer-YOLOv5</article-title>. <source>Remote Sens-Basel.</source>
<volume>13</volume> (<issue>18</issue>), <fpage>3555</fpage>. <pub-id pub-id-type="doi">10.3390/rs13183555</pub-id>
</mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X. W.</given-names></name><name><surname>Ren</surname><given-names>Y. F.</given-names></name><name><surname>Zhen</surname><given-names>G. Y.</given-names></name><name><surname>Shan</surname><given-names>Y. H.</given-names></name><name><surname>Chu</surname><given-names>C. Q.</given-names></name><name><surname>Liang</surname><given-names>F.</given-names></name></person-group> (<year>2020</year>). <article-title>Camera calibration method for solid spheres based on triangular primitives</article-title>. <source>Precis. Eng.</source>
<volume>65</volume>, <fpage>91</fpage>&#x02013;<lpage>102</lpage>. <pub-id pub-id-type="doi">10.1016/j.precisioneng.2020.04.013</pub-id>
</mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhu</surname><given-names>X. Z.</given-names></name><name><surname>Liu</surname><given-names>X. Y.</given-names></name><name><surname>Zhang</surname><given-names>Y. J.</given-names></name><name><surname>Wan</surname><given-names>Y.</given-names></name><name><surname>Duan</surname><given-names>Y. S.</given-names></name></person-group> (<year>2021</year>). <article-title>Robust 3-D plane segmentation from airborne point clouds based on quasi-a-contrario theory</article-title>. <source>IEEE J-Stars</source>
<volume>14</volume>, <fpage>7133</fpage>&#x02013;<lpage>7147</lpage>. <pub-id pub-id-type="doi">10.1109/JSTARS.2021.3093576</pub-id>
</mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zlatintsi</surname><given-names>A.</given-names></name><name><surname>Dometios</surname><given-names>A. C.</given-names></name><name><surname>Kardaris</surname><given-names>N.</given-names></name><name><surname>Rodomagoulakis</surname><given-names>I.</given-names></name><name><surname>Koutras</surname><given-names>P.</given-names></name><name><surname>Papageorgiou</surname><given-names>X.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>I-support: a robotic platform of an assistive bathing robot for the elderly population</article-title>. <source>Robot. Auton. Syst.</source>
<volume>126</volume>, <fpage>103451</fpage>. <pub-id pub-id-type="doi">10.1016/j.robot.2020.103451</pub-id>
</mixed-citation></ref></ref-list></back></article>