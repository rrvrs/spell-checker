<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006470</article-id><article-id pub-id-type="pmc">PMC11860555</article-id><article-id pub-id-type="doi">10.3390/s25041241</article-id><article-id pub-id-type="publisher-id">sensors-25-01241</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Extending Anxiety Detection from Multimodal Wearables in Controlled Conditions to Real-World Environments</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-3378-6519</contrib-id><name><surname>Alkurdi</surname><given-names>Abdulrahman</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01241" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>He</surname><given-names>Maxine</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af2-sensors-25-01241" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0901-2828</contrib-id><name><surname>Cerna</surname><given-names>Jonathan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af2-sensors-25-01241" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Clore</surname><given-names>Jean</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af3-sensors-25-01241" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0531-7918</contrib-id><name><surname>Sowers</surname><given-names>Richard</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af4-sensors-25-01241" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9373-3611</contrib-id><name><surname>Hsiao-Wecksler</surname><given-names>Elizabeth T.</given-names></name><xref rid="af1-sensors-25-01241" ref-type="aff">1</xref><xref rid="af2-sensors-25-01241" ref-type="aff">2</xref><xref rid="af5-sensors-25-01241" ref-type="aff">5</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3501-9508</contrib-id><name><surname>Hernandez</surname><given-names>Manuel E.</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-25-01241" ref-type="aff">2</xref><xref rid="af5-sensors-25-01241" ref-type="aff">5</xref><xref rid="af6-sensors-25-01241" ref-type="aff">6</xref><xref rid="af7-sensors-25-01241" ref-type="aff">7</xref><xref rid="af8-sensors-25-01241" ref-type="aff">8</xref><xref rid="c1-sensors-25-01241" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Fern&#x000e1;ndez-Mart&#x000ed;nez</surname><given-names>Fernando</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Gil-Mart&#x000ed;n</surname><given-names>Manuel</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>San-Segundo</surname><given-names>Rub&#x000e9;n</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01241"><label>1</label>Department of Mechanical Science &#x00026; Engineering, University of Illinois Urbana-Champaign, Urbana, IL 61801, USA; <email>alkurdi2@illinois.edu</email> (A.A.); <email>ethw@illinois.edu</email> (E.T.H.-W.)</aff><aff id="af2-sensors-25-01241"><label>2</label>Neuroscience Program, University of Illinois Urbana-Champaign, Urbana, IL 61801, USA; <email>maoyuan2@illinois.edu</email> (M.H.); <email>cerna3@illinois.edu</email> (J.C.)</aff><aff id="af3-sensors-25-01241"><label>3</label>Department of Psychiatry and Behavioral Medicine, University of Illinois College of Medicine at Peoria, Peoria, IL 61805, USA; <email>jayclore@uic.edu</email></aff><aff id="af4-sensors-25-01241"><label>4</label>Department of Industrial &#x00026; Enterprise Systems Engineering, University of Illinois Urbana-Champaign, Urbana, IL 61801, USA; <email>r-sowers@illinois.edu</email></aff><aff id="af5-sensors-25-01241"><label>5</label>Department of Biomedical and Translational Sciences, Carle Illinois College of Medicine, University of Illinois Urbana-Champaign, Urbana, IL 61801, USA</aff><aff id="af6-sensors-25-01241"><label>6</label>Department of Health and Kinesiology, University of Illinois Urbana-Champaign, Urbana, IL 61801, USA</aff><aff id="af7-sensors-25-01241"><label>7</label>Department of Bioengineering, University of Illinois Urbana-Champaign, Urbana, IL 61801, USA</aff><aff id="af8-sensors-25-01241"><label>8</label>Beckman Institute, University of Illinois Urbana-Champaign, Urbana, IL 61801, USA</aff><author-notes><corresp id="c1-sensors-25-01241"><label>*</label>Correspondence: <email>mhernand@illinois.edu</email>; Tel.: +1-217-244-8971</corresp></author-notes><pub-date pub-type="epub"><day>18</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1241</elocation-id><history><date date-type="received"><day>29</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>17</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>17</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>This study quantitatively evaluated whether and how machine learning (ML) models built by data from controlled conditions can fit real-world conditions. This study focused on feature-based models using wearable technology from real-world data collected from young adults, so as to provide insights into the models&#x02019; robustness and the specific challenges posed by diverse environmental noise. Feature-based models, particularly XGBoost and Decision Trees, demonstrated considerable resilience, maintaining higher accuracy and reliability across different noise levels. This investigation included an in-depth analysis of transfer learning, highlighting its potential and limitations in adapting models developed from standard datasets, like WESAD, to complex real-world scenarios. Moreover, this study analyzed the distributed feature importance across various physiological signals, such as electrodermal activity (EDA) and electrocardiography (ECG), considering their vulnerability to environmental factors. It was found that integrating multiple physiological data types could significantly enhance model robustness. The results underscored the need for a nuanced understanding of signal contributions to model efficacy, suggesting that feature-based models showed much promise in practical applications.</p></abstract><kwd-group><kwd>anxiety</kwd><kwd>machine learning</kwd><kwd>wearable technology</kwd><kwd>transfer learning</kwd><kwd>multimodal</kwd></kwd-group><funding-group><award-group><funding-source>Jump ARCHES endowment through the Health Care Engineering Systems Center</funding-source></award-group><award-group><funding-source>ACCESS MATCH program made possible by the U.S. National Science Foundation</funding-source></award-group><funding-statement>This research was funded by the Jump ARCHES endowment through the Health Care Engineering Systems Center and partly supported by the ACCESS MATCH program made possible by the U.S. National Science Foundation.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01241"><title>1. Introduction</title><p>Anxiety disorders are among the most prevalent mental health issues worldwide, affecting millions and significantly impacting the quality of life [<xref rid="B1-sensors-25-01241" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01241" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01241" ref-type="bibr">3</xref>]. While traditional methods like self-reports and clinical interviews offer insight into symptoms of anxiety, they are limited by subjectivity and intermittency [<xref rid="B4-sensors-25-01241" ref-type="bibr">4</xref>]. The advent of wearable technology presents new possibilities for the continuous, objective, and non-invasive monitoring of anxiety symptoms in real-world settings.</p><p>Prior work has demonstrated the potential of wearable devices for anxiety detection using various physiological signals, such as electrodermal activity (EDA), heart rate variability (HRV), and accelerometer data [<xref rid="B5-sensors-25-01241" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01241" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-01241" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01241" ref-type="bibr">8</xref>]. However, these studies are predominantly conducted in controlled laboratory settings, which may not accurately reflect realistic conditions. While wearable devices have been used to assess anxiety in social settings [<xref rid="B9-sensors-25-01241" ref-type="bibr">9</xref>] and predict the occurrence of anxiety [<xref rid="B10-sensors-25-01241" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01241" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01241" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01241" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01241" ref-type="bibr">14</xref>], further work remains on evaluating the generalizability of pre-trained anxiety classifiers on lab data to real-world data and the benefits of noise on classifier robustness.</p><p>A recent review highlighted the use of machine learning to detect anxiety using physiological signals [<xref rid="B15-sensors-25-01241" ref-type="bibr">15</xref>]. Building on recent work [<xref rid="B10-sensors-25-01241" ref-type="bibr">10</xref>,<xref rid="B14-sensors-25-01241" ref-type="bibr">14</xref>], this study extends beyond the controlled laboratory setting, delving into real-world environments to assess the practicality and effectiveness of wearable technology for monitoring anxiety. Using the open-source Wearable Stress and Affect Detection (WESAD) dataset [<xref rid="B8-sensors-25-01241" ref-type="bibr">8</xref>], which consists of multimodal wearable sensor data in different affective states, we used a subset of data to align with the wearable hardware from our study.</p><p>Traditional machine learning (ML) models are typically trained and evaluated using the same dataset. The use of a single dataset provides increased homogeneity and consistency in data characteristics for use in establishing a benchmark of model performance. Prior work examining anxiety prediction [<xref rid="B10-sensors-25-01241" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01241" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01241" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01241" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01241" ref-type="bibr">14</xref>,<xref rid="B16-sensors-25-01241" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01241" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01241" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-01241" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01241" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-01241" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01241" ref-type="bibr">22</xref>] has typically focused on the use of self-reported measures [<xref rid="B10-sensors-25-01241" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01241" ref-type="bibr">11</xref>,<xref rid="B13-sensors-25-01241" ref-type="bibr">13</xref>,<xref rid="B20-sensors-25-01241" ref-type="bibr">20</xref>] but has more recently focused on the use of objective physiological signals under controlled conditions [<xref rid="B12-sensors-25-01241" ref-type="bibr">12</xref>,<xref rid="B14-sensors-25-01241" ref-type="bibr">14</xref>,<xref rid="B16-sensors-25-01241" ref-type="bibr">16</xref>,<xref rid="B18-sensors-25-01241" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-01241" ref-type="bibr">19</xref>,<xref rid="B22-sensors-25-01241" ref-type="bibr">22</xref>]. However, few studies have examined the impact of physiological and non-physiological noise on anxiety prediction [<xref rid="B22-sensors-25-01241" ref-type="bibr">22</xref>]. Thus, the evaluation of an ML model&#x02019;s effectiveness under real-world conditions may provide insights into the models&#x02019; robustness and the specific challenges posed by diverse environmental noise.</p><p>Transfer learning is an approach that leverages knowledge from a pre-trained model to address a related problem with limited labeled data [<xref rid="B23-sensors-25-01241" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-01241" ref-type="bibr">24</xref>]. By adapting the pre-trained model to the new task, transfer learning reduces the need for extensive training data and computational resources [<xref rid="B25-sensors-25-01241" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-01241" ref-type="bibr">26</xref>]. It is particularly beneficial when the target task has limited labeled data, is similar to the pre-trained model&#x02019;s task, or when training from scratch is expensive or time-consuming [<xref rid="B26-sensors-25-01241" ref-type="bibr">26</xref>]. Transfer learning has been successfully applied in various domains, including computer vision, natural language processing, and speech [<xref rid="B26-sensors-25-01241" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01241" ref-type="bibr">27</xref>]. However, few studies have examined the robustness of anxiety detection ML models to new datasets [<xref rid="B19-sensors-25-01241" ref-type="bibr">19</xref>], and when they have, they have primarily focused on the use of controlled conditions.</p><p>This study addresses these gaps by evaluating feature-based machine learning (ML) models under a variety of real-world conditions. Feature-based ML models leverage domain expertise by providing a basis for features to use, based on prior evidence. We placed significant emphasis on distributed feature importance across different physiological signals, considering their specific failure modes which may affect detection accuracy. For example, EDA signals can be disrupted in wet or humid environments, acceleration data may be unreliable during physical activity, and electrocardiogram (ECG) signals may be affected by conductivity issues, making them challenging to integrate into wearable devices. Blood volume pulse (BVP) signals are also susceptible to disruptions caused by gaps between the device and the skin. By investigating the robustness and reliability of these models in challenging environments, this study provides an evaluation of the most effective modalities and measures for use in real-world applications.</p><p>The overarching goal of this study was to quantitatively evaluate whether and how ML models for anxiety detection built by data from wearables in controlled conditions could be applied to real-world environments. By leveraging data from young adults that reflect a wide range of environmental conditions, we aimed to identify the most informative features and modalities for anxiety detection and evaluate the use of open-source data to pre-train an anxiety classifier for use in novel tasks and conditions.</p></sec><sec id="sec2-sensors-25-01241"><title>2. Materials and Methods</title><p>To evaluate the feasibility of using an open-source dataset for transfer learning and real-world data, we used custom Python (version 3.8) data processing and feature extraction scripts on both the WESAD dataset and real-world data (see <xref rid="sensors-25-01241-f001" ref-type="fig">Figure 1</xref>). Custom Python scripts are available in the following code repository: <uri xlink:href="https://github.com/AbdulAlkurdi/anxietyFB">https://github.com/AbdulAlkurdi/anxietyFB</uri> (accessed on 3 February 2025). The WESAD dataset [<xref rid="B8-sensors-25-01241" ref-type="bibr">8</xref>] was used as a basis for anxiety detection with physiological signals and the introduction of synthetic noise. The study involved 15 participants, with a gender distribution of 20% female, and the use of two wearable devices. The RespiBAN Professional (PLUX Wireless Biosignal S.A., Lisbon, Portugal) was used to record ECG, EDA, EMG, RESP, skin temperature (TEMP), and 3-axis acceleration (ACC) at 700 Hz, but was downsampled to 70 Hz for all modalities. The E4 Wristband (Empatica, Inc., Boston, MA, USA) was used to record BVP, EDA, TEMP, and ACC data at varying sampling rates. The study&#x02019;s protocol included baseline (53%), amusement (17%), and stress (30%) conditions. Participant self-reports after each condition provided the ground truth, incorporating scales like Positive and Negative Affect Schedule (PANAS) [<xref rid="B28-sensors-25-01241" ref-type="bibr">28</xref>], State-Trait Anxiety Inventory (STAI) [<xref rid="B29-sensors-25-01241" ref-type="bibr">29</xref>], and Short Stress State Questionnaire (SSSQ) [<xref rid="B30-sensors-25-01241" ref-type="bibr">30</xref>], as previously described.</p><sec id="sec2dot1-sensors-25-01241"><title>2.1. Overview of Real-World Data</title><p>This study was designed to quantitatively evaluate whether and how ML models for anxiety detection built by data from wearables in controlled conditions could be applied to real-world environments using data from two ongoing studies. The WEAR study captures data from STEM-major undergraduate students. Participants included 12 males and 15 females with an average height of 161 &#x000b1; 5 cm, weight of 67.5 &#x000b1; 20 kg, and age of 23 &#x000b1; 2.3 years. The RADWear study captures the demanding nature of medical education through data from third-year students on clinical rotations. Participants included 4 males and 5 females with an average height of 167 &#x000b1; 7.5 cm, weight of 79.4 &#x000b1; 27 kg, and age of 27.3 &#x000b1; 2.3 years. Both studies have been approved by an institutional review board at either the University of Illinois College of Medicine at Peoria (IRB Protocol 1880297 for RADWear) or the University of Illinois Urbana-Champaign (IRB Protocol 21769 for WEAR).</p><p>For both studies, the E4 wristband and Hexoskin smart shirt (Carre Technologies, Montreal, QC, Canada) were used to collect data. The E4 wristband was utilized to collect various biophysiological signals including BVP, EDA, ACC, and skin temperature (TEMP). In parallel, the Hexoskin smart shirt recorded electrocardiogram (ECG), respiration (RESP), and 3-axis acceleration (ACC) data. For the Hexoskin smartshirt, ECG data were sampled at 256 Hz, while RESP and ACC data were sampled at 128 Hz and 64 Hz, respectively.</p><p>RADWear and WEAR involved a calibration protocol designed to establish baseline states for relaxation using a guided relaxation exercise and excitement/anxiety, captured during a cold pressor test in a lab setting. These initial sessions lasted about 30 min and aimed to provide reference points for trait anxiety levels, using the State-Trait Anxiety Inventory (STAI) X-2 questionnaire at the beginning and state anxiety changes after each task, using the STAI Y6.</p><p>In the WEAR study, participants underwent a comprehensive in-lab testing session lasting approximately four hours including the calibration session. After completing the calibration session, the participants performed an in-lab session that included a series of stress- and anxiety-inducing protocols: the Trier Social Stress Test (TSST), seated Stroop test, and walking Stroop test. These activities were selected to induce anxiety and serve as validated methods to establish baseline anxiety levels. WEAR participants began with an intake survey of trait anxiety (STAI X2) and completed follow-up questionnaires of state anxiety (STAI Y6) after each test to assess anxiety responses. Prior work has demonstrated significant differences in anxiety, based on the in-person tasks, and the ability to differentiate between different tasks using objective physiological signals [<xref rid="B31-sensors-25-01241" ref-type="bibr">31</xref>].</p><p>In the RADWear study, following the calibration protocol, participants engaged in clinical rotations, and data were collected in real-world conditions. These &#x0201c;in-the-wild&#x0201d; data collection periods were extensive, consisting of two, two-week sessions (during work hours at least 5 days per week, lasting 6&#x02013;8 h each day). For the current study, nine RADWear participants were included in the calibration sessions and seven in the in-the-wild data analysis, due to availability. However, future work remains necessary to increase the sample size and validate the effectiveness of the recorded data.</p><p>The current study datasets were organized into three subsets based on the similarity of the test conditions, including physical configuration and activities involved and environmental noise (<xref rid="sensors-25-01241-t001" ref-type="table">Table 1</xref>): (1) RADWear and WEAR calibration sessions, where participants are restricted to seated conditions with a fixed room temperature and lighting conditions; (2) WEAR in-lab sessions, where participants are seated, standing, or walking in a controlled environment with a fixed room temperature and lighting conditions; and (3) RADWear in-the-wild datasets, where physical configuration and activity and environmental noise are variable and unconstrained. This selection of data subsets will inform how models perform under varying levels of environmental conditions that restrict participants&#x02019; motion and the potential for confounding factors.</p></sec><sec id="sec2dot2-sensors-25-01241"><title>2.2. Addressing Noise</title><p>Because of the RADWear and WEAR studies&#x02019; experimental protocols containing physical activity in both lab and natural settings, feature extraction presented more of a challenge. This was due to significant amounts of motion-artifact noise, which negatively affected the signal and the ability to extract features. Given the significant impact of noise on the feature extraction of heart rate signals, a feature extraction algorithm was developed, based on concepts from Malik et al. [<xref rid="B32-sensors-25-01241" ref-type="bibr">32</xref>] and the Automatic Multiscale-based Peak Detection (AMPD) algorithm from Scholkmann et al. [<xref rid="B33-sensors-25-01241" ref-type="bibr">33</xref>], which was able to achieve a 98% peak detection accuracy in ECG data. For EDA and ACC signals, we used a lowpass Butterworth filter with cut-off frequencies of 5 Hz and 13 Hz, respectively. Further details on data preprocessing and feature extraction are provided in recent work [<xref rid="B22-sensors-25-01241" ref-type="bibr">22</xref>]. In summary, preprocessing steps included outlier removal, min&#x02013;max normalization, feature extraction (see <xref rid="sensors-25-01241-f001" ref-type="fig">Figure 1</xref>), and handling of missing data, which were crucial for preparing RADWear and WEAR study data for effective machine learning analysis.</p><p>Feature extraction was performed on all available modalities from the E4 wristband and Hexoskin smart shirt, using features (<xref rid="sensors-25-01241-f001" ref-type="fig">Figure 1</xref>) that had shown anxiety detection robustness across a wide range of signal-to-noise ratios in XGB and DT models [<xref rid="B22-sensors-25-01241" ref-type="bibr">22</xref>].</p><p>For the RADWear in-the-wild data, data loss due to excessive noise was partly resolved using imputation where sufficient data were available. If sufficient data in a segment were available, a mean imputation was employed to replace the missing points. If a significant portion of a segment was missing or too much noise existed, that segment was dropped.</p></sec><sec id="sec2dot3-sensors-25-01241"><title>2.3. Label Generation</title><p>Using a cutoff of 11 on the STAI Y6 (Range 6&#x02013;24), we identified data segments to correspond to either lower or higher anxiety, to serve as ground truth for the feature-based ML models. A representative sample of STAI scores across calibration and in-lab sessions is provided in <xref rid="sensors-25-01241-f002" ref-type="fig">Figure 2</xref>.</p></sec><sec id="sec2dot4-sensors-25-01241"><title>2.4. Class Balancing</title><p>Generally, to have effective classification outcomes from ML algorithms, datasets should be balanced between classes. Imbalanced datasets can lead to biased model performance, misleading performance metrics, limited generalization, and robustness. Thus, the use of balancing techniques to ensure fair and accurate machine learning models is of the utmost importance. In this study, the undersampling of the dominant class was utilized to reduce the imbalance. The laboratory-controlled datasets (RADWear + WEAR calibration, WEAR in-lab) had good balance. Due to the nature of the RADWear in-the-wild dataset, the anxious and non-anxious classes were imbalanced, with 80% of the data being labeled as not anxious. Undersampling was performed on the RADWear in-the-wild dataset to match that of the calibration and in-lab segments. Undersampling was performed by reducing the number of data points collected from the dominant class, which preserves the number of points of the minority class (<xref rid="sensors-25-01241-t002" ref-type="table">Table 2</xref>).</p></sec><sec id="sec2dot5-sensors-25-01241"><title>2.5. Traditional Machine Learning</title><p>Based on recent findings on the most robust machine learning models in predicting anxiety from a noise-augmented WESAD dataset [<xref rid="B22-sensors-25-01241" ref-type="bibr">22</xref>], feature-based models were employed. Feature-based models included the following: Decision Tree (DT), Random Forest (RF), Linear Discriminant Analysis (LDA), k-nearest neighbor (KNN), Adaboost (AB), Support Vector Machine (SVM), and XG Boost (XGB). All FB models were created using scikit-learn 1.2.1 ML library.</p><p>To train and evaluate each FB model, models were first initialized for each of the chosen algorithms, calculating performance metrics and feature importances directly during model evaluation. The training process involved setting up specific models, such as SVM, using the scikit-learn&#x02019;s fit method. A 5-fold cross-validation method was utilized. The dataset was randomly divided into equal five test sets. For each of the test sets, the rest of the data were split into training (80%) and validation (20%) sets. For each of the architectures, 5-fold cross-validation and 5 training iterations were performed. Comparisons of these predictions with the actual labels facilitated the calculation of key performance metrics including accuracy and F1 score, averaged across the 5 folds.</p><p>Further, we generated a feature importance list for each model, which ranked the features by their influence on the model&#x02019;s predictions. This provided critical insights into which variables significantly impacted the detection of anxiety. Detailed results for each model&#x02019;s performance, along with the top seven influential features, were systematically documented to underscore the predictive capabilities and strengths of each model within the study.</p></sec><sec id="sec2dot6-sensors-25-01241"><title>2.6. Transfer Learning</title><p>Afterwards, models that were trained using a subset of the WESAD dataset with and without noise augmentation that matched in modalities and were similar in locations were tested on the three subsets to test the validity of transfer learning for this application. Feature-based models were employed. DT, RF, LDA, KNN, AB, SVM, and XGB were utilized to test performance for transfer learning, using re-used models without additional fine-tuning. As seen in <xref rid="sensors-25-01241-f003" ref-type="fig">Figure 3</xref>, ML noise-augmented data were used to train feature-based models that were directly applied to the three subsets of RADWear and WEAR data.</p><p>Noise-augmented data derived from the WESAD dataset was used to assess the resilience of machine learning models to environmental disturbances. This approach was crucial for understanding how these models performed under simulated conditions that mimicked real-world noise, which is often encountered in daily activities and can significantly affect the accuracy of anxiety detection systems. Noise augmentation was carried out by adding Gaussian noise, ranging from 0.0001 to 0.6 signal-to-noise ratios (SNRs) from WESAD data.</p><p>By training models on both the original and noise-augmented WESAD data, we established a performance benchmark, examining how noise impacts model effectiveness and identifying which models maintain their predictive power despite increased noise levels. This process not only highlighted the robustness of certain models but also allowed for the optimization of these models to withstand typical real-world disruptions.</p><p>The transfer learning techniques applied here involved testing the models&#x02014;initially trained on WESAD and its noise-augmented version&#x02014;on the RADWear and WEAR data. The transition to testing on these studies, which feature real-life conditions and more complex environments than those simulated by noise augmentation, was facilitated by the preliminary insights gained from the noise impact analysis. This step ensured that the models not only generalized well across different types of input data but were also applicable in practical settings where environmental variability is the norm.</p><p>Overall, analyzing models with noise-augmented data enhances the relevance and applicability of these models for RADWear and WEAR data, where real-world conditions play a crucial role. It provided a solid foundation for understanding model performance in the face of unpredictable environmental factors, ensuring that the anxiety detection systems developed are both effective and reliable in varied real-world scenarios.</p><p>For transfer learning, models that have been tested on WESAD, and noise-augmented WESAD, were tested on the three subsets. The use of WESAD serves to establish a performance benchmark as a reference point. Since the models were initially trained and validated on this dataset, showcasing their performance on WESAD helps in understanding their efficiency and accuracy before they were tested under more variable conditions such as those provided by the RADWear and WEAR datasets. It is important to articulate that the WESAD dataset acts as a foundational dataset for training the models, whose performance metrics were crucial for establishing a comparative analysis.</p><p>The incorporation of noise-augmented data was a strategic decision to bolster the resilience of machine learning models against environmental noise, which is a common and disruptive element in real-world settings. The process of augmenting the WESAD dataset with noise simulates these realistic disturbances, thus enabling us to stress-test the models and prepare them for the unpredictable variances they would encounter in practical scenarios. By doing so, we aimed to ensure that the models retain their predictive accuracy even when faced with data that has been compromised or distorted by external factors. To assess the performance of each model evaluated at each subset, F<sub>1</sub> score and accuracy were used to evaluate the models.</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-01241"><title>3. Results</title><sec id="sec3dot1-sensors-25-01241"><title>3.1. Traditional ML Models</title><p>The traditional ML models refer to those tested and trained exclusively on each of the three data subsets, including the original WESAD dataset as a reference point for benchmarking. The analysis of the model performances with the RADWear and WEAR data (<xref rid="sensors-25-01241-t003" ref-type="table">Table 3</xref>) highlighted the excellent performance of the XGB and DT models. Notably, the XGB model achieved a 0.99 accuracy and F1 score in the WESAD dataset [<xref rid="B22-sensors-25-01241" ref-type="bibr">22</xref>]. This performance was maintained across the RADWear and WEAR calibration subsets, where XGB recorded a 0.95 accuracy and a 0.94 F1 score. Furthermore, even in the challenging conditions of the WEAR in-lab settings, XGB showed a robust performance with a 0.92 accuracy and a 0.90 F1 score. These results validate the effectiveness of traditional ML models in real-world scenarios, substantially outperforming the 50% accuracy expected of random guessing in binary classification.</p><p>Interestingly, the most predictive features varied across models and datasets. In <xref rid="sensors-25-01241-t004" ref-type="table">Table 4</xref>, the performance metrics of various features across different models are presented, offering a detailed comparison of how different data types influence the effectiveness of the models. The &#x0201c;Weighted average&#x0201d; represents the aggregated influence of each feature across different models. This calculation was performed using a weighted mean, where each feature&#x02019;s importance is squared within a model, summed across models, and then divided by the total sum of the feature importances. This calculation method emphasizes features that were consistently significant across various models, highlighting their predictive power in detecting anxiety. This approach particularly underscores the adaptability and critical role of specific features under conditions such as motion, where accelerometer data becomes increasingly significant.</p></sec><sec id="sec3dot2-sensors-25-01241"><title>3.2. Feature-Based Transfer Learning</title><p>When evaluating the transfer learning performance (<xref rid="sensors-25-01241-f004" ref-type="fig">Figure 4</xref> and <xref rid="sensors-25-01241-t005" ref-type="table">Table 5</xref>), we discovered that the RF model excelled in the calibration and in-lab settings, while SVM achieved the best results for in-the-wild data. Intriguingly, models like XGB and DT, which previously outperformed in the baseline case, did not maintain their superiority in the transfer learning scenario. This finding highlights the importance of carefully selecting models based on the specific characteristics of the target dataset and the potential limitations of directly transferring models trained on one dataset to another.</p><p>In exploring the performance of transfer learning across our datasets, we identified a significant trend related to the distribution of feature importance weights in the models. Models exhibiting more evenly distributed feature importance weights demonstrated more successful adaptation when applied to new datasets, particularly when transitioning from controlled to more variable real-world conditions. For instance, models like LDA, which generally maintained balanced importance across features, adapted more effectively compared to those heavily reliant on one or two specific features, such as XGB. The latter showed reduced performance during transfer learning tasks, especially when applied to the in-the-wild subset of the WEAR dataset. This suggests that a more uniform distribution of feature weights might enhance a model&#x02019;s adaptability, supporting better generalization across different experimental conditions and datasets. This finding underscores the importance of considering feature balance during model training for effective transfer learning applications.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-01241"><title>4. Discussion</title><p>Our study contributes to the field of anxiety detection using wearables and machine learning by demonstrating the feasibility of developing robust models that can accurately detect anxiety in noisy environments and can be transferred from controlled laboratory conditions. The strong performance of our models, particularly XGB and DT, in both controlled and real-world settings, consistent with recent work in open-source datasets [<xref rid="B22-sensors-25-01241" ref-type="bibr">22</xref>], underscores the potential for deploying such techniques in practical mental health monitoring applications.</p><p>In comparison to prior related work [<xref rid="B14-sensors-25-01241" ref-type="bibr">14</xref>,<xref rid="B19-sensors-25-01241" ref-type="bibr">19</xref>] examining physiological signals (<xref rid="sensors-25-01241-t006" ref-type="table">Table 6</xref>), we find that the XGB and DT classifiers provide improved accuracy across all datasets examined in this study when using test data from the same dataset. Similarly, the F1 scores for the DT classifier demonstrated improved performance across all datasets except the in-the-wild data balanced to 41%. When examining transfer learning, by evaluating the use of training on one dataset and comparing to another, we found the best performance was provided by an RF classifier, consistent with prior work, which found a RF anxiety classifier to demonstrate robust performance when applying to a new dataset [<xref rid="B19-sensors-25-01241" ref-type="bibr">19</xref>].</p><p>Prior work using the WESAD dataset has focused on stress detection using ML and deep learning approaches [<xref rid="B8-sensors-25-01241" ref-type="bibr">8</xref>,<xref rid="B34-sensors-25-01241" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-01241" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-01241" ref-type="bibr">36</xref>]. In addition, the in-lab portions of the WEAR study have been primarily focused on establishing the validity of physiological signal changes in stressful conditions [<xref rid="B31-sensors-25-01241" ref-type="bibr">31</xref>]. However, further work remains to validate the in-the-wild portions of the RADWEAR study.</p><p>This study advances the existing literature in several ways. First, we conducted an analysis of feature importance, shedding light on the relative contributions of different physiological signals and derived features in anxiety detection, which could be potentially beneficial for individualized treatment planning. Second, we explored the applicability of transfer learning, highlighting the challenges and opportunities in adapting models trained on one dataset to another. Notably, it seems that models with feature importances close in weight perform better for transfer learning, while models with feature importances heavily weighted on one or two features, like XGB for calibration and in-the-wild, do not perform transfer learning as well.</p><p>The contrast in the identification of the most important modalities across each of the datasets suggests that physical activity and environmental noise significantly impact the most important information to classify anxiety. In contrast to prior work examining data from controlled environments [<xref rid="B19-sensors-25-01241" ref-type="bibr">19</xref>,<xref rid="B22-sensors-25-01241" ref-type="bibr">22</xref>], where EDA and BVP have been found to be among the most important modalities for anxiety classification, we found that ACC, TEMP, and ECG provided the most important modalities overall, as physical activity and environmental noise increased.</p><p>The study&#x02019;s findings emphasize the importance of selecting features that were robust to various environmental factors and failure modes. Consequently, models that rely on a distributed feature importance, where multiple signals contribute to the detection of anxiety, were more likely to be resilient to individual signal failures. Future research should focus on identifying and engineering features that are robust to these challenges, enhancing the reliability of anxiety detection models in real-world settings.</p><p>However, our study is not without limitations. The sample sizes of the RADWear and WEAR datasets used in this study, while diverse, may not fully capture the entire spectrum of individual variability in anxiety responses. As those studies are ongoing, these datasets will expand, incorporating a broader range of participant demographics and clinical profiles, so as to better generalize new subjects. Additionally, while our feature-based models demonstrate promising performance, further validation in longitudinal studies and real-world deployments would be necessary to assess their long-term reliability and usefulness in clinical practice. Further, the high accuracies but lower F1 scores observed in some of the transfer learning suggest potential overfitting, which should be further examined in future work.</p><p>Developing accurate and robust anxiety detection models can pave the way for personalized mental health interventions that are delivered in real-time using wearable devices, potentially enhancing the effectiveness of current empirically supported therapy, improving the accuracy of self-report data, and providing &#x0201c;self-help&#x0201d; programs beyond common bibliotherapy, thereby reaching more rural populations where specialized clinicians may not be available. This could revolutionize the way we monitor and manage anxiety disorders, enabling early detection, timely support, and targeted treatments. However, translating these models into practical tools will require addressing challenges such as data privacy, user acceptance, and seamless integration with existing healthcare systems.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-01241"><title>5. Conclusions</title><p>This study underscores the efficacy of feature-based models, particularly XGBoost and DT, in accurately detecting anxiety under both controlled conditions and real-world environments, and for models such as RF to transfer learning well from different datasets. The introduction of the RADWear and WEAR studies enhances our understanding of the robustness of these models amidst environmental noise and varied conditions. The analysis of feature importance and the implementation of transfer learning have significantly contributed to advancing the field of anxiety detection using wearable technology. Key findings emphasize the resilience of feature-based models and the critical role of precise feature selection in maintaining model accuracy across diverse settings. These insights not only validate the effectiveness of current methodologies but also underscore the potential of these models in practical mental health monitoring applications.</p></sec></body><back><ack><title>Acknowledgments</title><p>We thank members of the Mobility and Fall Prevention Research Lab and the Human Dynamics and Control Lab that contributed to this study, and participants for making this study possible.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.A., J.C. (Jean Clore), R.S., E.T.H.-W., and M.E.H.; methodology, A.A., J.C. (Jonathan Cerna), and M.H.; software, A.A.; validation, A.A.; formal analysis, A.A.; resources, A.A.; data curation, A.A., M.H., and J.C. (Jonathan Cerna); writing&#x02014;original draft preparation, A.A., J.C. (Jean Clore and Jonathan Cerna), R.S., E.T.H.-W., and M.E.H.; visualization, A.A.; supervision, E.T.H.-W. and M.E.H.; project administration, M.E.H.; funding acquisition, J.C. (Jean Clore), R.S., E.T.H.-W., and M.E.H. All authors have contributed to writing&#x02014;review and editing. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>The study was conducted in accordance with the Declaration of Helsinki, and approved by the Institutional Review Board (or Ethics Committee) of the University of Illinois Urbana-Champaign (protocol 21769, approved on 1 December 2022) and University of Illinois College of Medicine at Peoria (protocol 1880297, approved on 27 April 2022).</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01241"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Celano</surname><given-names>C.M.</given-names></name>
<name><surname>Daunis</surname><given-names>D.J.</given-names></name>
<name><surname>Lokko</surname><given-names>H.N.</given-names></name>
<name><surname>Campbell</surname><given-names>K.A.</given-names></name>
<name><surname>Huffman</surname><given-names>J.C.</given-names></name>
</person-group><article-title>Anxiety Disorders and Cardiovascular Disease</article-title><source>Curr. Psychiatry Rep.</source><year>2016</year><volume>18</volume><fpage>101</fpage><pub-id pub-id-type="doi">10.1007/s11920-016-0739-5</pub-id><pub-id pub-id-type="pmid">27671918</pub-id>
</element-citation></ref><ref id="B2-sensors-25-01241"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Critchley</surname><given-names>H.D.</given-names></name>
</person-group><article-title>Study of the Stress Response: Role of Anxiety, Cortisol and DHEAs</article-title><source>Neuroscientist</source><year>2002</year><volume>8</volume><fpage>132</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1177/107385840200800209</pub-id><pub-id pub-id-type="pmid">11954558</pub-id>
</element-citation></ref><ref id="B3-sensors-25-01241"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>B&#x000e4;r</surname><given-names>K.J.</given-names></name>
<name><surname>Critchley</surname><given-names>H.</given-names></name>
</person-group><article-title>Autonomic Control</article-title><source>Brain Mapp. Encycl. Ref.</source><year>2015</year><volume>2</volume><fpage>635</fpage><lpage>642</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-397025-1.00058-0</pub-id></element-citation></ref><ref id="B4-sensors-25-01241"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Julian</surname><given-names>L.J.</given-names></name>
</person-group><article-title>Measures of Anxiety</article-title><source>Arthritis Care</source><year>2011</year><volume>63</volume><fpage>20561</fpage><pub-id pub-id-type="doi">10.1002/acr.20561</pub-id></element-citation></ref><ref id="B5-sensors-25-01241"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Healey</surname><given-names>J.A.</given-names></name>
<name><surname>Picard</surname><given-names>R.W.</given-names></name>
</person-group><article-title>Detecting Stress during Real-World Driving Tasks Using Physiological Sensors</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2005</year><volume>6</volume><fpage>156</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1109/TITS.2005.848368</pub-id></element-citation></ref><ref id="B6-sensors-25-01241"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Elgendi</surname><given-names>M.</given-names></name>
<name><surname>Galli</surname><given-names>V.</given-names></name>
<name><surname>Ahmadizadeh</surname><given-names>C.</given-names></name>
<name><surname>Menon</surname><given-names>C.</given-names></name>
</person-group><article-title>Dataset of Psychological Scales and Physiological Signals Collected for Anxiety Assessment Using a Portable Device</article-title><source>Data</source><year>2022</year><volume>7</volume><elocation-id>132</elocation-id><pub-id pub-id-type="doi">10.3390/data7090132</pub-id></element-citation></ref><ref id="B7-sensors-25-01241"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Haouij</surname><given-names>N.E.</given-names></name>
<name><surname>Poggi</surname><given-names>J.M.</given-names></name>
<name><surname>Sevestre-Ghalila</surname><given-names>S.</given-names></name>
<name><surname>Ghozi</surname><given-names>R.</given-names></name>
<name><surname>Jadane</surname><given-names>M.</given-names></name>
</person-group><article-title>Affective ROAD System and Database to Assess Driver&#x02019;s Attention</article-title><source>Proceedings of the 33rd Annual ACM Symposium on Applied Computing</source><conf-loc>Pau, France</conf-loc><conf-date>9&#x02013;13 April 2018</conf-date><publisher-name>Association for Computing Machinery</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2018</year><fpage>800</fpage><lpage>803</lpage><pub-id pub-id-type="doi">10.1145/3167132.3167395</pub-id></element-citation></ref><ref id="B8-sensors-25-01241"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>P.</given-names></name>
<name><surname>Reiss</surname><given-names>A.</given-names></name>
<name><surname>Duerichen</surname><given-names>R.</given-names></name>
<name><surname>Laerhoven</surname><given-names>K.</given-names></name>
</person-group><article-title>Van Introducing WeSAD, a Multimodal Dataset for Wearable Stress and Affect Detection</article-title><source>Proceedings of the 2018 International Conference on Multimodal Interaction</source><conf-loc>Boulder, CO, USA</conf-loc><conf-date>16&#x02013;20 October 2018</conf-date><publisher-name>Association for Computing Machinery</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2018</year><fpage>400</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1145/3242969.3242985</pub-id></element-citation></ref><ref id="B9-sensors-25-01241"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shaukat-Jali</surname><given-names>R.</given-names></name>
<name><surname>van Zalk</surname><given-names>N.</given-names></name>
<name><surname>Boyle</surname><given-names>D.E.</given-names></name>
</person-group><article-title>Detecting Subclinical Social Anxiety Using Physiological Data from a Wrist-Worn Wearable: Small-Scale Feasibility Study</article-title><source>JMIR Form. Res.</source><year>2021</year><volume>5</volume><fpage>32656</fpage><pub-id pub-id-type="doi">10.2196/32656</pub-id></element-citation></ref><ref id="B10-sensors-25-01241"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sanjay</surname><given-names>V.M.</given-names></name>
<name><surname>Ankith</surname><given-names>I.</given-names></name>
<name><surname>Dhanush</surname><given-names>G.C.</given-names></name>
<name><surname>Akshaya</surname><given-names>H.P.</given-names></name>
<name><surname>Malik</surname><given-names>J.</given-names></name>
<name><surname>Tejaswini</surname><given-names>B.N.</given-names></name>
</person-group><article-title>Anxiety Prediction during Stressful Scenarios Using Machine Learning</article-title><source>Proceedings of the 2022 6th International Conference on Intelligent Computing and Control Systems, ICICCS 2022</source><conf-loc>Madurai, India</conf-loc><conf-date>25&#x02013;27 May 2022</conf-date><fpage>1199</fpage><lpage>1202</lpage></element-citation></ref><ref id="B11-sensors-25-01241"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Albagmi</surname><given-names>F.M.</given-names></name>
<name><surname>Alansari</surname><given-names>A.</given-names></name>
<name><surname>Shawan</surname><given-names>D.S.A.</given-names></name>
<name><surname>AlNujaidi</surname><given-names>H.Y.</given-names></name>
<name><surname>Olatunji</surname><given-names>S.O.</given-names></name>
</person-group><article-title>Prediction of Generalized Anxiety Levels during the Covid-19 Pandemic: A Machine Learning-Based Modeling Approach</article-title><source>Inf. Inform. Med. Unlocked</source><year>2022</year><volume>28</volume><fpage>100854</fpage><pub-id pub-id-type="doi">10.1016/j.imu.2022.100854</pub-id></element-citation></ref><ref id="B12-sensors-25-01241"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ramani</surname><given-names>B.</given-names></name>
<name><surname>Patel</surname><given-names>W.</given-names></name>
<name><surname>Solanki</surname><given-names>K.</given-names></name>
</person-group><article-title>Stress Ocare: An Advance IoMT Based Physiological Data Analysis for Anxiety Status Prediction Using Cloud Computing</article-title><source>J. Discret. Math. Sci. Cryptogr.</source><year>2022</year><volume>25</volume><fpage>1019</fpage><lpage>1029</lpage><pub-id pub-id-type="doi">10.1080/09720529.2022.2072426</pub-id></element-citation></ref><ref id="B13-sensors-25-01241"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Srinath</surname><given-names>K.S.</given-names></name>
<name><surname>Kiran</surname><given-names>K.</given-names></name>
<name><surname>Pranavi</surname><given-names>S.</given-names></name>
<name><surname>Amrutha</surname><given-names>M.</given-names></name>
<name><surname>Shenoy</surname><given-names>P.D.</given-names></name>
<name><surname>Venugopal</surname><given-names>K.R.</given-names></name>
</person-group><article-title>Prediction of Depression, Anxiety and Stress Levels Using Dass-42</article-title><source>Proceedings of the 2022 IEEE 7th International conference for Convergence in Technology, I2CT 2022</source><conf-loc>Pune, India</conf-loc><conf-date>7&#x02013;9 April 2022</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B14-sensors-25-01241"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Perpetuini</surname><given-names>D.</given-names></name>
<name><surname>Chiarelli</surname><given-names>A.M.</given-names></name>
<name><surname>Cardone</surname><given-names>D.</given-names></name>
<name><surname>Filippini</surname><given-names>C.</given-names></name>
<name><surname>Rinella</surname><given-names>S.</given-names></name>
<name><surname>Massimino</surname><given-names>S.</given-names></name>
<name><surname>Bianco</surname><given-names>F.</given-names></name>
<name><surname>Bucciarelli</surname><given-names>V.</given-names></name>
<name><surname>Vinciguerra</surname><given-names>V.</given-names></name>
<name><surname>Fallica</surname><given-names>P.</given-names></name>
<etal/>
</person-group><article-title>Prediction of State Anxiety by Machine Learning Applied to Photoplethysmography Data</article-title><source>PeerJ</source><year>2021</year><volume>9</volume><fpage>e10448</fpage><pub-id pub-id-type="doi">10.7717/peerj.10448</pub-id><pub-id pub-id-type="pmid">33520434</pub-id>
</element-citation></ref><ref id="B15-sensors-25-01241"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ancillon</surname><given-names>L.</given-names></name>
<name><surname>Elgendi</surname><given-names>M.</given-names></name>
<name><surname>Menon</surname><given-names>C.</given-names></name>
</person-group><article-title>Machine Learning for Anxiety Detection Using Biosignals: A Review</article-title><source>Diagnostics</source><year>2022</year><volume>12</volume><elocation-id>1794</elocation-id><pub-id pub-id-type="doi">10.3390/diagnostics12081794</pub-id><pub-id pub-id-type="pmid">35892505</pub-id>
</element-citation></ref><ref id="B16-sensors-25-01241"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sato</surname><given-names>M.</given-names></name>
<name><surname>Ishikawa</surname><given-names>W.</given-names></name>
<name><surname>Suzuki</surname><given-names>T.</given-names></name>
<name><surname>Matsumoto</surname><given-names>T.</given-names></name>
<name><surname>Tsujii</surname><given-names>T.</given-names></name>
<name><surname>Sakatani</surname><given-names>K.</given-names></name>
</person-group><article-title>Bayesian STAI Anxiety Index Predictions Based on Prefrontal Cortex NIRS Data for the Resting State</article-title><source>Adv. Exp. Med. Biol.</source><year>2013</year><volume>765</volume><fpage>251</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1007/978-1-4614-4989-8_35</pub-id><pub-id pub-id-type="pmid">22879041</pub-id>
</element-citation></ref><ref id="B17-sensors-25-01241"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Baird</surname><given-names>A.</given-names></name>
<name><surname>Cummins</surname><given-names>N.</given-names></name>
<name><surname>Schnieder</surname><given-names>S.</given-names></name>
<name><surname>Krajewski</surname><given-names>J.</given-names></name>
<name><surname>Schuller</surname><given-names>B.W.</given-names></name>
</person-group><article-title>An Evaluation of the Effect of Anxiety on Speech ? Computational Prediction of Anxiety from Sustained Vowels</article-title><source>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</source><conf-loc>Shanghai, China</conf-loc><conf-date>25&#x02013;29 October 2020</conf-date><volume>Volume 2020</volume><fpage>4951</fpage><lpage>4955</lpage></element-citation></ref><ref id="B18-sensors-25-01241"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fukuda</surname><given-names>Y.</given-names></name>
<name><surname>Ishikawa</surname><given-names>W.</given-names></name>
<name><surname>Kanayama</surname><given-names>R.</given-names></name>
<name><surname>Matsumoto</surname><given-names>T.</given-names></name>
<name><surname>Takemura</surname><given-names>N.</given-names></name>
<name><surname>Sakatani</surname><given-names>K.</given-names></name>
</person-group><article-title>Bayesian Prediction of Anxiety Level in Aged People at Rest Using 2-Channel NIRS Data from Prefrontal Cortex</article-title><source>Adv. Exp. Med. Biol.</source><year>2014</year><volume>812</volume><fpage>303</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1007/978-1-4939-0620-8_40</pub-id><pub-id pub-id-type="pmid">24729247</pub-id>
</element-citation></ref><ref id="B19-sensors-25-01241"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Henry</surname><given-names>J.</given-names></name>
<name><surname>Lloyd</surname><given-names>H.</given-names></name>
<name><surname>Turner</surname><given-names>M.</given-names></name>
<name><surname>Kendrick</surname><given-names>C.</given-names></name>
</person-group><article-title>On the Robustness of Machine Learning Models for Stress and Anxiety Recognition from Heart Activity Signals</article-title><source>IEEE Sens. J.</source><year>2023</year><volume>23</volume><fpage>14428</fpage><lpage>14436</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2023.3276413</pub-id></element-citation></ref><ref id="B20-sensors-25-01241"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xiong</surname><given-names>H.</given-names></name>
<name><surname>Berkovsky</surname><given-names>S.</given-names></name>
<name><surname>Romano</surname><given-names>M.</given-names></name>
<name><surname>Sharan</surname><given-names>R.V.</given-names></name>
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Coiera</surname><given-names>E.</given-names></name>
<name><surname>McLellan</surname><given-names>L.F.</given-names></name>
</person-group><article-title>Prediction of Anxiety Disorders Using a Feature Ensemble Based Bayesian Neural Network</article-title><source>J. Biomed. Inf. Inform.</source><year>2021</year><volume>123</volume><elocation-id>103921</elocation-id><pub-id pub-id-type="doi">10.1016/j.jbi.2021.103921</pub-id></element-citation></ref><ref id="B21-sensors-25-01241"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Adheena</surname><given-names>M.A.</given-names></name>
<name><surname>Sindhu</surname><given-names>N.</given-names></name>
<name><surname>Jerritta</surname><given-names>S.</given-names></name>
</person-group><article-title>Physiological Detection of Anxiety</article-title><source>Proceedings of the 2018 International Conference on Circuits and Systems in Digital Enterprise Technology (ICCSDET)</source><conf-loc>Kottayam, India</conf-loc><conf-date>21&#x02013;22 December 2018</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway Township, NJ, USA</publisher-loc><year>2018</year><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B22-sensors-25-01241"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alkurdi</surname><given-names>A.</given-names></name>
<name><surname>Clore</surname><given-names>J.</given-names></name>
<name><surname>Sowers</surname><given-names>R.</given-names></name>
<name><surname>Hsiao-Wecksler</surname><given-names>E.T.</given-names></name>
<name><surname>Hernandez</surname><given-names>M.E.</given-names></name>
</person-group><article-title>Resilience of Machine Learning Models in Anxiety Detection: Assessing the Impact of Gaussian Noise on Wearable Sensors</article-title><source>Appl. Sci.</source><year>2025</year><volume>15</volume><elocation-id>88</elocation-id><pub-id pub-id-type="doi">10.3390/app15010088</pub-id></element-citation></ref><ref id="B23-sensors-25-01241"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pan</surname><given-names>S.J.</given-names></name>
<name><surname>Yang</surname><given-names>Q.</given-names></name>
</person-group><article-title>A Survey on Transfer Learning</article-title><source>IEEE Trans. Knowl. Data Eng.</source><year>2010</year><volume>22</volume><fpage>1345</fpage><lpage>1359</lpage><pub-id pub-id-type="doi">10.1109/TKDE.2009.191</pub-id></element-citation></ref><ref id="B24-sensors-25-01241"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Singh</surname><given-names>M.</given-names></name>
<name><surname>Prakash</surname><given-names>P.</given-names></name>
<name><surname>Kaur</surname><given-names>R.</given-names></name>
<name><surname>Sowers</surname><given-names>R.</given-names></name>
<name><surname>Bra&#x00161;i&#x00107;</surname><given-names>J.R.</given-names></name>
<name><surname>Hernandez</surname><given-names>M.E.</given-names></name>
</person-group><article-title>A Deep Learning Approach for Automatic and Objective Grading of the Motor Impairment Severity in Parkinson&#x02019;s Disease for Use in Tele-Assessments</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>9004</elocation-id><pub-id pub-id-type="doi">10.3390/s23219004</pub-id><pub-id pub-id-type="pmid">37960703</pub-id>
</element-citation></ref><ref id="B25-sensors-25-01241"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Weiss</surname><given-names>K.</given-names></name>
<name><surname>Khoshgoftaar</surname><given-names>T.M.</given-names></name>
<name><surname>Wang</surname><given-names>D.D.</given-names></name>
</person-group><article-title>A Survey of Transfer Learning</article-title><source>J. Big Data</source><year>2016</year><volume>3</volume><fpage>9</fpage><pub-id pub-id-type="doi">10.1186/s40537-016-0043-6</pub-id></element-citation></ref><ref id="B26-sensors-25-01241"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhuang</surname><given-names>F.</given-names></name>
<name><surname>Qi</surname><given-names>Z.</given-names></name>
<name><surname>Duan</surname><given-names>K.</given-names></name>
<name><surname>Xi</surname><given-names>D.</given-names></name>
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
<name><surname>Zhu</surname><given-names>H.</given-names></name>
<name><surname>Xiong</surname><given-names>H.</given-names></name>
<name><surname>He</surname><given-names>Q.</given-names></name>
</person-group><article-title>A Comprehensive Survey on Transfer Learning</article-title><source>Proc. IEEE</source><year>2021</year><volume>109</volume><fpage>43</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2020.3004555</pub-id></element-citation></ref><ref id="B27-sensors-25-01241"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shao</surname><given-names>L.</given-names></name>
<name><surname>Zhu</surname><given-names>F.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
</person-group><article-title>Transfer Learning for Visual Categorization: A Survey</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2015</year><volume>26</volume><fpage>1019</fpage><lpage>1034</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2014.2330900</pub-id><pub-id pub-id-type="pmid">25014970</pub-id>
</element-citation></ref><ref id="B28-sensors-25-01241"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Watson</surname><given-names>D.</given-names></name>
<name><surname>Clark</surname><given-names>L.A.</given-names></name>
<name><surname>Tellegen</surname><given-names>A.</given-names></name>
</person-group><article-title>Development and Validation of Brief Measures of Positive and Negative Affect: The PANAS Scales</article-title><source>J. Pers. Soc. Psychol.</source><year>1988</year><volume>54</volume><fpage>1063</fpage><lpage>1070</lpage><pub-id pub-id-type="doi">10.1037/0022-3514.54.6.1063</pub-id><pub-id pub-id-type="pmid">3397865</pub-id>
</element-citation></ref><ref id="B29-sensors-25-01241"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Spielberger</surname><given-names>C.D.</given-names></name>
<name><surname>Gonzalez-Reigosa</surname><given-names>F.</given-names></name>
<name><surname>Martinez-Urrutia</surname><given-names>A.</given-names></name>
<name><surname>Natalicio</surname><given-names>L.F.S.</given-names></name>
<name><surname>Natalicio</surname><given-names>D.S.</given-names></name>
</person-group><article-title>The State-Trait Anxiety Inventory</article-title><source>Interam. J. Psychol.</source><year>1971</year><volume>5</volume><fpage>3</fpage><lpage>4</lpage></element-citation></ref><ref id="B30-sensors-25-01241"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Helton</surname><given-names>W.S.</given-names></name>
</person-group><article-title>Validation of a Short Stress State Questionnaire</article-title><source>Proc. Hum. Human. Factors Ergon. Soc. Annu. Meet.</source><year>2004</year><volume>48</volume><fpage>1238</fpage><lpage>1242</lpage><pub-id pub-id-type="doi">10.1177/154193120404801107</pub-id></element-citation></ref><ref id="B31-sensors-25-01241"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>M.</given-names></name>
<name><surname>Cerna</surname><given-names>J.</given-names></name>
<name><surname>Alkurdi</surname><given-names>A.</given-names></name>
<name><surname>Dogan</surname><given-names>A.</given-names></name>
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Clore</surname><given-names>J.L.</given-names></name>
<name><surname>Sowers</surname><given-names>R.</given-names></name>
<name><surname>Hsiao-Wecksler</surname><given-names>E.T.</given-names></name>
<name><surname>Hernandez</surname><given-names>M.E.</given-names></name>
</person-group><article-title>Physical, Social and Cognitive Stressor Identification Using Electrocardiography-Derived Features and Machine Learning from a Wearable Device</article-title><source>Proceedings of the 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>15&#x02013;19 July 2024</conf-date><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B32-sensors-25-01241"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Malik</surname><given-names>M.</given-names></name>
<name><surname>Camm</surname><given-names>A.J.</given-names></name>
<name><surname>Bigger</surname><given-names>J.T.</given-names></name>
<name><surname>Breithardt</surname><given-names>G.</given-names></name>
<name><surname>Cerutti</surname><given-names>S.</given-names></name>
<name><surname>Cohen</surname><given-names>R.J.</given-names></name>
<name><surname>Coumel</surname><given-names>P.</given-names></name>
<name><surname>Fallen</surname><given-names>E.L.</given-names></name>
<name><surname>Kennedy</surname><given-names>H.L.</given-names></name>
<name><surname>Kleiger</surname><given-names>R.E.</given-names></name>
<etal/>
</person-group><article-title>Heart Rate Variability: Standards of Measurement, Physiological Interpretation, and Clinical Use</article-title><source>Circulation</source><year>1996</year><volume>93</volume><fpage>1043</fpage><lpage>1065</lpage><pub-id pub-id-type="doi">10.1093/oxfordjournals.eurheartj.a014868</pub-id><pub-id pub-id-type="pmid">8598068</pub-id>
</element-citation></ref><ref id="B33-sensors-25-01241"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Scholkmann</surname><given-names>F.</given-names></name>
<name><surname>Boss</surname><given-names>J.</given-names></name>
<name><surname>Wolf</surname><given-names>M.</given-names></name>
</person-group><article-title>An Efficient Algorithm for Automatic Peak Detection in Noisy Periodic and Quasi-Periodic Signals</article-title><source>Algorithms</source><year>2012</year><volume>5</volume><fpage>588</fpage><lpage>603</lpage><pub-id pub-id-type="doi">10.3390/a5040588</pub-id></element-citation></ref><ref id="B34-sensors-25-01241"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gil-Martin</surname><given-names>M.</given-names></name>
<name><surname>San-Segundo</surname><given-names>R.</given-names></name>
<name><surname>Mateos</surname><given-names>A.</given-names></name>
<name><surname>Ferreiros-Lopez</surname><given-names>J.</given-names></name>
</person-group><article-title>Human Stress Detection with Wearable Sensors Using Convolutional Neural Networks</article-title><source>IEEE Aerosp. Electron. Syst. Mag.</source><year>2022</year><volume>37</volume><fpage>60</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1109/MAES.2021.3115198</pub-id></element-citation></ref><ref id="B35-sensors-25-01241"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ghosh</surname><given-names>S.</given-names></name>
<name><surname>Kim</surname><given-names>S.</given-names></name>
<name><surname>Ijaz</surname><given-names>M.F.</given-names></name>
<name><surname>Singh</surname><given-names>P.K.</given-names></name>
<name><surname>Mahmud</surname><given-names>M.</given-names></name>
</person-group><article-title>Classification of Mental Stress from Wearable Physiological Sensors Using Image-Encoding-Based Deep Neural Network</article-title><source>Biosensors</source><year>2022</year><volume>12</volume><elocation-id>1153</elocation-id><pub-id pub-id-type="doi">10.3390/bios12121153</pub-id><pub-id pub-id-type="pmid">36551120</pub-id>
</element-citation></ref><ref id="B36-sensors-25-01241"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dziezyc</surname><given-names>M.</given-names></name>
<name><surname>Gjoreski</surname><given-names>M.</given-names></name>
<name><surname>Kazienko</surname><given-names>P.</given-names></name>
<name><surname>Saganowski</surname><given-names>S.</given-names></name>
<name><surname>Gams</surname><given-names>M.</given-names></name>
</person-group><article-title>Can We Ditch Feature Engineering? End-to-End Deep Learning for Affect Recognition from Physiological Sensor Data</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>6535</elocation-id><pub-id pub-id-type="doi">10.3390/s20226535</pub-id><pub-id pub-id-type="pmid">33207564</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01241-f001"><label>Figure 1</label><caption><p>(<bold>Left</bold>) Experimental setup for in-lab and real-world data in RADWear and WEAR studies. (<bold>Middle</bold>) Experimental setup for data collection using wearable sensors, and (<bold>Right</bold>) pipeline for processing and analysis.</p></caption><graphic xlink:href="sensors-25-01241-g001" position="float"/></fig><fig position="float" id="sensors-25-01241-f002"><label>Figure 2</label><caption><p>STAI score for one of the participants at the different test conditions experienced. The threshold considered to be indicative of the observation of anxiety is 11 for the STAI Y6. It can be observed that the meditation session reduced the anxiety level, while the cold pressor test (CPT) increased it. For this participant, the Trier Social Stress Test (TSST) seemed to be the highest cause of anxiety.</p></caption><graphic xlink:href="sensors-25-01241-g002" position="float"/></fig><fig position="float" id="sensors-25-01241-f003"><label>Figure 3</label><caption><p>Flowchart outlining pipeline for processing and analyzing wearable sensor data from raw data collection to machine learning.</p></caption><graphic xlink:href="sensors-25-01241-g003" position="float"/></fig><fig position="float" id="sensors-25-01241-f004"><label>Figure 4</label><caption><p>The F1 score of transfer learning models are shown with the left column showing F<sub>1</sub> score for models trained at different noise levels at different signal-to-noise ratios (SNRs), shown in the x-axis. The right column shows F<sub>1</sub> score for each of the tested models for all SNRs. F<sub>1</sub> score, as tested on calibration data for WEAR and RADWear in the top row, WEAR in-lab sessions in the middle row, and on RADWear in-the-wild in the bottom row.</p></caption><graphic xlink:href="sensors-25-01241-g004" position="float"/></fig><table-wrap position="float" id="sensors-25-01241-t001"><object-id pub-id-type="pii">sensors-25-01241-t001_Table 1</object-id><label>Table 1</label><caption><p>Noise factors considered for each subset of the datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th colspan="4" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Physical Configuration and Activity</th><th colspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Environmental Noise</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Seated</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standing</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Walking</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Other</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixed</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Variable</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WESAD</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Calibration (RADWear/WEAR)</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">In-lab session (WEAR)</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">In-the-wild session (RADWear)</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin;background:black" rowspan="1" colspan="1">
</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Black color used to indicate physical configuration and activity or environmental noise condition in each dataset.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-01241-t002"><object-id pub-id-type="pii">sensors-25-01241-t002_Table 2</object-id><label>Table 2</label><caption><p>Class distribution for each subset of the datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Not Anxious</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Anxiety</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Adjusted</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">WESAD</td><td align="center" valign="middle" rowspan="1" colspan="1">70%</td><td align="center" valign="middle" rowspan="1" colspan="1">30%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RADWear + WEAR Calibration</td><td align="center" valign="middle" rowspan="1" colspan="1">58%</td><td align="center" valign="middle" rowspan="1" colspan="1">42%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">WEAR in-lab</td><td align="center" valign="middle" rowspan="1" colspan="1">59%</td><td align="center" valign="middle" rowspan="1" colspan="1">41%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000d7;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RADWear in-the-wild</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.66%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.34%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">to 30% and 41%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01241-t003"><object-id pub-id-type="pii">sensors-25-01241-t003_Table 3</object-id><label>Table 3</label><caption><p>F1-score performance results of traditional ML models trained and tested on the same subset of the datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Data</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">XGB</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">DT</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">LDA</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">RF</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ACC</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F<sub>1</sub></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ACC</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F<sub>1</sub></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ACC</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F<sub>1</sub></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ACC</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F<sub>1</sub></th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WESAD</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#BFBFBF" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#BFBFBF" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#BFBFBF" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#BFBFBF" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.89</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RADWear + WEAR Calibration</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#BFBFBF" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.86</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WEAR in-lab</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#BFBFBF" rowspan="1" colspan="1">0.93</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#BFBFBF" rowspan="1" colspan="1">0.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.76</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RADWear in-the-wild (balanced to 30%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.77</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#BFBFBF" rowspan="1" colspan="1">0.97</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#BFBFBF" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.72</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RADWear in-the-wild (balanced to 41%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.58</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#BFBFBF" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#BFBFBF" rowspan="1" colspan="1">0.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Random guess</td><td colspan="8" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">0.5</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Grey color used to indicate top performing classifier, based on accuracy (ACC) or F1 score (F<sub>1</sub>), for each dataset.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-01241-t004"><object-id pub-id-type="pii">sensors-25-01241-t004_Table 4</object-id><label>Table 4</label><caption><p>Feature importance for traditional ML models on calibration, in-lab, and in-the-wild data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Modality</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Modality Weighted</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Feature</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Feature Weighted</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RF</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">DT</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">XGB</th></tr></thead><tbody><tr><td rowspan="13" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">RADWear in-the-wild</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">ECG</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin;background:#D9D9D9" colspan="1">0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">ECG<sub>min</sub></td><td align="center" valign="middle" style="background:#D9D9D9" rowspan="1" colspan="1">0.17</td><td align="center" valign="middle" style="background:#D9D9D9" rowspan="1" colspan="1">0.14</td><td align="center" valign="middle" style="background:#D9D9D9" rowspan="1" colspan="1">0.18</td><td align="center" valign="middle" style="background:#D9D9D9" rowspan="1" colspan="1">0.19</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ECG<sub>std</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ECG<sub>max</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.07</td></tr><tr><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Resp</td><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">Resp<sub>Exhal std</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Resp<sub>mean</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Resp<sub>rate</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Resp<sub>I/E</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resp<sub>min</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.08</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">EDA</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">EDA<sub>mean</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EDA<sub>drange</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.06</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BVP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BVP<sub>peak freq</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TEMP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TEMP<sub>mean</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ACC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ACC<sub>net w mean</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td rowspan="13" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">RADWear + WEAR Calibration </td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">EDA</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.1</td><td align="center" valign="middle" rowspan="1" colspan="1">EDA<sub>mean</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.17</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EDA<sub>max</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.05</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">RESP</td><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">Resp<sub>min</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Resp<sub>mean</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Resp<sub>rate</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resp<sub>Inhalmean</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BVP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BVP<sub>max</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">ACC</td><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin;background:#D9D9D9" colspan="1">0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">ACC<sub>net w std</sub></td><td align="center" valign="middle" style="background:#D9D9D9" rowspan="1" colspan="1">0.21</td><td align="center" valign="middle" style="background:#D9D9D9" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" style="background:#D9D9D9" rowspan="1" colspan="1">0.26</td><td align="center" valign="middle" style="background:#D9D9D9" rowspan="1" colspan="1">0.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ACC<sub>net w mean</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ACC<sub>x min</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ACC<sub>x C std</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ACC <sub>x min</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ACC<sub>x mean</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td rowspan="11" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">WEAR in-lab </td><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">ECG</td><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">ECG<sub>bpm</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ECG<sub>max</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.14</td><td align="center" valign="middle" style="background:#D9D9D9" rowspan="1" colspan="1">0.12</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ECG<sub>min</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ECG<sub>std</sub></td><td align="center" valign="middle" style="border-bottom:solid thin;background:#D9D9D9" rowspan="1" colspan="1">0.15</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#D9D9D9" rowspan="1" colspan="1">0.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.07</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">EDA</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">EDA<sub>max</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EDA<sub>mean</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">RESP</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">Resp<sub>mean</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resp<sub>min</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.05</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">ACC</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">ACC<sub>x mean</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ACC<sub>net w mean</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TEMP</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#D9D9D9" rowspan="1" colspan="1">0.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TEMP<sub>mean</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.12</td><td align="center" valign="middle" style="border-bottom:solid thin;background:#D9D9D9" rowspan="1" colspan="1">0.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Grey color used to indicate most important modality and feature for each dataset and classifier.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-01241-t005"><object-id pub-id-type="pii">sensors-25-01241-t005_Table 5</object-id><label>Table 5</label><caption><p>Performance results for transfer learning models on calibration, in-lab, and in-the-wild data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Model</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">DT</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">RF</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">XGB</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F<sub>1</sub>-Score</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F<sub>1</sub>-Score</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F<sub>1</sub>-Score</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">RADWear + WEAR Calibration</td><td align="center" valign="middle" rowspan="1" colspan="1">0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.76</td><td align="center" valign="middle" rowspan="1" colspan="1">0.76</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">WEAR in-lab</td><td align="center" valign="middle" rowspan="1" colspan="1">0.18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93</td><td align="center" valign="middle" rowspan="1" colspan="1">0.94</td><td align="center" valign="middle" rowspan="1" colspan="1">0.44</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RADWear in-the-wild </td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.18</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.18</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.93</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.93</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.44</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.44</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">anxiety class balanced to 41%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01241-t006"><object-id pub-id-type="pii">sensors-25-01241-t006_Table 6</object-id><label>Table 6</label><caption><p>Anxiety classification performance results of related works using physiological signals.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Authors</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset Used</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Features</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Psychological Measures</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Physical Activity or Condition</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Environmental Noise</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Classification/Cut-Off</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Performance (Accuracy)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Performance (F1)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">TL Performance (Accuracy)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">TL Performance (F1)</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Henry et al., 2023 [<xref rid="B19-sensors-25-01241" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CASE: customized anxiety index from self-report arousal and valance scores</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ECG and BVP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anxiety index</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Seated</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixed</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anxiety State, using anxiety index with 0.5 cutoff</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RF(BVP) = 0.752; SVM(ECG) = 0.763.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RF(BVP) = 0.759; SVM(ECG) = 0.770.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WESAD to CASE: SVM, RF, XGBoost, MLP, ResNet(BVP) = 0.501; XGBoost(ECG) = 0.605.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WESAD to CASE:FTTP(BVP) = 0.107; FTTP(ECG) = 0.554.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WESAD: Study protocol design of baseline, amusement, stress</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ECG and BVP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PANAS, STAI, SAM, and SSSQ</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Seated/Standing</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixed</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stress vs. Non-Stress using protocol label</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM(BVP) = 0.745; SVM(ECG) = 0.811.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM(BVP) = 0.773; SVM(ECG) = 0.818.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CASE to WESAD: MLP(BVP) = 0.587; RF(ECG) = 0.657.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CASE to WESAD: SVM(BVP) = 0.694; FTT(ECG) = 0.694.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Perpetuini et al., 2020 [<xref rid="B14-sensors-25-01241" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Study protocol examined supine rest with and without video</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BVP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">STAI</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Supine</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixed</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anxiety State, using STAI with 40 cutoff</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC GLM(BVP) = 0.88; </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA</td></tr></tbody></table></table-wrap></floats-group></article>