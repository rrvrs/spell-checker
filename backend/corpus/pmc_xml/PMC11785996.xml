<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39890963</article-id><article-id pub-id-type="pmc">PMC11785996</article-id><article-id pub-id-type="publisher-id">87358</article-id><article-id pub-id-type="doi">10.1038/s41598-025-87358-0</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Similarity and quality metrics for MR image-to-image translation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Dohmen</surname><given-names>Melanie</given-names></name><address><email>Melanie.Dohmen@bayer.com</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Klemens</surname><given-names>Mark A.</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Baltruschat</surname><given-names>Ivo M.</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Truong</surname><given-names>Tuan</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Lenga</surname><given-names>Matthias</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04hmn8g73</institution-id><institution-id institution-id-type="GRID">grid.420044.6</institution-id><institution-id institution-id-type="ISNI">0000 0004 0374 4101</institution-id><institution>Bayer AG, Radiology, </institution></institution-wrap>Berlin, Germany </aff></contrib-group><pub-date pub-type="epub"><day>31</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>31</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><elocation-id>3853</elocation-id><history><date date-type="received"><day>25</day><month>9</month><year>2024</year></date><date date-type="accepted"><day>17</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Image-to-image translation can create large impact in medical imaging, as images can be synthetically transformed to other modalities, sequence types, higher resolutions or lower noise levels. To ensure patient safety, these methods should be validated by human readers, which requires a considerable amount of time and costs. Quantitative metrics can effectively complement such studies and provide reproducible and objective assessment of synthetic images. If a reference is available, the similarity of MR images is frequently evaluated by SSIM and PSNR metrics, even though these metrics are not or too sensitive regarding specific distortions. When reference images to compare with are not available, non-reference quality metrics can reliably detect specific distortions, such as blurriness. To provide an overview on distortion sensitivity, we quantitatively analyze 11 similarity (reference) and 12 quality (non-reference) metrics for assessing synthetic images. We additionally include a metric on a downstream segmentation task. We investigate the sensitivity regarding 11 kinds of distortions and typical MR artifacts, and analyze the influence of different normalization methods on each metric and distortion. Finally, we derive recommendations for effective usage of the analyzed similarity and quality metrics for evaluation of image-to-image translation models.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Metrics</kwd><kwd>Image synthesis</kwd><kwd>MRI</kwd><kwd>Similarity</kwd><kwd>Image quality</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Computer science</kwd><kwd>Magnetic resonance imaging</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><sec id="Sec2"><title>Image synthesis</title><p id="Par2">Recent advances in generative artificial intelligence (AI) within the natural image domain have demonstrated a remarkable capability to produce synthetic images with high fidelity, capturing nuances such as lighting variations, textures, and object placements<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. The implications of these advancements are far-reaching, with applications spanning various domains such as computer vision, graphics, augmented and virtual reality, or creative arts. Still, many challenges remain, including potential biases in generated images, the need for enhanced diversity and controllability in generation, and ethical considerations surrounding the use of AI-generated content.<fig id="Fig1"><label>Fig. 1</label><caption><p>Overview of image-to-image translation and types of evaluation metrics. (1) A source image from a source domain is transformed to a prediction in the target domain by an image-to-image translation model. If a reference image is given, this also belongs to the target domain. Then there are multiple possibilities to apply metrics. (A) Reference metrics directly compare prediction and reference image. (B) Non-reference metrics can be applied to the prediction alone, but also&#x02014;if available&#x02014;to a reference image. Then both non-reference metric scores can be compared. As an additional option (C), the reference and the prediction can be further processed in a downstream task, i.e. a segmentation task as a second (2) step. The performance of both downstream task results is then assessed with a downstream task metric, i.e. a segmentation metric.</p></caption><graphic xlink:href="41598_2025_87358_Fig1_HTML" id="MO1"/></fig></p><p id="Par3">The adaptation of generative modeling concepts such as Generative Adversarial Networks (GANs)<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> or diffusion models<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup> to the medical imaging domain is being explored with continuously growing interest and many relevant use cases have already been identified, such as data augmentation<sup><xref ref-type="bibr" rid="CR6">6</xref></sup> or image conversion and enhancement by image-to-image translation<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. For training deep neural networks, typically very large and diverse data sets are needed, but these are rarely available for medical imaging tasks. Generative networks can amend available data with synthetic samples and thereby improve the performance of other image analysis tasks<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>.</p></sec><sec id="Sec3"><title>Image-to-image translation</title><p id="Par4">Another line of research develops conditional generative models for image-to-image translation, which aims to translate a given source image to a synthetic target image, showing the same content (e.g. patient, organ or biological sample) as the source but with a different appearance (e.g. contrasting structures differently, changing texture or resolution). Source and target typically belong to different image domains. Depending on the availability of source and target pairs of the same patient or structure in the training data, image-to-image translation can be performed in a paired<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> or unpaired<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> manner, also referred to as supervised or unsupervised. Image-to-image translation models are trained to transfer characteristics from the target domain to a specific image from the source domain without changing the represented image content.</p></sec><sec id="Sec4"><title>Medical image synthesis tasks</title><p id="Par5">These tasks are specifically interesting in medical applications<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, because they allow to translate a medical image from one domain to another. Source and target domain may differ by imaging modality or acquisition parameters, such that image-to-image translation allows translation, e.g., from computed tomography (CT) to magnetic resonance (MR) imaging<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> or vice-versa<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, from T1-weighted MR to T2-weighted MR<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, CT to positron emission tomography (PET)<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, PET to CT<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, or from native MR to contrast-enhanced MR<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>. There is a significant patient benefit, when the source image can be acquired with less harm, more quickly or at a lower cost, compared to the the target image, which might be preferred for diagnosis. Low-quality or low-resolution images can be restored, improved, or the resolution increased<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. For radiation therapy, which is planned on the basis of CT images, MR to CT synthesis has been investigated<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. MR to CT synthesis is also used to facilitate registration between both modalities<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. The translation between different MR sequences (T1-weighted, T2-weighted, T2-FLAIR) can complete missing series for improved diagnosis<sup><xref ref-type="bibr" rid="CR18">18</xref>&#x02013;<xref ref-type="bibr" rid="CR21">21</xref></sup>. For reducing patient burden with contrast agents, researchers work on the translation of native or low-dose MR images to synthetic high contrast-enhanced images<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>.</p></sec><sec id="Sec5"><title>Validation metrics for image synthesis</title><p id="Par6">However, validation of these approaches is not straightforward. If a reference image, representing the desired synthesis result, is available for each generated image, a group of metrics called reference metrics can assess the similarity between predicted and reference images (see Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>A). These reference images are often already leveraged for training synthesis models in a paired or supervised manner. Reference metrics are sometimes called full-reference metric to distinguish from weak-reference metrics, that only use partial information or features of the reference image. The term similarity metric is also synonymously used for reference metric.</p><p id="Par7">However, not always paired reference images are available. In this case, non-reference metrics, also called quality metrics, can be applied (see Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>B). As quality requirements may vary between tasks and image domains, metrics for different aspects have been developed, e.g. for measuring blurriness<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, contrast<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, noisiness<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> or other features inspired by human perception<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Depending on the application of synthesized images, the evaluation of the images in a downstream task is more appropriate, than the evaluation of the images themselves<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Downstream task metrics operate on further processed results and not on the predicted or reference image (see Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>C). For GANs, so-called distribution based metrics are very popular<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. These assess the distributions of extracted image features of a larger set of generated images. For example, the Inception score<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> assesses how distinct and evenly distributed classes are predicted by an Inception architecture based classifier trained on ImageNet (InceptionNet<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>). And the Fr&#x000ea;chet Inception Distance<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> assesses how well Gaussian modeled activation layer distributions of the InceptionNet match between generated and reference image sets. As these metrics do not assess single images, these metrics are not in the scope of this study.</p><p id="Par8">In the domain of natural images, reference metrics have been extensively tested on synthetically distorted images. The Tampere Image Database<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> and the LIVE Image Quality Assessment Database<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup> contain 25-29 reference images and differently distorted versions thereof, additionally annotated with a human quality score. Metric scores for various full-reference metrics were correlated with human scores to identify the best performing metrics.</p><p id="Par9">Specifically for image synthesis, a study<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> assessed metrics on outputs of image synthesis results. However, these results cannot be fully transferred to medical image synthesis. Even though similar studies for medical images<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup> exist, the included distortions, such as JPEG compression artifacts and white noise are less relevant for medical image synthesis. Instead, MR imaging, including acquisition and reconstruction, exhibits very specific artifacts, such as bias field, ghosting or stripe artifacts. Additionally, certain synthesis models may introduce other kinds of distortions, e.g. the insertion of artificial structures or registration artifacts that arise from misaligned source and target images. In this study, we create a similar benchmark dataset for the medical images domain consisting of 100 MR reference images and 11 mostly MR specific distortions. Applying the distortions in an isolated manner in five defined strengths results in well defined distortions. This allows us to compare different metrics regarding their sensitivity towards each kind of distortion separately, instead of averaging over a fixed group of distortions. This is crucial, because the effect of distortions on image quality may be rated differently for different medical applications.</p></sec><sec id="Sec6"><title>Validation in the medical domain</title><p id="Par10">Analysis, processing and generation of medical images can have severe impact on patient outcome and patient safety. Therefore, the Food and Drug Administration (FDA) in the United States requires technical and clinical evaluation for every software as a medical device to be approved<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. Technical validation provides objective evidence, that the software correctly and reliably processes input data and generates output data with the appropriate level of accuracy and reproducibility. Clinical validation measures the ability of a software to yield a clinically meaningful output in the target health care situation. For evaluating medical algorithms based on artificial intelligence, guidelines about trial protocol designs<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> have been agreed on by an expert consortium. However, systemic reviews of published papers on AI-based algorithms in the medical domain have revealed, that only a small fraction of studies adheres to such guidelines, i.e. external validation<sup><xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup>. Often, details of clinical validation studies, such as test population statistics, are not published, not even for FDA-approved software<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Another review found that the median number of health-care professionals engaged in clinical validation was only four<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, which limits the reliability and generizability of such studies. Recent validation studies of FDA approved image-to-image translation software rely strongly on technical assessment of phantom measurements by similarity metrics<sup><xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup>. While the number of FDA approved medical devices based on AI software is still increasing by 14% (2022), the increase has been slowing down compared to 2020 (39%)<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>.</p><p id="Par11">The lack of adequate trials for clinical validation is certainly only one part of the problem. Appropriate technical validation is crucial at an even earlier stage of development. Metrics for biomedical image analysis and segmentation have been extensively described<sup><xref ref-type="bibr" rid="CR47">47</xref>,<xref ref-type="bibr" rid="CR48">48</xref></sup>, and can be leveraged to indirectly assess synthetic images via downstream tasks<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. Even though a huge amount of metrics has been used for the evaluation of synthetic medical images<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR50">50</xref></sup>, to our knowledge guidelines for the selection of appropriate similarity and quality metrics are not available. Loss functions for medical image registration, if used to compare a synthetic image with its reference image, are also leveraged for measuring image similarity, and overlap strongly with the selected reference metrics in this study<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. For validating synthetic MR images, especially structural similarity index measure (SSIM), and peak signal-to-noise ratio (PSNR) are used extensively. A review on image-to-image translation with generative adversarial networks (GANs) or convolutional neural networks (CNNs) in the medical domain<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> reported the use of SSIM in 84% studies and PSNR in 61% of studies, that synthesize MR images. A further review on synthetic contrast-enhancement of MR images<sup><xref ref-type="bibr" rid="CR53">53</xref></sup> reports evaluation by SSIM and PSNR in 75% of the studies. This is in opposition to known crucial weak points of SSIM and PSNR, such as underestimated blurriness<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>, bad correlation to human perception<sup><xref ref-type="bibr" rid="CR55">55</xref>&#x02013;<xref ref-type="bibr" rid="CR57">57</xref></sup> and difficulties with float-valued images<sup><xref ref-type="bibr" rid="CR58">58</xref>,<xref ref-type="bibr" rid="CR59">59</xref></sup>. Therefore, a systematic analysis of appropriate metrics for MR image synthesis validation is needed.</p></sec><sec id="Sec7"><title>Contribution</title><p id="Par12">In this paper, we provide a comprehensive analysis of the sensitivity of 11 reference and 12 non-reference metrics to 11 different distortions, that are relevant for MR image synthesis and of which some have not been assessed with the selected metrics before. Furthermore, we investigate the influence of five normalization methods before metric assessment. After analyzing metric sensitivity in detail, and discussing specific shortcomings or advantages of the investigated metrics, we recommend how to select and best apply metrics for validating image-to-image translation methods specifically for MR image synthesis.</p></sec></sec><sec id="Sec8"><title>Methods</title><p id="Par13">In this section, we give an overview of reference and non-reference metrics for assessing the quality of images. Since most reference and non-reference metrics strongly depend on the intensity value ranges of the images they assess, the examination of metrics must be combined with the examination of normalization methods, that adjust the intensity value ranges. Therefore, we first introduce normalization methods that are frequently used to bring MR images to a common intensity value scale or as prerequisite for certain metrics.</p><sec id="Sec9"><title>Intensity ranges and data formats</title><p id="Par14">For two reasons, normalization of medical images is needed prior to similarity or quality assessment. First, image intensities of non-quantitative image modalities are not comparable between two images, due to missing standardization. For example, in MR imaging, the same tissue may be represented by different values depending on scanner, software version or surrounding tissue. In this case, normalization must be applied in order to achieve comparability between two or more images. Normalization or standardization is not only needed as a prerequisite for metric assessment but is usually already performed as a preprocessing step for image-to-image translation models. For deep-learning based methods, a reasonable and standardized scale such as <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[-1, 1]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq1.gif"/></alternatives></inline-formula> or [0,&#x000a0;1] is recommended<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>. By modality specific normalization, deep-learning based models may even improve generalizability in case of heterogeneous input data sources<sup><xref ref-type="bibr" rid="CR61">61</xref>,<xref ref-type="bibr" rid="CR62">62</xref></sup>.</p><p id="Par15">Second, most metrics were designed and developed for 8-bit unsigned integer data format. In many cases, medical images are acquired in a larger 16-bit integer or 32-bit float value range and need to be normalized into the [0,&#x000a0;255] range. Alternatively, an additional data range parameter is introduced to adapt to other intensity ranges. For 8-bit images, the data range parameter <italic>L</italic> is then set to 255, assuming an intensity value range between 0 and 255. For float valued images, the intensity value range is generally infinite, but <italic>L</italic> should be set to a finite value spanning the range of at least all observed intensity values. Therefore, <italic>L</italic> is typically set to the difference of maximum and minimum value of an image <italic>I</italic> (<inline-formula id="IEq2"><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_I = I_{\textrm{max}} - I_{\textrm{min}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq2.gif"/></alternatives></inline-formula>), or the difference of the joint maximum and minimum of two images <italic>I</italic> and <italic>R</italic> (<inline-formula id="IEq3"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{I,R}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq3.gif"/></alternatives></inline-formula>), or of a set of images <inline-formula id="IEq4"><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {D}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq4.gif"/></alternatives></inline-formula> (<inline-formula id="IEq5"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{\mathcal {D}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq5.gif"/></alternatives></inline-formula>). If <inline-formula id="IEq6"><alternatives><tex-math id="M6">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I, R \in \mathcal {D}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq6.gif"/></alternatives></inline-formula>, then <inline-formula id="IEq7"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_I \le L_{I,R} \le L_{\mathcal {D}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq7.gif"/></alternatives></inline-formula>. It is argued<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>, that using <inline-formula id="IEq8"><alternatives><tex-math id="M8">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{\mathcal {D}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq8.gif"/></alternatives></inline-formula> results in SSIM values, that do not vary with individual image minimum and maximum values. At least <inline-formula id="IEq9"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{I,R}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq9.gif"/></alternatives></inline-formula> should be used for reference metrics on two images <italic>I</italic> and <italic>R</italic> instead of <inline-formula id="IEq10"><alternatives><tex-math id="M10">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_I$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq10.gif"/></alternatives></inline-formula> or <inline-formula id="IEq11"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_R$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq11.gif"/></alternatives></inline-formula>, because possibly <inline-formula id="IEq12"><alternatives><tex-math id="M12">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_I \ne L_R$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq12.gif"/></alternatives></inline-formula>, and then the reference metric would yield different results when interchanging <italic>I</italic> and <italic>R</italic>.</p></sec><sec id="Sec10"><title>Normalization methods</title><p id="Par16">Several normalization methods<sup><xref ref-type="bibr" rid="CR62">62</xref>,<xref ref-type="bibr" rid="CR63">63</xref></sup>, such as Zscore, Minmax or Quantile normalization have been used for MR images. These normalization methods ensure, that intensity values are near <inline-formula id="IEq13"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[-1, 1]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq13.gif"/></alternatives></inline-formula> or strictly between [0,&#x000a0;1], before model training<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>. Other variants of Minmax utilize percentiles or quantiles for scaling and/or additional clipping, or estimate percentiles based on a region-of-interest (ROI) contained in the image. For example, the WhiteStripe normalization<sup><xref ref-type="bibr" rid="CR65">65</xref></sup> determines reference ranges in a white matter region of the brain. Afterwards, these parameters are used to shift and scale the intensity values.</p><p id="Par17">In addition, normalization methods have been developed specifically for MR (piece-wise linear histogram matching<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>) or even specifically for brain MR images (cf. WhiteStripe<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>) in order to obtain quantitative comparable intensities for the same brain or body structures.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Overview of normalization methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Description</th><th align="left">Parameters</th><th align="left">References</th></tr></thead><tbody><tr><td align="left">Minmax</td><td align="left">Shifts and scales image intensity values to a range with given minimum and maximum value</td><td align="left">Result range: <inline-formula id="IEq14"><alternatives><tex-math id="M14">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[j_1, j_2]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq14.gif"/></alternatives></inline-formula></td><td align="left"><sup><xref ref-type="bibr" rid="CR62">62</xref></sup></td></tr><tr><td align="left">cMinmax</td><td align="left">Clips at lower and upper percentiles before Minmax. More robust to high and low outliers</td><td align="left">Result range: <inline-formula id="IEq15"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[j_1, j_2]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq15.gif"/></alternatives></inline-formula></td><td align="left"/></tr><tr><td align="left">ZScore</td><td align="left">Shifts the intensity values to a mean <inline-formula id="IEq16"><alternatives><tex-math id="M16">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu _I$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq16.gif"/></alternatives></inline-formula> of 0 and scales to unit standard deviation <inline-formula id="IEq17"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma _I$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq17.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq18"><alternatives><tex-math id="M18">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu _I=0, \sigma _I=1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq18.gif"/></alternatives></inline-formula></td><td align="left"><sup><xref ref-type="bibr" rid="CR62">62</xref></sup></td></tr><tr><td align="left">Quantile</td><td align="left">Shifts the intensity values to a median of 0 and scales to a unit inter-quartile range (IQR = <inline-formula id="IEq19"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{p_{75\%}} - I_{p_{25\%}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq19.gif"/></alternatives></inline-formula>)</td><td align="left">median(<italic>I</italic>) = 0, IQR = 1</td><td align="left"><sup><xref ref-type="bibr" rid="CR62">62</xref></sup></td></tr><tr><td align="left">Binning</td><td align="left">Binning: All intensity values are mapped to B (=256) equidistant bins</td><td align="left">Discrete result range: <inline-formula id="IEq20"><alternatives><tex-math id="M20">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0, B-1]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq20.gif"/></alternatives></inline-formula></td><td align="left"/></tr><tr><td align="left">PL</td><td align="left">Piecewise-Linear: The histogram is scaled linearly in two pieces to match three landmarks <inline-formula id="IEq21"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s_1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq21.gif"/></alternatives></inline-formula>, <inline-formula id="IEq22"><alternatives><tex-math id="M22">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m_s$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq22.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq23"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s_2$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq23.gif"/></alternatives></inline-formula> of a standard histogram derived from a set of reference images</td><td align="left">Result range: <inline-formula id="IEq24"><alternatives><tex-math id="M24">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[s_1, s_2]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq24.gif"/></alternatives></inline-formula>, mode: <inline-formula id="IEq25"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m_s$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq25.gif"/></alternatives></inline-formula>, depends on a training dataset</td><td align="left"><sup><xref ref-type="bibr" rid="CR66">66</xref></sup></td></tr></tbody></table><table-wrap-foot><p>The target intensity range <inline-formula id="IEq26"><alternatives><tex-math id="M26">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[j_1, j_2]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq26.gif"/></alternatives></inline-formula> can be chosen arbitrarily, but is typically set to [0,&#x000a0;1] or <inline-formula id="IEq27"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[-1, 1].$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq27.gif"/></alternatives></inline-formula></p></table-wrap-foot></table-wrap></p><p id="Par18">In a similar fashion, other normalization types besides Zscore can be adapted. For instance, for deep learning based MRI liver tumor segmentation<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>, a Minmax normalization was applied by using the 2% and 98% percentiles <inline-formula id="IEq28"><alternatives><tex-math id="M28">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I^{\text {Liver}}_{2\%}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq28.gif"/></alternatives></inline-formula> and <inline-formula id="IEq29"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I^{\text {Liver}}_{98\%}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq29.gif"/></alternatives></inline-formula> of the local intensity distribution within a region of interest in the liver for rescaling. In order to match histogram modes and minimum and maximum percentiles of an MR dataset showing the same body region, a piece-wise linear standardization procedure<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> has been proposed. The authors argue that MR images exhibit an unimodal or bimodal histogram for most body regions, where the foreground concentrates around the first or second mode.</p><p id="Par19">However, we assume, that different MRI sequences may generally display different histogram shapes and a different distribution of contrasted tissue types. Therefore, each MRI sequence should be normalized separately. Possibly, detecting and excluding the background could be beneficially done simultaneously for multi-modal MR images. Binning can be regarded as a normalization method to convert float values to 8-bit integer values. In this case 256 bins are used. It also removes information, because close but different intensities are mapped to the same bin value. By additionally copying the binned gray value to three color channels, images can further be converted to RGB images. Binning is needed for some metrics, because they require a finite number of intensity values (see &#x0201c;<xref rid="Sec16" ref-type="sec">Statistical dependency metrics: NMI and PCC</xref>&#x0201d; section), or it is used to easily apply a method for 8-bit images to a binned float valued image. The normalization methods investigated within this study are defined in detail in Supplementary Sect.&#x000a0;<xref rid="MOESM1" ref-type="media">A.2</xref>, an overview is shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.</p></sec><sec id="Sec11"><title>Reference metrics</title><p id="Par21">Reference metrics are based on comparing a reference image <italic>R</italic> with another image <italic>I</italic>. Both images are assumed to have the same spatial dimensions. An overview of the reference metrics analyzed in this study is given in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>. If the image <italic>I</italic> was not acquired with the same modality or the same time point as image <italic>R</italic>, spatial alignment has to be ensured before applying a reference metric. Typically, this is achieved by image registration techniques.</p><p>
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Overview of reference (similarity) and non-reference (quality) metrics.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="2">Group</th><th align="left">Abbreviation</th><th align="left">Description</th><th align="left">Similarity <inline-formula id="IEq30"><alternatives><tex-math id="M30">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq30.gif"/></alternatives></inline-formula><break/>[min, max]</th><th align="left">Implementation</th><th align="left">References</th></tr></thead><tbody><tr><td align="left" rowspan="12"> Reference (Similarity) Metrics</td><td align="left" rowspan="3">SSIM</td><td align="left">SSIM</td><td align="left">Structural Similarity Index Measure: combination of structure, luminance and contrast</td><td align="left">
<inline-formula id="IEq31"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0, 1] \uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq31.gif"/></alternatives></inline-formula>
</td><td align="left">tm, ski</td><td align="left">
<sup><xref ref-type="bibr" rid="CR73">73</xref></sup>
</td></tr><tr><td align="left">MS-SSIM</td><td align="left">Multi-Scale SSIM: SSIM on original and 4 downscaled image resolutions</td><td align="left">
<inline-formula id="IEq32"><alternatives><tex-math id="M32">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0, 1] \uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq32.gif"/></alternatives></inline-formula>
</td><td align="left">tm</td><td align="left">
<sup><xref ref-type="bibr" rid="CR74">74</xref></sup>
</td></tr><tr><td align="left">CW-SSIM</td><td align="left">Complex Wavelet SSIM: ignores phase shifts in the wavelet domain, ignores small rotations and spatial translations</td><td align="left">
<inline-formula id="IEq33"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0, 1] \uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq33.gif"/></alternatives></inline-formula>
</td><td align="left">gh<sup><xref ref-type="bibr" rid="CR75">75</xref></sup></td><td align="left">
<sup><xref ref-type="bibr" rid="CR76">76</xref></sup>
</td></tr><tr><td align="left">PSNR</td><td align="left">PSNR</td><td align="left">Peak Signal-to-Noise-Ratio: relation of data range to MSE</td><td align="left">
<inline-formula id="IEq34"><alternatives><tex-math id="M34">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0, \infty ] \uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq34.gif"/></alternatives></inline-formula>
</td><td align="left">tm, ski</td><td align="left">
<sup><xref ref-type="bibr" rid="CR77">77</xref></sup>
</td></tr><tr><td align="left" rowspan="3">Error Metrics</td><td align="left">NMSE</td><td align="left">Normalized Mean Squared Error</td><td align="left">
<inline-formula id="IEq35"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0, \infty ] \downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq35.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/></tr><tr><td align="left">MSE</td><td align="left">Mean Squared Error</td><td align="left">
<inline-formula id="IEq36"><alternatives><tex-math id="M36">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0, \infty ] \downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq36.gif"/></alternatives></inline-formula>
</td><td align="left">skl</td><td align="left"/></tr><tr><td align="left">MAE</td><td align="left">Mean Absolute Error</td><td align="left">
<inline-formula id="IEq37"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0, \infty ] \downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq37.gif"/></alternatives></inline-formula>
</td><td align="left">skl</td><td align="left"/></tr><tr><td align="left" rowspan="2">Learned Metrics</td><td align="left">LPIPS</td><td align="left">Learned Perceptual Image Patch Similarity</td><td align="left">
<inline-formula id="IEq38"><alternatives><tex-math id="M38">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0, 1] \downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq38.gif"/></alternatives></inline-formula>
</td><td align="left">pypi<sup><xref ref-type="bibr" rid="CR78">78</xref></sup>, tm</td><td align="left">
<sup><xref ref-type="bibr" rid="CR79">79</xref></sup>
</td></tr><tr><td align="left">DISTS</td><td align="left">Deep Image Structure and Texture Similarity Metric</td><td align="left">
<inline-formula id="IEq39"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0, 1] \downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq39.gif"/></alternatives></inline-formula>
</td><td align="left">gh<sup><xref ref-type="bibr" rid="CR75">75</xref>,<xref ref-type="bibr" rid="CR80">80</xref></sup></td><td align="left">
<sup><xref ref-type="bibr" rid="CR81">81</xref></sup>
</td></tr><tr><td align="left" rowspan="2">Statist. Depend.</td><td align="left">NMI</td><td align="left">Normalized Mutual Information: MI with fixed range</td><td align="left">
<inline-formula id="IEq40"><alternatives><tex-math id="M40">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[1, 2] \uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq40.gif"/></alternatives></inline-formula>
</td><td align="left">ski</td><td align="left"/></tr><tr><td align="left">PCC</td><td align="left">Pearson Correlation Coefficient</td><td align="left">
<inline-formula id="IEq41"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0,1] \uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq41.gif"/></alternatives></inline-formula>
</td><td align="left">skl</td><td align="left"/></tr><tr><td align="left">Down-stream</td><td align="left">DSC</td><td align="left">Dice Similarity Coefficient: segmentation metric, evaluating overlap</td><td align="left">
<inline-formula id="IEq42"><alternatives><tex-math id="M42">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0,1] \uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq42.gif"/></alternatives></inline-formula>
</td><td align="left">itk</td><td align="left"/></tr><tr><td align="left" colspan="4"/><td align="left">Quality <inline-formula id="IEq43"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq43.gif"/></alternatives></inline-formula></td><td align="left" colspan="2"/></tr><tr><td align="left" rowspan="12">Non-Reference (Quality) Metrics</td><td align="left" rowspan="7">Blurriness</td><td align="left">BE</td><td align="left">Blur Effect: difference of gradients when additionally blurred</td><td align="left">
<inline-formula id="IEq44"><alternatives><tex-math id="M44">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq44.gif"/></alternatives></inline-formula>
</td><td align="left">ski</td><td align="left">
<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>
</td></tr><tr><td align="left">BR</td><td align="left">Blur Ratio: ratio of blurred pixels to edge pixels</td><td align="left">
<inline-formula id="IEq45"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq45.gif"/></alternatives></inline-formula>
</td><td align="left">&#x02013;</td><td align="left">
<sup><xref ref-type="bibr" rid="CR82">82</xref></sup>
</td></tr><tr><td align="left">MB</td><td align="left">Mean Blur: sum of inverse blurriness divided by number of blurred pixels</td><td align="left">
<inline-formula id="IEq46"><alternatives><tex-math id="M46">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq46.gif"/></alternatives></inline-formula>
</td><td align="left">&#x02013;</td><td align="left">
<sup><xref ref-type="bibr" rid="CR82">82</xref></sup>
</td></tr><tr><td align="left">VL</td><td align="left">Variance of Laplacian</td><td align="left">
<inline-formula id="IEq47"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq47.gif"/></alternatives></inline-formula>
</td><td align="left">ski+np</td><td align="left">
<sup><xref ref-type="bibr" rid="CR83">83</xref></sup>
</td></tr><tr><td align="left">BEW</td><td align="left">Blurred Edge Widths</td><td align="left">
<inline-formula id="IEq48"><alternatives><tex-math id="M48">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq48.gif"/></alternatives></inline-formula>
</td><td align="left">&#x02013;</td><td align="left">
<sup><xref ref-type="bibr" rid="CR84">84</xref></sup>
</td></tr><tr><td align="left">JNB</td><td align="left">Just Noticeable Blur</td><td align="left">
<inline-formula id="IEq49"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq49.gif"/></alternatives></inline-formula>
</td><td align="left">gh(C++)<sup><xref ref-type="bibr" rid="CR85">85</xref></sup></td><td align="left">
<sup><xref ref-type="bibr" rid="CR86">86</xref></sup>
</td></tr><tr><td align="left">CPBD</td><td align="left">Cumulative Probability of Blur Detection</td><td align="left">
<inline-formula id="IEq50"><alternatives><tex-math id="M50">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq50.gif"/></alternatives></inline-formula>
</td><td align="left">gh<sup><xref ref-type="bibr" rid="CR87">87</xref></sup></td><td align="left">
<sup><xref ref-type="bibr" rid="CR88">88</xref></sup>
</td></tr><tr><td align="left" rowspan="2">MR Quality</td><td align="left">MLC</td><td align="left">Mean Line Correlation (also average structural noise): mean correlation between neighbored rows and columns</td><td align="left">
<inline-formula id="IEq51"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq51.gif"/></alternatives></inline-formula>
</td><td align="left">&#x02013;</td><td align="left">
<sup><xref ref-type="bibr" rid="CR89">89</xref></sup>
</td></tr><tr><td align="left">MSLC</td><td align="left">Mean Shifted Line Correlation (also average nyquist ghosting): mean correlation between rows and columns, that are with half image distance apart</td><td align="left">
<inline-formula id="IEq52"><alternatives><tex-math id="M52">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq52.gif"/></alternatives></inline-formula>
</td><td align="left">&#x02013;</td><td align="left">
<sup><xref ref-type="bibr" rid="CR89">89</xref></sup>
</td></tr><tr><td align="left" rowspan="2">Learned Quality</td><td align="left">BRISQUE</td><td align="left">Blind/Referenceless Image Spatial Quality Evaluator</td><td align="left">
<inline-formula id="IEq53"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq53.gif"/></alternatives></inline-formula>
</td><td align="left">pypi<sup><xref ref-type="bibr" rid="CR90">90</xref></sup></td><td align="left">
<sup><xref ref-type="bibr" rid="CR91">91</xref></sup>
</td></tr><tr><td align="left">NIQE</td><td align="left">Natural Image Quality Evaluator</td><td align="left">
<inline-formula id="IEq54"><alternatives><tex-math id="M54">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq54.gif"/></alternatives></inline-formula>
</td><td align="left">gh<sup><xref ref-type="bibr" rid="CR92">92</xref></sup></td><td align="left">
<sup><xref ref-type="bibr" rid="CR93">93</xref></sup>
</td></tr><tr><td align="left">Noisiness</td><td align="left">MTV</td><td align="left">Mean Total Variation: Mean L2-normed gradient in x- and y- direction</td><td align="left">
<inline-formula id="IEq55"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq55.gif"/></alternatives></inline-formula>
</td><td align="left">&#x02013;</td><td align="left">
<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>
</td></tr></tbody></table><table-wrap-foot><p>The arrows indicate if the metric increases (<inline-formula id="IEq56"><alternatives><tex-math id="M56">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq56.gif"/></alternatives></inline-formula>) or decreases (<inline-formula id="IEq57"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq57.gif"/></alternatives></inline-formula>) with increasing similarity or quality. [Implementation sources: gh=gitHub, itk=Insight Segmentation and Registration Toolkit<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>, np=numpy<sup><xref ref-type="bibr" rid="CR69">69</xref></sup>, pypi=python package index, skl=scikit-learn<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>, ski=scikit-image<sup><xref ref-type="bibr" rid="CR71">71</xref></sup>, tm=torchmetrics<sup><xref ref-type="bibr" rid="CR72">72</xref></sup>].</p></table-wrap-foot></table-wrap>
</p><p id="Par22">Slight spatial misalignment between paired images has been identified as a problem when evaluating with reference metrics, such that specialized methods have been investigated for this purpose. By assessing similarity in the complex-wavelet domain, complex-wavelet SSIM (CW-SSIM) is able to ignore small translations, scaling and rotations<sup><xref ref-type="bibr" rid="CR76">76</xref></sup>. A score derived from features of the Segment Anything Model (SAM), mainly compares semantic features and therefore better ignores different style and small deformations<sup><xref ref-type="bibr" rid="CR94">94</xref></sup>. The learned Deep Image Structure and Texture Similarity (DISTS) metric gives more weight to texture similarity, ignoring fine-grained misalignment of these textures<sup><xref ref-type="bibr" rid="CR81">81</xref></sup>.</p><p id="Par23">For natural images, a large set of reference metrics has been benchmarked on the Tampere Image Dataset<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> or the LIVE Image Quality Assessment Database<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. Many of the standard metrics from natural imaging have been frequently applied to medical images. However, some careful modifications are necessary for images with a data type other than 8-bit unsigned integer. In the following, we introduce the investigated reference metrics with some more detail and background and highlight important adaptions. A full list of metrics and their calculation is found in Supplementary Sect.&#x000a0;<xref rid="MOESM1" ref-type="media">A.3</xref>.</p><sec id="Sec12"><title>Structural similarity index measure</title><p id="Par24">The structural similarity index measure (SSIM) combines image structure, luminance, and contrast, which are calculated locally for each pixel<sup><xref ref-type="bibr" rid="CR73">73</xref></sup>. Several variants of SSIM exist. Multi-scale SSIM (MS-SSIM)<sup><xref ref-type="bibr" rid="CR74">74</xref></sup> calculates local luminance, contrast and structure additionally for four downscaled versions of the images and combines them in a weighted fashion. MS-SSIM is more sensitive towards large-scale differences between the images to be compared. This puts less impact on high resolution details. The complex-wavelet SSIM (CW-SSIM)<sup><xref ref-type="bibr" rid="CR76">76</xref></sup> was specifically designed to compensate for small rotations and spatial translations. For this metric, only the coefficients of the complex-wavelet transformed images <italic>I</italic> and <italic>R</italic> are used, such that small phase shifts between both images are ignored. This additional freedom may allow unnatural results, when CW-SSIM is used for model optimization<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. As an adaption of SSIM to float valued images, the data range parameter <italic>L</italic> (see &#x0201c;<xref rid="Sec9" ref-type="sec">Intensity ranges and data formats</xref>&#x0201d; section) is used to scale the internal constants <inline-formula id="IEq58"><alternatives><tex-math id="M58">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_1 = (k_1\cdot L)^2 = (0.01 \cdot L)^2$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq58.gif"/></alternatives></inline-formula> and <inline-formula id="IEq59"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_2 = (k_2\cdot L)^2 = (0.03 \cdot L)^1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq59.gif"/></alternatives></inline-formula> (and <inline-formula id="IEq60"><alternatives><tex-math id="M60">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_3 = C_2/2$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq60.gif"/></alternatives></inline-formula>), which are used to make computations numerically stable (see supplement Sect.&#x000a0;<xref rid="MOESM1" ref-type="media">A.3</xref>, Eq.&#x000a0;(41)). It can be derived from the SSIM calculation, that a high data range parameter <italic>L</italic> and thereby high values for <inline-formula id="IEq61"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_1-C_3$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq61.gif"/></alternatives></inline-formula>, lead to SSIM values near 1, because the constants <inline-formula id="IEq62"><alternatives><tex-math id="M62">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_1-C_3$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq62.gif"/></alternatives></inline-formula> dominate the calculation compared to the observed intensity values. In these cases, SSIM is not very informative. This has been experimentally observed before<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>, and a default normalization of float value ranged images to the range [0,&#x000a0;1] and a modification of constants <inline-formula id="IEq63"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_1 = C_2$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq63.gif"/></alternatives></inline-formula> to <inline-formula id="IEq64"><alternatives><tex-math id="M64">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(k_1 \cdot L)^2 = (0.0001\cdot 1)^2 = 1 \times 10 ^{-8}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq64.gif"/></alternatives></inline-formula> was proposed. Common implementations also define different default values for <italic>L</italic>. As the skimage<sup><xref ref-type="bibr" rid="CR71">71</xref></sup> package generally assumes float valued images to be in the range <inline-formula id="IEq65"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[-1.0, 1.0]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq65.gif"/></alternatives></inline-formula>, its implementation of SSIM defines <inline-formula id="IEq66"><alternatives><tex-math id="M66">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L=2$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq66.gif"/></alternatives></inline-formula> as default. The torchmetrics<sup><xref ref-type="bibr" rid="CR72">72</xref></sup> implementation of SSIM sets <inline-formula id="IEq67"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L = L_{I, R}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq67.gif"/></alternatives></inline-formula> as default for images <italic>I</italic> and <italic>R</italic>.</p><p id="Par25">The choice for the <bold>data range parameter</bold>
<italic>L</italic> is directly related to normalization techniques that rescale image intensities: If images <italic>I</italic> and <italic>R</italic> are scaled by factor <italic>a</italic> to <inline-formula id="IEq68"><alternatives><tex-math id="M68">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I^{\prime } = I \cdot a$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq68.gif"/></alternatives></inline-formula> and <inline-formula id="IEq69"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{\prime } = R \cdot a$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq69.gif"/></alternatives></inline-formula>, then calculating SSIM on <inline-formula id="IEq70"><alternatives><tex-math id="M70">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I^{\prime }$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq70.gif"/></alternatives></inline-formula> and <inline-formula id="IEq71"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{\prime }$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq71.gif"/></alternatives></inline-formula> with <inline-formula id="IEq72"><alternatives><tex-math id="M72">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L^{\prime } = L \cdot a$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq72.gif"/></alternatives></inline-formula> will be the same as calculating SSIM on <italic>I</italic> and <italic>R</italic> with <italic>L</italic>. However, if image intensities are additionally shifted by an additive value <italic>b</italic>, the luminance term will increase with <italic>b</italic> and yield different SSIM values. If this shift <italic>b</italic> is negative, as it typically is with Zscore normalization (see Supplementary Sect.&#x000a0;<xref rid="MOESM1" ref-type="media">A.2</xref>, Eq.&#x000a0;(3)), this can lead to negative SSIM values<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>.</p></sec><sec id="Sec13"><title>Peak signal-to-noise ratio</title><p id="Par26">Peak signal-to-noise ratio (PSNR) was developed to measure the reconstruction quality of a lossy compressed image compared to the uncompressed reference image<sup><xref ref-type="bibr" rid="CR77">77</xref></sup>. However, it is frequently used as a metric for assessing image similarity. The PSNR is infinite for identical images and decreases monotonically as the differences between image <italic>I</italic> and reference <italic>R</italic> increase. The data range parameter <italic>L</italic> is incorporated in the PSNR as the peak signal. The noise in the PSNR is calculated as the mean squared error (MSE, see &#x0201c;<xref rid="Sec14" ref-type="sec">Error metrics</xref>&#x0201d; section). For natural images, improved variants of PSNR called PSNR-HVS and PSNR-HVS-M have been developed, that seem to correlate closer to the human visual system<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. Adapted implementations for variable intensity ranges and experiments with medical images are not available at this time. A deeper exploration of these metrics remains important future work.</p></sec><sec id="Sec14"><title>Error metrics</title><p id="Par27">This group of metrics, including mean absolute error (MAE), mean square error (MSE), root mean square error (RMSE) and normalized mean square error (NMSE), directly depends on the absolute difference of intensity values at equal pixel locations. The metrics MSE, RMSE, and NMSE are based on the squared difference and due to convex shape of the quadratic function, these metrics give more weight to large differences than MAE, which is based on the unsquared absolute difference. By normalization with the standard deviation of the reference image, NMSE assigns a higher similarity to images with a higher standard deviation, i.&#x000a0;e. with high variation and a large range of intensity values. On the contrary, the same intensity differences lead to a lower similarity, if the reference image has a very low standard deviation, i.&#x000a0;e. it appears very homogeneous. However, the scale and range of all these metrics strongly depend on the intensity value ranges and, thereby, also on the normalization method.</p></sec><sec id="Sec15"><title>Learned metrics: LPIPS and DISTS</title><p id="Par28">Learned perceptual image patch similarity (LPIPS) relies on image feature maps from a trained image classification model. For the LPIPS metric, an Alex-Net or VGG-architecture backbone exist<sup><xref ref-type="bibr" rid="CR79">79</xref></sup>. The VGG version of LPIPS was recommended for usage as a traditional perceptual loss, while the Alex-Net version should be preferred as a forward metric. The latter one is also faster at inference due to the smaller network, so we analyzed LPIPS with Alex-Net in &#x0201c;<xref rid="Sec23" ref-type="sec">Experiments</xref>&#x0201d; section. Even though the networks were trained on RGB images, the trained networks expect an input range of <inline-formula id="IEq73"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[-1,1]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq73.gif"/></alternatives></inline-formula>, e.g. by previous Minmax normalization. LPIPS has shown great correlation with human perception and outperforms many other similarity metrics on natural images while the type of employed architecture has only minor influence<sup><xref ref-type="bibr" rid="CR79">79</xref></sup>. It has occasionally been used for validation of medical image synthesis<sup><xref ref-type="bibr" rid="CR95">95</xref></sup>, and is commonly applied as perceptual loss for training medical image-to-image translation models<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>.</p><p id="Par29">The Deep Image Structure and Texture Similarity (DISTS) metric is an adaption of LPIPS giving more focus on texture<sup><xref ref-type="bibr" rid="CR81">81</xref></sup>.</p></sec><sec id="Sec16"><title>Statistical dependency metrics: NMI and PCC</title><p id="Par30">Mutual information (MI) estimates the amount of information of an image <italic>R</italic>, that can be predicted from image <italic>I</italic>. MI is widely used as an optimization criterion for multi-modal image registration<sup><xref ref-type="bibr" rid="CR96">96</xref></sup>. It has been used sporadically as a metric for validation of image synthesis<sup><xref ref-type="bibr" rid="CR52">52</xref>,<xref ref-type="bibr" rid="CR63">63</xref></sup>. The NMI has a fixed value range of [1,&#x000a0;2], which is preferable for comparing absolute metric scores and interpretability. The Pearson correlation coefficient (PCC), is a statistical dependency metric which measures the degree of linear dependency between the intensities in <italic>I</italic> and <italic>R</italic> at each pixel location. As PCC is defined by correlation and NMI and MI both operate on normalized binned images, previous normalization that purely scales and shifts, such as Minmax, Zscore or Quantile normalization, does not have any effect on the resulting scores. The reference metrics investigated within this study are defined in detail in Supplementary Sect.&#x000a0;<xref rid="MOESM1" ref-type="media">A.3</xref>, an overview is shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>.</p></sec></sec><sec id="Sec17"><title>Indirect evaluation with downstream tasks</title><p id="Par31">Another option for validation is to consider which tasks are going to be performed downstream from a synthesized image. Whenever one of these tasks is performed, the quality of synthetic images can also be assessed by measuring the performance of the specific task on the image compared to the performance of the reference image. In medical image-to-image translation, which aims for improved medical diagnosis or treatment, assessing the performance of medical diagnosis or treatment directly derived from digital images is very desirable. In this context, synthetic images must be processed in the same way as the reference image and should have equal outcomes in the downstream task. However, deviations between synthetic images and reference images can be accepted, when they have no impact on the downstream task. As an example, if a synthetic MR image is generated for detecting a brain tumor, it is to some extent irrelevant if healthy brain tissue in the synthetic image appears slightly different than in the true reference image, as long as it is clearly identified as the same type of healthy tissue. If a synthetic histology image is rated with the same grade of cancer as the reference image, the exact cell-wise correspondence might not be important. Many downstream tasks on medical images can be nowadays performed automatically, including:<list list-type="bullet"><list-item><p id="Par32">Detection or segmentation of organs, cells and lesions, e.g. the Segmentation of brain tumors from T1-weighted native, T1-weighted contrast enhanced, T2-weighted, and fluid attenuation inversion recovery (FLAIR) MR images<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> [Related downstream task metrics: DSC, Intersection over Union (IoU)].</p></list-item><list-item><p id="Par33">Classification of images or image segments, e.g. the synthesis of clinical skin images with 26 types of conditions, verified by classification scores of dermatologists<sup><xref ref-type="bibr" rid="CR97">97</xref></sup> [Related downstream task metrics: Accuracy, Precision, Recall, F1-Score].</p></list-item><list-item><p id="Par34">Transfer learning and data augmentation, e.g. the synthesis of chest X-ray for data augmentation and evaluation of classification model on real data with and without synthetic training data<sup><xref ref-type="bibr" rid="CR98">98</xref></sup> [Related downstream task metrics: e.g. Sensitivity, Specificity, Area Under the Receiver-Operator-Characteristic curve (AUROC)].</p></list-item><list-item><p id="Par35">Multi-modal registration, e.g. the registration of synthesized MR image from CT image to MRI atlas instead of registration of CT image to MRI atlas<sup><xref ref-type="bibr" rid="CR99">99</xref></sup> [Related downstream task metrics: MSE, MI].</p></list-item><list-item><p id="Par36">Dose calculation in radiation therapy planning, e.g. the synthesis of a planning CT from MRI for use in a radiation planning tool<sup><xref ref-type="bibr" rid="CR100">100</xref></sup> [Related downstream task metrics: relative difference of planned radiation dose].</p></list-item></list>Detection, segmentation and classification metrics for the biomedical domain have been well documented and discussed<sup><xref ref-type="bibr" rid="CR47">47</xref>,<xref ref-type="bibr" rid="CR48">48</xref></sup>. Also, a study of a segmentation metric systematically analyzed the sensitivity to relevant simulated distortions<sup><xref ref-type="bibr" rid="CR101">101</xref></sup>. Therefore, the performance of such tasks with synthetic images can be well compared to the performance with reference images to validate the use of synthetic images for a specific task. The concept of downstream task evaluation metrics recognizes that the final goal of image synthesis in the medical domain is to generate useful and correct images rather than images, that are visually appealing<sup><xref ref-type="bibr" rid="CR102">102</xref></sup>. However, if image synthesis was optimized regarding a certain downstream task, the resulting images might not be optimal for other non-related tasks. Specifically, they might have a fake appearance, that does not interfere with the downstream task, but would be misleading for direct review of medical practitioners. Furthermore, the evaluation of downstream tasks can substantially depend on the performance of the downstream task method. If a segmentation model fails on a large set of reference images, the comparison to segmentations on synthetic images is obsolete. The amount and variety of downstream tasks and corresponding metrics are almost unlimited, but to discuss and analyze the value of downstream tasks, we include the evaluation of a downstream segmentation model with a popular segmentation metric, namely the Dice Similarity Coefficient (DSC)<sup><xref ref-type="bibr" rid="CR47">47</xref>,<xref ref-type="bibr" rid="CR103">103</xref></sup>.</p></sec><sec id="Sec18"><title>Non-reference quality metrics</title><p id="Par37">Non-reference metrics, often also called quality metrics or blind metrics, try to assess the quality of a distorted image without knowing the undistorted reference. As a reference might not be available, these metrics can be applied in many evaluation settings. However, there is a huge amount of such metrics and most of them assume a certain kind of distortion to be detected. The correlation of many of these metrics with human perception has been investigated<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. But also deviations between these scores and diagnostic quality perceived by radiologists have been observed<sup><xref ref-type="bibr" rid="CR104">104</xref></sup>. Blurriness metrics were quite successful in detecting images with reduced quality as perceived by humans in different image domains.</p><p id="Par38">In this paper, we select and present a set of quality metrics (see Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>) that could complement reference metrics and detect especially those distortions, which reference metrics can miss. It has often been discussed<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR105">105</xref></sup>, that error metrics are not sensitive to blurring, which can be problematic because synthesis models may create blurry results. That is why we evaluated a set of blurriness metrics, that do not need a reference. Similar to the learned similarity metrics, also learned quality metrics have shown to provide useful quality scores for natural images<sup><xref ref-type="bibr" rid="CR91">91</xref>,<xref ref-type="bibr" rid="CR93">93</xref></sup>. Last, we assessed metrics, that detect MR acquisition artifacts<sup><xref ref-type="bibr" rid="CR89">89</xref></sup> or noise<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>.</p><sec id="Sec19"><title>Blurriness metrics</title><p id="Par39">A large set of metrics has been developed to measure the sharpness or, inversely, the blurriness of images to filter out low-quality images. Methods assessing image blur operate either on the spatial domain, the spectral domain, e.g. through wavelet or fast Fourier transform. In addition, there are learned blur detection methods as well as combinations<sup><xref ref-type="bibr" rid="CR106">106</xref></sup>. Blur assessment methods in the spatial domain, such as the Blur-Effect<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> or the variance of the Laplacian (VL)<sup><xref ref-type="bibr" rid="CR83">83</xref></sup>, can exploit local image gradients. Others rely on binary edge detection, with the drawback, that thresholds are needed to decide which pixel belongs to an edge and which one does not. Hence, thresholds need to be adapted for varying intensity value ranges. In general, spectral domain transforms are computationally more costly, so methods on the spatial domain tend to perform faster.</p><p id="Par40">The mean blur (MB) and blur ratio (BR) metrics were jointly<sup><xref ref-type="bibr" rid="CR82">82</xref></sup> designed to assess blurriness and edges based on a ratio called inverse blurriness. A set of blurriness metrics has been derived from the concept of measuring blurred edge widths (BEW)<sup><xref ref-type="bibr" rid="CR84">84</xref></sup>. This idea was extended with a notion of just noticeable blur (JNB)<sup><xref ref-type="bibr" rid="CR86">86</xref></sup>, and evaluates the image in smaller blocks. A further blurriness metric measures the cumulative probability of blur detection (CPBD)<sup><xref ref-type="bibr" rid="CR88">88</xref></sup> extending the approach of JNB.</p><p id="Par41">The incorporated just noticeable blur width is based on experiments with 8-bit integer valued images. Similarly the MB and BR metrics were designed for 8-bit integer valued images. Therefore, our implementation uses a data range parameter for adapting to larger intensity ranges. Further details of the implementations can be found in our published repository at <ext-link ext-link-type="uri" xlink:href="http://www.github.com/bayer-group/mr-image-metrics">www.github.com/bayer-group/mr-image-metrics</ext-link>.</p></sec><sec id="Sec20"><title>MR quality metrics</title><p id="Par42">In MR images, specific artifacts may appear, which are related to image acquisition and reconstruction. These artifacts may not only appear on real images, but could be reproduced in synthetic images, which is undesirable. Therefore, the use of MR specific quality metrics could efficiently improve validation of MR synthesis models. In order to select the preferred image from a repeated set of image acquisitions of the same patient, Schuppert et al.<sup><xref ref-type="bibr" rid="CR89">89</xref></sup>, evaluated a set of image quality metrics. Mean line correlation (MLC, in<sup><xref ref-type="bibr" rid="CR89">89</xref></sup> denoted as &#x0201c;average structural noise&#x0201d;) and mean shifted line correlation (MSLC, in<sup><xref ref-type="bibr" rid="CR89">89</xref></sup> denoted as &#x0201c;average nyquist ghosting&#x0201d;) were revealed to be among the best metrics to predict which image was preferred among repeated acquisitions. Possibly, these metrics are able to detect common MR acquisition artifacts, such as ghosting or motion artifacts, that would lead to repeated acquisitions. The MLC metric is defined as the mean correlation between neighboring lines of pixels in an image. The MSLC metric is defined as the mean correlation between image lines, that are separated by half of the image width or height respectively.</p></sec><sec id="Sec21"><title>Learned quality metrics</title><p id="Par43">Similar to learned reference metrics, also non-reference metrics have been developed from learned image features. The blind/ reference-less image spatial quality evaluator BRISQUE<sup><xref ref-type="bibr" rid="CR91">91</xref></sup> leverages 18 spatial image features extracted from distorted training images of the LIVE database<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> annotated with a quality score. A simple support vector machine regression model was trained to predict the annotated quality scores from the extracted set of features.</p><p id="Par44">The natural image quality evaluator (NIQE)<sup><xref ref-type="bibr" rid="CR93">93</xref></sup> does not rely on training with annotated images. Instead, a multi-variate Gaussian model is parameterized from the same set of 18 spatial features, but extracted from two scales. A reference model was parameterized from features from a training set of undistorted images to obtain the multivariate Gaussian model. Images were selected from copyright free Flickr data and from the Berkeley image segmentation database<sup><xref ref-type="bibr" rid="CR107">107</xref></sup>. The NIQE metric assesses the distance of fitted test image parameters to the parameters of the reference model. Due to the characteristics of the training set and assumed differences to MR images regarding intensity value distributions, the NIQE metric may not be directly transferable to MR images.</p></sec><sec id="Sec22"><title>Noise metrics</title><p id="Par45">For denoising of images, total variation<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> has been used as a criterion for noisiness. Therefore, mean total variation seems promising as a measure of undesired noise.</p><p id="Par46">Because noise can be reduced by blurring, blurriness metrics might act as inverse noisiness metrics. In other words, an increasing degree of blurriness may correlate with decreasing noise. Inversely, adding noise may disguise blurriness and therefore impair blurriness metrics for image quality assessment.</p><p id="Par47">The non-reference metrics investigated within this study are defined in detail in Supplementary Sect.&#x000a0;<xref rid="MOESM1" ref-type="media">A.3</xref>, an overview is shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>.</p></sec></sec></sec><sec id="Sec23"><title>Experiments</title><p id="Par49">In order to systematically investigate reference and non-reference metrics, we distorted 100 T1-weighted contrast enhanced MR images with 11 different types of distortions in five strengths. For the reference metrics (see &#x0201c;<xref rid="Sec11" ref-type="sec">Reference metrics</xref>&#x0201d; section), the similarity between each distorted image and its undistorted reference was calculated. For the non-reference metrics (see &#x0201c;<xref rid="Sec18" ref-type="sec">Non-reference quality metrics</xref>&#x0201d; section), the metric scores for all distorted and undistorted images were assessed. For the segmentation metric (see &#x0201c;<xref rid="Sec17" ref-type="sec">Indirect evaluation with downstream tasks</xref>&#x0201d; section), we trained a model and predicted segmentations for all distorted and undistorted images. The segmentation metric assessed the agreement between segmentations derived from distorted images and segmentations derived from the respective undistorted reference image. In addition, images individually normalized with one of six different normalization methods, including no normalization, leaving the images with raw intensity values. The workflow of the experiments is illustrated in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>. In image-to-image tasks, MR source or target images are typically normalized for model training and the synthesized images are generated in this normalized space. Validation of synthesized images can either be performed in this normalized space, such that the normalized target image is used as reference and the synthesized image is assumed to already be normalized appropriately. Another possibility is to invert the previously performed normalization method on the synthesized image to the original intensity range. Then the synthesized image can be compared to the target image in the original intensity range. In our experiments, we test the metrics in the original and a normalized intensity range. As some distortions slightly or more drastically extend or reduce the intensity range of the reference image, different normalization methods result in different alignment of histograms of the reference and the distorted images. The LPIPS metric requires an input range of [&#x02212;&#x000a0;1 and 1], therefore we decided to apply Minmax and cMinmax normalization to the required target range of <inline-formula id="IEq74"><alternatives><tex-math id="M74">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[-1, 1]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq74.gif"/></alternatives></inline-formula>. The DISTS metric requires an input range of [0,&#x000a0;1]. Even though all normalization methods besides Minmax and cMinmax do not satisfy the required input ranges, we did evaluate the metrics after these normalization methods to investigate deviations to the recommended type of normalization.</p><p id="Par48">
<fig id="Fig2"><label>Fig. 2</label><caption><p>Workflow of experiments. (1) 100 reference images were distorted with one of 11 distortions (see &#x0201c;<xref rid="Sec26" ref-type="sec">Distortions</xref>&#x0201d; section) with one of five strengths. (2) The distorted images and the reference images were individually normalized with one of six normalization methods (see &#x0201c;<xref rid="Sec10" ref-type="sec">Normalization methods</xref>&#x0201d; section), including no normalization and omitting piece-wise linear (PL) normalization, which depends on a reference dataset. (A) Reference and (B) non-Reference metric scores were obtained from normalized distorted images and normalized reference images. (3) The segmentation model was applied to one normalization method only, because the fully automatic segmentation setup integrated all preprocessing steps, including Zscore normalization.</p></caption><graphic xlink:href="41598_2025_87358_Fig2_HTML" id="MO2"/></fig>
</p><p id="Par50">This experimental setup allows to qualitatively derive the sensitivity of each analyzed metrics to each of the tested distortion types. Assuming that different distortions applied with the same strength should receive the same similarity or quality score, deviations of metric scores between different distortions of the same strengths can be interpreted as strong or weak sensitivity to certain distortions.</p><sec id="Sec24"><title>Data</title><p id="Par51">We selected the first 100 cases of the Brain Synthesis (BraSyn) 2023 Challenge, available at <ext-link ext-link-type="uri" xlink:href="http://www.synapse.org/brats2023">www.synapse.org/brats2023</ext-link><sup><xref ref-type="bibr" rid="CR108">108</xref>&#x02013;<xref ref-type="bibr" rid="CR110">110</xref></sup>. We further selected only T1-weighted contrast-enhanced (T1c) images. The images all show human brains with glioma tumors. The provided data has already been preprocessed including skull-stripping (removal of the skull), background voxel intensities are set to 0, resampling to a unit mm voxel spacing, registration to a centered brain atlas. For better visualization and reduced computation time, we extracted the centered 2D slice of each 3D volume with a size of 240 <inline-formula id="IEq75"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\times}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq75.gif"/></alternatives></inline-formula> 240 pixels.</p></sec><sec id="Sec25"><title>Segmentation model for downstream task</title><p id="Par52">We trained an automatically configuring U-Net based segmentation network<sup><xref ref-type="bibr" rid="CR111">111</xref></sup> on the T1c images of the BraSyn dataset train split. The prediction of three classes (1: whole tumor, 2: tumor core, 3: enhancing tumor) was optimized for 300 epochs, using 1 fold and with a DICE cross entropy loss and deep supervision. The model was selected by the best validation score at epoch 323. The architecture of the U-Net included five residual blocks, with downsampling factors 1, 2, 2, 4 and 4, initially 32 features and one output channel activated by a sigmoid function per class, resulting in approx. 29 million parameters. The model was trained to segment all three annotated tumor classes. As a preprocessing step for training and inference, Zscore normalization was applied to the input images. Therefore, no other normalization methods were tested. For evaluation of the DSC metric (see Supplementary Sect.&#x000a0;<xref rid="MOESM1" ref-type="media">A.3</xref>, Eq.&#x000a0;(18)), we infered the model on all reference and all distorted images.</p></sec><sec id="Sec26"><title>Distortions</title><p id="Par53">We selected a wide range of distortions, which we expect to appear with MR image synthesis. The parameters of all distortions were scaled to five increasing strengths, where a strength of one should be a minimal distortion, which is not immediately visible and five a strongly visible distortion, which clearly impedes any diagnosis. We initially scaled the distortion parameters to comparable strengths by a reader study with six experienced researchers (see Supplementary Sect. <xref rid="MOESM1" ref-type="media">A5</xref>). The final parameters for each distortion are listed in Supplementary Sect.&#x000a0;<xref rid="MOESM1" ref-type="media">A.4</xref>. Examples for minimum (strength = 1) and maximum (strength = 5) distortions are shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>.<fig id="Fig3"><label>Fig. 3</label><caption><p>Examples of distorted images for lowest strength <inline-formula id="IEq76"><alternatives><tex-math id="M76">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq76.gif"/></alternatives></inline-formula>, up to the maximal distortion strength <inline-formula id="IEq77"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=5$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq77.gif"/></alternatives></inline-formula>. For <inline-formula id="IEq78"><alternatives><tex-math id="M78">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq78.gif"/></alternatives></inline-formula>, the distortions are hardly visible and therefore the images appear all the same. All distorted images are displayed with the same intensity range as the reference image, i.e. the range was clipped in case of higher or lower values. The change in the image distorted by ghosting is highlighted by a green arrow. Further examples of distorted images are provided in the Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">S.2</xref>&#x02013;<xref rid="MOESM1" ref-type="media">S.7</xref>.</p></caption><graphic xlink:href="41598_2025_87358_Fig3_HTML" id="MO3"/></fig></p><p id="Par54">Among the selected distortions, <bold>Translation</bold> and <bold>Elastic Deformation</bold> were applied as spatial transforms, that are commonly found, when the reference is not well aligned to the image to be tested. This is frequently the case in image-to-image translation, when the input image was acquired with a different modality or at a different time point. Usually, the patient has moved in between and registration was possibly not sufficient. Translation was modeled as an equal shift of all pixels along the x and y-axis and parameterized with a fraction of the image width and height. Elastic deformation was modeled by placing a grid with a given number of points on the image, randomly displacing grid points, and linearly interpolating between the new point positions. The displacements were sampled from a normal distribution with increasing parameter <inline-formula id="IEq79"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq79.gif"/></alternatives></inline-formula> and the number of points was reduced for higher distortion strengths.</p><p id="Par55">Intensity distortions, that shift, stretch or compress the histogram, such as gamma transforms or an intensity shift, can appear between different scanning parameters, because MR does not guarantee a fixed intensity scale. For gamma transforms, images are first normalized by Minmax to range [0,&#x000a0;1]. This ensures, that the intensity value range is unchanged under gamma transformation, which is simply potentiating with a parameter <inline-formula id="IEq80"><alternatives><tex-math id="M80">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq80.gif"/></alternatives></inline-formula>. Then, the intensities are scaled back to the original intensity range. We call gamma transforms for <inline-formula id="IEq81"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma &#x0003e; 1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq81.gif"/></alternatives></inline-formula> denominated <bold>Gamma High</bold>, while those with <inline-formula id="IEq82"><alternatives><tex-math id="M82">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma &#x0003c;1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq82.gif"/></alternatives></inline-formula> are named <bold>Gamma Low</bold>. Both types of distortions are parameterized with increasing or decreasing values of <inline-formula id="IEq83"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq83.gif"/></alternatives></inline-formula> respectively for increasing distortion strengths. <bold>Intensity shifts</bold> are modeled by adding a fraction of the maximum intensity range to the intensity value of all pixels.</p><p id="Par56">Further distortions, that represent typical acquisition artifacts of MR images are ghosting, stripe and bias field artifacts. <bold>Ghosting</bold> artifacts appear as shifted copies of the image, arising from erroneous sampling in the frequency space. Scaling a single pixel with an intensity parameter in the frequency space causes artificial <bold>Stripe Artifacts</bold>. <bold>Bias fields</bold> appear as low frequency background signals, that we model by multiplying with an exponential of a polynomial function of degree three (see Supplementary Sect.&#x000a0;<xref rid="MOESM1" ref-type="media">A.4</xref> Eq.&#x000a0;(45)). All of these MR acquisition distortions may moderately expand the intensity range of the distorted image compared to the reference image.</p><p id="Par57">Gaussian noise or Gaussian blurring are not restricted to MR acquisition but, as in most imaging modalities, they are frequently observed and were also analyzed in our study. Gaussian noise adds intensity values randomly sampled from a normal distribution with <inline-formula id="IEq84"><alternatives><tex-math id="M84">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu =0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq84.gif"/></alternatives></inline-formula> and increasing <inline-formula id="IEq85"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq85.gif"/></alternatives></inline-formula> to each pixel intensity. Gaussian blur convolves the reference image with a Gaussian filter with increasing <inline-formula id="IEq86"><alternatives><tex-math id="M86">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq86.gif"/></alternatives></inline-formula>.</p><p id="Par58">Last, we investigate the effect of <bold>Replace Artifacts</bold>, where parts of the image content are replaced, in this case by mirrored regions. In the BraSyn data set, in most cases, there is a tumor in exactly one hemisphere of the brain. By mirroring one hemisphere onto the other one, a second tumor is inserted into, or a tumor is removed from the second hemisphere. We scaled this distortion by mirroring an increasing fraction of the hemisphere. Replacing brain structures in one hemisphere by structure in the other one, simulates the generation of synthetic structures, that were not in the input image. This is a known problem of some synthesis models, e.&#x000a0;g. of cycleGAN architectures<sup><xref ref-type="bibr" rid="CR112">112</xref></sup>. The detection of such synthetically inserted structures is highly desired for image-to-image translation model validation.</p></sec></sec><sec id="Sec27"><title>Results</title><sec id="Sec28"><title>Reference metrics</title><p id="Par60">The complete results for all reference metrics, all distortion strengths and all normalization methods are contained in the Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">S.8</xref>&#x02013;<xref rid="MOESM1" ref-type="media">S.10</xref>. There, for each metric and normalization method, the trends of all distortions for increasing strengths are shown. The selected and compressed results in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>, contain the median metric values over all strengths and images for one selected and recommended normalization method. For LPIPS and DISTS, images we show results for Minmax normalization to range <inline-formula id="IEq92"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[-1,1]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq92.gif"/></alternatives></inline-formula> and [0,&#x000a0;1] respectively as recommended by the authors of the used implementations. For DSC, Zscore normalization was part of the segmentation process. For all other metrics, we show results without normalization as these seem representative and we were not aware of any recommended normalization method. Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> only contains the DSC on the foreground (union of all three tumor classes), as the DSC segmentation scores for all three classes are very similar (see Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S.15</xref>). In the performed experiments, dependencies between distortions of different strengths and metric scores can be observed. The majority of metrics indicate decreasing similarity or decreasing quality for increasing distortion strengths. This can be described as sensitivity of a metric to a distortion or an effect of a distortion to a metric. Specifically, stronger changes of metrics scores observed for one distortion, compared to other distortions can be interpreted as a higher sensitivity of the metric or a stronger effect of the distortion to this metric. The results are described in the following, ordered by distortions.</p><p>
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Median reference metric values for each distortion evaluated on 100 images and all distortions strength.All reference metric scores shown here were assessed on images without normalization, except LPIPS*, DISTS* and DSC*. The darker the background the higher the sensitivity of the metric to the respective distortion compared to all other distortions. The arrows indicate, if the metric increases (<inline-formula id="IEq87"><alternatives><tex-math id="M88">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq87.gif"/></alternatives></inline-formula>) or decreases (<inline-formula id="IEq88"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq88.gif"/></alternatives></inline-formula>) with higher similarity. <inline-formula id="IEq89"><alternatives><tex-math id="M90">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^*$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq89.gif"/></alternatives></inline-formula>: For LPIPS Minmax normalization to [&#x02212;&#x000a0;1, 1] was used, for DISTS Minmax normalization to [0, 1] was applied and DSC was assessed after Zscore normalization and segmentation. Results for all normalization methods can be found in Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">S.8</xref>&#x02013;<xref rid="MOESM1" ref-type="media">S.15</xref>.</p></caption><graphic position="anchor" xlink:href="41598_2025_87358_Tab3_HTML" id="MO4"/></table-wrap>
</p><p id="Par61"><bold>Bias Field</bold> has a moderate effect on most metrics. MS-SSIM is much more sensitive to simulated bias field artifacts than simple SSIM or CW-SSIM. The error metrics show clearly increased (dissimilarity) scores, while the LPIPS score is hardly effected. Compared to other distortions, PCC drops noticeably with bias field distortions. <bold>Ghosting</bold> generally has a weak effect on most metrics, except on NMI and DISTS. <bold>Stripe Artifacts</bold> strongly influence a subset of metrics, including SSIM, LPIPS, DISTS and NMI, while most other metrics are not sensitive to this type of distortion. <bold>Blurring</bold> is hardly accounted for by most metrics, the strongest changes can be observed by the DISTS and NMI metrics. Regarding <bold>Gaussian Noise</bold>, SSIM is very sensitive, while MS-SSIM and CW-SSIM are not. The effect on error metrics is limited, while both learned metrics and NMI clearly indicate dissimilarity. <bold>Replace Artifacts</bold> are hardly detected by most metrics. Besides DSC, which aligns well with the distortion strength, PSNR and NMI are most sensitive. <bold>Gamma transforms</bold> with an increasing <inline-formula id="IEq93"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma &#x0003e; 1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq93.gif"/></alternatives></inline-formula> (Gamma High) or a decreasing <inline-formula id="IEq94"><alternatives><tex-math id="M92">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma &#x0003c; 1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq94.gif"/></alternatives></inline-formula> (Gamma Low) similarly influence all metrics. All of them, except PSNR, assess the distortions with <inline-formula id="IEq95"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma &#x0003e; 1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq95.gif"/></alternatives></inline-formula> as stronger. Regarding constant <bold>Intensity Shifts</bold>, NMI and PCC are invariant. Their scores reflect perfect similarity. The same holds, when the images are normalized by any of the five normalization methods (see Supplementary Figs. <xref rid="MOESM1" ref-type="media">S.8</xref>&#x02013;<xref rid="MOESM1" ref-type="media">S.15</xref>). Particularly for LPIPS and DISTS, Minmax normalization is recommended as standard. For all other metrics, except MS-SSIM and CW-SSIM, intensity shifts substantially decrease similarity. <bold>Translation</bold> strongly reduces the assessed similarity for all metrics besides DISTS. Even very small translations of strength <inline-formula id="IEq96"><alternatives><tex-math id="M94">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq96.gif"/></alternatives></inline-formula>, which corresponds to a 1% shift (2.4 pixels in our experiments), are clearly noticeable, as shown in Supplementary Sect.&#x000a0;<xref rid="MOESM1" ref-type="media">B.1</xref>. Only CW-SSIM is quite insensitive with respect to small translations, but is still very sensitive, when translation is strong. Compared to translation, <bold>Elastic Deforms</bold> only influence similarity metrics decently. Even stronger deformations have less impact on the metric scores, than the weakest translation does.</p><p id="Par62">There are some coherent observations regarding the <bold>normalization</bold> method and certain types of distortions. Selected results are shown in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>. <inline-formula id="IEq97"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textrm{cMinmax}_{5\%}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq97.gif"/></alternatives></inline-formula> normalization reduces gamma transforms, such that most metrics are less sensitive to gamma transforms after <inline-formula id="IEq98"><alternatives><tex-math id="M96">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textrm{cMinmax}_{5\%}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq98.gif"/></alternatives></inline-formula> normalization, compared to their standard normalization method. Minmax normalization amplifies Gaussian noise, Zscore normalization amplifies stripe artifacts.</p><p>
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Comparison of <bold>relative</bold> metric scores for selected distortions for comparison of normalization methods. As a relative metric score, the median of one selected distortion is divided by the median metric score of all distortions. The gray background indicates higher sensitivity in the comparison of two normalization methods. LPIPS, DISTS and DSC metric are not shown, due to fixed recommendations regarding the normalization method. Normalization methods and distortions were selected, where the similarity uniformly changed for between two normalization methods within one type of distortion and for all reference metrics. The arrows indicate, if the metrics metric increases (<inline-formula id="IEq90"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq90.gif"/></alternatives></inline-formula>) or decreases (<inline-formula id="IEq91"><alternatives><tex-math id="M98">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq91.gif"/></alternatives></inline-formula>) with increasing similarity.</p></caption><graphic position="anchor" xlink:href="41598_2025_87358_Tab4_HTML" id="MO5"/></table-wrap>
</p><p id="Par63">Gaussian noise and stripe artifacts most strongly decrease the DSC. Translated segmentations have decreasing overlap and thereby very low DSC. Tumors, which were inserted or removed by the Replace Reflect distortion are well indicated by decreasing DSC.</p></sec><sec id="Sec29"><title>Non-reference metrics</title><p id="Par65">The results for all non-reference metrics, all distortion strengths and all normalization methods are given in Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">S.11</xref>&#x02013;<xref rid="MOESM1" ref-type="media">S.14</xref>. The median metric scores are summarized in Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref> for images, that were normalized by Binning, as most of the metrics were designed for 8-bit images. This assures, that the metrics are most likely used as intended and sensitivity to a type of distortion is expected to be more consistent. Large differences between normalization methods are found for MB, BR and NIQE. We describe the results ordered by quality metric groups.</p><p>
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Median non-reference metric values for each distortion over 100 images. All non-reference metrics were assessed with binning normalization, which best resembles the intended use on 8-bit integer valued images for which most of these metrics were developed. Results for all normalization methods can be found in Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">S.11</xref>&#x02013;<xref rid="MOESM1" ref-type="media">S.14</xref>. The darker the background the higher the difference to the median metric value of the reference images. The arrows indicate, if the metric increases (<inline-formula id="IEq99"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\uparrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq99.gif"/></alternatives></inline-formula>) or decreases (<inline-formula id="IEq100"><alternatives><tex-math id="M100">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq100.gif"/></alternatives></inline-formula> with image quality.</p></caption><graphic position="anchor" xlink:href="41598_2025_87358_Tab5_HTML" id="MO6"/></table-wrap>
</p><p id="Par66">All <bold>Blurriness and Noisiness Metrics</bold>, except MB, can distinguish well between different strengths of blurring. At the same time, the blurriness scores diverge in the opposite direction for increasing strengths of stripe artifacts and Gaussian noise. Only BEW scores are not influenced much by these distortions. All blurriness and noisiness metrics show coherent deviations for images with spatial Translation (see also Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">S.12</xref>&#x02013;<xref rid="MOESM1" ref-type="media">S.13</xref>). <bold>MR Quality Metrics</bold>, i.e. MLC and MSLC, clearly identify stripe artifacts. MLC is very sensitive to noise, but generally hardly reflects the distortion level. MSLC can identify ghosting, best with <inline-formula id="IEq101"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textrm{cMinmax}_{5\%}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_87358_Article_IEq101.gif"/></alternatives></inline-formula> normalization (see also Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S.13</xref>). The <bold>Learned Quality Metrics</bold> BRISQUE and NIQE clearly attest low quality to images with stripe artifacts. However, the BRISQUE score indicates higher quality for very weak stripe artifacts and for images with Gaussian noise or ghosting (see also Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">S.14</xref>). The NIQE score indicates lower quality for Gaussian noise.</p></sec></sec><sec id="Sec30"><title>Discussion</title><p id="Par67">The experiments demonstrate specific weaknesses of the popular SSIM and PSNR metrics. They are strongly decreased by constant intensity shifts if no normalization is applied. PSNR is very dependent on the kind of normalization, which complicates its use as a comprehensive metric for comparing studies of different authors. This has been pointed out before<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>, but the normalization parameters are still not reported in many papers about medical synthesis models<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. SSIM and PSNR underestimate blurring and thereby favor blurred images over differently distorted images. This is inline with reported findings of previous studies<sup><xref ref-type="bibr" rid="CR57">57</xref>,<xref ref-type="bibr" rid="CR113">113</xref></sup>. SSIM and PSNR are both very sensitive to spatial translation, which is a frequently occurring issue in paired image-to-image translation. The source image and the reference image may not acquired by the same hardware or not immediately at the same time points and therefore the patient may not have the same location in both images. The proper use of the data range parameter of SSIM for medical images and potential biases have been investigated<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>, suggesting to use the dataset minimum and maximum values. In existing implementations of SSIM or PSNR, the default data range parameter might not be appropriate. For SSIM, although commonly applied on Zscore normalized MR images, a bias was demonstrated<sup><xref ref-type="bibr" rid="CR58">58</xref></sup> with negative values, which we did not specifically investigate. Even though the generation of synthetic structures is an issue in image-to-image translation<sup><xref ref-type="bibr" rid="CR112">112</xref></sup>, highly relevant replace artifacts are not sufficiently assessed by SSIM and PSNR. Unexpected behaviour of SSIM and PSNR as mentioned above was illustrated by examples<sup><xref ref-type="bibr" rid="CR114">114</xref></sup>.</p><p id="Par68">So how can these weaknesses be overcome? CW-SSIM is able to ignore small translations, due to calculation in the complex-wavelet domain. The DISTS metric also successfully focuses more on texture than spatial alignment. Precise registration would also strongly improve similarity assessment. However, interpolation may introduce blurring as shown by coherent variations of assessed quality by all non-reference quality metrics on spatially translated images. Our experiments also suggest, that a few strong local elastic deformations have a lot less impact on similarity metrics than rigid translations. At the same time, rigid registration is easier to solve via optimization than elastic registration, because only a few parameters need to be determined.</p><p id="Par69">Replace artifacts remain underestimated by most similarity metrics. For those artifacts, that resemble structures of diagnostic interest, the evaluation of segmentations with a specific segmentation model is useful. In our evaluation, we use a segmentation model, that was trained to detect different tumor regions. It successfully detects replace artifacts, where the tumor is doubled or removed. In this study, we did not perform further stability tests of the segmentation model. The DSC scores evaluated on the segmented images generally very well represent the expected differences between the reference and its distorted versions and manual inspection of selected segmentations did not uncover any obvious segmentation failures.</p><p id="Par70">The non-reference quality metrics can give valuable additional information about the quality of synthesized images. Blurring is easily and reliably detected by all blurriness metrics. For most metrics, assessed blurriness also decreases with other distortions, such as Gaussian noise, ghosting or gamma transforms, when these increase image contrast.</p><p id="Par71">For assessing ghosting, the MR quality metrics MLC and MSLC were evaluated. The MLC metric represents the line-wise correlation between neighboring lines. The reference value in our experiments lies at 96%, which is probably caused by high anatomical consistency in the present pixel spacing. It strongly decreases for stripe artifacts, which fits to the fact, that the stripes change relations between local image intensities and are not oriented along the x- or y-axis. Random Gaussian noise reduces statistical correlation and thereby also MLC. Ghosting also reduces MLC, as it additionally distorts image intensities locally. The MSLC metric only slightly increases with ghosting, a bit more using cMinmax normalization. One reason could be the relative weak scaling of the ghost intensity in our experiments. In contrast, stripe artifacts significantly increased the MSLC metrics. Compared to MLC, by coincidence, stripes seem to be in the same phase at the half-image width distance and thereby drastically increase line-wise correlation.</p><p id="Par72">Although the result tables for reference metrics (Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>) and for non-reference metrics (Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>) show absolute metric scores with and without normalization, both are representative for all results. In our experiments, reference images and distorted images had similar intensity ranges. Of course, MSE, MAE and NMSE yield much larger values without normalization, but by filling the background of the result tables with different shades of gray we emphasize the relative sensitivity of the metrics, and we found that the relative sensitivity is very similar between all normalizations for all metrics, with three exceptions: (1) Intensity shift is fully removed by any normalization leading to perfect similarity or equal quality to the reference. (2) BR, MB and NIQE display large differences between normalization methods and do not seem to measure blurriness or quality as intended when applied on non-8-bit integer valued images, which could be a problem of the adapted implementation, but also a problem of metric design. For all other non-reference metrics, the relative sensitivity does not seem to change with normalization. (3) We highlighted all uniform effects of normalization methods to reference metrics in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>, where we observe that certain normalization methods amplify or mitigate certain distortions. Gaussian noise and Ghosting, which extend the intensity value range compared to the reference image, are amplified by Minmax normalization, because the extended intensity value range is more strongly compressed. Stripe artifacts shift image mean values and therefore Zscore normalization creates shifts between originally corresponding intensity values. Gamma transforms, that compress intensity values at very high and low values are mitigated by cMinMax normalization, as this stretches intensities apart again. Therefore, normalization methods can be used to improve similarity or quality of images. Further distortions could be reduced by other methods, e.g. Bias field correction<sup><xref ref-type="bibr" rid="CR115">115</xref></sup>, which was designed to remove bias field artifacts.</p><p id="Par73">Our experiments were performed on T1-weighted contrast-enhanced MR images of the human brain and are therefore restricted to the BraSyn dataset. Future work should include a much broader set of MR images with other sequences and body regions to make sure, that the results are valid more generally. However, we assume, that the results are transferable to most MR sequences, because the characteristics, that are critical for metric assessment, are equally present (large value range, non-quantitative measurement, types of artifacts). The background in the BraSyn dataset takes up a large fraction of the image and was specifically preprocessed and set to 0. These factors largely influence some of the metrics, especially SSIM, PSNR and the error based metrics and therefore other data could yield different experimental results. We expect that using masks with these metrics will change absolute metric scores, but relative observations probably persist. However, the application of masks with metrics, that do not assess similarity or quality on a per-pixel basis, but require a neighborhood, is still to be implemented as future work. For the segmentation related DSC, taking different sub-regions into account, is already being explored<sup><xref ref-type="bibr" rid="CR101">101</xref></sup>. Furthermore, we did not investigate differences between a slice-wise 2D and a 3D application of the selected metrics. This should be considered specifically for MR images, and has already been reported to have significant impact<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>.</p><p id="Par74">The distortions analyzed in this study are to different degrees realistic. Ghosting<sup><xref ref-type="bibr" rid="CR116">116</xref></sup>, Stripe artifacts<sup><xref ref-type="bibr" rid="CR117">117</xref></sup> and Bias field<sup><xref ref-type="bibr" rid="CR118">118</xref></sup> were performed by simulation methods based on MR physics. Other distortions are perhaps less realistic, but account for important problems when acquiring and processing MR images. While other studies<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup> collected real image outputs of image synthesis models, we applied well-defined distortions in an isolated manner. This allows to better distinguish sensitivity of metrics between different distortions and to better understand specific metric properties.</p><p id="Par75">In comparison to other metric benchmarks<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup>, we do not compare the metric scores to human quality assessment. For each type of distortion, we tried to select five comparable strengths. As shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, the lowest and highest strengths were scaled to appear almost not visible or to decrease quality to an equally poor level. Even though distortion parameters might not be scaled perfectly to human perception, we assume, that the overall qualitative observations about which metrics are most sensitive towards which kind of distortions and the following conclusions are still valid.</p><p id="Par76">We are aware, that absolute values of different metrics cannot be directly compared, because most metrics relate non-linearly to human perception<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup>. Therefore, rank correlation is often utilized to compare different metrics. However, we provide absolute scores, which better display how far apart they are for single distortions. At the same time, we consider the ranking of different distortion types for each metric to derive the sensitivity. For the observation of different dynamics with distortion strengths we refer the reader to the Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">S.8</xref>&#x02013;<xref rid="MOESM1" ref-type="media">S.15</xref>.</p><p id="Par77">After LPIPS, a huge amount of deep-learning based reference and non-reference methods, among them PieApp<sup><xref ref-type="bibr" rid="CR119">119</xref></sup>, MetaIQA<sup><xref ref-type="bibr" rid="CR120">120</xref></sup>, P2P-BM<sup><xref ref-type="bibr" rid="CR121">121</xref></sup>, HyperIQA<sup><xref ref-type="bibr" rid="CR122">122</xref></sup> and AHIQ<sup><xref ref-type="bibr" rid="CR123">123</xref></sup>, has been proposed in the recent years, which were able to further improve performance on quality benchmark datasets such as LIVE and TID2013. The number of methods is increasing incredibly fast, and not all methods could be included in this study. This also holds true for a further number of more established metrics, such as FSIM<sup><xref ref-type="bibr" rid="CR124">124</xref></sup>, VIF<sup><xref ref-type="bibr" rid="CR125">125</xref></sup> and IW-SSIM<sup><xref ref-type="bibr" rid="CR126">126</xref></sup>, that were not included. We especially focused on methods that have been applied to the medical domain more frequently<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. The investigation of further methods and their applicability to the medical domain remains further work.</p></sec><sec id="Sec31"><title>Conclusions</title><p id="Par78">For the validation of medical image-to-image translation, we gave a broad overview of possible metrics. For 11 reference, 12 non-reference metrics and a segmentation metric, we presented a detailed study of their sensitivity to 11 types of distortions, which are specific for MR images. As a conclusion, we give a few recommendations for the selection and application of appropriate validation metrics.<list list-type="bullet"><list-item><p id="Par79">There is no optimal metric. Depending on the application, sensitivity to specific distortions is desired or not. Usually, a combination of metrics should be chosen, considering all expected distortions of the application. As reference metrics, the combination of SSIM, LPIPS, MSE and NMI is able to detect a large set of undesired distortions. MS-SSIM, PCC, DISTS, NMSE, and MAE do not give much additional information. MSE is the most frequently used error metric and therefore best suited for comparison to previous studies. In general, the use of PSNR should not be recommended. CW-SSIM is appropriate in addition to ignore slight misalignments to reference images.</p></list-item><list-item><p id="Par80">MB, BR and NIQE do not yield consistent quality scores for increasing distortion strengths and dramatically deviate for different intensity value ranges. They cannot be recommended for medical images.</p></list-item><list-item><p id="Par81">For detecting blurriness, BE and BEW and CPBD perform very robustly. When images are noisy or show stripe artifacts, BEW is hardly influenced. MB, BR and NIQE do not yield consistent quality scores for increasing distortion strengths and dramatically differ for different normalization methods. They cannot be recommended.</p></list-item><list-item><p id="Par82">Of all tested non-reference metrics, MLC and MSLC are best able to indicate ghosting artifacts.</p></list-item><list-item><p id="Par83">If normalization was used before metric assessment, the method and all relevant parameters must be reported in detail. These parameters must be considered, when comparing scores across studies, because normalization parameters can have a significant effect on absolute metric values.</p></list-item><list-item><p id="Par84">The data range parameter <italic>L</italic> is commonly used to adapt metrics, that were originally designed for a fixed 8-bit value range, to float valued images with potentially infinite intensity value ranges. However, scaling or binning to the range (0,&#x000a0;255) is often more consistent with the original design due to internal constants (e.g. SSIM) and thresholds (e.g. JNB). For reference metrics, the data range should be derived at least from both images if not from the whole dataset.</p></list-item><list-item><p id="Par85">For NMI and PCC, a data range parameter <italic>L</italic> is not needed and normalization can be omitted, if similar intensity values represent similar tissue types. Otherwise, an appropriate normalization method should aim for mapping similar tissue types to similar intensity values.</p></list-item><list-item><p id="Par86">If source images and target images are not spatially aligned, they must be registered with highest possible precision before evaluating with reference metrics. Rigid translation reduces assessed similarity more substantially than small local elastic deformations. CW-SSIM is robust against small translations. The type of interpolation used for registration may additionally blur the images. For LPIPS and DISTS, Minmax normalization to [0, 1] and [&#x02212;&#x000a0;1,1] is recommended. For non-quantitative image modalities such as MR, any kind of normalization is always recommended.&#x0201d;</p></list-item><list-item><p id="Par87">A segmentation downstream task is extremely useful for evaluation, because small but relevant structures are assessed. The performance of the segmentation model must be verified before using the segmentations for similarity assessment.</p></list-item></list>In summary, the metrics for evaluation of image-to-image MR synthesis models must be selected carefully. Frequently used SSIM and PSNR cover a large range of distortions, but have specific weaknesses, that must be covered by other metrics. Specifically PSNR does not seem appropriate for the evaluation of synthetic images. We suggest to always select metrics, that are able to detect undesired distortions specific and typical for the desired application and that are insensitive towards admitted and expected distortions. Which metrics are most appropriate can be directly derived from our experimental relative metric scores. As metrics are also often used as loss functions for model training<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> or validation metrics for model selection, the choice of appropriate metrics can directly improve image synthesis models before clinical validation by human readers and thereby reduce development time and costs.</p></sec><sec id="Sec32" sec-type="supplementary-material"><title>Supplementary Information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2025_87358_MOESM1_ESM.pdf"><caption><p>Supplementary Information.</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-025-87358-0.</p></sec><ack><title>Acknowledgements</title><p>The authors like to thank the Bayer team of the AI Innovation Platform for providing compute infrastructure and technical support.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>M.D. and M.L. designed the study, M.D. performed the experiments, and analysed the results. All authors reviewed and contributed to the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The BraSyn 2023 dataset is available at <ext-link ext-link-type="uri" xlink:href="http://www.synapse.org/brats2023">www.synapse.org/brats2023</ext-link>. The code for all metrics and distortions is available at <ext-link ext-link-type="uri" xlink:href="http://www.github.com/bayer-group/mr-image-metrics">www.github.com/bayer-group/mr-image-metrics</ext-link>.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par91">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Saharia, C. et al. Palette: Image-to-image diffusion models. In <italic>ACM SIGGRAPH 2022 Conference Proceedings, SIGGRAPH&#x02019;22</italic> (Association for Computing Machinery, 2022). 10.1145/3528233.3530757.</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Isola, P., Zhu, J.-Y., Zhou, T. &#x00026; Efros, A.&#x000a0;A. Image-to-image translation with conditional adversarial networks. In <italic>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 5967&#x02013;5976 (2017). 10.1109/CVPR.2017.632.</mixed-citation></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Yi</surname><given-names>X</given-names></name><name><surname>Walia</surname><given-names>E</given-names></name><name><surname>Babyn</surname><given-names>P</given-names></name></person-group><article-title>Generative adversarial network in medical imaging: A review</article-title><source>Med. Image Anal.</source><year>2019</year><volume>58</volume><fpage>101552</fpage><pub-id pub-id-type="doi">10.1016/j.media.2019.101552</pub-id><pub-id pub-id-type="pmid">31521965</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Yi, X., Walia, E. &#x00026; Babyn, P. Generative adversarial network in medical imaging: A review. <italic>Med. Image Anal.</italic><bold>58</bold>, 101552. 10.1016/j.media.2019.101552 (2019).<pub-id pub-id-type="pmid">31521965</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Ho, J., Jain, A. &#x00026; Abbeel, P. Denoising diffusion probabilistic models. In <italic>Advances in Neural Information Processing Systems</italic> Vol.&#x000a0;33 (eds Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. &#x00026; Lin, H.), 6840&#x02013;6851 (Curran Associates, Inc., 2020).</mixed-citation></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>M&#x000fc;ller-Franzes</surname><given-names>G</given-names></name><etal/></person-group><article-title>A multimodal comparison of latent denoising diffusion probabilistic models and generative adversarial networks for medical image synthesis</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><fpage>12098</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-39278-0</pub-id><pub-id pub-id-type="pmid">37495660</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">M&#x000fc;ller-Franzes, G. et al. A multimodal comparison of latent denoising diffusion probabilistic models and generative adversarial networks for medical image synthesis. <italic>Sci. Rep.</italic><bold>13</bold>, 12098 (2023).<pub-id pub-id-type="pmid">37495660</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Kebaili</surname><given-names>A</given-names></name><name><surname>Lapuyade-Lahorgue</surname><given-names>J</given-names></name><name><surname>Ruan</surname><given-names>S</given-names></name></person-group><article-title>Deep learning approaches for data augmentation in medical imaging: A review</article-title><source>J. Imaging</source><year>2023</year><volume>9</volume><fpage>81</fpage><pub-id pub-id-type="doi">10.3390/jimaging9040081</pub-id><pub-id pub-id-type="pmid">37103232</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Kebaili, A., Lapuyade-Lahorgue, J. &#x00026; Ruan, S. Deep learning approaches for data augmentation in medical imaging: A review. <italic>J. Imaging</italic><bold>9</bold>, 81. 10.3390/jimaging9040081 (2023).<pub-id pub-id-type="pmid">37103232</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Kaji</surname><given-names>S</given-names></name><name><surname>Kida</surname><given-names>S</given-names></name></person-group><article-title>Overview of image-to-image translation by use of deep neural networks: Denoising, super-resolution, modality conversion, and reconstruction in medical imaging</article-title><source>Radiol. Phys. Technol.</source><year>2019</year><volume>12</volume><fpage>235</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1007/s12194-019-00520-y</pub-id><pub-id pub-id-type="pmid">31222562</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Kaji, S. &#x00026; Kida, S. Overview of image-to-image translation by use of deep neural networks: Denoising, super-resolution, modality conversion, and reconstruction in medical imaging. <italic>Radiol. Phys. Technol.</italic><bold>12</bold>, 235&#x02013;248 (2019).<pub-id pub-id-type="pmid">31222562</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Zhu, J.-Y., Park, T., Isola, P. &#x00026; Efros, A.&#x000a0;A. Unpaired image-to-image translation using cycle-consistent adversarial networks. In <italic>2017 IEEE International Conference on Computer Vision (ICCV)</italic> 2242&#x02013;2251 (2017). 10.1109/ICCV.2017.244.</mixed-citation></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>K-T</given-names></name><etal/></person-group><article-title>Lumbar spine computed tomography to magnetic resonance imaging synthesis using generative adversarial network: Visual Turing test</article-title><source>Diagnostics</source><year>2022</year><volume>12</volume><issue>2</issue><fpage>530</fpage><pub-id pub-id-type="doi">10.3390/diagnostics12020530</pub-id><pub-id pub-id-type="pmid">35204619</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Hong, K.-T. et al. Lumbar spine computed tomography to magnetic resonance imaging synthesis using generative adversarial network: Visual Turing test. <italic>Diagnostics</italic><bold>12</bold>(2), 530 (2022).<pub-id pub-id-type="pmid">35204619</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Bahrami</surname><given-names>A</given-names></name><name><surname>Karimian</surname><given-names>A</given-names></name><name><surname>Arabi</surname><given-names>H</given-names></name></person-group><article-title>Comparison of different deep learning architectures for synthetic CT generation from MR images</article-title><source>Phys. Medica</source><year>2021</year><volume>90</volume><fpage>99</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/j.ejmp.2021.09.006</pub-id></element-citation><mixed-citation id="mc-CR10" publication-type="journal">Bahrami, A., Karimian, A. &#x00026; Arabi, H. Comparison of different deep learning architectures for synthetic CT generation from MR images. <italic>Phys. Medica</italic><bold>90</bold>, 99&#x02013;107. 10.1016/j.ejmp.2021.09.006 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Zhu, L. et al. Make-a-volume: Leveraging latent diffusion models for cross-modality 3d brain MRI synthesis. In <italic>Medical Image Computing and Computer Assisted Intervention&#x02014;MICCAI 2023</italic> (eds Greenspan, H. et al.) 592&#x02013;601 (Springer, 2023).</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Ben-Cohen, A., Klang, E., Raskin, S.&#x000a0;P., Amitai, M.&#x000a0;M. &#x00026; Greenspan, H. Virtual pet images from CT data using deep convolutional networks: Initial results. In <italic>Simulation and Synthesis in Medical Imaging</italic> (eds Tsaftaris, S.&#x000a0;A., Gooya, A., Frangi, A.&#x000a0;F. &#x00026; Prince, J.&#x000a0;L.) 49&#x02013;57 (Springer, 2017).</mixed-citation></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Eliminating CT radiation for clinical pet examination using deep learning</article-title><source>Eur. J. Radiol.</source><year>2022</year><volume>154</volume><fpage>110422</fpage><pub-id pub-id-type="doi">10.1016/j.ejrad.2022.110422</pub-id><pub-id pub-id-type="pmid">35767933</pub-id>
</element-citation><mixed-citation id="mc-CR13" publication-type="journal">Li, Q. et al. Eliminating CT radiation for clinical pet examination using deep learning. <italic>Eur. J. Radiol.</italic><bold>154</bold>, 110422 (2022).<pub-id pub-id-type="pmid">35767933</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Ammari</surname><given-names>S</given-names></name><etal/></person-group><article-title>Can deep learning replace gadolinium in neuro-oncology?: A reader study</article-title><source>Investig. Radiol.</source><year>2022</year><volume>57</volume><fpage>99</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1097/RLI.0000000000000811</pub-id><pub-id pub-id-type="pmid">34324463</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Ammari, S. et al. Can deep learning replace gadolinium in neuro-oncology?: A reader study. <italic>Investig. Radiol.</italic><bold>57</bold>, 99&#x02013;107 (2022).<pub-id pub-id-type="pmid">34324463</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Baltruschat, I.&#x000a0;M., Kreis, F., Hoelscher, A., Dohmen, M. &#x00026; Lenga, M. fRegGAN with k-space loss regularization for medical image translation (2023). <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2303.15938">https://arxiv.org/abs/2303.15938</ext-link>.</mixed-citation></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Weigert</surname><given-names>M</given-names></name><etal/></person-group><article-title>Content-aware image restoration: Pushing the limits of fluorescence microscopy</article-title><source>Nat. Methods</source><year>2018</year><volume>15</volume><fpage>1090</fpage><lpage>1097</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0216-7</pub-id><pub-id pub-id-type="pmid">30478326</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Weigert, M. et al. Content-aware image restoration: Pushing the limits of fluorescence microscopy. <italic>Nat. Methods</italic><bold>15</bold>, 1090&#x02013;1097 (2018).<pub-id pub-id-type="pmid">30478326</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>J</given-names></name><name><surname>&#x000d6;fverstedt</surname><given-names>J</given-names></name><name><surname>Lindblad</surname><given-names>J</given-names></name><name><surname>Sladoje</surname><given-names>N</given-names></name></person-group><article-title>Is image-to-image translation the panacea for multimodal image registration? A comparative study</article-title><source>PLoS ONE</source><year>2022</year><volume>17</volume><fpage>1</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1371/journal.pone.0276196</pub-id></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Lu, J., &#x000d6;fverstedt, J., Lindblad, J. &#x00026; Sladoje, N. Is image-to-image translation the panacea for multimodal image registration? A comparative study. <italic>PLoS ONE</italic><bold>17</bold>, 1&#x02013;33. 10.1371/journal.pone.0276196 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Raut</surname><given-names>P</given-names></name><name><surname>Baldini</surname><given-names>G</given-names></name><name><surname>Sch&#x000f6;neck</surname><given-names>M</given-names></name><name><surname>Caldeira</surname><given-names>L</given-names></name></person-group><article-title>Using a generative adversarial network to generate synthetic MRI images for multi-class automatic segmentation of brain tumors</article-title><source>Front. Radiol.</source><year>2024</year><volume>3</volume><fpage>1336902</fpage><pub-id pub-id-type="doi">10.3389/fradi.2023.1336902</pub-id><pub-id pub-id-type="pmid">38304344</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">Raut, P., Baldini, G., Sch&#x000f6;neck, M. &#x00026; Caldeira, L. Using a generative adversarial network to generate synthetic MRI images for multi-class automatic segmentation of brain tumors. <italic>Front. Radiol.</italic><bold>3</bold>, 1336902. 10.3389/fradi.2023.1336902 (2024).<pub-id pub-id-type="pmid">38304344</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>A</given-names></name><name><surname>Hamarneh</surname><given-names>G</given-names></name></person-group><article-title>Missing MRI pulse sequence synthesis using multi-modal generative adversarial network</article-title><source>IEEE Trans. Med. Imaging</source><year>2020</year><volume>39</volume><fpage>1170</fpage><lpage>1183</lpage><pub-id pub-id-type="doi">10.1109/TMI.2019.2945521</pub-id><pub-id pub-id-type="pmid">31603773</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Sharma, A. &#x00026; Hamarneh, G. Missing MRI pulse sequence synthesis using multi-modal generative adversarial network. <italic>IEEE Trans. Med. Imaging</italic><bold>39</bold>, 1170&#x02013;1183. 10.1109/TMI.2019.2945521 (2020).<pub-id pub-id-type="pmid">31603773</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhan</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Wu</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group><article-title>Multi-modal MRI image synthesis via GAN with multi-scale gate mergence</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2022</year><volume>26</volume><fpage>17</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2021.3088866</pub-id><pub-id pub-id-type="pmid">34125692</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">Zhan, B., Li, D., Wu, X., Zhou, J. &#x00026; Wang, Y. Multi-modal MRI image synthesis via GAN with multi-scale gate mergence. <italic>IEEE J. Biomed. Health Inform.</italic><bold>26</bold>, 17&#x02013;26. 10.1109/JBHI.2021.3088866 (2022).<pub-id pub-id-type="pmid">34125692</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Yurt</surname><given-names>M</given-names></name><etal/></person-group><article-title>mustGAN: Multi-stream generative adversarial networks for MR image synthesis</article-title><source>Med. Image Anal.</source><year>2021</year><volume>70</volume><fpage>101944</fpage><pub-id pub-id-type="doi">10.1016/j.media.2020.101944</pub-id><pub-id pub-id-type="pmid">33690024</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Yurt, M. et al. mustGAN: Multi-stream generative adversarial networks for MR image synthesis. <italic>Med. Image Anal.</italic><bold>70</bold>, 101944. 10.1016/j.media.2020.101944 (2021).<pub-id pub-id-type="pmid">33690024</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Mallio</surname><given-names>CA</given-names></name><etal/></person-group><article-title>Artificial intelligence to reduce or eliminate the need for gadolinium-based contrast agents in brain and cardiac MRI: A literature review</article-title><source>Investig. Radiol.</source><year>2023</year><volume>58</volume><fpage>746</fpage><pub-id pub-id-type="doi">10.1097/RLI.0000000000000983</pub-id><pub-id pub-id-type="pmid">37126454</pub-id>
</element-citation><mixed-citation id="mc-CR22" publication-type="journal">Mallio, C. A. et al. Artificial intelligence to reduce or eliminate the need for gadolinium-based contrast agents in brain and cardiac MRI: A literature review. <italic>Investig. Radiol.</italic><bold>58</bold>, 746 (2023).<pub-id pub-id-type="pmid">37126454</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Cr&#x000e9;t&#x000e9;-Roffet, F., Dolmiere, T., Ladret, P. &#x00026; Nicolas, M. The Blur Effect: Perception and estimation with a new no-reference perceptual blur metric. In <italic>SPIE Electronic Imaging Symposium Conference on Human Vision and Electronic Imaging</italic> Vol. XII, EI 6492&#x02013;16 (San Jose, 2007).</mixed-citation></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Shurcliff</surname><given-names>WA</given-names></name></person-group><article-title>Studies in optics: A.A. Michelson, University of Chicago Press, 1927, republished in 1962 as phoenix science series no. 514 paperback, 176 pp. illustrated, $1.75</article-title><source>J. Phys. Chem. Solids</source><year>1963</year><volume>24</volume><fpage>498</fpage><lpage>499</lpage><pub-id pub-id-type="doi">10.1016/0022-3697(63)90212-9</pub-id></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Shurcliff, W. A. Studies in optics: A.A. Michelson, University of Chicago Press, 1927, republished in 1962 as phoenix science series no. 514 paperback, 176 pp. illustrated, \$1.75. <italic>J. Phys. Chem. Solids</italic><bold>24</bold>, 498&#x02013;499 (1963).</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Rudin</surname><given-names>LI</given-names></name><name><surname>Osher</surname><given-names>S</given-names></name><name><surname>Fatemi</surname><given-names>E</given-names></name></person-group><article-title>Nonlinear total variation based noise removal algorithms</article-title><source>Phys. D Nonlinear Phenom.</source><year>1992</year><volume>60</volume><fpage>259</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1016/0167-2789(92)90242-F</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Rudin, L. I., Osher, S. &#x00026; Fatemi, E. Nonlinear total variation based noise removal algorithms. <italic>Phys. D Nonlinear Phenom.</italic><bold>60</bold>, 259&#x02013;268. 10.1016/0167-2789(92)90242-F (1992).</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Pianykh</surname><given-names>OS</given-names></name><name><surname>Pospelova</surname><given-names>K</given-names></name><name><surname>Kamboj</surname><given-names>NH</given-names></name></person-group><article-title>Modeling human perception of image quality</article-title><source>J. Digit. Imaging</source><year>2018</year><volume>31</volume><fpage>768</fpage><lpage>775</lpage><pub-id pub-id-type="doi">10.1007/s10278-018-0096-5</pub-id><pub-id pub-id-type="pmid">29968109</pub-id>
</element-citation><mixed-citation id="mc-CR26" publication-type="journal">Pianykh, O. S., Pospelova, K. &#x00026; Kamboj, N. H. Modeling human perception of image quality. <italic>J. Digit. Imaging</italic><bold>31</bold>, 768&#x02013;775 (2018).<pub-id pub-id-type="pmid">29968109</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Xing, X., Nan, Y., Felder, F., Walsh, S. &#x00026; Yang, G. The beauty or the beast: Which aspect of synthetic medical images deserves our focus? In <italic>2023 IEEE 36th International Symposium on Computer-Based Medical Systems (CBMS)</italic> 523&#x02013;528 (IEEE Computer Society, 2023). 10.1109/CBMS58004.2023.00273.</mixed-citation></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Borji</surname><given-names>A</given-names></name></person-group><article-title>Pros and cons of GAN evaluation measures: New developments</article-title><source>Comput. Vis. Image Underst.</source><year>2022</year><volume>215</volume><fpage>103329</fpage><pub-id pub-id-type="doi">10.1016/j.cviu.2021.103329</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Borji, A. Pros and cons of GAN evaluation measures: New developments. <italic>Comput. Vis. Image Underst.</italic><bold>215</bold>, 103329. 10.1016/j.cviu.2021.103329 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Salimans, T. et al. Improved techniques for training GANs. CoRR abs/1606.03498 (2016). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1606.03498">arXiv:1606.03498</ext-link>.</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Szegedy, C. et al. Going deeper with convolutions. In <italic>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 1&#x02013;9 (2015). 10.1109/CVPR.2015.7298594.</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B. &#x00026; Hochreiter, S. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In <italic>Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&#x02019;17</italic> 6629&#x02013;6640 (Curran Associates Inc., 2017).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Ponomarenko, N. et al. A new color image database tid2013: Innovations and results. In <italic>Advanced Concepts for Intelligent Vision Systems</italic> (eds Blanc-Talon, J., Kasinski, A., Philips, W., Popescu, D. &#x00026; Scheunders, P.) 402&#x02013;413 (Springer, 2013).</mixed-citation></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Sheikh</surname><given-names>H</given-names></name><name><surname>Sabir</surname><given-names>M</given-names></name><name><surname>Bovik</surname><given-names>A</given-names></name></person-group><article-title>A statistical evaluation of recent full reference image quality assessment algorithms</article-title><source>IEEE Trans. Image Process.</source><year>2006</year><volume>15</volume><fpage>3440</fpage><lpage>3451</lpage><pub-id pub-id-type="doi">10.1109/TIP.2006.881959</pub-id><pub-id pub-id-type="pmid">17076403</pub-id>
</element-citation><mixed-citation id="mc-CR33" publication-type="journal">Sheikh, H., Sabir, M. &#x00026; Bovik, A. A statistical evaluation of recent full reference image quality assessment algorithms. <italic>IEEE Trans. Image Process.</italic><bold>15</bold>, 3440&#x02013;3451. 10.1109/TIP.2006.881959 (2006).<pub-id pub-id-type="pmid">17076403</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Sheikh, H., Wang, Z., Cormack, L. &#x00026; Bovik, A. Live image quality assessment database release 2. <ext-link ext-link-type="uri" xlink:href="http://live.ece.utexas.edu/research/quality">http://live.ece.utexas.edu/research/quality</ext-link>.</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Ding, K., Ma, K., Wang, S. &#x00026; Simoncelli, E.&#x000a0;P. Comparison of image quality models for optimization of image processing systems. CoRR abs/2005.01338 (2020).</mixed-citation></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name><surname>Chow</surname><given-names>LS</given-names></name><name><surname>Rajagopal</surname><given-names>H</given-names></name><name><surname>Paramesran</surname><given-names>R</given-names></name></person-group><article-title>Correlation between subjective and objective assessment of magnetic resonance (MR) images</article-title><source>Magn. Resonance Imaging</source><year>2016</year><volume>34</volume><fpage>820</fpage><lpage>831</lpage><pub-id pub-id-type="doi">10.1016/j.mri.2016.03.006</pub-id></element-citation><mixed-citation id="mc-CR36" publication-type="journal">Chow, L. S., Rajagopal, H. &#x00026; Paramesran, R. Correlation between subjective and objective assessment of magnetic resonance (MR) images. <italic>Magn. Resonance Imaging</italic><bold>34</bold>, 820&#x02013;831. 10.1016/j.mri.2016.03.006 (2016).</mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name><surname>Kastryulin</surname><given-names>S</given-names></name><name><surname>Zakirov</surname><given-names>J</given-names></name><name><surname>Pezzotti</surname><given-names>N</given-names></name><name><surname>Dylov</surname><given-names>DV</given-names></name></person-group><article-title>Image quality assessment for magnetic resonance imaging</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>14154</fpage><lpage>14168</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3243466</pub-id></element-citation><mixed-citation id="mc-CR37" publication-type="journal">Kastryulin, S., Zakirov, J., Pezzotti, N. &#x00026; Dylov, D. V. Image quality assessment for magnetic resonance imaging. <italic>IEEE Access</italic><bold>11</bold>, 14154&#x02013;14168. 10.1109/ACCESS.2023.3243466 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">International Medical Device Regulators Forum. Software as a medical device (SAMD): Clinical evaluation&#x02014;Guidance for industry and food and drug administration staff (2017). <ext-link ext-link-type="uri" xlink:href="https://www.fda.gov/regulatory-information/search-fda-guidance-documents/software-medical-device-samd-clinical-evaluation">https://www.fda.gov/regulatory-information/search-fda-guidance-documents/software-medical-device-samd-clinical-evaluation</ext-link>.</mixed-citation></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name><surname>Cruz Rivera</surname><given-names>S</given-names></name><etal/></person-group><article-title>Guidelines for clinical trial protocols for interventions involving artificial intelligence: The spirit-AI extension</article-title><source>Lancet Digit. Health</source><year>2020</year><volume>2</volume><fpage>e549</fpage><lpage>e560</lpage><pub-id pub-id-type="doi">10.1016/S2589-7500(20)30219-3</pub-id><pub-id pub-id-type="pmid">33328049</pub-id>
</element-citation><mixed-citation id="mc-CR39" publication-type="journal">Cruz Rivera, S. et al. Guidelines for clinical trial protocols for interventions involving artificial intelligence: The spirit-AI extension. <italic>Lancet Digit. Health</italic><bold>2</bold>, e549&#x02013;e560 (2020).<pub-id pub-id-type="pmid">33328049</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>DW</given-names></name><name><surname>Jang</surname><given-names>HY</given-names></name><name><surname>Kim</surname><given-names>KW</given-names></name><name><surname>Shin</surname><given-names>Y</given-names></name><name><surname>Park</surname><given-names>SH</given-names></name></person-group><article-title>Design characteristics of studies reporting the performance of artificial intelligence algorithms for diagnostic analysis of medical images: Results from recently published papers</article-title><source>Korean J. Radiol.</source><year>2019</year><volume>20</volume><fpage>405</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.3348/kjr.2019.0025</pub-id><pub-id pub-id-type="pmid">30799571</pub-id>
</element-citation><mixed-citation id="mc-CR40" publication-type="journal">Kim, D. W., Jang, H. Y., Kim, K. W., Shin, Y. &#x00026; Park, S. H. Design characteristics of studies reporting the performance of artificial intelligence algorithms for diagnostic analysis of medical images: Results from recently published papers. <italic>Korean J. Radiol.</italic><bold>20</bold>, 405&#x02013;410 (2019).<pub-id pub-id-type="pmid">30799571</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X</given-names></name><etal/></person-group><article-title>A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: A systematic review and meta-analysis</article-title><source>Lancet Digit. Health</source><year>2019</year><volume>1</volume><fpage>e271</fpage><lpage>e297</lpage><pub-id pub-id-type="doi">10.1016/S2589-7500(19)30123-2</pub-id><pub-id pub-id-type="pmid">33323251</pub-id>
</element-citation><mixed-citation id="mc-CR41" publication-type="journal">Liu, X. et al. A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: A systematic review and meta-analysis. <italic>Lancet Digit. Health</italic><bold>1</bold>, e271&#x02013;e297. 10.1016/S2589-7500(19)30123-2 (2019).<pub-id pub-id-type="pmid">33323251</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Khunte</surname><given-names>M</given-names></name><etal/></person-group><article-title>Trends in clinical validation and usage of US food and drug administration-cleared artificial intelligence algorithms for medical imaging</article-title><source>Clin. Radiol.</source><year>2023</year><volume>78</volume><fpage>123</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1016/j.crad.2022.09.122</pub-id><pub-id pub-id-type="pmid">36625218</pub-id>
</element-citation><mixed-citation id="mc-CR42" publication-type="journal">Khunte, M. et al. Trends in clinical validation and usage of US food and drug administration-cleared artificial intelligence algorithms for medical imaging. <italic>Clin. Radiol.</italic><bold>78</bold>, 123&#x02013;129. 10.1016/j.crad.2022.09.122 (2023) (<bold>Special Issue Section: Artificial Intelligence and Machine Learning</bold>).<pub-id pub-id-type="pmid">36625218</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Nagendran</surname><given-names>M</given-names></name><etal/></person-group><article-title>Artificial intelligence versus clinicians: Systematic review of design, reporting standards, and claims of deep learning studies</article-title><source>BMJ</source><year>2020</year><volume>368</volume><fpage>m689</fpage><pub-id pub-id-type="doi">10.1136/bmj.m689</pub-id><pub-id pub-id-type="pmid">32213531</pub-id>
</element-citation><mixed-citation id="mc-CR43" publication-type="journal">Nagendran, M. et al. Artificial intelligence versus clinicians: Systematic review of design, reporting standards, and claims of deep learning studies. <italic>BMJ</italic><bold>368</bold>, m689. 10.1136/bmj.m689 (2020).<pub-id pub-id-type="pmid">32213531</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name><surname>Yasui</surname><given-names>K</given-names></name><etal/></person-group><article-title>Validation of deep learning-based CT image reconstruction for treatment planning</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><fpage>15413</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-42775-x</pub-id><pub-id pub-id-type="pmid">37723226</pub-id>
</element-citation><mixed-citation id="mc-CR44" publication-type="journal">Yasui, K. et al. Validation of deep learning-based CT image reconstruction for treatment planning. <italic>Sci. Rep.</italic><bold>13</bold>, 15413 (2023).<pub-id pub-id-type="pmid">37723226</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Bonardel</surname><given-names>G</given-names></name><etal/></person-group><article-title>Clinical and phantom validation of a deep learning based denoising algorithm for F-18-FDG pet images from lower detection counting in comparison with the standard acquisition</article-title><source>EJNMMI Phys.</source><year>2022</year><volume>9</volume><fpage>36</fpage><pub-id pub-id-type="doi">10.1186/s40658-022-00465-z</pub-id><pub-id pub-id-type="pmid">35543894</pub-id>
</element-citation><mixed-citation id="mc-CR45" publication-type="journal">Bonardel, G. et al. Clinical and phantom validation of a deep learning based denoising algorithm for F-18-FDG pet images from lower detection counting in comparison with the standard acquisition. <italic>EJNMMI Phys.</italic><bold>9</bold>, 36 (2022).<pub-id pub-id-type="pmid">35543894</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Food and Drug Administration (FDA). Artificial intelligence and machine learning (AI/ML)-enabled medical devices, accessed 14 March 2024; <ext-link ext-link-type="uri" xlink:href="https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices">https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices</ext-link>.</mixed-citation></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Reinke</surname><given-names>A</given-names></name><etal/></person-group><article-title>Understanding metric-related pitfalls in image analysis validation</article-title><source>Nat. Methods</source><year>2024</year><volume>21</volume><fpage>82</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1038/s41592-023-02150-0</pub-id></element-citation><mixed-citation id="mc-CR47" publication-type="journal">Reinke, A. et al. Understanding metric-related pitfalls in image analysis validation. <italic>Nat. Methods</italic><bold>21</bold>, 82&#x02013;194 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="journal"><person-group person-group-type="author"><name><surname>Maier-Hein</surname><given-names>L</given-names></name><etal/></person-group><article-title>Metrics reloaded: Recommendations for image analysis validation</article-title><source>Nat. Methods</source><year>2024</year><volume>21</volume><fpage>195</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1038/s41592-023-02151-z</pub-id><pub-id pub-id-type="pmid">38347141</pub-id>
</element-citation><mixed-citation id="mc-CR48" publication-type="journal">Maier-Hein, L. et al. Metrics reloaded: Recommendations for image analysis validation. <italic>Nat. Methods</italic><bold>21</bold>, 195&#x02013;212 (2024).<pub-id pub-id-type="pmid">38347141</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Baltruschat, I.&#x000a0;M., Janbakhshi, P. &#x00026; Lenga, M. Brasyn 2023 challenge: Missing MRI synthesis and the effect of different learning objectives (2024). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2403.07800">arXiv:2403.07800</ext-link>.</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Ne&#x0010d;asov&#x000e1;, T., Burgos, N. &#x00026; Svoboda, D. Validation and evaluation metrics for medical and biomedical image synthesis. In <italic>Biomedical Image Synthesis and Simulation</italic> (eds Burgos, N. &#x00026; Svoboda, D.) 573&#x02013;600. The MICCAI Society Book Series, chap.&#x000a0;25 (Academic Press, 2022).</mixed-citation></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><etal/></person-group><article-title>A survey on deep learning in medical image registration: New technologies, uncertainty, evaluation metrics, and beyond</article-title><source>Med. Image Anal.</source><year>2024</year><volume>100</volume><fpage>103385</fpage><pub-id pub-id-type="doi">10.1016/j.media.2024.103385</pub-id><pub-id pub-id-type="pmid">39612808</pub-id>
</element-citation><mixed-citation id="mc-CR51" publication-type="journal">Chen, J. et al. A survey on deep learning in medical image registration: New technologies, uncertainty, evaluation metrics, and beyond. <italic>Med. Image Anal.</italic><bold>100</bold>, 103385. 10.1016/j.media.2024.103385 (2024).<pub-id pub-id-type="pmid">39612808</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><citation-alternatives><element-citation id="ec-CR52" publication-type="journal"><person-group person-group-type="author"><name><surname>McNaughton</surname><given-names>J</given-names></name><etal/></person-group><article-title>Machine learning for medical image translation: A systematic review</article-title><source>Bioengineering</source><year>2023</year><volume>10</volume><fpage>1078</fpage><pub-id pub-id-type="doi">10.3390/bioengineering10091078</pub-id><pub-id pub-id-type="pmid">37760180</pub-id>
</element-citation><mixed-citation id="mc-CR52" publication-type="journal">McNaughton, J. et al. Machine learning for medical image translation: A systematic review. <italic>Bioengineering</italic><bold>10</bold>, 1078 (2023).<pub-id pub-id-type="pmid">37760180</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR53"><label>53.</label><citation-alternatives><element-citation id="ec-CR53" publication-type="journal"><person-group person-group-type="author"><name><surname>Haase</surname><given-names>R</given-names></name><etal/></person-group><article-title>Artificial contrast: Deep learning for reducing gadolinium-based contrast agents in neuroradiology</article-title><source>Investig. Radiol.</source><year>2023</year><volume>58</volume><fpage>539</fpage><lpage>547</lpage><pub-id pub-id-type="doi">10.1097/RLI.0000000000000963</pub-id><pub-id pub-id-type="pmid">36822654</pub-id>
</element-citation><mixed-citation id="mc-CR53" publication-type="journal">Haase, R. et al. Artificial contrast: Deep learning for reducing gadolinium-based contrast agents in neuroradiology. <italic>Investig. Radiol.</italic><bold>58</bold>, 539&#x02013;547 (2023).<pub-id pub-id-type="pmid">36822654</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR54"><label>54.</label><citation-alternatives><element-citation id="ec-CR54" publication-type="journal"><person-group person-group-type="author"><name><surname>Mudeng</surname><given-names>V</given-names></name><name><surname>Kim</surname><given-names>M</given-names></name><name><surname>Choe</surname><given-names>S-W</given-names></name></person-group><article-title>Prospects of structural similarity index for medical image analysis</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><fpage>3754</fpage><pub-id pub-id-type="doi">10.3390/app12083754</pub-id></element-citation><mixed-citation id="mc-CR54" publication-type="journal">Mudeng, V., Kim, M. &#x00026; Choe, S.-W. Prospects of structural similarity index for medical image analysis. <italic>Appl. Sci.</italic><bold>12</bold>, 3754 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR55"><label>55.</label><citation-alternatives><element-citation id="ec-CR55" publication-type="journal"><person-group person-group-type="author"><name><surname>Huynh-Thu</surname><given-names>Q</given-names></name><name><surname>Ghanbari</surname><given-names>M</given-names></name></person-group><article-title>Scope of validity of PSNR in image/video quality assessment</article-title><source>Electron. Lett.</source><year>2008</year><volume>44</volume><fpage>800</fpage><lpage>801</lpage><pub-id pub-id-type="doi">10.1049/el:20080522</pub-id></element-citation><mixed-citation id="mc-CR55" publication-type="journal">Huynh-Thu, Q. &#x00026; Ghanbari, M. Scope of validity of PSNR in image/video quality assessment. <italic>Electron. Lett.</italic><bold>44</bold>, 800&#x02013;801 (2008).</mixed-citation></citation-alternatives></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Korhonen, J. &#x00026; You, J. Peak signal-to-noise ratio revisited: Is simple beautiful? In <italic>2012 Fourth International Workshop on Quality of Multimedia Experience</italic>, 37&#x02013;38 (2012). 10.1109/QoMEX.2012.6263880.</mixed-citation></ref><ref id="CR57"><label>57.</label><citation-alternatives><element-citation id="ec-CR57" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Bovik</surname><given-names>AC</given-names></name></person-group><article-title>Mean squared error: Love it or leave it? a new look at signal fidelity measures</article-title><source>IEEE Signal Process. Mag.</source><year>2009</year><volume>26</volume><fpage>98</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1109/MSP.2008.930649</pub-id></element-citation><mixed-citation id="mc-CR57" publication-type="journal">Wang, Z. &#x00026; Bovik, A. C. Mean squared error: Love it or leave it? a new look at signal fidelity measures. <italic>IEEE Signal Process. Mag.</italic><bold>26</bold>, 98&#x02013;117. 10.1109/MSP.2008.930649 (2009).</mixed-citation></citation-alternatives></ref><ref id="CR58"><label>58.</label><citation-alternatives><element-citation id="ec-CR58" publication-type="journal"><person-group person-group-type="author"><name><surname>Gourdeau</surname><given-names>D</given-names></name><name><surname>Duchesne</surname><given-names>S</given-names></name><name><surname>Archambault</surname><given-names>L</given-names></name></person-group><article-title>On the proper use of structural similarity for the robust evaluation of medical image synthesis models</article-title><source>Med. Phys.</source><year>2022</year><volume>49</volume><fpage>2462</fpage><lpage>2474</lpage><pub-id pub-id-type="doi">10.1002/mp.15514</pub-id><pub-id pub-id-type="pmid">35106778</pub-id>
</element-citation><mixed-citation id="mc-CR58" publication-type="journal">Gourdeau, D., Duchesne, S. &#x00026; Archambault, L. On the proper use of structural similarity for the robust evaluation of medical image synthesis models. <italic>Med. Phys.</italic><bold>49</bold>, 2462&#x02013;2474. 10.1002/mp.15514 (2022).<pub-id pub-id-type="pmid">35106778</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR59"><label>59.</label><citation-alternatives><element-citation id="ec-CR59" publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>AH</given-names></name><name><surname>Pinard</surname><given-names>A</given-names></name><name><surname>Hammerling</surname><given-names>DM</given-names></name></person-group><article-title>On a structural similarity index approach for floating-point data</article-title><source>IEEE Trans. Vis. Comput. Gr.</source><year>2023</year><volume>30</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2023.3332843</pub-id></element-citation><mixed-citation id="mc-CR59" publication-type="journal">Baker, A. H., Pinard, A. &#x00026; Hammerling, D. M. On a structural similarity index approach for floating-point data. <italic>IEEE Trans. Vis. Comput. Gr.</italic><bold>30</bold>, 1&#x02013;13. 10.1109/TVCG.2023.3332843 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Goodfellow, I., Bengio, Y. &#x00026; Courville, A. <italic>Deep Learning</italic>, chap. 12.2.1 Preprocessing, 448 (MIT Press, 2016). <ext-link ext-link-type="uri" xlink:href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</ext-link>.</mixed-citation></ref><ref id="CR61"><label>61.</label><citation-alternatives><element-citation id="ec-CR61" publication-type="journal"><person-group person-group-type="author"><name><surname>Tellez</surname><given-names>D</given-names></name><etal/></person-group><article-title>Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology</article-title><source>Med. Image Anal.</source><year>2019</year><volume>58</volume><fpage>101544</fpage><pub-id pub-id-type="doi">10.1016/j.media.2019.101544</pub-id><pub-id pub-id-type="pmid">31466046</pub-id>
</element-citation><mixed-citation id="mc-CR61" publication-type="journal">Tellez, D. et al. Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology. <italic>Med. Image Anal.</italic><bold>58</bold>, 101544. 10.1016/j.media.2019.101544 (2019).<pub-id pub-id-type="pmid">31466046</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="other">Onofrey, J.&#x000a0;A. et al. Generalizable multi-site training and testing of deep neural networks using image normalization. <italic>Proceedings. IEEE International Symposium on Biomedical Imaging</italic> 348&#x02013;351 (2019).</mixed-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Reinhold, J.&#x000a0;C., Dewey, B.&#x000a0;E., Carass, A. &#x00026; Prince, J.&#x000a0;L. Evaluating the impact of intensity normalization on MR image synthesis. In <italic>Medical Imaging 2019: Image Processing</italic> vol. 10949 (eds Angelini, E.&#x000a0;D. &#x00026; Landman, B.&#x000a0;A.) 109493H (International Society for Optics and Photonics SPIE, 2019). 10.1117/12.2513089.</mixed-citation></ref><ref id="CR64"><label>64.</label><citation-alternatives><element-citation id="ec-CR64" publication-type="journal"><person-group person-group-type="author"><name><surname>Haase</surname><given-names>R</given-names></name><etal/></person-group><article-title>Reduction of gadolinium-based contrast agents in MRI using convolutional neural networks and different input protocols: Limited interchangeability of synthesized sequences with original full-dose images despite excellent quantitative performance</article-title><source>Investig. Radiol.</source><year>2023</year><volume>58</volume><fpage>420</fpage><pub-id pub-id-type="doi">10.1097/RLI.0000000000000955</pub-id><pub-id pub-id-type="pmid">36735399</pub-id>
</element-citation><mixed-citation id="mc-CR64" publication-type="journal">Haase, R. et al. Reduction of gadolinium-based contrast agents in MRI using convolutional neural networks and different input protocols: Limited interchangeability of synthesized sequences with original full-dose images despite excellent quantitative performance. <italic>Investig. Radiol.</italic><bold>58</bold>, 420 (2023).<pub-id pub-id-type="pmid">36735399</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR65"><label>65.</label><citation-alternatives><element-citation id="ec-CR65" publication-type="journal"><person-group person-group-type="author"><name><surname>Shinohara</surname><given-names>RT</given-names></name><etal/></person-group><article-title>Statistical normalization techniques for magnetic resonance imaging</article-title><source>NeuroImage Clin.</source><year>2014</year><volume>6</volume><fpage>9</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.nicl.2014.08.008</pub-id><pub-id pub-id-type="pmid">25379412</pub-id>
</element-citation><mixed-citation id="mc-CR65" publication-type="journal">Shinohara, R. T. et al. Statistical normalization techniques for magnetic resonance imaging. <italic>NeuroImage Clin.</italic><bold>6</bold>, 9&#x02013;19 (2014).<pub-id pub-id-type="pmid">25379412</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR66"><label>66.</label><citation-alternatives><element-citation id="ec-CR66" publication-type="journal"><person-group person-group-type="author"><name><surname>Ny&#x000fa;l</surname><given-names>LG</given-names></name><name><surname>Udupa</surname><given-names>JK</given-names></name></person-group><article-title>On standardizing the MR image intensity scale</article-title><source>Magn. Resonance Med.</source><year>1999</year><volume>42</volume><fpage>1072</fpage><lpage>1081</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1522-2594(199912)42:6&#x0003c;1072::AID-MRM11&#x0003e;3.0.CO;2-M</pub-id></element-citation><mixed-citation id="mc-CR66" publication-type="journal">Ny&#x000fa;l, L. G. &#x00026; Udupa, J. K. On standardizing the MR image intensity scale. <italic>Magn. Resonance Med.</italic><bold>42</bold>, 1072&#x02013;1081 (1999).</mixed-citation></citation-alternatives></ref><ref id="CR67"><label>67.</label><citation-alternatives><element-citation id="ec-CR67" publication-type="journal"><person-group person-group-type="author"><name><surname>H&#x000e4;nsch</surname><given-names>A</given-names></name><name><surname>Chlebus</surname><given-names>G</given-names></name><name><surname>Meine</surname><given-names>H</given-names></name></person-group><article-title>Improving automatic liver tumor segmentation in late-phase MRI using multi-model training and 3d convolutional neural networks</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>12262</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-16388-9</pub-id><pub-id pub-id-type="pmid">35851322</pub-id>
</element-citation><mixed-citation id="mc-CR67" publication-type="journal">H&#x000e4;nsch, A., Chlebus, G. &#x00026; Meine, H. Improving automatic liver tumor segmentation in late-phase MRI using multi-model training and 3d convolutional neural networks. <italic>Sci. Rep.</italic><bold>12</bold>, 12262. 10.1038/s41598-022-16388-9 (2022).<pub-id pub-id-type="pmid">35851322</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR68"><label>68.</label><citation-alternatives><element-citation id="ec-CR68" publication-type="journal"><person-group person-group-type="author"><name><surname>McCormick</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Ibanez</surname><given-names>L</given-names></name><name><surname>Jomier</surname><given-names>J</given-names></name><name><surname>Marion</surname><given-names>C</given-names></name></person-group><article-title>ITK: Enabling reproducible research and open science</article-title><source>Front. Neuroinform.</source><year>2014</year><volume>8</volume><fpage>13</fpage><pub-id pub-id-type="doi">10.3389/fninf.2014.00013</pub-id><pub-id pub-id-type="pmid">24600387</pub-id>
</element-citation><mixed-citation id="mc-CR68" publication-type="journal">McCormick, M., Liu, X., Ibanez, L., Jomier, J. &#x00026; Marion, C. ITK: Enabling reproducible research and open science. <italic>Front. Neuroinform.</italic><bold>8</bold>, 13. 10.3389/fninf.2014.00013 (2014).<pub-id pub-id-type="pmid">24600387</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Numpy Developers. The fundamental package for scientific computing with python. <ext-link ext-link-type="uri" xlink:href="https://numpy.org/">https://numpy.org/</ext-link>.</mixed-citation></ref><ref id="CR70"><label>70.</label><citation-alternatives><element-citation id="ec-CR70" publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><etal/></person-group><article-title>Scikit-learn: Machine learning in Python</article-title><source>J. Mach. Learn. Res.</source><year>2011</year><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation><mixed-citation id="mc-CR70" publication-type="journal">Pedregosa, F. et al. Scikit-learn: Machine learning in Python. <italic>J. Mach. Learn. Res.</italic><bold>12</bold>, 2825&#x02013;2830 (2011).</mixed-citation></citation-alternatives></ref><ref id="CR71"><label>71.</label><citation-alternatives><element-citation id="ec-CR71" publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname><given-names>S</given-names></name><etal/></person-group><article-title>Scikit-image: Image processing in Python</article-title><source>PeerJ</source><year>2014</year><volume>2</volume><fpage>e453</fpage><pub-id pub-id-type="doi">10.7717/peerj.453</pub-id><pub-id pub-id-type="pmid">25024921</pub-id>
</element-citation><mixed-citation id="mc-CR71" publication-type="journal">van der Walt, S. et al. Scikit-image: Image processing in Python. <italic>PeerJ</italic><bold>2</bold>, e453. 10.7717/peerj.453 (2014).<pub-id pub-id-type="pmid">25024921</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR72"><label>72.</label><mixed-citation publication-type="other">Torchmetrics&#x02014;Measuring reproducibility in Pytorch (2022). 10.21105/joss.04101.</mixed-citation></ref><ref id="CR73"><label>73.</label><citation-alternatives><element-citation id="ec-CR73" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Bovik</surname><given-names>AC</given-names></name><name><surname>Sheikh</surname><given-names>HR</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Image quality assessment: From error visibility to structural similarity</article-title><source>IEEE Trans. Image Process.</source><year>2004</year><volume>13</volume><fpage>600</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1109/TIP.2003.819861</pub-id><pub-id pub-id-type="pmid">15376593</pub-id>
</element-citation><mixed-citation id="mc-CR73" publication-type="journal">Wang, Z., Bovik, A. C., Sheikh, H. R. &#x00026; Simoncelli, E. P. Image quality assessment: From error visibility to structural similarity. <italic>IEEE Trans. Image Process.</italic><bold>13</bold>, 600&#x02013;12 (2004).<pub-id pub-id-type="pmid">15376593</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR74"><label>74.</label><mixed-citation publication-type="other">Wang, Z., Simoncelli, E. &#x00026; Bovik, A. Multiscale structural similarity for image quality assessment. In <italic>The Thirty-Seventh Asilomar Conference on Signals, Systems and Computers, 2003</italic>, vol.&#x000a0;2, 1398&#x02013;1402 (2003). 10.1109/ACSSC.2003.1292216.</mixed-citation></ref><ref id="CR75"><label>75.</label><mixed-citation publication-type="other">Ding, K. IQA optimization (2020). <ext-link ext-link-type="uri" xlink:href="https://github.com/dingkeyan93/IQA-optimization/">https://github.com/dingkeyan93/IQA-optimization/</ext-link>.</mixed-citation></ref><ref id="CR76"><label>76.</label><citation-alternatives><element-citation id="ec-CR76" publication-type="journal"><person-group person-group-type="author"><name><surname>Sampat</surname><given-names>MP</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Gupta</surname><given-names>S</given-names></name><name><surname>Bovik</surname><given-names>AC</given-names></name><name><surname>Markey</surname><given-names>MK</given-names></name></person-group><article-title>Complex wavelet structural similarity: A new image similarity index</article-title><source>IEEE Trans. Image Process.</source><year>2009</year><volume>18</volume><fpage>2385</fpage><lpage>2401</lpage><pub-id pub-id-type="doi">10.1109/TIP.2009.2025923</pub-id><pub-id pub-id-type="pmid">19556195</pub-id>
</element-citation><mixed-citation id="mc-CR76" publication-type="journal">Sampat, M. P., Wang, Z., Gupta, S., Bovik, A. C. &#x00026; Markey, M. K. Complex wavelet structural similarity: A new image similarity index. <italic>IEEE Trans. Image Process.</italic><bold>18</bold>, 2385&#x02013;2401. 10.1109/TIP.2009.2025923 (2009).<pub-id pub-id-type="pmid">19556195</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR77"><label>77.</label><citation-alternatives><element-citation id="ec-CR77" publication-type="journal"><person-group person-group-type="author"><name><surname>Huynh-Thu</surname><given-names>Q</given-names></name><name><surname>Ghanbari</surname><given-names>M</given-names></name></person-group><article-title>Scope of validity of PSNR in image/video quality assessment</article-title><source>Electron. Lett.</source><year>2008</year><volume>44</volume><fpage>800</fpage><lpage>801</lpage><pub-id pub-id-type="doi">10.1049/el:20080522</pub-id></element-citation><mixed-citation id="mc-CR77" publication-type="journal">Huynh-Thu, Q. &#x00026; Ghanbari, M. Scope of validity of PSNR in image/video quality assessment. <italic>Electron. Lett.</italic><bold>44</bold>, 800&#x02013;801 (2008).</mixed-citation></citation-alternatives></ref><ref id="CR78"><label>78.</label><mixed-citation publication-type="other">Perceptual similarity metric and dataset. <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/lpips/">https://pypi.org/project/lpips/</ext-link>.</mixed-citation></ref><ref id="CR79"><label>79.</label><mixed-citation publication-type="other">Zhang, R., Isola, P., Efros, A.&#x000a0;A., Shechtman, E. &#x00026; Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. CoRR ayn/1801.03924 (2018). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1801.03924">arXiv:1801.03924</ext-link>.</mixed-citation></ref><ref id="CR80"><label>80.</label><mixed-citation publication-type="other">Ding, K. <ext-link ext-link-type="uri" xlink:href="https://github.com/dingkeyan93/DISTS">https://github.com/dingkeyan93/DISTS</ext-link>.</mixed-citation></ref><ref id="CR81"><label>81.</label><citation-alternatives><element-citation id="ec-CR81" publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>K</given-names></name><name><surname>Ma</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Image quality assessment: Unifying structure and texture similarity</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>44</volume><fpage>2567</fpage><lpage>2581</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2020.3045810</pub-id><pub-id pub-id-type="pmid">33338012</pub-id>
</element-citation><mixed-citation id="mc-CR81" publication-type="journal">Ding, K., Ma, K., Wang, S. &#x00026; Simoncelli, E. P. Image quality assessment: Unifying structure and texture similarity. <italic>IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>44</bold>, 2567&#x02013;2581. 10.1109/TPAMI.2020.3045810 (2022).<pub-id pub-id-type="pmid">33338012</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR82"><label>82.</label><citation-alternatives><element-citation id="ec-CR82" publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>MG</given-names></name><name><surname>Jung</surname><given-names>JH</given-names></name><name><surname>Jeon</surname><given-names>JW</given-names></name></person-group><article-title>No-reference image quality assessment using blur and noise</article-title><source>Int. J. Electr. Comput. Eng.</source><year>2009</year><volume>3</volume><fpage>184</fpage><lpage>188</lpage></element-citation><mixed-citation id="mc-CR82" publication-type="journal">Choi, M. G., Jung, J. H. &#x00026; Jeon, J. W. No-reference image quality assessment using blur and noise. <italic>Int. J. Electr. Comput. Eng.</italic><bold>3</bold>, 184&#x02013;188 (2009).</mixed-citation></citation-alternatives></ref><ref id="CR83"><label>83.</label><mixed-citation publication-type="other">Pech-Pacheco, J., Cristobal, G., Chamorro-Martinez, J. &#x00026; Fernandez-Valdivia, J. Diatom autofocusing in brightfield microscopy: A comparative study. In <italic>Proceedings 15th International Conference on Pattern Recognition. ICPR-2000</italic>, Vol.&#x000a0;3, 314&#x02013;317 (2000). 10.1109/ICPR.2000.903548.</mixed-citation></ref><ref id="CR84"><label>84.</label><mixed-citation publication-type="other">Marziliano, P., Dufaux, F., Winkler, S. &#x00026; Ebrahimi, T. A no-reference perceptual blur metric. In <italic>Proceedings. International Conference on Image Processing</italic>, vol.&#x000a0;3, III&#x02013;III (2002). 10.1109/ICIP.2002.1038902.</mixed-citation></ref><ref id="CR85"><label>85.</label><mixed-citation publication-type="other">Roberts, D. <ext-link ext-link-type="uri" xlink:href="https://github.com/davidatroberts/No-Reference-Sharpness-Metric">https://github.com/davidatroberts/No-Reference-Sharpness-Metric</ext-link>.</mixed-citation></ref><ref id="CR86"><label>86.</label><citation-alternatives><element-citation id="ec-CR86" publication-type="journal"><person-group person-group-type="author"><name><surname>Ferzli</surname><given-names>R</given-names></name><name><surname>Karam</surname><given-names>LJ</given-names></name></person-group><article-title>A no-reference objective image sharpness metric based on the notion of just noticeable blur (JNB)</article-title><source>IEEE Trans. Image Process.</source><year>2009</year><volume>18</volume><fpage>717</fpage><lpage>728</lpage><pub-id pub-id-type="doi">10.1109/TIP.2008.2011760</pub-id><pub-id pub-id-type="pmid">19278916</pub-id>
</element-citation><mixed-citation id="mc-CR86" publication-type="journal">Ferzli, R. &#x00026; Karam, L. J. A no-reference objective image sharpness metric based on the notion of just noticeable blur (JNB). <italic>IEEE Trans. Image Process.</italic><bold>18</bold>, 717&#x02013;728. 10.1109/TIP.2008.2011760 (2009).<pub-id pub-id-type="pmid">19278916</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR87"><label>87.</label><mixed-citation publication-type="other">Karam, L. &#x00026; Narvekar, N. <ext-link ext-link-type="uri" xlink:href="https://github.com/0x64746b/python-cpbd">https://github.com/0x64746b/python-cpbd</ext-link>.</mixed-citation></ref><ref id="CR88"><label>88.</label><citation-alternatives><element-citation id="ec-CR88" publication-type="journal"><person-group person-group-type="author"><name><surname>Narvekar</surname><given-names>ND</given-names></name><name><surname>Karam</surname><given-names>LJ</given-names></name></person-group><article-title>A no-reference image blur metric based on the cumulative probability of blur detection (CPBD)</article-title><source>IEEE Trans. Image Process.</source><year>2011</year><volume>20</volume><fpage>2678</fpage><lpage>2683</lpage><pub-id pub-id-type="doi">10.1109/TIP.2011.2131660</pub-id><pub-id pub-id-type="pmid">21447451</pub-id>
</element-citation><mixed-citation id="mc-CR88" publication-type="journal">Narvekar, N. D. &#x00026; Karam, L. J. A no-reference image blur metric based on the cumulative probability of blur detection (CPBD). <italic>IEEE Trans. Image Process.</italic><bold>20</bold>, 2678&#x02013;2683. 10.1109/TIP.2011.2131660 (2011).<pub-id pub-id-type="pmid">21447451</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR89"><label>89.</label><citation-alternatives><element-citation id="ec-CR89" publication-type="journal"><person-group person-group-type="author"><name><surname>Schuppert</surname><given-names>C</given-names></name><etal/></person-group><article-title>Whole-body magnetic resonance imaging in the large population-based German national cohort study: Predictive capability of automated image quality assessment for protocol repetitions</article-title><source>Investig. Radiol.</source><year>2022</year><volume>57</volume><fpage>478</fpage><pub-id pub-id-type="doi">10.1097/RLI.0000000000000861</pub-id><pub-id pub-id-type="pmid">35184102</pub-id>
</element-citation><mixed-citation id="mc-CR89" publication-type="journal">Schuppert, C. et al. Whole-body magnetic resonance imaging in the large population-based German national cohort study: Predictive capability of automated image quality assessment for protocol repetitions. <italic>Investig. Radiol.</italic><bold>57</bold>, 478 (2022).<pub-id pub-id-type="pmid">35184102</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR90"><label>90.</label><mixed-citation publication-type="other">Guha, R. <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/brisque">https://pypi.org/project/brisque</ext-link>.</mixed-citation></ref><ref id="CR91"><label>91.</label><citation-alternatives><element-citation id="ec-CR91" publication-type="journal"><person-group person-group-type="author"><name><surname>Mittal</surname><given-names>A</given-names></name><name><surname>Moorthy</surname><given-names>AK</given-names></name><name><surname>Bovik</surname><given-names>AC</given-names></name></person-group><article-title>No-reference image quality assessment in the spatial domain</article-title><source>IEEE Trans. Image Process.</source><year>2012</year><volume>21</volume><fpage>4695</fpage><lpage>4708</lpage><pub-id pub-id-type="doi">10.1109/TIP.2012.2214050</pub-id><pub-id pub-id-type="pmid">22910118</pub-id>
</element-citation><mixed-citation id="mc-CR91" publication-type="journal">Mittal, A., Moorthy, A. K. &#x00026; Bovik, A. C. No-reference image quality assessment in the spatial domain. <italic>IEEE Trans. Image Process.</italic><bold>21</bold>, 4695&#x02013;4708. 10.1109/TIP.2012.2214050 (2012).<pub-id pub-id-type="pmid">22910118</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR92"><label>92.</label><mixed-citation publication-type="other">Gupta, P. <ext-link ext-link-type="uri" xlink:href="https://github.com/guptapraful/niqe">https://github.com/guptapraful/niqe</ext-link>.</mixed-citation></ref><ref id="CR93"><label>93.</label><citation-alternatives><element-citation id="ec-CR93" publication-type="journal"><person-group person-group-type="author"><name><surname>Mittal</surname><given-names>A</given-names></name><name><surname>Soundararajan</surname><given-names>R</given-names></name><name><surname>Bovik</surname><given-names>AC</given-names></name></person-group><article-title>Making a &#x0201c;completely blind&#x0201d; image quality analyzer</article-title><source>IEEE Signal Process. Lett.</source><year>2013</year><volume>20</volume><fpage>209</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1109/LSP.2012.2227726</pub-id></element-citation><mixed-citation id="mc-CR93" publication-type="journal">Mittal, A., Soundararajan, R. &#x00026; Bovik, A. C. Making a &#x0201c;completely blind&#x02019;&#x02019; image quality analyzer. <italic>IEEE Signal Process. Lett.</italic><bold>20</bold>, 209&#x02013;212. 10.1109/LSP.2012.2227726 (2013).</mixed-citation></citation-alternatives></ref><ref id="CR94"><label>94.</label><mixed-citation publication-type="other">Li, Y. et al. Samscore: A semantic structural similarity metric for image translation evaluation (2023). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2305.15367">arXiv:2305.15367</ext-link>.</mixed-citation></ref><ref id="CR95"><label>95.</label><citation-alternatives><element-citation id="ec-CR95" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><etal/></person-group><article-title>One model to synthesize them all: Multi-contrast multi-scale transformer for missing data imputation</article-title><source>IEEE Trans. Med. Imaging</source><year>2023</year><volume>42</volume><fpage>2577</fpage><lpage>2591</lpage><pub-id pub-id-type="doi">10.1109/TMI.2023.3261707</pub-id><pub-id pub-id-type="pmid">37030684</pub-id>
</element-citation><mixed-citation id="mc-CR95" publication-type="journal">Liu, J. et al. One model to synthesize them all: Multi-contrast multi-scale transformer for missing data imputation. <italic>IEEE Trans. Med. Imaging</italic><bold>42</bold>, 2577&#x02013;2591. 10.1109/TMI.2023.3261707 (2023).<pub-id pub-id-type="pmid">37030684</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR96"><label>96.</label><citation-alternatives><element-citation id="ec-CR96" publication-type="journal"><person-group person-group-type="author"><name><surname>Maes</surname><given-names>F</given-names></name><name><surname>Collignon</surname><given-names>A</given-names></name><name><surname>Vandermeulen</surname><given-names>D</given-names></name><name><surname>Marchal</surname><given-names>G</given-names></name><name><surname>Suetens</surname><given-names>P</given-names></name></person-group><article-title>Multimodality image registration by maximization of mutual information</article-title><source>IEEE Trans. Med. Imaging</source><year>1997</year><volume>16</volume><fpage>187</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1109/42.563664</pub-id><pub-id pub-id-type="pmid">9101328</pub-id>
</element-citation><mixed-citation id="mc-CR96" publication-type="journal">Maes, F., Collignon, A., Vandermeulen, D., Marchal, G. &#x00026; Suetens, P. Multimodality image registration by maximization of mutual information. <italic>IEEE Trans. Med. Imaging</italic><bold>16</bold>, 187&#x02013;198. 10.1109/42.563664 (1997).<pub-id pub-id-type="pmid">9101328</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR97"><label>97.</label><mixed-citation publication-type="other">Ghorbani, A., Natarajan, V., Coz, D. &#x00026; Liu, Y. DermGAN: Synthetic generation of clinical skin images with pathology. In <italic>Proceedings of the Machine Learning for Health NeurIPS Workshop</italic> (eds Dalca, A.&#x000a0;V. et al.), vol. 116 of <italic>Proceedings of Machine Learning Research</italic> 155&#x02013;170 (PMLR, 2020).</mixed-citation></ref><ref id="CR98"><label>98.</label><citation-alternatives><element-citation id="ec-CR98" publication-type="journal"><person-group person-group-type="author"><name><surname>Jang</surname><given-names>M</given-names></name><etal/></person-group><article-title>Image Turing test and its applications on synthetic chest radiographs by using the progressive growing generative adversarial network</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><fpage>2356</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-28175-1</pub-id><pub-id pub-id-type="pmid">36759636</pub-id>
</element-citation><mixed-citation id="mc-CR98" publication-type="journal">Jang, M. et al. Image Turing test and its applications on synthetic chest radiographs by using the progressive growing generative adversarial network. <italic>Sci. Rep.</italic><bold>13</bold>, 2356 (2023).<pub-id pub-id-type="pmid">36759636</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR99"><label>99.</label><citation-alternatives><element-citation id="ec-CR99" publication-type="journal"><person-group person-group-type="author"><name><surname>McNaughton</surname><given-names>J</given-names></name><etal/></person-group><article-title>Synthetic MRI generation from CT scans for stroke patients</article-title><source>BioMedInformatics</source><year>2023</year><volume>3</volume><fpage>791</fpage><lpage>816</lpage><pub-id pub-id-type="doi">10.3390/biomedinformatics3030050</pub-id></element-citation><mixed-citation id="mc-CR99" publication-type="journal">McNaughton, J. et al. Synthetic MRI generation from CT scans for stroke patients. <italic>BioMedInformatics</italic><bold>3</bold>, 791&#x02013;816. 10.3390/biomedinformatics3030050 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR100"><label>100.</label><citation-alternatives><element-citation id="ec-CR100" publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmermann</surname><given-names>L</given-names></name><etal/></person-group><article-title>An MRI sequence independent convolutional neural network for synthetic head CT generation in proton therapy</article-title><source>Z. Med. Phys.</source><year>2022</year><volume>32</volume><fpage>218</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1016/j.zemedi.2021.10.003</pub-id><pub-id pub-id-type="pmid">34920940</pub-id>
</element-citation><mixed-citation id="mc-CR100" publication-type="journal">Zimmermann, L. et al. An MRI sequence independent convolutional neural network for synthetic head CT generation in proton therapy. <italic>Z. Med. Phys.</italic><bold>32</bold>, 218&#x02013;227 (2022).<pub-id pub-id-type="pmid">34920940</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR101"><label>101.</label><mixed-citation publication-type="other">Andrews, S. &#x00026; Hamarneh, G. Multi-region probabilistic dice similarity coefficient using the aitchison distance and bipartite graph matching (2015). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1509.07244">arXiv:1509.07244</ext-link>.</mixed-citation></ref><ref id="CR102"><label>102.</label><mixed-citation publication-type="other">Xing, X., Nan, Y., Felder, F., Walsh, S. &#x00026; Yang, G. The beauty or the beast: Which aspect of synthetic medical images deserves our focus? In <italic>2023 IEEE 36th International Symposium on Computer-Based Medical Systems (CBMS)</italic> 523&#x02013;528 (IEEE Computer Society, 2023). 10.1109/CBMS58004.2023.00273.</mixed-citation></ref><ref id="CR103"><label>103.</label><citation-alternatives><element-citation id="ec-CR103" publication-type="journal"><person-group person-group-type="author"><name><surname>Dice</surname><given-names>LR</given-names></name></person-group><article-title>Measures of the amount of ecologic association between species</article-title><source>Ecology</source><year>1945</year><volume>26</volume><fpage>297</fpage><lpage>302</lpage><pub-id pub-id-type="doi">10.2307/1932409</pub-id></element-citation><mixed-citation id="mc-CR103" publication-type="journal">Dice, L. R. Measures of the amount of ecologic association between species. <italic>Ecology</italic><bold>26</bold>, 297&#x02013;302. 10.2307/1932409 (1945).</mixed-citation></citation-alternatives></ref><ref id="CR104"><label>104.</label><citation-alternatives><element-citation id="ec-CR104" publication-type="journal"><person-group person-group-type="author"><name><surname>Mason</surname><given-names>A</given-names></name><etal/></person-group><article-title>Comparison of objective image quality metrics to expert radiologists&#x02019; scoring of diagnostic quality of MR images</article-title><source>IEEE Trans. Med. Imaging</source><year>2020</year><volume>39</volume><fpage>1064</fpage><lpage>1072</lpage><pub-id pub-id-type="doi">10.1109/TMI.2019.2930338</pub-id><pub-id pub-id-type="pmid">31535985</pub-id>
</element-citation><mixed-citation id="mc-CR104" publication-type="journal">Mason, A. et al. Comparison of objective image quality metrics to expert radiologists&#x02019; scoring of diagnostic quality of MR images. <italic>IEEE Trans. Med. Imaging</italic><bold>39</bold>, 1064&#x02013;1072. 10.1109/TMI.2019.2930338 (2020).<pub-id pub-id-type="pmid">31535985</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR105"><label>105.</label><citation-alternatives><element-citation id="ec-CR105" publication-type="journal"><person-group person-group-type="author"><name><surname>Jayachandran Preetha</surname><given-names>C</given-names></name><etal/></person-group><article-title>Deep-learning-based synthesis of post-contrast t1-weighted MRI for tumour response assessment in neuro-oncology: A multicentre, retrospective cohort study</article-title><source>Lancet Digit. Health</source><year>2021</year><volume>3</volume><fpage>e784</fpage><lpage>e794</lpage><pub-id pub-id-type="doi">10.1016/S2589-7500(21)00205-3</pub-id><pub-id pub-id-type="pmid">34688602</pub-id>
</element-citation><mixed-citation id="mc-CR105" publication-type="journal">Jayachandran Preetha, C. et al. Deep-learning-based synthesis of post-contrast t1-weighted MRI for tumour response assessment in neuro-oncology: A multicentre, retrospective cohort study. <italic>Lancet Digit. Health</italic><bold>3</bold>, e784&#x02013;e794. 10.1016/S2589-7500(21)00205-3 (2021).<pub-id pub-id-type="pmid">34688602</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR106"><label>106.</label><citation-alternatives><element-citation id="ec-CR106" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>M</given-names></name><name><surname>Yu</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Ke</surname><given-names>Z</given-names></name><name><surname>Zhi</surname><given-names>C</given-names></name></person-group><article-title>Review: A survey on objective evaluation of image sharpness</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><fpage>2652</fpage><pub-id pub-id-type="doi">10.3390/app13042652</pub-id></element-citation><mixed-citation id="mc-CR106" publication-type="journal">Zhu, M., Yu, L., Wang, Z., Ke, Z. &#x00026; Zhi, C. Review: A survey on objective evaluation of image sharpness. <italic>Appl. Sci.</italic><bold>13</bold>, 2652. 10.3390/app13042652 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR107"><label>107.</label><mixed-citation publication-type="other">Martin, D., Fowlkes, C., Tal, D. &#x00026; Malik, J. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In <italic>Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001</italic>, vol.&#x000a0;2 416&#x02013;423 (2001). 10.1109/ICCV.2001.937655.</mixed-citation></ref><ref id="CR108"><label>108.</label><mixed-citation publication-type="other">Baid, U. et.&#x000a0;al. The RSNA-ASNR-MICCAI brats 2021 benchmark on brain tumor segmentation and radiogenomic classification (2021). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2107.02314">arXiv:2107.02314</ext-link>.</mixed-citation></ref><ref id="CR109"><label>109.</label><citation-alternatives><element-citation id="ec-CR109" publication-type="journal"><person-group person-group-type="author"><name><surname>Menze</surname><given-names>BH</given-names></name><etal/></person-group><article-title>The multimodal brain tumor image segmentation benchmark (BRATS)</article-title><source>IEEE Trans. Med. Imaging</source><year>2015</year><volume>34</volume><fpage>1993</fpage><pub-id pub-id-type="doi">10.1109/TMI.2014.2377694</pub-id><pub-id pub-id-type="pmid">25494501</pub-id>
</element-citation><mixed-citation id="mc-CR109" publication-type="journal">Menze, B. H. et al. The multimodal brain tumor image segmentation benchmark (BRATS). <italic>IEEE Trans. Med. Imaging</italic><bold>34</bold>, 1993. 10.1109/TMI.2014.2377694 (2015).<pub-id pub-id-type="pmid">25494501</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR110"><label>110.</label><citation-alternatives><element-citation id="ec-CR110" publication-type="journal"><person-group person-group-type="author"><name><surname>Bakas</surname><given-names>S</given-names></name><etal/></person-group><article-title>Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features</article-title><source>Nat. Sci. Data</source><year>2017</year><volume>4</volume><fpage>170117</fpage><pub-id pub-id-type="doi">10.1038/sdata.2017.117</pub-id></element-citation><mixed-citation id="mc-CR110" publication-type="journal">Bakas, S. et al. Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features. <italic>Nat. Sci. Data</italic><bold>4</bold>, 170117. 10.1038/sdata.2017.117 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR111"><label>111.</label><mixed-citation publication-type="other">Consortium, M. MONAI: Medical open network for AI (2023). <ext-link ext-link-type="uri" xlink:href="https://docs.monai.io/en/stable/auto3dseg.html">https://docs.monai.io/en/stable/auto3dseg.html</ext-link>.</mixed-citation></ref><ref id="CR112"><label>112.</label><mixed-citation publication-type="other">Cohen, J.&#x000a0;P., Luck, M. &#x00026; Honari, S. Distribution matching losses can hallucinate features in medical image translation. In <italic>Medical Image Computing and Computer Assisted Intervention&#x02014;MICCAI 2018</italic> (eds Frangi, A.&#x000a0;F., Schnabel, J.&#x000a0;A., Davatzikos, C., Alberola-L&#x000f3;pez, C. &#x00026; Fichtinger, G.) 529&#x02013;536 (Springer, 2018).</mixed-citation></ref><ref id="CR113"><label>113.</label><mixed-citation publication-type="other">Liu, J. <italic>et al.</italic> Dyefreenet: Deep virtual contrast CT synthesis. In <italic>Simulation and Synthesis in Medical Imaging</italic> (eds Burgos, N., Svoboda, D., Wolterink, J.&#x000a0;M. &#x00026; Zhao, C.) 80&#x02013;89 (Springer, 2020).</mixed-citation></ref><ref id="CR114"><label>114.</label><mixed-citation publication-type="other">Dohmen, M., Truong, T., Baltruschat, I.&#x000a0;M. &#x00026; Lenga, M. Five pitfalls when assessing synthetic medical images with reference metrics (2024), accepted at Deep Generative Models workshop at MICCAI 2024; <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2408.06075">arXiv:2408.06075</ext-link>.</mixed-citation></ref><ref id="CR115"><label>115.</label><citation-alternatives><element-citation id="ec-CR115" publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>NJ</given-names></name><etal/></person-group><article-title>N4ITK: Improved N3 bias correction</article-title><source>IEEE Trans. Med. Imaging</source><year>2010</year><volume>29</volume><fpage>1310</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id><pub-id pub-id-type="pmid">20378467</pub-id>
</element-citation><mixed-citation id="mc-CR115" publication-type="journal">Tustison, N. J. et al. N4ITK: Improved N3 bias correction. <italic>IEEE Trans. Med. Imaging</italic><bold>29</bold>, 1310&#x02013;20 (2010).<pub-id pub-id-type="pmid">20378467</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR116"><label>116.</label><citation-alternatives><element-citation id="ec-CR116" publication-type="journal"><person-group person-group-type="author"><name><surname>Axel</surname><given-names>L</given-names></name><name><surname>Summers</surname><given-names>RM</given-names></name><name><surname>Kressel</surname><given-names>HY</given-names></name><name><surname>Charles</surname><given-names>C</given-names></name></person-group><article-title>Respiratory effects in two-dimensional Fourier transform MR imaging</article-title><source>Radiology</source><year>1986</year><volume>160</volume><fpage>795</fpage><lpage>801</lpage><pub-id pub-id-type="doi">10.1148/radiology.160.3.3737920</pub-id><pub-id pub-id-type="pmid">3737920</pub-id>
</element-citation><mixed-citation id="mc-CR116" publication-type="journal">Axel, L., Summers, R. M., Kressel, H. Y. &#x00026; Charles, C. Respiratory effects in two-dimensional Fourier transform MR imaging. <italic>Radiology</italic><bold>160</bold>, 795&#x02013;801 (1986).<pub-id pub-id-type="pmid">3737920</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR117"><label>117.</label><citation-alternatives><element-citation id="ec-CR117" publication-type="journal"><person-group person-group-type="author"><name><surname>Stadler</surname><given-names>A</given-names></name><name><surname>Schima</surname><given-names>W</given-names></name><name><surname>Ba&#x02019;ssalamah</surname><given-names>A</given-names></name><name><surname>Kettenbach</surname><given-names>J</given-names></name><name><surname>Eisenhuber</surname><given-names>E</given-names></name></person-group><article-title>Artifacts in body MR imaging: Their appearance and how to eliminate them</article-title><source>Eur. Radiol.</source><year>2007</year><volume>17</volume><fpage>1242</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1007/s00330-006-0470-4</pub-id><pub-id pub-id-type="pmid">17149625</pub-id>
</element-citation><mixed-citation id="mc-CR117" publication-type="journal">Stadler, A., Schima, W., Ba&#x02019;ssalamah, A., Kettenbach, J. &#x00026; Eisenhuber, E. Artifacts in body MR imaging: Their appearance and how to eliminate them. <italic>Eur. Radiol.</italic><bold>17</bold>, 1242&#x02013;55. 10.1007/s00330-006-0470-4 (2007).<pub-id pub-id-type="pmid">17149625</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR118"><label>118.</label><citation-alternatives><element-citation id="ec-CR118" publication-type="journal"><person-group person-group-type="author"><name><surname>Van Leemput</surname><given-names>K</given-names></name><name><surname>Maes</surname><given-names>F</given-names></name><name><surname>Vandermeulen</surname><given-names>D</given-names></name><name><surname>Suetens</surname><given-names>P</given-names></name></person-group><article-title>Automated model-based bias field correction of MR images of the brain</article-title><source>IEEE Trans. Med. Imaging</source><year>1999</year><volume>18</volume><fpage>885</fpage><lpage>896</lpage><pub-id pub-id-type="doi">10.1109/42.811268</pub-id><pub-id pub-id-type="pmid">10628948</pub-id>
</element-citation><mixed-citation id="mc-CR118" publication-type="journal">Van Leemput, K., Maes, F., Vandermeulen, D. &#x00026; Suetens, P. Automated model-based bias field correction of MR images of the brain. <italic>IEEE Trans. Med. Imaging</italic><bold>18</bold>, 885&#x02013;896. 10.1109/42.811268 (1999).<pub-id pub-id-type="pmid">10628948</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR119"><label>119.</label><mixed-citation publication-type="other">Prashnani, E., Cai, H., Mostofi, Y. &#x00026; Sen, P. PieAPP: Perceptual image-error assessment through pairwise preference. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (2018).</mixed-citation></ref><ref id="CR120"><label>120.</label><mixed-citation publication-type="other">Zhu, H., Li, L., Wu, J., Dong, W. &#x00026; Shi, G. MetaIQA: Deep meta-learning for no-reference image quality assessment. In <italic>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 14131&#x02013;14140 (2020). 10.1109/CVPR42600.2020.01415.</mixed-citation></ref><ref id="CR121"><label>121.</label><mixed-citation publication-type="other">Ying, Z. <italic>et al.</italic> From patches to pictures (PaQ-2-PiQ): Mapping the perceptual space of picture quality. In <italic>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 3572&#x02013;3582 (2020). 10.1109/CVPR42600.2020.00363.</mixed-citation></ref><ref id="CR122"><label>122.</label><mixed-citation publication-type="other">Su, S. et al. Blindly assess image quality in the wild guided by a self-adaptive hyper network. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (2020).</mixed-citation></ref><ref id="CR123"><label>123.</label><mixed-citation publication-type="other">Lao, S. et al. Attentions help CNNs see better: Attention-based hybrid image quality assessment network. In <italic>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</italic> 1139&#x02013;1148 (2022). 10.1109/CVPRW56347.2022.00123.</mixed-citation></ref><ref id="CR124"><label>124.</label><citation-alternatives><element-citation id="ec-CR124" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Mou</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>D</given-names></name></person-group><article-title>FSIM: A feature similarity index for image quality assessment</article-title><source>IEEE Trans. Image Process.</source><year>2011</year><volume>20</volume><fpage>2378</fpage><lpage>2386</lpage><pub-id pub-id-type="doi">10.1109/TIP.2011.2109730</pub-id><pub-id pub-id-type="pmid">21292594</pub-id>
</element-citation><mixed-citation id="mc-CR124" publication-type="journal">Zhang, L., Zhang, L., Mou, X. &#x00026; Zhang, D. FSIM: A feature similarity index for image quality assessment. <italic>IEEE Trans. Image Process.</italic><bold>20</bold>, 2378&#x02013;2386. 10.1109/TIP.2011.2109730 (2011).<pub-id pub-id-type="pmid">21292594</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR125"><label>125.</label><citation-alternatives><element-citation id="ec-CR125" publication-type="journal"><person-group person-group-type="author"><name><surname>Sheikh</surname><given-names>H</given-names></name><name><surname>Bovik</surname><given-names>A</given-names></name></person-group><article-title>Image information and visual quality</article-title><source>IEEE Trans. Image Process.</source><year>2006</year><volume>15</volume><fpage>430</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1109/TIP.2005.859378</pub-id><pub-id pub-id-type="pmid">16479813</pub-id>
</element-citation><mixed-citation id="mc-CR125" publication-type="journal">Sheikh, H. &#x00026; Bovik, A. Image information and visual quality. <italic>IEEE Trans. Image Process.</italic><bold>15</bold>, 430&#x02013;444. 10.1109/TIP.2005.859378 (2006).<pub-id pub-id-type="pmid">16479813</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR126"><label>126.</label><citation-alternatives><element-citation id="ec-CR126" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name></person-group><article-title>Information content weighting for perceptual image quality assessment</article-title><source>IEEE Trans. Image Process.</source><year>2011</year><volume>20</volume><fpage>1185</fpage><lpage>1198</lpage><pub-id pub-id-type="doi">10.1109/TIP.2010.2092435</pub-id><pub-id pub-id-type="pmid">21078577</pub-id>
</element-citation><mixed-citation id="mc-CR126" publication-type="journal">Wang, Z. &#x00026; Li, Q. Information content weighting for perceptual image quality assessment. <italic>IEEE Trans. Image Process.</italic><bold>20</bold>, 1185&#x02013;1198. 10.1109/TIP.2010.2092435 (2011).<pub-id pub-id-type="pmid">21078577</pub-id>
</mixed-citation></citation-alternatives></ref></ref-list></back></article>