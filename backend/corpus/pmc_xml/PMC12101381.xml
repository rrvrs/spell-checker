<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Vision (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Vision (Basel)</journal-id><journal-id journal-id-type="publisher-id">vision</journal-id><journal-title-group><journal-title>Vision</journal-title></journal-title-group><issn pub-type="epub">2411-5150</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40407621</article-id><article-id pub-id-type="pmc">PMC12101381</article-id>
<article-id pub-id-type="doi">10.3390/vision9020039</article-id><article-id pub-id-type="publisher-id">vision-09-00039</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Training Improves Avoidance of Natural Sick Faces: Changes in Visual Attention and Approach Decisions</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6719-4370</contrib-id><name><surname>Leung</surname><given-names>Tiffany S.</given-names></name><xref rid="af1-vision-09-00039" ref-type="aff">1</xref><xref rid="c1-vision-09-00039" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Jakobsen</surname><given-names>Krisztina V.</given-names></name><xref rid="af2-vision-09-00039" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7388-8640</contrib-id><name><surname>Maylott</surname><given-names>Sarah E.</given-names></name><xref rid="af3-vision-09-00039" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Malik</surname><given-names>Arushi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-vision-09-00039" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Shuo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-vision-09-00039" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2715-2533</contrib-id><name><surname>Simpson</surname><given-names>Elizabeth A.</given-names></name><xref rid="af1-vision-09-00039" ref-type="aff">1</xref></contrib></contrib-group><aff id="af1-vision-09-00039"><label>1</label>Department of Psychology, University of Miami, Coral Gables, FL 33124, USA</aff><aff id="af2-vision-09-00039"><label>2</label>Department of Psychology, James Madison University, Harrisonburg, VA 22801, USA</aff><aff id="af3-vision-09-00039"><label>3</label>Department of Psychiatry and Behavioral Sciences, Duke University, Durham, NC 27708, USA</aff><author-notes><corresp id="c1-vision-09-00039"><label>*</label>Correspondence: <email>tsl51@miami.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>6</month><year>2025</year></pub-date><volume>9</volume><issue>2</issue><elocation-id>39</elocation-id><history><date date-type="received"><day>26</day><month>2</month><year>2025</year></date><date date-type="rev-recd"><day>18</day><month>4</month><year>2025</year></date><date date-type="accepted"><day>24</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Humans evolved a behavioral immune system to avoid infectious disease, including the ability to detect sickness in faces. However, it is unclear whether the ability to recognize and avoid facial cues of disease is malleable, flexibly calibrated by experience. Thus, we experimentally tested whether we can improve adults&#x02019; (<italic toggle="yes">N</italic> = 133) lassitude (sick) face perception, measuring their recognition, avoidance, and visual attention to naturally sick and healthy faces. Participants randomly assigned to a training about disease, but not a control group, were better at avoiding sick people. The disease-trained group also looked more equally between sick and healthy faces when identifying who was sick compared to the control group who looked longer at the sick faces than the healthy faces. Though we detected no group differences in time looking at the eyes and at the mouths, the disease-trained group used these features more to decide who was sick, reflecting key features of the lassitude expression. Our findings suggest that facial sickness perception may be flexible, influenced by experience, and underscore the need for future studies to test how to further strengthen this skill. Ultimately, developing interventions that use this sick face plasticity may reduce disease transmission.</p></abstract><kwd-group><kwd>visual attention</kwd><kwd>behavioral immune system</kwd><kwd>pathogen avoidance</kwd><kwd>sick face perception</kwd></kwd-group><funding-group><award-group><funding-source>University of Miami College of Arts and Sciences Academic Year Dissertation Award</funding-source></award-group><award-group><funding-source>American Psychological Foundation Elizabeth Munsterberg Koppitz Child Psychology Graduate Student Fellowship</funding-source></award-group><award-group><funding-source>Alvin V., Jr. and Nancy C. Baird Professorship</funding-source></award-group><award-group><funding-source>NSF CAREER Award</funding-source><award-id>1653737</award-id></award-group><funding-statement>This research was funded by the University of Miami College of Arts and Sciences Academic Year Dissertation Award (T.S.L.), American Psychological Foundation Elizabeth Munsterberg Koppitz Child Psychology Graduate Student Fellowship (T.S.L.), Alvin V., Jr. and Nancy C. Baird Professorship (K.V.J.), and NSF CAREER Award 1653737 (E.A.S.).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-vision-09-00039"><title>1. Introduction</title><p>Humans evolved a behavioral immune system that facilitates rapid detection, interpretation, and avoidance of communicable diseases [<xref rid="B1-vision-09-00039" ref-type="bibr">1</xref>]. Part of this system is disease detection, which is theorized to continually calibrate (i.e., flexibly adjust) as people encounter signals of disease [<xref rid="B2-vision-09-00039" ref-type="bibr">2</xref>,<xref rid="B3-vision-09-00039" ref-type="bibr">3</xref>]. A malleable disease detection system with continual calibration would allow individuals to balance avoiding socially transmitted pathogens (risks that fluctuate over time) with the need for engaging in beneficial social interactions.</p><p>There is empirical support for a flexible behavioral immune system. Pathogen avoidance may be improved following exposure to disease cues [<xref rid="B4-vision-09-00039" ref-type="bibr">4</xref>,<xref rid="B5-vision-09-00039" ref-type="bibr">5</xref>,<xref rid="B6-vision-09-00039" ref-type="bibr">6</xref>,<xref rid="B7-vision-09-00039" ref-type="bibr">7</xref>,<xref rid="B8-vision-09-00039" ref-type="bibr">8</xref>,<xref rid="B9-vision-09-00039" ref-type="bibr">9</xref>,<xref rid="B10-vision-09-00039" ref-type="bibr">10</xref>,<xref rid="B11-vision-09-00039" ref-type="bibr">11</xref>]. For example, people distance themselves from those who are coughing [<xref rid="B12-vision-09-00039" ref-type="bibr">12</xref>] and the sound of sneezing increases people&#x02019;s perceived vulnerability to disease [<xref rid="B13-vision-09-00039" ref-type="bibr">13</xref>]. Further, people primed to think about pathogens&#x02014;either by viewing a video about infectious disease [<xref rid="B14-vision-09-00039" ref-type="bibr">14</xref>], viewing a slideshow of disease-related images (e.g., bacteria on household items) [<xref rid="B7-vision-09-00039" ref-type="bibr">7</xref>], or reading about disease transmission [<xref rid="B8-vision-09-00039" ref-type="bibr">8</xref>]&#x02014;display more disease vigilance (e.g., avoidance of people with chronic illness).</p><sec id="sec1dot1-vision-09-00039"><title>1.1. Sensitivity to Facial Cues of Sickness</title><p>One type of disease vigilance that may be flexible is sensitivity to health-related facial cues. For example, watching a video about the dangers of infectious disease increases people&#x02019;s accuracy in recognizing facial cues of chronic illness [<xref rid="B14-vision-09-00039" ref-type="bibr">14</xref>]. Further, people who read a disease-related story containing disgusting events (e.g., being sneezed on) or view a slideshow with disease-related cues (e.g., people with infected wounds), compared to those in a control condition, have an increased preference for facial qualities associated with healthiness (e.g., attractiveness) [<xref rid="B11-vision-09-00039" ref-type="bibr">11</xref>,<xref rid="B15-vision-09-00039" ref-type="bibr">15</xref>]. Together, these findings suggest that people detect sickness through facial cues and this sensitivity to facial health may be enhanced through priming.</p><p>However, it remains unknown whether people can be primed to recognize and avoid facial cues of <italic toggle="yes">contagious</italic> illness, which have important survival consequences. Humans may be especially attuned to visual cues of illness [<xref rid="B16-vision-09-00039" ref-type="bibr">16</xref>]. When people have a contagious illness they may exhibit <italic toggle="yes">lassitude</italic>, a negative emotion characterized by slack facial muscles, drooping eyelids, and slightly parted lips [<xref rid="B17-vision-09-00039" ref-type="bibr">17</xref>]. Indeed, even without priming, humans recognize and automatically avoid sick faces [<xref rid="B18-vision-09-00039" ref-type="bibr">18</xref>,<xref rid="B19-vision-09-00039" ref-type="bibr">19</xref>,<xref rid="B20-vision-09-00039" ref-type="bibr">20</xref>]. However, people&#x02019;s accuracy in recognizing and avoiding facial sickness is low (&#x0003c;70%) [<xref rid="B20-vision-09-00039" ref-type="bibr">20</xref>] and it is unclear whether this skill is malleable.</p></sec><sec id="sec1dot2-vision-09-00039"><title>1.2. Visual Attentional Biases to Sick Faces</title><p>Sick faces may receive more attention than other types of stimuli. Evolutionarily relevant social threats, such as angry faces, tend to hold attention more than non-threats [<xref rid="B21-vision-09-00039" ref-type="bibr">21</xref>,<xref rid="B22-vision-09-00039" ref-type="bibr">22</xref>]. In fact, in a categorization task (in which sickness was not task-relevant) event-related potentials reveal that faces edited to appear sick hold attention longer compared to unedited healthy faces [<xref rid="B23-vision-09-00039" ref-type="bibr">23</xref>]. Further, in a dot-probe paradigm, people are slower to shift their attention away from faces with disfigurements, which cue the behavioral immune system, compared to healthy faces, suggesting that faces perceived as unhealthy more strongly hold attention [<xref rid="B24-vision-09-00039" ref-type="bibr">24</xref>,<xref rid="B25-vision-09-00039" ref-type="bibr">25</xref>]. However, it is unclear whether naturally sick faces hold attention longer than healthy faces and whether this attention-holding is flexible.</p></sec><sec id="sec1dot3-vision-09-00039"><title>1.3. Current Study</title><p>While previous studies demonstrated that people can avoid pathogens using facial cues [<xref rid="B19-vision-09-00039" ref-type="bibr">19</xref>,<xref rid="B20-vision-09-00039" ref-type="bibr">20</xref>], the malleability of this skill&#x02014;whether sick face perception and its associated visual attention patterns can be enhanced through experiences&#x02014;is unclear. To improve public health, it may be beneficial to develop methods to support sick face perception, given that even small inaccuracies may have consequences: misjudging a healthy individual as sick may lead to missed social interactions, while failing to recognize sickness may increase the risk of disease transmission [<xref rid="B26-vision-09-00039" ref-type="bibr">26</xref>]. Thus, in the current study, we experimentally tested whether disease training can improve sick face recognition and avoidance while also tracking visual attention to uncover potential mechanisms. We randomly assigned participants to either a brief educational experimental manipulation designed to improve sick face recognition/avoidance (disease training group) or to a control group. We asked adults to identify who they would approach (to capture avoidance) and who they think is sick (recognition) using face photos of people with naturally occurring acute, contagious illnesses and face photos of the same people when healthy. We predicted that the disease training would improve the accuracy of sick face avoidance and recognition and make participants faster at avoiding sick people. We also predicted that sick faces, particularly the eye and mouth regions, would hold visual attention longer than healthy faces, especially for the disease training group. Finally, we predicted the disease training group would be more likely than the control group to report using the eye and mouth regions to identify sickness.</p></sec></sec><sec id="sec2-vision-09-00039"><title>2. Materials and Methods</title><sec sec-type="subjects" id="sec2dot1-vision-09-00039"><title>2.1. Participants</title><p>We recruited adults (<italic toggle="yes">N</italic> = 133) from an undergraduate student research participant pool at the University of Miami (see <xref rid="vision-09-00039-t001" ref-type="table">Table 1</xref> for demographic details). An a priori power analysis in G*Power [<xref rid="B27-vision-09-00039" ref-type="bibr">27</xref>] determined that a sample size of 124 participants would provide 80% power to detect small&#x02013;moderate effect sizes. The participants received course credit for their participation. The University of Miami Institutional Review Board approved this study.</p></sec><sec id="sec2dot2-vision-09-00039"><title>2.2. Materials</title><sec id="sec2dot2dot1-vision-09-00039"><title>2.2.1. Face Stimuli</title><p>We included 32 photos from 16 donors of diverse races (5 Asian, 2 Black or African American, 6 White, 1 American Indian/Alaska Native and White, and 2 Black or African American and White), ethnicities (6 Hispanic or Latino and 10 non-Hispanic or Latino), genders (6 men, 2 boys, 7 women, and 1 nonbinary adult), and ages (<italic toggle="yes">M</italic> = 29 years old, <italic toggle="yes">SD</italic> = 17, range: 8&#x02013;79 years). Each donor contributed two photos: one sick and one healthy (<xref rid="vision-09-00039-f001" ref-type="fig">Figure 1</xref>, <xref rid="app1-vision-09-00039" ref-type="app">Table S1</xref>). All the photos were validated on perceived health in separate samples [<xref rid="B19-vision-09-00039" ref-type="bibr">19</xref>,<xref rid="B20-vision-09-00039" ref-type="bibr">20</xref>]. At the time of the sick photo, the donors reported having a contagious disease, including COVID-19, streptococcal pharyngitis, rhinovirus, and influenza. A within-subjects comparison of sick and healthy faces controlled for between-subjects variables (e.g., facial symmetry or age). The photo donors maintained a neutral facial expression and were instructed to relax their faces (i.e., reduce tension in their facial muscles).</p></sec><sec id="sec2dot2dot2-vision-09-00039"><title>2.2.2. Avoidance Task Materials</title><p>The faces were displayed side-by-side with one image of the donor&#x02019;s face when they were sick and a second image of that same donor&#x02019;s face when they were healthy. We horizontally and vertically centered each face on its respective left or right side of the image. The faces were displayed on a Dell P2214H Monitor and were sized 4.30&#x02013;5.20 &#x000d7; 5.90&#x02013;8.10 cm.</p></sec><sec id="sec2dot2dot3-vision-09-00039"><title>2.2.3. Recognition Task Materials</title><p>We again presented the side-by-side face pairs (described above), slightly larger to fit the screen: each face was 299&#x02013;360 pixels wide (<italic toggle="yes">M</italic> = 327.56, <italic toggle="yes">SD</italic> = 19.67) &#x000d7; 426&#x02013;581 pixels tall (<italic toggle="yes">M</italic> = 495.06, <italic toggle="yes">SD</italic> = 39.29, which is 7.91&#x02013;9.52 &#x000d7; 11.27&#x02013;15.37 cm and 7.54&#x02013;9.07 &#x000d7; 10.73&#x02013;14.60&#x000b0;). The faces were spatially separated by 282&#x02013;340 pixels (<italic toggle="yes">M</italic> = 310.38, <italic toggle="yes">SD</italic> = 19.53), which is 7.46&#x02013;9.00 cm and 7.11&#x02013;8.58&#x000b0;. A circle or a square was below each face, allowing the participants to say the shape that corresponded to the face they wanted to select.</p><p>We used a Tobii TX300 eye-tracker (Tobii Technology, Danderyd, Sweden), with a remote 58.4 cm monitor (51 cm in width &#x000d7; 28 cm in height) with integrated dark pupil eye-tracking technology, with a resolution of 1280 &#x000d7; 720 pixels, and a sampling rate of 300 hertz. The test room had no windows and a constant illumination of approximately 202 lux. We used the Tobii Studio software version 2.0 (Tobii Technology, Danderyd, Sweden) to collect and summarize the eye-tracking data.</p></sec></sec><sec id="sec2dot3-vision-09-00039"><title>2.3. Procedure</title><p>The participants were tested in a controlled laboratory setting. Following informed consent, the participants completed demographic questions (e.g., age and race).</p><sec id="sec2dot3dot1-vision-09-00039"><title>2.3.1. Sick Face Avoidance Pre-Manipulation Baseline</title><p>To verify that the participants did not differ in their sick face perception prior to the experimental manipulation, we collected a baseline measure of sickness avoidance (<xref rid="vision-09-00039-f002" ref-type="fig">Figure 2</xref>A). We chose this task that did not explicitly mention sickness to ensure that we would not unintentionally prime participants (the control group, in particular) to think about sickness. The participants were seated in front of a computer screen on which they viewed side-by-side pairs of sick and healthy faces that were described as &#x0201c;twins&#x0201d; to increase the credibility of the scenario, though they were in fact from the same person [<xref rid="B20-vision-09-00039" ref-type="bibr">20</xref>]. The participants were instructed to imagine they are in a restaurant where there are only two seats available. As each available seat is next to a twin, they must decide which twin they would prefer to sit next to and share dinner with. The participants completed 2 practice trials with cartoon characters. After that, the participants viewed the experimental trials, each consisting of two human faces side-by-side. The experimenter asked, &#x0201c;Who would you rather share dinner with?&#x0201d; and the participants responded with a mouse click. The participants completed 16 trials randomized using a Latin Square Design. The side of the sick face was counterbalanced to ensure it was equally often on the left and the right. Each face pair remained on the screen until the participant made a choice.</p></sec><sec id="sec2dot3dot2-vision-09-00039"><title>2.3.2. Disease Training Experimental Manipulation</title><p>We included a three-part experimental manipulation&#x02014;a story (part 1), a video (part 2), and a training session (part 3)&#x02014;based on previous studies that had success with providing information about disease transmission and videos related to infectious disease [<xref rid="B8-vision-09-00039" ref-type="bibr">8</xref>,<xref rid="B14-vision-09-00039" ref-type="bibr">14</xref>]. After random assignment to either the disease training condition or the control condition, the participants engaged in an interactive 3-minute story with an experimenter (experimental manipulation part 1; <xref rid="vision-09-00039-f002" ref-type="fig">Figure 2</xref>B). For the disease training condition, the story described facial cues of sickness and the importance of disease prevention behaviors (e.g., maintaining a safe distance from sick people) and included images of sick and healthy faces, generated by Midjourney (Version 5.2) artificial intelligence, to illustrate how disease spreads and how temporary social distancing can reduce illness transmission. The participants in the control condition heard a story about animals while viewing Midjourney-generated images of characters and animals. The control condition story was unrelated to disease to prevent priming the participants. The stories were matched on length, complexity, and interactive components and featured the same characters. Both stories were read aloud by the experimenter and included audio clips (i.e., a person coughing in the disease training and a parrot in the control condition) and open-ended questions (e.g., &#x0201c;What would you think if you heard that sound inside this room?&#x0201d;). We verified that the participants were attentive by asking them to recall where the story took place.</p><p>Following the story, the participants in the disease training condition watched an animated 3-minute video (experimental manipulation part 2; <xref rid="vision-09-00039-f002" ref-type="fig">Figure 2</xref>C) about infectious diseases (e.g., influenza) that included descriptions of virus transmission, immune system responses, and symptoms [<xref rid="B28-vision-09-00039" ref-type="bibr">28</xref>]. The participants in the control condition watched a 3-minute video about birds that was unrelated to disease [<xref rid="B29-vision-09-00039" ref-type="bibr">29</xref>]. The two videos were matched in length and featured the same adult woman presenter, who did not appear elsewhere in the study.</p><p>In the final portion of the experimental manipulation, the participants completed a training session (experimental manipulation part 3; <xref rid="vision-09-00039-f002" ref-type="fig">Figure 2</xref>D): in the disease training condition, the participants viewed examples of sick and healthy faces and were told, &#x0201c;When someone is feeling sick and their body is fighting off germs, their face may look different. Faces can give us clues about how someone is feeling, which helps us figure out if they are sick or healthy&#x0201d;. The participants were asked to note some of the commonalities among the sick faces. The experimenter then confirmed the participants&#x02019; observations and, if necessary, provided further details about facial features common in sick faces paired with photo examples. The experimenter explained that people tend to have drooping eyes [<xref rid="B18-vision-09-00039" ref-type="bibr">18</xref>] and relaxed facial muscles [<xref rid="B17-vision-09-00039" ref-type="bibr">17</xref>,<xref rid="B30-vision-09-00039" ref-type="bibr">30</xref>] when they are sick. The participants then completed 7 training trials of sickness recognition with feedback on their choices. If the participant chose the incorrect face, they were reminded of facial features associated with sickness (e.g., drooping eyes). The participants in the control condition completed a parallel bird recognition training that focused on the color and body features of cockatiels and parakeets. We asked the participants to identify features of cockatiels, and explained, &#x0201c;When you see a cockatiel, you might notice that the hair sticks up. This is called a crest, which sometimes sticks up when it&#x02019;s feeling excited or happy&#x0201d;. The participants then completed 7 training trials of bird recognition with feedback on their choices. In total, the experimental manipulation lasted approximately 8 min.</p></sec><sec id="sec2dot3dot3-vision-09-00039"><title>2.3.3. Sick Face Avoidance Post-Manipulation</title><p>Following the experimental manipulation, the participants repeated the pre-manipulation avoidance task (<xref rid="vision-09-00039-f002" ref-type="fig">Figure 2</xref>E). The tasks were identical prior to the manipulation and following the manipulation.</p></sec><sec id="sec2dot3dot4-vision-09-00039"><title>2.3.4. Sick Face Recognition (Eye Tracking)</title><p>Within the same testing room, the participants moved to the eye-tracking screen and were seated approximately 60 cm from the screen. Following a 9-point calibration, we tracked the participants&#x02019; gaze while they viewed faces. We asked the participants to imagine that they are doctors working in a hospital with the goal of identifying which individual in a pair of twins is sick (<xref rid="vision-09-00039-f002" ref-type="fig">Figure 2</xref>F). As with the avoidance tasks, the face pairs were presented side-by-side and were described as twins but were in fact from the same donor. The participants completed 2 practice trials with cartoon characters and then viewed 16 test trials consisting of side-by-side sick-healthy face pairs. The experimenter asked, &#x0201c;Which twin do you think is sick?&#x0201d; and the participants verbally reported their choices by saying the shape that corresponded to the face they wanted to choose (i.e., circle or square). The experimenter, blind to which faces were sick and healthy, recorded the participants&#x02019; responses to move to the next trial. The side on which the sick face appeared was counterbalanced to ensure it was equally often on the left and the right, and the trials were randomized using a Latin Square Design. After the task, we asked the participants, &#x0201c;How did you determine which one of the faces was sick?&#x0201d;</p></sec></sec><sec id="sec2dot4-vision-09-00039"><title>2.4. Measures</title><sec id="sec2dot4dot1-vision-09-00039"><title>2.4.1. Sickness Avoidance and Recognition Accuracy Scores</title><p>We calculated the number of correct trials in the pre-manipulation baseline avoidance task, post-manipulation avoidance task, and recognition task.</p></sec><sec id="sec2dot4dot2-vision-09-00039"><title>2.4.2. Sickness Avoidance Speed&#x02014;Manual Response Latency</title><p>For the avoidance task, we extracted the duration of time from when the trial first appeared to when the participant made their decision about which face to approach using a mouse click.</p></sec><sec id="sec2dot4dot3-vision-09-00039"><title>2.4.3. Visual Attention Holding to Sickness&#x02014;Look Duration Difference Scores</title><p>Using look duration as a measure of visual attention allows for the capture of subtle and covert sick face processing skills, which may be more robust compared to other behavioral measures, such as self-report responses that primarily capture overt and conscious awareness of disease cues [<xref rid="B19-vision-09-00039" ref-type="bibr">19</xref>]. For the recognition task, we calculated a mean look duration difference score for each participant by subtracting the total time looking at healthy face AOIs from the total time looking at sick face AOIs, capturing differences in attention holding [<xref rid="B19-vision-09-00039" ref-type="bibr">19</xref>]. Thus, positive difference scores in the recognition task indicated that the participants looked longer at the sick faces than the healthy faces, suggesting that sick faces hold attention, and negative difference scores indicated longer looking at the healthy faces than the sick faces, suggesting that healthy faces hold attention. We also calculated a difference score for looking at the eye regions, subtracting the total time looking at the healthy face eye AOIs from the total time looking at the sick face eye AOIs. Similarly, we calculated a difference score for looking at the mouth region AOIs, subtracting the time looking at the healthy face mouth from the time looking at the sick face mouth AOIs. We also did this for the remaining face regions (e.g., nose and cheeks): we calculated a difference score for looking at the non-eye&#x02013;mouth regions (face AOI minus the eye and mouth AOIs), subtracting the time looking at the healthy face non-eye&#x02013;mouth regions from the time looking at the sick face non-eye&#x02013;mouth regions.</p></sec><sec id="sec2dot4dot4-vision-09-00039"><title>2.4.4. Visual Comparison of Sickness&#x02014;Number of Alternating Gaze Shifts</title><p>We extracted the number of visits to each face AOI, with each visit defined as one or more consecutive fixations within an AOI prior to a fixation outside the AOI. To index the number of times the participants looked back and forth between the sick and healthy faces, reflecting comparisons between the two images [<xref rid="B31-vision-09-00039" ref-type="bibr">31</xref>], we summed the number of visits to the sick face AOI and the number of visits to the healthy face AOI and subtracted one (to account for the fact that they had to make an initial look). We calculated similar alternating gaze scores for looking back and forth at the eye regions (eye AOIs) between the faces and at the mouth regions (mouth AOIs) between the faces.</p></sec><sec id="sec2dot4dot5-vision-09-00039"><title>2.4.5. Report of Facial Sickness Cues</title><p>We transcribed the participants&#x02019; verbal responses to the question, &#x0201c;How did you determine which one of the faces was sick?&#x0201d; We then searched for the most common themes reported and coded whether each participant mentioned them or not (yes/no).</p></sec></sec><sec id="sec2dot5-vision-09-00039"><title>2.5. Analytic Approach</title><p>All analyses were conducted in R and R Studio Version 2023.09.1.</p><sec id="sec2dot5dot1-vision-09-00039"><title>2.5.1. Preliminary Analyses</title><p>We conducted an independent samples <italic toggle="yes">t</italic> test to confirm that the participants in the disease training group did not differ from the control group in their baseline sickness avoidance accuracy (number of correct trials) prior to the experimental manipulation.</p><p>We conducted one-sample <italic toggle="yes">t</italic> tests to confirm that participants&#x02019; accuracy at avoiding and recognizing sick faces was above chance performance (0.50) overall, as previously reported in adults [<xref rid="B20-vision-09-00039" ref-type="bibr">20</xref>].</p></sec><sec id="sec2dot5dot2-vision-09-00039"><title>2.5.2. Primary Analysis 1 (Prediction 1) Sickness Avoidance: Accuracy and Speed Before and After Disease Training</title><p>We conducted two 2 (Condition: disease training, control) &#x000d7; 2 (Time: pre-manipulation, post-manipulation) mixed-design ANOVAs to compare accuracy (number of correct trials) and manual response latency in the avoidance task.</p></sec><sec id="sec2dot5dot3-vision-09-00039"><title>2.5.3. Primary Analysis 2 (Prediction 2): Sickness Recognition: Accuracy and Visual Attention</title><p>We also conducted independent samples <italic toggle="yes">t</italic> tests (or Welch&#x02019;s <italic toggle="yes">t</italic> tests, when group variances were significantly unequal) to compare the disease training group and the control group on three types of measures collected in the recognition task: (1) accuracy (number of correct trials), (2) look duration difference scores (i.e., sick minus healthy) to the face eyes, and mouth, and (3) the number of alternating gaze shifts to the face, eyes, and mouth.</p></sec><sec id="sec2dot5dot4-vision-09-00039"><title>2.5.4. Primary Analysis 3 (Prediction 3): Facial Sickness Cues</title><p>Finally, we conducted three logistic regressions to test whether the disease training group was more likely than the control group to report using the eye, mouth, and nose regions to identify the sick face.</p></sec></sec></sec><sec sec-type="results" id="sec3-vision-09-00039"><title>3. Results</title><sec id="sec3dot1-vision-09-00039"><title>3.1. Preliminary Results</title><sec id="sec3dot1dot1-vision-09-00039"><title>3.1.1. Data Inclusion</title><p>We excluded the participants who failed the attention check and removed a small number of cases across specific measures that were outliers (see <xref rid="app1-vision-09-00039" ref-type="app">Supplementary Materials</xref> for details).</p></sec><sec id="sec3dot1dot2-vision-09-00039"><title>3.1.2. Baseline (Pre-Experimental Manipulation) Check</title><p>We confirmed that participants in the disease training group (<italic toggle="yes">M</italic> = 10.44 correct trials [65% accuracy], <italic toggle="yes">SD</italic> = 2.01) did not differ from the control group (<italic toggle="yes">M</italic> = 10.39 correct trials [65% accuracy], <italic toggle="yes">SD</italic> = 1.43) in their baseline sickness avoidance accuracy (number of correct trials) in the pre-manipulation baseline, <italic toggle="yes">t</italic>(108.15) = 0.18, <italic toggle="yes">p</italic> = 0.861.</p></sec><sec id="sec3dot1dot3-vision-09-00039"><title>3.1.3. Experimental Manipulation Disease Training Check</title><p>We first checked the disease training group&#x02019;s accuracy in the seven training trials (experimental manipulation part 3), confirming that they performed above chance in their sickness recognition (<italic toggle="yes">M</italic> = 6.5 correct trials, <italic toggle="yes">SD</italic> = 0.59, range: 5&#x02013;7), <italic toggle="yes">t</italic>(65) = 41.43, <italic toggle="yes">p</italic> &#x0003c; 0.001, <italic toggle="yes">d</italic> = 4.93. Given that previous studies reported relatively low accuracy (below 70% accuracy) on this task [<xref rid="B20-vision-09-00039" ref-type="bibr">20</xref>], 93% accuracy in the current training task suggests that the disease training was effective.</p></sec><sec id="sec3dot1dot4-vision-09-00039"><title>3.1.4. Post-Experimental Manipulation Avoidance Replication Check</title><p>We replicated previous findings that, pooling all the participants across both groups, the participants were above chance (eight trials; 50% accuracy) at avoiding sick faces with a one-sample <italic toggle="yes">t</italic> test (<italic toggle="yes">M</italic> = 10.41 correct trials [65% accuracy], <italic toggle="yes">SD</italic> = 1.74, range: 6 to 15), <italic toggle="yes">t</italic>(122) = 252.83, <italic toggle="yes">p</italic> &#x0003c; 0.001, <italic toggle="yes">d</italic> = 1.39. We also replicated these findings within each group: the disease training group (<italic toggle="yes">M</italic> = 10.39 correct trials [65% accuracy], <italic toggle="yes">SD</italic> = 1.43, range: 6&#x02013;15), <italic toggle="yes">t</italic>(60) = 153.5, <italic toggle="yes">p</italic> &#x0003c; 0.001, <italic toggle="yes">d</italic> = 1.21 and the control group (<italic toggle="yes">M</italic> = 10.44 correct trials [65% accuracy], <italic toggle="yes">SD</italic> = 2.01, range: 6&#x02013;13), <italic toggle="yes">t</italic>(61) = 218.15, <italic toggle="yes">p</italic> &#x0003c; 0.001, <italic toggle="yes">d</italic> = 1.67, were both above chance in their avoidance of sick faces, consistent with a prior study using these tasks [<xref rid="B20-vision-09-00039" ref-type="bibr">20</xref>].</p></sec></sec><sec id="sec3dot2-vision-09-00039"><title>3.2. Sickness Avoidance: Accuracy and Speed Before and After Disease Training (Prediction 1)</title><sec id="sec3dot2dot1-vision-09-00039"><title>3.2.1. Sickness Avoidance Accuracy</title><p>Our ANOVA on sickness avoidance accuracy (number of correct trials) in the avoidance task revealed marginal main effects of condition, <italic toggle="yes">F</italic>(1, 120) = 3.08, <italic toggle="yes">p</italic> = 0.082, &#x003b7;<sup>2</sup>&#x0209a; = 0.02, in which the disease training group (<italic toggle="yes">M</italic> = 10.80, <italic toggle="yes">SD</italic> = 1.76) was marginally more accurate than the control group (<italic toggle="yes">M</italic> = 10.39, <italic toggle="yes">SD</italic> = 1.64). We also detected a marginal main effect of time, <italic toggle="yes">F</italic>(1, 120) = 3.41, <italic toggle="yes">p</italic> = 0.067, &#x003b7;<sup>2</sup>&#x0209a; = 0.03, in which sickness avoidance accuracy in the post-manipulation task (<italic toggle="yes">M</italic> = 10.77, <italic toggle="yes">SD</italic> = 1.68) was marginally greater than in the pre-manipulation task (<italic toggle="yes">M</italic> = 10.41, <italic toggle="yes">SD</italic> = 1.74), potentially because both groups were slightly improving with practice as the tasks progressed, even without feedback.</p><p>These main effects were qualified by a marginal interaction between condition and time, <italic toggle="yes">F</italic>(1, 120) = 3.47, <italic toggle="yes">p</italic> = 0.065, &#x003b7;<sup>2</sup>&#x0209a; = 0.03. As reported above, in the pre-manipulation avoidance task (baseline check), the disease training group (<italic toggle="yes">M</italic> = 10.44 correct trials [65% accuracy], <italic toggle="yes">SD</italic> = 2.01) did not differ from the control group (<italic toggle="yes">M</italic> = 10.39 correct trials [65% accuracy], <italic toggle="yes">SD</italic> = 1.43), <italic toggle="yes">t</italic>(108.15) = 0.18, <italic toggle="yes">p</italic> = 0.861. However, as predicted, in the post-manipulation avoidance task, the disease training group (<italic toggle="yes">M</italic> = 11.16 correct trials [70% accuracy], <italic toggle="yes">SD</italic> = 1.39) was more accurate (greater number of correct trials) than the control condition group (<italic toggle="yes">M</italic> = 10.38 correct trials [65% accuracy], <italic toggle="yes">SD</italic> = 1.85), <italic toggle="yes">t</italic>(113.3) = 2.63, <italic toggle="yes">p</italic> = 0.010, <italic toggle="yes">d</italic> = 0.48. As predicted, the disease training group improved from before to after the manipulation (from 10.44 trials correct [65% accuracy] to 11.16 trials correct [70% accuracy]), <italic toggle="yes">t</italic>(60) = 2.68, <italic toggle="yes">p</italic> = 0.010, <italic toggle="yes">d</italic> = 1.16, while the control group showed no changes (pre-manipulation: <italic toggle="yes">M</italic> = 10.40, <italic toggle="yes">SD</italic> = 1.43; post-manipulation: <italic toggle="yes">M</italic> = 10.39, <italic toggle="yes">SD</italic> = 1.85), <italic toggle="yes">t</italic>(61) &#x0003c; 0.01, <italic toggle="yes">p</italic> &#x0003e; 0.99, (<xref rid="vision-09-00039-f003" ref-type="fig">Figure 3</xref>).</p></sec><sec id="sec3dot2dot2-vision-09-00039"><title>3.2.2. Sickness Avoidance Speed</title><p>Our ANOVA on sickness avoidance manual response latency revealed a main effect of condition, <italic toggle="yes">F</italic>(1, 126) = 25.83, <italic toggle="yes">p</italic> = 0.004, &#x003b7;<sup>2</sup>&#x0209a; = 0.06, in which the disease training group (<italic toggle="yes">M</italic> = 3.82, <italic toggle="yes">SD</italic> = 1.51) responded more slowly, taking more time to make their decisions, than the control group (<italic toggle="yes">M</italic> = 1.51, <italic toggle="yes">SD</italic> = 1.22). We also found a main effect of time, in which the participants responded more slowly in the pre-manipulation task (<italic toggle="yes">M</italic> = 3.73, <italic toggle="yes">SD</italic> = 1.38) than in the post-manipulation task (<italic toggle="yes">M</italic> = 3.28, <italic toggle="yes">SD</italic> = 1.41), <italic toggle="yes">F</italic>(1, 125) = 13.40, <italic toggle="yes">p</italic> &#x0003c; 0.001, <italic toggle="yes">d</italic> = 0.18, even without feedback. The participants, overall, grew slower and more accurate as the study progressed, possibly because simply by performing the avoidance task, participants&#x02019; face perception skills were improving.</p><p>These main effects were qualified by an interaction between condition and time, <italic toggle="yes">F</italic>(1, 125) = 18.52, <italic toggle="yes">p</italic> &#x0003c; 0.001, &#x003b7;<sup>2</sup>&#x0209a; = 0.23, <xref rid="vision-09-00039-f004" ref-type="fig">Figure 4</xref>. In the pre-manipulation avoidance task, we detected no difference in manual response latency between the disease training group (<italic toggle="yes">M</italic> = 3.78, <italic toggle="yes">SD</italic> = 1.42) and the control group (<italic toggle="yes">M</italic> = 3.67, <italic toggle="yes">SD</italic> = 1.35), <italic toggle="yes">t</italic>(127) = 0.43, <italic toggle="yes">p</italic> = 0.669, potentially because both groups carefully examined both faces before making their decision. However, in the post-manipulation avoidance task, the disease training group (<italic toggle="yes">M</italic> = 3.85, <italic toggle="yes">SD</italic> = 1.60) was slower than the control condition group (<italic toggle="yes">M</italic> = 2.69, <italic toggle="yes">SD</italic> = 0.85), <italic toggle="yes">t</italic>(127) = 5.13, <italic toggle="yes">p</italic> &#x0003c; 0.001, <italic toggle="yes">d</italic> = 0.90. This finding may indicate a speed&#x02013;accuracy trade-off in which the disease training group was slower to respond but more accurate in their responses compared to the control group. The disease training group showed no changes in their sickness avoidance manual response latency from the pre- to post-manipulation task, <italic toggle="yes">t</italic>(64) = 0.56, <italic toggle="yes">p</italic> = 0.572, while the control group had slower responses in the pre- than the post-manipulation task, <italic toggle="yes">t</italic>(62) = 8.15, <italic toggle="yes">p</italic> &#x0003c; 0.001, <italic toggle="yes">d</italic> = 1.96 (<xref rid="vision-09-00039-f004" ref-type="fig">Figure 4</xref>). The disease training group may have taken more time to make their choices in the post-manipulation task, because they received new task-relevant information in the disease training, altering their interpretation of the task instructions (i.e., they used health cues even though not explicitly instructed to). In contrast, for the control group, the task remained the same, so they showed a practice effect.</p></sec></sec><sec id="sec3dot3-vision-09-00039"><title>3.3. Sickness Recognition: Accuracy, Visual Attention, and Report of Facial Sickness Cues (Prediction 2)</title><sec id="sec3dot3dot1-vision-09-00039"><title>3.3.1. Sickness Recognition Accuracy</title><p>We did not detect a statistically significant difference between the disease training group (<italic toggle="yes">M</italic> = 11.15, <italic toggle="yes">SD</italic> = 1.85) and the control condition group (<italic toggle="yes">M</italic> = 11.37, <italic toggle="yes">SD</italic> = 1.84) for the number of correct trials in the post-manipulation sickness recognition task, <italic toggle="yes">t</italic>(127) = 0.70, <italic toggle="yes">p</italic> = 0.485. These findings indicate that our disease training did not seem to improve post-manipulation explicit sickness recognition accuracy, though we did not have a pre-manipulation task for this measure.</p></sec><sec id="sec3dot3dot2-vision-09-00039"><title>3.3.2. Look Duration to Faces and Face Regions</title><p>However, the groups differed in their post-manipulation look duration at the faces (i.e., difference scores: number of seconds looking at the healthy face subtracted from the number of seconds looking at the sick face): The control group (<italic toggle="yes">M</italic> = 0.17, <italic toggle="yes">SD</italic> = 0.16) had larger difference scores than the disease training group (<italic toggle="yes">M</italic> = 0.10, <italic toggle="yes">SD</italic> = 0.17), <italic toggle="yes">t</italic>(133) = 2.40, <italic toggle="yes">p</italic> = 0.018, <italic toggle="yes">d</italic> = 0.46 (<xref rid="vision-09-00039-f005" ref-type="fig">Figure 5</xref>). This finding suggests that the disease training group looked more equally at sick and healthy faces (<italic toggle="yes">M</italic> = 1.25 s, <italic toggle="yes">SD</italic> = 0.73 s, and <italic toggle="yes">M</italic> = 1.17 s, <italic toggle="yes">SD</italic> = 0.79 s, respectively), while the control group looked more to the sick faces (<italic toggle="yes">M</italic> = 1.18 s, <italic toggle="yes">SD</italic> = 0.45 s) relative to the healthy faces (<italic toggle="yes">M</italic> = 1.02 s, <italic toggle="yes">SD</italic> = 0.39 s). These findings suggest that the disease training altered the participants&#x02019; relative viewing times to sick and healthy faces.</p><p>For the post-manipulation look duration difference scores for the participants&#x02019; attention to the eyes (look duration to healthy eyes subtracted from look duration to sick eyes), we detected no difference between the disease training group (<italic toggle="yes">M</italic> = 0.03, <italic toggle="yes">SD</italic> = 0.14) and the control group (<italic toggle="yes">M</italic> = 0.06, <italic toggle="yes">SD</italic> = 0.20), <italic toggle="yes">t</italic>(105.77) = 1.08, <italic toggle="yes">p</italic> = 0.281. However, while the control group&#x02019;s eye difference score was above chance, <italic toggle="yes">t</italic>(59) = 2.39, <italic toggle="yes">p</italic> = 0.020, <italic toggle="yes">d</italic> = 0.31, the disease training group&#x02019;s difference score was at chance, <italic toggle="yes">t</italic>(57) = 1.49, <italic toggle="yes">p</italic> = 0.142. These findings suggest that the control group was looking more at the sick face eyes than at the healthy face eyes, but the disease training group was looking more equally at the sick and healthy face eyes.</p><p>For the post-manipulation look duration difference scores for their attention to the mouth (look duration to healthy mouth subtracted from look duration to sick mouth), we detected no difference between the disease training group (<italic toggle="yes">M</italic> = 0.01, <italic toggle="yes">SD</italic> = 0.18) and the control group (<italic toggle="yes">M</italic> = 0.03, <italic toggle="yes">SD</italic> = 0.17), <italic toggle="yes">t</italic>(100) = 0.54, <italic toggle="yes">p</italic> = 0.591. Neither group&#x02019;s mouth difference score was above chance (disease training group: <italic toggle="yes">t</italic>(53) = 0.35, <italic toggle="yes">p</italic> = 0.731; control group: <italic toggle="yes">t</italic>(47) = 1.14, <italic toggle="yes">p</italic> = 0.261), indicating both groups looked for similar amounts of time to the sick and healthy face mouth regions.</p><p>For the look duration to the remaining face parts (face AOI minus eye and mouth region AOIs) in the post-manipulation sickness recognition task, the disease training group&#x02019;s difference scores did not differ from zero (<italic toggle="yes">M</italic> = 0.00, <italic toggle="yes">SD</italic> = 0.31, <italic toggle="yes">t</italic>(50) = 0.08, <italic toggle="yes">p</italic> = 0.937), indicating they looked about equally between the sick and healthy faces&#x02019; remaining face parts. The disease training group&#x02019;s difference scores were less than those of the control group (<italic toggle="yes">M</italic> = 0.12, <italic toggle="yes">SD</italic> = 0.36), <italic toggle="yes">t</italic>(89) = 1.77, <italic toggle="yes">p</italic> = 0.080, <italic toggle="yes">d</italic> = 0.37, who spent more time looking at the sick faces&#x02019; remaining parts relative to the healthy faces&#x02019;, and did so at rates above chance, <italic toggle="yes">t</italic>(39) = 2.15, <italic toggle="yes">p</italic> = 0.038, <italic toggle="yes">d</italic> = 0.34. These findings mirror the other effects of the disease training group looking more equally at the sick and healthy faces (entire face AOIs), eyes, and mouths, while the control group consistently appears to show greater looking to all parts of the sick faces relative to the healthy faces.</p></sec><sec id="sec3dot3dot3-vision-09-00039"><title>3.3.3. Gaze Alternations</title><p>Additionally, the disease training group (<italic toggle="yes">M</italic> = 4.19, <italic toggle="yes">SD</italic> = 1.71) did not differ from the control group (<italic toggle="yes">M</italic> = 3.89, <italic toggle="yes">SD</italic> = 1.11) in the number of alternating gaze shifts between sick and healthy faces, <italic toggle="yes">t</italic>(126) = 1.18, <italic toggle="yes">p</italic> = 0.241, <xref rid="vision-09-00039-f006" ref-type="fig">Figure 6</xref>. The disease training group (<italic toggle="yes">M</italic> = 3.14, <italic toggle="yes">SD</italic> = 1.17) also did not differ from the control group (<italic toggle="yes">M</italic> = 2.95, <italic toggle="yes">SD</italic> = 0.94) in the number of alternating gaze shifts between the sick and healthy faces&#x02019; eyes, <italic toggle="yes">t</italic>(114) = 0.94, <italic toggle="yes">p</italic> = 0.348. However, the disease training group (<italic toggle="yes">M</italic> = 2.15, <italic toggle="yes">SD</italic> = 0.91) had marginally more alternating gaze shifts between the sick and healthy faces&#x02019; mouths than the control group (<italic toggle="yes">M</italic> = 1.84, <italic toggle="yes">SD</italic> = 0.72), <italic toggle="yes">t</italic>(97) = 1.89, <italic toggle="yes">p</italic> = 0.062, <italic toggle="yes">d</italic> = 0.38. This finding is consistent with what we predicted, given that drooping corners of the mouth is a crucial piece of the lassitude expression (Schrock et al., 2020) [<xref rid="B17-vision-09-00039" ref-type="bibr">17</xref>].</p></sec><sec id="sec3dot3dot4-vision-09-00039"><title>3.3.4. Report of Facial Sickness Cues (Prediction 3)</title><p>Finally, a logistic regression revealed that the participants in the disease training group (92%) were more likely than those in the control group (66%) to report using the eyes to determine which person was sick, <italic toggle="yes">b</italic> = 1.81, <italic toggle="yes">SE</italic> = 0.53, <italic toggle="yes">z</italic> = 3.40, <italic toggle="yes">p</italic> &#x0003c; 0.001, 95% CI: 0.84, 2.97; the disease training group was 6.14 times more likely to say eyes compared to the control group. See <xref rid="app1-vision-09-00039" ref-type="app">Table S2</xref> for representative examples of participant responses. The participants in the disease training group (40%) were also more likely than those in the control group (18%) to report using the mouth or lips to determine which person was sick, <italic toggle="yes">b</italic> = 1.08, <italic toggle="yes">SE</italic> = 0.41, <italic toggle="yes">z</italic> = 2.65, <italic toggle="yes">p</italic> = 0.008, 95% CI: 0.30, 1.91. These results are in line with the visual attention differences between the disease training and control groups in their attention to these face regions. A limited number of the participants in the disease training group (0%) and the control group (5%) reported using the nose. See <xref rid="vision-09-00039-t002" ref-type="table">Table 2</xref> for a summary of the primary results.</p></sec></sec></sec><sec sec-type="discussion" id="sec4-vision-09-00039"><title>4. Discussion</title><p>Given that contagious diseases can spread through brief social interactions, it is important to understand how to support the behavioral immune system. Yet, it is unclear whether the sick face detection system is malleable. Building on previous studies reporting people differentiate between sick and healthy faces [<xref rid="B19-vision-09-00039" ref-type="bibr">19</xref>,<xref rid="B20-vision-09-00039" ref-type="bibr">20</xref>], the current study is the first, to our knowledge, to test whether disease training improves the avoidance and recognition of naturally sick faces with acontagious illness. The participants who were primed to think about pathogens and trained on facial cues of sickness were more accurate at avoiding sick people and displayed differing attentional patterns on faces when evaluating health compared to the control group. Our study reveals that there is potential for improvement in sickness perception for faces with confirmed contagious diseases, similar to previous studies reporting that it is possible to increase vigilance towards potentially unhealthy individuals (e.g., people with physical deformities and older people [<xref rid="B4-vision-09-00039" ref-type="bibr">4</xref>,<xref rid="B32-vision-09-00039" ref-type="bibr">32</xref>,<xref rid="B33-vision-09-00039" ref-type="bibr">33</xref>]. Faces are more consistently available than other cues (e.g., body odor and biological motion) and are detected and processed more rapidly than nearly any other stimulus [<xref rid="B34-vision-09-00039" ref-type="bibr">34</xref>,<xref rid="B35-vision-09-00039" ref-type="bibr">35</xref>,<xref rid="B36-vision-09-00039" ref-type="bibr">36</xref>]. Thus, improving sensitivity and visual attention to facial cues of natural, contagious illness may be beneficial for protecting health [<xref rid="B18-vision-09-00039" ref-type="bibr">18</xref>].</p><p>Given these potential benefits, we trained the participants on specific facial cues of illness (e.g., drooping eyelids and downturned corners of the mouth). Those who were trained were more likely to report using these facial regions, which co-occurred with both increased accuracy of sick face avoidance and changes in visual attention to sick and healthy faces. We used faces of people who were naturally sick, paired with the same peoples&#x02019; faces when they were healthy, enabling us to control various individual differences (e.g., age, weight, race, and attractiveness). Overall, our results suggest that avoidance of sick faces can be improved with training, consistent with the proposal of a flexible behavioral immune system.</p><sec id="sec4dot1-vision-09-00039"><title>4.1. Disease Training Improves Sickness Avoidance (Prediction 1)</title><p>The disease training group was slower and more accurate than the control group in deciding who to approach. In the post-manipulation task, the disease training group, compared to the control group, took more time before making their responses, potentially reflecting greater thoughtfulness and careful examination of the facial features associated with health. Of note, while there is typically a speed and accuracy trade-off [<xref rid="B37-vision-09-00039" ref-type="bibr">37</xref>], we found that the disease training group improved accuracy but not at the expense of speed (i.e., there was no change in response latency from pre- to post-manipulation). Our findings suggest that disease training supports both accurate and efficient responses.</p><p>Our goal of using the avoidance task was to simulate a common social situation that people may encounter when making decisions about how to spatially navigate around others (e.g., waiting rooms and public transport). However, given that we removed all contextual information (e.g., movement, voice, posture, and odor) that people have in the real world, the fact that we still found accuracy improvement in the disease-trained group is particularly notable. Nonetheless, we acknowledge that the observed effect sizes were small; therefore, future studies are needed to enhance this type of disease training by lengthening the training to include additional practice trials to improve generalizability across a broader range of faces and by tracking how visual attention changes with learning about sick faces.</p></sec><sec id="sec4dot2-vision-09-00039"><title>4.2. Disease Training Alters Visual Attention During Sickness Recognition (Predictions 2 and 3)</title><p>The participants in the disease training group and the control group did not differ in their accuracy of explicitly recognizing sick people. However, their attention patterns to the faces did differ, suggesting that the experimental manipulation altered some aspects of the recognition process. One potential interpretation is that there may be more than one way to recognize sick faces, either through a fast and automatic implicit process, used by the control group, or through a more controlled, explicit process, used by the disease training group. Impressively, both processes resulted in comparably accurate levels of sickness recognition accuracy.</p><p>The control group looked more at sick than healthy faces when making judgments about health. This result is in line with a previous study that found that faces edited to appear sick received more attention capture and holding compared to unedited healthy faces during a health-irrelevant facial categorization task [<xref rid="B23-vision-09-00039" ref-type="bibr">23</xref>]. Together, our findings and prior findings suggest that faces appearing sick may receive prioritized attention without priming or training, regardless of whether health is task-relevant, consistent with threat detection theories [<xref rid="B21-vision-09-00039" ref-type="bibr">21</xref>,<xref rid="B22-vision-09-00039" ref-type="bibr">22</xref>]. On the other hand, another study reported that healthy faces held attention longer than naturally sick faces in a passive viewing paradigm, possibly reflecting a bias to attend to faces of people who appear prosocial or attractive [<xref rid="B19-vision-09-00039" ref-type="bibr">19</xref>]. These different findings suggest that the instructions given to participants may influence how they naturally view faces varying in health.</p><p>In our study, we also found that the disease training group looked more equally at sick and healthy faces, potentially because they knew what they were looking for, leading them to more directly compare the two faces. In fact, compared to the control group, the disease training group looked more equally between the sick and healthy eye regions and showed marginally, though not statistically significantly, more alternating gaze shifts between the sick and healthy mouth regions. During the disease training, we highlighted changes in the eye and mouth regions of sick faces, because previous studies report that these are key features of sick faces [<xref rid="B17-vision-09-00039" ref-type="bibr">17</xref>,<xref rid="B18-vision-09-00039" ref-type="bibr">18</xref>]. Consistent with these visual attention results, the participants in the disease training group were more likely to report using the eye and mouth regions to determine which faces were sick, suggesting that they retained and applied the knowledge gained from the disease training. Overall, the participants&#x02019; visual attention and self-reports of the regions to which they attended reinforce the use of the eye and mouth regions in future sick face perception intervention studies.</p></sec></sec><sec sec-type="conclusions" id="sec5-vision-09-00039"><title>5. Conclusions</title><p>Overall, our findings suggest that sick face perception may be malleable. The participants who engaged in training about disease were more accurate at avoiding sick people using only face photos compared to the participants in the control condition. Further, the disease training group looked more evenly between the sick and healthy faces than the control group, and they made more alternating gaze shifts between the sick and healthy mouths, in line with the disease training group being more likely to report using the mouth to decide which face was sick, a crucial piece of the lassitude expression [<xref rid="B17-vision-09-00039" ref-type="bibr">17</xref>]. This study supports the hypothesis that sick face perception is a plastic skill, flexibly influenced by experience, and serves as a first step to exploring ways to improve this ability. Future studies are still needed to develop ways to further strengthen sick face perception, test how long-lasting the effects are, and uncover the role of visual attention in driving this flexibility. Enhancing behavioral avoidance and visual attention to disease cues could reduce the spread of illness and maximize opportunities for social engagement.</p></sec></body><back><ack><title>Acknowledgments</title><p>We are grateful to all of the participants who donated photos of their sick and healthy faces, without which this study would not be possible and to the people who helped us collect and prepare stimuli, including Shantalle Martinez, Isabella Cabrera, Amy Ahn, and Baylee Brochu. We thank Guangyu Zeng for contributions to the study design. We also thank the people who participated in this study.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><app-group><app id="app1-vision-09-00039"><title>Supplementary Materials</title><p>The following supporting information can be downloaded at <uri xlink:href="https://www.mdpi.com/article/10.3390/vision9020039/s1">https://www.mdpi.com/article/10.3390/vision9020039/s1</uri>, supplementary materials and methods, Table S1, supplementary results, Table S2, avoidance task data, look duration data, alternating gaze shifts data.</p><supplementary-material id="vision-09-00039-s001" position="float" content-type="local-data"><media xlink:href="vision-09-00039-s001.zip"/></supplementary-material></app></app-group><notes><title>Author Contributions</title><p>Conceptualization, T.S.L. and E.A.S.; Methodology, all authors; Software, T.S.L.; Formal Analysis, T.S.L.; Resources, E.A.S.; Data Curation, T.S.L.; Writing&#x02014;Original Draft Preparation, T.S.L.; Writing&#x02014;Review &#x00026; Editing, E.A.S. and K.V.J.; Visualization, T.S.L.; Supervision, E.A.S.; Funding Acquisition, T.S.L., K.V.J. and E.A.S. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>The study was conducted according to the guidelines of the Declaration of Helsinki and approved by the Institutional Review Board (or Ethics Committee) of the University of Miami (IRB #: 20230723, approved: July 21, 2023). A consent form for participation was distributed to all participants.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data associated with this paper are available in the <xref rid="app1-vision-09-00039" ref-type="app">Supplementary Materials</xref>.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-vision-09-00039"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schaller</surname><given-names>M.</given-names></name>
</person-group><article-title>The behavioural immune system and the psychology of human sociality</article-title><source>Philos. Trans. R. Soc. B Biol. Sci.</source><year>2011</year><volume>366</volume><fpage>3418</fpage><lpage>3426</lpage><pub-id pub-id-type="doi">10.1098/rstb.2011.0029</pub-id></element-citation></ref><ref id="B2-vision-09-00039"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Murray</surname><given-names>D.R.</given-names></name>
<name><surname>Schaller</surname><given-names>M.</given-names></name>
</person-group><article-title>Chapter Two-The Behavioral Immune System: Implications for Social Cognition, Social Interaction and Social Influence</article-title><source>Adv. Exp. Soc. Psychol.</source><year>2016</year><volume>53</volume><fpage>75</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1016/bs.aesp.2015.09.002</pub-id></element-citation></ref><ref id="B3-vision-09-00039"><label>3.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Schaller</surname><given-names>M.</given-names></name>
<name><surname>Park</surname><given-names>J.H.</given-names></name>
<name><surname>Kenrick</surname><given-names>D.</given-names></name>
</person-group><article-title>Human evolution and social cognition</article-title><source>Oxford Handbook of Evolutionary Psychology</source><publisher-name>Oxford University Press</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2012</year><pub-id pub-id-type="doi">10.1093/oxfordhb/9780198568308.013.0033</pub-id></element-citation></ref><ref id="B4-vision-09-00039"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ainsworth</surname><given-names>S.E.</given-names></name>
<name><surname>Maner</surname><given-names>J.K.</given-names></name>
</person-group><article-title>Pathogen avoidance mechanisms affect women&#x02019;s preference for symmetrical male faces</article-title><source>Evol. Behav. Sci.</source><year>2019</year><volume>13</volume><fpage>265</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1037/ebs0000139</pub-id></element-citation></ref><ref id="B5-vision-09-00039"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Brown</surname><given-names>M.</given-names></name>
<name><surname>Sacco</surname><given-names>D.F.</given-names></name>
</person-group><article-title>How and when crowd salience activates pathogen-avoidant motives</article-title><source>Evol. Behav. Sci.</source><year>2022</year><volume>16</volume><fpage>23</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1037/ebs0000191</pub-id></element-citation></ref><ref id="B6-vision-09-00039"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Brown</surname><given-names>M.</given-names></name>
<name><surname>Tracy</surname><given-names>R.E.</given-names></name>
<name><surname>Young</surname><given-names>S.G.</given-names></name>
<name><surname>Sacco</surname><given-names>D.F.</given-names></name>
</person-group><article-title>Crowd salience heightens tolerance to healthy facial features</article-title><source>Adapt. Hum. Behav. Physiol.</source><year>2021</year><volume>7</volume><fpage>432</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1007/s40750-021-00176-2</pub-id></element-citation></ref><ref id="B7-vision-09-00039"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Duncan</surname><given-names>L.A.</given-names></name>
<name><surname>Schaller</surname><given-names>M.</given-names></name>
</person-group><article-title>Prejudicial Attitudes Toward Older Adults May Be Exaggerated When People Feel Vulnerable to Infectious Disease: Evidence and Implications</article-title><source>Anal. Soc. Issues Public Policy</source><year>2009</year><volume>9</volume><fpage>97</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1111/j.1530-2415.2009.01188.x</pub-id></element-citation></ref><ref id="B8-vision-09-00039"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Millar</surname><given-names>M.</given-names></name>
<name><surname>Fink-Armold</surname><given-names>A.</given-names></name>
<name><surname>Lovitt</surname><given-names>A.</given-names></name>
</person-group><article-title>Disease salience effects on desire for affiliation with in-group and out-group members: Cognitive and affective mediators</article-title><source>Evol. Psychol.</source><year>2020</year><volume>18</volume><fpage>1474704920930700</fpage><pub-id pub-id-type="doi">10.1177/1474704920930700</pub-id><pub-id pub-id-type="pmid">32643386</pub-id>
</element-citation></ref><ref id="B9-vision-09-00039"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rengiiyiler</surname><given-names>S.</given-names></name>
<name><surname>Tek&#x000f6;zel</surname><given-names>M.</given-names></name>
</person-group><article-title>Visual attention is not attuned to non-human animal targets&#x02019; pathogenicity: An evolutionary mismatch perspective</article-title><source>J. Gen. Psychol.</source><year>2025</year><volume>152</volume><fpage>36</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1080/00221309.2024.2349005</pub-id><pub-id pub-id-type="pmid">38733318</pub-id>
</element-citation></ref><ref id="B10-vision-09-00039"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tybur</surname><given-names>J.M.</given-names></name>
<name><surname>Bryan</surname><given-names>A.D.</given-names></name>
<name><surname>Magnan</surname><given-names>R.E.</given-names></name>
<name><surname>Hooper</surname><given-names>A.E.C.</given-names></name>
</person-group><article-title>Smells like safe sex: Olfactory pathogen primes increase intentions to use condoms</article-title><source>Psychol. Sci.</source><year>2011</year><volume>22</volume><fpage>478</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1177/0956797611400096</pub-id><pub-id pub-id-type="pmid">21350181</pub-id>
</element-citation></ref><ref id="B11-vision-09-00039"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>White</surname><given-names>A.E.</given-names></name>
<name><surname>Kenrick</surname><given-names>D.T.</given-names></name>
<name><surname>Neuberg</surname><given-names>S.L.</given-names></name>
</person-group><article-title>Beauty at the ballot box: Disease threats predict preferences for physically attractive leaders</article-title><source>Psychol. Sci.</source><year>2013</year><volume>24</volume><fpage>2429</fpage><lpage>2436</lpage><pub-id pub-id-type="doi">10.1177/0956797613493642</pub-id><pub-id pub-id-type="pmid">24121414</pub-id>
</element-citation></ref><ref id="B12-vision-09-00039"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bouayed</surname><given-names>J.</given-names></name>
</person-group><article-title>Sorry, I am sneezing and coughing but I do not have COVID-19</article-title><source>Brain Behav. Immun.</source><year>2022</year><volume>101</volume><fpage>57</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.bbi.2021.12.018</pub-id><pub-id pub-id-type="pmid">34968716</pub-id>
</element-citation></ref><ref id="B13-vision-09-00039"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>S.W.</given-names></name>
<name><surname>Schwarz</surname><given-names>N.</given-names></name>
<name><surname>Taubman</surname><given-names>D.</given-names></name>
<name><surname>Hou</surname><given-names>M.</given-names></name>
</person-group><article-title>Sneezing in times of a flu pandemic: Public sneezing increases perception of unrelated risks and shifts preferences for federal spending</article-title><source>Psychol. Sci.</source><year>2010</year><volume>21</volume><fpage>375</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1177/0956797609359876</pub-id><pub-id pub-id-type="pmid">20424072</pub-id>
</element-citation></ref><ref id="B14-vision-09-00039"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tskhay</surname><given-names>K.O.</given-names></name>
<name><surname>Wilson</surname><given-names>J.P.</given-names></name>
<name><surname>Rule</surname><given-names>N.O.</given-names></name>
</person-group><article-title>People use psychological cues to detect physical disease from faces</article-title><source>Personal. Soc. Psychol. Bull.</source><year>2016</year><volume>42</volume><fpage>1309</fpage><lpage>1320</lpage><pub-id pub-id-type="doi">10.1177/0146167216656357</pub-id></element-citation></ref><ref id="B15-vision-09-00039"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Young</surname><given-names>S.G.</given-names></name>
<name><surname>Sacco</surname><given-names>D.F.</given-names></name>
<name><surname>Hugenberg</surname><given-names>K.</given-names></name>
</person-group><article-title>Vulnerability to disease is associated with a domain-specific preference for symmetrical faces relative to symmetrical non-face stimuli</article-title><source>Eur. J. Soc. Psychol.</source><year>2011</year><volume>41</volume><fpage>558</fpage><lpage>563</lpage><pub-id pub-id-type="doi">10.1002/ejsp.800</pub-id></element-citation></ref><ref id="B16-vision-09-00039"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schmid</surname><given-names>C.</given-names></name>
<name><surname>B&#x000fc;chel</surname><given-names>C.</given-names></name>
<name><surname>Rose</surname><given-names>M.</given-names></name>
</person-group><article-title>The neural basis of visual dominance in the context of audio-visual object processing</article-title><source>NeuroImage</source><year>2011</year><volume>55</volume><fpage>304</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.11.051</pub-id><pub-id pub-id-type="pmid">21112404</pub-id>
</element-citation></ref><ref id="B17-vision-09-00039"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schrock</surname><given-names>J.M.</given-names></name>
<name><surname>Snodgrass</surname><given-names>J.J.</given-names></name>
<name><surname>Sugiyama</surname><given-names>L.S.</given-names></name>
</person-group><article-title>Lassitude: The emotion of being sick</article-title><source>Evol. Hum. Behav.</source><year>2020</year><volume>41</volume><fpage>44</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1016/j.evolhumbehav.2019.09.002</pub-id></element-citation></ref><ref id="B18-vision-09-00039"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Axelsson</surname><given-names>J.</given-names></name>
<name><surname>Sundelin</surname><given-names>T.</given-names></name>
<name><surname>Olsson</surname><given-names>M.J.</given-names></name>
<name><surname>Sorjonen</surname><given-names>K.</given-names></name>
<name><surname>Axelsson</surname><given-names>C.</given-names></name>
<name><surname>Lasselin</surname><given-names>J.</given-names></name>
<name><surname>Lekander</surname><given-names>M.</given-names></name>
</person-group><article-title>Identification of acutely sick people and facial cues of sickness</article-title><source>Proc. R. Soc. B Biol. Sci.</source><year>2018</year><volume>285</volume><elocation-id>20172430</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2017.2430</pub-id></element-citation></ref><ref id="B19-vision-09-00039"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Leung</surname><given-names>T.S.</given-names></name>
<name><surname>Maylott</surname><given-names>S.E.</given-names></name>
<name><surname>Zeng</surname><given-names>G.</given-names></name>
<name><surname>Nascimben</surname><given-names>D.N.</given-names></name>
<name><surname>Jakobsen</surname><given-names>K.V.</given-names></name>
<name><surname>Simpson</surname><given-names>E.A.</given-names></name>
</person-group><article-title>Behavioral and physiological sensitivity to natural sick faces</article-title><source>Brain Behav. Immun.</source><year>2023</year><volume>110</volume><fpage>195</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1016/j.bbi.2023.03.007</pub-id><pub-id pub-id-type="pmid">36893923</pub-id>
</element-citation></ref><ref id="B20-vision-09-00039"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Leung</surname><given-names>T.S.</given-names></name>
<name><surname>Zeng</surname><given-names>G.</given-names></name>
<name><surname>Maylott</surname><given-names>S.E.</given-names></name>
<name><surname>Martinez</surname><given-names>S.N.</given-names></name>
<name><surname>Jakobsen</surname><given-names>K.V.</given-names></name>
<name><surname>Simpson</surname><given-names>E.A.</given-names></name>
</person-group><article-title>Infection detection in faces: Children&#x02019;s development of pathogen avoidance</article-title><source>Child Dev.</source><year>2024</year><volume>95</volume><fpage>e35</fpage><lpage>e46</lpage><pub-id pub-id-type="doi">10.1111/cdev.13983</pub-id><pub-id pub-id-type="pmid">37589080</pub-id>
</element-citation></ref><ref id="B21-vision-09-00039"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Berdica</surname><given-names>E.</given-names></name>
<name><surname>Gerdes</surname><given-names>A.B.</given-names></name>
<name><surname>Bublatzky</surname><given-names>F.</given-names></name>
<name><surname>White</surname><given-names>A.J.</given-names></name>
<name><surname>Alpers</surname><given-names>G.W.</given-names></name>
</person-group><article-title>Threat vs. threat: Attention to fear-related animals and threatening faces</article-title><source>Front. Psychol.</source><year>2018</year><volume>9</volume><fpage>1154</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2018.01154</pub-id><pub-id pub-id-type="pmid">30083115</pub-id>
</element-citation></ref><ref id="B22-vision-09-00039"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Feldmann-W&#x000fc;stefeld</surname><given-names>T.</given-names></name>
<name><surname>Schmidt-Daffy</surname><given-names>M.</given-names></name>
<name><surname>Schub&#x000f6;</surname><given-names>A.</given-names></name>
</person-group><article-title>Neural evidence for the threat detection advantage: Differential attention allocation to angry and happy faces</article-title><source>Psychophysiology</source><year>2011</year><volume>48</volume><fpage>697</fpage><lpage>707</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2010.01130.x</pub-id><pub-id pub-id-type="pmid">20883506</pub-id>
</element-citation></ref><ref id="B23-vision-09-00039"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>R.</given-names></name>
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Sima</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>F.</given-names></name>
<name><surname>Zou</surname><given-names>F.</given-names></name>
<name><surname>Luo</surname><given-names>Y.</given-names></name>
</person-group><article-title>Self-reference processing of fat-face and sick-face in individuals with different disgust sensitivity: Evidence from behavioral and neuroelectrophysiology</article-title><source>Neuropsychologia</source><year>2022</year><volume>175</volume><fpage>108368</fpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2022.108368</pub-id><pub-id pub-id-type="pmid">36100072</pub-id>
</element-citation></ref><ref id="B24-vision-09-00039"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Miller</surname><given-names>S.L.</given-names></name>
<name><surname>Maner</surname><given-names>J.K.</given-names></name>
</person-group><article-title>Sick body, vigilant mind: The biological immune system activates the behavioral immune system</article-title><source>Psychol. Sci.</source><year>2011</year><volume>22</volume><fpage>1467</fpage><lpage>1471</lpage><pub-id pub-id-type="doi">10.1177/0956797611420166</pub-id><pub-id pub-id-type="pmid">22058109</pub-id>
</element-citation></ref><ref id="B25-vision-09-00039"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tybur</surname><given-names>J.M.</given-names></name>
<name><surname>Jones</surname><given-names>B.C.</given-names></name>
<name><surname>DeBruine</surname><given-names>L.M.</given-names></name>
<name><surname>Ackerman</surname><given-names>J.M.</given-names></name>
<name><surname>Fasolt</surname><given-names>V.</given-names></name>
</person-group><article-title>Preregistered direct replication of &#x0201c;Sick body, vigilant mind: The biological immune system activates the behavioral immune system&#x0201d;</article-title><source>Psychol. Sci.</source><year>2020</year><volume>31</volume><fpage>1461</fpage><lpage>1469</lpage><pub-id pub-id-type="doi">10.1177/0956797620955209</pub-id><pub-id pub-id-type="pmid">33079639</pub-id>
</element-citation></ref><ref id="B26-vision-09-00039"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bressan</surname><given-names>P.</given-names></name>
</person-group><article-title>First impressions of a new face are shaped by infection concerns</article-title><source>Evol. Med. Public Health</source><year>2023</year><volume>11</volume><fpage>309</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1093/emph/eoad025</pub-id><pub-id pub-id-type="pmid">37706031</pub-id>
</element-citation></ref><ref id="B27-vision-09-00039"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Erdfelder</surname><given-names>E.</given-names></name>
<name><surname>Faul</surname><given-names>F.</given-names></name>
<name><surname>Buchner</surname><given-names>A.</given-names></name>
</person-group><article-title>GPOWER: A general power analysis program</article-title><source>Behav. Res. Methods Instrum. Comput.</source><year>1996</year><volume>28</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.3758/BF03203630</pub-id></element-citation></ref><ref id="B28-vision-09-00039"><label>28.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>SciShow Kids (Director)</collab>
</person-group><article-title>Colds, the Flu, and You. [Video Recording]</article-title><day>29</day><month>December</month><year>2016</year><comment>Available online: <ext-link xlink:href="https://www.youtube.com/watch?v=4uzNnKm41W8" ext-link-type="uri">https://www.youtube.com/watch?v=4uzNnKm41W8</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-04-25">(accessed on 25 April 2025)</date-in-citation></element-citation></ref><ref id="B29-vision-09-00039"><label>29.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>SciShow Kids (Director)</collab>
</person-group><article-title>Birds That Talk! [Video Recording]</article-title><day>3</day><month>July</month><year>2018</year><comment>Available online: <ext-link xlink:href="https://www.youtube.com/watch?v=g9Gj8JE72O4" ext-link-type="uri">https://www.youtube.com/watch?v=g9Gj8JE72O4</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-04-25">(accessed on 25 April 2025)</date-in-citation></element-citation></ref><ref id="B30-vision-09-00039"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Brown</surname><given-names>W.</given-names></name>
</person-group><article-title>The study of the face and of the facial expression of the sick child</article-title><source>Edinb. Med. J.</source><year>1937</year><volume>44</volume><fpage>T141</fpage><lpage>T172</lpage><pub-id pub-id-type="pmid">29647540</pub-id>
</element-citation></ref><ref id="B31-vision-09-00039"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Armann</surname><given-names>R.</given-names></name>
<name><surname>B&#x000fc;lthoff</surname><given-names>I.</given-names></name>
</person-group><article-title>Gaze behavior in face comparison: The roles of sex, task, and symmetry</article-title><source>Atten. Percept. Psychophys.</source><year>2009</year><volume>71</volume><fpage>1107</fpage><lpage>1126</lpage><pub-id pub-id-type="doi">10.3758/APP.71.5.1107</pub-id><pub-id pub-id-type="pmid">19525541</pub-id>
</element-citation></ref><ref id="B32-vision-09-00039"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Faulkner</surname><given-names>J.</given-names></name>
<name><surname>Schaller</surname><given-names>M.</given-names></name>
<name><surname>Park</surname><given-names>J.H.</given-names></name>
<name><surname>Duncan</surname><given-names>L.A.</given-names></name>
</person-group><article-title>Evolved disease-avoidance mechanisms and contemporary xenophobic attitudes</article-title><source>Group Process. Intergroup Relat.</source><year>2004</year><volume>7</volume><fpage>333</fpage><lpage>353</lpage><pub-id pub-id-type="doi">10.1177/1368430204046142</pub-id></element-citation></ref><ref id="B33-vision-09-00039"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Park</surname><given-names>J.H.</given-names></name>
<name><surname>Schaller</surname><given-names>M.</given-names></name>
<name><surname>Crandall</surname><given-names>C.S.</given-names></name>
</person-group><article-title>Pathogen-avoidance mechanisms and the stigmatization of obese people</article-title><source>Evol. Hum. Behav.</source><year>2007</year><volume>28</volume><fpage>410</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1016/j.evolhumbehav.2007.05.008</pub-id></element-citation></ref><ref id="B34-vision-09-00039"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Crouzet</surname><given-names>S.M.</given-names></name>
<name><surname>Kirchner</surname><given-names>H.</given-names></name>
<name><surname>Thorpe</surname><given-names>S.J.</given-names></name>
</person-group><article-title>Fast saccades toward faces: Face detection in just 100 ms</article-title><source>J. Vis.</source><year>2010</year><volume>10</volume><fpage>16</fpage><pub-id pub-id-type="doi">10.1167/10.4.16</pub-id><pub-id pub-id-type="pmid">20465335</pub-id>
</element-citation></ref><ref id="B35-vision-09-00039"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jack</surname><given-names>R.E.</given-names></name>
<name><surname>Schyns</surname><given-names>P.G.</given-names></name>
</person-group><article-title>The human face as a dynamic tool for social communication</article-title><source>Curr. Biol.</source><year>2015</year><volume>25</volume><fpage>R621</fpage><lpage>R634</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.05.052</pub-id><pub-id pub-id-type="pmid">26196493</pub-id>
</element-citation></ref><ref id="B36-vision-09-00039"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wardle</surname><given-names>S.G.</given-names></name>
<name><surname>Taubert</surname><given-names>J.</given-names></name>
<name><surname>Teichmann</surname><given-names>L.</given-names></name>
<name><surname>Baker</surname><given-names>C.I.</given-names></name>
</person-group><article-title>Rapid and dynamic processing of face pareidolia in the human brain</article-title><source>Nat. Commun.</source><year>2020</year><volume>11</volume><fpage>4518</fpage><pub-id pub-id-type="doi">10.1038/s41467-020-18325-8</pub-id><pub-id pub-id-type="pmid">32908146</pub-id>
</element-citation></ref><ref id="B37-vision-09-00039"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Heitz</surname><given-names>R.P.</given-names></name>
</person-group><article-title>The speed-accuracy tradeoff: History, physiology, methodology, and behavior</article-title><source>Front. Neurosci.</source><year>2014</year><volume>8</volume><elocation-id>150</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00150</pub-id><pub-id pub-id-type="pmid">24966810</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="vision-09-00039-f001"><label>Figure 1</label><caption><p>(<bold>A</bold>) Examples of sick and healthy face stimuli. (<bold>B</bold>) Examples of AOIs drawn around the faces (pink ovals), eyes (black rectangles), and mouths (black rectangles).</p></caption><graphic xlink:href="vision-09-00039-g001" position="float"/></fig><fig position="float" id="vision-09-00039-f002"><label>Figure 2</label><caption><p>Study procedure tasks and measures: (<bold>A</bold>) pre-manipulation baseline avoidance task, (<bold>B</bold>) experimental manipulation: interactive story, (<bold>C</bold>) experimental manipulation: video, (<bold>D</bold>) interactive experimental manipulation: training trials, (<bold>E</bold>) post-manipulation avoidance task, (<bold>F</bold>) recognition task with eye tracking, and (<bold>G</bold>) participant self-report of what facial features they used to identify sick faces.</p></caption><graphic xlink:href="vision-09-00039-g002" position="float"/></fig><fig position="float" id="vision-09-00039-f003"><label>Figure 3</label><caption><p>Accuracy in the avoidance task (&#x0201c;Who would you rather share dinner with?&#x0201d;) measured by the number of correct trials (out of 16 total) before the experimental manipulation (pre-manipulation; left) and after the experimental manipulation (post-manipulation; right) for the control condition group (red) and the disease training group (blue). Boxes indicate the first and third quartiles, whiskers indicate 1.5&#x000d7; above and below the interquartile range, horizontal lines within the boxes indicate medians, Xs indicate means, and n.s. indicates not statistically significant (<italic toggle="yes">p</italic>s &#x0003e; 0.10). All the subgroups were above chance (dashed line; 8 out of 16 trials; 50% correct).</p></caption><graphic xlink:href="vision-09-00039-g003" position="float"/></fig><fig position="float" id="vision-09-00039-f004"><label>Figure 4</label><caption><p>Manual response latency (seconds) in the avoidance task (&#x0201c;Who would you rather share dinner with?&#x0201d;) before the experimental manipulation (pre-manipulation; left) and after the experimental manipulation (post-manipulation; right) for the control condition group (red) and the disease training group (blue). Boxes indicate the first and third quartiles, whiskers indicate 1.5&#x000d7; above and below the interquartile range, horizontal lines within the boxes indicate medians, Xs indicate means, and n.s. indicates not statistically significant (<italic toggle="yes">p</italic>s &#x0003e; 0.10).</p></caption><graphic xlink:href="vision-09-00039-g004" position="float"/></fig><fig position="float" id="vision-09-00039-f005"><label>Figure 5</label><caption><p>Look duration difference scores in the recognition task (&#x0201c;Which twin do you think is sick?&#x0201d;) after the experimental manipulation (post-manipulation only) for the control condition group (red) and the disease training group (blue), reflecting visual attention to the (<bold>A</bold>) faces, (<bold>B</bold>) eye regions, (<bold>C</bold>) mouth regions, and (<bold>D</bold>) remaining face regions. The number of seconds looking at the healthy face/eyes was subtracted from the number of seconds looking at the sick face/eyes, so scores closer to zero (dashed line) indicate more equal looking at sick and healthy faces, and larger absolute value scores indicate more differential looking, with positive values indicating more looking at the healthy faces and negative values indicating more looking at the sick faces. Boxes indicate the first and third quartiles, whiskers indicate 1.5&#x000d7; above and below the interquartile range, horizontal lines within the boxes indicate medians, Xs indicate means, and n.s. indicates not statistically significant (<italic toggle="yes">p</italic>s &#x0003e; 0.10). One-sample <italic toggle="yes">t</italic> tests indicated looking was different from chance (dashed line), * <italic toggle="yes">p</italic>s &#x0003c; 0.05.</p></caption><graphic xlink:href="vision-09-00039-g005" position="float"/></fig><fig position="float" id="vision-09-00039-f006"><label>Figure 6</label><caption><p>Alternating gaze shifts back and forth between the sick and healthy faces (left), eye regions (middle), and mouth regions (right) in the recognition task (&#x0201c;Which twin do you think is sick?&#x0201d;) for the control condition group (red) and the disease training group (blue). Boxes indicate the first and third quartiles, whiskers indicate 1.5&#x000d7; above and below the interquartile range, horizontal lines within the boxes indicate medians, Xs indicate means, and n.s. indicates not statistically significant (<italic toggle="yes">p</italic>s &#x0003e; 0.10).</p></caption><graphic xlink:href="vision-09-00039-g006" position="float"/></fig><table-wrap position="float" id="vision-09-00039-t001"><object-id pub-id-type="pii">vision-09-00039-t001_Table 1</object-id><label>Table 1</label><caption><p>Demographic information.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Control Group</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Disease Training Group</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>
<italic toggle="yes">Age in Years</italic>
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mean (SD)</td><td align="center" valign="middle" rowspan="1" colspan="1">19 (1)</td><td align="center" valign="middle" rowspan="1" colspan="1">19 (3)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Range</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18&#x02013;23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18&#x02013;44</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>
<italic toggle="yes">Gender</italic>
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Man</td><td align="center" valign="middle" rowspan="1" colspan="1">23</td><td align="center" valign="middle" rowspan="1" colspan="1">23</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Nonbinary</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Woman</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>
<italic toggle="yes">Ethnicity</italic>
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Hispanic or Latino</td><td align="center" valign="middle" rowspan="1" colspan="1">18</td><td align="center" valign="middle" rowspan="1" colspan="1">12</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Not Hispanic or Latino</td><td align="center" valign="middle" rowspan="1" colspan="1">46</td><td align="center" valign="middle" rowspan="1" colspan="1">54</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Unknown or Other</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>
<italic toggle="yes">Race</italic>
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">American Indian or Alaska Native and White</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Arab, Middle Eastern, or North African</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Arab, Middle Eastern, or North African and Jewish</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Arab, Middle Eastern, or North African and White</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Asian</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Asian and Native Hawaiian or Other Pacific Islander</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Asian and Native Hawaiian or Other Pacific Islander and White</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Asian and White</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Black or African American</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Black or African American and Native Hawaiian or Other Pacific Islander</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Black or African American and White</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mixed (wrote-in)</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Native Hawaiian or Other Pacific Islander</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Unknown or prefer not to say</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">White</td><td align="center" valign="middle" rowspan="1" colspan="1">46</td><td align="center" valign="middle" rowspan="1" colspan="1">44</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">White and Unknown or prefer not to say</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td></tr></tbody></table><table-wrap-foot><fn><p>Note. Sample sizes for the control condition group (left) and disease training group (right).</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="vision-09-00039-t002"><object-id pub-id-type="pii">vision-09-00039-t002_Table 2</object-id><label>Table 2</label><caption><p>Summary of primary analyses and results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Measure</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Result</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">Avoidance Task (Primary Analysis 1)</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">The disease training group was more accurate than the control group in the post-manipulation avoidance task; the disease training group was more accurate in the post-manipulation avoidance task than the pre-manipulation avoidance task.</td></tr><tr><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Response latency</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">The disease training group was slower than the control group in the post-manipulation avoidance task; the control group was faster in the post-manipulation avoidance task than in the pre-manipulation avoidance task.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">Recognition Task (Primary Analysis 2)</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Look duration&#x02014;Face</td><td align="center" valign="middle" rowspan="1" colspan="1">The disease training group showed more even looking at sick and healthy faces than the control group who looked more at the sick faces than the healthy faces.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Look duration&#x02014;Eyes</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Look duration&#x02014;Mouth</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Alternating gazes&#x02014;Face</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Alternating gazes&#x02014;Eyes</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Alternating gazes&#x02014;Mouth</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">The disease training group showed marginally <sup>#</sup> more alternating gaze shifts than the control group.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">Self-Report (Primary Analysis 3)</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Use of eyes</td><td align="center" valign="middle" rowspan="1" colspan="1">The disease training group was more likely to report using the eyes.</td></tr><tr><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Use of mouth</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">The disease training group was more likely to report using the mouth.</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Accuracy measures were the number of correct trials. Duration-based measures (response latency and look duration) were difference scores (healthy minus sick) in seconds; n.s. indicates not statistically significant (<italic toggle="yes">p</italic>s &#x0003e; 0.05); <sup>#</sup>
<italic toggle="yes">p</italic> = 0.062. Alternating gazes refer to the number of times the participants looked back and forth between the sick and healthy faces.</p></fn></table-wrap-foot></table-wrap></floats-group></article>