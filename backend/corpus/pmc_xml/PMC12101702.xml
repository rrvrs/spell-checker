<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408384</article-id><article-id pub-id-type="pmc">PMC12101702</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0324392</article-id><article-id pub-id-type="publisher-id">PONE-D-24-54977</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Image Processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Information Theory</subject><subj-group><subject>Information Entropy</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Mathematical and Statistical Techniques</subject><subj-group><subject>Mathematical Functions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Mathematical and Statistical Techniques</subject><subj-group><subject>Mathematical Functions</subject><subj-group><subject>Wavelet Transforms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Optimization</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Digital Imaging</subject><subj-group><subject>Grayscale</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Image information optimization processing based on fractional order differentiation and WT algorithm</article-title><alt-title alt-title-type="running-head">Fractional order differentiation and WT algorithm</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0005-4746-3883</contrib-id><name><surname>Long</surname><given-names>Qiong</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="cor001" ref-type="corresp">*</xref><xref rid="aff001" ref-type="aff"/></contrib></contrib-group><aff id="aff001">
<addr-line>School of General Education, Chengdu Jincheng College, Chengdu, China</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Uysal</surname><given-names>Fatih</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Kafkas University: Kafkas Universitesi, T&#x000dc;RKIYE</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>longqiong2022@163.com</email></corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0324392</elocation-id><history><date date-type="received"><day>28</day><month>11</month><year>2024</year></date><date date-type="accepted"><day>25</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Qiong Long</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Qiong Long</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0324392.pdf">
</self-uri><abstract><p>As one of the most important ways for humans to perceive the world, images contain a wealth of visual information. Digital image processing is a technology that uses computer methods to process and enhance photographs in order to extract meaningful information and improve image quality. However, current image processing techniques have poor performance in processing complex images. To improve the quality of complex images, research proposes an image information optimization processing method based on fractional order differentiation and WT algorithm. Image edge detection and image fusion are important technologies in the field of image processing, with wide application value. Therefore, the study is based on wavelet transform algorithm and fractional order differentiation to perform edge detection and image fusion. The results revealed that when the study used the four evaluation metrics of information entropy, recall, mean square error, and precision to evaluate the effectiveness of image edge detection, the Sobel operator had the highest precision of detection recall, and the smallest information entropy and mean square error. The method achieved an 80% recall rate, a minimum information entropy of 3.13, a highest detection precision of 78.9%, and a minimum mean square error of 152. The average gradient, information entropy, spatial frequency, mutual information of the method adopted by the study for image fusion was compared with other methods in case of different groups of images. The method adopted by the study for image fusion provided the best results. The precision of the proposed method edge detection by the study was higher and the performance of image fusion was better and effective in improving the quality of the image.</p></abstract><funding-group><funding-statement>The author(s) received no specific funding for this work.</funding-statement></funding-group><counts><fig-count count="14"/><table-count count="4"/><page-count count="19"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All relevant data are within the article and its supporting information files.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All relevant data are within the article and its supporting information files.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>1. Introduction</title><p>As the visual basis of human perception of the world, images play a crucial role in the information-rich society of the 21st century by enabling people to express, receive, and transmit information [<xref rid="pone.0324392.ref001" ref-type="bibr">1</xref>&#x02013;<xref rid="pone.0324392.ref002" ref-type="bibr">2</xref>]. However, people can perceive the world more accurately and objectively by using digital image processing technologies. It is possible to restore clarity and brightness to blurred or even invisible photographs using image enhancement techniques. There are two important steps in image processing, namely image edge detection and image fusion [<xref rid="pone.0324392.ref003" ref-type="bibr">3</xref>]. Many scholars have pointed out that current image processing techniques may distort or lose details when processing complex images. Moreover, the noise and complex background of images make it difficult for current image processing techniques to accurately segment and recognize targets [<xref rid="pone.0324392.ref004" ref-type="bibr">4</xref>&#x02013;<xref rid="pone.0324392.ref005" ref-type="bibr">5</xref>]. The use of wavelet transform multi-scale processing technology for image denoising and feature extraction has demonstrated efficient performance. With the increase of data volume, how to effectively remove noise and interference from images while maintaining image details remains a challenge. When using fractional differentiation for edge detection, not only can it effectively extract the edge information of the image, but it can also preserve weak edge information in smooth regions. The image fusion method based on fractional wavelet transform (FRWT) can remove noise and interference in the image while maintaining image details, improving the quality and clarity of the image [<xref rid="pone.0324392.ref006" ref-type="bibr">6</xref>]. The image quality requirements in fields such as medical diagnosis and geological exploration are relatively high. There are problems in multi-focus image fusion, such as easy detail loss and artifacts at image edges. In view of this, in order to improve the quality of multi focus image fusion, avoid the loss of local details and edge artifacts, a method based on wavelet transform (WT) algorithm and fractional order differentiation (FOD) is proposed to optimize the information of medical and satellite images.</p><p>When studying the optimization processing of image information based on fractional order differentiation and WT algorithms, the innovation of this study lies in the combination of fractional order differentiation and WT algorithm. Compared with traditional integer order differentiation, fractional order differentiation can more flexibly adjust the order of differentiation, thereby finely controlling the local feature extraction of images. The main contribution of this study is to provide a high-performance solution for edge detection and fusion of digital images, which greatly promotes the development of digital image processing research and the intelligence of image fusion. It provides an effective method for the application of image edge detection in displacement monitoring equipment, medical images and remote sensing images.</p><p>The research will investigate the image information optimization processing method based on FOD and WT algorithm from four aspects. First, an overview of the state of FOD and WT algorithm research is presented. Second is the research on image information optimization processing methods for FOD and WT algorithms. Next comes the experimental verification of the suggested research design. Lastly, there is a summary of the research.</p></sec><sec id="sec002"><title>2. Related works</title><p>Fractional calculus generalizes differential and integral operations by unifying them into a single fractional-order (FO) derivative of arbitrary order, and is commonly used in finance, engineering, and science [<xref rid="pone.0324392.ref007" ref-type="bibr">7</xref>]. To address mismatched disturbances in DC/DC buck converters, X. Lin et al. presented a FO sliding mode management approach based on a higher-order nonlinear disturbance observer. The findings showed that the technique suggested by the study enhanced the system&#x02019;s transient performance while lessening the system&#x02019;s sensitivity to mismatch disturbances [<xref rid="pone.0324392.ref008" ref-type="bibr">8</xref>]. B. Long et al. presented a fuzzy FO non-singular terminal sliding mode controller (SMC) for lower control limit-grid connected converter (LCL-GCC) grid current management in an effort to solve the issue that chattering in SMC tended to excite low-frequency (LF) unmodeled dynamics and may deteriorate tracking precision. The study was based on state estimation of minimally sampled sensors by Kalman filtering. The results indicated that the proposed controller of the study could converge quickly with high tracking precision and robustness [<xref rid="pone.0324392.ref009" ref-type="bibr">9</xref>]. A novel finite-time FO command filtering implementation approach was proposed by X. You et al. [<xref rid="pone.0324392.ref010" ref-type="bibr">10</xref>]. The stabilizing function&#x02019;s FO derivatives did not need to be analytically calculated in the command filtering implementation technique. The outcomes showed that the suggested strategy could guarantee that the tracking error converges in a finite amount of time to a tiny neighborhood around the origin [<xref rid="pone.0324392.ref011" ref-type="bibr">11</xref>]. B. Babes et al. suggested an adaptive fuzzy FO non-singular terminal SMC for synthetic robustness in an effort to enhance a DC-DC buck converter&#x02019;s output voltage tracking control performance. The study combined the fractional calculus with the hybrid control method of non-singular terminal sliding mode controller (NTSMC). The study&#x02019;s suggested method enhanced the conventional SMCs&#x02019; resilience to disturbances and parameter fluctuations [<xref rid="pone.0324392.ref012" ref-type="bibr">12</xref>]. To prevent repeated resonances that could impair system stability and power quality, M. A. Azghandi et al. presented a delay based lossless FO virtual capacitor analysis and design. The capacitor used as a resonance damper for a multiple parallel paralleling current-source inverters (CSI) system. The suggested capacitor, according to the data, offered more degrees of freedom to improve the control&#x02019;s robustness and frequency behavior [<xref rid="pone.0324392.ref013" ref-type="bibr">13</xref>].</p><p>For time-frequency (TF) analysis and signal processing, the WT is a perfect instrument since it offers a frequency-varying &#x0201c;TF&#x0201d; window [<xref rid="pone.0324392.ref014" ref-type="bibr">14</xref>]. A WT real-time defect detection technique based on machine learning was presented by Y. Ma et al. for naval DC pulsating loads. The study&#x02019;s suggested method could locate further flaws that could cause anomalous disruptions in the load current profile [<xref rid="pone.0324392.ref015" ref-type="bibr">15</xref>]. To create four distinct types of insulation failures in power cables, M. H. Wang et al. suggested a convolutional neural network power cable fault detection method based on discrete wavelet transform (DWT) and chaotic system. The findings showed that the suggested approach was capable of accurately identifying fault status changes in power cables and fault signal variations in power cables in real time [<xref rid="pone.0324392.ref016" ref-type="bibr">16</xref>]. R. P. Medeiros et al. found some delays in phase-based differential protection during fault occurrence due to phase convergence and proposed a new time-domain power transformer differential protection based on Clarke and WT that could be used for any power transformer. The suggested approach was effective, quick, easy to use, and independent of the differential current&#x02019;s fundamental and harmonic components, according to the results [<xref rid="pone.0324392.ref017" ref-type="bibr">17</xref>]. Empirical WT and differential fault energy were used by J. Gao and colleagues to decompose the differential fault energy using empirical WT, which allowed for the distinction between high impedance faults and normal disturbances in distribution systems. The findings showed that the study&#x02019;s suggested method could accurately identify high impedance defects from normal disturbances [<xref rid="pone.0324392.ref018" ref-type="bibr">18</xref>]. Xu et al. proposed an interference suppression technique for vehicle-mounted millimeter-wave radar in the adjustable Q-factor WT domain. The technique involved optimizing the model using a split-enhanced Lagrangian contraction algorithm, which addressed the issue of increased noise level caused by the distortion of radar echoes in the presence of mutual interference. As evidenced by the data, the suggested strategy lessens radar echo interference [<xref rid="pone.0324392.ref019" ref-type="bibr">19</xref>].</p><p>In summary, the above literature demonstrates the powerful application potential and advantages of fractional calculus and WT techniques in multiple fields. These studies have achieved significant results in improving system performance, enhancing robustness, and solving specific problems. However, there are still some challenges and limitations in terms of implementation complexity, computational cost, model dependency, dynamic response performance, long-term stability, and adaptability to complex environments, which need to be addressed and optimized in further research in the future. The proposed method combines fractional order differentiation and wavelet transform algorithms, which can effectively extract image edge information while preserving weak edge information in smooth areas. This method introduces fractional order differentiation, allowing the algorithm to adjust the differentiation order more flexibly and thus more finely control the extraction of local features in the image.</p></sec><sec id="sec003"><title>3. Image information optimization processing</title><sec id="sec004"><title>3.1. Image edge detection based on fod</title><p>There are many ways of defining fractional calculus and the commonly used forms of definition are Riemann-Liouville (RL) definition, Gr&#x000fc;mwald-Letnikov (G-L) definition, and Caputo definition [<xref rid="pone.0324392.ref020" ref-type="bibr">20</xref>&#x02013;<xref rid="pone.0324392.ref021" ref-type="bibr">21</xref>]. The Grumwald-Letnikov fractional calculus definition is obtained by higher-order differential generalization, which is a fundamental concept in fractional calculus, providing an extension of the traditional notion of integer-order derivatives. The G-L formula is important in fractional calculus because it provides a way to compute non-integer order derivatives. With FO derivatives, the derivative&#x02019;s order might be a real number as opposed to only an integer [<xref rid="pone.0324392.ref022" ref-type="bibr">22</xref>]. The G-L formula defines the FO derivative <inline-formula id="pone.0324392.e001"><alternatives><graphic xlink:href="pone.0324392.e001.jpg" id="pone.0324392.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> of the function <inline-formula id="pone.0324392.e002"><alternatives><graphic xlink:href="pone.0324392.e002.jpg" id="pone.0324392.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> at the point <inline-formula id="pone.0324392.e003"><alternatives><graphic xlink:href="pone.0324392.e003.jpg" id="pone.0324392.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. The FO derivative <inline-formula id="pone.0324392.e004"><alternatives><graphic xlink:href="pone.0324392.e004.jpg" id="pone.0324392.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is illustrated in <xref rid="pone.0324392.e005" ref-type="disp-formula">Equation (1)</xref>.</p><disp-formula id="pone.0324392.e005">
<alternatives><graphic xlink:href="pone.0324392.e005.jpg" id="pone.0324392.e005g" position="anchor"/><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>lim</mml:mo></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>&#x02211;</mml:mo><mml:msubsup><mml:mi>\nolimits</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x0221e;</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mi>a</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>k</mml:mi><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(1)</label>
</disp-formula><p>In <xref rid="pone.0324392.e005" ref-type="disp-formula">Equation (1)</xref>, <inline-formula id="pone.0324392.e006"><alternatives><graphic xlink:href="pone.0324392.e006.jpg" id="pone.0324392.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mi>a</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes a binomial coefficient and <inline-formula id="pone.0324392.e007"><alternatives><graphic xlink:href="pone.0324392.e007.jpg" id="pone.0324392.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes a real number. <inline-formula id="pone.0324392.e008"><alternatives><graphic xlink:href="pone.0324392.e008.jpg" id="pone.0324392.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the integration time step. <inline-formula id="pone.0324392.e009"><alternatives><graphic xlink:href="pone.0324392.e009.jpg" id="pone.0324392.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes a non-negative integer. When <inline-formula id="pone.0324392.e010"><alternatives><graphic xlink:href="pone.0324392.e010.jpg" id="pone.0324392.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is equal to 0, the binomial coefficient is defined as 1. The RL integral is a FO promotion of the successive indefinite integrals [<xref rid="pone.0324392.ref023" ref-type="bibr">23</xref>]. <xref rid="pone.0324392.e012" ref-type="disp-formula">Equation (2)</xref> expresses the RL integral of the function <inline-formula id="pone.0324392.e011"><alternatives><graphic xlink:href="pone.0324392.e011.jpg" id="pone.0324392.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p><disp-formula id="pone.0324392.e012">
<alternatives><graphic xlink:href="pone.0324392.e012.jpg" id="pone.0324392.e012g" position="anchor"/><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>&#x00393;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>c</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>t</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>t</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></alternatives>
<label>(2)</label>
</disp-formula><p>In <xref rid="pone.0324392.e012" ref-type="disp-formula">Equation (2)</xref>, <inline-formula id="pone.0324392.e013"><alternatives><graphic xlink:href="pone.0324392.e013.jpg" id="pone.0324392.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x00393;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the gamma function and <inline-formula id="pone.0324392.e014"><alternatives><graphic xlink:href="pone.0324392.e014.jpg" id="pone.0324392.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the integral variable. t denotes the integration time, <inline-formula id="pone.0324392.e015"><alternatives><graphic xlink:href="pone.0324392.e015.jpg" id="pone.0324392.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the initial point of integration taken. <inline-formula id="pone.0324392.e016"><alternatives><graphic xlink:href="pone.0324392.e016.jpg" id="pone.0324392.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the derivatives of the original function and <inline-formula id="pone.0324392.e017"><alternatives><graphic xlink:href="pone.0324392.e017.jpg" id="pone.0324392.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the bounded linear operator. Caputo fractional derivative is one of the generalizations of integer order derivatives, which actually means any order derivative greater than or equal to 0 [<xref rid="pone.0324392.ref024" ref-type="bibr">24</xref>]. Caputo fractional derivative is shown in <xref rid="pone.0324392.e018" ref-type="disp-formula">Equation (3)</xref>.</p><disp-formula id="pone.0324392.e018">
<alternatives><graphic xlink:href="pone.0324392.e018.jpg" id="pone.0324392.e018g" position="anchor"/><mml:math id="M18" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>&#x00393;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>v</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>c</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:math></alternatives>
<label>(3)</label>
</disp-formula><p>In <xref rid="pone.0324392.e018" ref-type="disp-formula">Equation (3)</xref>, <inline-formula id="pone.0324392.e019"><alternatives><graphic xlink:href="pone.0324392.e019.jpg" id="pone.0324392.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the smallest integer greater than <inline-formula id="pone.0324392.e020"><alternatives><graphic xlink:href="pone.0324392.e020.jpg" id="pone.0324392.e020g" position="anchor"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. <inline-formula id="pone.0324392.e021"><alternatives><graphic xlink:href="pone.0324392.e021.jpg" id="pone.0324392.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>v</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0324392.e022"><alternatives><graphic xlink:href="pone.0324392.e022.jpg" id="pone.0324392.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the <inline-formula id="pone.0324392.e023"><alternatives><graphic xlink:href="pone.0324392.e023.jpg" id="pone.0324392.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> order derivative of <inline-formula id="pone.0324392.e024"><alternatives><graphic xlink:href="pone.0324392.e024.jpg" id="pone.0324392.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. In image processing, difference of Gaussian (DOG) is a very useful technique for detecting edges and corner points in an image. The difference between these two results is obtained by convolving the image with Gaussian kernels of different standard deviations [<xref rid="pone.0324392.ref025" ref-type="bibr">25</xref>]. DOG calculation is shown in <xref rid="pone.0324392.e025" ref-type="disp-formula">Equation (4)</xref>.</p><disp-formula id="pone.0324392.e025">
<alternatives><graphic xlink:href="pone.0324392.e025.jpg" id="pone.0324392.e025g" position="anchor"/><mml:math id="M25" display="block" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mi>e</mml:mi></mml:mrow></mml:math></alternatives>
<label>(4)</label>
</disp-formula><p>In <xref rid="pone.0324392.e025" ref-type="disp-formula">Equation (4)</xref>, <inline-formula id="pone.0324392.e026"><alternatives><graphic xlink:href="pone.0324392.e026.jpg" id="pone.0324392.e026g" position="anchor"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the variance and <inline-formula id="pone.0324392.e027"><alternatives><graphic xlink:href="pone.0324392.e027.jpg" id="pone.0324392.e027g" position="anchor"/><mml:math id="M27" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes the coordinates. The smaller <inline-formula id="pone.0324392.e028"><alternatives><graphic xlink:href="pone.0324392.e028.jpg" id="pone.0324392.e028g" position="anchor"/><mml:math id="M28" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the higher the peak of the Gaussian function. The larger <inline-formula id="pone.0324392.e029"><alternatives><graphic xlink:href="pone.0324392.e029.jpg" id="pone.0324392.e029g" position="anchor"/><mml:math id="M29" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is, the smoother the peaks are and the blurrier the image convolution effect is. There is a large change in the gray scale (GS) of the image edge, and the gradient value can reflect the drastic degree of GS change, so it is necessary to find the gradient in each direction of the image, and thus determine the edge region of the image. Currently the commonly used operators for edge detection (ED) and extraction are Roberts operator (RO), Prewitt operator (PO), Sobel operator (SO), LOG operator (LOG-O) and Canny algorithm (CA). The fractional gradient operator can finely control the local feature extraction of signals or images by adjusting the differential order, thereby achieving optimal performance in different application scenarios. Compared with integer order differentiation, fractional gradient operators can better capture subtle changes in signals, especially when dealing with complex nonlinear systems. Secondly, the fractional gradient operator also has memory, that is, it not only considers the value of the current point, but also considers the values of all past points. In addition, the fractional gradient operator can more effectively suppress noise while extracting signal features. This is because the fractional gradient operator can balance the smoothness of the signal and the accuracy of feature extraction by adjusting the order, thereby preserving important features of the signal while removing noise. The following is the specific ED process. First, the acquired image is transformed into a GS image, and then noise is removed from the image using Gaussian filtering (GF). Subsequently, an appropriate ED method is chosen to identify the image, and the edges that are identified are binaryized to produce an image with black and white edges. Finally, the detected edges are connected and detailed to output the processed edge image. The steps of image ED are shown in <xref rid="pone.0324392.g001" ref-type="fig">Fig 1</xref>.</p><fig position="float" id="pone.0324392.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g001</object-id><label>Fig 1</label><caption><title>Improved image edge detection algorithm flow based on fractional gradient operator.</title></caption><graphic xlink:href="pone.0324392.g001" position="float"/></fig><p>The RO is a gradient algorithm that is based on cross-differentials and is also referred to as the cross-differential algorithm [<xref rid="pone.0324392.ref026" ref-type="bibr">26</xref>]. The approach is often used to process steep, low-noise images and discovers edge lines through the computation of local differences. When the image edges are near plus or minus 45&#x02009;degrees, the algorithm performs better. When RO performs ED on an image, first the selected operator template is used to operate on the image to be detected and calculate the gradient magnitude value (GAV) of the image. Secondly, a suitable threshold is selected and the GAV of the image is judged point by point. Points that meet the criteria are assigned a value of 1, while the remaining points are assigned a value of 0. Finally, the edge image of the image to be detected is obtained. The bias derivation of RO for <inline-formula id="pone.0324392.e030"><alternatives><graphic xlink:href="pone.0324392.e030.jpg" id="pone.0324392.e030g" position="anchor"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0324392.e031"><alternatives><graphic xlink:href="pone.0324392.e031.jpg" id="pone.0324392.e031g" position="anchor"/><mml:math id="M31" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> direction is shown in <xref rid="pone.0324392.e032" ref-type="disp-formula">Equation (5)</xref>.</p><disp-formula id="pone.0324392.e032">
<alternatives><graphic xlink:href="pone.0324392.e032.jpg" id="pone.0324392.e032g" position="anchor"/><mml:math id="M32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix" stretchy="true"/></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(5)</label>
</disp-formula><p>The LOG-O smooths the image by GF function to suppress the effect of noise [<xref rid="pone.0324392.ref027" ref-type="bibr">27</xref>]. The bias derivation of the LOG-O for the <inline-formula id="pone.0324392.e033"><alternatives><graphic xlink:href="pone.0324392.e033.jpg" id="pone.0324392.e033g" position="anchor"/><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0324392.e034"><alternatives><graphic xlink:href="pone.0324392.e034.jpg" id="pone.0324392.e034g" position="anchor"/><mml:math id="M34" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> directions is shown in <xref rid="pone.0324392.e035" ref-type="disp-formula">Equation (6)</xref>.</p><disp-formula id="pone.0324392.e035">
<alternatives><graphic xlink:href="pone.0324392.e035.jpg" id="pone.0324392.e035g" position="anchor"/><mml:math id="M35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>&#x02207;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">[</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo fence="true" form="postfix" stretchy="true">]</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(6)</label>
</disp-formula><p>In <xref rid="pone.0324392.e035" ref-type="disp-formula">Equation (6)</xref>, <inline-formula id="pone.0324392.e036"><alternatives><graphic xlink:href="pone.0324392.e036.jpg" id="pone.0324392.e036g" position="anchor"/><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the variance and <inline-formula id="pone.0324392.e037"><alternatives><graphic xlink:href="pone.0324392.e037.jpg" id="pone.0324392.e037g" position="anchor"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>&#x000b7;</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes the exponential function. <xref rid="pone.0324392.g002" ref-type="fig">Fig 2</xref> displays the LOG-O&#x02019;s ED step.</p><fig position="float" id="pone.0324392.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g002</object-id><label>Fig 2</label><caption><title>LOG operator edge detection steps.</title></caption><graphic xlink:href="pone.0324392.g002" position="float"/></fig><p>PO is used to achieve ED by using the difference generated by the gray values of the pixels in a particular region. The image is first subjected to a convolution operation in order to extract the gradient values of each pixel point (PP) in both the horizontal and vertical directions. Secondly, the binarized edge image is obtained by thresholding the gradient intensity for segmentation [<xref rid="pone.0324392.ref028" ref-type="bibr">28</xref>]. The ED effects of the PO are more noticeable in the horizontal and vertical dimensions since it employs a 3&#x02009;&#x000d7;&#x02009;3 template for the pixel values in the region. It works well for identifying photographs with a higher level of noise and GS gradient. The bias derivation of PO for <inline-formula id="pone.0324392.e038"><alternatives><graphic xlink:href="pone.0324392.e038.jpg" id="pone.0324392.e038g" position="anchor"/><mml:math id="M38" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0324392.e039"><alternatives><graphic xlink:href="pone.0324392.e039.jpg" id="pone.0324392.e039g" position="anchor"/><mml:math id="M39" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> direction is shown in <xref rid="pone.0324392.e040" ref-type="disp-formula">Equation (7)</xref>.</p><disp-formula id="pone.0324392.e040">
<alternatives><graphic xlink:href="pone.0324392.e040.jpg" id="pone.0324392.e040g" position="anchor"/><mml:math id="M40" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix" stretchy="true"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(7)</label>
</disp-formula><p>The goal of CA is to find an optimal ED solution or to find the location in an image where the gray intensity varies the most [<xref rid="pone.0324392.ref029" ref-type="bibr">29</xref>]. Three factors are mostly used to determine optimal ED: lowest response, high localizability, and low error rate. The specific steps of ED by Canny operator (CO) are as follows. First, the image is smoothed and its gradient is found using GF. Second, double thresholding is employed to determine the possible boundaries and non-maximum suppression technique is utilized to filter out the non-edge pixels. Finally, hysteresis technique is utilized to track the boundaries. <xref rid="pone.0324392.g003" ref-type="fig">Fig 3</xref> depicts the CO ED process.</p><fig position="float" id="pone.0324392.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g003</object-id><label>Fig 3</label><caption><title>Canny operator edge detection steps.</title></caption><graphic xlink:href="pone.0324392.g003" position="float"/></fig><p>Combining differential derivation and GF, the SO is a discrete differential operator [<xref rid="pone.0324392.ref030" ref-type="bibr">30</xref>]. By labeling certain points in the region that exceed a certain number as edges, depending on how bright or dark they are adjacent to the edge of the image, the SO is used to approximate the brightness and darkness of an image. The SO adds the concept of weight to the PO and considers that the proximity of neighboring points has a different effect on the current PP. The image is sharpened and the edge contours are emphasized when the PPs are closer together, since they have a greater effect on the current pixel. The weighted difference of the gray levels of a PP&#x02019;s upper and lower, left and right neighbors reaches an extreme value near the edge, which is how SO determines the edge. The expression of FOD of SO is shown in <xref rid="pone.0324392.e041" ref-type="disp-formula">Equation (8)</xref>.</p><disp-formula id="pone.0324392.e041">
<alternatives><graphic xlink:href="pone.0324392.e041.jpg" id="pone.0324392.e041g" position="anchor"/><mml:math id="M41" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mi>G</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">[</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>&#x02202;</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>&#x02202;</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mo fence="true" form="postfix" stretchy="true"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true"/><mml:mi>a</mml:mi><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>&#x02202;</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>&#x02202;</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo fence="true" form="postfix" stretchy="true">]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(8)</label>
</disp-formula><p>In <xref rid="pone.0324392.e041" ref-type="disp-formula">Equation (8)</xref>, <inline-formula id="pone.0324392.e042"><alternatives><graphic xlink:href="pone.0324392.e042.jpg" id="pone.0324392.e042g" position="anchor"/><mml:math id="M42" display="inline" overflow="scroll"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msqrt><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. <inline-formula id="pone.0324392.e043"><alternatives><graphic xlink:href="pone.0324392.e043.jpg" id="pone.0324392.e043g" position="anchor"/><mml:math id="M43" display="inline" overflow="scroll"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> indicates that the derivative with respect to <inline-formula id="pone.0324392.e044"><alternatives><graphic xlink:href="pone.0324392.e044.jpg" id="pone.0324392.e044g" position="anchor"/><mml:math id="M44" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is of order <inline-formula id="pone.0324392.e045"><alternatives><graphic xlink:href="pone.0324392.e045.jpg" id="pone.0324392.e045g" position="anchor"/><mml:math id="M45" display="inline" overflow="scroll"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. The specific steps for ED of SO are as follows. The original image is first subjected to Gaussian blurring before being transformed into a GS image. Second, the Sobel function is used to get the derivatives in both the x and y axes. The derivatives in the x and y directions are finally overlaid. The rate of change of the image&#x02019;s GS is reflected by spatial frequency (SF), and a larger SF suggests a clearer image and higher-quality fused image. The FO SF is shown in <xref rid="pone.0324392.e046" ref-type="disp-formula">Equation (9)</xref>.</p><disp-formula id="pone.0324392.e046">
<alternatives><graphic xlink:href="pone.0324392.e046.jpg" id="pone.0324392.e046g" position="anchor"/><mml:math id="M46" display="block" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>F</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mi>F</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives>
<label>(9)</label>
</disp-formula><p>In <xref rid="pone.0324392.e046" ref-type="disp-formula">Equation (9)</xref>, <inline-formula id="pone.0324392.e047"><alternatives><graphic xlink:href="pone.0324392.e047.jpg" id="pone.0324392.e047g" position="anchor"/><mml:math id="M47" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes FO SF. <inline-formula id="pone.0324392.e048"><alternatives><graphic xlink:href="pone.0324392.e048.jpg" id="pone.0324392.e048g" position="anchor"/><mml:math id="M48" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes FO row frequency and <inline-formula id="pone.0324392.e049"><alternatives><graphic xlink:href="pone.0324392.e049.jpg" id="pone.0324392.e049g" position="anchor"/><mml:math id="M49" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes FO column frequency. To accurately refine the wide edges of the gradient amplitude image, it is necessary to perform non-maximum suppression on the gradient image and remove some non-edge points. The direction of the gradient is discriminated into four directions: 1, 2, 3, and 4, and replace them with directions. Map the gradient direction of the pixel to the nearest direction. Map the gradient direction of the pixel to the nearest direction. If the gradient direction of the central pixel belongs to the second direction and is greater than the average gray value and the maximum gray value of the gradient image, then the pixel is determined to be a local maximum point. In addition, to determine the appropriate threshold, the Otsu algorithm is used to calculate the threshold. This method traverses the gray values of the entire image and selects the gray value corresponding to the maximum inter-class variance as the threshold. The formula for calculating the inter-class variance is shown in <xref rid="pone.0324392.e050" ref-type="disp-formula">Equation (10)</xref>.</p><disp-formula id="pone.0324392.e050">
<alternatives><graphic xlink:href="pone.0324392.e050.jpg" id="pone.0324392.e050g" position="anchor"/><mml:math id="M50" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi>&#x003c3;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>u</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>u</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix" stretchy="true"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(10)</label>
</disp-formula><p>In <xref rid="pone.0324392.e050" ref-type="disp-formula">Equation (10)</xref>, <inline-formula id="pone.0324392.e051"><alternatives><graphic xlink:href="pone.0324392.e051.jpg" id="pone.0324392.e051g" position="anchor"/><mml:math id="M51" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> represents inter class variance. <inline-formula id="pone.0324392.e052"><alternatives><graphic xlink:href="pone.0324392.e052.jpg" id="pone.0324392.e052g" position="anchor"/><mml:math id="M52" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> represents the probability that the grayscale is less than the segmentation threshold. <inline-formula id="pone.0324392.e053"><alternatives><graphic xlink:href="pone.0324392.e053.jpg" id="pone.0324392.e053g" position="anchor"/><mml:math id="M53" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> represents the average grayscale of images with grayscale less than the segmentation threshold. <inline-formula id="pone.0324392.e054"><alternatives><graphic xlink:href="pone.0324392.e054.jpg" id="pone.0324392.e054g" position="anchor"/><mml:math id="M54" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> represents the average grayscale of images with grayscale greater than the segmentation threshold. <inline-formula id="pone.0324392.e055"><alternatives><graphic xlink:href="pone.0324392.e055.jpg" id="pone.0324392.e055g" position="anchor"/><mml:math id="M55" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> represents the probability that the grayscale is greater than the segmentation threshold. <inline-formula id="pone.0324392.e056"><alternatives><graphic xlink:href="pone.0324392.e056.jpg" id="pone.0324392.e056g" position="anchor"/><mml:math id="M56" display="inline" overflow="scroll"><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> represents the total average grayscale of the image.</p></sec><sec id="sec005"><title>3.2. FRWT-based image fusion</title><p>The study of image ED based on FOD followed by IF based on FRWT is able to accurately extract edge information. WT is a brand-new transform analysis technique that is perfect for processing and analyzing signal TF. While FRWT is a multi-resolution analysis of signal TF domain. Performing FRWT on a signal is equivalent to passing the signal through an FO filter bank consisting of a scale function and wavelet functions with different scales to obtain the LF portion of the signal and the high-frequency (HF) portion of the signal in different frequency bands [<xref rid="pone.0324392.ref031" ref-type="bibr">31</xref>&#x02013;<xref rid="pone.0324392.ref032" ref-type="bibr">32</xref>]. Its primary capabilities include the ability to fully highlight the features of specific problem aspects through transformation and the ability to localize time and SF analysis. The method eventually accomplishes time subdivision at HF and frequency subdivision at LF, which can automatically adjust to the requirements of TF signal analysis, thereby focusing on arbitrary features of the signal. The signal is gradually refined at numerous scales by telescopic translation operations. Compared with the traditional WT, it has better resolution, stronger localization and better anti-noise performance, especially suitable for processing signals with complex spatio-temporal features. The image processing based on FRWT is shown in <xref rid="pone.0324392.g004" ref-type="fig">Fig 4</xref>.</p><fig position="float" id="pone.0324392.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g004</object-id><label>Fig 4</label><caption><title>Image based on fractional wavelet transform.</title></caption><graphic xlink:href="pone.0324392.g004" position="float"/></fig><p>In WT theory, DWT and continuous wavelet transform (CWT) are the two main transformations [<xref rid="pone.0324392.ref033" ref-type="bibr">33</xref>]. The signal is transformed in the discrete time or space domain by DWT, whereas the signal is transformed in the continuous time or space domain by CWT. The discretized WT performs and is more stable in real time [<xref rid="pone.0324392.ref034" ref-type="bibr">34</xref>&#x02013;<xref rid="pone.0324392.ref035" ref-type="bibr">35</xref>]. The basic principle of DWT is to discretize the CWT, which can decompose the signal or image into components of different frequencies [<xref rid="pone.0324392.ref036" ref-type="bibr">36</xref>]. DWT has the ability to analyze both time and frequency domains. Compared with general pyramid decomposition, DWT image decomposition can obtain LF information in horizontal, vertical and diagonal directions while extracting LF information of the image. By choosing the mother wavelet reasonably, DWT can make DWT more effective in extracting significant information such as texture and edge while compressing noise. Pyramid decomposition has information correlation between scales, while DWT has higher independence in different scales. For IF based on DWT, firstly, WT is performed on the source image. Secondly, the transformed coefficients are merged according to certain rules. Ultimately, the fused image is obtained by applying the wavelet inverse transform to the joined coefficients. In <xref rid="pone.0324392.g005" ref-type="fig">Fig 5</xref>, the IF based on DWT is displayed.</p><fig position="float" id="pone.0324392.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g005</object-id><label>Fig 5</label><caption><title>Image fusion based on discrete wavelet transform.</title></caption><graphic xlink:href="pone.0324392.g005" position="float"/></fig><p>WT is a representation of the original signal (OS) using a scale function. The scale grows larger and larger as the scale level drops, representing the OS in an increasingly erratic and hazy manner while also increasing the gap between the OS. It is used as a scale function to meet the requirements that the scale function is orthogonal to its integer translation, and that the function space grown at low scales is nested within high scales. The scale function expression is shown in <xref rid="pone.0324392.e057" ref-type="disp-formula">Equation (11)</xref>.</p><disp-formula id="pone.0324392.e057">
<alternatives><graphic xlink:href="pone.0324392.e057.jpg" id="pone.0324392.e057g" position="anchor"/><mml:math id="M57" display="block" overflow="scroll"><mml:mrow><mml:mi>&#x003c6;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msqrt><mml:mi>&#x003c6;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mn>2</mml:mn><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>k</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(11)</label>
</disp-formula><p>In Equation (11), <inline-formula id="pone.0324392.e058"><alternatives><graphic xlink:href="pone.0324392.e058.jpg" id="pone.0324392.e058g" position="anchor"/><mml:math id="M58" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0324392.e059"><alternatives><graphic xlink:href="pone.0324392.e059.jpg" id="pone.0324392.e059g" position="anchor"/><mml:math id="M59" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> determines the position of <inline-formula id="pone.0324392.e060"><alternatives><graphic xlink:href="pone.0324392.e060.jpg" id="pone.0324392.e060g" position="anchor"/><mml:math id="M60" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c6;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. <inline-formula id="pone.0324392.e061"><alternatives><graphic xlink:href="pone.0324392.e061.jpg" id="pone.0324392.e061g" position="anchor"/><mml:math id="M61" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c6;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes the scale function and <inline-formula id="pone.0324392.e062"><alternatives><graphic xlink:href="pone.0324392.e062.jpg" id="pone.0324392.e062g" position="anchor"/><mml:math id="M62" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes the expansion coefficient. The relationship between scale space and wavelet space is shown in Equation (12).</p><disp-formula id="pone.0324392.e063">
<alternatives><graphic xlink:href="pone.0324392.e063.jpg" id="pone.0324392.e063g" position="anchor"/><mml:math id="M63" display="block" overflow="scroll"><mml:mrow><mml:mi>&#x003c8;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msqrt><mml:mi>&#x003c6;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mn>2</mml:mn><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>k</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(12)</label>
</disp-formula><p>In Equation (12), <inline-formula id="pone.0324392.e064"><alternatives><graphic xlink:href="pone.0324392.e064.jpg" id="pone.0324392.e064g" position="anchor"/><mml:math id="M64" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c8;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes the wavelet function and <inline-formula id="pone.0324392.e065"><alternatives><graphic xlink:href="pone.0324392.e065.jpg" id="pone.0324392.e065g" position="anchor"/><mml:math id="M65" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes the wavelet function coefficients. In the discrete case, the approximation coefficients are calculated in Equation (13).</p><disp-formula id="pone.0324392.e066">
<alternatives><graphic xlink:href="pone.0324392.e066.jpg" id="pone.0324392.e066g" position="anchor"/><mml:math id="M66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mi>k</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(13)</label>
</disp-formula><p>In Equation (13), <inline-formula id="pone.0324392.e067"><alternatives><graphic xlink:href="pone.0324392.e067.jpg" id="pone.0324392.e067g" position="anchor"/><mml:math id="M67" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> represents the approximation coefficient. <inline-formula id="pone.0324392.e068"><alternatives><graphic xlink:href="pone.0324392.e068.jpg" id="pone.0324392.e068g" position="anchor"/><mml:math id="M68" display="inline" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0324392.e069"><alternatives><graphic xlink:href="pone.0324392.e069.jpg" id="pone.0324392.e069g" position="anchor"/><mml:math id="M69" display="inline" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> determines the width and height, and <inline-formula id="pone.0324392.e070"><alternatives><graphic xlink:href="pone.0324392.e070.jpg" id="pone.0324392.e070g" position="anchor"/><mml:math id="M70" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. The fine coefficients are calculated in Equation (14).</p><disp-formula id="pone.0324392.e071">
<alternatives><graphic xlink:href="pone.0324392.e071.jpg" id="pone.0324392.e071g" position="anchor"/><mml:math id="M71" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mi>k</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(14)</label>
</disp-formula><p>In Equation (14), <inline-formula id="pone.0324392.e072"><alternatives><graphic xlink:href="pone.0324392.e072.jpg" id="pone.0324392.e072g" position="anchor"/><mml:math id="M72" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes the fine coefficient. Bootstrap filtering is an image filtering technique that can achieve edge smoothing with double filtering by filtering the initial image with one bootstrap image. The bootstrap filtering uses one bootstrap image to generate weights, which are processed on the input image. This process expression is shown in Equation (15).</p><disp-formula id="pone.0324392.e073">
<alternatives><graphic xlink:href="pone.0324392.e073.jpg" id="pone.0324392.e073g" position="anchor"/><mml:math id="M73" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>I</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mi>&#x000b7;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(15)</label>
</disp-formula><p>In Equation (15), <inline-formula id="pone.0324392.e074"><alternatives><graphic xlink:href="pone.0324392.e074.jpg" id="pone.0324392.e074g" position="anchor"/><mml:math id="M74" display="inline" overflow="scroll"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the output image. <inline-formula id="pone.0324392.e075"><alternatives><graphic xlink:href="pone.0324392.e075.jpg" id="pone.0324392.e075g" position="anchor"/><mml:math id="M75" display="inline" overflow="scroll"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the bootstrap image. <inline-formula id="pone.0324392.e076"><alternatives><graphic xlink:href="pone.0324392.e076.jpg" id="pone.0324392.e076g" position="anchor"/><mml:math id="M76" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the input image. <inline-formula id="pone.0324392.e077"><alternatives><graphic xlink:href="pone.0324392.e077.jpg" id="pone.0324392.e077g" position="anchor"/><mml:math id="M77" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0324392.e078"><alternatives><graphic xlink:href="pone.0324392.e078.jpg" id="pone.0324392.e078g" position="anchor"/><mml:math id="M78" display="inline" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> are the index of the PP in the image. <inline-formula id="pone.0324392.e079"><alternatives><graphic xlink:href="pone.0324392.e079.jpg" id="pone.0324392.e079g" position="anchor"/><mml:math id="M79" display="inline" overflow="scroll"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the weights. The bootstrap filtering implementation steps are shown in <xref rid="pone.0324392.g006" ref-type="fig">Fig 6</xref>.</p><fig position="float" id="pone.0324392.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g006</object-id><label>Fig 6</label><caption><title>Steps for implementing guided filtering.</title></caption><graphic xlink:href="pone.0324392.g006" position="float"/></fig><p>As shown in <xref rid="pone.0324392.g006" ref-type="fig">Fig 6</xref>, firstly, the boxFilter filter is used to complete the correlation coefficient parameter (CCP). The mean value (MV) includes autocorrelation MV, cross-correlation MV, bootstrap image MV, and the original image MV that needs to be filtered. CCP calculates based on MV, including autocorrelation variance (var) and cross-correlation covariance (cov). Secondly, calculate the coefficients of the window linear transformation parameters a and b. The MV of parameters a and b is calculated according to the formula. Finally, use these parameters to obtain the output image of the bootstrap filter. The regions of the image where the brightness or gray value varies slowly are represented by the LF component. This expansive, level region of the picture characterizes the majority of the image and provides a thorough gauge of the overall intensity of the image. Equation (16) illustrates the fusion of the LF coefficient.</p><disp-formula id="pone.0324392.e080">
<alternatives><graphic xlink:href="pone.0324392.e080.jpg" id="pone.0324392.e080g" position="anchor"/><mml:math id="M80" display="block" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>*</mml:mo><mml:mi>L</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>*</mml:mo><mml:mi>L</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(16)</label>
</disp-formula><p>In Equation (16), <inline-formula id="pone.0324392.e081"><alternatives><graphic xlink:href="pone.0324392.e081.jpg" id="pone.0324392.e081g" position="anchor"/><mml:math id="M81" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0324392.e082"><alternatives><graphic xlink:href="pone.0324392.e082.jpg" id="pone.0324392.e082g" position="anchor"/><mml:math id="M82" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denote the weighting coefficients. <inline-formula id="pone.0324392.e083"><alternatives><graphic xlink:href="pone.0324392.e083.jpg" id="pone.0324392.e083g" position="anchor"/><mml:math id="M83" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0324392.e084"><alternatives><graphic xlink:href="pone.0324392.e084.jpg" id="pone.0324392.e084g" position="anchor"/><mml:math id="M84" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denote the LF coefficients. <inline-formula id="pone.0324392.e085"><alternatives><graphic xlink:href="pone.0324392.e085.jpg" id="pone.0324392.e085g" position="anchor"/><mml:math id="M85" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the fused LF coefficient. The areas of the image that fluctuate significantly, such as the borders, noise, and detail, are corresponding to the HF components. HF coefficient fusion first calculates the absolute value of the HF coefficients, and second selects the HF coefficients with the largest absolute value at each position as the fusion result. The specific steps of FRWT based IF are as follows. Firstly, the input images are pre-processed, such as gray-scaling and normalization. Then DFRWT decomposition is performed on the two input images to get the LF coefficients and LF coefficients of the images. Secondly, the LF coefficients are weighted and fused. The weights can be determined according to the image quality, information content and other factors. For example, for better quality images, higher weights can be assigned. Maximum value fusion is performed for LF coefficients. That is, the HF coefficients with the largest absolute value at each position are selected as the fusion result. Finally, the fused LF coefficients and HF coefficients are inverted by DFRWT to obtain the fused image. The specific steps of FRWT-based IF are shown in <xref rid="pone.0324392.g007" ref-type="fig">Fig 7</xref>.</p><fig position="float" id="pone.0324392.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g007</object-id><label>Fig 7</label><caption><title>Image fusion based on fractional wavelet transform.</title></caption><graphic xlink:href="pone.0324392.g007" position="float"/></fig></sec></sec><sec id="sec006"><title>4. Quality analysis of image edge detection and fusion</title><sec id="sec007"><title>4.1. Experimental parameter setting</title><p>The study employs Sobel algorithm (SA), Roberts algorithm (RA), LOG algorithm (LOG-A), Prewitt algorithm (PA) and CA for three sets of image ED simulation experiments. Considering the diversity and representativeness of the dataset, as well as the comprehensiveness of performance evaluation, the selected datasets for the experiment include Labeled Faces in the Wild (LFW) dataset, Kaggle PCB defect dataset, and Rice image dataset. Compared to the datasets used in other literature, experiments conducted on the aforementioned three datasets can verify the applicability of the algorithm in various practical applications. For example, edge detection of facial images can be applied to facial recognition and expression analysis; edge detection of circuit board images can be used for industrial quality control; edge detection of rice images can be utilized for image analysis in the agricultural field. To test the generalization performance of the proposed algorithm, the LFW dataset and Kaggle PCB Defects dataset will be used for training, while the Rice Image dataset will be used as an independent dataset to test the algorithm&#x02019;s performance. The LFW dataset is used to test the edge detection performance of the proposed method, the Kaggle PCB dataset is used to verify the edge detection effect of the research algorithm on images with rich edges, and the Rice Image dataset is used to verify the algorithm&#x02019;s noise resistance performance. Among them, Experiment 1 performs ED on face images. Experiment 2 performs ED on complex circuit board images. Experiment 3 performs ED on rice grain images with 0.05 pretzel noise added. Fifteen sets of images of size 512*512 are selected for fusion in the study. The three methods NSCT method, PNCC method and L_P method are also selected for comparison and analysis with the FRWT method used in the study. The operating system chosen for the experiment is Windows 11X64, RAM is 16GB, CPU is 12th Gen Intel(R) Core(TM) i7-12700H @ 2.30GHz, the graphics card model is NVIDIA GeForce RTX 3060, and the graphics card memory is 16G, and the software chosen is MATLAB R2018a. FO P takes the values of 0.4 and 0.5, respectively.</p></sec><sec id="sec008"><title>4.2. Quality analysis of image edge detection based on fod</title><p>The study employs RA, SA, PA, LOG-A and CA to perform ED on three images with different levels of complexity. Among them, the image structure of Experiment 1 is more complex, the image of Experiment 2 has more complex textures at the edges, and the image structure of Experiment 3 is simpler. B is the eligible edge points and A is the total detected points. The results of ED are shown in <xref rid="pone.0324392.t001" ref-type="table">Table 1</xref>. The B/A value of SA is the smallest in all the three sets of experiments and ED has the highest precision rate. When the detected image structure is more complex, the B/A value of SA is the smallest and the B/A value of PA is the highest, which are 0.0143, 0.0915 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05), respectively. When the image edges have more complex textures, the B/A value of SA is the smallest and the B/A value of RA is the highest, which are 0.0219, 0.3039 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05), respectively. When the detected image structure is simple, SA has the smallest B/A value and PA has the highest B/A value, which is 0.0146, 0.0270 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05), respectively.</p><table-wrap position="float" id="pone.0324392.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.t001</object-id><label>Table 1</label><caption><title>Quantitative analysis results of edge detection.</title></caption><alternatives><graphic xlink:href="pone.0324392.t001" id="pone.0324392.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Image Name</th><th align="left" rowspan="1" colspan="1">B/A of PA</th><th align="left" rowspan="1" colspan="1">B/A of RA</th><th align="left" rowspan="1" colspan="1">B/A of LOG-A</th><th align="left" rowspan="1" colspan="1">B/A of CA</th><th align="left" rowspan="1" colspan="1">B/A of SA</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Experiment 1</td><td align="left" rowspan="1" colspan="1">0.0915</td><td align="left" rowspan="1" colspan="1">0.0880</td><td align="left" rowspan="1" colspan="1">0.0652</td><td align="left" rowspan="1" colspan="1">0.0248*</td><td align="left" rowspan="1" colspan="1">0.0143*</td></tr><tr><td align="left" rowspan="1" colspan="1">Experiment 2</td><td align="left" rowspan="1" colspan="1">0.2934*</td><td align="left" rowspan="1" colspan="1">0.3039*</td><td align="left" rowspan="1" colspan="1">0.0868*</td><td align="left" rowspan="1" colspan="1">0.0270*</td><td align="left" rowspan="1" colspan="1">0.0219*</td></tr><tr><td align="left" rowspan="1" colspan="1">Experiment 3</td><td align="left" rowspan="1" colspan="1">0.0270*</td><td align="left" rowspan="1" colspan="1">0.0222*</td><td align="left" rowspan="1" colspan="1">0.0224*</td><td align="left" rowspan="1" colspan="1">0.0152*</td><td align="left" rowspan="1" colspan="1">0.0146*</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t001fn001"><p>Note: * indicates that <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05.</p></fn></table-wrap-foot></table-wrap><p>The execution efficiencies of RA, SA, PA, LOG-A and CA are shown in <xref rid="pone.0324392.t002" ref-type="table">Table 2</xref> when the complexity of the detected images is different. When the structure of the detected image is more complex, the RA has the lowest execution efficiency and the CA has the highest execution efficiency of 0.0497 and 0.0518 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05), respectively. When the image edges have more complex textures, the PA is executed with the lowest efficiency and the CA is executed with the highest efficiency with 0.0598 and 0.0801 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05), respectively. When the structure of the detected image is relatively simple, the PA is executed with the lowest efficiency and the CA is executed with the highest efficiency with 0.0363 and 0.0492 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05), respectively. Overall, the performance of the algorithm proposed in the study on independent datasets is not significantly different from that of the LFW dataset and Kaggle PCB Defects dataset, indicating that the model has good generalization performance.</p><table-wrap position="float" id="pone.0324392.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.t002</object-id><label>Table 2</label><caption><title>Comparison of algorithm execution efficiency.</title></caption><alternatives><graphic xlink:href="pone.0324392.t002" id="pone.0324392.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Image Name</th><th align="left" rowspan="1" colspan="1">PA</th><th align="left" rowspan="1" colspan="1">RA</th><th align="left" rowspan="1" colspan="1">LOG-A</th><th align="left" rowspan="1" colspan="1">CA</th><th align="left" rowspan="1" colspan="1">SA</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Experiment 1</td><td align="left" rowspan="1" colspan="1">0.0505</td><td align="left" rowspan="1" colspan="1">0.0497</td><td align="left" rowspan="1" colspan="1">0.0513*</td><td align="left" rowspan="1" colspan="1">0.0518*</td><td align="left" rowspan="1" colspan="1">0.0500</td></tr><tr><td align="left" rowspan="1" colspan="1">Experiment 2</td><td align="left" rowspan="1" colspan="1">0.0598*</td><td align="left" rowspan="1" colspan="1">0.0614*</td><td align="left" rowspan="1" colspan="1">0.0698*</td><td align="left" rowspan="1" colspan="1">0.0801*</td><td align="left" rowspan="1" colspan="1">0.0739*</td></tr><tr><td align="left" rowspan="1" colspan="1">Experiment 3</td><td align="left" rowspan="1" colspan="1">0.0363*</td><td align="left" rowspan="1" colspan="1">0.0379*</td><td align="left" rowspan="1" colspan="1">0.0489*</td><td align="left" rowspan="1" colspan="1">0.0492*</td><td align="left" rowspan="1" colspan="1">0.0472*</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t002fn001"><p>Note: * indicates that <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05.</p></fn></table-wrap-foot></table-wrap><p>The study adopts two evaluation metrics, mean square error (MSE) and precision rate to evaluate the effect of image ED. In this, the study detects different representative regions of added noise image. The precision rate denotes the probability that the boundary pixels generated in ED are the real boundary pixels and its statistical results are shown in <xref rid="pone.0324392.g008" ref-type="fig">Fig 8</xref>. In <xref rid="pone.0324392.g008" ref-type="fig">Fig 8(a)</xref>, with the increase of representative regions, the precision rate of image ED using various techniques exhibits an increasing and subsequently declining trend. When 1&#x02013;5 regions are selected as representative regions, the precision rate of image ED by different methods shows a gradual increasing trend. Compared with other methods, SO has the highest detection precision rate and PO has the lowest detection precision rate. The highest detection precision rate is 78.9% for SO and 66.3% (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05) for PO. In <xref rid="pone.0324392.g008" ref-type="fig">Fig 8(b)</xref>, the MSE curves of different operators for image ED fluctuate little as the representative region increases. SA has the smallest MSE and PA has the largest MSE, with SA&#x02019;s MSE range being [775,850] and PA&#x02019;s MSE range being [152,200] (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05).</p><fig position="float" id="pone.0324392.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g008</object-id><label>Fig 8</label><caption><title>Image edge detection MSE with accuracy for different operators.</title></caption><graphic xlink:href="pone.0324392.g008" position="float"/></fig><p>The study adopts two evaluation metrics, information entropy and recall, to evaluate the image ED effect. Among them, the recall rate indicates the probability of detecting a real boundary pixel over all real boundary pixels. The statistical results are shown in <xref rid="pone.0324392.g009" ref-type="fig">Fig 9</xref>. In <xref rid="pone.0324392.g009" ref-type="fig">Fig 9(a)</xref>, the recall of different methods for image ED increases with the increase of representative regions. When 5&#x02013;7 regions are selected as representative regions, the fluctuation of the recall curve of different methods for image ED is small. Compared with other methods, SO has the highest detection recall and RO has the lowest detection recall. The highest detection recall is 80% for SO and 67.9% (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05) for PO. In <xref rid="pone.0324392.g009" ref-type="fig">Fig 9(b)</xref>, the information entropy curves of different operators for image ED fluctuate less as the representative region increases. SA has the minimum information entropy and PA has the maximum information entropy. The minimum information entropy of SA and PA is 3.13 and 5.61 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05), respectively. The maximum information entropy of SA and PA is 3.99 and 5.98 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05), respectively.</p><fig position="float" id="pone.0324392.g009"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g009</object-id><label>Fig 9</label><caption><title>Information entropy and recall of image edge detection by different operators.</title></caption><graphic xlink:href="pone.0324392.g009" position="float"/></fig><p>To further analyze the performance of the proposed edge detection algorithm, it is compared with the Triple B-spline wavelet transform and Franklin moment (TBSWT-FM) and the improved canny operator (ICO). The test results are shown in <xref rid="pone.0324392.t003" ref-type="table">Table 3</xref>.</p><table-wrap position="float" id="pone.0324392.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.t003</object-id><label>Table 3</label><caption><title>B/A of different edge detection algorithms.</title></caption><alternatives><graphic xlink:href="pone.0324392.t003" id="pone.0324392.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Image Name</th><th align="left" rowspan="1" colspan="1">TBSWT-FM</th><th align="left" rowspan="1" colspan="1">ICO</th><th align="left" rowspan="1" colspan="1">Our Method</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Experiment 1</td><td align="left" rowspan="1" colspan="1">0.036*</td><td align="left" rowspan="1" colspan="1">0.041*</td><td align="left" rowspan="1" colspan="1">0.015*</td></tr><tr><td align="left" rowspan="1" colspan="1">Experiment 2</td><td align="left" rowspan="1" colspan="1">0.042*</td><td align="left" rowspan="1" colspan="1">0.039*</td><td align="left" rowspan="1" colspan="1">0.022*</td></tr><tr><td align="left" rowspan="1" colspan="1">Experiment 3</td><td align="left" rowspan="1" colspan="1">0.033*</td><td align="left" rowspan="1" colspan="1">0.028*</td><td align="left" rowspan="1" colspan="1">0.015*</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t003fn001"><p>Note: * indicates that <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;0.05.</p></fn></table-wrap-foot></table-wrap><p>According to <xref rid="pone.0324392.t003" ref-type="table">Table 3</xref>, compared to other algorithms, the proposed edge detection algorithm has a smaller B/A ratio, which does not exceed 0.022 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05). It can be concluded that the proposed edge detection algorithm in this study has higher accuracy and higher edge integrity. The effects of edge detection for the different datasets are shown in Figure <xref rid="pone.0324392.g010" ref-type="fig">Fig 10</xref>.</p><fig position="float" id="pone.0324392.g010"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g010</object-id><label>Fig 10</label><caption><title>Edge detection effects for different datasets.</title></caption><graphic xlink:href="pone.0324392.g010" position="float"/></fig><p>As shown in <xref rid="pone.0324392.g010" ref-type="fig">Fig 10</xref>, compared to other algorithms, the proposed method not only achieves accurate edge detection of ordinary images, but also improves the edge detection effect of images with rich edges. In addition, this method has good resistance to salt and pepper noise, and its noise profile is significantly reduced during the detection process.</p></sec><sec id="sec009"><title>4.3. FRWT-based quality analysis of image fusion</title><p>To verify the performance of the algorithm proposed in the study, five typical methods are selected for comparative analysis with the proposed method. Three sets of multi focus images with a size of 512*512 are selected for fusion in the experiment, with fractional order p values of 0.4 and 0.5. The quantitative evaluation indicators based on WT image fusion are compared with existing methods, and the statistical results are shown in <xref rid="pone.0324392.t004" ref-type="table">Table 4</xref>. Among them, MI represents the mutual information between two random variables, SF represents SF, and QAB/F represents the degree of image edge preservation. Observing <xref rid="pone.0324392.t004" ref-type="table">Table 4</xref>, it can be seen that when the fractional order p&#x02009;=&#x02009;0.4, the MI, QAB/F, SF, and values of the WT based image fusion method are the best, which are 5.0183, 0.6981, and 12.6585, respectively. The image fusion methods based on L_P have the lowest values of MI, QAB/F, SF, and are 4.6235, 0.5817, and 7.1638, respectively. When the fractional order p&#x02009;=&#x02009;0.5, the MI, QAB/F, SF, and values of the WT based image fusion method are optimal, with values of 5.1205, 0.7405, and 9.0975, respectively. The image fusion methods based on L_P have the lowest values of MI, QAB/F, SF, and are 4.2372, 0.4987, and 5.1348, respectively.</p><table-wrap position="float" id="pone.0324392.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.t004</object-id><label>Table 4</label><caption><title>The image fusion quality of six algorithm.</title></caption><alternatives><graphic xlink:href="pone.0324392.t004" id="pone.0324392.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1">Fusion method</th><th align="left" colspan="3" rowspan="1">P&#x02009;=&#x02009;0.4</th><th align="left" colspan="3" rowspan="1">P&#x02009;=&#x02009;0.5</th><th align="left" rowspan="1" colspan="1">Reference</th></tr><tr><th align="left" rowspan="1" colspan="1">MI</th><th align="left" rowspan="1" colspan="1">QAB/F</th><th align="left" rowspan="1" colspan="1">SF</th><th align="left" rowspan="1" colspan="1">MI</th><th align="left" rowspan="1" colspan="1">QAB/F</th><th align="left" rowspan="1" colspan="1">SF</th><th align="left" rowspan="1" colspan="1"/></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">FRWT</td><td align="left" rowspan="1" colspan="1">5.018</td><td align="left" rowspan="1" colspan="1">0.698*</td><td align="left" rowspan="1" colspan="1">12.659*</td><td align="left" rowspan="1" colspan="1">5.121*</td><td align="left" rowspan="1" colspan="1">0.741*</td><td align="left" rowspan="1" colspan="1">9.098*</td><td align="left" rowspan="1" colspan="1">/</td></tr><tr><td align="left" rowspan="1" colspan="1">NSCT</td><td align="left" rowspan="1" colspan="1">4.923</td><td align="left" rowspan="1" colspan="1">0.681</td><td align="left" rowspan="1" colspan="1">9.033*</td><td align="left" rowspan="1" colspan="1">4.814*</td><td align="left" rowspan="1" colspan="1">0.675*</td><td align="left" rowspan="1" colspan="1">7.976*</td><td align="left" rowspan="1" colspan="1">Lawrance N et al [<xref rid="pone.0324392.ref037" ref-type="bibr">37</xref>]</td></tr><tr><td align="left" rowspan="1" colspan="1">WT</td><td align="left" rowspan="1" colspan="1">4.823</td><td align="left" rowspan="1" colspan="1">0.611*</td><td align="left" rowspan="1" colspan="1">8.620*</td><td align="left" rowspan="1" colspan="1">4.743*</td><td align="left" rowspan="1" colspan="1">0.654*</td><td align="left" rowspan="1" colspan="1">7.688*</td><td align="left" rowspan="1" colspan="1">Pramanik S. et al [<xref rid="pone.0324392.ref031" ref-type="bibr">31</xref>]</td></tr><tr><td align="left" rowspan="1" colspan="1">DWT</td><td align="left" rowspan="1" colspan="1">4.767</td><td align="left" rowspan="1" colspan="1">0.602*</td><td align="left" rowspan="1" colspan="1">7.598*</td><td align="left" rowspan="1" colspan="1">4.533*</td><td align="left" rowspan="1" colspan="1">0.520*</td><td align="left" rowspan="1" colspan="1">5.983*</td><td align="left" rowspan="1" colspan="1">R. Ahmadian et al [<xref rid="pone.0324392.ref033" ref-type="bibr">33</xref>]</td></tr><tr><td align="left" rowspan="1" colspan="1">PNCC</td><td align="left" rowspan="1" colspan="1">4.823</td><td align="left" rowspan="1" colspan="1">0.611*</td><td align="left" rowspan="1" colspan="1">8.620*</td><td align="left" rowspan="1" colspan="1">4.743*</td><td align="left" rowspan="1" colspan="1">0.654*</td><td align="left" rowspan="1" colspan="1">7.688*</td><td align="left" rowspan="1" colspan="1">KP B et al [<xref rid="pone.0324392.ref038" ref-type="bibr">38</xref>]</td></tr><tr><td align="left" rowspan="1" colspan="1">L_P</td><td align="left" rowspan="1" colspan="1">4.767</td><td align="left" rowspan="1" colspan="1">0.602*</td><td align="left" rowspan="1" colspan="1">7.598*</td><td align="left" rowspan="1" colspan="1">4.533*</td><td align="left" rowspan="1" colspan="1">0.520*</td><td align="left" rowspan="1" colspan="1">5.983*</td><td align="left" rowspan="1" colspan="1">Song M et al [<xref rid="pone.0324392.ref039" ref-type="bibr">39</xref>]</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t004fn001"><p>Note: * indicates that <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05.</p></fn></table-wrap-foot></table-wrap><p>The average gradient, information entropy of the method used in the study for IF is compared with other methods in case of different groups of images. In <xref rid="pone.0324392.g010" ref-type="fig">Fig 10</xref>, the statistical data are displayed. In <xref rid="pone.0324392.g011" ref-type="fig">Fig 11(a)</xref>, the average gradient of the method adopted by the study for IF is higher than the other methods in the case of different image groups. The higher the average gradient, the more image layers and the clearer the image. FRWT, PNCC, NSCT and L_P methods have the highest average gradient of 3.75, 3.19, 3.28 and 2.91 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05), respectively. In <xref rid="pone.0324392.g011" ref-type="fig">Fig 11(b)</xref>, the information entropy of the method used in the study for IF is lower than the other methods in the case of different image groups. The lesser the information entropy, the lesser the IF. FRWT, PNCC, NSCT and L_P methods have the least information entropy of 5.61, 6.59, 7.50 and 8.10 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05), respectively.</p><fig position="float" id="pone.0324392.g011"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g011</object-id><label>Fig 11</label><caption><title>Average gradient and information entropy for different image fusion methods.</title></caption><graphic xlink:href="pone.0324392.g011" position="float"/></fig><p>In the case of different groups of images, the study used the method for IF of SF, mutual information is compared with other methods. In <xref rid="pone.0324392.g011" ref-type="fig">Fig 11</xref>, the statistical data are displayed. In <xref rid="pone.0324392.g012" ref-type="fig">Fig 12(a)</xref>, the space of the method adopted by the study for IF is higher than the other methods in the case of different image groups. The image&#x02019;s GS&#x02019;s rate of change is reflected in the SF. Greater clarity and improved fused image quality are indicated by the larger SF. The highest SF is 6.820, 6.7836.805 and 6.748 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05) for FRWT, PNCC, NSCT and L_P methods, respectively. In <xref rid="pone.0324392.g012" ref-type="fig">Fig 12(b)</xref>, the mutual information of the methods used in the study for IF is lower than the other methods in case of different image groups. FRWT, PNCC, NSCT and L_P methods have minimum mutual information of 0.630.78, 0.94 and 1.14 (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05), respectively.</p><fig position="float" id="pone.0324392.g012"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g012</object-id><label>Fig 12</label><caption><title>The spatial frequency and mutual information of the different image fusion methods.</title></caption><graphic xlink:href="pone.0324392.g012" position="float"/></fig><p>The bootstrap filtering determines the size of the filtering window. With different filter radii, the study used the metric SF to evaluate the effect of fusion of different five sets of images. The statistical results are shown in <xref rid="pone.0324392.g013" ref-type="fig">Fig 13</xref>. SF decreases with increasing filter radius. When the filter radius is greater than 6, the SF converges gradually. When the filter radius is 2, the SF values of the first, second, third, fourth and fifth group of images are 6.417, 6.376, 6.369, 6.310, and 6.275 respectively. When the filter radius is 6, the SF values of the first, second, third, fourth and fifth group of images are 6.404, 6.367, 6.335, 6.296, and 6.234 respectively. When the filter radius is 10, the SF values of the first, two, three, four and five groups of images have SF values of 6.378, 6.336, 6.324, 6.273, and 6.206, respectively.</p><fig position="float" id="pone.0324392.g013"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g013</object-id><label>Fig 13</label><caption><title>The impact of different radii on fusion results.</title></caption><graphic xlink:href="pone.0324392.g013" position="float"/></fig><sec sec-type="conclusions" id="sec010"><title>Discussion.</title><p>The study proposed a method for optimizing image information processing that is based on FOD and WT algorithms. This method has demonstrated effectiveness in the domains of image edge detection and image fusion. Through experimental verification, this method performs well in image edge detection and image fusion, especially in key indicators such as detection accuracy, information entropy, recall rate, and mean square error (MSE), which are superior to traditional methods. X. Luo et al. proposed a fusion method based on quaternion WT and feature level copula model for image fusion problems. This method was developed to avoid the visual quality degradation of images caused by false information, as compared with other methods [<xref rid="pone.0324392.ref040" ref-type="bibr">40</xref>]. D. Agrawal et al. proposed an image fusion method based on DWT. This approach was developed to improve the visibility and quantitative performance of fused images, as compared with other methods [<xref rid="pone.0324392.ref041" ref-type="bibr">41</xref>]. However, despite significant achievements, there are still some potential optimization directions and application extensions worth further exploration. To further improve the performance of this method and apply it to a wider range of fields, such as video processing and real-time applications. In real-time applications, such as surveillance systems and autonomous driving, the real-time and accuracy of image processing are crucial. To achieve real-time processing, the methods proposed in the research can be simplified and optimized. For example, reducing the number of layers in wavelet decomposition or using fast fractional order differentiation algorithms. In addition, the combination of hardware acceleration technology can further improve real-time processing capabilities. Meanwhile, to adapt to the constantly changing environment in real-time applications, online learning mechanisms can be introduced to enable algorithms to dynamically adjust parameters to adapt to new image features. Different image information processing optimization methods are shown in <xref rid="pone.0324392.g014" ref-type="fig">Fig 14</xref>.</p><fig position="float" id="pone.0324392.g014"><object-id pub-id-type="doi">10.1371/journal.pone.0324392.g014</object-id><label>Fig 14</label><caption><title>Different optimization methods for image information processing.</title></caption><graphic xlink:href="pone.0324392.g014" position="float"/></fig><p>As can be seen from <xref rid="pone.0324392.g014" ref-type="fig">Fig 14</xref>, compared with literature [<xref rid="pone.0324392.ref040" ref-type="bibr">40</xref>] and literature [<xref rid="pone.0324392.ref041" ref-type="bibr">41</xref>], the image information optimization quality of the proposed method is better, which not only retains the new solution intact, but also basically has no noise interference.</p></sec></sec></sec><sec sec-type="conclusions" id="sec011"><title>Conclusion</title><p>To facilitate image analysis, image storage, image transmission and improve image quality, the study proposed an image information optimization processing method based on FOD and WT algorithm. The study used FOD based approach for detecting the edges of the image. Among them, RO, SO, PO, LOG-O, and CA were used to extract the edges of the image. Second, FRWT based method was used for fusion of images. The results revealed that when the study used the four evaluation metrics of information entropy, recall rate, MSE, and precision rate to evaluate the effectiveness of image ED, the SO had the highest precision rate for detection recall rate, and the lowest information entropy and MSE. The method achieved an 80% recall rate, a minimum information entropy of 3.13, a highest detection precision rate of 78.9%, and a minimum MSE of 152. The WT based IF methods had the best MI, QAB/F, and SF values when the FO took the values of 0.4 and 0.5. When the structure of the detected image was complex, RA had the lowest execution efficiency and CA had the highest execution efficiency of 0.0497 and 0.0518, respectively. PA, RA, and SA had execution efficiencies of 0.0505, 0.0513, and 0.0500, respectively. Compared with other methods, the precision rate and execution efficiency of ED of the research proposed method are improved, and the IF is more effective. Due to the fact that only 2D and grayscale images were considered when constructing the proposed algorithm, its ability to handle higher dimensional or more complex image data is not yet clear. In view of this, it is considered to introduce directional fractional wavelet transform in the future to improve the algorithm&#x02019;s ability to process high-dimensional images.</p></sec></body><back><glossary><title>List of Abbreviations</title><def-list><def-item><term>FRWT</term><def><p>Fractional Wavelet Transform</p></def></def-item><def-item><term>WT</term><def><p>Wavelet Transform</p></def></def-item><def-item><term>FOD</term><def><p>Fractional order derivative</p></def></def-item><def-item><term>FO</term><def><p>Fractional order</p></def></def-item><def-item><term>SMC</term><def><p>Sliding mode controller</p></def></def-item><def-item><term>LF</term><def><p>Low frequency</p></def></def-item><def-item><term>TF</term><def><p>Temporal frequency</p></def></def-item><def-item><term>DWT</term><def><p>Discrete wavelet transform</p></def></def-item><def-item><term>RL</term><def><p>Riemann-Liouville</p></def></def-item><def-item><term>G-L</term><def><p>Grimwald-Letnikov</p></def></def-item><def-item><term>ED</term><def><p>Edge detection</p></def></def-item><def-item><term>RO</term><def><p>Roberts Operators</p></def></def-item><def-item><term>PO</term><def><p>Prewitt Operators</p></def></def-item><def-item><term>SO</term><def><p>Sobel Operators</p></def></def-item><def-item><term>LOG-O</term><def><p>LOG Operators</p></def></def-item><def-item><term>CA</term><def><p>Canny algorithm</p></def></def-item><def-item><term>GF</term><def><p>Gaussian filtering</p></def></def-item><def-item><term>GAV</term><def><p>Gradient amplitude value</p></def></def-item><def-item><term>PP</term><def><p>Pixel point</p></def></def-item><def-item><term>CO</term><def><p>Canny Operators</p></def></def-item><def-item><term>HF</term><def><p>High frequency</p></def></def-item><def-item><term>CWT</term><def><p>Continuous wavelet transform</p></def></def-item><def-item><term>OS</term><def><p>Original signal</p></def></def-item><def-item><term>MV</term><def><p>Mean value</p></def></def-item><def-item><term>var</term><def><p>variance</p></def></def-item><def-item><term>cov</term><def><p>Covariance</p></def></def-item><def-item><term>SA</term><def><p>Sobel algorithm</p></def></def-item><def-item><term>RA</term><def><p>Roberts algorithm</p></def></def-item><def-item><term>LOG-A</term><def><p>LOG algorithm</p></def></def-item><def-item><term>PA</term><def><p>Prewitt algorithm</p></def></def-item><def-item><term>LFW</term><def><p>Labeled Faces in the Wild</p></def></def-item></def-list></glossary><ref-list><title>References</title><ref id="pone.0324392.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Xu</surname><given-names>J</given-names></name>, <name><surname>Ai</surname><given-names>B</given-names></name>, <name><surname>Chen</surname><given-names>W</given-names></name>, <name><surname>Yang</surname><given-names>A</given-names></name>, <name><surname>Sun</surname><given-names>P</given-names></name>, <name><surname>Rodrigues</surname><given-names>M</given-names></name>. <article-title>Wireless Image Transmission Using Deep Source Channel Coding With Attention Modules</article-title>. <source>IEEE Trans Circuits Syst Video Technol</source>. <year>2022</year>;<volume>32</volume>(<issue>4</issue>):<fpage>2315</fpage>&#x02013;<lpage>28</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tcsvt.2021.3082521</pub-id></mixed-citation></ref><ref id="pone.0324392.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Kurka</surname><given-names>DB</given-names></name>, <name><surname>Gunduz</surname><given-names>D</given-names></name>. <article-title>Bandwidth-Agile Image Transmission With Deep Joint Source-Channel Coding</article-title>. <source>IEEE Trans Wireless Commun</source>. <year>2021</year>;<volume>20</volume>(<issue>12</issue>):<fpage>8081</fpage>&#x02013;<lpage>95</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/twc.2021.3090048</pub-id></mixed-citation></ref><ref id="pone.0324392.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Hasanvand</surname><given-names>M</given-names></name>, <name><surname>Nooshyar</surname><given-names>M</given-names></name>, <name><surname>Moharamkhani</surname><given-names>E</given-names></name>, <name><surname>Selyari</surname><given-names>A</given-names></name>. <article-title>Machine Learning Methodology for Identifying Vehicles Using Image Processing</article-title>. <source>AIA</source>. <year>2023</year>;<volume>1</volume>(<issue>3</issue>):<fpage>154</fpage>&#x02013;<lpage>62</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.47852/bonviewaia3202833</pub-id></mixed-citation></ref><ref id="pone.0324392.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Gan</surname><given-names>C</given-names></name>, <name><surname>Li</surname><given-names>Z</given-names></name>, <name><surname>Rekik</surname><given-names>I</given-names></name>, <name><surname>Yin</surname><given-names>Z</given-names></name>, <name><surname>Ji</surname><given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Transformers in medical image analysis</article-title>. <source>Intelligent Medicine</source>. <year>2023</year>;<volume>3</volume>(<issue>1</issue>):<fpage>59</fpage>&#x02013;<lpage>78</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.imed.2022.07.002</pub-id></mixed-citation></ref><ref id="pone.0324392.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Windhager</surname><given-names>J</given-names></name>, <name><surname>Zanotelli</surname><given-names>VRT</given-names></name>, <name><surname>Schulz</surname><given-names>D</given-names></name>, <name><surname>Meyer</surname><given-names>L</given-names></name>, <name><surname>Daniel</surname><given-names>M</given-names></name>, <name><surname>Bodenmiller</surname><given-names>B</given-names></name>, <etal>et al</etal>. <article-title>An end-to-end workflow for multiplexed image processing and analysis</article-title>. <source>Nat Protoc</source>. <year>2023</year>;<volume>18</volume>(<issue>11</issue>):<fpage>3565</fpage>&#x02013;<lpage>613</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41596-023-00881-0</pub-id>
<pub-id pub-id-type="pmid">37816904</pub-id>
</mixed-citation></ref><ref id="pone.0324392.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Khushaba</surname><given-names>RN</given-names></name>, <name><surname>Hill</surname><given-names>AJ</given-names></name>. <article-title>Radar-Based Materials Classification Using Deep Wavelet Scattering Transform: A Comparison of Centimeter vs. Millimeter Wave Units</article-title>. <source>IEEE Robot Autom Lett</source>. <year>2022</year>;<volume>7</volume>(<issue>2</issue>):<fpage>2016</fpage>&#x02013;<lpage>22</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/lra.2022.3143200</pub-id></mixed-citation></ref><ref id="pone.0324392.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Cao</surname><given-names>J</given-names></name>, <name><surname>Udhayakumar</surname><given-names>K</given-names></name>, <name><surname>Rakkiyappan</surname><given-names>R</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>, <name><surname>Lu</surname><given-names>J</given-names></name>. <article-title>A Comprehensive Review of Continuous-/Discontinuous-Time Fractional-Order Multidimensional Neural Networks</article-title>. <source>IEEE Trans Neural Netw Learn Syst</source>. <year>2023</year>;<volume>34</volume>(<issue>9</issue>):<fpage>5476</fpage>&#x02013;<lpage>96</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TNNLS.2021.3129829</pub-id>
<pub-id pub-id-type="pmid">34962883</pub-id>
</mixed-citation></ref><ref id="pone.0324392.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Lin</surname><given-names>X</given-names></name>, <name><surname>Liu</surname><given-names>J</given-names></name>, <name><surname>Liu</surname><given-names>F</given-names></name>, <name><surname>Liu</surname><given-names>Z</given-names></name>, <name><surname>Gao</surname><given-names>Y</given-names></name>, <name><surname>Sun</surname><given-names>G</given-names></name>. <article-title>Fractional-Order Sliding Mode Approach of Buck Converters With Mismatched Disturbances</article-title>. <source>IEEE Trans Circuits Syst I</source>. <year>2021</year>;<volume>68</volume>(<issue>9</issue>):<fpage>3890</fpage>&#x02013;<lpage>900</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tcsi.2021.3092138</pub-id></mixed-citation></ref><ref id="pone.0324392.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Long</surname><given-names>B</given-names></name>, <name><surname>Lu</surname><given-names>PJ</given-names></name>, <name><surname>Chong</surname><given-names>KT</given-names></name>, <name><surname>Rodriguez</surname><given-names>J</given-names></name>, <name><surname>Guerrero</surname><given-names>J</given-names></name>. <article-title>Robust Fuzzy-Fractional-Order Nonsingular Terminal Sliding-Mode Control of LCL-Type Grid-Connected Converters</article-title>. <source>IEEE Trans Ind Electron</source>. <year>2022</year>;<volume>69</volume>(<issue>6</issue>):<fpage>5854</fpage>&#x02013;<lpage>66</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tie.2021.3094411</pub-id></mixed-citation></ref><ref id="pone.0324392.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Shi</surname><given-names>J</given-names></name>, <name><surname>Cao</surname><given-names>J</given-names></name>, <name><surname>Liu</surname><given-names>H</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>. <article-title>Compound Adaptive Fuzzy Output Feedback Control for Uncertain Fractional-Order Nonlinear Systems with Fuzzy Dead-Zone Input</article-title>. <source>Int J Fuzzy Syst</source>. <year>2023</year>;<volume>25</volume>(<issue>6</issue>):<fpage>2439</fpage>&#x02013;<lpage>52</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s40815-022-01457-y</pub-id></mixed-citation></ref><ref id="pone.0324392.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>You</surname><given-names>X</given-names></name>, <name><surname>Dian</surname><given-names>S</given-names></name>, <name><surname>Liu</surname><given-names>K</given-names></name>, <name><surname>Guo</surname><given-names>B</given-names></name>, <name><surname>Xiang</surname><given-names>G</given-names></name>, <name><surname>Zhu</surname><given-names>Y</given-names></name>. <article-title>Command Filter-Based Adaptive Fuzzy Finite-Time Tracking Control for Uncertain Fractional-Order Nonlinear Systems</article-title>. <source>IEEE Trans Fuzzy Syst</source>. <year>2023</year>;<volume>31</volume>(<issue>1</issue>):<fpage>226</fpage>&#x02013;<lpage>40</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tfuzz.2022.3185453</pub-id></mixed-citation></ref><ref id="pone.0324392.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Babes</surname><given-names>B</given-names></name>, <name><surname>Mekhilef</surname><given-names>S</given-names></name>, <name><surname>Boutaghane</surname><given-names>A</given-names></name>, <name><surname>Rahmani</surname><given-names>L</given-names></name>. <article-title>Fuzzy Approximation-Based Fractional-Order Nonsingular Terminal Sliding Mode Controller for DC&#x02013;DC Buck Converters</article-title>. <source>IEEE Trans Power Electron</source>. <year>2022</year>;<volume>37</volume>(<issue>3</issue>):<fpage>2749</fpage>&#x02013;<lpage>60</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tpel.2021.3114277</pub-id></mixed-citation></ref><ref id="pone.0324392.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Azghandi</surname><given-names>MA</given-names></name>, <name><surname>Barakati</surname><given-names>SM</given-names></name>, <name><surname>Yazdani</surname><given-names>A</given-names></name>. <article-title>Passivity-Based Design of a Fractional-Order Virtual Capacitor for Active Damping of Multiparalleled Grid-Connected Current-Source Inverters</article-title>. <source>IEEE Trans Power Electron</source>. <year>2022</year>;<volume>37</volume>(<issue>7</issue>):<fpage>7809</fpage>&#x02013;<lpage>18</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tpel.2022.3148242</pub-id></mixed-citation></ref><ref id="pone.0324392.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>C</given-names></name>, <name><surname>Yu</surname><given-names>Y</given-names></name>, <name><surname>Yang</surname><given-names>Z</given-names></name>, <name><surname>Liu</surname><given-names>Q</given-names></name>, <name><surname>Peng</surname><given-names>X</given-names></name>. <article-title>ESR Estimation for Aluminum Electrolytic Capacitor of Power Electronic Converter Based on Compressed Sensing and Wavelet Transform</article-title>. <source>IEEE Trans Ind Electron</source>. <year>2022</year>;<volume>69</volume>(<issue>2</issue>):<fpage>1948</fpage>&#x02013;<lpage>57</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tie.2021.3055164</pub-id></mixed-citation></ref><ref id="pone.0324392.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Ma</surname><given-names>Y</given-names></name>, <name><surname>Maqsood</surname><given-names>A</given-names></name>, <name><surname>Oslebo</surname><given-names>D</given-names></name>, <name><surname>Corzine</surname><given-names>K</given-names></name>. <article-title>Wavelet Transform Data-Driven Machine Learning-Based Real-Time Fault Detection for Naval DC Pulsating Loads</article-title>. <source>IEEE Trans Transp Electrific</source>. <year>2022</year>;<volume>8</volume>(<issue>2</issue>):<fpage>1956</fpage>&#x02013;<lpage>65</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tte.2021.3130044</pub-id></mixed-citation></ref><ref id="pone.0324392.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>M-H</given-names></name>, <name><surname>Lu</surname><given-names>S-D</given-names></name>, <name><surname>Liao</surname><given-names>R-M</given-names></name>. <article-title>Fault Diagnosis for Power Cables Based on Convolutional Neural Network With Chaotic System and Discrete Wavelet Transform</article-title>. <source>IEEE Trans Power Delivery</source>. <year>2022</year>;<volume>37</volume>(<issue>1</issue>):<fpage>582</fpage>&#x02013;<lpage>90</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tpwrd.2021.3065342</pub-id></mixed-citation></ref><ref id="pone.0324392.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Medeiros</surname><given-names>RP</given-names></name>, <name><surname>Bezerra Costa</surname><given-names>F</given-names></name>, <name><surname>Melo Silva</surname><given-names>K</given-names></name>, <name><surname>Muro J de</surname><given-names>JC</given-names></name>, <name><surname>Junior</surname><given-names>JRL</given-names></name>, <name><surname>Popov</surname><given-names>M</given-names></name>. <article-title>A Clarke-Wavelet-Based Time-Domain Power Transformer Differential Protection</article-title>. <source>IEEE Trans Power Delivery</source>. <year>2022</year>;<volume>37</volume>(<issue>1</issue>):<fpage>317</fpage>&#x02013;<lpage>28</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tpwrd.2021.3059732</pub-id></mixed-citation></ref><ref id="pone.0324392.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Gao</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Yang</surname><given-names>A</given-names></name>, <name><surname>Yuan</surname><given-names>H</given-names></name>, <name><surname>Wei</surname><given-names>X</given-names></name>. <article-title>A High-Impedance Fault Detection Method for Distribution Systems Based on Empirical Wavelet Transform and Differential Faulty Energy</article-title>. <source>IEEE Trans Smart Grid</source>. <year>2022</year>;<volume>13</volume>(<issue>2</issue>):<fpage>900</fpage>&#x02013;<lpage>12</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tsg.2021.3129315</pub-id></mixed-citation></ref><ref id="pone.0324392.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Xu</surname><given-names>Z</given-names></name>, <name><surname>Yuan</surname><given-names>M</given-names></name>. <article-title>An Interference Mitigation Technique for Automotive Millimeter Wave Radars in the Tunable Q-Factor Wavelet Transform Domain</article-title>. <source>IEEE Trans Microwave Theory Techn</source>. <year>2021</year>;<volume>69</volume>(<issue>12</issue>):<fpage>5270</fpage>&#x02013;<lpage>83</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tmtt.2021.3121322</pub-id></mixed-citation></ref><ref id="pone.0324392.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Bezerra</surname><given-names>FDM</given-names></name>, <name><surname>Santos</surname><given-names>LA</given-names></name>. <article-title>Chebyshev polynomials for higher order differential equations and fractional powers</article-title>. <source>Math Ann</source>. <year>2022</year>;<volume>388</volume>(<issue>1</issue>):<fpage>675</fpage>&#x02013;<lpage>702</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s00208-022-02554-x</pub-id></mixed-citation></ref><ref id="pone.0324392.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Yu</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Jiang</surname><given-names>B</given-names></name>, <name><surname>Su</surname><given-names>C-Y</given-names></name>, <name><surname>Fu</surname><given-names>J</given-names></name>, <name><surname>Jin</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Enhanced Recurrent Fuzzy Neural Fault-Tolerant Synchronization Tracking Control of Multiple Unmanned Airships via Fractional Calculus and Fixed-Time Prescribed Performance Function</article-title>. <source>IEEE Trans Fuzzy Syst</source>. <year>2022</year>;<volume>30</volume>(<issue>10</issue>):<fpage>4515</fpage>&#x02013;<lpage>29</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tfuzz.2022.3154441</pub-id></mixed-citation></ref><ref id="pone.0324392.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Ragb</surname><given-names>O</given-names></name>, <name><surname>Wazwaz</surname><given-names>A</given-names></name>, <name><surname>Mohamed</surname><given-names>M</given-names></name>, <name><surname>Matbuly</surname><given-names>MS</given-names></name>, <name><surname>Salah</surname><given-names>M</given-names></name>. <article-title>Fractional differential quadrature techniques for fractional order Cauchy reaction&#x02010;diffusion equations</article-title>. <source>Math Methods in App Sciences</source>. <year>2023</year>;<volume>46</volume>(<issue>9</issue>):<fpage>10216</fpage>&#x02013;<lpage>33</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/mma.9112</pub-id></mixed-citation></ref><ref id="pone.0324392.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Ma</surname><given-names>Z</given-names></name>, <name><surname>Liu</surname><given-names>Z</given-names></name>, <name><surname>Huang</surname><given-names>P</given-names></name>, <name><surname>Kuang</surname><given-names>Z</given-names></name>. <article-title>Adaptive Fractional-Order Sliding Mode Control for Admittance-Based Telerobotic System With Optimized Order and Force Estimation</article-title>. <source>IEEE Trans Ind Electron</source>. <year>2022</year>;<volume>69</volume>(<issue>5</issue>):<fpage>5165</fpage>&#x02013;<lpage>74</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tie.2021.3078385</pub-id></mixed-citation></ref><ref id="pone.0324392.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Plouraboue</surname><given-names>F</given-names></name>, <name><surname>Uszes</surname><given-names>P</given-names></name>, <name><surname>Guibert</surname><given-names>R</given-names></name>. <article-title>Source Identification of Propagating Waves Inside a Network</article-title>. <source>IEEE Trans Netw Sci Eng</source>. <year>2022</year>;<volume>9</volume>(<issue>3</issue>):<fpage>1437</fpage>&#x02013;<lpage>50</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tnse.2022.3144647</pub-id></mixed-citation></ref><ref id="pone.0324392.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>H</given-names></name>, <name><surname>Ong</surname><given-names>Y-S</given-names></name>, <name><surname>Yu</surname><given-names>Z</given-names></name>, <name><surname>Cai</surname><given-names>J</given-names></name>, <name><surname>Shen</surname><given-names>X</given-names></name>. <article-title>Scalable Gaussian Process Classification With Additive Noise for Non-Gaussian Likelihoods</article-title>. <source>IEEE Trans Cybern</source>. <year>2022</year>;<volume>52</volume>(<issue>7</issue>):<fpage>5842</fpage>&#x02013;<lpage>54</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tcyb.2020.3043355</pub-id><pub-id pub-id-type="pmid">33449897</pub-id>
</mixed-citation></ref><ref id="pone.0324392.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>W</given-names></name>, <name><surname>Li</surname><given-names>L</given-names></name>, <name><surname>Zhang</surname><given-names>F</given-names></name>. <article-title>Crack image recognition on fracture mechanics cross valley edge detection by fractional differential with multi-scale analysis</article-title>. <source>SIViP</source>. <year>2022</year>;<volume>17</volume>(<issue>1</issue>):<fpage>47</fpage>&#x02013;<lpage>55</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11760-022-02202-6</pub-id></mixed-citation></ref><ref id="pone.0324392.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Ramasamy</surname><given-names>MD</given-names></name>, <name><surname>Periasamy</surname><given-names>K</given-names></name>, <name><surname>Periasamy</surname><given-names>S</given-names></name>, <name><surname>Muthusamy</surname><given-names>S</given-names></name>, <name><surname>Ramamoorthi</surname><given-names>P</given-names></name>, <name><surname>Thangavel</surname><given-names>G</given-names></name>, <etal>et al</etal>. <article-title>A novel Adaptive Neural Network-Based Laplacian of Gaussian (AnLoG) classification algorithm for detecting diabetic retinopathy with colour retinal fundus images</article-title>. <source>Neural Comput &#x00026; Applic</source>. <year>2023</year>;<volume>36</volume>(<issue>7</issue>):<fpage>3513</fpage>&#x02013;<lpage>24</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s00521-023-09324-z</pub-id></mixed-citation></ref><ref id="pone.0324392.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Budiman</surname><given-names>M</given-names></name>, <name><surname>Sunardi</surname><given-names>S</given-names></name>. <article-title>Deteksi Tepi Plat Nomor Kendaraan Menggunakan Metode Operator Prewitt</article-title>. <source>ARL</source>. <year>2023</year>;<volume>7</volume>(<issue>2</issue>):<fpage>157</fpage>&#x02013;<lpage>65</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.46799/arl.v7i2.135</pub-id></mixed-citation></ref><ref id="pone.0324392.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>L</given-names></name>, <name><surname>Liu</surname><given-names>Z</given-names></name>, <name><surname>Hou</surname><given-names>A</given-names></name>, <name><surname>Qian</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>H</given-names></name>. <article-title>Adaptive edge detection of rebar thread head image based on improved Canny operator</article-title>. <source>IET Image Processing</source>. <year>2023</year>;<volume>18</volume>(<issue>5</issue>):<fpage>1145</fpage>&#x02013;<lpage>60</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1049/ipr2.13015</pub-id></mixed-citation></ref><ref id="pone.0324392.ref030"><label>30</label><mixed-citation publication-type="journal">S. <name><surname>Supiyandi</surname><given-names>T</given-names></name>, <name><surname>Panggabean</surname><given-names>N</given-names></name>, <name><surname>Ramadhan</surname><given-names>S</given-names></name>, <name><surname>Dewi</surname><given-names>R</given-names></name>, <name><surname>Yusra</surname><given-names>S</given-names></name>. &#x0201c;<article-title>Deteksi Tepi Sederhana Pada Citra Menggunakan Operator Sobel</article-title>. <source>Repeater</source>. <year>2024</year>;<volume>2</volume>(<issue>3</issue>):<fpage>43</fpage>&#x02013;<lpage>56</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.62951/repeater.v2i3.90</pub-id></mixed-citation></ref><ref id="pone.0324392.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Pramanik</surname><given-names>S</given-names></name>. <article-title>An adaptive image steganography approach depending on integer wavelet transform and genetic algorithm</article-title>. <source>Multimed Tools Appl</source>. <year>2023</year>;<volume>82</volume>(<issue>22</issue>):<fpage>34287</fpage>&#x02013;<lpage>319</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11042-023-14505-y</pub-id></mixed-citation></ref><ref id="pone.0324392.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Dai</surname><given-names>TKV</given-names></name>, <name><surname>Yu</surname><given-names>Y</given-names></name>, <name><surname>Theilmann</surname><given-names>P</given-names></name>, <name><surname>Fathy</surname><given-names>AE</given-names></name>, <name><surname>Kilic</surname><given-names>O</given-names></name>. <article-title>Remote Vital Sign Monitoring With Reduced Random Body Swaying Motion Using Heartbeat Template and Wavelet Transform Based on Constellation Diagrams</article-title>. <source>IEEE J Electromagn RF Microw Med Biol</source>. <year>2022</year>;<volume>6</volume>(<issue>3</issue>):<fpage>429</fpage>&#x02013;<lpage>36</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/jerm.2022.3140900</pub-id></mixed-citation></ref><ref id="pone.0324392.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Ahmadian</surname><given-names>R</given-names></name>, <name><surname>Ghatee</surname><given-names>M</given-names></name>, <name><surname>Wahlstrom</surname><given-names>J</given-names></name>. <article-title>Discrete Wavelet Transform for Generative Adversarial Network to Identify Drivers Using Gyroscope and Accelerometer Sensors</article-title>. <source>IEEE Sensors J</source>. <year>2022</year>;<volume>22</volume>(<issue>7</issue>):<fpage>6879</fpage>&#x02013;<lpage>86</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/jsen.2022.3152518</pub-id></mixed-citation></ref><ref id="pone.0324392.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Xavier</surname><given-names>GVR</given-names></name>, <name><surname>Coelho R de</surname><given-names>A</given-names></name>, <name><surname>Silva</surname><given-names>HS</given-names></name>, <name><surname>Serres</surname><given-names>AJR</given-names></name>, <name><surname>da Costa</surname><given-names>EG</given-names></name>, <name><surname>Oliveira</surname><given-names>ASR</given-names></name>. <article-title>Partial Discharge Location Through Application of Stationary Discrete Wavelet Transform on UHF Signals</article-title>. <source>IEEE Sensors J</source>. <year>2021</year>;<volume>21</volume>(<issue>21</issue>):<fpage>24644</fpage>&#x02013;<lpage>52</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/jsen.2021.3116491</pub-id></mixed-citation></ref><ref id="pone.0324392.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>D</given-names></name>, <name><surname>Dysko</surname><given-names>A</given-names></name>, <name><surname>Hong</surname><given-names>Q</given-names></name>, <name><surname>Tzelepis</surname><given-names>D</given-names></name>, <name><surname>Booth</surname><given-names>CD</given-names></name>. <article-title>Transient Wavelet Energy-Based Protection Scheme for Inverter-Dominated Microgrid</article-title>. <source>IEEE Trans Smart Grid</source>. <year>2022</year>;<volume>13</volume>(<issue>4</issue>):<fpage>2533</fpage>&#x02013;<lpage>46</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tsg.2022.3163669</pub-id></mixed-citation></ref><ref id="pone.0324392.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Singh</surname><given-names>G</given-names></name>, <name><surname>Chiluveru</surname><given-names>SR</given-names></name>, <name><surname>Raman</surname><given-names>B</given-names></name>, <name><surname>Tripathy</surname><given-names>M</given-names></name>, <name><surname>Kaushik</surname><given-names>BK</given-names></name>. <article-title>Novel Architecture for Lifting Discrete Wavelet Packet Transform With Arbitrary Tree Structure</article-title>. <source>IEEE Trans VLSI Syst</source>. <year>2021</year>;<volume>29</volume>(<issue>7</issue>):<fpage>1490</fpage>&#x02013;<lpage>4</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tvlsi.2021.3079989</pub-id></mixed-citation></ref><ref id="pone.0324392.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>A. Lawrance</surname><given-names>N</given-names></name>, <name><surname>S. Shiny Angel</surname><given-names>T</given-names></name>. <article-title>Image Fusion Based on NSCT and Sparse Representation for Remote Sensing Data</article-title>. <source>Computer Systems Science and Engineering</source>. <year>2023</year>;<volume>46</volume>(<issue>3</issue>):<fpage>3439</fpage>&#x02013;<lpage>55</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.32604/csse.2023.030311</pub-id></mixed-citation></ref><ref id="pone.0324392.ref038"><label>38</label><mixed-citation publication-type="journal"><name><surname>K P</surname><given-names>B</given-names></name>, <name><surname>M</surname><given-names>RK</given-names></name>. <article-title>ELM speaker identification for limited dataset using multitaper based MFCC and PNCC features with fusion score</article-title>. <source>Multimed Tools Appl</source>. <year>2020</year>;<volume>79</volume>(39&#x02013;40):<fpage>28859</fpage>&#x02013;<lpage>83</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11042-020-09353-z</pub-id></mixed-citation></ref><ref id="pone.0324392.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>Song</surname><given-names>M</given-names></name>, <name><surname>Lim</surname><given-names>S</given-names></name>, <name><surname>Kim</surname><given-names>W</given-names></name>. <article-title>Monocular Depth Estimation Using Laplacian Pyramid-Based Depth Residuals</article-title>. <source>IEEE Trans Circuits Syst Video Technol</source>. <year>2021</year>;<volume>31</volume>(<issue>11</issue>):<fpage>4381</fpage>&#x02013;<lpage>93</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tcsvt.2021.3049869</pub-id></mixed-citation></ref><ref id="pone.0324392.ref040"><label>40</label><mixed-citation publication-type="journal"><name><surname>Luo</surname><given-names>X</given-names></name>, <name><surname>Li</surname><given-names>K</given-names></name>, <name><surname>Wang</surname><given-names>A</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Wu</surname><given-names>X</given-names></name>. <article-title>Infrared and visible image fusion based on quaternion wavelets transform and feature-level Copula model</article-title>. <source>Multimed Tools Appl</source>. <year>2023</year>;<volume>83</volume>(<issue>10</issue>):<fpage>28549</fpage>&#x02013;<lpage>77</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11042-023-15536-1</pub-id></mixed-citation></ref><ref id="pone.0324392.ref041"><label>41</label><mixed-citation publication-type="journal"><name><surname>Agrawal</surname><given-names>D</given-names></name>, <name><surname>Yadav</surname><given-names>AC</given-names></name>, <name><surname>Tyagi</surname><given-names>PK</given-names></name>. <article-title>Low-light and hazy image enhancement using retinex theory and wavelet transform fusion</article-title>. <source>Multimed Tools Appl</source>. <year>2024</year>;<volume>83</volume>(<issue>29</issue>):<fpage>72519</fpage>&#x02013;<lpage>36</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11042-024-18459-7</pub-id></mixed-citation></ref></ref-list></back><sub-article article-type="author-comment" id="pone.0324392.r001" specific-use="rebutted-decision-letter-unavailable"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0324392.r001</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">28 Nov 2024</named-content>
</p></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0324392.r002" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0324392.r002</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Howard</surname><given-names>Helen</given-names></name><role>Staff Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Helen Howard</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Helen Howard</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0324392" id="rel-obj002" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">23 Jan 2025</named-content>
</p><p>PONE-D-24-54977Image Information Optimization Processing Based on Fractional Order Differentiation and WT AlgorithmPLOS ONE</p><p>Dear Dr. Long,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>The manuscript has been evaluated by two reviewers, and their comments are available below.&#x000a0;The reviewers have raised a number of major concerns. In particular, they request improvements to the writing, further discussion and the addition of statistical significance testing.</p><p>Could you please carefully revise the manuscript to address all comments raised?</p><p>Please submit your revised manuscript by Mar 09 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email> . When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:</p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link> . Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link> .</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Helen Howard</p><p>Staff Editor</p><p>PLOS ONE</p><p>Journal Requirements:</p><p>When submitting your revision, we need you to address these additional requirements.</p><p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at</p><p><ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and</p><p>
<ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
</p><p>2. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, we expect all author-generated code to be made available without restrictions upon publication of the work. Please review our guidelines at <ext-link xlink:href="https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code" ext-link-type="uri">https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code</ext-link> and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse.</p><p>3. We note that your Data Availability Statement is currently as follows: [All relevant data are within the manuscript and its Supporting Information files.]</p><p>Please confirm at this time whether or not your submission contains all raw data required to replicate the results of your study. Authors must share the &#x0201c;minimal data set&#x0201d; for their submission. PLOS defines the minimal data set to consist of the data required to replicate all study findings reported in the article, as well as related metadata and methods (<ext-link xlink:href="https://journals.plos.org/plosone/s/data-availability#loc-minimal-data-set-definition)." ext-link-type="uri">https://journals.plos.org/plosone/s/data-availability#loc-minimal-data-set-definition</ext-link>).</p><p>For example, authors should submit the following data:</p><p>- The values behind the means, standard deviations and other measures reported;</p><p>- The values used to build graphs;</p><p>- The points extracted from images for analysis.</p><p>Authors do not need to submit their entire data set if only a portion of the data was used in the reported study.</p><p>If your submission does not contain these data, please either upload them as Supporting Information files or deposit them to a stable, public repository and provide us with the relevant URLs, DOIs, or accession numbers. For a list of recommended repositories, please see <ext-link xlink:href="https://journals.plos.org/plosone/s/recommended-repositories." ext-link-type="uri">https://journals.plos.org/plosone/s/recommended-repositories</ext-link>.</p><p>If there are ethical or legal restrictions on sharing a de-identified data set, please explain them in detail (e.g., data contain potentially sensitive information, data are owned by a third-party organization, etc.) and who has imposed them (e.g., an ethics committee). Please also provide contact information for a data access committee, ethics committee, or other institutional body to which data requests may be sent. If data are owned by a third party, please indicate how others may request data access.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<bold>Comments to the Author</bold>
</p><p>1. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p>2. Has the statistical analysis been performed appropriately and rigorously? </p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p>3. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.</p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p>4. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p>5. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p><p>Reviewer #1:&#x000a0;Paper is nicely written</p><p>All experimental analysis is done nicely</p><p>Paper is of good quality. Literature is done in detailed.</p><p>Methodology is explained in details. Results shown in graphical as well as tabular form.</p><p>Reviewer #2:&#x000a0;The proposed method, combining fractional order differentiation (FOD) and wavelet transform (WT), is innovative and technically sound. The integration of edge detection and image fusion techniques is well-justified and aligns with current challenges in complex image processing.</p><p>However, further discussion on the method's generalizability to other datasets and scenarios would enhance its impact.</p><p>The results are adequately supported by relevant metrics such as precision, recall, and information entropy. While these provide a strong foundation, adding statistical significance testing (e.g., t-tests or ANOVA) would improve the rigour of your analysis.</p><p>While the manuscript is mostly clear, specific phrases and sentences are awkwardly structured or repetitive. For instance:</p><p>"The highest recall rate was 80%, the minimum information entropy was 3.13..." could be simplified to "The method achieved an 80% recall rate and a minimum information entropy of 3.13."</p><p>Avoid redundancy, such as "the study proposed method edge detection by the study."</p><p>Ensure consistency in technical terminology, and avoid using acronyms without proper explanation.</p><p>The figures and tables effectively support the narrative but could benefit from more explicit captions that summarize their findings.</p><p>Ensure all axes and labels are legible, particularly in performance comparison graphs (e.g., Figures 8&#x02013;11).</p><p>The discussion section should expand on how the proposed method could be optimized or applied to other domains like video processing or real-time applications.</p><p>Address potential limitations of the method, such as computational efficiency or dependency on parameter tuning.</p><p>**********</p><p>6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link> ). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link> .</p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link> . PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email> . Please note that Supporting Information files do not need this step.</p></body></sub-article><sub-article article-type="author-comment" id="pone.0324392.r003"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0324392.r003</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0324392" id="rel-obj003" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">7 Mar 2025</named-content>
</p><p>The manuscript has been modified.</p><supplementary-material id="pone.0324392.s002" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Response to comments.doc</named-content></p></caption><media xlink:href="pone.0324392.s002.doc"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0324392.r004" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0324392.r004</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Uysal</surname><given-names>Fatih</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Fatih Uysal</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Fatih Uysal</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0324392" id="rel-obj004" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">25 Mar 2025</named-content>
</p><p>PONE-D-24-54977R1Image Information Optimization Processing Based on Fractional Order Differentiation and WT AlgorithmPLOS ONE</p><p>Dear Dr. Long,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please revise the paper based on reviewer comments. Also consider the editor comments below.</p><p>1. A more detailed literature on the study should be added at the end of the Related works section. After this process, the difference of this study from the literature should be interpreted more clearly.</p><p>2. Although there are many different methods that can be used for image edge detection in the literature, why a method based on feature fractional gradient operator was used in this study and its originality should be stated more clearly.</p><p>3. The results obtained from the scope of the study and the metric types seem to be at an appropriate level when compared to the literature.</p><p>4. The steps for implementing guided filtering are clearly stated and clearly reveal the quality of the study.</p><p>5. The reason for the dataset preferences used in the study and their places in the literature compared to other datasets in the literature should be stated more clearly.</p><p>6. The hardware or software (toolbox etc.) used in the study should be stated more clearly and detailed.</p><p>Please submit your revised manuscript by May 09 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email> . When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:</p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link> . Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link> .</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Fatih Uysal, Ph.D.</p><p>Academic Editor</p><p>PLOS ONE</p><p>Additional Editor Comments (if provided):</p><p>Please revise the paper based on reviewer comments. Also consider the editor comments below.</p><p>1. A more detailed literature on the study should be added at the end of the Related works section. After this process, the difference of this study from the literature should be interpreted more clearly.</p><p>2. Although there are many different methods that can be used for image edge detection in the literature, why a method based on feature fractional gradient operator was used in this study and its originality should be stated more clearly.</p><p>3. The results obtained from the scope of the study and the metric types seem to be at an appropriate level when compared to the literature.</p><p>4. The steps for implementing guided filtering are clearly stated and clearly reveal the quality of the study.</p><p>5. The reason for the dataset preferences used in the study and their places in the literature compared to other datasets in the literature should be stated more clearly.</p><p>6. The hardware or software (toolbox etc.) used in the study should be stated more clearly and detailed.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.</p><p>Reviewer #3:&#x000a0;(No Response)</p><p>Reviewer #4:&#x000a0;(No Response)</p><p>**********</p><p>2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p><p>Reviewer #3:&#x000a0;(No Response)</p><p>Reviewer #4:&#x000a0;Partly</p><p>**********</p><p>3. Has the statistical analysis been performed appropriately and rigorously? </p><p>Reviewer #3:&#x000a0;I Don't Know</p><p>Reviewer #4:&#x000a0;N/A</p><p>**********</p><p>4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.</p><p>Reviewer #3:&#x000a0;(No Response)</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p>5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p>6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p><p>Reviewer #3:&#x000a0;The authors have clearly stated the proposed method.</p><p>The findings obtained demonstrate the effectiveness of the study.</p><p>Up-to-date and sufficient references have been added.</p><p>The authors have responded constructively to the comments of other reviewers.</p><p>Reviewer #4:&#x000a0;The referee comments for the study titled "Image Information Optimization Processing Based on Fractional Order Differentiation and WT Algorithm" are as follows.</p><p>*The purpose of the study should be clearly explained.</p><p>*The effect of the proposed method should be shown on sample images.</p><p>*The discussion section should be made more understandable with images and tables.</p><p>**********</p><p>7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link> ). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link> .</p><p>Reviewer #3:&#x000a0;No</p><p>Reviewer #4:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link> . PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email> . Please note that Supporting Information files do not need this step.</p></body></sub-article><sub-article article-type="author-comment" id="pone.0324392.r005"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0324392.r005</article-id><title-group><article-title>Author response to Decision Letter 2</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0324392" id="rel-obj005" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">23 Apr 2025</named-content>
</p><p>The manuscript has been modified.</p><supplementary-material id="pone.0324392.s003" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Response to reviewers comments.docx</named-content></p></caption><media xlink:href="pone.0324392.s003.docx"/></supplementary-material></body></sub-article><sub-article article-type="editor-report" id="pone.0324392.r006" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0324392.r006</article-id><title-group><article-title>Decision Letter 2</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Uysal</surname><given-names>Fatih</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Fatih Uysal</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Fatih Uysal</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0324392" id="rel-obj006" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">25 Apr 2025</named-content>
</p><p>Image Information Optimization Processing Based on Fractional Order Differentiation and WT Algorithm</p><p>PONE-D-24-54977R2</p><p>Dear Dr. Long,</p><p>We&#x02019;re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p><p>Within one week, you&#x02019;ll receive an e-mail detailing the required amendments. When these have been addressed, you&#x02019;ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p><p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">Editorial Manager&#x000ae;</ext-link> &#x000a0;and clicking the &#x02018;Update My Information' link at the top of the page. If you have any questions relating to publication charges, please contact our Author Billing department directly at authorbilling@plos.org.</p><p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they&#x02019;ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p><p>Kind regards,</p><p>Fatih Uysal, Ph.D.</p><p>Academic Editor</p><p>PLOS ONE</p><p>Additional Editor Comments (optional):</p><p>Considering the authors&#x02019; detailed responses to the editor and reviewer feedback, the improvements made during revision, and the overall quality of the manuscript, we have decided to accept the paper for its strong potential to contribute to the field and the robustness of its final version.</p><p>Reviewers' comments:</p></body></sub-article><sub-article article-type="editor-report" id="pone.0324392.r007" specific-use="acceptance-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0324392.r007</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Uysal</surname><given-names>Fatih</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Fatih Uysal</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Fatih Uysal</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0324392" id="rel-obj007" related-article-type="reviewed-article"/></front-stub><body><p>PONE-D-24-54977R2</p><p>PLOS ONE</p><p>Dear Dr. Long,</p><p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p><p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p><p>* All references, tables, and figures are properly cited</p><p>* All relevant supporting information is included in the manuscript submission,</p><p>* There are no issues that prevent the paper from being properly typeset</p><p>You will receive further&#x000a0;instructions from the production team, including instructions on how to review your proof when it&#x000a0;is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps.</p><p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p><p>If we can help with anything else, please email us at customercare@plos.org.</p><p>Thank you for submitting your work to PLOS ONE and supporting open access.</p><p>Kind regards,</p><p>PLOS ONE Editorial Office Staff</p><p>on behalf of</p><p>Dr. Fatih Uysal</p><p>Academic Editor</p><p>PLOS ONE</p></body></sub-article></article>