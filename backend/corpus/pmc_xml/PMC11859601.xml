<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006245</article-id><article-id pub-id-type="pmc">PMC11859601</article-id><article-id pub-id-type="doi">10.3390/s25041016</article-id><article-id pub-id-type="publisher-id">sensors-25-01016</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>CylinDeRS: A Benchmark Visual Dataset for Robust Gas Cylinder Detection and Attribute Classification in Real-World Scenes</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0008-3673-4678</contrib-id><name><surname>Stavrothanasopoulos</surname><given-names>Klearchos</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7711-3773</contrib-id><name><surname>Gkountakos</surname><given-names>Konstantinos</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="c1-sensors-25-01016" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6767-8762</contrib-id><name><surname>Ioannidis</surname><given-names>Konstantinos</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4148-9028</contrib-id><name><surname>Tsikrika</surname><given-names>Theodora</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2505-9178</contrib-id><name><surname>Vrochidis</surname><given-names>Stefanos</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6447-9020</contrib-id><name><surname>Kompatsiaris</surname><given-names>Ioannis</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Doulamis</surname><given-names>Anastasios</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01016">Centre for Research &#x00026; Technology Hellas, Information Technologies Institute, 57001 Thessaloniki, Greece; <email>klearchos_stav@iti.gr</email> (K.S.); <email>kioannid@iti.gr</email> (K.I.); <email>theodora.tsikrika@iti.gr</email> (T.T.); <email>stefanos@iti.gr</email> (S.V.); <email>ikom@iti.gr</email> (I.K.)</aff><author-notes><corresp id="c1-sensors-25-01016"><label>*</label>Correspondence: <email>gountakos@iti.gr</email></corresp></author-notes><pub-date pub-type="epub"><day>08</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1016</elocation-id><history><date date-type="received"><day>17</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>19</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>05</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Gas cylinder detection and the identification of their characteristics hold considerable potential for enhancing safety and operational efficiency in several applications, including industrial and warehouse operations. These tasks gain significance with the growth of online trade, emerging as critical instruments to combat environmental crimes associated with hazardous substances&#x02019; illegal commerce. However, the lack of relevant datasets hinders the effective utilization of deep learning techniques within this domain. In this study, we introduce CylinDeRS, a domain-specific dataset for gas cylinder detection and the classification of their attributes in real-world scenes. CylinDeRS contains 7060 RGB images, depicting various challenging environments and featuring over 25,250 annotated instances. It addresses two tasks: (a) the detection of gas cylinders as objects of interest, and (b) the attribute classification of the detected gas cylinder objects for material, size, and orientation. Extensive experiments using state-of-the-art (SotA) models are reported to validate the dataset&#x02019;s significance and application prospects, providing baselines for further performance evaluation and in-depth analysis. The results show a maximum mAP of 91% for the gas cylinder detection task and a maximum accuracy of 71.6% for the attribute classification task, highlighting the challenges posed by real-world scenarios and underlining the proposed dataset&#x02019;s importance in advancing the field.</p></abstract><kwd-group><kwd>cylinder</kwd><kwd>dataset</kwd><kwd>object detection</kwd><kwd>image classification</kwd><kwd>attribute extraction</kwd><kwd>deep learning</kwd></kwd-group><funding-group><award-group><funding-source>European Commission</funding-source><award-id>101073952</award-id></award-group><funding-statement>This research was funded by the European Commission through the Horizon Europe project PERIVALLON under Grant Agreement ID No. 101073952.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01016"><title>1. Introduction</title><p>In recent years, computer vision has witnessed remarkable advancements, enabling machines to perceive and understand visual information across a wide range of applications. Pivotal and well-established focus areas, including object detection and object attribute classification, have been extensively studied, while the availability of standardized datasets [<xref rid="B1-sensors-25-01016" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01016" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01016" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01016" ref-type="bibr">4</xref>] has greatly facilitated the exploration of several research challenges associated with these tasks by serving as means to assess the comparative performance of different algorithms and techniques. Recently published deep learning techniques have shown that they can be highly effective and efficient in handling detection and classification tasks when coupled with image datasets with high-quality annotations [<xref rid="B5-sensors-25-01016" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01016" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-01016" ref-type="bibr">7</xref>]. This progress has led to significant breakthroughs in several fields, such as face detection and recognition, activity recognition, crowd analysis, as well as intelligent surveillance [<xref rid="B8-sensors-25-01016" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01016" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01016" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01016" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01016" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01016" ref-type="bibr">13</xref>].</p><p>Apart from the need for datasets for typical visual tasks, the demand for domain-specific datasets is rising, driven by the increasing need for automation, continuous monitoring, and optimization of critical tasks across several domains, such as agriculture [<xref rid="B14-sensors-25-01016" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01016" ref-type="bibr">15</xref>] and biomedical research [<xref rid="B16-sensors-25-01016" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01016" ref-type="bibr">17</xref>]. In addition, domain-specific datasets affect the performance of machine learning models by improving their capability to accurately detect patterns and objects, while also enhancing their ability to generalize in unobserved content. An interesting and intriguing domain-specific application is related to the detection and analysis of cylinders, particularly used for transferring gases under low or high pressure, namely gas cylinders [<xref rid="B18-sensors-25-01016" ref-type="bibr">18</xref>]. This application holds significant potential use cases in various operations, such as monitoring industrial facilities handling gas cylinders, preventing accidents associated with these items, as well as improving warehouse safety and storage efficiency [<xref rid="B19-sensors-25-01016" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01016" ref-type="bibr">20</xref>]. In addition, the illegal trade and use of Ozone-Depleting Substances (ODSs) and hydrofluorocarbons (HFCs) included in gas cylinders, and the growing concern regarding the online commerce of hazardous substances that pose significant threats to the environment [<xref rid="B21-sensors-25-01016" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01016" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-01016" ref-type="bibr">23</xref>], require innovative and efficient solutions that can identify and mitigate the publishing of such advertisements on various digital platforms.</p><p>While significant progress has been made in generic object detection and attribute classification research [<xref rid="B24-sensors-25-01016" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-01016" ref-type="bibr">25</xref>], the existing datasets primarily focus on everyday scenes containing common objects [<xref rid="B26-sensors-25-01016" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01016" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-01016" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-01016" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-01016" ref-type="bibr">30</xref>]. This focus overlooks the specificity and diversity required for robust gas cylinder detection since the available datasets fail to account for the variations in gas cylinders&#x02019; material, size, and orientation, required for the identification of these attributes. In the context of domain-specific computer vision applications, synthetic data are widely used to compensate for the lack of real-world data [<xref rid="B31-sensors-25-01016" ref-type="bibr">31</xref>]. However, there usually exists a notable gap between synthetic and real-world datasets in terms of quality, data bias, fairness, as well as potential ethical considerations and legal implications [<xref rid="B32-sensors-25-01016" ref-type="bibr">32</xref>].</p><p>To address these limitations, we introduce CylinDeRS [<xref rid="B33-sensors-25-01016" ref-type="bibr">33</xref>], a comprehensive domain-specific dataset for <bold>cylin</bold>der <bold>de</bold>tection in <bold>r</bold>eal-world <bold>s</bold>cenes and the classification of their attributes. The dataset contains a diverse compilation of images captured in various settings, ranging from industrial facilities and warehouse complexes to commercial spaces, covering both indoor and outdoor environments. The images feature gas cylinder objects in challenging conditions, including varying lighting scenarios, occlusions, and cluttered backgrounds, closely representing real-world situations. <xref rid="sensors-25-01016-f001" ref-type="fig">Figure 1</xref> illustrates a selection of these images, highlighting the variety and complexity present in the dataset. The CylinDeRS dataset is publicly available from the Roboflow repository (<uri xlink:href="https://universe.roboflow.com/klearchos-stavrothanasopoulos-konstantinos-gkountakos-6jwgj/cylinders-iaq6n">https://universe.roboflow.com/klearchos-stavrothanasopoulos-konstantinos-gkountakos-6jwgj/cylinders-iaq6n</uri> (accessed on 17 January 2025)), enabling the research community to evaluate and extend its applications further.</p><p>The CylinDeRS dataset contains 7060 images and 25,260 annotated instances, making it a dataset of practical scale tailored to object detection and classification within the specialized domain of gas cylinders. Although not as extensive as some general-purpose object detection datasets, its domain-specific focus ensures high relevance and utility for this application. CylinDeRS distinguishes itself through its diversity, encompassing cylinders of varying sizes (short and long), materials (metal and fiber), and orientations (standing and fallen), often coexisting within a single image. This combination of material attributes, alongside semantic annotations for size and orientation, provide a more comprehensive representation than many SotA visual datasets. The dataset also introduces significant complexity, driven by the subtle distinctions among the included gas cylinders, thus making it challenging to achieve accurate detection and classification. Moreover, the presence in a single image of anywhere from one to one-hundred-forty-one cylinders with varying attributes further amplifies its difficulty, setting it apart from the existing object detection datasets. Captured in diverse real-world scenarios, the images feature cluttered environments, occlusions, and other challenges, underscoring the dataset&#x02019;s value for advancing object detection and attribute classification tasks.</p><p>Every image in the CylinDeRS dataset is manually inspected and thoroughly annotated, with precise bounding box annotations outlining the contours of the gas cylinders using the Roboflow Annotate tool (<uri xlink:href="https://docs.roboflow.com/annotate/use-roboflow-annotate">https://docs.roboflow.com/annotate/use-roboflow-annotate</uri> (accessed on 17 January 2025)). Each gas cylinder instance within an image is further associated with three labels, indicating its attributes from a total of nine distinct classes, grouped as follows: material (metal, fiber, or unknown material), size (short, long, or unknown size), and orientation (standing, fallen, or unknown orientation). The two sets of annotations (bounding boxes and attribute classes) serve as ground truth labels for training and evaluating the performance of object detection and attribute classification models, respectively.</p><p>The entire dataset creation process, from the collection of images to the data pre-processing steps and the data annotation steps, is provided in the form of a well-defined methodology, which could serve as a guide for researchers aiming to create domain-specific datasets. To establish the application value of CylinDeRS, an extensive benchmark evaluation has been conducted using state-of-the-art (SotA) deep learning models trained on the proposed dataset. The experiments not only demonstrate the effectiveness of the dataset in terms of training deep learning models to automate practical gas-cylinder-related applications but also highlight the challenges that emerge with these tasks. Our contributions are summarized as follows:<list list-type="bullet"><list-item><p>The introduction of CylinDeRS, a comprehensive dataset of 7060 images, with a total of 25,269 gas cylinder object instances, explicitly designed for gas cylinder detection and the classification of their key attributes (material, size, and orientation) in real-world visual scenes;</p></list-item><list-item><p>The proposal of a systematic methodology for creating domain-specific datasets, including all the steps from data collection and pre-processing to data annotation;</p></list-item><list-item><p>Extensive experiments using SotA object detection and attribute classification models, resulting in setting baselines and providing pre-trained benchmarks for further research, performance evaluation, and in-depth analysis.</p></list-item></list></p><p>The rest of the paper is organized as follows. <xref rid="sec2-sensors-25-01016" ref-type="sec">Section 2</xref> covers the related well-known publicly available datasets from the fields of object detection and object attribute classification. <xref rid="sec3-sensors-25-01016" ref-type="sec">Section 3</xref> presents the detailed methodology followed for the development of the dataset and defines its structure and characteristics. <xref rid="sec4-sensors-25-01016" ref-type="sec">Section 4</xref> describes the experimental setup, while <xref rid="sec5-sensors-25-01016" ref-type="sec">Section 5</xref> presents and discusses the evaluation results and the models&#x02019; performance. Finally, <xref rid="sec6-sensors-25-01016" ref-type="sec">Section 6</xref> concludes this work and outlines future steps.</p></sec><sec id="sec2-sensors-25-01016"><title>2. Related&#x000a0;Work</title><p>Datasets have always been pivotal resources for computer vision research. Within the domains of object detection and object attribute classification, the training of Deep Neural Networks (DNNs) is inseparable from various image datasets since they play a crucial role in developing DNN-based models for such tasks. Common object detection datasets, including (but not limited to) Pascal Visual Object Classes (Pascal VOC) 2012 [<xref rid="B1-sensors-25-01016" ref-type="bibr">1</xref>], Common Objects in Context (COCO) [<xref rid="B2-sensors-25-01016" ref-type="bibr">2</xref>], and Open Images [<xref rid="B39-sensors-25-01016" ref-type="bibr">39</xref>], provide annotations related to the spatial position of objects within the image for a wide range of categories. Moreover, since visual attributes represent a significant portion of the details present within a scene, objects can be described by a diverse range of attributes that capture their visual appearance (color, texture, material) and geometry (size, shape, and orientation) [<xref rid="B29-sensors-25-01016" ref-type="bibr">29</xref>]. To this end, widely used object attribute classification datasets, including COCO Attributes [<xref rid="B3-sensors-25-01016" ref-type="bibr">3</xref>], Web Image Dataset for Event Recognition (WIDER) Attribute [<xref rid="B40-sensors-25-01016" ref-type="bibr">40</xref>], iMaterialist [<xref rid="B4-sensors-25-01016" ref-type="bibr">4</xref>], Visual Attributes in the Wild (VAW) [<xref rid="B29-sensors-25-01016" ref-type="bibr">29</xref>], and Parts and Attributes of Common Objects (PACO) [<xref rid="B30-sensors-25-01016" ref-type="bibr">30</xref>], incorporate comprehensive annotations, enriching the understanding of object attributes in various contexts.</p><sec id="sec2dot1-sensors-25-01016"><title>2.1. Datasets for Visual Object&#x000a0;Detection</title><p>Pascal VOC 2012 is a benchmark dataset featuring 11,530 images across 20 categories, with 27,540 Regions of Interest (RoIs) and 6929 segmentation annotations. Its straightforward categories and annotations have made it a fundamental resource for object detection and segmentation tasks. COCO comprises over 300,000 images annotated with 80 object categories and more than two million instances. Its diversity in scale, pose, and lighting conditions have established it as a valuable dataset for complex tasks such as object detection and panoptic segmentation. Open Images is one of the largest object detection datasets, containing <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> million images with 16 million bounding boxes for 600 categories. Its vast scale and extensive task support make it a versatile dataset for object recognition research.</p></sec><sec id="sec2dot2-sensors-25-01016"><title>2.2. Datasets for Visual Object Attribute Classification</title><p>COCO Attributes extends the COCO dataset by annotating 196 attributes for 180,000 objects across 84,044 images, resulting in <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> million object&#x02013;attribute pairs. These annotations offer a nuanced understanding of objects&#x02019; visual and contextual properties. WIDER Attributes [<xref rid="B40-sensors-25-01016" ref-type="bibr">40</xref>] focuses on human-specific characteristics, with 800,000 attribute labels across 13,789 images. Each bounding box includes 14 human attributes, such as age, clothing, and activity. The iMaterialist Fashion Attribute [<xref rid="B4-sensors-25-01016" ref-type="bibr">4</xref>] dataset specializes in fashion-related tasks, offering over 1 million images annotated with 228 fashion attributes grouped into eight categories. It provides high-quality, fine-grained labels for tasks like attribute recognition and clothing recommendation systems. VAW [<xref rid="B29-sensors-25-01016" ref-type="bibr">29</xref>] contains 72,000 images with 620 positive and negative visual attributes, resulting in 927,000 attribute labels, including color, shape, and texture, making it a valuable resource for attribute prediction.</p><p>While common object detection and object attribute classification datasets have played a vital role in advancing the field, they lack the specific characteristics required for the robust detection of gas cylinders and extraction of their attributes. To the best of our knowledge, there is a scarcity of dedicated datasets specifically designed for this purpose. The proposed CylinDeRS dataset fills this gap and aims to overcome the limitations of the existing publicly available datasets by providing a diverse collection of images capturing several arrangements of gas cylinders in challenging real-world scenarios.</p></sec></sec><sec id="sec3-sensors-25-01016"><title>3. The CylinDeRS Dataset&#x000a0;Creation</title><p>In this section, we describe the steps followed for the creation of the CylinDeRS dataset. Initially, the process of exploring candidate image sources for data acquisition and selecting retrieval keywords is presented. Next, the data pre-processing steps are defined, and the annotation pipeline is thoroughly showcased. The ethical considerations regarding individual privacy and the employed anonymization process are addressed. Finally, the dataset&#x02019;s structure is reported. These clearly defined steps also serve as a proposed methodology that could be used by researchers to construct their own domain-specific datasets.</p><sec sec-type="methods" id="sec3dot1-sensors-25-01016"><title>3.1. Dataset Creation&#x000a0;Methodology</title><p><xref rid="sensors-25-01016-f002" ref-type="fig">Figure 2</xref> illustrates the proposed methodology for creating domain-specific datasets. In the first step, candidate sources for the data acquisition process are identified. Each source undergoes exploration to identify relevant data (images and potentially available ground truth labels). In the next step, duplicate image data are removed using an image hashing algorithm, followed by manual review and removal processes, generating an updated image collection. The annotation process follows, involving meticulous curation through manual review, refinement of existing annotations, and addition of new annotations. Finally, the data are anonymized (by blurring identifiable faces) to address any privacy risks, resulting in the final list of images and ground truth annotations. This comprehensive process ensures the consistency and reliability of both image and annotation data, consequently promoting reproducibility and facilitating in-depth explorations and analyses of the dataset. Next, each of the main steps of this process are described in detail.</p><sec id="sec3dot1dot1-sensors-25-01016"><title>3.1.1. Data&#x000a0;Acquisition</title><p>The first step in the creation of the CylinDeRs dataset involves the collection of candidate images for gas cylinder detection and attribute classification. To this end, we identified potential sources by conducting extensive search, primarily focusing on publicly available repositories, including Kaggle (<uri xlink:href="https://www.kaggle.com/">https://www.kaggle.com/</uri> (accessed on 17 January 2025)), Roboflow (<uri xlink:href="https://roboflow.com/">https://roboflow.com/</uri> (accessed on 17 January 2025)), V7 Labs (<uri xlink:href="https://www.v7labs.com/open-datasets">https://www.v7labs.com/open-datasets</uri> (accessed on 17 January 2025)), and Google Datasets (<uri xlink:href="https://datasetsearch.research.google.com/">https://datasetsearch.research.google.com/</uri>) (accessed on 17 January 2025). Through this exploration, Roboflow [<xref rid="B41-sensors-25-01016" ref-type="bibr">41</xref>] was identified as the most suitable source for our specific scope as it offers publicly available datasets closely related to gas cylinder objects.</p><p>The proposed process of acquiring candidate images suggests utilizing a keyword-based search through synonyms, synonym sets, and English words linked to the prospective classes for the object detection and attribute classification tasks, resulting in more than fifty datasets [<xref rid="B34-sensors-25-01016" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-01016" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-01016" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-01016" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-01016" ref-type="bibr">38</xref>,<xref rid="B42-sensors-25-01016" ref-type="bibr">42</xref>,<xref rid="B43-sensors-25-01016" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-01016" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-01016" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-01016" ref-type="bibr">46</xref>,<xref rid="B47-sensors-25-01016" ref-type="bibr">47</xref>,<xref rid="B48-sensors-25-01016" ref-type="bibr">48</xref>,<xref rid="B49-sensors-25-01016" ref-type="bibr">49</xref>,<xref rid="B50-sensors-25-01016" ref-type="bibr">50</xref>,<xref rid="B51-sensors-25-01016" ref-type="bibr">51</xref>,<xref rid="B52-sensors-25-01016" ref-type="bibr">52</xref>,<xref rid="B53-sensors-25-01016" ref-type="bibr">53</xref>,<xref rid="B54-sensors-25-01016" ref-type="bibr">54</xref>,<xref rid="B55-sensors-25-01016" ref-type="bibr">55</xref>,<xref rid="B56-sensors-25-01016" ref-type="bibr">56</xref>,<xref rid="B57-sensors-25-01016" ref-type="bibr">57</xref>,<xref rid="B58-sensors-25-01016" ref-type="bibr">58</xref>,<xref rid="B59-sensors-25-01016" ref-type="bibr">59</xref>,<xref rid="B60-sensors-25-01016" ref-type="bibr">60</xref>,<xref rid="B61-sensors-25-01016" ref-type="bibr">61</xref>,<xref rid="B62-sensors-25-01016" ref-type="bibr">62</xref>,<xref rid="B63-sensors-25-01016" ref-type="bibr">63</xref>,<xref rid="B64-sensors-25-01016" ref-type="bibr">64</xref>,<xref rid="B65-sensors-25-01016" ref-type="bibr">65</xref>,<xref rid="B66-sensors-25-01016" ref-type="bibr">66</xref>,<xref rid="B67-sensors-25-01016" ref-type="bibr">67</xref>,<xref rid="B68-sensors-25-01016" ref-type="bibr">68</xref>,<xref rid="B69-sensors-25-01016" ref-type="bibr">69</xref>,<xref rid="B70-sensors-25-01016" ref-type="bibr">70</xref>,<xref rid="B71-sensors-25-01016" ref-type="bibr">71</xref>,<xref rid="B72-sensors-25-01016" ref-type="bibr">72</xref>,<xref rid="B73-sensors-25-01016" ref-type="bibr">73</xref>,<xref rid="B74-sensors-25-01016" ref-type="bibr">74</xref>,<xref rid="B75-sensors-25-01016" ref-type="bibr">75</xref>,<xref rid="B76-sensors-25-01016" ref-type="bibr">76</xref>,<xref rid="B77-sensors-25-01016" ref-type="bibr">77</xref>,<xref rid="B78-sensors-25-01016" ref-type="bibr">78</xref>,<xref rid="B79-sensors-25-01016" ref-type="bibr">79</xref>,<xref rid="B80-sensors-25-01016" ref-type="bibr">80</xref>,<xref rid="B81-sensors-25-01016" ref-type="bibr">81</xref>,<xref rid="B82-sensors-25-01016" ref-type="bibr">82</xref>,<xref rid="B83-sensors-25-01016" ref-type="bibr">83</xref>,<xref rid="B84-sensors-25-01016" ref-type="bibr">84</xref>,<xref rid="B85-sensors-25-01016" ref-type="bibr">85</xref>,<xref rid="B86-sensors-25-01016" ref-type="bibr">86</xref>,<xref rid="B87-sensors-25-01016" ref-type="bibr">87</xref>,<xref rid="B88-sensors-25-01016" ref-type="bibr">88</xref>,<xref rid="B89-sensors-25-01016" ref-type="bibr">89</xref>,<xref rid="B90-sensors-25-01016" ref-type="bibr">90</xref>]. In particular, the specific keywords we used are <italic toggle="yes">&#x0201c;cylinder&#x0201d;</italic>, <italic toggle="yes">&#x0201c;gas cylinder&#x0201d;</italic>, <italic toggle="yes">&#x0201c;gas tank&#x0201d;</italic>, <italic toggle="yes">&#x0201c;gas cylinder detection&#x0201d;</italic>, <italic toggle="yes">&#x0201c;gas cylinder classification&#x0201d;</italic>, <italic toggle="yes">&#x0201c;short cylinder&#x0201d;</italic>, <italic toggle="yes">&#x0201c;long cylinder&#x0201d;</italic>, <italic toggle="yes">&#x0201c;oxygen cylinder&#x0201d;</italic>, <italic toggle="yes">&#x0201c;lpg cylinder&#x0201d;</italic>, <italic toggle="yes">&#x0201c;horizontal cylinder&#x0201d;</italic>, <italic toggle="yes">&#x0201c;fallen cylinder&#x0201d;</italic>, <italic toggle="yes">&#x0201c;steel cylinder&#x0201d;</italic>, <italic toggle="yes">&#x0201c;metal cylinder&#x0201d;</italic>, <italic toggle="yes">&#x0201c;fiber cylinder&#x0201d;</italic>, <italic toggle="yes">&#x0201c;metal tank&#x0201d;</italic>, <italic toggle="yes">&#x0201c;steel tank&#x0201d;</italic>, and <italic toggle="yes">&#x0201c;fiber tank&#x0201d;</italic>. This approach yielded a diverse collection of 29,487 relevant images sourced from various settings, including industrial facilities, warehouses, and commercial spaces, covering a wide range of variations, accounting for diverse lighting conditions, occlusions, and cluttered backgrounds, reflecting the complexities encountered in real-world applications.</p></sec><sec id="sec3dot1dot2-sensors-25-01016"><title>3.1.2. Data&#x000a0;Filtering</title><p>Following the download of all the candidate images and the respective pre-existing annotations (if any), the next step was an extensive filtering process, meticulously curating the images to maintain high reliability and quality. As part of this process, a Python (<uri xlink:href="https://www.python.org/">https://www.python.org/</uri> (accessed on 17 January 2025)) script was employed to identify and remove duplicate images, streamlining the dataset for more efficient analysis. Initially, an MD5 hash-based approach [<xref rid="B91-sensors-25-01016" ref-type="bibr">91</xref>] was utilized to eliminate redundant copies while storing one representative image from each set of duplicates. This step reduced the total number of images to 12,563. However, the manual inspection of the dataset images revealed that this approach occasionally retained multiple copies of nearly identical photos due to subtle variations, leading to distinct MD5 hash values.</p><p>To enhance the precision of duplicate identification, a more sophisticated approach was adopted, incorporating perceptual hashing, specifically the pHash (<uri xlink:href="https://github.com/JohannesBuchner/imagehash">https://github.com/JohannesBuchner/imagehash</uri> (accessed on 17 January 2025)) algorithm. The updated script, which integrated the imagehash (<uri xlink:href="https://pypi.org/project/ImageHash/">https://pypi.org/project/ImageHash/</uri> (accessed on 17 January 2025)) library, achieved more robust and accurate performance on discerning and managing similar images within the dataset, ensuring the preservation of distinct visual content while effectively eliminating redundancy. As a concluding step in the filtering process, the remaining images underwent manual review to identify and remove any duplicate images the algorithm may have overlooked. After this process, we concluded with a total collection of 7060 images.</p></sec><sec id="sec3dot1dot3-sensors-25-01016"><title>3.1.3. Data&#x000a0;Annotation</title><p>The next step includes the annotation process of the curated collection of images through the inspection and refinement of the potentially pre-existing annotations and the addition of new annotations. A team of four researchers was employed, each responsible for manually annotating a subset of the dataset. The splits were overlapping, so it could be ensured that every image (and subsequently each instance) is annotated by at least two annotators. In case of a disagreement, a third annotator was involved to resolve it.</p><p>It is essential to highlight the distinction between the two tasks covered in this study: gas cylinder detection and gas cylinder attribute classification. For the detection task, the CylinDeRS dataset has been thoroughly annotated with precise bounding box annotations around the objects of interest, providing the location information (bounding box coordinates) and the corresponding class (i.e., &#x0201c;gas cylinder&#x0201d;) for every gas cylinder instance in the image. For the attribute classification, each instance is associated with three labels describing its material, size, and orientation attributes. Currently, CylinDeRS supports three attributes with a total of nine classes: material (metal, fiber, and unknown material), size (short, long, and unknown size), and orientation (standing, fallen, and unknown orientation).</p><p>The annotation process for each image and the respective instances followed a systematic procedure consisting of three essential steps:<list list-type="bullet"><list-item><p><bold>Inspection:</bold> Firstly, a comprehensive manual inspection of the pre-existing annotations should be considered, as provided by the original datasets gathered during the data acquisition step (<xref rid="sec3dot1dot1-sensors-25-01016" ref-type="sec">Section 3.1.1</xref>). In our case, this critical examination aimed to evaluate the initial accuracy and reliability of the bounding box annotations. Moreover, the process involves a thorough examination of the dataset to identify images where gas cylinders lack bounding box annotations. Any duplicate annotations of the same instance are removed, and potential discrepancies or errors in the existing annotations are identified, establishing a solid foundation for subsequent refinement and the creation of new annotations. Overall, a total of 448 duplicate bounding boxes were identified, 672 bounding boxes were missing, and 3239 instances contained errors in the existing annotations since the bounding boxes were not perfectly aligned with the instances, requiring resizing or adjustments in position (left, right, up, or down).</p></list-item><list-item><p><bold>Refinements:</bold> The next stage revolves around refining the size and labels of the pre-existing bounding boxes. In CylinDeRS, a diligent process was followed where the bounding boxes were manually adjusted to precisely encapsulate the gas cylinder instances, maximizing their accuracy and adherence to the true boundaries of the objects. Furthermore, the original labels associated with the pre-existing bounding boxes were refined, ensuring they accurately represent the class of &#x0201c;gas cylinder&#x0201d;, which is in scope for the proposed dataset&#x02019;s object detection task. This iterative refinement process elevated the precision and consistency of the annotations, further enhancing the dataset&#x02019;s overall quality.</p></list-item><list-item><p><bold>New annotations:</bold> The final stage of the annotation process resides in providing new annotations for the objects of interest. This process involves generating new annotations, serving a two-fold purpose: rectifying missing bounding box annotations for gas cylinder objects and assigning instance-level labels for the attribute classification task. Initially, the process includes the annotation of gas cylinder instances in images lacking annotations as well as in those with partial annotations, as identified through the inspection step. The boundaries of the gas cylinders are precisely delineated, accompanied by the corresponding &#x0201c;gas cylinder&#x0201d; class. Each instance is associated with an additional set of three labels representing the gas cylinder&#x02019;s material, size, and orientation attributes, considering the overall context of the image. The new annotations contribute to a comprehensive dataset that accurately depicts the gas cylinders&#x02019; spatial arrangement, also providing detailed information about their attributes.</p></list-item></list></p></sec><sec id="sec3dot1dot4-sensors-25-01016"><title>3.1.4. Ethical and Legal&#x000a0;Considerations</title><p>The CylinDeRS dataset, while primarily focused on gas cylinder objects, inadvertently features instances of individuals. Through a rigorous manual inspection of the entire dataset, individuals were identified in 800 out of the 7060 images. Considering that all collected images sourced from the datasets [<xref rid="B34-sensors-25-01016" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-01016" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-01016" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-01016" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-01016" ref-type="bibr">38</xref>,<xref rid="B42-sensors-25-01016" ref-type="bibr">42</xref>,<xref rid="B43-sensors-25-01016" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-01016" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-01016" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-01016" ref-type="bibr">46</xref>,<xref rid="B47-sensors-25-01016" ref-type="bibr">47</xref>,<xref rid="B48-sensors-25-01016" ref-type="bibr">48</xref>,<xref rid="B49-sensors-25-01016" ref-type="bibr">49</xref>,<xref rid="B50-sensors-25-01016" ref-type="bibr">50</xref>,<xref rid="B51-sensors-25-01016" ref-type="bibr">51</xref>,<xref rid="B52-sensors-25-01016" ref-type="bibr">52</xref>,<xref rid="B53-sensors-25-01016" ref-type="bibr">53</xref>,<xref rid="B54-sensors-25-01016" ref-type="bibr">54</xref>,<xref rid="B55-sensors-25-01016" ref-type="bibr">55</xref>,<xref rid="B56-sensors-25-01016" ref-type="bibr">56</xref>,<xref rid="B57-sensors-25-01016" ref-type="bibr">57</xref>,<xref rid="B58-sensors-25-01016" ref-type="bibr">58</xref>,<xref rid="B59-sensors-25-01016" ref-type="bibr">59</xref>,<xref rid="B60-sensors-25-01016" ref-type="bibr">60</xref>,<xref rid="B61-sensors-25-01016" ref-type="bibr">61</xref>,<xref rid="B62-sensors-25-01016" ref-type="bibr">62</xref>,<xref rid="B63-sensors-25-01016" ref-type="bibr">63</xref>,<xref rid="B64-sensors-25-01016" ref-type="bibr">64</xref>,<xref rid="B65-sensors-25-01016" ref-type="bibr">65</xref>,<xref rid="B66-sensors-25-01016" ref-type="bibr">66</xref>,<xref rid="B67-sensors-25-01016" ref-type="bibr">67</xref>,<xref rid="B68-sensors-25-01016" ref-type="bibr">68</xref>,<xref rid="B69-sensors-25-01016" ref-type="bibr">69</xref>,<xref rid="B70-sensors-25-01016" ref-type="bibr">70</xref>,<xref rid="B71-sensors-25-01016" ref-type="bibr">71</xref>,<xref rid="B72-sensors-25-01016" ref-type="bibr">72</xref>,<xref rid="B73-sensors-25-01016" ref-type="bibr">73</xref>,<xref rid="B74-sensors-25-01016" ref-type="bibr">74</xref>,<xref rid="B75-sensors-25-01016" ref-type="bibr">75</xref>,<xref rid="B76-sensors-25-01016" ref-type="bibr">76</xref>,<xref rid="B77-sensors-25-01016" ref-type="bibr">77</xref>,<xref rid="B78-sensors-25-01016" ref-type="bibr">78</xref>,<xref rid="B79-sensors-25-01016" ref-type="bibr">79</xref>,<xref rid="B80-sensors-25-01016" ref-type="bibr">80</xref>,<xref rid="B81-sensors-25-01016" ref-type="bibr">81</xref>,<xref rid="B82-sensors-25-01016" ref-type="bibr">82</xref>,<xref rid="B83-sensors-25-01016" ref-type="bibr">83</xref>,<xref rid="B84-sensors-25-01016" ref-type="bibr">84</xref>,<xref rid="B85-sensors-25-01016" ref-type="bibr">85</xref>,<xref rid="B86-sensors-25-01016" ref-type="bibr">86</xref>,<xref rid="B87-sensors-25-01016" ref-type="bibr">87</xref>,<xref rid="B88-sensors-25-01016" ref-type="bibr">88</xref>,<xref rid="B89-sensors-25-01016" ref-type="bibr">89</xref>,<xref rid="B90-sensors-25-01016" ref-type="bibr">90</xref>] are licensed under the CC-BY <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> license, enabling derivative works, the ethical considerations were explored, particularly regarding individual privacy. In adherence to legal frameworks and ethical principles, we addressed potential identifiability concerns and privacy risks associated with the dataset. As part of our commitment to responsible data usage, all identifiable faces within the dataset exhibiting clear details have been anonymized using a blurring technique. This precautionary measure mitigates the risk of unintentional identification, ensuring compliance with data privacy regulations and ethical standards. It is important to note that anonymization was applied selectively; for faces with limited visibility, anonymization was deemed unnecessary to maintain the dataset&#x02019;s utility for gas cylinder object analysis while still adhering to ethical standards. This balanced approach emphasizes our commitment to responsible data management.</p></sec><sec id="sec3dot1dot5-sensors-25-01016"><title>3.1.5. Challenges</title><p>The creation of the CylinDeRS dataset involved several challenges that impacted both the annotation process and the overall representativeness of the dataset. One major challenge was the annotation complexity, especially when dealing with images containing multiple gas cylinders or instances where the gas cylinders were occluded. Drawing accurate bounding boxes around the gas cylinders and assigning the correct attributes required extensive manual review. Another challenge was the variability in gas cylinder appearance as gas cylinders differ widely in shape, size, and material depending on their use and environment. Cylinders in industrial settings, for example, are often stored in groups, stacked, or partially obscured by other objects, necessitating careful selection of images and data augmentation strategies to ensure a representative sample. Environmental factors also played a role, with the surrounding context&#x02014;such as lighting, background clutter, or the presence of large objects&#x02014;affecting the apparent size and orientation of gas cylinders. For instance, a gas cylinder might appear smaller if placed next to a large building or vehicle, requiring semantic analysis to accurately determine its size. Lastly, the dataset faced a data imbalance issue as some attribute categories, such as &#x0201c;fallen&#x0201d; and &#x0201c;unknown&#x0201d; orientations, had fewer instances compared to more common categories like &#x0201c;standing&#x0201d; or &#x0201c;short&#x0201d;. This imbalance could introduce bias in model training, making it more challenging for algorithms to accurately detect and classify less frequent scenarios.</p></sec></sec><sec id="sec3dot2-sensors-25-01016"><title>3.2. CylinDeRS&#x000a0;Dataset</title><p>The result of the aforementioned steps is an extensive collection of 7060 images, each featuring a variable number of gas cylinder objects. In particular, CylinDeRS contains a total of 25,269 instances of gas cylinders, with an average of <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> instances per image. <xref rid="sensors-25-01016-t001" ref-type="table">Table 1</xref> provides an overall understanding of the dataset&#x02019;s scale and presents a comprehensive overview of its structure. It details the distribution of images and gas cylinder instances as established after the annotation process, ensuring a fair distribution of both the number of images and instances among training, validation, and test sets. The table also includes the average number of gas cylinders per image for each set. Notably, following a 70:20:10 split ratio, the dataset comprises 4915 training images, 1434 validation images, and 711 test images, with 18,137 instances in the training, 4862 in the validation, and 2270 gas cylinders in the test set.</p><p><xref rid="sensors-25-01016-f003" ref-type="fig">Figure 3</xref> showcases a selection of representative samples for the two supported tasks, providing a glimpse into the dataset&#x02019;s content and highlighting its varied and complex nature. In particular, <xref rid="sensors-25-01016-f003" ref-type="fig">Figure 3</xref>a illustrates indicative samples regarding the gas cylinder detection task, where each cylinder object is localized and the corresponding bounding box is delineated. Beyond bounding boxes, the dataset includes attribute annotations for each gas cylinder instance. <xref rid="sensors-25-01016-f003" ref-type="fig">Figure 3</xref>b showcases representative samples for the supported attributes used in the attribute classification task. These annotations delve into various aspects and characteristics, enhancing the dataset&#x02019;s utility and expanding its applicability.</p><p>To comprehensively characterize gas-cylinder-related objects, we defined three key attributes based on [<xref rid="B92-sensors-25-01016" ref-type="bibr">92</xref>], each with at least three associated classes:</p><p><bold>Material:</bold> The CylinDeRS dataset covers three classes in terms of the material attribute, &#x0201c;metal&#x0201d;, &#x0201c;fiber&#x0201d;, and &#x0201c;material unk(nown)&#x0201d;, to represent distinct compositions of gas cylinders. Cylinders in the &#x0201c;metal&#x0201d; class are primarily composed of metallic materials, such as steel or aluminum [<xref rid="B92-sensors-25-01016" ref-type="bibr">92</xref>] (p. 73), exhibiting robustness, strength, and durability, making them suitable for various industrial, commercial, and medical settings. The &#x0201c;fiber&#x0201d; material class comprises gas cylinders consisting of an inner container that is over-wrapped with durable lightweight fiber-based materials (glass, aramid, or carbon), offering a high strength-to-weight ratio [<xref rid="B92-sensors-25-01016" ref-type="bibr">92</xref>] (p. 70). They find applications in scenarios where weight reduction is essential, such as aerospace or portable gas storage for outdoor activities. The &#x0201c;material unk(nown)&#x0201d; class encompasses gas cylinders that may lack distinct features and exhibit irregularities or burn marks, complicating the identification of their material composition.</p><p><bold>Size:</bold> The size attribute corresponds to three classes, &#x0201c;short&#x0201d;, &#x0201c;long&#x0201d;, and &#x0201c;size unk(nown)&#x0201d;, representing variations in the physical dimensions of gas cylinders and providing granularity for size-based categorization. It is important to emphasize that defining the size attribute of an object within an image requires semantic analysis of the content as the surrounding scene significantly contributes to the contextual interpretation of the object&#x02019;s size. For instance, a gas cylinder next to a skyscraper will appear smaller than the same cylinder next to a bicycle. The CylinDeRS dataset considers this semantic information during annotation, enabling accurate size attribute determination. Gas cylinders categorized as &#x0201c;short&#x0201d; have relatively compact vertical length (height) dimensions with a low Length-to-Diameter ratio (L/D), primarily used for low-pressure liquefied gas services [<xref rid="B92-sensors-25-01016" ref-type="bibr">92</xref>] (p. 70). They are typically more maneuverable and more accessible to handle in environments with spatial constraints, finding application in diverse settings such as domestic use and laboratories. Conversely, &#x0201c;long&#x0201d; instances in the CylinDeRS dataset exhibit taller dimensions compared to their &#x0201c;short&#x0201d; counterparts, with a high L/D ratio accommodating larger volumes of gases. They are generally used for high-pressure non-liquefied gas service [<xref rid="B92-sensors-25-01016" ref-type="bibr">92</xref>] (p. 70) and are usually well suited for applications requiring extended usage before replacement or refill, such as industrial processes and manufacturing. Within the &#x0201c;size unk(nown)&#x0201d; class, the instances predominantly feature occluded gas cylinders, resulting in difficulty in determining their size. These occlusions are usually due to structural obstacles (walls, beams, pillars, etc.), stacking/overlapping arrangements (stored in storage facilities, transportation vehicles, etc.), or partial obstruction by other objects.</p><p><bold>Orientation:</bold> Three classes are supported for the orientation attribute, &#x0201c;standing&#x0201d;, &#x0201c;fallen&#x0201d;, and &#x0201c;orientation unk(nown)&#x0201d;. Similarly to the size attribute, orientation is inherently a contextual feature, with its determination depending on the semantic interpretation of the given scene. In CylinDeRS, semantic information related to the orientation of the gas cylinders is incorporated during annotation, wherein the context of the entire image is taken into consideration before assigning labels to specific instances within the image. Gas cylinders labeled as &#x0201c;standing&#x0201d; are positioned upright, with their longitudinal axis aligned vertically. They are typically secured stably, ensuring proper containment of pressurized gases [<xref rid="B92-sensors-25-01016" ref-type="bibr">92</xref>] (pp. 186, 228, 494, 525). Gas cylinders categorized as &#x0201c;fallen&#x0201d; are depicted in a horizontal or tilted orientation, suggesting that they are not upright and have deviated from their standard standing position. This class indicates instances of gas cylinders potentially posing safety concerns due to the risk of uncontrolled movement or gas release.</p><p><xref rid="sensors-25-01016-t002" ref-type="table">Table 2</xref> provides detailed statistics regarding the different attribute categories. As depicted in the table&#x02019;s last column, the &#x0201c;metal&#x0201d; category dominates the material attribute with a total of 20,514 instances, with &#x0201c;fiber&#x0201d; gas cylinders depicted in 4518 instances, while the &#x0201c;material unk&#x0201d; category exhibits comparatively lower counts of 237. Within the size attribute, the total instances labeled as &#x0201c;short&#x0201d; are 13,450, followed by &#x0201c;long&#x0201d; with 6528 and &#x0201d;size unk&#x0201d; with 5291. For the orientation attribute, the total &#x0201c;standing&#x0201d; instances, 23,311, significantly outnumber the others, with the &#x0201c;fallen&#x0201d; and &#x0201c;orientation unk&#x0201d; classes depicted in 1379 and 579 instances, respectively.</p><p>The scarcity of instances in the &#x0201c;material unk&#x0201d;, &#x0201c;fallen&#x0201d;, and &#x0201c;orientation unk&#x0201d; categories within the CylinDeRS dataset can be attributed to the relatively uncommon nature of these scenarios in real-world settings. Burnt or damaged gas cylinders (&#x0201c;material unk&#x0201d;) are uncommon in typical industrial and warehouse settings. Similarly, safety regulations minimize instances of &#x0201c;fallen&#x0201d; gas cylinders that are generally securely positioned and adherent to safety standards to prevent accidents. Furthermore, the limited instances within the &#x0201c;orientation unk&#x0201d; class primarily involve gas cylinders where the visible cues and the semantic context necessary for accurate orientation determination are limited. This is an unusual scenario in real-world cases. The imbalances in these categories reflect the dataset&#x02019;s fidelity to real-world occurrences, where certain situations are less frequent, making them crucial for training models to also recognize and handle such uncommon scenarios effectively.</p></sec></sec><sec id="sec4-sensors-25-01016"><title>4. Experimental&#x000a0;Setup</title><p>This section delves into the details of the conducted experiments, covering the object detection and attribute classification algorithms selected for training and evaluation using the introduced gas-cylinder-related dataset. The primary objective of these experiments is to assess the effectiveness and robustness of the selected models in detecting gas cylinders and classifying their attributes, with a particular focus on enhancing their practical utility in real-world scenarios. Moreover, the implementation and training processes are presented, along with the augmentation techniques and the hardware and software configurations. Finally, the metrics adopted for evaluating the models are discussed.</p><sec id="sec4dot1-sensors-25-01016"><title>4.1. Gas Cylinder&#x000a0;Detection</title><p>For the gas cylinder detection experiments, SotA object detection algorithms are selected from three different object detector categories in order to explore a wide range of architectures: Convolutional Neural Network (CNN)-based two-stage methods, CNN-based single-stage approaches, and transformer-based architectures. The representative models Faster R-CNN [<xref rid="B93-sensors-25-01016" ref-type="bibr">93</xref>], YOLOv8 [<xref rid="B94-sensors-25-01016" ref-type="bibr">94</xref>], YOLOv11 [<xref rid="B95-sensors-25-01016" ref-type="bibr">95</xref>], and RT-DETR [<xref rid="B96-sensors-25-01016" ref-type="bibr">96</xref>] are utilized for each respective category:</p><p><bold>Faster R-CNN</bold> [<xref rid="B93-sensors-25-01016" ref-type="bibr">93</xref>] integrates a Region Proposal Network (RPN) into its architecture, improving object detection performance. For experiments, we use Faster R-CNN with ResNet-101 [<xref rid="B97-sensors-25-01016" ref-type="bibr">97</xref>] and Feature Pyramid Network (FPN) [<xref rid="B98-sensors-25-01016" ref-type="bibr">98</xref>], which enhances detection, especially for smaller objects.</p><p><bold>YOLOv8</bold> [<xref rid="B94-sensors-25-01016" ref-type="bibr">94</xref>] is a unified model that predicts bounding boxes and object classes in a single pass. It features components like the Backbone, Spatial Pyramid Pooling Fusion (SPPF) layer, and C2f module for high accuracy and speed. We use the &#x0201c;YOLOv8m&#x0201d; version for its favorable trade-off between performance and computational cost.</p><p><bold>YOLOv11</bold> [<xref rid="B95-sensors-25-01016" ref-type="bibr">95</xref>] is the latest model in the YOLO family, combining adaptive attention mechanisms and improved feature fusion for real-time performance and precise detection across various object scales. The &#x0201c;YOLOv11m&#x0201d; model is used in our experiments.</p><p><bold>RT-DETR</bold> [<xref rid="B96-sensors-25-01016" ref-type="bibr">96</xref>] leverages transformer-based multi-head attention for real-time object detection. It uses an efficient hybrid encoder for multi-scale feature interaction. We use the &#x0201c;rtdetr-l&#x0201d; implementation for consistency with YOLOv8 and YOLOv11 in terms of model comparison. Upon acceptance of this work, all models trained for the gas cylinder detection task on the proposed CylinDeRS dataset will be made accessible to the community.</p></sec><sec id="sec4dot2-sensors-25-01016"><title>4.2. Gas Cylinder Attribute Classification</title><p>Regarding object attribute classification, CNNs have also been the predominant selection in recent years [<xref rid="B99-sensors-25-01016" ref-type="bibr">99</xref>,<xref rid="B100-sensors-25-01016" ref-type="bibr">100</xref>]. Object attribute prediction is formally characterized as a multi-label classification task, requiring the prediction of all attributes associated with a given object [<xref rid="B29-sensors-25-01016" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-01016" ref-type="bibr">30</xref>,<xref rid="B100-sensors-25-01016" ref-type="bibr">100</xref>]. However, the multi-label approach is unsuitable for the objective of this work, which requires the prediction of exactly three labels per gas cylinder instance&#x02014;one for each attribute. This requirement arises from the constraint that an object cannot simultaneously exhibit more than one class of the same attribute. Moreover, the conventional multi-class classification method is not applicable due to its inherent design for assigning a single label per object that contradicts the core requirement of CylinDeRS, which necessitates the prediction of three labels (material, size, and orientation) for each instance.</p><p>To overcome these issues, we adopted a multi-head multi-class classification approach for this task that enables the extraction of three labels for each gas cylinder instance, resulting in a single prediction for each attribute type, in one pass (see <xref rid="sensors-25-01016-f004" ref-type="fig">Figure 4</xref>). By approaching this as a multi-class classification task with three different output heads, the conducted experiments involve the exploration and modification of SotA deep-learning-based classifiers. To provide a set of widely used pre-trained models to the research community and for implementation simplicity, the widely used ResNet family [<xref rid="B97-sensors-25-01016" ref-type="bibr">97</xref>] of classifiers, specifically ResNet-50 and ResNet-101, are selected and adapted to effectively support this multifaceted task.</p><p><bold>ResNet-50</bold> [<xref rid="B97-sensors-25-01016" ref-type="bibr">97</xref>] is a pivotal member of the ResNet family. As a variant of the ResNet architecture with 50 layers, it has established itself as a benchmark in computer vision tasks. Employing skip connections and forming residual blocks, ResNet-50 facilitates the smooth flow of information through the network and excels in capturing representative features. <bold>ResNet-101</bold> [<xref rid="B97-sensors-25-01016" ref-type="bibr">97</xref>] is another member of the ResNet family that extends the architecture to 101 layers. It builds upon the success of ResNet-50, utilizing a larger number of residual connections and blocks, providing a deeper model. The additional layers contribute to a more comprehensive understanding of image features, potentially improving accuracy, especially in scenarios with complex details or subtle variations.</p><p>In the experiments, the PyTorch implementation (<uri xlink:href="https://pytorch.org/vision/main/models/resnet.html">https://pytorch.org/vision/main/models/resnet.html</uri> (accessed on 17 January 2025)) of the ResNet-50 and ResNet-101 models is deployed, where the stride for downsampling is placed to the second <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution. Upon acceptance of this work, both models trained for the attribute classification task using the proposed CylinDeRS dataset will be made publicly available.</p></sec><sec id="sec4dot3-sensors-25-01016"><title>4.3. Implementation and&#x000a0;Training</title><p>To achieve improved performance, transfer learning is employed for the selected models for both object detection and object attribute classification tasks. Additionally, during the training process, we carefully tune a range of hyperparameters for both tasks, including the learning rate, batch size, momentum, weight decay, and optimization algorithms, through an extensive validation process. This leads to reporting optimal performance with an ideal balance between convergence speed and generalization capability for the training models that are evaluated in this work.</p><p>In order to conduct a fair comparison, all models for the gas cylinder detection task are pre-trained on the widely used COCO dataset. In terms of hyperparameters, the learning rate is initially set to <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and decayed by a factor of <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> at the 50th and 100th epochs. The total number of training epochs is 200. The models are trained using Stochastic Gradient Descent (SGD) [<xref rid="B101-sensors-25-01016" ref-type="bibr">101</xref>] as the optimizer with a batch size of 32, momentum of <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and weight decay of <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The deep learning models for the gas cylinder detection task are trained using the training set and evaluated on the validation and test sets of CylinDeRS; the evaluation results are summarized below in <xref rid="sensors-25-01016-t003" ref-type="table">Table 3</xref> and discussed in <xref rid="sec5-sensors-25-01016" ref-type="sec">Section 5</xref>.</p><p>For the attribute classification task, the selected models are fine-tuned using pre-trained ImageNet weights to establish a benchmark comparison. The training process runs for a total of 150 epochs, with an initial learning rate of <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, which decays by a factor of <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> every 40 epochs. The Adam optimizer [<xref rid="B102-sensors-25-01016" ref-type="bibr">102</xref>] is used, with a batch size of 32. To prepare the dataset for attribute classification, all cylinder objects are extracted from the images using ground truth bounding boxes, creating a collection of 25,269 images resized to <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>224</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>224</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels for model input. The models are trained on the CylinDeRS training set, with their performance evaluated on both the validation and test sets; the evaluation results are summarized below in <xref rid="sensors-25-01016-t004" ref-type="table">Table 4</xref> and discussed in <xref rid="sec5-sensors-25-01016" ref-type="sec">Section 5</xref>.</p><p>It is important to note that padding is applied to preserve the original features of the gas cylinder instances, preventing the neural network from learning potentially inaccurate features during training. This strategic use of padding ensures the network&#x02019;s understanding of the spatial characteristics and contextual relationships within and around each cylinder. During convolution operations, the input image undergoes a series of convolution filters, potentially shrinking its spatial dimensions (height and width) [<xref rid="B103-sensors-25-01016" ref-type="bibr">103</xref>]. Padding addresses this issue by adding a border of zeros around the original image. This maintains the original spatial dimensions throughout the network, enabling the CNN to capture features from the entire image. Without padding, these features might be distorted due to the shrinking size, potentially leading to inaccurate feature learning and classification errors. Furthermore, the edge pixels might be excluded from the convolution operation, causing the network to miss these potentially informative details. This can lead to the network learning inaccurate features that are biased towards the center of the image, ultimately impacting classification performance.</p></sec><sec id="sec4dot4-sensors-25-01016"><title>4.4. Data&#x000a0;Augmentation</title><p>To address the issue of data imbalance and enhance the generalization capability of the models, particularly for underrepresented classes (i.e., &#x0201c;fallen&#x0201d; and &#x0201c;material unknown&#x0201d;), data augmentation pre-processing techniques were employed. These techniques aim to artificially increase the diversity of the training dataset, reducing the risk of overfitting and improving the model&#x02019;s robustness. These techniques include random cropping and horizontal flipping as the geometrical transformations, and contrast and brightness enhancement as the intensity adjustments. A random crop is an arbitrary sample of the original image. The randomly cropped portion is resized to the original image size and fed to the network. Horizontal flipping involves flipping the image horizontally 180 degrees to increase the diversity of the samples. The contrast enhancement method expands the range between the image&#x02019;s lightest and darkest pixels, resulting in a more pronounced distinction between features and details. Brightness enhancement focuses on adjusting an image&#x02019;s overall luminance or brightness level for better visual perception.</p></sec><sec id="sec4dot5-sensors-25-01016"><title>4.5. Hardware and Software&#x000a0;Configurations</title><p>All experiments were conducted on a computer system with Ubuntu 20.04 equipped with an Intel Core i7 3.6 GHz CPU, 128 GB RAM, and an Nvidia RTX 3090 GPU with 24 GB VRAM. The software stack consists of the Python language, along with popular computer vision libraries such as OpenCV (<uri xlink:href="https://opencv.org/">https://opencv.org/</uri> (accessed on 17 January 2025)), TensorFlow (<uri xlink:href="https://tensorflow.org/">https://tensorflow.org/</uri> (accessed on 17 January 2025)), and PyTorch (<uri xlink:href="https://pytorch.org/">https://pytorch.org/</uri> (accessed on 17 January 2025)). The deep learning models are trained and evaluated with GPU acceleration, harnessing the enhanced processing capabilities for efficient training and inference.</p><p>The training times for the object detection models on the CylinDeRS dataset vary depending on the architecture. Faster R-CNN required approximately 12 h, while YOLOv8m demonstrated faster training, taking around 4 h. YOLOv11m showed a similarly efficient training time of approximately 3 h and 30 m, reflecting its optimized architecture for quicker convergence. RT-DETR, with its transformer-based architecture, took longer, requiring around 6 h and 30 m due to the additional computational overhead. For the attribute classification task, ResNet-50 required approximately 5 h for the training process, while ResNet-101 took around 7 h, reflecting the increased complexity of the deeper ResNet-101 architecture compared to ResNet-50.</p></sec><sec id="sec4dot6-sensors-25-01016"><title>4.6. Evaluation&#x000a0;Metrics</title><p>To assess the performance of the gas cylinder detection algorithms, the evaluation metrics commonly used in object detection tasks are adopted: precision, recall, and mean average precision (mAP). Precision measures the accuracy of positive predictions made by an object detection model, while recall quantifies the ability of an object detection model to find all relevant objects in the provided data. The mAP metric evaluates the overall performance of an object detection model across multiple object classes. It combines both precision and recall by calculating the area under the precision&#x02013;recall curve&#x02014;known as average precision (AP)&#x02014;for each class and then taking the mean of these AP scores. The values are calculated as depicted in the following equations:<disp-formula id="FD1-sensors-25-01016"><label>(1)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Precision</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Recall</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mi mathvariant="bold">mAP</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:munderover><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where TP, FP, FN, and AP denote the true positive, false positive, false negative, and average precision values, respectively.</p><p>For the evaluation of the attribute classification models, the accuracy evaluation metric is utilized. Accuracy is a highly intuitive way to evaluate the performance of any classification algorithm by calculating the percentage of the correct predictions. Specifically, it corresponds to the division of the number of correct predictions with the total number of predictions, as shown below:<disp-formula id="FD2-sensors-25-01016"><label>(2)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Accuracy</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where TP, TN, FP, and FN denote the true positive, true negative, false positive, and false negative values, respectively.</p></sec></sec><sec sec-type="results" id="sec5-sensors-25-01016"><title>5. Results and&#x000a0;Discussion</title><p>In this section, the evaluation of the performance of SotA models on the CylinDeRS dataset is presented regarding both gas cylinder detection and attribute classification tasks. By conducting this evaluation, we aim to understand the strengths and limitations of the models and identify potential areas for improvement, which is essential for advancing the accuracy and reliability of automated systems for real-world applications. The results for each analysis are thoroughly presented, and a comprehensive discussion unfolds around the outcomes.</p><sec id="sec5dot1-sensors-25-01016"><title>5.1. Gas Cylinder Detection&#x000a0;Results</title><p>We initiate the evaluation by conducting a series of experiments to compare the performance of SotA algorithms on the CylinDeRS dataset. Four object detection methods were selected, Faster R-CNN [<xref rid="B93-sensors-25-01016" ref-type="bibr">93</xref>], YOLOv8 [<xref rid="B94-sensors-25-01016" ref-type="bibr">94</xref>], YOLOv11 [<xref rid="B95-sensors-25-01016" ref-type="bibr">95</xref>], and RT-DETR [<xref rid="B96-sensors-25-01016" ref-type="bibr">96</xref>] (as introduced in <xref rid="sec4dot1-sensors-25-01016" ref-type="sec">Section 4.1</xref>), each representing a unique category in terms of how they handle and process images.</p><sec id="sec5dot1dot1-sensors-25-01016"><title>5.1.1. Performance&#x000a0;Evaluation</title><p>The summary of the quantitative evaluation results, employing the three selected state-of-the-art models and various metrics, is presented in <xref rid="sensors-25-01016-t003" ref-type="table">Table 3</xref>. The evaluated deep-learning-based models report adequate results in terms of precision, recall, and mAP regarding both the validation (Val) and test (Test) sets. YOLOv11 exhibits the highest performance across all the metrics, achieving a notable precision of <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.915</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> on the validation set and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.905</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> on the test set, alongside strong recall values of <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.853</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Val) and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.816</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Test), resulting in high mAP scores of <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.923</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Val) and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.910</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Test). This suggests that YOLOv11 maintains consistency across both dataset splits, reinforcing its reliability. Meanwhile, YOLOv8 and RT-DETR also exhibit strong performance, with slightly lower precision than YOLOv11 but maintaining competitive mAP scores of <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.916</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.922</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Val) and <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.904</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.903</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Test), respectively, highlighting their balanced precision&#x02013;recall profiles. In comparison, Faster R-CNN shows relatively moderate performance. With a validation precision of <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.640</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and a test precision of 0.610, alongside recall values of <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.568</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Val) and <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.550</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Test), this two-stage model may benefit from enhancements, particularly in terms of recall, on both sets. Overall, the quantitative analysis reveals that, within the proposed dataset, the single-stage approach for object detection outperforms the two-stage approach and exhibits a performance advantage over the transformer-based method.</p><p>To gain deeper insights into the performance of the fine-tuned models, qualitative results are also presented in <xref rid="sensors-25-01016-f005" ref-type="fig">Figure 5</xref>. Example images from the CylinDeRS dataset are showcased along with the corresponding detection results for each of the models. In the left column, the ground truth bounding box annotations are highlighted in yellow color, delineating the boundaries of each gas cylinder instance in the showcased images from the CylinDeRS dataset within the image. The remaining three columns illustrate the predicted output for each of the explored models, featuring bounding boxes around detected gas cylinder instances, accompanied by the corresponding mAP scores. We can observe that Faster R-CNN exhibits moderate performance with lower confidence scores and a higher number of false positives. YOLOv8 and YOLOv11 demonstrate the best overall performance, accurately detecting most gas cylinder instances with high confidence scores. RT-DETR also performs well, achieving results similar to the YOLO models, with high confidence scores, albeit missing detecting a challenging instance.</p><table-wrap position="anchor" id="sensors-25-01016-t003"><object-id pub-id-type="pii">sensors-25-01016-t003_Table 3</object-id><label>Table 3</label><caption><p>Gas cylinder detection performance comparison of Faster R-CNN, YOLOv8, YOLOv11, and RT-DETR fine-tuned using the CylinDeRS dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Model</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Precision</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Recall</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">mAP</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Val
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Test
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Val
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Test
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Val
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Test
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Faster R-CNN&#x000a0;[<xref rid="B93-sensors-25-01016" ref-type="bibr">93</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.640</td><td align="center" valign="middle" rowspan="1" colspan="1">0.610</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.568</td><td align="center" valign="middle" rowspan="1" colspan="1">0.550</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.612</td><td align="center" valign="middle" rowspan="1" colspan="1">0.590</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8&#x000a0;[<xref rid="B94-sensors-25-01016" ref-type="bibr">94</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.915</td><td align="center" valign="middle" rowspan="1" colspan="1">0.902</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.850</td><td align="center" valign="middle" rowspan="1" colspan="1">0.812</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.916</td><td align="center" valign="middle" rowspan="1" colspan="1">0.904</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv11&#x000a0;[<xref rid="B95-sensors-25-01016" ref-type="bibr">95</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.915</td><td align="center" valign="middle" rowspan="1" colspan="1">0.905</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.853</td><td align="center" valign="middle" rowspan="1" colspan="1">0.816</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.923</td><td align="center" valign="middle" rowspan="1" colspan="1">0.910</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RT-DETR&#x000a0;[<xref rid="B96-sensors-25-01016" ref-type="bibr">96</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.913</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.884</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.843</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.825</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.922</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.903</td></tr></tbody></table></table-wrap></sec><sec id="sec5dot1dot2-sensors-25-01016"><title>5.1.2. Limitations</title><p>To identify the limitations and challenges within the target domain, a thorough qualitative examination is conducted, revealing the following key findings: (a) detection accuracy may be compromised in instances of severe occlusion or partial concealment of gas cylinders behind objects, (b) the models might encounter difficulties in handling unusual settings, including highly reflective surfaces or non-standard gas cylinder shapes, and (c) attention is required for instances of false positive detections, where objects are incorrectly identified as gas cylinders mostly due to objects with similar shapes or appearances, such as bottles, pipes, or cylindrical containers. All these cases can have significant implications in critical applications, where misidentification might lead to potential risks or errors.</p><p><xref rid="sensors-25-01016-f006" ref-type="fig">Figure 6</xref> provides a visual representation of instances where these limitations become apparent. The top row illustrates representative&#x02014;to each limitation&#x02014;samples highlighted by the bounding box annotations, as included in ground truth annotation files of the CylinDeRS dataset, while, in the second row, image samples of the detections extracted using the YOLOv11 model are depicted. In the last row, the reason for the erroneous prediction is depicted. The first sample reveals instances where occlusion led to failure in identifying certain gas cylinders (scenario (a)). In the second sample, a false positive detection is performed due to a gas cylinder instance reflection on a highly reflective surface (scenario (b)). The third and fourth samples depict false positive detections of objects with similar shapes as gas cylinders (scenario (c)).</p></sec></sec><sec id="sec5dot2-sensors-25-01016"><title>5.2. Gas Cylinder Attribute Classification&#x000a0;Results</title><p>The evaluation process is applied on object attribute classification, employing the CylinDeRS dataset to rigorously assess the accuracy of prominent deep learning models on the material, size, and orientation attributes. In this domain, we focus on two distinguished members of the ResNet family&#x02014;ResNet-50 and ResNet-101&#x02014;both known for their commendable performance in feature extraction and representation learning.</p><sec id="sec5dot2dot1-sensors-25-01016"><title>5.2.1. Performance&#x000a0;Evaluation</title><p>The results presented in <xref rid="sensors-25-01016-t004" ref-type="table">Table 4</xref> include the comparative performance of ResNet-50 and ResNet-101 for the performance evaluation of the attribute classification task on both the validation and test sets. Notably, the individual accuracy scores for material, size, and orientation provide insights into the models&#x02019; effectiveness in discerning each specific attribute. ResNet-50 exhibits commendable accuracy scores of <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.946</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Val) and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.956</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Test) in material and <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.932</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Val) and <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.954</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Test) in orientation classification but demonstrates relatively lower Val and Test accuracy of <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.735</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.753</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in the size attribute predictions. On the other hand, ResNet-101 showcases improved performance across all the attribute categories, achieving higher accuracy values in the material classification, with scores of <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.959</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Val) and <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.962</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Test), and shows similar improvements in the size attribute, scoring <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.751</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Val) and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.772</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Test). Additionally, its orientation accuracy is slightly higher than ResNet-50&#x02019;s, reaching <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.944</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Val) and <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.958</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Test).</p><p>Since the individual attribute scores reflect the model&#x02019;s predictive accuracy for each attribute separately, the overall accuracy metric is incorporated for a more accurate evaluation, providing the model&#x02019;s performance across all three attributes. In this case, a true positive result indicates that the material, size, and orientation attributes were correctly predicted for a gas cylinder. The overall accuracy metric reflects each model&#x02019;s holistic effectiveness in accurately identifying all three attributes concurrently. In this regard, ResNet-101 leads, with an overall accuracy of <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.704</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> on the validation set and <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.716</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> on the test set, representing a performance boost compared to ResNet-50, which scores <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.670</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Val) and <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.675</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (Test).</p><p>To further assess the performance of the attribute classification models fine-tuned on the proposed dataset, qualitative results are showcased in <xref rid="sensors-25-01016-f007" ref-type="fig">Figure 7</xref>. The models are provided with input in the form of images derived by cropping the gas cylinder instances from the original images based on the ground truth bounding boxes. The visualization includes sample gas cylinder instances and their corresponding material, size, and orientation classifications predicted by the models. The top row presents the input images alongside their ground truth attribute labels. The predictions for ResNet-50 and ResNet-101 are displayed in the middle and bottom rows.</p><table-wrap position="anchor" id="sensors-25-01016-t004"><object-id pub-id-type="pii">sensors-25-01016-t004_Table 4</object-id><label>Table 4</label><caption><p>SotA performance comparison across the different attribute categories of the CylinDeRS dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Model</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Material</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Size</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Orientation</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Overall</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Val
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Test
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Val
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Test
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Val
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Test
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Val
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Test
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">ResNet-50&#x000a0;[<xref rid="B97-sensors-25-01016" ref-type="bibr">97</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.946</td><td align="center" valign="middle" rowspan="1" colspan="1">0.956</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.735</td><td align="center" valign="middle" rowspan="1" colspan="1">0.753</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.932</td><td align="center" valign="middle" rowspan="1" colspan="1">0.954</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.670</td><td align="center" valign="middle" rowspan="1" colspan="1">0.675</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet-101&#x000a0;[<xref rid="B97-sensors-25-01016" ref-type="bibr">97</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.959</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.962</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.751</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.772</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.944</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.958</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.704</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.716</td></tr></tbody></table></table-wrap></sec><sec id="sec5dot2dot2-sensors-25-01016"><title>5.2.2. Limitations</title><p>The exploration of limitations is crucial for a comprehensive understanding of the attribute classification outcomes within the CylinDeRS dataset. After a thorough qualitative evaluation, two main limitations have been observed: (a) one notable limitation stems from the inherent complexity of gas cylinder attributes and the data imbalance due to unusual scenarios (e.g., burnt or heavily damaged gas cylinder), as described in <xref rid="sec3dot2-sensors-25-01016" ref-type="sec">Section 3.2</xref>, and (b) the occurrence of ambiguous instances, such as gas cylinders with uncertain material compositions or obscured orientations, posing challenges for accurate classification.</p><p><xref rid="sensors-25-01016-f008" ref-type="fig">Figure 8</xref> illustrates misclassified visual samples. The first sample is a case of size attribute misclassification, where both models inaccurately predict the size attribute as &#x0201c;short&#x0201d; instead of &#x0201c;size unk&#x0201d;, possibly attributed to misleading visual cues resulting from the heavily cropped gas cylinder instance. Similarly, in the second example, ResNet-50 categorizes the gas cylinder object as &#x0201c;short&#x0201d; instead of &#x0201c;size unk&#x0201d;. This could be due to the resemblance of this instance to a short round-shaped cylinder. In the last image, the ResNet-50-based method struggles to classify size and orientation attributes, while ResNet-101 predicts all the attributes correctly. With fewer layers, ResNet-50 might primarily focus on learning lower-level features like edges and corners, which might be insufficient for tasks requiring understanding the object&#x02019;s size and spatial position within the scene.</p><p>For further interpretation of the results and analysis of the limitations for the attribute classification task, the confusion matrices for the material, size, and orientation attributes are presented in <xref rid="sensors-25-01016-f009" ref-type="fig">Figure 9</xref>. These matrices illustrate a detailed and comprehensive examination of the top-performing model&#x02019;s results for each class compared to the actual labels.</p><p>Regarding the <bold>material attribute</bold>, for instances genuinely belonging to the &#x0201c;metal&#x0201d; class, the model predicts three-thousand-eight-hundred-one (96.3%) instances correctly; however, there are one-hundred-thirty-nine (3.5%) instances misclassified as &#x0201c;fiber&#x0201d; and eight (0.2%) misclassified as &#x0201c;material unk&#x0201d;. For the &#x0201c;fiber&#x0201d; class, the model predicts seven-hundred-eighty-five (91%) correctly, but there are seventy-three (7.5%) instances misclassified as &#x0201c;metal&#x0201d; and four (0.5%) as &#x0201c;material unk&#x0201d;. For instances of the &#x0201c;material unk&#x0201d; class, the model correctly predicts only four (7.7%) cases labeled as &#x0201c;material unk&#x0201d;. There are thirty-three (63.5%) instances misclassified as &#x0201c;metal&#x0201d; and fifteen (28.8%) misclassified as &#x0201c;fiber&#x0201d;. The high misclassification rates for the &#x0201c;material unk&#x0201d; class reflect that the model&#x02019;s ability to discern this category is limited, which suggests a potential need for additional data to increase its accuracy in identifying such instances.</p><p>For the <bold>size attribute</bold>, the model correctly identifies 1846 (72.3%) instances labeled as &#x0201c;short&#x0201d;. There are 468 (18.3%) instances misclassified as &#x0201c;long&#x0201d; and 239 (9.4%) misclassified as &#x0201c;size unk&#x0201d;. Similarly, the model correctly predicts 998 (77.8%) instances labeled as &#x0201c;long&#x0201d;. There are 191 (14.9%) instances misclassified as &#x0201c;short&#x0201d; and 94 (7.3%) as &#x0201c;size unk&#x0201d;. The model correctly identifies 681 (66.4%) instances labeled as &#x0201c;unknown size.&#x0201d; There are 146 (14.2%) instances misclassified as &#x0201c;short&#x0201d; and 199 (19.4%) misclassified as &#x0201c;long&#x0201d;, revealing challenges in correctly assigning instances to the &#x0201c;size unk&#x0201d; class. The results indicate that, while the model performs relatively well in distinguishing between &#x0201c;short&#x0201d; and &#x0201c;long&#x0201d; instances, it struggles with the &#x0201c;size unk&#x0201d; category, potentially due to insufficient or ambiguous features distinguishing it from the other two classes.</p><p>In regard to the <bold>orientation attribute</bold>, the model performs well in correctly identifying instances labeled as &#x0201c;standing&#x0201d;, with a high count of 4275 (95.1%) true positives. However, there are 176 (3.9%) instances misclassified as &#x0201c;fallen&#x0201d; and 46 (1%) misclassified as &#x0201c;orientation unk&#x0201d;. For the &#x0201c;fallen&#x0201d; class, the model correctly identifies one-hundred-fifty-eight (63.2%) instances. Still, there are eighty-nine (35.6%) instances misclassified as &#x0201c;standing&#x0201d; and three (1.2%) misclassified as &#x0201c;orientation unk&#x0201d;. The accuracy for the &#x0201c;fallen&#x0201d; class is reasonable, but there is room for improvement, especially in reducing misclassifications as &#x0201c;standing&#x0201d;. For the &#x0201c;orientation unk&#x0201d; class, the model correctly predicts thirty-seven (32.2%) instances, but there are seventy-one (61.7%) instances misclassified as &#x0201c;standing&#x0201d; and seven (6.1%) misclassified as &#x0201c;fallen&#x0201d;. The accuracy for the &#x0201c;orientation unk&#x0201d; class is lower compared to the other classes, suggesting a need for further improvement in classification for instances with unknown orientation.</p><p>In summary, the model performs well in several areas, but notable challenges remain. While the model excels at classifying the &#x0201c;metal&#x0201d; and &#x0201c;fiber&#x0201d; material attributes, it struggles with the &#x0201c;material unk&#x0201d; class, indicating the need for more data or improved feature extraction. For size classification, the model performs well with &#x0201c;short&#x0201d; and &#x0201c;long&#x0201d; gas cylinders but faces challenges with the &#x0201c;size unk&#x0201d; category, likely due to ambiguous features. Orientation classification is strong for &#x0201c;standing&#x0201d; gas cylinders but requires improvement mainly for the &#x0201c;orientation unk&#x0201d; class. These limitations highlight areas for future work, particularly in increasing data diversity and exploring new models&#x02019; capabilities in handling ambiguous and rare cases.</p></sec></sec><sec id="sec5dot3-sensors-25-01016"><title>5.3. Future&#x000a0;Research</title><p>For future research involving CylinDeRS, there are several key directions that researchers could investigate based on the limitations of the current version of the dataset. First, future efforts could focus on augmenting rare scenarios, including creating synthetic data, to mitigate data imbalance across attribute categories while also ensuring that models can handle edge cases, such as cylinders in hazardous conditions or extreme environments. Addressing the limitations in terms of model performance for highly occluded or reflective instances is also a critical avenue for improvement. Techniques like multi-view imaging, 3D reconstruction, or leveraging transformer-based architectures that can capture spatial and contextual relationships could be explored. Furthermore, pre-processing steps such as glare removal, image normalization, and advanced feature extraction could also help to reduce noise and improve performance. Additionally, models that incorporate contextual information&#x02014;such as surrounding objects or background&#x02014;are likely to yield improved performance, particularly in scenarios involving partially occluded cylinders or those in unusual orientations. Lastly, expanding the dataset with additional attributes (e.g., cylinder usage, pressure ratings, or markings and labels) could enable more specialized applications, such as safety monitoring or regulatory compliance, further enhancing the dataset&#x02019;s utility.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-01016"><title>6. Conclusions</title><p>This work introduced CylinDeRS, a domain-specific computer vision dataset specifically curated for two tasks: (a) object detection for detecting gas cylinder instances and (b) gas cylinder attribute classification for material, size, and orientation attributes. The proposed dataset consists of 7060 images, captured under varying settings and closely representing real-life situations, featuring a total of 25,269 annotated gas cylinder object instances. Furthermore, a systematic methodology for the creation of domain-specific datasets is proposed, covering the entire process from data collection and annotation to structure definition. A series of experiments were conducted using deep-learning-based frameworks from various categories to verify the practical application of CylinDeRS and provide insights into the strengths and limitations in this domain. While the baseline results are encouraging, achieving a maximum mAP of 91% for gas cylinder object detection and a maximum accuracy of <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>71.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>% for attribute classification, the complex characteristics of gas cylinder instances pose notable challenges that need to be addressed. Future work includes expanding the dataset with challenging classes by adding new attribute categories and exploring performance improvements for deep learning models.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, K.S. and K.G.; methodology, K.S. and K.G.; software, K.S.; validation, K.S. and K.G.; formal analysis, K.S. and K.G.; investigation, K.G.; resources, K.I. and T.T.; data curation, K.S., K.G., K.I. and T.T.; writing&#x02014;original draft preparation, K.S.; writing&#x02014;review and editing, K.S., K.G., K.I. and T.T.; visualization, K.S.; supervision, K.G., K.I. and T.T.; project administration, S.V. and I.K.; funding acquisition, S.V. and I.K. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The CylinDeRS dataset is publicly available from the Roboflow repository (<uri xlink:href="https://universe.roboflow.com/klearchos-stavrothanasopoulos-konstantinos-gkountakos-6jwgj/cylinders-iaq6n">https://universe.roboflow.com/klearchos-stavrothanasopoulos-konstantinos-gkountakos-6jwgj/cylinders-iaq6n</uri> (accessed on 17 January 2025)). The trained models will be released upon publication of this work (<uri xlink:href="https://m4d.iti.gr/results/">https://m4d.iti.gr/results/</uri> (accessed on 17 January 2025)).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">DCNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Deep Convolutional Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FPN</td><td align="left" valign="middle" rowspan="1" colspan="1">Feature Pyramid Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HFC</td><td align="left" valign="middle" rowspan="1" colspan="1">Hydrofluorocarbon</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Ozone-Depleting Substance</td><td align="left" valign="middle" rowspan="1" colspan="1">ODS</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SotA</td><td align="left" valign="middle" rowspan="1" colspan="1">State of the Art</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ILSVRC</td><td align="left" valign="middle" rowspan="1" colspan="1">ImageNet Large-Scale Visual Recognition Challenge</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PACO</td><td align="left" valign="middle" rowspan="1" colspan="1">Parts and Attributes of Common Objects</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SPPF</td><td align="left" valign="middle" rowspan="1" colspan="1">Spatial Pyramid Pooling Fusion</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VAW</td><td align="left" valign="middle" rowspan="1" colspan="1">Visual Attributes in the Wild</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">WIDER</td><td align="left" valign="middle" rowspan="1" colspan="1">Web Image Dataset for Event Recognition</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-01016"><label>1.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Everingham</surname><given-names>M.</given-names></name>
<name><surname>Van Gool</surname><given-names>L.</given-names></name>
<name><surname>Williams</surname><given-names>C.K.I.</given-names></name>
<name><surname>Winn</surname><given-names>J.</given-names></name>
<name><surname>Zisserman</surname><given-names>A.</given-names></name>
</person-group><article-title>The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</article-title><comment>Available online: <ext-link xlink:href="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" ext-link-type="uri">http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-01-17">(accessed on 17 January 2025)</date-in-citation></element-citation></ref><ref id="B2-sensors-25-01016"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>T.Y.</given-names></name>
<name><surname>Maire</surname><given-names>M.</given-names></name>
<name><surname>Belongie</surname><given-names>S.</given-names></name>
<name><surname>Bourdev</surname><given-names>L.</given-names></name>
<name><surname>Girshick</surname><given-names>R.</given-names></name>
<name><surname>Hays</surname><given-names>J.</given-names></name>
<name><surname>Perona</surname><given-names>P.</given-names></name>
<name><surname>Ramanan</surname><given-names>D.</given-names></name>
<name><surname>Zitnick</surname><given-names>C.L.</given-names></name>
<name><surname>Doll&#x000e1;r</surname><given-names>P.</given-names></name>
</person-group><article-title>Microsoft COCO: Common Objects in Context</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1405.0312</pub-id></element-citation></ref><ref id="B3-sensors-25-01016"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Patterson</surname><given-names>G.</given-names></name>
<name><surname>Hays</surname><given-names>J.</given-names></name>
</person-group><article-title>Coco attributes: Attributes for people, animals, and objects</article-title><source>Proceedings of the Computer Vision&#x02014;ECCV 2016: 14th European Conference</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>11&#x02013;14 October 2016</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2016</year><comment>Proceedings, Part VI 14</comment><fpage>85</fpage><lpage>100</lpage></element-citation></ref><ref id="B4-sensors-25-01016"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>S.</given-names></name>
<name><surname>Huang</surname><given-names>W.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Srikhanta</surname><given-names>P.</given-names></name>
<name><surname>Cui</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Adam</surname><given-names>H.</given-names></name>
<name><surname>Scott</surname><given-names>M.R.</given-names></name>
<name><surname>Belongie</surname><given-names>S.</given-names></name>
</person-group><article-title>The imaterialist fashion attribute dataset</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27&#x02013;28 October 2019</conf-date></element-citation></ref><ref id="B5-sensors-25-01016"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Carion</surname><given-names>N.</given-names></name>
<name><surname>Massa</surname><given-names>F.</given-names></name>
<name><surname>Synnaeve</surname><given-names>G.</given-names></name>
<name><surname>Usunier</surname><given-names>N.</given-names></name>
<name><surname>Kirillov</surname><given-names>A.</given-names></name>
<name><surname>Zagoruyko</surname><given-names>S.</given-names></name>
</person-group><article-title>End-to-end object detection with transformers</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#x02013;28 August 2020</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>213</fpage><lpage>229</lpage></element-citation></ref><ref id="B6-sensors-25-01016"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xie</surname><given-names>E.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Yu</surname><given-names>Z.</given-names></name>
<name><surname>Anandkumar</surname><given-names>A.</given-names></name>
<name><surname>Alvarez</surname><given-names>J.M.</given-names></name>
<name><surname>Luo</surname><given-names>P.</given-names></name>
</person-group><article-title>SegFormer: Simple and efficient design for semantic segmentation with transformers</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>12077</fpage><lpage>12090</lpage></element-citation></ref><ref id="B7-sensors-25-01016"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>C.Y.</given-names></name>
<name><surname>Bochkovskiy</surname><given-names>A.</given-names></name>
<name><surname>Liao</surname><given-names>H.Y.M.</given-names></name>
</person-group><article-title>YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#x02013;24 June 2023</conf-date><fpage>7464</fpage><lpage>7475</lpage></element-citation></ref><ref id="B8-sensors-25-01016"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gkountakos</surname><given-names>K.</given-names></name>
<name><surname>Ioannidis</surname><given-names>K.</given-names></name>
<name><surname>Tsikrika</surname><given-names>T.</given-names></name>
<name><surname>Vrochidis</surname><given-names>S.</given-names></name>
<name><surname>Kompatsiaris</surname><given-names>I.</given-names></name>
</person-group><article-title>A crowd analysis framework for detecting violence scenes</article-title><source>Proceedings of the 2020 International Conference on Multimedia Retrieval</source><conf-loc>Dublin, Ireland</conf-loc><conf-date>8&#x02013;11 June 2020</conf-date><fpage>276</fpage><lpage>280</lpage></element-citation></ref><ref id="B9-sensors-25-01016"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Maungmai</surname><given-names>W.</given-names></name>
<name><surname>Nuthong</surname><given-names>C.</given-names></name>
</person-group><article-title>Vehicle Classification with Deep Learning</article-title><source>Proceedings of the 2019 IEEE 4th International Conference on Computer and Communication Systems (ICCCS)</source><conf-loc>Singapore</conf-loc><conf-date>23&#x02013;25 February 2019</conf-date><fpage>294</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1109/CCOMS.2019.8821689</pub-id></element-citation></ref><ref id="B10-sensors-25-01016"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gkountakos</surname><given-names>K.</given-names></name>
<name><surname>Touska</surname><given-names>D.</given-names></name>
<name><surname>Ioannidis</surname><given-names>K.</given-names></name>
<name><surname>Tsikrika</surname><given-names>T.</given-names></name>
<name><surname>Vrochidis</surname><given-names>S.</given-names></name>
<name><surname>Kompatsiaris</surname><given-names>I.</given-names></name>
</person-group><article-title>Spatio-temporal activity detection and recognition in untrimmed surveillance videos</article-title><source>Proceedings of the 2021 International Conference on Multimedia Retrieval</source><conf-loc>Taipei, Taiwan</conf-loc><conf-date>16&#x02013;19 November 2021</conf-date><fpage>451</fpage><lpage>455</lpage></element-citation></ref><ref id="B11-sensors-25-01016"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Qi</surname><given-names>D.</given-names></name>
<name><surname>Tan</surname><given-names>W.</given-names></name>
<name><surname>Yao</surname><given-names>Q.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
</person-group><article-title>YOLO5Face: Why reinventing a face detector</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#x02013;27 October 2022</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzewrland</publisher-loc><year>2022</year><fpage>228</fpage><lpage>244</lpage></element-citation></ref><ref id="B12-sensors-25-01016"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Stavrothanasopoulos</surname><given-names>K.</given-names></name>
<name><surname>Gkountakos</surname><given-names>K.</given-names></name>
<name><surname>Ioannidis</surname><given-names>K.</given-names></name>
<name><surname>Tsikrika</surname><given-names>T.</given-names></name>
<name><surname>Vrochidis</surname><given-names>S.</given-names></name>
<name><surname>Kompatsiaris</surname><given-names>I.</given-names></name>
</person-group><article-title>Vehicle Color Identification Framework using Pixel-level Color Estimation from Segmentation Masks of Car Parts</article-title><source>Proceedings of the 2022 IEEE 5th International Conference on Image Processing Applications and Systems (IPAS)</source><conf-loc>Genova, Italy</conf-loc><conf-date>5&#x02013;7 December 2022</conf-date><volume>Volume 5</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1109/IPAS55744.2022.10052969</pub-id></element-citation></ref><ref id="B13-sensors-25-01016"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Usha Rani</surname><given-names>J.</given-names></name>
<name><surname>Raviraj</surname><given-names>P.</given-names></name>
</person-group><article-title>Real-Time Human Detection for Intelligent Video Surveillance: An Empirical Research and In-depth Review of its Applications</article-title><source>SN Comput. Sci.</source><year>2023</year><volume>4</volume><fpage>258</fpage><pub-id pub-id-type="doi">10.1007/s42979-022-01654-4</pub-id></element-citation></ref><ref id="B14-sensors-25-01016"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hou</surname><given-names>S.</given-names></name>
<name><surname>Feng</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
</person-group><article-title>Vegfru: A domain-specific dataset for fine-grained visual categorization</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#x02013;29 October 2017</conf-date><fpage>541</fpage><lpage>549</lpage></element-citation></ref><ref id="B15-sensors-25-01016"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>R.</given-names></name>
<name><surname>Liu</surname><given-names>L.</given-names></name>
<name><surname>Xie</surname><given-names>C.</given-names></name>
<name><surname>Yang</surname><given-names>P.</given-names></name>
<name><surname>Li</surname><given-names>R.</given-names></name>
<name><surname>Zhou</surname><given-names>M.</given-names></name>
</person-group><article-title>Agripest: A large-scale domain-specific benchmark dataset for practical agricultural pest detection in the wild</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>1601</elocation-id><pub-id pub-id-type="doi">10.3390/s21051601</pub-id><pub-id pub-id-type="pmid">33668820</pub-id>
</element-citation></ref><ref id="B16-sensors-25-01016"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bustos</surname><given-names>A.</given-names></name>
<name><surname>Pertusa</surname><given-names>A.</given-names></name>
<name><surname>Salinas</surname><given-names>J.M.</given-names></name>
<name><surname>De La Iglesia-Vaya</surname><given-names>M.</given-names></name>
</person-group><article-title>Padchest: A large chest x-ray image dataset with multi-label annotated reports</article-title><source>Med. Image Anal.</source><year>2020</year><volume>66</volume><fpage>101797</fpage><pub-id pub-id-type="doi">10.1016/j.media.2020.101797</pub-id><pub-id pub-id-type="pmid">32877839</pub-id>
</element-citation></ref><ref id="B17-sensors-25-01016"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Xu</surname><given-names>Y.</given-names></name>
<name><surname>Usuyama</surname><given-names>N.</given-names></name>
<name><surname>Bagga</surname><given-names>J.</given-names></name>
<name><surname>Tinn</surname><given-names>R.</given-names></name>
<name><surname>Preston</surname><given-names>S.</given-names></name>
<name><surname>Rao</surname><given-names>R.</given-names></name>
<name><surname>Wei</surname><given-names>M.</given-names></name>
<name><surname>Valluri</surname><given-names>N.</given-names></name>
<name><surname>Wong</surname><given-names>C.</given-names></name>
<etal/>
</person-group><article-title>Large-scale domain-specific pretraining for biomedical vision-language processing</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2303.00915</pub-id></element-citation></ref><ref id="B18-sensors-25-01016"><label>18.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Schoen</surname><given-names>H.</given-names></name>
<name><surname>Sch&#x000f6;n</surname><given-names>H.</given-names></name>
</person-group><article-title>Compressed gas cylinders</article-title><source>Handbook of Purified Gases</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2015</year><fpage>143</fpage><lpage>153</lpage></element-citation></ref><ref id="B19-sensors-25-01016"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tschirschwitz</surname><given-names>R.</given-names></name>
<name><surname>Krentel</surname><given-names>D.</given-names></name>
<name><surname>Kluge</surname><given-names>M.</given-names></name>
<name><surname>Askar</surname><given-names>E.</given-names></name>
<name><surname>Habib</surname><given-names>K.</given-names></name>
<name><surname>Kohlhoff</surname><given-names>H.</given-names></name>
<name><surname>Neumann</surname><given-names>P.P.</given-names></name>
<name><surname>Storm</surname><given-names>S.U.</given-names></name>
<name><surname>Rudolph</surname><given-names>M.</given-names></name>
<name><surname>Schoppa</surname><given-names>A.</given-names></name>
<etal/>
</person-group><article-title>Mobile gas cylinders in fire: Consequences in case of failure</article-title><source>Fire Saf. J.</source><year>2017</year><volume>91</volume><fpage>989</fpage><lpage>996</lpage><pub-id pub-id-type="doi">10.1016/j.firesaf.2017.05.006</pub-id></element-citation></ref><ref id="B20-sensors-25-01016"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wood</surname><given-names>M.H.</given-names></name>
<name><surname>Hailwood</surname><given-names>M.</given-names></name>
<name><surname>Koutelos</surname><given-names>K.</given-names></name>
</person-group><article-title>Reducing the risk of oxygen-related fires and explosions in hospitals treating COVID-19 patients</article-title><source>Process Saf. Environ. Prot.</source><year>2021</year><volume>153</volume><fpage>278</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1016/j.psep.2021.06.023</pub-id><pub-id pub-id-type="pmid">34188364</pub-id>
</element-citation></ref><ref id="B21-sensors-25-01016"><label>21.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Elliott</surname><given-names>L.</given-names></name>
</person-group><article-title>Smuggling networks and the black market in ozone depleting substances</article-title><source>Hazardous Waste and Pollution: Detecting and Preventing Green Crimes</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2016</year><fpage>45</fpage><lpage>60</lpage></element-citation></ref><ref id="B22-sensors-25-01016"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Welch</surname><given-names>G.D.</given-names></name>
</person-group><article-title>HFC Smuggling: Preventing the Illicit (and Lucrative) Sale of Greenhouse Gases</article-title><source>BC Envtl. Aff. L. Rev.</source><year>2017</year><volume>44</volume><fpage>525</fpage></element-citation></ref><ref id="B23-sensors-25-01016"><label>23.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Ujfalusi</surname><given-names>M.</given-names></name>
<name><surname>Gaustad</surname><given-names>A.</given-names></name>
<name><surname>Secher Paludan</surname><given-names>D.</given-names></name>
<name><surname>Bj&#x000f6;rk B&#x000e6;ringsd&#x000f3;ttir</surname><given-names>B.</given-names></name>
<name><surname>Gunnleivsd&#x000f3;ttir Hansen</surname><given-names>M.</given-names></name>
<name><surname>Finel</surname><given-names>N.</given-names></name>
<name><surname>Johansson</surname><given-names>A.</given-names></name>
</person-group><source>Illegal trade of HFCs: Report from the Workshop: Fight Against Illegal Trade of Hydrofluorocarbons in the Nordic and Baltic Countries. 20th and 27th of January 2021</source><publisher-name>Nordic Co-Operation</publisher-name><publisher-loc>Copenhagen, Denmark</publisher-loc><year>2021</year></element-citation></ref><ref id="B24-sensors-25-01016"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Arkin</surname><given-names>E.</given-names></name>
<name><surname>Yadikar</surname><given-names>N.</given-names></name>
<name><surname>Xu</surname><given-names>X.</given-names></name>
<name><surname>Aysa</surname><given-names>A.</given-names></name>
<name><surname>Ubul</surname><given-names>K.</given-names></name>
</person-group><article-title>A survey: Object detection methods from CNN to transformer</article-title><source>Multimed. Tools Appl.</source><year>2023</year><volume>82</volume><fpage>21353</fpage><lpage>21383</lpage><pub-id pub-id-type="doi">10.1007/s11042-022-13801-3</pub-id></element-citation></ref><ref id="B25-sensors-25-01016"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yaghoubi</surname><given-names>E.</given-names></name>
<name><surname>Khezeli</surname><given-names>F.</given-names></name>
<name><surname>Borza</surname><given-names>D.</given-names></name>
<name><surname>Kumar</surname><given-names>S.A.</given-names></name>
<name><surname>Neves</surname><given-names>J.</given-names></name>
<name><surname>Proen&#x000e7;a</surname><given-names>H.</given-names></name>
</person-group><article-title>Human attribute recognition&#x02014;A comprehensive survey</article-title><source>Appl. Sci.</source><year>2020</year><volume>10</volume><elocation-id>5608</elocation-id><pub-id pub-id-type="doi">10.3390/app10165608</pub-id></element-citation></ref><ref id="B26-sensors-25-01016"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gupta</surname><given-names>A.</given-names></name>
<name><surname>Dollar</surname><given-names>P.</given-names></name>
<name><surname>Girshick</surname><given-names>R.</given-names></name>
</person-group><article-title>Lvis: A dataset for large vocabulary instance segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#x02013;20 June 2019</conf-date><fpage>5356</fpage><lpage>5364</lpage></element-citation></ref><ref id="B27-sensors-25-01016"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Shao</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>T.</given-names></name>
<name><surname>Peng</surname><given-names>C.</given-names></name>
<name><surname>Yu</surname><given-names>G.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Objects365: A large-scale, high-quality dataset for object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#x02013;2 November 2019</conf-date><fpage>8430</fpage><lpage>8439</lpage></element-citation></ref><ref id="B28-sensors-25-01016"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Krizhevsky</surname><given-names>A.</given-names></name>
<name><surname>Sutskever</surname><given-names>I.</given-names></name>
<name><surname>Hinton</surname><given-names>G.E.</given-names></name>
</person-group><article-title>ImageNet Classification with Deep Convolutional Neural Networks</article-title><source>Proceedings of the 25th International Conference on Neural Information Processing Systems</source><conf-loc>Red Hook, NY, USA</conf-loc><conf-date>3&#x02013;6 December 2012</conf-date><volume>Volume 1</volume><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="B29-sensors-25-01016"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Pham</surname><given-names>K.</given-names></name>
<name><surname>Kafle</surname><given-names>K.</given-names></name>
<name><surname>Lin</surname><given-names>Z.</given-names></name>
<name><surname>Ding</surname><given-names>Z.</given-names></name>
<name><surname>Cohen</surname><given-names>S.</given-names></name>
<name><surname>Tran</surname><given-names>Q.</given-names></name>
<name><surname>Shrivastava</surname><given-names>A.</given-names></name>
</person-group><article-title>Learning To Predict Visual Attributes in the Wild</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>19&#x02013;25 June 2021</conf-date><fpage>13018</fpage><lpage>13028</lpage></element-citation></ref><ref id="B30-sensors-25-01016"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ramanathan</surname><given-names>V.</given-names></name>
<name><surname>Kalia</surname><given-names>A.</given-names></name>
<name><surname>Petrovic</surname><given-names>V.</given-names></name>
<name><surname>Wen</surname><given-names>Y.</given-names></name>
<name><surname>Zheng</surname><given-names>B.</given-names></name>
<name><surname>Guo</surname><given-names>B.</given-names></name>
<name><surname>Wang</surname><given-names>R.</given-names></name>
<name><surname>Marquez</surname><given-names>A.</given-names></name>
<name><surname>Kovvuri</surname><given-names>R.</given-names></name>
<name><surname>Kadian</surname><given-names>A.</given-names></name>
<etal/>
</person-group><article-title>Paco: Parts and attributes of common objects</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#x02013;24 June 2023</conf-date><fpage>7141</fpage><lpage>7151</lpage></element-citation></ref><ref id="B31-sensors-25-01016"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Niemeijer</surname><given-names>J.</given-names></name>
<name><surname>Mittal</surname><given-names>S.</given-names></name>
<name><surname>Brox</surname><given-names>T.</given-names></name>
</person-group><article-title>Synthetic Dataset Acquisition for a Specific Target Domain</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</source><conf-loc>Paris, France</conf-loc><conf-date>1&#x02013;6 October 2023</conf-date><fpage>4055</fpage><lpage>4064</lpage></element-citation></ref><ref id="B32-sensors-25-01016"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hao</surname><given-names>S.</given-names></name>
<name><surname>Han</surname><given-names>W.</given-names></name>
<name><surname>Jiang</surname><given-names>T.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>H.</given-names></name>
<name><surname>Zhong</surname><given-names>C.</given-names></name>
<name><surname>Zhou</surname><given-names>Z.</given-names></name>
<name><surname>Tang</surname><given-names>H.</given-names></name>
</person-group><article-title>Synthetic Data in AI: Challenges, Applications, and Ethical Implications</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2401.01629</pub-id></element-citation></ref><ref id="B33-sensors-25-01016"><label>33.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Gkountakos</surname><given-names>K.S.K.</given-names></name>
</person-group><article-title>CylinDeRS Dataset</article-title><year>2024</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/klearchos-stavrothanasopoulos-konstantinos-gkountakos-6jwgj/cylinders-iaq6n" ext-link-type="uri">https://universe.roboflow.com/klearchos-stavrothanasopoulos-konstantinos-gkountakos-6jwgj/cylinders-iaq6n</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-01-14">(accessed on 14 January 2025)</date-in-citation></element-citation></ref><ref id="B34-sensors-25-01016"><label>34.</label><element-citation publication-type="webpage"><article-title>zxc. Gas Tank Yolo Dataset Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/zxc-lgbhl/gas-tank-yolo-dataset" ext-link-type="uri">https://universe.roboflow.com/zxc-lgbhl/gas-tank-yolo-dataset</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B35-sensors-25-01016"><label>35.</label><element-citation publication-type="webpage"><article-title>twoseptember. Oxygen Cylinder Dataset</article-title><year>2024</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/twoseptember/oxygen-cylinder" ext-link-type="uri">https://universe.roboflow.com/twoseptember/oxygen-cylinder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B36-sensors-25-01016"><label>36.</label><element-citation publication-type="webpage"><article-title>Tatware. Gas Tank Detection Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/tatware/gas-tank-detection" ext-link-type="uri">https://universe.roboflow.com/tatware/gas-tank-detection</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B37-sensors-25-01016"><label>37.</label><element-citation publication-type="webpage"><article-title>1118. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1118-pyyim/lpg-cylinder-ejjlz" ext-link-type="uri">https://universe.roboflow.com/1118-pyyim/lpg-cylinder-ejjlz</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B38-sensors-25-01016"><label>38.</label><element-citation publication-type="webpage"><article-title>Project. Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/project-1iwh1/cylinder-c7bry" ext-link-type="uri">https://universe.roboflow.com/project-1iwh1/cylinder-c7bry</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B39-sensors-25-01016"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kuznetsova</surname><given-names>A.</given-names></name>
<name><surname>Rom</surname><given-names>H.</given-names></name>
<name><surname>Alldrin</surname><given-names>N.</given-names></name>
<name><surname>Uijlings</surname><given-names>J.</given-names></name>
<name><surname>Krasin</surname><given-names>I.</given-names></name>
<name><surname>Pont-Tuset</surname><given-names>J.</given-names></name>
<name><surname>Kamali</surname><given-names>S.</given-names></name>
<name><surname>Popov</surname><given-names>S.</given-names></name>
<name><surname>Malloci</surname><given-names>M.</given-names></name>
<name><surname>Kolesnikov</surname><given-names>A.</given-names></name>
<etal/>
</person-group><article-title>The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</article-title><source>Int. J. Comput. Vis.</source><year>2020</year><volume>128</volume><fpage>1956</fpage><lpage>1981</lpage><pub-id pub-id-type="doi">10.1007/s11263-020-01316-z</pub-id></element-citation></ref><ref id="B40-sensors-25-01016"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Huang</surname><given-names>C.</given-names></name>
<name><surname>Loy</surname><given-names>C.C.</given-names></name>
<name><surname>Tang</surname><given-names>X.</given-names></name>
</person-group><article-title>Human attribute recognition by deep hierarchical contexts</article-title><source>Proceedings of the Computer Vision&#x02013;ECCV 2016: 14th European Conference</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>11&#x02013;14 October 2016</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2016</year><comment>Proceedings, Part VI 14</comment><fpage>684</fpage><lpage>700</lpage></element-citation></ref><ref id="B41-sensors-25-01016"><label>41.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Dwyer</surname><given-names>B.</given-names></name>
<name><surname>Nelson</surname><given-names>J.</given-names></name>
<name><surname>Solawetz</surname><given-names>J.</given-names></name>
</person-group><article-title>Roboflow (Version 1.0) [Software]. Computer Vision 2022</article-title><comment>Available online: <ext-link xlink:href="https://roboflow.com" ext-link-type="uri">https://roboflow.com</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-01-17">(accessed on 17 January 2025)</date-in-citation></element-citation></ref><ref id="B42-sensors-25-01016"><label>42.</label><element-citation publication-type="webpage"><article-title>Cylinder. Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/cylinder-prxc1/cylinder-wmpfc" ext-link-type="uri">https://universe.roboflow.com/cylinder-prxc1/cylinder-wmpfc</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B43-sensors-25-01016"><label>43.</label><element-citation publication-type="webpage"><article-title>Iti. Gas Tank Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/iti-wqxi8/gas-tank-pnfez" ext-link-type="uri">https://universe.roboflow.com/iti-wqxi8/gas-tank-pnfez</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B44-sensors-25-01016"><label>44.</label><element-citation publication-type="webpage"><article-title>SH. gas_tank Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/sh-l3jzm/gas_tank-p9yyz" ext-link-type="uri">https://universe.roboflow.com/sh-l3jzm/gas_tank-p9yyz</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B45-sensors-25-01016"><label>45.</label><element-citation publication-type="webpage"><article-title>Project. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/project-1iwh1/lpg-cylinder" ext-link-type="uri">https://universe.roboflow.com/project-1iwh1/lpg-cylinder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B46-sensors-25-01016"><label>46.</label><element-citation publication-type="webpage"><article-title>Nathawut. LPG Tank Dataset</article-title><year>2021</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/nathawut/lpg-tank" ext-link-type="uri">https://universe.roboflow.com/nathawut/lpg-tank</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B47-sensors-25-01016"><label>47.</label><element-citation publication-type="webpage"><article-title>Prohibiteditems. gas_tank Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/prohibiteditems/gas_tank-ygf9i" ext-link-type="uri">https://universe.roboflow.com/prohibiteditems/gas_tank-ygf9i</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B48-sensors-25-01016"><label>48.</label><element-citation publication-type="webpage"><article-title>Class. Gazdata Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/class-824gh/gazdata" ext-link-type="uri">https://universe.roboflow.com/class-824gh/gazdata</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B49-sensors-25-01016"><label>49.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Shetye</surname><given-names>P.</given-names></name>
</person-group><article-title>Gas-Cylinders Dataset</article-title><year>2023</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/prathamesh-shetye-l2bvr/gas-cylinders-gyes7" ext-link-type="uri">https://universe.roboflow.com/prathamesh-shetye-l2bvr/gas-cylinders-gyes7</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B50-sensors-25-01016"><label>50.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Naik</surname><given-names>S.</given-names></name>
</person-group><article-title>Cylinder Dataset</article-title><year>2021</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/sushrut-naik/cylinder-tgcnz" ext-link-type="uri">https://universe.roboflow.com/sushrut-naik/cylinder-tgcnz</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B51-sensors-25-01016"><label>51.</label><element-citation publication-type="webpage"><article-title>Coro Laut. CAPSTONE Dataset</article-title><year>2023</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/coro-laut-zunwm/capstone-1kavf" ext-link-type="uri">https://universe.roboflow.com/coro-laut-zunwm/capstone-1kavf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B52-sensors-25-01016"><label>52.</label><element-citation publication-type="webpage"><article-title>Test. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/test-ujfv7/lpg-cylinder-auuqb" ext-link-type="uri">https://universe.roboflow.com/test-ujfv7/lpg-cylinder-auuqb</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B53-sensors-25-01016"><label>53.</label><element-citation publication-type="webpage"><article-title>t. final_cylinders_dataa Dataset</article-title><year>2023</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/t-j7j1n/final_cylinders_dataa" ext-link-type="uri">https://universe.roboflow.com/t-j7j1n/final_cylinders_dataa</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B54-sensors-25-01016"><label>54.</label><element-citation publication-type="webpage"><article-title>1. 221028_lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1-vdmlq/221028_lpg-cylinder" ext-link-type="uri">https://universe.roboflow.com/1-vdmlq/221028_lpg-cylinder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B55-sensors-25-01016"><label>55.</label><element-citation publication-type="webpage"><article-title>Test. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/test-c7cor/lpg-cylinder-iiha7" ext-link-type="uri">https://universe.roboflow.com/test-c7cor/lpg-cylinder-iiha7</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B56-sensors-25-01016"><label>56.</label><element-citation publication-type="webpage"><article-title>Icity. Steel-Fiber Gas Tank Dataset</article-title><year>2023</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/icity-yonpa/steel-fiber-gas-tank" ext-link-type="uri">https://universe.roboflow.com/icity-yonpa/steel-fiber-gas-tank</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B57-sensors-25-01016"><label>57.</label><element-citation publication-type="webpage"><article-title>1108. Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1108/cylinder-ebwob" ext-link-type="uri">https://universe.roboflow.com/1108/cylinder-ebwob</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B58-sensors-25-01016"><label>58.</label><element-citation publication-type="webpage"><article-title>20221111. 1118 Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/20221111/1118-lpg-cylinder" ext-link-type="uri">https://universe.roboflow.com/20221111/1118-lpg-cylinder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B59-sensors-25-01016"><label>59.</label><element-citation publication-type="webpage"><article-title>Project be6od. 1101lpg Cyilnder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/project-be6od/1101lpg-cyilnder" ext-link-type="uri">https://universe.roboflow.com/project-be6od/1101lpg-cyilnder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B60-sensors-25-01016"><label>60.</label><element-citation publication-type="webpage"><article-title>Temp. Oxygen Cylinder Horizontal Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/temp-uvvms/oxygen-cylinder-horizontal" ext-link-type="uri">https://universe.roboflow.com/temp-uvvms/oxygen-cylinder-horizontal</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B61-sensors-25-01016"><label>61.</label><element-citation publication-type="webpage"><article-title>GAS. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/gas/lpg-cylinder-zbvbb" ext-link-type="uri">https://universe.roboflow.com/gas/lpg-cylinder-zbvbb</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B62-sensors-25-01016"><label>62.</label><element-citation publication-type="webpage"><article-title>1025. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1025/lpg-cylinder-eoanc" ext-link-type="uri">https://universe.roboflow.com/1025/lpg-cylinder-eoanc</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B63-sensors-25-01016"><label>63.</label><element-citation publication-type="webpage"><article-title>Project. Merge 1 Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/project-1iwh1/merge-1" ext-link-type="uri">https://universe.roboflow.com/project-1iwh1/merge-1</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B64-sensors-25-01016"><label>64.</label><element-citation publication-type="webpage"><article-title>20221111. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/20221111/lpg-cylinder-97ac0" ext-link-type="uri">https://universe.roboflow.com/20221111/lpg-cylinder-97ac0</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B65-sensors-25-01016"><label>65.</label><element-citation publication-type="webpage"><article-title>1118. Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1118-pyyim/cylinder-hc8zl" ext-link-type="uri">https://universe.roboflow.com/1118-pyyim/cylinder-hc8zl</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B66-sensors-25-01016"><label>66.</label><element-citation publication-type="webpage"><article-title>Kor Gas. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/kor-gas/lpg-cylinder-1yj1v" ext-link-type="uri">https://universe.roboflow.com/kor-gas/lpg-cylinder-1yj1v</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B67-sensors-25-01016"><label>67.</label><element-citation publication-type="webpage"><article-title>Burn. Lpg Cylinder1 Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/burn-oyj47/lpg-cylinder1" ext-link-type="uri">https://universe.roboflow.com/burn-oyj47/lpg-cylinder1</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B68-sensors-25-01016"><label>68.</label><element-citation publication-type="webpage"><article-title>GAS. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/gas/lpg-cylinder-xxygt" ext-link-type="uri">https://universe.roboflow.com/gas/lpg-cylinder-xxygt</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B69-sensors-25-01016"><label>69.</label><element-citation publication-type="webpage"><article-title>Test. Work 7 Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/test-ujfv7/work7" ext-link-type="uri">https://universe.roboflow.com/test-ujfv7/work7</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B70-sensors-25-01016"><label>70.</label><element-citation publication-type="webpage"><article-title>1111. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1111-iq4b9/lpg-cylinder-sabgl" ext-link-type="uri">https://universe.roboflow.com/1111-iq4b9/lpg-cylinder-sabgl</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B71-sensors-25-01016"><label>71.</label><element-citation publication-type="webpage"><article-title>Test. Work 5 Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/test-ujfv7/work-5" ext-link-type="uri">https://universe.roboflow.com/test-ujfv7/work-5</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B72-sensors-25-01016"><label>72.</label><element-citation publication-type="webpage"><article-title>Project be6od. 1104lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/project-be6od/1104lpg-cylinder" ext-link-type="uri">https://universe.roboflow.com/project-be6od/1104lpg-cylinder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B73-sensors-25-01016"><label>73.</label><element-citation publication-type="webpage"><article-title>1129. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1129/lpg-cylinder-unqn4" ext-link-type="uri">https://universe.roboflow.com/1129/lpg-cylinder-unqn4</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B74-sensors-25-01016"><label>74.</label><element-citation publication-type="webpage"><article-title>Test. Work Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/test-ujfv7/work-sisvh" ext-link-type="uri">https://universe.roboflow.com/test-ujfv7/work-sisvh</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B75-sensors-25-01016"><label>75.</label><element-citation publication-type="webpage"><article-title>Test. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/test-gqwj1/lpg-cylinder-aewhv" ext-link-type="uri">https://universe.roboflow.com/test-gqwj1/lpg-cylinder-aewhv</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B76-sensors-25-01016"><label>76.</label><element-citation publication-type="webpage"><article-title>1108. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1108/lpg-cylinder-jpho3" ext-link-type="uri">https://universe.roboflow.com/1108/lpg-cylinder-jpho3</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B77-sensors-25-01016"><label>77.</label><element-citation publication-type="webpage"><article-title>1. 221025_lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1-vdmlq/221025_lpg-cylinder" ext-link-type="uri">https://universe.roboflow.com/1-vdmlq/221025_lpg-cylinder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B78-sensors-25-01016"><label>78.</label><element-citation publication-type="webpage"><article-title>1111. Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1111-iq4b9/cylinder-zi4ri" ext-link-type="uri">https://universe.roboflow.com/1111-iq4b9/cylinder-zi4ri</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B79-sensors-25-01016"><label>79.</label><element-citation publication-type="webpage"><article-title>Submit. 1014 Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/submit/1014-8zn8g" ext-link-type="uri">https://universe.roboflow.com/submit/1014-8zn8g</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B80-sensors-25-01016"><label>80.</label><element-citation publication-type="webpage"><article-title>1. 221101_lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1-vdmlq/221101_lpg-cylinder" ext-link-type="uri">https://universe.roboflow.com/1-vdmlq/221101_lpg-cylinder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B81-sensors-25-01016"><label>81.</label><element-citation publication-type="webpage"><article-title>20221111. 1125 Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/20221111/1125-cylinder" ext-link-type="uri">https://universe.roboflow.com/20221111/1125-cylinder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B82-sensors-25-01016"><label>82.</label><element-citation publication-type="webpage"><article-title>202212. 1129 Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/202212/1129-lpg-cylinder" ext-link-type="uri">https://universe.roboflow.com/202212/1129-lpg-cylinder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B83-sensors-25-01016"><label>83.</label><element-citation publication-type="webpage"><article-title>GAS. GAS1021 Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/gas/gas1021" ext-link-type="uri">https://universe.roboflow.com/gas/gas1021</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B84-sensors-25-01016"><label>84.</label><element-citation publication-type="webpage"><article-title>20221111. 1111cyilnder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/20221111/1111cyilnder" ext-link-type="uri">https://universe.roboflow.com/20221111/1111cyilnder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B85-sensors-25-01016"><label>85.</label><element-citation publication-type="webpage"><article-title>1115. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1115/lpg-cylinder-i8gyn" ext-link-type="uri">https://universe.roboflow.com/1115/lpg-cylinder-i8gyn</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B86-sensors-25-01016"><label>86.</label><element-citation publication-type="webpage"><article-title>Test. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/test-6mwhf/lpg-cylinder-kvcv0" ext-link-type="uri">https://universe.roboflow.com/test-6mwhf/lpg-cylinder-kvcv0</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B87-sensors-25-01016"><label>87.</label><element-citation publication-type="webpage"><article-title>20221111. 1118 Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/20221111/1118-cylinder" ext-link-type="uri">https://universe.roboflow.com/20221111/1118-cylinder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B88-sensors-25-01016"><label>88.</label><element-citation publication-type="webpage"><article-title>1221. Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/1221-rdyxw/lpg-cylinder-a0fxk" ext-link-type="uri">https://universe.roboflow.com/1221-rdyxw/lpg-cylinder-a0fxk</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B89-sensors-25-01016"><label>89.</label><element-citation publication-type="webpage"><article-title>Test. Test Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/test-2813d/test-zbhbi" ext-link-type="uri">https://universe.roboflow.com/test-2813d/test-zbhbi</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B90-sensors-25-01016"><label>90.</label><element-citation publication-type="webpage"><article-title>20221111. 1115 Lpg Cylinder Dataset</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://universe.roboflow.com/20221111/1115-lpg-cylinder" ext-link-type="uri">https://universe.roboflow.com/20221111/1115-lpg-cylinder</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-10">(accessed on 10 April 2024)</date-in-citation></element-citation></ref><ref id="B91-sensors-25-01016"><label>91.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Rivest</surname><given-names>R.L.</given-names></name>
</person-group><article-title>Request for Comments 1321: The MD5 message digest algorithm</article-title><source>The Internet Engineering Task Force</source><year>1992</year><comment>Available online: <ext-link xlink:href="https://scholar.google.com/scholar?q=Request%20for%20Comments%201321:%20The%20MD5%20message%20digest%20algorithm" ext-link-type="uri">https://scholar.google.com/scholar?q=Request%20for%20Comments%201321:%20The%20MD5%20message%20digest%20algorithm</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-01-17">(accessed on 17 January 2025)</date-in-citation></element-citation></ref><ref id="B92-sensors-25-01016"><label>92.</label><element-citation publication-type="book"><person-group person-group-type="author">
<collab>Compressed Gas Association, Inc.</collab>
</person-group><source>Handbook of Compressed Gases</source><publisher-name>Springer Science &#x00026; Business Media</publisher-name><publisher-loc>Berlin, Germany</publisher-loc><year>2012</year></element-citation></ref><ref id="B93-sensors-25-01016"><label>93.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Girshick</surname><given-names>R.B.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1506.01497</pub-id><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id></element-citation></ref><ref id="B94-sensors-25-01016"><label>94.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Jocher</surname><given-names>G.</given-names></name>
<name><surname>Chaurasia</surname><given-names>A.</given-names></name>
<name><surname>Qiu</surname><given-names>J.</given-names></name>
</person-group><article-title>Ultralytics YOLO</article-title><year>2023</year><comment>Available online: <ext-link xlink:href="https://www.ultralytics.com/zh/yolo" ext-link-type="uri">https://www.ultralytics.com/zh/yolo</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-01-17">(accessed on 17 January 2025)</date-in-citation></element-citation></ref><ref id="B95-sensors-25-01016"><label>95.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Khanam</surname><given-names>R.</given-names></name>
<name><surname>Hussain</surname><given-names>M.</given-names></name>
</person-group><article-title>Yolov11: An overview of the key architectural enhancements</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2410.17725</pub-id></element-citation></ref><ref id="B96-sensors-25-01016"><label>96.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lv</surname><given-names>W.</given-names></name>
<name><surname>Xu</surname><given-names>S.</given-names></name>
<name><surname>Zhao</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>G.</given-names></name>
<name><surname>Wei</surname><given-names>J.</given-names></name>
<name><surname>Cui</surname><given-names>C.</given-names></name>
<name><surname>Du</surname><given-names>Y.</given-names></name>
<name><surname>Dang</surname><given-names>Q.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
</person-group><article-title>DETRs Beat YOLOs on Real-time Object Detection</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2304.08069</pub-id></element-citation></ref><ref id="B97-sensors-25-01016"><label>97.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1512.03385</pub-id></element-citation></ref><ref id="B98-sensors-25-01016"><label>98.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>T.Y.</given-names></name>
<name><surname>Dollar</surname><given-names>P.</given-names></name>
<name><surname>Girshick</surname><given-names>R.</given-names></name>
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Hariharan</surname><given-names>B.</given-names></name>
<name><surname>Belongie</surname><given-names>S.</given-names></name>
</person-group><article-title>Feature Pyramid Networks for Object Detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;26 July 2017</conf-date></element-citation></ref><ref id="B99-sensors-25-01016"><label>99.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rafique</surname><given-names>A.A.</given-names></name>
<name><surname>Ghadi</surname><given-names>Y.Y.</given-names></name>
<name><surname>Alsuhibany</surname><given-names>S.A.</given-names></name>
<name><surname>Chelloug</surname><given-names>S.A.</given-names></name>
<name><surname>Jalal</surname><given-names>A.</given-names></name>
<name><surname>Park</surname><given-names>J.</given-names></name>
</person-group><article-title>CNN based multi-object segmentation and feature fusion for scene recognition</article-title><source>Comput. Mat. Contin.</source><year>2022</year><volume>73</volume><fpage>4657</fpage><lpage>4675</lpage></element-citation></ref><ref id="B100-sensors-25-01016"><label>100.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Zheng</surname><given-names>S.</given-names></name>
<name><surname>Yang</surname><given-names>R.</given-names></name>
<name><surname>Zheng</surname><given-names>A.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Tang</surname><given-names>J.</given-names></name>
<name><surname>Luo</surname><given-names>B.</given-names></name>
</person-group><article-title>Pedestrian attribute recognition: A survey</article-title><source>Pattern Recognit.</source><year>2022</year><volume>121</volume><fpage>108220</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2021.108220</pub-id></element-citation></ref><ref id="B101-sensors-25-01016"><label>101.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kiefer</surname><given-names>J.</given-names></name>
<name><surname>Wolfowitz</surname><given-names>J.</given-names></name>
</person-group><article-title>Stochastic estimation of the maximum of a regression function</article-title><source>Ann. Math. Stat.</source><year>1952</year><volume>23</volume><fpage>462</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177729392</pub-id></element-citation></ref><ref id="B102-sensors-25-01016"><label>102.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kingma</surname><given-names>D.P.</given-names></name>
<name><surname>Ba</surname><given-names>J.</given-names></name>
</person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1412.6980</pub-id></element-citation></ref><ref id="B103-sensors-25-01016"><label>103.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Islam</surname><given-names>M.A.</given-names></name>
<name><surname>Kowal</surname><given-names>M.</given-names></name>
<name><surname>Jia</surname><given-names>S.</given-names></name>
<name><surname>Derpanis</surname><given-names>K.G.</given-names></name>
<name><surname>Bruce</surname><given-names>N.D.</given-names></name>
</person-group><article-title>Position, padding and predictions: A deeper look at position information in cnns</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2101.12322</pub-id><pub-id pub-id-type="doi">10.1007/s11263-024-02069-9</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01016-f001"><label>Figure 1</label><caption><p>Sample images from the proposed CylinDeRS dataset, acquired under CC BY 4.0 license: (top left to bottom right) images 1&#x02013;2 [<xref rid="B34-sensors-25-01016" ref-type="bibr">34</xref>], images 3&#x02013;4 [<xref rid="B35-sensors-25-01016" ref-type="bibr">35</xref>], images 5&#x02013;6 [<xref rid="B36-sensors-25-01016" ref-type="bibr">36</xref>], image 7 [<xref rid="B37-sensors-25-01016" ref-type="bibr">37</xref>], and image 8 [<xref rid="B38-sensors-25-01016" ref-type="bibr">38</xref>].</p></caption><graphic xlink:href="sensors-25-01016-g001" position="float"/></fig><fig position="float" id="sensors-25-01016-f002"><label>Figure 2</label><caption><p>The step-by-step presentation of the proposed dataset creation methodology. Four main phases are illustrated: data acquisition, filtering, annotation, and anonymization, all contributing to the creation of the final list of images and annotations.</p></caption><graphic xlink:href="sensors-25-01016-g002" position="float"/></fig><fig position="float" id="sensors-25-01016-f003"><label>Figure 3</label><caption><p>CylinDeRS dataset samples regarding (<bold>a</bold>) gas cylinder detection, where images are depicted with the corresponding ground truth bounding boxes, and (<bold>b</bold>) gas cylinder attribute classification, where gas cylinder instances are listed with their material (fiber, metal, or unknown), size (short, long, or unknown), and orientation (standing, fallen, or unknown) attribute labels.</p></caption><graphic xlink:href="sensors-25-01016-g003" position="float"/></fig><fig position="float" id="sensors-25-01016-f004"><label>Figure 4</label><caption><p>Multi-head multi-class classification in CylinDeRS, wherein each output head corresponds to a distinct attribute type, extracting a label from the three available categories for each type.</p></caption><graphic xlink:href="sensors-25-01016-g004" position="float"/></fig><fig position="float" id="sensors-25-01016-f005"><label>Figure 5</label><caption><p>Visual representation of indicative results for the gas cylinder detection task using the CylinDeRS dataset. The left column displays the input images fed into the models with the ground truth annotation of bounding boxes colorized with yellow, while the subsequent columns showcase the corresponding detection outcomes for each of the fine-tuned models.</p></caption><graphic xlink:href="sensors-25-01016-g005" position="float"/></fig><fig position="float" id="sensors-25-01016-f006"><label>Figure 6</label><caption><p>Cylinder detection false positive and negative samples in CylinDeRS dataset. The ground truth labels are depicted in the first row, while the samples in the second row indicate visual instances where the models have yielded erroneous predictions. In the last row, the potential cause of the erroneous prediction is provided.</p></caption><graphic xlink:href="sensors-25-01016-g006" position="float"/></fig><fig position="float" id="sensors-25-01016-f007"><label>Figure 7</label><caption><p>Attribute classification results on CylinDeRS dataset: representative true positive results. The top row displays images with their corresponding ground truth attribute labels. The middle and bottom rows show the predictions for the ResNet-50 and ResNet-101 models, respectively.</p></caption><graphic xlink:href="sensors-25-01016-g007" position="float"/></fig><fig position="float" id="sensors-25-01016-f008"><label>Figure 8</label><caption><p>Attribute classification results on CylinDeRS dataset: representative erroneous predictions. The ground truth attribute labels are depicted in the first row. The middle and bottom rows show the predictions for the ResNet-50 and ResNet-101 models, respectively. Erroneous predictions are highlighted in red.</p></caption><graphic xlink:href="sensors-25-01016-g008" position="float"/></fig><fig position="float" id="sensors-25-01016-f009"><label>Figure 9</label><caption><p>Confusion matrices for the gas-cylinder-related attributes: (<bold>a</bold>) material, (<bold>b</bold>) size, and (<bold>c</bold>) orientation attributes.</p></caption><graphic xlink:href="sensors-25-01016-g009" position="float"/></fig><table-wrap position="float" id="sensors-25-01016-t001"><object-id pub-id-type="pii">sensors-25-01016-t001_Table 1</object-id><label>Table 1</label><caption><p>CylinDeRS dataset statistics: number of images, number of gas cylinder instances, and average number of gas cylinders per image for the training, validation, test, and overall sets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Train</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Val</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Test</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">No. of Images</td><td align="center" valign="middle" rowspan="1" colspan="1">4915</td><td align="center" valign="middle" rowspan="1" colspan="1">1434</td><td align="center" valign="middle" rowspan="1" colspan="1">711</td><td align="center" valign="middle" rowspan="1" colspan="1">7060</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">No. of Instances</td><td align="center" valign="middle" rowspan="1" colspan="1">18,137</td><td align="center" valign="middle" rowspan="1" colspan="1">4862</td><td align="center" valign="middle" rowspan="1" colspan="1">2270</td><td align="center" valign="middle" rowspan="1" colspan="1">25,269</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Avg. No. of Instances per Image</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.6</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01016-t002"><object-id pub-id-type="pii">sensors-25-01016-t002_Table 2</object-id><label>Table 2</label><caption><p>CylinDeRS dataset attribute category statistics: number of instances per attribute category for the training, validation, test, and overall sets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Attribute</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Category</th><th align="right" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Train</th><th align="right" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Val</th><th align="right" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Test</th><th align="right" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Material</td><td align="left" valign="middle" rowspan="1" colspan="1">metal</td><td align="right" valign="middle" rowspan="1" colspan="1">14,769 (72%)</td><td align="right" valign="middle" rowspan="1" colspan="1">3948 (19%)</td><td align="right" valign="middle" rowspan="1" colspan="1">1797 (9%)</td><td align="right" valign="middle" rowspan="1" colspan="1">20,514 (100%)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">fiber</td><td align="right" valign="middle" rowspan="1" colspan="1">3201 (71%)</td><td align="right" valign="middle" rowspan="1" colspan="1">862 (19%)</td><td align="right" valign="middle" rowspan="1" colspan="1">455 (10%)</td><td align="right" valign="middle" rowspan="1" colspan="1">4518 (100%)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">material unk</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">167 (70%)</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52 (22%)</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18 (8%)</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">237 (100%)</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Size</td><td align="left" valign="middle" rowspan="1" colspan="1">short</td><td align="right" valign="middle" rowspan="1" colspan="1">9705 (72%)</td><td align="right" valign="middle" rowspan="1" colspan="1">2553 (19%)</td><td align="right" valign="middle" rowspan="1" colspan="1">1192 (9%)</td><td align="right" valign="middle" rowspan="1" colspan="1">13,450 (100%)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">long</td><td align="right" valign="middle" rowspan="1" colspan="1">4598 (70%)</td><td align="right" valign="middle" rowspan="1" colspan="1">1283 (20%)</td><td align="right" valign="middle" rowspan="1" colspan="1">647 (10%)</td><td align="right" valign="middle" rowspan="1" colspan="1">6528 (100%)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">size unk</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3834 (72%)</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1026 (19%)</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">431 (9%)</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5291 (100%)</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Orientation</td><td align="left" valign="middle" rowspan="1" colspan="1">standing</td><td align="right" valign="middle" rowspan="1" colspan="1">16,688 (72%)</td><td align="right" valign="middle" rowspan="1" colspan="1">4497 (19%)</td><td align="right" valign="middle" rowspan="1" colspan="1">2126 (9%)</td><td align="right" valign="middle" rowspan="1" colspan="1">23,311 (100%)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">fallen</td><td align="right" valign="middle" rowspan="1" colspan="1">1028 (74%)</td><td align="right" valign="middle" rowspan="1" colspan="1">250 (18%)</td><td align="right" valign="middle" rowspan="1" colspan="1">101 (8%)</td><td align="right" valign="middle" rowspan="1" colspan="1">1379 (100%)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">orientation unk</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">421 (73%)</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">115 (20%)</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43 (7%)</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">579 (100%)</td></tr></tbody></table></table-wrap></floats-group></article>