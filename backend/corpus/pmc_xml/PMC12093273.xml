<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Neurophotonics</journal-id><journal-id journal-id-type="iso-abbrev">Neurophotonics</journal-id><journal-id journal-id-type="coden">NEUROW</journal-id><journal-id journal-id-type="publisher-id">NPh</journal-id><journal-title-group><journal-title>Neurophotonics</journal-title></journal-title-group><issn pub-type="ppub">2329-423X</issn><issn pub-type="epub">2329-4248</issn><publisher><publisher-name>Society of Photo-Optical Instrumentation Engineers</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40401216</article-id><article-id pub-id-type="pmc">PMC12093273</article-id><article-id pub-id-type="doi">10.1117/1.NPh.12.2.025012</article-id><article-id pub-id-type="publisher-manuscript">NPh-24082GRR</article-id><article-id pub-id-type="publisher-id">24082GRR</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Papers</subject></subj-group><subj-group subj-group-type="SPIE-art-type"><subject>Paper</subject></subj-group></article-categories><title-group><article-title>Brain-wide 3D neuron detection and mapping with deep learning</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Yuanyang</given-names></name><xref rid="aff1" ref-type="aff">a</xref><xref rid="fn1" ref-type="author-notes">&#x02020;</xref><xref rid="b1" ref-type="bio"/><email>liuyuanyang926@163.com</email></contrib><contrib contrib-type="author"><name><surname>Gao</surname><given-names>Ziyan</given-names></name><xref rid="aff1" ref-type="aff">a</xref><xref rid="fn1" ref-type="author-notes">&#x02020;</xref><xref rid="b2" ref-type="bio"/><email>2399768703@qq.com</email></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Zhehao</given-names></name><xref rid="aff1" ref-type="aff">a</xref><xref rid="fn1" ref-type="author-notes">&#x02020;</xref><xref rid="b3" ref-type="bio"/><email>zhehao_xu@qq.com</email></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Chaoyue</given-names></name><xref rid="aff2" ref-type="aff">b</xref><xref rid="fn1" ref-type="author-notes">&#x02020;</xref><xref rid="b4" ref-type="bio"/><email>chaoyue_yang@hotmail.com</email></contrib><contrib contrib-type="author"><name><surname>Sun</surname><given-names>Pei</given-names></name><xref rid="aff2" ref-type="aff">b</xref><email>sunpei0615@qq.com</email></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Longhui</given-names></name><xref rid="aff1" ref-type="aff">a</xref><email>lilonghui@cqu.edu.cn</email></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1585-2161</contrib-id><name><surname>Jia</surname><given-names>Hongbo</given-names></name><xref rid="aff3" ref-type="aff">c</xref><email>jiahb@sibet.ac.cn</email></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Xiaowei</given-names></name><xref rid="aff2" ref-type="aff">b</xref><xref rid="aff4" ref-type="aff">d</xref><email>xiaowei_chen@tmmu.edu.cn</email></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6324-5206</contrib-id><name><surname>Liao</surname><given-names>Xiang</given-names></name><xref rid="aff1" ref-type="aff">a</xref><xref rid="cor1" ref-type="corresp">*</xref><xref rid="b5" ref-type="bio"/><email>xiang.liao@cqu.edu.cn</email></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Pan</surname><given-names>Junxia</given-names></name><xref rid="aff2" ref-type="aff">b</xref><xref rid="cor1" ref-type="corresp">*</xref><xref rid="b6" ref-type="bio"/><email>junxia@tmmu.edu.cn</email></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Wang</surname><given-names>Meng</given-names></name><xref rid="aff1" ref-type="aff">a</xref><xref rid="aff5" ref-type="aff">e</xref><xref rid="cor1" ref-type="corresp">*</xref><xref rid="b7" ref-type="bio"/><email>ezhwm@163.com</email></contrib><aff id="aff1"><label>a</label><institution>Chongqing University</institution>, School of Medicine, Center for Neurointelligence, Chongqing, <country>China</country></aff><aff id="aff2"><label>b</label><institution>Third Military Medical University</institution>, Brain Research Center, State Key Laboratory of Trauma and Chemical Poisoning, Chongqing, <country>China</country></aff><aff id="aff3"><label>c</label><institution>Chinese Academy of Sciences</institution>, Suzhou Institute of Biomedical Engineering and Technology, Brain Research Instrument Innovation Center, Suzhou, <country>China</country></aff><aff id="aff4"><label>d</label><institution>Chongqing Institute for Brain and Intelligence</institution>, Guangyang Bay Laboratory, Chongqing, <country>China</country></aff><aff id="aff5"><label>e</label><institution>Chongqing Medical University</institution>, Institute for Brain Science and Disease, Chongqing, <country>China</country></aff></contrib-group><author-notes><corresp id="cor1"><label>*</label>Address all correspondence to Xiang Liao, <email>xiang.liao@cqu.edu.cn</email>; Junxia Pan, <email>junxia@tmmu.edu.cn</email>; Meng Wang, <email>ezhwm@163.com</email></corresp><fn id="fn1"><label>&#x02020;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>20</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="ppub"><month>4</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>5</month><year>2025</year></pub-date><!--PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>.--><volume>12</volume><issue>2</issue><elocation-id>025012</elocation-id><history><date date-type="received"><day>18</day><month>9</month><year>2024</year></date><date date-type="rev-recd"><day>6</day><month>4</month><year>2025</year></date><date date-type="accepted"><day>18</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 The Authors</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>The Authors</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Published by SPIE under a Creative Commons Attribution 4.0 International License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.</license-p></license></permissions><self-uri xlink:title="pdf" xlink:href="NPh_12_2_025012.pdf"/><abstract><title>Abstract.</title><sec><title>Significance</title><p>Mapping the spatial distribution of specific neurons across the entire brain is essential for understanding the neural circuits associated with various brain functions, which in turn requires automated and reliable neuron detection and mapping techniques.</p></sec><sec><title>Aim</title><p>To accurately identify somatic regions from 3D imaging data and generate reliable soma locations for mapping to diverse brain regions, we introduce NeuronMapper, a brain-wide 3D neuron detection and mapping approach that leverages the power of deep learning.</p></sec><sec><title>Approach</title><p>NeuronMapper is implemented as a four-stage framework encompassing preprocessing, classification, detection, and mapping. Initially, whole-brain imaging data is divided into 3D sub-blocks during the preprocessing phase. A lightweight classification network then identifies the sub-blocks containing somata. Following this, a Video Swin Transformer&#x02013;based segmentation network delineates the soma regions within the identified sub-blocks. Last, the locations of the somata are extracted and registered with the Allen Brain Atlas for comprehensive whole-brain neuron mapping.</p></sec><sec><title>Results</title><p>Through the accurate detection and localization of somata, we achieved the mapping of somata at the one million level within the mouse brain. Comparative analyses with other soma detection techniques demonstrated that our method exhibits remarkably superior performance for whole-brain 3D soma detection.</p></sec><sec><title>Conclusions</title><p>Our approach has demonstrated its effectiveness in detecting and mapping somata within whole-brain imaging data. This method can serve as a computational tool to facilitate a deeper understanding of the brain&#x02019;s complex networks and functions.</p></sec></abstract><kwd-group><title>Keywords:</title><kwd>whole-brain imaging</kwd><kwd>neuron mapping</kwd><kwd>3D soma detection</kwd><kwd>deep learning</kwd><kwd>video swin transformer</kwd></kwd-group><funding-group><award-group id="sp1"><funding-source>National Natural Science Foundation of China</funding-source><award-id>32171096</award-id><award-id>32100812</award-id><award-id>31925018</award-id><award-id>32127801</award-id></award-group><award-group id="sp2"><funding-source>Chongqing Postdoctoral Special Funding project</funding-source><award-id>2021XM2012</award-id></award-group><funding-statement>This work was supported by grants from the National Natural Science Foundation of China (Grant Nos.&#x000a0;32171096, 32100812, 31925018, and 32127801) and the Chongqing Postdoctoral Special Funding project (Grant No.&#x000a0;2021XM2012).</funding-statement></funding-group><counts><fig-count count="7"/><table-count count="0"/><ref-count count="58"/><page-count count="22"/></counts><custom-meta-group><custom-meta><meta-name>running-head</meta-name><meta-value>Liu et&#x000a0;al.: Brain-wide 3D neuron detection and mapping with deep learning</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="sec1"><label>1</label><title>Introduction</title><p>To comprehend the neural circuits that underlie computations in the brain, it is essential to map individual, labeled neurons across the brain structure, which is one of the major challenges in contemporary neuroscience.<xref rid="r1" ref-type="bibr"><sup>1</sup></xref><named-content content-type="online"><xref rid="r2" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r3" ref-type="bibr"><sup>3</sup></xref> As the exact location and spatial arrangement of neurons are intrinsically linked to brain functions, it is crucial to achieve accurate neuron localization, particularly at a whole-brain scale.<xref rid="r4" ref-type="bibr"><sup>4</sup></xref> The advancements in labeling and optical imaging methodologies have enabled the monitoring of the brain which contains millions of neurons.<xref rid="r5" ref-type="bibr"><sup>5</sup></xref><named-content content-type="online"><xref rid="r6" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r7" ref-type="bibr"><sup>7</sup></xref> However, even if only 1% of these neurons are labeled, the manual annotation becomes extremely laborious and prone to error.<xref rid="r8" ref-type="bibr"><sup>8</sup></xref> Computational approaches provide a promising avenue by automating the detection of neurons, thus accelerating the neuron mapping process.<xref rid="r9" ref-type="bibr"><sup>9</sup></xref> Nevertheless, the complexities inherent in large-scale optical imaging datasets&#x02014;such as brightness fluctuations within 3D images, the diverse morphology of neurons, and the frequent overlap of somata with indistinct boundaries&#x02014;pose significant obstacles to automated processing. These complexities highlight the urgent necessity to develop robust and accurate tools for automatic soma detection, which is crucial for advancing whole-brain neuronal mapping efforts and will play a critical role in dissecting the brain&#x02019;s mechanisms.</p><p>In previous studies, a series of traditional methods and tools have been developed to detect somata in biomedical images. Intensity threshold is used when there are notable grayscale differences between somata and the background.<xref rid="r10" ref-type="bibr"><sup>10</sup></xref> Static threshold is applied in cell segmentation for extracting soma regions.<xref rid="r11" ref-type="bibr"><sup>11</sup></xref> The OTSU threshold determines the grayscale histogram of the image based on an appropriate setting to extract the foreground of the image.<xref rid="r12" ref-type="bibr"><sup>12</sup></xref> OTSU is generally efficient for intensity-based thresholding, but it tends to perform poorly in the presence of data imbalance, outliers, and artifacts. However, these methods are mainly confined to 2D images and yield limited results. Although some methods excel at identifying labeled cells in serial 2D sections, 2D analysis can introduce bias due to potential under- or overestimation of cell numbers based on sampling within the third dimension. The 3D analysis methods inherently address this limitation by accounting for the full spatial context. For example, Quan et&#x000a0;al.<xref rid="r13" ref-type="bibr"><sup>13</sup></xref> proposed NeuroGPS to extract morphological features for initial detection and combine L1 to minimize the foreground of modeled images via a spherical model (performance: true positive rate of 88%; false positive rate of 8%). Hu et&#x000a0;al.<xref rid="r14" ref-type="bibr"><sup>14</sup></xref> proposed an automatic soma segmentation method, based on the Rayburst sampling algorithm, which is suitable for datasets featuring touching or overlapping somata (performance: precision of 0.965; recall of 0.975). In addition, He et&#x000a0;al.<xref rid="r15" ref-type="bibr"><sup>15</sup></xref> initially achieved the soma detection results through distance transformation and then screened the results through a logistic regression model, which performed well in fruit-fly brain datasets (performance: sensitivity of 0.918; accuracy of 0.844; <inline-formula><mml:math id="math1" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>-score of 0.872). Despite these progressions, their detection methods remain basic, dataset-specific, and lack efficiency for those with dense somata.</p><p>Recent advancements in deep learning have shed light on new avenues for enhancing biological image processing. Convolutional neural networks (CNNs) have been extensively utilized in biomedical image processing owing to their capability to automatically learn multilevel features from raw data. The application of appropriate data augmentation techniques, such as random translations, rotations, and scaling, imparts CNNs with invariance to these transformations. This invariance has been key to achieving robust performance across various applications.<xref rid="r16" ref-type="bibr"><sup>16</sup></xref> For neuronal soma segmentation and detection tasks, some promising results have been achieved in previous studies. Among them, U-Net, a prominent CNN architecture, has performed exceptionally well in segmenting complex neuronal structures from electron microscopy images (performance: intersection over union of 0.9163; accuracy of 0.9779).<xref rid="r17" ref-type="bibr"><sup>17</sup></xref> Stringer et&#x000a0;al.<xref rid="r18" ref-type="bibr"><sup>18</sup></xref> proposed Cellpose, a deep learning&#x02013;based method that segments cells from a wide range of image types without requiring model retraining or parameter adjustments (performance: average precision of 0.91). Li et&#x000a0;al.<xref rid="r19" ref-type="bibr"><sup>19</sup></xref> utilized CNNs to classify the presence of somata, advancing the efforts in whole-brain data categorization. Tyson et&#x000a0;al.<xref rid="r9" ref-type="bibr"><sup>9</sup></xref> developed a pipeline that involves the application of a deep neural network to distinguish true cells and to accelerate the analysis (performance: Pearson correlation coefficient of 0.999, algorithm versus expert). Dong et&#x000a0;al.<xref rid="r20" ref-type="bibr"><sup>20</sup></xref> developed a deep supervised network and a novel training strategy for soma segmentation at single-neuron resolution (performance: Jaccard of 0.6811; Dice of 0.7910; precision of 0.7870; recall of 0.8329). Hu et&#x000a0;al.<xref rid="r21" ref-type="bibr"><sup>21</sup></xref> achieved neuronal soma segmentation using 3D multitask learning with U-shaped fully convolutional neural networks (performance: precision of 0.94; recall of 0.90; <inline-formula><mml:math id="math2" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>-score of 0.92; Dice coefficient of 0.8436). Wei et&#x000a0;al.<xref rid="r22" ref-type="bibr"><sup>22</sup></xref> developed two-stage deep neural networks for soma detection in large, high-resolution whole mouse brain images (performance: precision of 0.9812; recall of 0.891; <inline-formula><mml:math id="math3" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>-score of 0.934). However, these CNN-based networks, which are designed for sparsely distributed somata, may yield less than ideal results when dealing with densely distributed somata due to overlapping structures. Therefore, there remains a crucial need for methods specifically tailored to dense soma images. A significant challenge in deep learning&#x02013;based approaches is the effective encoding of long-range dependencies, which is hindered by the restricted receptive fields of convolution kernels.<xref rid="r23" ref-type="bibr"><sup>23</sup></xref> This limitation undermines dense prediction tasks and potentially leads to a loss of essential global information.<xref rid="r24" ref-type="bibr"><sup>24</sup></xref><sup>,</sup><xref rid="r25" ref-type="bibr"><sup>25</sup></xref></p><p>To deal with the challenges of accurately detecting somata and effectively analyzing 3D whole-brain datasets for comprehensive neuronal mapping, we present a brain-wide 3D neuron detection and mapping method (NeuronMapper). Our approach integrates preprocessing, classification, detection, and mapping modules, which are designed to streamline and optimize the entire processing workflow. In our pipeline, the classification module has a crucial role in reducing data redundancy by selectively filtering out irrelevant data blocks from the whole-brain dataset. This processing step ensures that only relevant information is passed on to the subsequent stages. Following this, the detection module uses a Video Swin Transformer to deal with long-range dependency issues and segment the somatic regions. By analyzing the connected domains defined by these segments and determining the center points as the locations of the somata, we establish a robust framework for neuron detection and mapping. The coordinates derived from individual sub-blocks are then aggregated, enabling the construction of a comprehensive, whole-brain neuronal map. This foundational mapping supports quantitative analysis in specific functional circuit studies and provides an integrated view of upstream and downstream neural circuitry, thereby advancing our understanding of brain networks.<xref rid="r26" ref-type="bibr"><sup>26</sup></xref><sup>,</sup><xref rid="r27" ref-type="bibr"><sup>27</sup></xref></p></sec><sec id="sec2"><label>2</label><title>Materials and Methods</title><sec id="sec2.1"><label>2.1</label><title>Experimental Dataset</title><sec id="sec2.1.1"><label>2.1.1</label><title>Animals and surgery</title><p>C57BL/6J male mice (aged 8 to 12 weeks) were obtained from the Laboratory Animal Center, Third Military Medical University. These animals were housed in a temperature- and humidity-controlled room on a cycle of 12-h light/dark (lights off at 19:00). All experimental procedures were carried out in accordance with the institutional animal welfare guidelines with the approval of the Third Military Medical University Animal Care and Use Committee. In the experiments, glass electrodes with a tip diameter of 20 to <inline-formula><mml:math id="math4" display="inline" overflow="scroll"><mml:mrow><mml:mn>30</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> filled with AAV2/2Retro Plus-hSyn-nuclear-EGFP were inserted, and <inline-formula><mml:math id="math5" display="inline" overflow="scroll"><mml:mrow><mml:mo form="prefix">&#x0223c;</mml:mo><mml:mn>200</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:math></inline-formula> was slowly injected into the auditory cortex. Thirty days after the injection, the animals were anesthetized, perfused, and fixated.</p></sec><sec id="sec2.1.2"><label>2.1.2</label><title>Fluorescence micro-optical sectioning tomography data acquisition and preprocessing</title><p>The preparation of brain specimens for fluorescence micro-optical sectioning tomography (fMOST) imaging was carried out following the Resin/HM20 Embedding Protocol.<xref rid="r28" ref-type="bibr"><sup>28</sup></xref> Basically, the samples were rinsed with PBS solution three times and dehydrated with ethanol solution at 4&#x000b0;C in the dark. Subsequently, the samples were penetrated with resin and quenched with acetic acid. The resin polymerization took place at 50&#x000b0;C for 8&#x000a0;h. For mouse whole-brain imaging, the embedded brain samples were imaged by the fMOST system at a resolution of <inline-formula><mml:math id="math6" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.35</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>0.35</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>. Image preprocessing procedures encompassed image stitching, brightness adjustment, and noise filtering to improve the signal-to-noise ratio of the soma data from the GFP channel. We transformed the data into cuboid format via TDat2017 software,<xref rid="r29" ref-type="bibr"><sup>29</sup></xref> facilitating the ease of manipulation and analysis in subsequent computational workflows.</p></sec></sec><sec id="sec2.2"><label>2.2</label><title>Neuron Detection and Mapping</title><sec id="sec2.2.1"><label>2.2.1</label><title>Overall process of analyzing whole-brain imaging data</title><p>We present a deep learning&#x02013;based approach (NeuronMapper) for the detection and mapping of somata within whole-brain images, which is implemented as a four-stage framework, including preprocessing, classification, detection, and mapping. Initially, the three-dimensional brain images undergo a strategic partitioning process, dividing it into a series of smaller, analyzable sub-blocks. Here, the maximum intensity projection (MIP) was utilized to transform 3D image data into a single 2D image by projecting the brightest pixel (voxel) from each layer of the stack onto the final image. This process highlights the most intense signals within the original 3D image data. For each sub-block, an MIP image is calculated, which is a preparatory step that facilitates efficient parallel processing and optimizes the computational workflow for subsequent analysis. The classification processing is inspired by the previous study,<xref rid="r22" ref-type="bibr"><sup>22</sup></xref> and it plays a crucial role by sifting through the sub-blocks to exclude those without detectable somata, thereby significantly reducing the dataset size for the segmentation stage. This selective filtering ensures that only sub-blocks containing somata are forwarded for further processing. The segmentation network then utilizes Video Swin Transformer&#x02013;based algorithms to outline the boundaries of somata, providing a precise morphological delineation. Following segmentation, a key step involves the application of a connected domain analysis to determine the centroid coordinates for each detected soma. This process enables us to accurately identify the location of each soma within the complex topology of the brain, contributing to a comprehensive understanding of neuron distribution. Through the proposed method, robust and precise detection of neuronal somata can be achieved throughout the entire brain, resulting in detailed soma locations. Finally, the combination of the coordinates and Allen Brain Atlas produces a comprehensive mapping of the somata in various brain regions. This facilitates a deep insight into the spatial arrangement and connectivity of neurons within the brain. Comprehensive elucidations of the algorithms used in the classification, segmentation, and mapping stages are provided in Secs.&#x000a0;<xref rid="sec2.2.2" ref-type="sec">2.2.2</xref>&#x02013;<xref rid="sec2.2.6" ref-type="sec">2.2.6</xref>.</p></sec><sec id="sec2.2.2"><label>2.2.2</label><title>Neuronal soma identification using the classification network</title><p>The preprocessing stage divided whole-brain image data into sub-blocks and processing them directly through detection would incur unnecessary computational expenses. To strategically avoid this inefficiency, we design a classification network that serves as a gatekeeper, selecting sub-blocks containing soma information while eliminating those lacking such information. Our approach initiates with the augmentation of the channel dimension through a convolutional layer, laying the foundation for feature extraction. Four successive stages, each composed of a convolutional layer, strided convolution, and a shortcut link, are sequentially employed to reduce the parameters while simultaneously capturing the essential characteristics of the images. Contrary to the conventional methods that advocate the use of max pooling layers, our method adopts convolutional layers with a stride of two. This application not only reduces the feature dimensions but also learns features efficiently and preserves spatial information. An average pooling layer is then placed to further compress the data, reducing the dimensionality of the feature map to a more manageable size. Except for the output layer, every convolutional layer is paired with a batch normalization layer and activated through the rectified linear unit (ReLU), a combination that facilitates the model&#x02019;s rapid and stable convergence. The multidimensional features learned by the convolutional layer are converted into one-dimensional features using the fully connected layer. Finally, the Softmax function is used to calculate the likelihood of each image belonging to a specific class, resulting in the generation of definite classification results (Fig. S1 in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>).</p></sec><sec id="sec2.2.3"><label>2.2.3</label><title>3D soma segmentation network</title><p>Following the classification stage, we design a segmentation network for separating the somatic regions from the remaining constituents of the image. The network architecture, which is shown in Fig. S2(a) in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>, is devised based on the Video Swin Transformer. This design starts with a series of convolutional layers, stacked to extract a detailed range of local features, followed by the strategic use of strided convolution to further refine these features. The resulting features are adjusted to meet the specifications of the transformer layer. Within the transformer layers, these features engage in a dynamic interaction, absorbing comprehensive global context and establishing intricate relationships across the entire image block. This enables the module to perceive the somatic regions not just as isolated entities but as integral parts of the broader spatial environment. Once exiting the transformer layers, the output undergoes a transformative upscaling process, returning to its original resolution through an up-sampling process. Concurrently, through the use of skip connections, the output is seamlessly merged with the corresponding encoder layers&#x02019; outputs, orchestrating a harmonious flow of information. This step-by-step restoration of resolution ensures that the fine balance between local detail and global perspective is maintained throughout the segmentation process. Each convolutional layer is enhanced with batch normalization to regularize the inputs and supported by the ReLU activation function, which introduces nonlinearity and facilitates feature learning. This design enables the network&#x02019;s ability to seamlessly integrate local and global information, significantly enhancing the precision of the segmentation outcome. The distinct components in the segmentation module are described in the following.</p><p>For the feature extraction, the input is a 3D image sub-block, denoted as <inline-formula><mml:math id="math7" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where it has a spatial resolution of <inline-formula><mml:math id="math8" display="inline" overflow="scroll"><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula> (height) &#x000d7; <inline-formula><mml:math id="math9" display="inline" overflow="scroll"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> (width), <inline-formula><mml:math id="math10" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula> is the depth dimension, and <inline-formula><mml:math id="math11" display="inline" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> is the number of channels. The initial convolutional layer maps the number of channels of the original data to the <inline-formula><mml:math id="math12" display="inline" overflow="scroll"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>-dimension, denoted as <inline-formula><mml:math id="math13" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. After the first stage of the encoder layer, the dimension remains unchanged, and then, the down-sampling effect is achieved through convolution with a step size of 2, and the size of the three dimensions of the feature is halved, denoted as <inline-formula><mml:math id="math14" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The second and third stages are both constituted by stacking two encoders and one convolution with a step size of 2, and the size of the feature is reduced while the input data is encoded layer-by-layer into low-resolution, high-level features, which are represented as <inline-formula><mml:math id="math15" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mn>8</mml:mn></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>8</mml:mn></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mi>D</mml:mi><mml:mn>8</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The data are down-sampled three times, and the size of the feature is one-eighth of the dimension of the original input. After the fourth stage of the four-layer convolutional encoders, the feature details are further extracted, and the dimensions do not change. Each data input is composed of a <inline-formula><mml:math id="math16" display="inline" overflow="scroll"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>-dimensional feature vector, and the rich local spatial context features are effectively embedded into the vector <inline-formula><mml:math id="math17" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula> after being processed by the feature extraction layers, which is represented as <inline-formula><mml:math id="math18" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>The transformer layer is designed to capture both localized and global contextual information, enabling enhanced feature representation for 3D soma segmentation. The transformer layer is composed of <inline-formula><mml:math id="math19" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> Video Swin Transformers, through two consecutive multihead self-attention layers and the self-attention. The number of Video Swin Transformer layers impacts both the model&#x02019;s feature-capturing capabilities and its computational complexity. A greater number of layers improve the model&#x02019;s ability to capture global features and enhance its expressiveness, which is beneficial for complex tasks such as neuron segmentation that involve intricate spatial structures. However, deeper networks can lead to overfitting and require more computational resources. Conversely, fewer layers emphasize local feature extraction, reducing computational demands and increasing efficiency, which is advantageous for real-time applications or scenarios with limited resources. In our model, we set <inline-formula><mml:math id="math20" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> to 1, achieving a balance between performance and efficiency. This design efficiently captures the essential features for 3D soma segmentation while minimizing computational overhead and reducing the risk of overfitting. The comparative results for different settings of <inline-formula><mml:math id="math21" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> are illustrated in Fig. S2(b) in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>. In the structure of Video Swin Transformers, the first layer uses the conventional window partitioning strategy to obtain nonoverlapping 3D windows. The second layer shifts the window according to a certain step size and then completes the self-attention processing. Using <inline-formula><mml:math id="math22" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> as the input of the transformer layer, the process applies layer normalization before setting the window size to <inline-formula><mml:math id="math23" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>, dividing the number of windows to <inline-formula><mml:math id="math24" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, and representing the window as <inline-formula><mml:math id="math25" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Then, the window-based multihead self-attention/shifted-window-based multihead self-attention (W-MSA/SW-MSA) operates within each 3D window, where W-MSA stands for window multi-head self-attention module, and SW-MSA represents shifted window-based multihead self-attention module. The feedforward processing is then applied, and the residual connection is applied after each processing. For the second layer, the output obtained by the previous layer is taken as the input of the SW-MSA layer, and windows are moved along the axes of height, width, and depth <inline-formula><mml:math id="math26" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and the other parts are consistent with the first layer.</p><p>The preceding encoder layer learns and extracts both local and global features. To obtain the ultimate spatial prediction results of the 3D data block, the up-sampling and restoration of the high-resolution images are completed by 3D convolution and the skip connection in the decoder. First, the channel dimension is reduced to be consistent with the input through two convolutional layers, and the feature map is reduced to a <inline-formula><mml:math id="math27" display="inline" overflow="scroll"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>-dimensional feature, which is represented as <inline-formula><mml:math id="math28" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. In the first stage, the output obtained in the third stage of feature extraction and the output of the restoration are concatenated by skip connection. In the second stage and the third stage, the feature is gradually restored to a high-resolution image through the decoder layer, and then the segmentation is completed.</p></sec><sec id="sec2.2.4"><label>2.2.4</label><title>Soma detection using connected domain analysis</title><p>Following the segmentation stage in which the soma regions are outlined, the segmented regions themselves are utilized as the connectivity domain. Within this domain, the centroid of each somatic region is calculated, representing the most likely location of the soma. An analysis is conducted to compare the predicted soma centroids with those manually labeled, using a predefined threshold as the acceptance criterion for the soma centroid matches. The threshold was determined by calculating the average radius of the somata, which resulted in a threshold value of 10 pixels. If the Euclidean distance, i.e., the straight-line distance in the three-dimensional space, between the predicted and labeled points is below this established threshold, the detection of the soma is confirmed as a successful identification. This algorithm ensures that the performance of the detection algorithm is examined and validated with ground-truth data, thereby confirming its effectiveness and precision.</p></sec><sec id="sec2.2.5"><label>2.2.5</label><title>Registration and quantitative analysis of the detected soma</title><p>The whole-brain imaging data were registered to the Allen Common Coordinate Framework.<xref rid="r30" ref-type="bibr"><sup>30</sup></xref> The registration process starts with the correction of spatial distortions in the imaged brains through a rigid registration approach. This is followed by a grayscale-guided, three-dimensional affine registration that aligns the imaging data with the Allen Brain Atlas. To further enhance the registration accuracy, particularly in more complex and localized regions, a dense landmark-based two-dimensional registration technique is adopted for focusing on specific areas to ensure an optimal fit. The Elastix software is utilized to carry out all registration steps, providing a framework for the alignment process. Once the registration is completed, the coordinates of the detected somata are transformed into the Allen Brain Atlas coordinate system, guided by the transformation parameters obtained during the registration stage. To ensure the reliability of the registration results, two experienced analysts independently evaluated the alignment across various mouse brain regions. They then reviewed any discordant results and discussed them to reach a final consensus (a back-to-back process), thereby ensuring the highest level of accuracy. Following this validation, neuron coordinates were systematically assigned to their respective brain regions, enabling an automated counting of neurons within each subregion. To provide a comparison of neuronal density and connection strength across different brain regions, the neuron counts were normalized by calculating the proportion of neurons in each subregion relative to the total number of neurons in the whole brain. This standardization facilitated a comprehensive analysis of neural connectivity patterns and densities, providing deep insights into the brain&#x02019;s complex neural architecture and functional organization.</p></sec><sec id="sec2.2.6"><label>2.2.6</label><title>Neural network model training and testing</title><p>Our approach was assessed on a 3D whole-brain imaging dataset; the model training included the classification network and the segmentation network. In the training stage, the model was constructed using Python 3.6 within the PyTorch 1.10.2 framework and was trained on an NVIDIA RTX A5000 GPU. For the image classification, soma segmentation, and detection tasks, two experienced annotators independently labeled each image and soma within the dataset. They subsequently compared their annotations to resolve any discrepancies, reaching a final consensus that established a reliable ground truth (GT). This GT was then used for training and testing the methods, providing a consistent and accurate benchmark for performance evaluation. For training the classification network, the dataset consisted of 180 images derived from four different mice. Positive samples were manually selected that covered diverse cases, including various instances of soma sub-blocks such as different densities, brightness levels, and distribution locations. Negative samples were randomly selected from the background images. The dataset was randomly divided into three sub-datasets: the training dataset (120 images), the validation dataset (30 images), and the testing dataset (30 images). We randomly repeated the data assignment to the datasets 10 times to validate the stability of our classification network. We trained the four methods (our classification network, ResNet18, VGG16, and DenseNet121) for 100 epochs. The classification network was trained using the stochastic gradient descent optimizer with an initial learning rate of 0.0001 and a minimum batch size of 8. To enhance the model&#x02019;s generalization and prevent overfitting, an early stopping strategy was employed. Specifically, after each epoch, the area under the curve (AUC) on the validation dataset was calculated and monitored. Training was halted early if there was no significant improvement in the validation AUC for 10 consecutive epochs [Fig. S3(a) in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>].</p><p>During the model training stage of segmentation, the dataset was divided into three sub-datasets from two different mice: two training datasets (22 blocks per dataset), two validation datasets (6 blocks per dataset), and two testing datasets (10 blocks per dataset). Two-round cross-validation was conducted by selecting sub-datasets from the mouse imaging data, ensuring separate training, validation, and testing datasets for each round. In this study, we evaluated seven deep learning&#x02013;based methods: our segmentation network, SomaSegmenter, Cellpose, SegNet, 3D U-Net, ResUNet, and TransBTS. Each method was trained from scratch without pretrained weights, using a dynamic learning rate and a training duration of 200 epochs. The Adam optimizer with an initial learning rate of 0.0001 was used to train the segmentation network. The learning rate decay strategy was adopted, and the learning rate decayed by 10% every 10 epochs. The best model was obtained using the early stopping strategy, where training was stopped if the Dice coefficient did not improve over 30 consecutive epochs [Fig. S3(b) in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>]. Each epoch contained 80 iterations and the minimum batch size was set to 1 in the segmentation network training stage. Dice loss was used in the training network task.</p><p>During the training stage, a comprehensive data augmentation pipeline was applied to the input images to enhance sample diversity and improve the robustness of the segmentation network. This pipeline comprised a random crop operation, a left-right random flip, and an up-down random flip. The random crop operation extracted fixed-size patches with a specified crop size of 128&#x000a0;pixels. Both the left-right and up-down flips were applied with a probability of 50% each. To maintain the spatial correspondence between images and their respective segmentation labels, these transformations were simultaneously applied to both.</p><p>In the testing phase, preprocessing was applied to the input images to optimize the segmentation network&#x02019;s performance. The process began with resampling to standardize image resolution, ensuring all inputs have a uniform scale despite potential variations in their original resolutions. Images were resized by applying down-sampling factors to the <inline-formula><mml:math id="math29" display="inline" overflow="scroll"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:math></inline-formula>-axis (depth) and <inline-formula><mml:math id="math30" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="math31" display="inline" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> axes (width and height), aligning the dimensions with the model&#x02019;s input requirements. Mathematically, this involves scaling each dimension by a factor that reduces computational load while preserving key features. Next, padding was added to the images to ensure compatibility with the convolutional layers of the network. By adding zeros around the edges, padding maintains the alignment of convolutional operations with the image boundaries, thereby preventing edge information loss. The padding size is calculated to make the image dimensions multiples of the stride length, allowing for consistent processing across the entire image. To manage large images efficiently and avoid memory overflow, patch extraction was employed. The image was divided into smaller, overlapping patches based on specified patch size and stride parameters. The sliding window technique used for patch extraction ensures that critical features at the boundaries are captured through the overlap, whereas the number of patches is determined by how many strides fit within the padded image dimensions. This approach enables efficient handling of large images without losing important boundary information.</p></sec></sec><sec id="sec2.3"><label>2.3</label><title>Performance Evaluation</title><p>To quantify classification performance, the AUC, accuracy, sensitivity, and specificity were used as the metrics to evaluate the methods.<xref rid="r31" ref-type="bibr"><sup>31</sup></xref>
<disp-formula id="e001"><mml:math id="math32" display="block" overflow="scroll"><mml:mrow><mml:mtext>Accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>TN</mml:mi></mml:mrow><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>TN</mml:mi><mml:mo>+</mml:mo><mml:mi>FP</mml:mi><mml:mo>+</mml:mo><mml:mi>FN</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(1)</label></disp-formula><disp-formula id="e002"><mml:math id="math33" display="block" overflow="scroll"><mml:mrow><mml:mtext>Sensitivity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>TP</mml:mi></mml:mrow><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>FN</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(2)</label></disp-formula><disp-formula id="e003"><mml:math id="math34" display="block" overflow="scroll"><mml:mrow><mml:mtext>Specificity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>TN</mml:mi></mml:mrow><mml:mrow><mml:mi>TN</mml:mi><mml:mo>+</mml:mo><mml:mi>FP</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(3)</label></disp-formula>where true positive (TP) regions refer to the number of cases where positives are correctly predicted, true negative (TN) regions refer to the number of cases where negatives are correctly predicted, false positive (FP) regions indicate the number of cases incorrectly predicted as positive, and false negative (FN) regions denote the number of cases incorrectly predicted as negative.</p><p>To quantitatively analyze segmentation accuracy, Dice similarity coefficient (DSC) and average Euclidean distance were used to evaluate the methods.<xref rid="r32" ref-type="bibr"><sup>32</sup></xref> The metrics are defined as follows: <disp-formula id="e004"><mml:math id="math35" display="block" overflow="scroll"><mml:mrow><mml:mi>DCS</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(4)</label></disp-formula>where <inline-formula><mml:math id="math36" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math37" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represent the regions inside the automatic segmentation and the manually marked segmentation (ground truth), respectively. <disp-formula id="e005"><mml:math id="math38" display="block" overflow="scroll"><mml:mrow><mml:mtext>Average Euclidean Distance</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext>&#x02009;</mml:mtext><mml:msub><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">&#x02016;</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02016;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">&#x02016;</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02016;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(5)</label></disp-formula><disp-formula id="e006"><mml:math id="math39" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">&#x02016;</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#x02016;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(6)</label></disp-formula>where <inline-formula><mml:math id="math40" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> represents the number of matched points, <inline-formula><mml:math id="math41" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> represents the prediction results, <inline-formula><mml:math id="math42" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula> is the specified threshold, <inline-formula><mml:math id="math43" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the <inline-formula><mml:math id="math44" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>&#x02019;th point in the ground truth dataset, and <inline-formula><mml:math id="math45" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the corresponding matched point in the prediction dataset. During the calculation, the distance between <inline-formula><mml:math id="math46" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and each point in <inline-formula><mml:math id="math47" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> is computed, and the point that satisfies the threshold condition is identified as a candidate match. If multiple points satisfy the threshold, the point with the minimum distance is selected as the unique match.</p><p>Three metrics (precision, recall, and <inline-formula><mml:math id="math48" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>-score) were used for the evaluation of the detection performance:<xref rid="r33" ref-type="bibr"><sup>33</sup></xref>
<disp-formula id="e007"><mml:math id="math49" display="block" overflow="scroll"><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>TP</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>GT</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(7)</label></disp-formula><disp-formula id="e008"><mml:math id="math50" display="block" overflow="scroll"><mml:mrow><mml:mtext>Recall</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>TP</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mtext>detected</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(8)</label></disp-formula><disp-formula id="e009"><mml:math id="math51" display="block" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mtext>Recall</mml:mtext><mml:mo>&#x000d7;</mml:mo><mml:mtext>Precision</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Recall</mml:mtext><mml:mo>+</mml:mo><mml:mtext>Precision</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(9)</label></disp-formula>where <inline-formula><mml:math id="math52" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>GT</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of ground truth somata, <inline-formula><mml:math id="math53" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>TP</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of true-positive somata, and <inline-formula><mml:math id="math54" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mtext>detected</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the number of detected somata.</p></sec><sec id="sec2.4"><label>2.4</label><title>Statistical Analysis</title><p>In the results, the summarized data are presented as the mean &#x000b1; <italic>SD</italic>. To compare data between two groups, we used a two-sided Wilcoxon signed-rank test to determine statistical significance, with a threshold of <inline-formula><mml:math id="math55" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> for the statistical difference. All data processing in this study was performed using scripts written in Python.</p><p>For further statistical information regarding the experiments, please refer to the figure legends.</p></sec></sec><sec id="sec3"><label>3</label><title>Results</title><sec id="sec3.1"><label>3.1</label><title>Brain-Wide Imaging of Individual Neuronal Somata</title><p>The neocortex of mammal functions as a central processor, handling complex neuronal signals within localized microcircuits and promoting the synthesis of information across diverse brain layers and regions. To elucidate the complexity and to demonstrate the capabilities of neuronal mapping, we utilized whole-brain images from mice that had undergone retrograde viral labeling. The experimental data were acquired from four male C57BL/6 mice, aged 11 to 12 weeks, which were labeled using AAV2/2Retro Plus-hSyn-nuclear-EGFP and imaged using the fMOST imaging system. The detailed experimental procedure is provided in Sec.&#x000a0;<xref rid="sec2" ref-type="sec">2</xref>. <xref rid="f1" ref-type="fig">Figure&#x000a0;1</xref> shows a visualization of the soma distribution patterns obtained in a representative mouse [<xref rid="f1" ref-type="fig">Fig.&#x000a0;1(a)</xref>], highlighting the areas with densely distributed somata [<xref rid="f1" ref-type="fig">Fig.&#x000a0;1(b)</xref>] and the variability in fluorescence intensity among somata [<xref rid="f1" ref-type="fig">Fig.&#x000a0;1(c)</xref>]. Representative somata and instances of background noise are depicted in <xref rid="f1" ref-type="fig">Figs.&#x000a0;1(d)</xref> and <xref rid="f1" ref-type="fig">1(e)</xref>, respectively. Certain sub-blocks without somata may contain noises that closely resemble somata, potentially leading to misclassification. It should be noted that the 3D imaging of the whole mouse brain with single-neuron resolution produces several terabytes of image data. To address this issue and to standardize the imaging data for further processing, cubic interpolation was employed to resize the raw <inline-formula><mml:math id="math56" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.35</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>0.35</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> image blocks. Subsequently, these blocks were subdivided into smaller sub-blocks (<inline-formula><mml:math id="math57" display="inline" overflow="scroll"><mml:mrow><mml:mn>512</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>512</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>512</mml:mn><mml:mrow><mml:mtext>&#x02009;&#x02009;</mml:mtext></mml:mrow><mml:mrow><mml:mtext>voxels</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>) with a standardized spatial resolution of <inline-formula><mml:math id="math58" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.35</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>0.35</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>0.35</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mtext>voxel</mml:mtext></mml:mrow></mml:math></inline-formula>, ensuring uniform voxel dimensions across the dataset. Comprehensive details about the imaging dataset can be found in Table S1 in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>.</p><fig position="float" id="f1"><label>Fig. 1</label><caption><p>Examples of somata and background noises in the whole-brain imaging data. (a)&#x000a0;An example of whole-brain imaging data that contains a large number of somata. (b)&#x000a0;Sub-blocks having densely distributed somata. (c)&#x000a0;Sub-blocks having somata with diverse fluorescence intensity. (d)&#x000a0;Representative somata within the sub-blocks. (e)&#x000a0;Representative background noises within the sub-blocks.</p></caption><graphic xlink:href="NPh-012-025012-g001" position="float"/></fig></sec><sec id="sec3.2"><label>3.2</label><title>Pipeline of the Proposed Neuron Detection and Mapping Approach</title><p>Our approach is devised to enable high-throughput detection and analysis of neuronal somata, conforming to a structured workflow (<xref rid="f2" ref-type="fig">Fig.&#x000a0;2</xref>). This process commences with the preparatory stage of transforming 3D whole-brain imaging data into smaller, manageable sub-blocks, and then generating 2D MIP images at this stage [<xref rid="f2" ref-type="fig">Fig.&#x000a0;2(a)</xref>]. The subsequent phase involves the classification module [<xref rid="f2" ref-type="fig">Fig.&#x000a0;2(b)</xref>], where MIP 2D images are assessed to identify those carrying soma information. Once classified, the detection module employs a Video Swin Transformer&#x02013;based segmentation network to process the corresponding 3D sub-blocks. This network segments the soma regions within these sub-blocks [<xref rid="f2" ref-type="fig">Fig.&#x000a0;2(c)</xref>]. After segmentation, a connected domain analysis is conducted to determine the precise coordinates of each detected neuron. Finally, the mapping module integrates the soma spatial information, correlating neuron positions with a 3D brain atlas. This mapping assigns each neuron to a specific brain region [<xref rid="f2" ref-type="fig">Fig.&#x000a0;2(d)</xref>]. In total, the proposed approach represents an integrated technique for soma detection and analysis in large-scale 3D brain imaging datasets, enabling efficient investigations of neuronal structure patterns.</p><fig position="float" id="f2"><label>Fig. 2</label><caption><p>Processing pipeline of the proposed 3D whole-brain neuron detection and mapping. (a)&#x000a0;A 3D whole-brain imaging data is first processed into multiple sub-blocks and then further converted into 2D MIP images which serve as the input for classification. (b)&#x000a0;The classification module identifies the images that contain soma information and further tags the corresponding 3D sub-blocks. (c)&#x000a0;The detection module follows the tag and segments the soma regions in the corresponding sub-blocks, and the detected coordinates are obtained by taking the center points of the soma regions. (d)&#x000a0;The coordinates of the detected neurons are transformed into the Allen Brain Atlas coordinate system (left). The detected neurons are counted automatically and the number of neurons in each brain region is divided by the total in the whole brain (right).</p></caption><graphic xlink:href="NPh-012-025012-g002" position="float"/></fig><p>During the training and inference stages of the NeuronMapper framework, the time performance of each component is summarized as follows: For the classification task, training proceeds at a rate of 4.5&#x000a0;s per epoch with a batch size of 8, whereas inference operates at a speed of 10.2&#x000a0;ms per sample using the same batch size. Regarding segmentation, training occurs at a pace of 12&#x000a0;s per epoch when using a batch size of 1, and inference is performed at a rate of 91.7&#x000a0;ms per sample with the batch size maintained at 1.</p></sec><sec id="sec3.3"><label>3.3</label><title>Identification of Images Containing Somata Within Whole-Brain Imaging Data</title><p>Considering the abundant and complex soma information generated by whole-brain imaging, the precision by which a method can identify somata turns into a crucial factor. In our quest to enhance computational efficiency, we initiated by converting the 3D sub-blocks into 2D MIP images, a step that is crucial for the classification process. We designed a classification network based on the principle of ResNet architecture, including an initial convolutional layer that is responsible for increasing the image channel dimensions.<xref rid="r34" ref-type="bibr"><sup>34</sup></xref> After this, the network expands through four residual blocks, along with a pooling layer, which jointly works to extract the core features of the images while prudently reducing the number of parameters, ensuring that no essential image characteristics are compromised in the process (Fig. S1 in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>). The detailed design of each layer within the classification network is described in Table S2 in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>.</p><p>To validate the performance of our classification module, we assembled a dataset that consists of 180 exemplary sub-blocks, which were carefully selected to reflect a wide range of soma densities, diverse fluorescence intensities, and instances of background noise. This dataset was randomly divided as: 120 sub-blocks were designated for training, 30 sub-blocks for validation, and 30 sub-blocks for testing. This random assignment was repeated 10 times, and the performance metrics were averaged over the 10 outcomes. <xref rid="f3" ref-type="fig">Figure&#x000a0;3(a)</xref> illustrates four representative classification results, featuring different soma distribution patterns, along with images that solely consist of background noises [<xref rid="f3" ref-type="fig">Fig.&#x000a0;3(b)</xref>]. Furthermore, we conducted a performance comparison involving our classification module and other three deep neural networks, namely, DenseNet121, ResNet18, and VGG16.<xref rid="r34" ref-type="bibr"><sup>34</sup></xref><named-content content-type="online"><xref rid="r35" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r36" ref-type="bibr"><sup>36</sup></xref> All compared neural networks were trained on the same dataset as NeuronMapper, ensuring that the weights were learned within our study. The preprocessing steps and other methodologies were directly applied from previous studies. We first assessed the models&#x02019; capabilities using two performance indicators&#x02014;the AUC [<xref rid="f3" ref-type="fig">Fig.&#x000a0;3(c)</xref>] and accuracy scores [<xref rid="f3" ref-type="fig">Fig.&#x000a0;3(d)</xref>]. Our classification module yielded promising results, achieving an AUC of 0.93 and an accuracy of 0.90. In comparison, when evaluated on the same whole-brain imaging dataset, no other neural networks attained an AUC exceeding 0.90 or an accuracy surpassing 0.80 [<xref rid="f3" ref-type="fig">Figs.&#x000a0;3(c)</xref> and <xref rid="f3" ref-type="fig">3(d)</xref>]. Moreover, to ensure that the recognition methods are both sensitive and specific, we evaluated sensitivity [<xref rid="f3" ref-type="fig">Fig.&#x000a0;3(e)</xref>] and specificity [<xref rid="f3" ref-type="fig">Fig.&#x000a0;3(f)</xref>]. Our proposed network exhibits superior sensitivity and specificity compared with other methods, although some of these differences do not reach statistical significance. In addition, our classification module achieved the smallest size of model parameters and the fastest speed among the tested methods [<xref rid="f3" ref-type="fig">Fig.&#x000a0;3(g)</xref>]. This significant disparity clearly demonstrates the superior performance of our classification module in accurately identifying sub-blocks with somata in the intricate pattern of whole-brain imaging data.</p><fig position="float" id="f3"><label>Fig. 3</label><caption><p>Representative results of classification and comparison with other methods. (a)&#x000a0;2D MIP images are classified as containing soma information. (b)&#x000a0;2D MIP images are classified as not having soma information. Yellow arrows indicate somata in the images and blue arrows imply background noises. (c)&#x02013;(f)&#x000a0;Comparison of our classification module with other methods in terms of the AUC (c), accuracy (d), sensitivity (e), and specificity (f). The score for each random test is represented as a gray dot. *<inline-formula><mml:math id="math59" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>; **<inline-formula><mml:math id="math60" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="math61" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> images; two-sided Wilcoxon signed-rank test; error bars are SD. (g)&#x000a0;Comparison of our classification module with other methods in terms of processing speed and parameters.</p></caption><graphic xlink:href="NPh-012-025012-g003" position="float"/></fig></sec><sec id="sec3.4"><label>3.4</label><title>Segmentation of Somata Within 3D Imaging Data</title><p>Upon the completion of the classification stage, the image blocks that were verified to contain somata underwent indexing, which enabled the detection module to selectively target the relevant 3D sub-blocks in accordance with the established index. The design of the segmentation network aimed to maximize the utilization of both high-level semantic information and detailed low-level features, which was accomplished through the strategic combination of convolutional layers with the U-Net architecture. This architecture is characterized by its sequence of encoder and decoder layers, facilitating the preservation of spatial context during the segmentation process. To further enhance the segmentation&#x02019;s accuracy, especially in dealing with long-range dependencies and incorporating comprehensive contextual information, the module incorporated the Video Swin Transformer. This integration of the structural advantages of U-Net with the transformer&#x02019;s prowess in handling complex spatial relationships was specifically intended to reduce occurrences of incomplete or inaccurate segmentations, thereby enabling significantly more refined and precise somata boundaries [Fig. S2(a) in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>]. Figure S4 in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link> depicts representative examples of the segmentation outcomes generated by the segmentation network and other four deep learning segmentation networks. These comparisons clearly reveal the enhanced precision and robustness of our segmentation network, particularly in addressing challenges such as nonuniform brightness and incomplete somatic details. In the task of segmenting somata, our network consistently outperformed SegNet, 3D U-Net, ResUNet, and TransBTS, providing more reliable segmentation results.<xref rid="r37" ref-type="bibr"><sup>37</sup></xref><named-content content-type="online"><xref rid="r38" ref-type="bibr"/><xref rid="r39" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r40" ref-type="bibr"><sup>40</sup></xref> The competing methods frequently overlook soma information to varying degrees, potentially degrading the accuracy of subsequent coordinate acquisitions [Figs. S4(a) and S4(b) in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>]. Notably, when dealing with sub-blocks that contain a single soma among elements, our segmentation module demonstrated remarkable reliability. It effectively segmented the soma while simultaneously excluding similar noises. By contrast, the other methods incorrectly segmented the noises to different extents [Fig. S4(c) in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>].</p><p>For a more comprehensive understanding, Fig. S5 in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link> provides a 2D view through the 3D segmentation results across a range of conditions. In sub-blocks marked by unevenly distributed fluorescence intensity, our segmentation module had better performance than the other methods. For example, 3D U-Net, ResUNet, and TransBTS only partially segmented the soma [Fig. S5(a) in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>]. In situations where sub-blocks had incomplete soma morphology, our segmentation network was able to accurately delineate the soma regions. Meanwhile, SegNet exhibited deficiency by missing the soma [Fig. S5(b) in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>]. In the challenging situation of sub-blocks with weak fluorescence signals, our approach achieved the highest level of segmentation accuracy among all the evaluated methods [Figs. S5(c)&#x02013;S5(e) in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>]. By contrast, the competing methods often failed to locate neurons in various circumstances.</p></sec><sec id="sec3.5"><label>3.5</label><title>Comparison of Neuron Segmentation and Detection Methods</title><p>Following the image segmentation, a subsequent connected domain analysis was carried out to locate and extract the precise centroid coordinates of each segmented neuronal soma. <xref rid="f4" ref-type="fig">Figure&#x000a0;4(a)</xref> shows two representative examples that highlight the raw data, the segmentation of soma structures, and the detection of their exact locations at the sub-block scale. These demonstrations clearly verify the robustness of our approach in accurately recognizing and spatially mapping neurons within the 3D image data while simultaneously ensuring an outstanding level of consistency and reliability when applied to various sub-block regions.</p><fig position="float" id="f4"><label>Fig. 4</label><caption><p>Comparison of soma segmentation and detection methods. (a)&#x000a0;Two representative detection processes from raw data to segmented and detected results are presented at the sub-block level. (b)&#x02013;(f)&#x000a0;The comparison between our approach and other methods (SomaSegmenter, Cellpose, NeuroGPS, Rayburst, Cellfinder, SegNet, 3D U-Net, ResUNet, and TransBTS) for segmentation metrics, including Dice coefficient (b), Euclidean distance (unit: pixels) (c), and detection metrics, including precision (d), recall rate (e), and <inline-formula><mml:math id="math62" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>-score (f). The score for testing each image block is indicated as a gray dot. *<inline-formula><mml:math id="math63" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>; **<inline-formula><mml:math id="math64" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>; ***<inline-formula><mml:math id="math65" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>; <italic>ns</italic>, not significant; <inline-formula><mml:math id="math66" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> image blocks; two-sided Wilcoxon signed-rank test; error bars are SD.</p></caption><graphic xlink:href="NPh-012-025012-g004" position="float"/></fig><p>To further validate the capability of our approach, we carried out comparisons against nine other methods: SomaSegmenter (available at <ext-link xlink:href="https://github.com/lens-biophotonics/SomaSegmenter" ext-link-type="uri">https://github.com/lens-biophotonics/SomaSegmenter</ext-link>), Cellpose, NeuroGPS, Rayburst, Cellfinder, SegNet, 3D U-Net, ResUNet, and TransBTS, for neuron segmentation and detection.<xref rid="r9" ref-type="bibr"><sup>9</sup></xref><sup>,</sup><xref rid="r13" ref-type="bibr"><sup>13</sup></xref><sup>,</sup><xref rid="r14" ref-type="bibr"><sup>14</sup></xref><sup>,</sup><xref rid="r18" ref-type="bibr"><sup>18</sup></xref><sup>,</sup><xref rid="r37" ref-type="bibr"><sup>37</sup></xref><named-content content-type="online"><xref rid="r38" ref-type="bibr"/><xref rid="r39" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r40" ref-type="bibr"><sup>40</sup></xref> The statistical comparison results are summarized in <xref rid="f4" ref-type="fig">Figs.&#x000a0;4(b)</xref>&#x02013;<xref rid="f4" ref-type="fig">4(f)</xref>. In the tests, we used image sub-blocks containing different distribution patterns of somata and performed the segmentation and detection processing. Our initial emphasis was also on the segmentation fidelity, and thus, we quantified the Dice similarity coefficient and Euclidean distance for our proposed method and the competing approaches. The results revealed a significant disparity in favor of our approach and a significantly and statistically higher Dice coefficient [<xref rid="f4" ref-type="fig">Fig.&#x000a0;4(b)</xref>], which was in sharp contrast to those of Cellpose. Furthermore, the Euclidean distance measure of our approach shows the lowest values [<xref rid="f4" ref-type="fig">Fig.&#x000a0;4(c)</xref>], also suggesting better segmentation performance than the other methods. These results demonstrated the superiority of our method in the segmentation of neuronal somata. After that, we performed an assessment of the efficacy of neuron detection. Using the metrics of precision, recall rate, and <inline-formula><mml:math id="math67" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>-score, we evaluated the detection performance of each method. The comprehensive evaluation confirmed that our approach clearly outperformed the competing methods, boasting a significantly higher precision, recall rate, and <inline-formula><mml:math id="math68" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>-score [<xref rid="f4" ref-type="fig">Figs.&#x000a0;4(d)</xref>&#x02013;<xref rid="f4" ref-type="fig">4(f)</xref>]. This remarkable performance attests to the reliability and effectiveness of our method.</p><p>Given the challenges posed by overlapping cells for segmentation, we examined such cases and compared the segmentation performance (Fig. S6 in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>). A representative example clearly demonstrates that NeuronMapper provided superior segmentation quality, whereas other methods did not match our results. Specifically, Cellpose and Rayburst missed many somata during segmentation, and the other methods either failed to capture certain regions of somata or produced lower-quality segmentations.</p><p>In addition, as NeuronMapper is a multistage approach that includes an initial classification stage to exclude images not requiring further analysis, we conducted a test to validate the role of this classification module. We compared the performance of our method both with and without the classification module to assess its impact. The detection results, shown in Fig. S7 in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>, indicate that there is no significant difference in performance whether the classification module is used or not. Specifically, for images lacking somata, our detection module seemed to not generate noticeable false positives. Therefore, these findings suggest that the inclusion of the classification stage did not appreciably affect the overall accuracy.</p></sec><sec id="sec3.6"><label>3.6</label><title>Generalization Testing of Classification and Detection Methods</title><p>To further validate the generality and stability of our method, we employed publicly available brain imaging datasets from mice for testing our classification, segmentation, and detection algorithms. The classification images were sourced from the Allen Brain Institute&#x02019;s fMOST imaging system, specifically using three mouse brain images with IDs 321237-17302, 321244-17545, and 373368-18455.<xref rid="r22" ref-type="bibr"><sup>22</sup></xref><sup>,</sup><xref rid="r41" ref-type="bibr"><sup>41</sup></xref> For this study, we selected 30 3D sub-blocks from these datasets and converted them into 2D images using slicing and MIP, creating a dataset that included both images with and without somata for generalization testing. We then applied the classification networks, previously trained on our lab&#x02019;s datasets, to infer the presence or absence of somata in these unseen brain images [<xref rid="f5" ref-type="fig">Fig.&#x000a0;5(a)</xref>]. Although there was a slight performance drop compared with the results shown in <xref rid="f3" ref-type="fig">Fig.&#x000a0;3</xref>, this is attributed to the classification networks not having been retrained on the new public dataset. Despite this, as illustrated in <xref rid="f5" ref-type="fig">Figs.&#x000a0;5(b)</xref>&#x02013;<xref rid="f5" ref-type="fig">5(e)</xref>, NeuronMapper consistently outperformed the other three methods across various performance metrics, aligning with our earlier findings.</p><fig position="float" id="f5"><label>Fig. 5</label><caption><p>Comparison with other methods for classification in public dataset. (a)&#x000a0;2D MIP images are classified as containing soma information and not having soma information. (b)&#x02013;(e)&#x000a0;Comparison of our classification module with other methods in terms of the AUC (b), accuracy (c), sensitivity (d), and specificity (e). The score for each random test is represented as a gray dot. *<inline-formula><mml:math id="math69" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>; **<inline-formula><mml:math id="math70" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>; ***<inline-formula><mml:math id="math71" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>; ns, not significant; <inline-formula><mml:math id="math72" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> images; two-sided Wilcoxon signed-rank test; error bars are SD.</p></caption><graphic xlink:href="NPh-012-025012-g005" position="float"/></fig><p>In addition, we applied our detection module to a Nissl-stained brain imaging dataset acquired using the micro-optical sectioning tomography system,<xref rid="r21" ref-type="bibr"><sup>21</sup></xref> specifically utilizing data from ID 01 (<ext-link xlink:href="https://github.com/keepersecond/neuronal-soma-segmentation/releases/" ext-link-type="uri">https://github.com/keepersecond/neuronal-soma-segmentation/releases/</ext-link>). This dataset was divided into 25 sub-blocks, each with dimensions of <inline-formula><mml:math id="math73" display="inline" overflow="scroll"><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>86</mml:mn><mml:mrow><mml:mtext>&#x02009;&#x02009;</mml:mtext></mml:mrow><mml:mrow><mml:mtext>voxels</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="math74" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.35</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>0.35</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>0.35</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mtext>voxel</mml:mtext></mml:mrow></mml:math></inline-formula>), to form a segmentation dataset for generalization testing. Given that the original dataset lacked ground truth (GT) annotations, we relabeled it. Two experienced annotators independently labeled each neuron and subsequently compared their results to reach a final consensus for the GT. After training the models on our dataset, we applied them to the unseen Nissl-stained data. The representative examples in <xref rid="f6" ref-type="fig">Fig.&#x000a0;6(a)</xref> demonstrate that our approach achieved good performance in both segmentation and detection. By comparing performance metrics for segmentation (Dice coefficient and Euclidean distance) and detection (precision, recall, and <inline-formula><mml:math id="math75" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>-score), it is evident that NeuronMapper outperformed SomaSegmenter, Cellpose, NeuroGPS, Rayburst, and Cellfinder [<xref rid="f6" ref-type="fig">Figs.&#x000a0;6(b)</xref>&#x02013;<xref rid="f6" ref-type="fig">6(d)</xref>]. Consequently, NeuronMapper consistently delivers reliable and robust soma segmentation and detection results.</p><fig position="float" id="f6"><label>Fig. 6</label><caption><p>Comparison with other methods for segmentation and detection in public dataset. (a)&#x000a0;Two representative detection processes from raw data to segmented and detected results are presented at the sub-block level. (b)&#x02013;(d)&#x000a0;The comparison between our approach and five other methods for segmentation metrics, including Dice coefficient (b), Euclidean distance (unit: pixels) (c), and detection metrics, including precision, recall rate, and <inline-formula><mml:math id="math76" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>-score (d). The score for testing each image block is indicated as a gray dot. *<inline-formula><mml:math id="math77" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>; **<inline-formula><mml:math id="math78" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>; ***<inline-formula><mml:math id="math79" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>; ns, not significant; <inline-formula><mml:math id="math80" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula> image blocks; two-sided Wilcoxon signed-rank test; error bars are SD.</p></caption><graphic xlink:href="NPh-012-025012-g006" position="float"/></fig></sec><sec id="sec3.7"><label>3.7</label><title>Brain-Wide Neuron Detection and Mapping</title><p>We then performed whole-brain soma detection, with the detected individual soma over a million level (e.g., 1,231,906 detected neurons from mouse #1, Table S1 in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>). A 3D visualization of the total input neurons within a mouse brain is exhibited in <xref rid="f7" ref-type="fig">Fig.&#x000a0;7(a)</xref>.</p><fig position="float" id="f7"><label>Fig. 7</label><caption><p>Brain-wide soma detection and mapping based on the proposed method. (a)&#x000a0;The 3D whole-brain neuron detection result (1,231,906 detected neurons, mouse #1) is shown in horizontal (left) and coronal (right) views. (b)&#x000a0;A 3D view of the detected somata in the whole-brain atlas, with the brain regions being marked by different colors. (c)&#x000a0;Quantitative analysis of input neurons in different brain regions of the mouse is shown in panel (a). SS; somatosensory area; VIS, visual areas, MO, somatomotor areas; ACA, anterior cingulate area; AI, agranular insular area; PERI, Peririnal area; ECT, Ectorhinal area; VISC, visceral area; RSP, retrosplenial area; TEA, temporal association areas; RHP, retrohippocampal region.</p></caption><graphic xlink:href="NPh-012-025012-g007" position="float"/></fig><p>To demonstrate the capabilities of our approach developed for 3D whole-brain neuron mapping, we applied our method to the whole-brain imaging datasets for the detection and mapping of somata (Table S1 in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>). With the detected somata, we proceeded to map neurons with registration and perform quantitative analyses for the different brain regions [<xref rid="f7" ref-type="fig">Fig.&#x000a0;7(b)</xref>]. The integration of whole-brain atlas and quantitative analysis revealed the distribution of neurons in a series of brain regions while simultaneously uncovering the interesting ipsi-contra bias shown by input neurons [<xref rid="f7" ref-type="fig">Fig.&#x000a0;7(c)</xref>]. This comprehensive analysis thus provides a profound insight into the underlying neuroanatomical architecture, highlighting the potential of our approach in advancing neuroscience research.</p></sec></sec><sec id="sec4"><label>4</label><title>Discussion</title><p>Mapping of the spatial distribution of specific neurons is essential for understanding the neural circuits that are related to various brain functions.<xref rid="r42" ref-type="bibr"><sup>42</sup></xref><sup>,</sup><xref rid="r43" ref-type="bibr"><sup>43</sup></xref> Developing accurate and fast soma detection techniques is able to accelerate our quest for understanding the brain.<xref rid="r44" ref-type="bibr"><sup>44</sup></xref><sup>,</sup><xref rid="r45" ref-type="bibr"><sup>45</sup></xref> However, the innovations of whole-brain imaging techniques have brought about high-resolution, large-scale data that far exceeds the capacity of manual labeling.<xref rid="r46" ref-type="bibr"><sup>46</sup></xref> To address this dilemma, we present an automated approach designed to analyze labeled somata within whole-brain imaging data. The proposed approach initiates with dividing whole-brain data into smaller, more manageable sub-blocks, and transforming them into MIP 2D images. Then, a classification network is used to screen these images to accurately identify those containing somata and tag the associated sub-blocks. Subsequently, a segmentation network uses the tags to precisely segment the soma regions within the corresponding sub-blocks. Based on these segmented areas, the centroid coordinates are extracted for whole-brain and registered with Allen Brain Atlas for neuron mapping analyses.</p><p>Our approach provides a solution to the following issues: first, our classification module is devised to distinguish the MIP 2D images that contain somata from those that do not. Our classification network is designed as a four-stage network structure. This network structure is shallow, and each layer has a single basic block, which makes the network model simple and lightweight (Fig. S1 in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>). In comparison to deeper networks such as ResNet18 and DenseNet121, our network parameters and computational complexity are smaller, reducing the risk of overfitting during classification training. Each basic block contains two convolution layers and a shortcut connection that directly adds the input to the output without changing the feature dimension. This connection helps gradient propagation, avoids the issue of gradient disappearing, makes the feature extraction process efficient, and can well retain the original information while extracting effective high-level features. Regarding down-sampling, we utilized a convolution layer with a step length of 2. Compared with VGG16, which employs the maximum pooling layer for down-sampling, the convolution layer can retain more information for down-sampling, which is conducive to enhancing the performance of classification tasks. The results strongly confirm the superiority and effectiveness of our approach in accurately identifying sub-block images with the presence of soma (<xref rid="f3" ref-type="fig">Fig.&#x000a0;3</xref>).</p><p>Second, in contrast to previous studies that mainly focus on brain slices,<xref rid="r19" ref-type="bibr"><sup>19</sup></xref><sup>,</sup><xref rid="r47" ref-type="bibr"><sup>47</sup></xref><sup>,</sup><xref rid="r48" ref-type="bibr"><sup>48</sup></xref> our approach directly processes soma information within 3D whole-brain image data (<xref rid="f4" ref-type="fig">Fig.&#x000a0;4</xref>). Our segmentation network exhibits exceptional precision in outlining soma regions, with few cases of misclassification or segmentation incompleteness. In brain-wide 3D neuron detection and mapping, a single neuron can span multiple tiles, with the information contained in a single tile often being insufficient to capture the neuron&#x02019;s boundary continuity or global morphology. This insufficiency can lead to a single neuron instance being fragmented into multiple parts during segmentation. Our method addresses this challenge by leveraging SW-MSA and a hierarchical structure to build both local and long-range dependencies. By repeatedly applying self-attention modules within this framework, the network establishes cross-tile dependencies at deeper layers and across larger spatial extents. Following this, we employ connected-component analysis to consolidate instance segmentation results at tile boundaries, ensuring each neuron receives a unified label. This approach effectively prevents neuronal fragmentation, thereby enhancing the accuracy and integrity of the segmentation.</p><p>We tested our NeuronMapper against other neural methods, including SomaSegmenter, Cellpose, NeuroGPS, Rayburst, Cellfinder, SegNet, 3D UNet, ResUNet, and TransBTS. Comparative evaluations of segmentation outcomes are shown in <xref rid="f4" ref-type="fig">Figs.&#x000a0;4</xref> and <xref rid="f6" ref-type="fig">6</xref> and Figs. S4&#x02013;S6 in the <ext-link xlink:href="https://doi.org/10.1117/1.NPh.12.2.025012.s01" ext-link-type="uri">Supplementary Material</ext-link>, demonstrating our network&#x02019;s accuracy even under adverse imaging conditions such as fluctuating brightness levels, the presence of a contaminant, and discontinuous soma data. The strategic integration of the Video Swin Transformer within our segmentation network substantially enhances the handling of long-range dependent information, a capability that is often lacking in traditional CNN models.<xref rid="r49" ref-type="bibr"><sup>49</sup></xref> Despite the traditional dominance of CNNs in biological image segmentation, their inherent limitations in capturing the global context have hindered their full potential. On the contrary, the Swin Transformer, distinguished by its computation through shifted window mechanisms, has proven its effectiveness in various applications, including noise reduction<xref rid="r50" ref-type="bibr"><sup>50</sup></xref> and tumor segmentation,<xref rid="r51" ref-type="bibr"><sup>51</sup></xref><sup>,</sup><xref rid="r52" ref-type="bibr"><sup>52</sup></xref> and most recently, a study highlighted its successful use in two-photon imaging data processing.<xref rid="r53" ref-type="bibr"><sup>53</sup></xref> However, these applications were confined to 2D segmentation paradigms. Notably, a recent contribution emphasized the utilization of Swin Transformers for 3D segmentation tasks in brain tumor MRI datasets, although not at the cellular resolution.<xref rid="r54" ref-type="bibr"><sup>54</sup></xref> Complementing this, the inclusion of skip connections facilitates the retention of high-resolution image details throughout the segmentation process, ensuring that no detail is overlooked in the pursuit of precision.<xref rid="r55" ref-type="bibr"><sup>55</sup></xref> Based on these improvements, our segmentation network provided unbiased and reliable segmentation similar to manual annotations for 3D image sub-blocks.</p><p>Given the ongoing challenges in processing complex brain imaging data across various scenarios,<xref rid="r33" ref-type="bibr"><sup>33</sup></xref> we demonstrated the generalizability of our trained models by evaluating their classification, segmentation, and detection capabilities on new, unseen public mouse brain imaging datasets. Although dataset division somewhat affected performance, the comparison results (<xref rid="f5" ref-type="fig">Figs.&#x000a0;5</xref> and <xref rid="f6" ref-type="fig">6</xref>) show that NeuronMapper maintains robust generalization capabilities. Our model exhibits consistently high performance in classification, segmentation, and detection tasks, even when applied to imaging data from different laboratories. Moving forward, we aim to further enhance our approach to improve model generalizability and performance across diverse datasets.</p><p>Finally, our approach uses connected domain analysis to identify and locate the somata in three-dimensional space. By analyzing the segmented results, the geometric center of each soma was determined and regarded as its coordinate. These coordinates were then integrated to enable the visualization and exploration of the distribution of somata at both the sub-block and whole-brain levels. The 3D visualization of somata provides a valuable understanding of the overall organization of the neurons in the brain. Advancements in neuroscience technologies have positioned transsynaptic viral tracers as a powerful tool for mapping functional neural circuits. Retrograde transsynaptic viral tracers, such as rabies virus and pseudorabies virus, have been widely utilized to delineate the presynaptic inputs to neurons that have been transduced. Therefore, we introduced nuclear-locating elements into our labeling strategy and combined retrograde and monosynaptic anterograde viral labeling methods with high-resolution whole-brain imaging to uncover the brain-wide fine connectivity structures. By categorizing neurons based on their spatial location and distribution, it allows screening of functional neural connectivity embedded in complex brain circuits. This enabled us to examine the quantitative, target-specific, and layer-specific characteristics of the neural input and output connectivity. In <xref rid="f7" ref-type="fig">Fig.&#x000a0;7(a)</xref>, asymmetry in neuronal density can be observed between the two hemispheres. The results indicate that for both input and output connections, nearly all regions were ipsilateral-biased to the side of virus injection, with neuronal density predominantly observed in the ipsilateral hemisphere, which is consistent with the previous studies.<xref rid="r3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="r56" ref-type="bibr"><sup>56</sup></xref> Researchers can observe the spatial arrangements of somata, identify spatial patterns or clustering, and acquire a deep understanding of the brain&#x02019;s intricate connectivity. By mapping the detected soma to specific brain regions [<xref rid="f7" ref-type="fig">Figs.&#x000a0;7(b)</xref> and <xref rid="f7" ref-type="fig">7(c)</xref>], it has the potential to uncover new aspects of insight into the functional circuitry and the underlying topology of neural networks, promoting the field of neuroscience toward a deeper understanding of brain functions.<xref rid="r57" ref-type="bibr"><sup>57</sup></xref><sup>,</sup><xref rid="r58" ref-type="bibr"><sup>58</sup></xref></p><p>Although NeuronMapper has demonstrated success in precisely segmenting the regions of neuronal somata at the whole-brain level, it encounters challenges when it comes to accurately segmenting closely adjacent somata without the usage of post-processing. Even though this limitation did not have a substantial influence on the quantification results presented in the current study, it is an aspect that requires further improvement to overcome the challenging spatial proximity and provide a more effective segmentation of neuronal bodies. Furthermore, we plan to improve the functionalities of the whole-brain registration and brain region analysis. The aim is to provide researchers with a more comprehensive and reliable tool for analyzing input and output neurons in various brain regions. Therefore, we will focus on improving our approach that can seamlessly integrate specific brain region information into the segmentation and quantification processes. This would streamline the analysis workflow, saving researchers&#x02019; time and effort while ensuring accurate results.</p></sec><sec id="sec5"><label>5</label><title>Conclusion</title><p>In summary, our approach demonstrated its effectiveness in the detection and mapping of soma in whole-brain imaging data. It accurately distinguished soma regions from background noises, performed precise segmentation, and generated reliable soma locations for mapping to diverse brain regions. This brain-wide neuron detection and mapping capability enhances our understanding of brain connectivity, neural circuitry, and underlying computational processes.</p></sec><sec id="sec6" sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="s01" position="float" content-type="local-data"><object-id pub-id-type="doi">10.1117/1.NPh.12.2.025012.s01</object-id><media xlink:href="NPh_012_025012_SD001.pdf" id="d101e2329" position="anchor"/></supplementary-material></sec></body><back><ack><title>Acknowledgments</title><p>
<named-content content-type="funding-statement">This work was supported by grants from the National Natural Science Foundation of China (Grant Nos.&#x000a0;32171096, 32100812, 31925018, and 32127801) and the Chongqing Postdoctoral Special Funding project (Grant No.&#x000a0;2021XM2012).</named-content>
</p></ack><bio id="b1"><p><bold>Yuanyang Liu</bold> received her master&#x02019;s degree in biomedical engineering from Chongqing University, China, in June 2024. Her research focuses on neuroimaging data analysis.</p></bio><bio id="b2"><p><bold>Ziyan Gao</bold> received her master&#x02019;s degree in basic medicine from Chongqing University, China, in June 2023. Her research focuses on data analysis for neural circuit research.</p></bio><bio id="b3"><p><bold>Zhehao Xu</bold> received his master&#x02019;s degree in biomedical engineering from Guangxi University, China, in June 2023. Currently, he is pursuing an EngD in biomedical engineering at Chongqing University, China. His research focuses on neuroimaging data analysis with deep learning.</p></bio><bio id="b4"><p><bold>Chaoyue Yang</bold> received her bachelor&#x02019;s degree in preventive medicine from Third Military Medical University, China, in June 2023. Currently, she is pursuing a master&#x02019;s degree at the same institution. Her research focuses on neuronal imaging, the mapping of brain functions, and the exploration of brain enhancement.</p></bio><bio id="b5"><p><bold>Xiang Liao</bold> is an associate professor at the School of Medicine, Chongqing University, China. His current research interests include two-photon imaging, whole-brain optical imaging, and neuroinformatics. He develops automated tools to analyze neuroimaging data by using computer vision and deep learning approaches.</p></bio><bio id="b6"><p><bold>Junxia Pan</bold> is an assistant researcher at Third Military Medical University. Her current research interests include two-photon microscopy, whole-brain imaging with single-cell resolution, and auditory information processing in the brain. The research achievements include brain-wide projection reconstruction of single functionally defined neurons.</p></bio><bio id="b7"><p><bold>Meng Wang</bold> is an associate professor at Chongqing Medical University. His research interests are the encoding of behavior-associated sensory information and the biological application of an advanced two-photon microscope. The research achievements include chronic two-photon imaging and electrophysiology, and electroporation techniques have been established in the mouse auditory cortex; holistic bursting cells that specifically encode complex sound information are found in the 2/3 layers of the auditory cortex.</p></bio><bio id="d101e3875" content-type="general"><p>Biographies of the other authors are not available.</p></bio><sec sec-type="conflict"><title>Disclosures</title><p>All authors declare no conflicts of financial interest.</p></sec><sec sec-type="data-availability"><title>Code and Data Availability</title><p>The code of NeuronMapper is freely available at <ext-link xlink:href="https://github.com/XZH-James/NeuronMapper" ext-link-type="uri">https://github.com/XZH-James/NeuronMapper</ext-link>. Data underlying the results presented in this paper are not publicly available but may be obtained from the corresponding author Xiang Liao (xiang.liao@cqu.edu.cn) upon reasonable request.</p></sec><ref-list><title>References</title><ref id="r1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Morphological diversity of single neurons in molecularly defined cell types</article-title>,&#x0201d; <source>Nature</source>
<volume>598</volume>(<issue>7879</issue>), <fpage>174</fpage>&#x02013;<lpage>181</lpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1038/s41586-021-03941-1</pub-id><pub-id pub-id-type="pmid">34616072</pub-id>
</mixed-citation></ref><ref id="r2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poo</surname><given-names>M. M.</given-names></name></person-group>, &#x0201c;<article-title>Transcriptome, connectome and neuromodulation of the primate brain</article-title>,&#x0201d; <source>Cell</source>
<volume>185</volume>(<issue>15</issue>), <fpage>2636</fpage>&#x02013;<lpage>2639</lpage> (<year>2022</year>).<pub-id pub-id-type="coden">CELLB5</pub-id><issn>0092-8674</issn><pub-id pub-id-type="doi">10.1016/j.cell.2022.05.011</pub-id><pub-id pub-id-type="pmid">35732175</pub-id>
</mixed-citation></ref><ref id="r3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Whole-brain direct inputs to and axonal projections from excitatory and inhibitory neurons in the mouse primary auditory area</article-title>,&#x0201d; <source>Neurosci. Bull.</source>
<volume>38</volume>(<issue>6</issue>), <fpage>576</fpage>&#x02013;<lpage>590</lpage> (<year>2022</year>).<pub-id pub-id-type="coden">NRPBA2</pub-id><issn>0028-3967</issn><pub-id pub-id-type="doi">10.1007/s12264-022-00838-5</pub-id><pub-id pub-id-type="pmid">35312957</pub-id>
</mixed-citation></ref><ref id="r4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frasconi</surname><given-names>P.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Large-scale automated identification of mouse brain cells in confocal light sheet microscopy images</article-title>,&#x0201d; <source>Bioinformatics</source>
<volume>30</volume>(<issue>17</issue>), <fpage>i587</fpage>&#x02013;<lpage>i593</lpage> (<year>2014</year>).<pub-id pub-id-type="coden">BOINFP</pub-id><issn>1367-4803</issn><pub-id pub-id-type="doi">10.1093/bioinformatics/btu469</pub-id><pub-id pub-id-type="pmid">25161251</pub-id>
</mixed-citation></ref><ref id="r5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>T. H.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Long-term optical access to an estimated one million neurons in the live mouse cortex</article-title>,&#x0201d; <source>Cell Rep.</source>
<volume>17</volume>(<issue>12</issue>), <fpage>3385</fpage>&#x02013;<lpage>3394</lpage> (<year>2016</year>).<pub-id pub-id-type="doi">10.1016/j.celrep.2016.12.004</pub-id><pub-id pub-id-type="pmid">28009304</pub-id>
</mixed-citation></ref><ref id="r6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>T.</given-names></name><name><surname>Gong</surname><given-names>H.</given-names></name><name><surname>Yuan</surname><given-names>J.</given-names></name></person-group>, &#x0201c;<article-title>Whole-brain optical imaging: a powerful tool for precise brain mapping at the mesoscopic level</article-title>,&#x0201d; <source>Neurosci. Bull.</source>
<volume>39</volume>(<issue>12</issue>), <fpage>1840</fpage>&#x02013;<lpage>1858</lpage> (<year>2023</year>).<pub-id pub-id-type="coden">NRPBA2</pub-id><issn>0028-3967</issn><pub-id pub-id-type="doi">10.1007/s12264-023-01112-y</pub-id><pub-id pub-id-type="pmid">37715920</pub-id>
</mixed-citation></ref><ref id="r7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manley</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Simultaneous, cortex-wide dynamics of up to 1&#x000a0;million neurons reveal unbounded scaling of dimensionality with neuron number</article-title>,&#x0201d; <source>Neuron</source>
<volume>112</volume>(<issue>10</issue>), <fpage>1694</fpage>&#x02013;<lpage>1709.e5</lpage> (<year>2024</year>).<pub-id pub-id-type="coden">NERNET</pub-id><issn>0896-6273</issn><pub-id pub-id-type="doi">10.1016/j.neuron.2024.02.011</pub-id><pub-id pub-id-type="pmid">38452763</pub-id>
</mixed-citation></ref><ref id="r8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Bartheld</surname><given-names>C. S.</given-names></name><name><surname>Bahney</surname><given-names>J.</given-names></name><name><surname>Herculano-Houzel</surname><given-names>S.</given-names></name></person-group>, &#x0201c;<article-title>The search for true numbers of neurons and glial cells in the human brain: a review of 150 years of cell counting</article-title>,&#x0201d; <source>J. Comp. Neurol.</source>
<volume>524</volume>(<issue>18</issue>), <fpage>3865</fpage>&#x02013;<lpage>3895</lpage> (<year>2016</year>).<pub-id pub-id-type="coden">JCNEAM</pub-id><issn>0021-9967</issn><pub-id pub-id-type="doi">10.1002/cne.24040</pub-id><pub-id pub-id-type="pmid">27187682</pub-id>
</mixed-citation></ref><ref id="r9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyson</surname><given-names>A. L.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>A deep learning algorithm for 3D cell detection in whole mouse brain image datasets</article-title>,&#x0201d; <source>PLoS Comput. Biol.</source>
<volume>17</volume>(<issue>5</issue>), <fpage>e1009074</fpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1371/journal.pcbi.1009074</pub-id><pub-id pub-id-type="pmid">34048426</pub-id>
</mixed-citation></ref><ref id="r10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oberlaender</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Automated three-dimensional detection and counting of neuron somata</article-title>,&#x0201d; <source>J. Neurosci. Methods</source>
<volume>180</volume>(<issue>1</issue>), <fpage>147</fpage>&#x02013;<lpage>160</lpage> (<year>2009</year>).<pub-id pub-id-type="coden">JNMEDT</pub-id><issn>0165-0270</issn><pub-id pub-id-type="doi">10.1016/j.jneumeth.2009.03.008</pub-id><pub-id pub-id-type="pmid">19427542</pub-id>
</mixed-citation></ref><ref id="r11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kesler</surname><given-names>B.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Automated cell boundary and 3D nuclear segmentation of cells in suspension</article-title>,&#x0201d; <source>Sci. Rep.</source>
<volume>9</volume>(<issue>1</issue>), <fpage>10237</fpage> (<year>2019</year>).<pub-id pub-id-type="coden">SRCEC3</pub-id><issn>2045-2322</issn><pub-id pub-id-type="doi">10.1038/s41598-019-46689-5</pub-id><pub-id pub-id-type="pmid">31308458</pub-id>
</mixed-citation></ref><ref id="r12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B. L.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Gain: a graphical method to automatically analyze individual neurite outgrowth</article-title>,&#x0201d; <source>J. Neurosci. Methods</source>
<volume>283</volume>, <fpage>62</fpage>&#x02013;<lpage>71</lpage> (<year>2017</year>).<pub-id pub-id-type="coden">JNMEDT</pub-id><issn>0165-0270</issn><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.03.013</pub-id><pub-id pub-id-type="pmid">28336360</pub-id>
</mixed-citation></ref><ref id="r13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quan</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>NeuroGPS: automated localization of neurons for brain circuits using L1 minimization model</article-title>,&#x0201d; <source>Sci. Rep.</source>
<volume>3</volume>, <fpage>1414</fpage> (<year>2013</year>).<pub-id pub-id-type="coden">SRCEC3</pub-id><issn>2045-2322</issn><pub-id pub-id-type="doi">10.1038/srep01414</pub-id><pub-id pub-id-type="pmid">23546385</pub-id>
</mixed-citation></ref><ref id="r14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Touching soma segmentation based on the Rayburst sampling algorithm</article-title>,&#x0201d; <source>Neuroinformatics</source>
<volume>15</volume>(<issue>4</issue>), <fpage>383</fpage>&#x02013;<lpage>393</lpage> (<year>2017</year>).<issn>1539-2791</issn><pub-id pub-id-type="doi">10.1007/s12021-017-9336-y</pub-id><pub-id pub-id-type="pmid">28940176</pub-id>
</mixed-citation></ref><ref id="r15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>G. W.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Soma detection in 3D images of neurons using machine learning technique</article-title>,&#x0201d; <source>Neuroinformatics</source>
<volume>16</volume>(<issue>1</issue>), <fpage>31</fpage>&#x02013;<lpage>41</lpage> (<year>2018</year>).<issn>1539-2791</issn><pub-id pub-id-type="doi">10.1007/s12021-017-9342-0</pub-id><pub-id pub-id-type="pmid">29032511</pub-id>
</mixed-citation></ref><ref id="r16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anwar</surname><given-names>S. M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Medical image analysis using convolutional neural networks: a review</article-title>,&#x0201d; <source>J. Med. Syst.</source>
<volume>42</volume>(<issue>11</issue>), <fpage>226</fpage> (<year>2018</year>).<pub-id pub-id-type="coden">JMSYDA</pub-id><issn>0148-5598</issn><pub-id pub-id-type="doi">10.1007/s10916-018-1088-1</pub-id><pub-id pub-id-type="pmid">30298337</pub-id>
</mixed-citation></ref><ref id="r17"><label>17.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>An effective encoder&#x02013;decoder network for neural cell bodies and cell nucleus segmentation of EM images</article-title>,&#x0201d; in <conf-name>IEEE Eng. Med. Biol. Soc. Annu. Int. Conf. (EMBC)</conf-name>, <publisher-name>IEEE</publisher-name>, pp.&#x000a0;<fpage>6302</fpage>&#x02013;<lpage>6305</lpage> (<year>2019</year>).<pub-id pub-id-type="doi">10.1109/EMBC.2019.8857887</pub-id></mixed-citation></ref><ref id="r18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title>,&#x0201d; <source>Nat. Methods</source>
<volume>18</volume>(<issue>1</issue>), <fpage>100</fpage>&#x02013;<lpage>106</lpage> (<year>2021</year>).<issn>1548-7091</issn><pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id><pub-id pub-id-type="pmid">33318659</pub-id>
</mixed-citation></ref><ref id="r19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Q.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Deep learning based neuronal soma detection and counting for Alzheimer&#x02019;s disease analysis</article-title>,&#x0201d; <source>Comput. Meth. Prog. Bio.</source>
<volume>203</volume>, <fpage>106023</fpage> (<year>2021</year>).<pub-id pub-id-type="coden">CMPBEK</pub-id><issn>0169-2607</issn><pub-id pub-id-type="doi">10.1016/j.cmpb.2021.106023</pub-id></mixed-citation></ref><ref id="r20"><label>20.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dong</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>3D CNN-based soma segmentation from brain images at single-neuron resolution</article-title>,&#x0201d; in <conf-name>IEEE Int. Conf. Image Process. (ICIP)</conf-name>, <publisher-name>IEEE</publisher-name>, pp.&#x000a0;<fpage>126</fpage>&#x02013;<lpage>130</lpage> (<year>2018</year>).<pub-id pub-id-type="doi">10.1109/ICIP.2018.8451389</pub-id></mixed-citation></ref><ref id="r21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Accurate neuronal soma segmentation using 3D multi-task learning U-shaped fully convolutional neural networks</article-title>,&#x0201d; <source>Front. Neuroanat.</source>
<volume>14</volume>, <fpage>592806</fpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.3389/fnana.2020.592806</pub-id><pub-id pub-id-type="pmid">33551758</pub-id>
</mixed-citation></ref><ref id="r22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>X.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>3D soma detection in large-scale whole brain images via a two-stage neural network</article-title>,&#x0201d; <source>IEEE T. Med. Imaging</source>
<volume>42</volume>(<issue>1</issue>), <fpage>148</fpage>&#x02013;<lpage>157</lpage> (<year>2023</year>).<pub-id pub-id-type="coden">ITMID4</pub-id><issn>0278-0062</issn><pub-id pub-id-type="doi">10.1109/TMI.2022.3206605</pub-id></mixed-citation></ref><ref id="r23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>B. J.</given-names></name></person-group>, &#x0201c;<article-title>A transform module to enhance lightweight attention by expanding receptive field</article-title>,&#x0201d; <source>Expert Syst. Appl.</source>
<volume>248</volume>, <fpage>123359</fpage> (<year>2024</year>).<pub-id pub-id-type="doi">10.1016/j.eswa.2024.123359</pub-id></mixed-citation></ref><ref id="r24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Z.</given-names></name><name><surname>Ozay</surname><given-names>M.</given-names></name><name><surname>Okatani</surname><given-names>T.</given-names></name></person-group>, &#x0201c;<article-title>Design of kernels in convolutional neural networks for image classification</article-title>,&#x0201d; <source>Lect. Notes Comput. Sci.</source>
<volume>9911</volume>, <fpage>51</fpage>&#x02013;<lpage>66</lpage> (<year>2016</year>).<pub-id pub-id-type="coden">LNCSD9</pub-id><issn>0302-9743</issn><pub-id pub-id-type="doi">10.1007/978-3-319-46478-7_4</pub-id></mixed-citation></ref><ref id="r25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuo</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Vision transformers for dense prediction: a survey</article-title>,&#x0201d; <source>Knowl.-Based Syst.</source>
<volume>253</volume>, <fpage>109552</fpage> (<year>2022</year>).<pub-id pub-id-type="coden">KNSYET</pub-id><issn>0950-7051</issn><pub-id pub-id-type="doi">10.1016/j.knosys.2022.109552</pub-id></mixed-citation></ref><ref id="r26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafner</surname><given-names>G.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Mapping brain-wide afferent inputs of parvalbumin-expressing GABAergic neurons in barrel cortex reveals local and long-range circuit motifs</article-title>,&#x0201d; <source>Cell Rep.</source>
<volume>28</volume>(<issue>13</issue>), <fpage>3450</fpage>&#x02013;<lpage>3461.e3458</lpage> (<year>2019</year>).<pub-id pub-id-type="doi">10.1016/j.celrep.2019.08.064</pub-id><pub-id pub-id-type="pmid">31553913</pub-id>
</mixed-citation></ref><ref id="r27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>A whole-brain monosynaptic input connectome to neuron classes in mouse visual cortex</article-title>,&#x0201d; <source>Nat. Neurosci.</source>
<volume>26</volume>(<issue>2</issue>), <fpage>350</fpage>&#x02013;<lpage>364</lpage> (<year>2023</year>).<pub-id pub-id-type="coden">NANEFN</pub-id><issn>1097-6256</issn><pub-id pub-id-type="doi">10.1038/s41593-022-01219-x</pub-id><pub-id pub-id-type="pmid">36550293</pub-id>
</mixed-citation></ref><ref id="r28"><label>28.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Zhou</surname><given-names>C.</given-names></name></person-group>, &#x0201c;<article-title>Resin/HM20 embedding and FMOST imaging protocol</article-title>,&#x0201d; protocols.io (<year>2018</year>).</mixed-citation></ref><ref id="r29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>TDAT: an efficient platform for processing petabyte-scale whole-brain volumetric images</article-title>,&#x0201d; <source>Front. Neural Circuits</source>
<volume>11</volume>, <fpage>51</fpage> (<year>2017</year>).<pub-id pub-id-type="doi">10.3389/fncir.2017.00051</pub-id><pub-id pub-id-type="pmid">28824382</pub-id>
</mixed-citation></ref><ref id="r30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>The Allen Mouse Brain Common Coordinate Framework: a 3D reference atlas</article-title>,&#x0201d; <source>Cell</source>
<volume>181</volume>(<issue>4</issue>), <fpage>936</fpage>&#x02013;<lpage>953.e20</lpage> (<year>2020</year>).<pub-id pub-id-type="coden">CELLB5</pub-id><issn>0092-8674</issn><pub-id pub-id-type="doi">10.1016/j.cell.2020.04.007</pub-id><pub-id pub-id-type="pmid">32386544</pub-id>
</mixed-citation></ref><ref id="r31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rainio</surname><given-names>O.</given-names></name><name><surname>Teuho</surname><given-names>J.</given-names></name><name><surname>Kl&#x000e9;n</surname><given-names>R.</given-names></name></person-group>, &#x0201c;<article-title>Evaluation metrics and statistical tests for machine learning</article-title>,&#x0201d; <source>Sci. Rep.</source>
<volume>14</volume>(<issue>1</issue>), <fpage>6086</fpage> (<year>2024</year>).<pub-id pub-id-type="coden">SRCEC3</pub-id><issn>2045-2322</issn><pub-id pub-id-type="doi">10.1038/s41598-024-56706-x</pub-id><pub-id pub-id-type="pmid">38480847</pub-id>
</mixed-citation></ref><ref id="r32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taha</surname><given-names>A. A.</given-names></name><name><surname>Hanbury</surname><given-names>A.</given-names></name></person-group>, &#x0201c;<article-title>Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</article-title>,&#x0201d; <source>BMC Med. Imaging</source>
<volume>15</volume>, <fpage>29</fpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1186/s12880-015-0068-x</pub-id><pub-id pub-id-type="pmid">26263899</pub-id>
</mixed-citation></ref><ref id="r33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Z.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Neuroseg-II: a deep learning approach for generalized neuron segmentation in two-photon ca(2+) imaging</article-title>,&#x0201d; <source>Front. Cell. Neurosci.</source>
<volume>17</volume>, <fpage>1127847</fpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.3389/fncel.2023.1127847</pub-id><pub-id pub-id-type="pmid">37091918</pub-id>
</mixed-citation></ref><ref id="r34"><label>34.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Deep residual learning for image recognition</article-title>,&#x0201d; in <conf-name>Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</conf-name>, pp.&#x000a0;<fpage>770</fpage>&#x02013;<lpage>778</lpage> (<year>2016</year>).<pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></mixed-citation></ref><ref id="r35"><label>35.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>G.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Densely connected convolutional networks</article-title>,&#x0201d; in <conf-name>Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</conf-name>, pp.&#x000a0;<fpage>4700</fpage>&#x02013;<lpage>4708</lpage> (<year>2017</year>).<pub-id pub-id-type="doi">10.1109/CVPR.2017.243</pub-id></mixed-citation></ref><ref id="r36"><label>36.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Zisserman</surname><given-names>A. J.</given-names></name></person-group>, &#x0201c;<article-title>Very deep convolutional networks for large-scale image recognition</article-title>,&#x0201d; arXiv:1409.1556 (<year>2014</year>).</mixed-citation></ref><ref id="r37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badrinarayanan</surname><given-names>V.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>SegNet: a deep convolutional encoder-decoder architecture for image segmentation</article-title>,&#x0201d; <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>39</volume>(<issue>12</issue>), <fpage>2481</fpage>&#x02013;<lpage>2495</lpage> (<year>2017</year>).<pub-id pub-id-type="coden">ITPIDJ</pub-id><issn>0162-8828</issn><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id><pub-id pub-id-type="pmid">28060704</pub-id>
</mixed-citation></ref><ref id="r38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>&#x000c7;i&#x000e7;ek</surname><given-names>&#x000d6;.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>3D U-Net: learning dense volumetric segmentation from sparse annotation</article-title>,&#x0201d; <source>Lect. Notes Comput. Sci.</source>
<volume>9901</volume>, <fpage>424</fpage>&#x02013;<lpage>432</lpage> (<year>2016</year>).<pub-id pub-id-type="coden">LNCSD9</pub-id><issn>0302-9743</issn><pub-id pub-id-type="doi">10.1007/978-3-319-46723-8_49</pub-id></mixed-citation></ref><ref id="r39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diakogiannis</surname><given-names>F. I.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>ResUNet-a: a deep learning framework for semantic segmentation of remotely sensed data</article-title>,&#x0201d; <source>ISPRS J. Photogramm.</source>
<volume>162</volume>, <fpage>94</fpage>&#x02013;<lpage>114</lpage> (<year>2020</year>).<pub-id pub-id-type="coden">IRSEE9</pub-id><issn>0924-2716</issn><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2020.01.013</pub-id></mixed-citation></ref><ref id="r40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wenxuan</surname><given-names>W.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>TransBTS: multimodal brain tumor segmentation using transformer</article-title>,&#x0201d; <source>Lect. Notes Comput. Sci.</source>
<volume>12901</volume>, <fpage>109</fpage>&#x02013;<lpage>119</lpage> (<year>2021</year>).<pub-id pub-id-type="coden">LNCSD9</pub-id><issn>0302-9743</issn><pub-id pub-id-type="doi">10.1007/978-3-030-87193-2_11</pub-id></mixed-citation></ref><ref id="r41"><label>41.</label><mixed-citation publication-type="webpage"><ext-link xlink:href="https://download.brainimagelibrary.org/biccn/zeng/luo/fMOST/" ext-link-type="uri">https://download.brainimagelibrary.org/biccn/zeng/luo/fMOST/</ext-link>.</mixed-citation></ref><ref id="r42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Brain-wide projection reconstruction of single functionally defined neurons</article-title>,&#x0201d; <source>Nat. Commun.</source>
<volume>13</volume>(<issue>1</issue>), <fpage>1531</fpage> (<year>2022</year>).<pub-id pub-id-type="coden">NCAOBW</pub-id><issn>2041-1723</issn><pub-id pub-id-type="doi">10.1038/s41467-022-29229-0</pub-id><pub-id pub-id-type="pmid">35318336</pub-id>
</mixed-citation></ref><ref id="r43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Whole-brain spatial organization of hippocampal single-neuron projectomes</article-title>,&#x0201d; <source>Science</source>
<volume>383</volume>(<issue>6682</issue>), <fpage>eadj9198</fpage> (<year>2024</year>).<pub-id pub-id-type="coden">SCIEAS</pub-id><issn>0036-8075</issn><pub-id pub-id-type="doi">10.1126/science.adj9198</pub-id><pub-id pub-id-type="pmid">38300992</pub-id>
</mixed-citation></ref><ref id="r44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>L.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Organizational principles of amygdalar input-output neuronal circuits</article-title>,&#x0201d; <source>Mol. Psychiatry</source>
<volume>26</volume>(<issue>12</issue>), <fpage>7118</fpage>&#x02013;<lpage>7129</lpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1038/s41380-021-01262-3</pub-id><pub-id pub-id-type="pmid">34400771</pub-id>
</mixed-citation></ref><ref id="r45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Whole-brain mapping of inputs and outputs of specific orbitofrontal cortical neurons in mice</article-title>,&#x0201d; <source>Neurosci. Bull.</source>
<volume>40</volume>(<issue>11</issue>), <fpage>1681</fpage>&#x02013;<lpage>1698</lpage> (<year>2024</year>).<pub-id pub-id-type="coden">NRPBA2</pub-id><issn>0028-3967</issn><pub-id pub-id-type="doi">10.1007/s12264-024-01229-8</pub-id><pub-id pub-id-type="pmid">38801564</pub-id>
</mixed-citation></ref><ref id="r46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname><given-names>Q.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>High-definition imaging using line-illumination modulation microscopy</article-title>,&#x0201d; <source>Nat. Methods</source>
<volume>18</volume>(<issue>3</issue>), <fpage>309</fpage>&#x02013;<lpage>315</lpage> (<year>2021</year>).<issn>1548-7091</issn><pub-id pub-id-type="doi">10.1038/s41592-021-01074-x</pub-id><pub-id pub-id-type="pmid">33649587</pub-id>
</mixed-citation></ref><ref id="r47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>C.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Automated and accurate detection of soma location and surface morphology in large-scale 3D neuron images</article-title>,&#x0201d; <source>PloS One</source>
<volume>8</volume>(<issue>4</issue>), <fpage>e62579</fpage> (<year>2013</year>).<pub-id pub-id-type="coden">POLNCL</pub-id><issn>1932-6203</issn><pub-id pub-id-type="doi">10.1371/journal.pone.0062579</pub-id><pub-id pub-id-type="pmid">23638117</pub-id>
</mixed-citation></ref><ref id="r48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>D.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Automated 3D soma segmentation with morphological surface evolution for neuron reconstruction</article-title>,&#x0201d; <source>Neuroinformatics</source>
<volume>16</volume>(<issue>2</issue>), <fpage>153</fpage>&#x02013;<lpage>166</lpage> (<year>2018</year>).<issn>1539-2791</issn><pub-id pub-id-type="doi">10.1007/s12021-017-9353-x</pub-id><pub-id pub-id-type="pmid">29344781</pub-id>
</mixed-citation></ref><ref id="r49"><label>49.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Video Swin Transformer</article-title>,&#x0201d; in <conf-name>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.</conf-name>, pp.&#x000a0;<fpage>3202</fpage>&#x02013;<lpage>3211</lpage> (<year>2022</year>).<pub-id pub-id-type="doi">10.1109/CVPR52688.2022.00320</pub-id></mixed-citation></ref><ref id="r50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>L.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>STEDNet: Swin Transformer-based encoder-decoder network for noise reduction in low-dose CT</article-title>,&#x0201d; <source>Med. Phys.</source>
<volume>50</volume>(<issue>7</issue>), <fpage>4443</fpage>&#x02013;<lpage>4458</lpage> (<year>2023</year>).<pub-id pub-id-type="coden">MPHYA6</pub-id><issn>0094-2405</issn><pub-id pub-id-type="doi">10.1002/mp.16249</pub-id><pub-id pub-id-type="pmid">36708286</pub-id>
</mixed-citation></ref><ref id="r51"><label>51.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Masood</surname><given-names>A.</given-names></name><name><surname>Naseem</surname><given-names>U.</given-names></name><name><surname>Kim</surname><given-names>J.</given-names></name></person-group>, &#x0201c;<article-title>Multi-level Swin Transformer enabled automatic segmentation and classification of breast metastases</article-title>,&#x0201d; in <conf-name>Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. (EMBC)</conf-name>, pp.&#x000a0;<fpage>1</fpage>&#x02013;<lpage>4</lpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.1109/EMBC40787.2023.10340831</pub-id></mixed-citation></ref><ref id="r52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazouani</surname><given-names>F. F.</given-names></name><name><surname>Vera</surname><given-names>P.</given-names></name><name><surname>Ruan</surname><given-names>S.</given-names></name></person-group>, &#x0201c;<article-title>Efficient brain tumor segmentation using Swin Transformer and enhanced local self-attention</article-title>,&#x0201d; <source>Int. J. Comput. Assist. Radiol. Surg.</source>
<volume>19</volume>(<issue>2</issue>), <fpage>273</fpage>&#x02013;<lpage>281</lpage> (<year>2024</year>).<pub-id pub-id-type="doi">10.1007/s11548-023-03024-8</pub-id><pub-id pub-id-type="pmid">37796413</pub-id>
</mixed-citation></ref><ref id="r53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Islam</surname><given-names>M. S.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>A deep learning approach for neuronal cell body segmentation in neurons expressing GCaMP using a Swin Transformer</article-title>,&#x0201d; <source>eNeuro</source>
<volume>10</volume>(<issue>9</issue>), <fpage>ENEURO.0148-23.2023</fpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.1523/ENEURO.0148-23.2023</pub-id></mixed-citation></ref><ref id="r54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>SwinBTS: a method for 3D multimodal brain tumor segmentation using Swin Transformer</article-title>,&#x0201d; <source>Brain Sci.</source>
<volume>12</volume>(<issue>6</issue>), <fpage>797</fpage> (<year>2022</year>).<pub-id pub-id-type="doi">10.3390/brainsci12060797</pub-id><pub-id pub-id-type="pmid">35741682</pub-id>
</mixed-citation></ref><ref id="r55"><label>55.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>F.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Rethinking skip connection with layer normalization in transformers and ResNets</article-title>,&#x0201d; arXiv:2105.07205 (<year>2021</year>).</mixed-citation></ref><ref id="r56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tudi</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Subregion preference in the long-range connectome of pyramidal neurons in the medial prefrontal cortex</article-title>,&#x0201d; <source>BMC Biol.</source>
<volume>22</volume>(<issue>1</issue>), <fpage>95</fpage> (<year>2024</year>).<issn>1741-7007</issn><pub-id pub-id-type="doi">10.1186/s12915-024-01880-7</pub-id><pub-id pub-id-type="pmid">38679719</pub-id>
</mixed-citation></ref><ref id="r57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Bi-channel image registration and deep-learning segmentation (BIRDS) for efficient, versatile 3D mapping of mouse brain</article-title>,&#x0201d; <source>eLife</source>
<volume>10</volume>, <fpage>e63455</fpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.7554/eLife.63455</pub-id><pub-id pub-id-type="pmid">33459255</pub-id>
</mixed-citation></ref><ref id="r58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>D-LMBmap: a fully automated deep-learning pipeline for whole-brain profiling of neural circuitry</article-title>,&#x0201d; <source>Nat. Methods</source>
<volume>20</volume>(<issue>10</issue>), <fpage>1593</fpage>&#x02013;<lpage>1604</lpage> (<year>2023</year>).<issn>1548-7091</issn><pub-id pub-id-type="doi">10.1038/s41592-023-01998-6</pub-id><pub-id pub-id-type="pmid">37770711</pub-id>
</mixed-citation></ref></ref-list></back></article>