<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006405</article-id><article-id pub-id-type="pmc">PMC11859647</article-id><article-id pub-id-type="doi">10.3390/s25041176</article-id><article-id pub-id-type="publisher-id">sensors-25-01176</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Autonomous Mission Planning for Fixed-Wing Unmanned Aerial Vehicles in Multiscenario Reconnaissance</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-4774-5635</contrib-id><name><surname>Chen</surname><given-names>Bei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01176" ref-type="aff">1</xref><xref rid="af2-sensors-25-01176" ref-type="aff">2</xref><xref rid="af3-sensors-25-01176" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2735-0882</contrib-id><name><surname>Yan</surname><given-names>Jiaxin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01176" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3687-7133</contrib-id><name><surname>Zhou</surname><given-names>Zebo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-01176" ref-type="aff">1</xref><xref rid="af3-sensors-25-01176" ref-type="aff">3</xref><xref rid="c1-sensors-25-01176" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Lai</surname><given-names>Rui</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af2-sensors-25-01176" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-4056-9662</contrib-id><name><surname>Lin</surname><given-names>Jiejian</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01176" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Mar&#x000ed;n</surname><given-names>Sergio Toral</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Hu</surname><given-names>Gaoge</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Gao</surname><given-names>Bingbing</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01176"><label>1</label>School of Aeronautics and Astronautics, University of Electronic Science and Technology of China, Chengdu 611731, China; <email>202112100618@std.uestc.edu.cn</email> (B.C.); <email>202411100611@std.uestc.edu.cn</email> (J.Y.); <email>o1n1n7@gmail.com</email> (J.L.)</aff><aff id="af2-sensors-25-01176"><label>2</label>AVIC (Chengdu) Unmanned Aerial Vehicle Systems Co., Ltd., Chengdu 611743, China; <email>lairuihit@163.com</email></aff><aff id="af3-sensors-25-01176"><label>3</label>System of Systems and Artificial Intelligence Laboratory, Chendu 610041, China</aff><author-notes><corresp id="c1-sensors-25-01176"><label>*</label>Correspondence: <email>klinsmann.zhou@gmail.com</email></corresp></author-notes><pub-date pub-type="epub"><day>14</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1176</elocation-id><history><date date-type="received"><day>28</day><month>11</month><year>2024</year></date><date date-type="rev-recd"><day>20</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>10</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Before a fixed-wing UAV executes target tracking missions, it is essential to identify targets through reconnaissance mission areas using onboard payloads. This paper presents an autonomous mission planning method designed for such reconnaissance operations, enabling effective target identification prior to tracking. Existing planning methods primarily focus on flight performance, energy consumption, and obstacle avoidance, with less attention to integrating payload. Our proposed method emphasizes the combination of two key functions: flight path planning and payload mission planning. In terms of path planning, we introduce a method based on the Hierarchical Traveling Salesman Problem (HTSP), which utilizes the nearest neighbor algorithm to find the optimal visit sequence and entry points for area targets. When dealing with area targets containing no-fly zones, HTSP quickly calculates a set of waypoints required for coverage path planning (CPP) based on the Generalized Traveling Salesman Problem (GTSP), ensuring thorough and effective reconnaissance coverage. In terms of payload mission planning, our proposed method fully considers payload characteristics such as scan resolution, imaging width, and operating modes to generate predefined mission instruction sets. By meticulously analyzing payload constraints, we further optimized the path planning results, ensuring that each instruction meets the payload performance requirements. Finally, simulations validated the effectiveness and superiority of the proposed autonomous mission planning method in reconnaissance tasks.</p></abstract><kwd-group><kwd>fixed-wing UAV</kwd><kwd>mission planning</kwd><kwd>coverage path planning</kwd><kwd>payload characteristics</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>42474017</award-id><award-id>42074038</award-id></award-group><award-group><funding-source>System of Systems and Artificial Intelligence Laboratory</funding-source></award-group><funding-statement>This work was supported by the National Natural Science Foundation of China under Grant No. (42474017; 42074038) and, in part, by the System of Systems and Artificial Intelligence Laboratory pioneer fund grant.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01176"><title>1. Introduction</title><p>In recent years, fixed-wing UAVs have played an increasingly important role in various reconnaissance mission scenarios. Their unique flight performance and long endurance capabilities have demonstrated significant application potential in both military reconnaissance and civilian monitoring fields [<xref rid="B1-sensors-25-01176" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01176" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01176" ref-type="bibr">3</xref>]. One crucial application is target tracking [<xref rid="B4-sensors-25-01176" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01176" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01176" ref-type="bibr">6</xref>]. Before a fixed-wing UAV executes target tracking missions, it is crucial to first identify targets within reconnaissance mission areas using onboard payloads. This paper presents an autonomous mission planning method designed for these reconnaissance operations, enabling effective target identification prior to initiating tracking tasks. We believe that UAVs completing reconnaissance missions must address two main aspects: path planning and payload planning. However, traditional planning methods are often limited to single-objective optimization, such as focusing on flight performance, obstacle avoidance, and energy consumption [<xref rid="B7-sensors-25-01176" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01176" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01176" ref-type="bibr">9</xref>], while neglecting the deep integration of payload characteristics.</p><p>Currently, UAV path planning works are primarily focused on applications with single scenarios, lacking consideration of different scenarios. Cui [<xref rid="B10-sensors-25-01176" ref-type="bibr">10</xref>] formulated the problem of multiple single-point reconnaissance targets as a Dynamic Constrained Traveling Salesman Problem with Neighborhoods (DCTSPN) and proposed a hierarchical algorithm based on deep reinforcement learning to solve it. Although the effectiveness of this algorithm was validated, it is well known that learning-based methods are not interpretable. Zhang et al. [<xref rid="B11-sensors-25-01176" ref-type="bibr">11</xref>] proposed a bi-level hybridization-based metaheuristic algorithm for fixed-wing UAVs used in forest fire monitoring. While this algorithm can reduce data collection cycles and energy consumption in most cases, it is limited to single-point targets, restricting its applicability to other scenarios. At the same time, the coverage path planning (CPP) problem has garnered increasing attention [<xref rid="B12-sensors-25-01176" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01176" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01176" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01176" ref-type="bibr">15</xref>], especially in missions requiring UAVs to scan entire areas. The CPP problem involves planning a path that covers all Regions of Interest (ROI) while avoiding obstacles or no-fly zones [<xref rid="B16-sensors-25-01176" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01176" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01176" ref-type="bibr">18</xref>]. Different CPP methods have their own advantages and limitations. Selecting an appropriate CPP method requires the consideration of specific application requirements and environments [<xref rid="B19-sensors-25-01176" ref-type="bibr">19</xref>]. Barrientos et al. [<xref rid="B20-sensors-25-01176" ref-type="bibr">20</xref>] proposed a Breadth First Search (BFS)-based coverage planner, which can be applied to grid-based workspaces to generate paths with minimal turning maneuvers. However, search-based methods have high computational complexity, particularly in large-scale environments where they require substantial computational resources, making them unsuitable for long-endurance fixed-wing UAVs. In [<xref rid="B21-sensors-25-01176" ref-type="bibr">21</xref>], a random walk-based CPP algorithm for robots was introduced. This method is simple and has low memory requirements, making it easy to deploy. However, random walk paths are only suitable for smaller environments and struggle to cover ROI with obstacles. Additionally, the robot may traverse the same path multiple times, leading to overall inefficiency in the coverage path. B&#x000e4;hnemann et al. [<xref rid="B22-sensors-25-01176" ref-type="bibr">22</xref>] proposed a decomposition-based CPP algorithm for quadrotor, which extends the Boustrophedon Cell Decompositon (BCD) [<xref rid="B23-sensors-25-01176" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-01176" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-01176" ref-type="bibr">25</xref>] by optimizing different sweep combinations to find the optimal sweep path and accounting for obstacles between the decomposed cells. This method is computationally efficient, easy to implement, and effectively handles complex areas.</p><p>Although the above works have achieved significant results in reconnaissance path planning for single-point or ROI targets, few works have been able to combine these two typical scenarios. Additionally, these studies have not adequately considered the role of payloads in the planning process. We need to clearly recognize that path planning without effective payload used for reconnaissance integration cannot ensure the successful execution of flight missions [<xref rid="B26-sensors-25-01176" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01176" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-01176" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-01176" ref-type="bibr">29</xref>]. We introduce a reconnaissance fixed-wing UAV, typically operating at altitudes above 5000m, equipped with Synthetic Aperture Radar (SAR) systems for aerial reconnaissance [<xref rid="B30-sensors-25-01176" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-01176" ref-type="bibr">31</xref>]. SAR systems have two operating modes: beam forming [<xref rid="B32-sensors-25-01176" ref-type="bibr">32</xref>] and strip modes [<xref rid="B33-sensors-25-01176" ref-type="bibr">33</xref>]. One of the most challenging aspects of mission planning is adjusting the UAV&#x02019;s path based on the payload. This complexity arises from the need to combine the payload&#x02019;s system response time, imaging width, and others with the existing flight plan. Such integration is crucial for meeting mission requirements.</p><p>Therefore, this paper takes this as a motivation to explore how to achieve autonomous mission planning for fixed-wing UAVs in multiscenario reconnaissance. The innovations of this paper include the following: Firstly, in <xref rid="sec2-sensors-25-01176" ref-type="sec">Section 2</xref>, we designed a path planning algorithm featuring the HTSP. This algorithm initially solves the TSP problem using the nearest neighbor algorithm to determine the reconnaissance sequence for different mission targets. Additionally, by incorporating the BCD technique, it addresses the coverage problem for area targets. We transformed the GTSP problem into a graph structure, ensuring the accuracy of the planning results. Fixed-wing UAVs need to switch flexibly between different mission scenarios. In <xref rid="sec3-sensors-25-01176" ref-type="sec">Section 3</xref>, the proposed payload planning module considers the characteristics, workflow, and instruction parameters of the payload. It adjusts the pre-planned waypoint set according to the payload&#x02019;s operating mode. Furthermore, to ensure the safety of UAV transitions between distinct mission scenarios, a visibility graph obstacle avoidance algorithm was integrated to refine the waypoint sets. Finally, in <xref rid="sec4-sensors-25-01176" ref-type="sec">Section 4</xref>, we developed a user-friendly UI interface that supports visualization and adjustment, and validated the effectiveness of the autonomous mission planning module in multiscenario situtations.</p></sec><sec id="sec2-sensors-25-01176"><title>2. HTSP Path Planning</title><p>In multiscenario situations, UAVs must visit multiple point targets, which are specific locations, and area targets, which are ROI with defined boundaries. This problem can be modeled as an HTSP. The main challenge of HTSP is to determine the sequence of visiting each target to ensure the shortest flight path, as well as to plan the coverage flight path within each area target.</p><p>To address the HTSP problem, we adopt a two-stage strategy:<list list-type="order"><list-item><p>Sequence Planning: This stage determines the order of visiting different scenarios. By optimizing the sequence, the UAV can minimize the overall flight range.</p></list-item><list-item><p>Coverage Path Planning: The lower level addresses the coverage of area targets using BCD. This ensures that the UAV efficiently covers each area while adhering to constraints such as no-fly zones.</p></list-item></list></p><sec id="sec2dot1-sensors-25-01176"><title>2.1. Sequence Planning</title><p>The inputs for the autonomous mission planning module include the entry point <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow></mml:math></inline-formula>, the exit point <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:math></inline-formula>, the set of no-fly zones or obstacle <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Obs</mml:mi></mml:mrow></mml:math></inline-formula>, the set of area target <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:math></inline-formula>, and the set of point targets <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:math></inline-formula>. <disp-formula id="FD1-sensors-25-01176"><label>(1)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">S</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">E</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">Obs</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">obs</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">obs</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">obs</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="bold">obs</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold">o</mml:mi><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">o</mml:mi><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold">o</mml:mi><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">o</mml:mi><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">F</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>w</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">m</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="bold">m</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>w</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula></p><p>In (1), <italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, and <italic toggle="yes">h</italic> represent the two-dimensional coordinates and altitude, respectively. <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">obs</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the no-fly zones constructed as polygons, with their vertices represented by <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">o</mml:mi><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the area targets, also constructed as polygons, with their vertices represented by <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">m</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> refers to the point targets. <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are integer variables indicating the payload operating mode and imaging width, respectively.</p><p>Sequence planning uses the nearest neighbor algorithm not only to determine the traversal order but also to further output the entry points for the coverage paths of polygonal area targets. Identifying the entry points for area targets helps minimize the overall path length. When covering polygons, choosing entry points judiciously can prevent redundant paths, thereby reducing the total flight range. The point set <italic toggle="yes">V</italic> is constructed as follows:<disp-formula id="FD2-sensors-25-01176"><label>(2)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:munderover><mml:mo>&#x022c3;</mml:mo><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfenced><mml:mo>&#x0222a;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:munderover><mml:mo>&#x022c3;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>l</mml:mi></mml:munderover><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#x0222a;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">E</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">m</italic> represents the number of area targets and <italic toggle="yes">l</italic> represents the number of point targets. To compute the Euclidean distance between any two points in the set <italic toggle="yes">V</italic>, we obtain the distance matrix <italic toggle="yes">D</italic>:<disp-formula id="FD3-sensors-25-01176"><label>(3)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the coordinates of points <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. Using the nearest neighbor algorithm, we solve for the path from the entry point <italic toggle="yes">S</italic> to the exit point <italic toggle="yes">E</italic> that passes through all points, denoted as <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. We initialize the current node as <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the starting point as <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. We initialize a boolean array <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to mark the visit status of all nodes, where all nodes are initially unvisited: <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>false</mml:mi><mml:mo>,</mml:mo><mml:mi>false</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>false</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We mark the starting node <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> as visited: <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mi>true</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). We initialize the sequence <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>seq</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and add the starting node to the sequence: <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>seq</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>We loop from the starting node to the second-to-last node, executing the following iterative steps: For each node <italic toggle="yes">i</italic> from 1 to <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, we select the node <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> closest to node <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> among all unvisited nodes, and thus we have:<disp-formula id="FD4-sensors-25-01176"><label>(4)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix">arg</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02209;</mml:mo><mml:mi>visited</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We add <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to the sequence:<disp-formula id="FD5-sensors-25-01176"><label>(5)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0222a;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We update the current node.<disp-formula id="FD6-sensors-25-01176"><label>(6)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We mark node <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> as visited.<disp-formula id="FD7-sensors-25-01176"><label>(7)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>[</mml:mo><mml:mi>next</mml:mi><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mi>true</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, we add the exit point <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to the sequence.<disp-formula id="FD8-sensors-25-01176"><label>(8)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0222a;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>From Equations (4)&#x02013;(8), we use the nearest neighbor algorithm to obtain the shortest sequence of point set <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>Finally, we determine the entry point of each polygon based on <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. We initialize the entry point array for each polygon <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD9-sensors-25-01176"><label>(9)</label><mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We initialize the polygon visit array <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD10-sensors-25-01176"><label>(10)</label><mml:math id="mm47" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mi>false</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We iterate over each element <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in the sequence <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. If <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> belongs to the <italic toggle="yes">i</italic>-th polygon and this polygon has not yet been visited, then we set <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to:<disp-formula id="FD11-sensors-25-01176"><label>(11)</label><mml:math id="mm52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We mark the <italic toggle="yes">i</italic>-th polygon as visited.<disp-formula id="FD12-sensors-25-01176"><label>(12)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Through the formalization shown above, we can comprehensively describe the nearest neighbor algorithm for solving the approximate TSP problem, ensuring that the shortest sequence of target visits and entry points is determined from the start <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow></mml:math></inline-formula> to the end <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec2dot2-sensors-25-01176"><title>2.2. Coverage Path Planning with Area Target</title><p>For UAVs equipped with SAR systems, continuous scanning is performed along the coverage path during a single pass over the area target. The data collected are processed by algorithms to generate high-resolution images. However, suboptimal flight paths can significantly degrade image quality. To ensure optimal scanning, the planning of a UAV&#x02019;s coverage path becomes even more critical.</p><p>To address the coverage path planning problem, the module employs the BCD algorithm to divide area targets with no-fly zones into multiple cells. This decomposition ensures that each cell is a simple polygon free of no-fly zones.</p><p>BCD is a method used to divide an AOI into smaller, simpler cells that can be covered systematically. The main goal is to ensure the complete coverage of the area while avoiding obstacles, ensuring efficient traversal and minimizing the total path length.</p><p>Let us consider an AOI <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represented as a polygon with vertices <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The AOI may contain no-fly zones <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Obs</mml:mi></mml:mrow></mml:math></inline-formula>, each represented as a polygonal region. The task is to cover the area <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> while avoiding these no-fly zones. The algorithm identifies critical points on the boundaries of <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Obs</mml:mi></mml:mrow></mml:math></inline-formula>, which are points where the sweep line direction changes. This could be the vertices of <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Obs</mml:mi></mml:mrow></mml:math></inline-formula>. A sweep line moves from left to right (or top to bottom), dividing the area into cells whenever it encounters a critical point. The cells are simple polygons, free from any no-fly zones.</p><p>Each cell <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is formed as a result of the sweep line passing through critical points. Mathematically, a cell <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is represented as:<disp-formula id="FD13-sensors-25-01176"><label>(13)</label><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02216;</mml:mo><mml:munderover><mml:mo>&#x022c3;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi mathvariant="bold">obs</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a simple polygon without holes, and <italic toggle="yes">m</italic> is the number of obstacles in the AOI.</p><p>Within each cell, a systematic back-and-forth (Boustrophedon) motion is used to cover the entire area. This motion can be described mathematically by generating parallel lines inside the cell, ensuring that they cover the cell fully without overlapping.</p><p>We choose a direction for the parallel lines. This is often chosen as perpendicular to the longest edge or the edge that aligns best with the general shape of the cell to minimize the number of turns. Let this direction be denoted by a unit vector <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>The coverage path within <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is generated as a set of parallel lines <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> that are spaced apart by the offset <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. These lines are perpendicular to the chosen direction <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:math></inline-formula>. Mathematically, <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> can be represented as:<disp-formula id="FD14-sensors-25-01176"><label>(14)</label><mml:math id="mm74" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x00394;</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mi mathvariant="bold">d</mml:mi><mml:mo>&#x022a5;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:math></inline-formula> is the position vector of a point on the line. <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the position vector of the starting point of the first line <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, typically taken as the intersection of the first line with the boundary of <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. These lines are separated by a distance called offset, denoted as <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">d</mml:mi><mml:mo>&#x022a5;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is a unit vector perpendicular to <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:math></inline-formula>. <italic toggle="yes">n</italic> is the number of parallel lines required to cover the entire cell.</p><p>For each line segment <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, we identify the start and end points:<disp-formula id="FD15-sensors-25-01176"><label>(15)</label><mml:math id="mm83" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close="}"><mml:msubsup><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Consequently, we obtain a set of all waypoints for the UAV to cover <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which can be represented as:<disp-formula id="FD16-sensors-25-01176"><label>(16)</label><mml:math id="mm85" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>k</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To minimize the total path length of the UAV, the coverage paths between adjacent cells are optimized. The objective is to find the optimal sequence for visiting cells, ensuring that each cell is visited exactly once. This problem is typically addressed using graph-based algorithms, and it is modeled as a GTSP.</p><p>The GTSP formulation [<xref rid="B34-sensors-25-01176" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-01176" ref-type="bibr">35</xref>] for UAV coverage path planning is defined as follows:<disp-formula id="FD17-sensors-25-01176"><label>(17)</label><mml:math id="mm86" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Min</mml:mi><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the travel distance between cell <italic toggle="yes">i</italic> and cell <italic toggle="yes">j</italic>. <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a binary decision variable indicating whether the UAV travels directly from cell <italic toggle="yes">i</italic> to cell <italic toggle="yes">j</italic>. <italic toggle="yes">N</italic> is the set of all cells.</p><p>The optimization is subject to the following constraints:</p><p>Constraints:<list list-type="order"><list-item><p>Binary decision variables:<disp-formula id="FD18-sensors-25-01176"><label>(18)</label><mml:math id="mm89" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mspace width="1.em"/><mml:mo>&#x02200;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>We set the start <italic toggle="yes">O</italic> to the cell where the entry point <italic toggle="yes">E</italic> obtained in <xref rid="sec2dot1-sensors-25-01176" ref-type="sec">Section 2.1</xref> is located.<disp-formula id="FD19-sensors-25-01176"><label>(19)</label><mml:math id="mm90" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><italic toggle="yes">V</italic> is the set of cells except <italic toggle="yes">O</italic>.</p></list-item><list-item><p>We ensure that each cell is entered and left at most once:<disp-formula id="FD20-sensors-25-01176"><label>(20)</label><mml:math id="mm91" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mn>1</mml:mn><mml:mspace width="1.em"/><mml:mo>&#x02200;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD21-sensors-25-01176"><label>(21)</label><mml:math id="mm92" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mn>1</mml:mn><mml:mspace width="1.em"/><mml:mo>&#x02200;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>Subtour elimination constraints (Miller&#x02013;Tucker&#x02013;Zemlin formulation):<disp-formula id="FD22-sensors-25-01176"><label>(22)</label><mml:math id="mm93" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mspace width="1.em"/><mml:mo>&#x02200;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD23-sensors-25-01176"><label>(23)</label><mml:math id="mm94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mspace width="1.em"/><mml:mo>&#x02200;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is an auxiliary variable which keeps the number of visited cells until cell <italic toggle="yes">i</italic> in the solution. These constraints ensure that the solution forms a single continuous path.</p></list-item></list></p><p>We solve the GTSP using GLKH (<uri xlink:href="http://webhotel4.ruc.dk/~keld/research/GLKH/">http://webhotel4.ruc.dk/~keld/research/GLKH/</uri>, accessed on 5 November 2024) as an open source solver [<xref rid="B36-sensors-25-01176" ref-type="bibr">36</xref>]. After solving the GTSP, the coverage waypoints <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of visiting the area target <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are obtained. From the sequence planning in the previous section, we obtain the access order of the target <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represent the sequence in which the targets are accessed, where each <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is either a point target or an area target. <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the waypoints associated with target <italic toggle="yes">T</italic>. For point target <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">m</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the waypoint set <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> may consist of a single waypoint. For area target <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the waypoint set <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> consists of multiple waypoints that cover the entire surface area. In summary, the pre-planned waypoints <italic toggle="yes">P</italic> of the HTSP are obtained by connecting the waypoints of each target <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in the sequence <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD24-sensors-25-01176"><label>(24)</label><mml:math id="mm108" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x022c3;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec3-sensors-25-01176"><title>3. Payload Mission Planning</title><p>In reconnaissance missions, the performance and efficiency of the UAV&#x02019;s payload are paramount. The payload mission planning process must therefore account for the specific operational characteristics and constraints. This section integrates payload characteristics to adjust the pre-planned waypoints <italic toggle="yes">P</italic>. Additionally, obstacle-avoidance waypoints during UAV cruising are generated based on a visibility graph method. The computational workflow for this section is designed to optimize the integration of payload characteristics and waypoint adjustments, as outlined in <xref rid="sensors-25-01176-f001" ref-type="fig">Figure 1</xref>.</p><p>Equation (1) is the input for the autonomous mission planning module. It will provide the required payload <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (e.g., SAR Beam Focusing or SAR Strip) and <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for each target. We will complete mission planning based on this information.</p><sec id="sec3dot1-sensors-25-01176"><title>3.1. Power Instruction</title><p>The first step in payload mission planning is managing the power state of the payload, which includes turning it on, standby, etc., depending on the mission&#x02019;s requirements. The goal here is to ensure that the payload is fully operational when the UAV reaches the target area, thereby avoiding any delays or inefficiencies in data acquisition.</p><p>Assuming that the payload&#x02019;s power-on time is <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> with the input <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in (1) and the UAV&#x02019;s speed is <italic toggle="yes">v</italic>, we calculate the distance <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> from the target <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> where the power-on command should be issued:<disp-formula id="FD25-sensors-25-01176"><label>(25)</label><mml:math id="mm115" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:mo>&#x02265;</mml:mo></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this formulation, <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the waypoint that needs to be inserted, and <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the position of <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which may be the point target position or the entry position of an area target. The updated set of waypoints <italic toggle="yes">P</italic> after inserting <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> can be expressed as:<disp-formula id="FD26-sensors-25-01176"><label>(26)</label><mml:math id="mm120" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Similarly, other kinds of commands for the payload can be inserted into the pre-planned waypoint set in the same manner, which will not be elaborated further here. By ensuring that the power command is issued at the correct waypoint, the UAV&#x02019;s payload will be in the appropriate operational state upon reaching the target.</p></sec><sec id="sec3dot2-sensors-25-01176"><title>3.2. Coverage Path Optimization with Payload</title><p>Different operating modes of the UAV&#x02019;s payload have different imaging widths based on factors like resolution and scan mode. By adjusting the offset of the coverage path to match the specific imaging width of the payload in its current mode, we ensure that each pass of the UAV maximizes its coverage area. This minimizes the number of required passes, thereby reducing mission time and energy consumption while ensuring complete coverage without gaps or excessive overlaps.</p><p>To adjust the offset of the full coverage path of <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we need to replace the offset <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in Equation (<xref rid="FD14-sensors-25-01176" ref-type="disp-formula">14</xref>) with the effective <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of the payload.</p><p>We adjust <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, so the updated formula for the coverage path lines <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> becomes:<disp-formula id="FD27-sensors-25-01176"><label>(27)</label><mml:math id="mm127" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold">d</mml:mi><mml:mo>&#x022a5;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Based on Equations (14)&#x02013;(24), we can obtain the adjusted coverage waypoint set <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The modified coverage waypoints are then inserted into the pre-planned waypoint set <italic toggle="yes">P</italic>, as shown in the following equation:<disp-formula id="FD28-sensors-25-01176"><label>(28)</label><mml:math id="mm130" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The payload mission planning process described in this section plays a crucial role in the overall mission planning for reconnaissance fixed-wing UAVs. By carefully managing the power and optimizing the coverage path based on the payload&#x02019;s characteristics, a UAV can achieve optimal reconnaissance performance, ensuring that all mission objectives are met efficiently and effectively.</p></sec><sec id="sec3dot3-sensors-25-01176"><title>3.3. Obstacle Avoidance Based on Visibility Graph</title><p>Regardless of whether the target is a point or an area, obstacles may still exist in the region between two targets. Therefore, an obstacle avoidance method based on visibility graphs is adopted. The key steps of the visibility graph algorithm are as shown in <xref rid="sensors-25-01176-f002" ref-type="fig">Figure 2</xref>:</p><p>Firstly, each obstacle is represented as a convex polygon. For non-convex obstacles, convex decomposition is performed to divide them into multiple convex components. Subsequently, the vertices of the obstacle polygons are expanded by a safety buffer distance <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>buffer</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to account for the UAV&#x02019;s size and maneuvering limitations:<disp-formula><mml:math id="mm132" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">obs</mml:mi><mml:mi>k</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">op</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>buffer</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">n</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mi mathvariant="bold">op</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi mathvariant="bold">obs</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">n</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the outward normal vector at vertex <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">op</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>Next, for the last waypoint <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the preceding target and the first waypoint <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the next target in the planned path, we examine whether the line segment <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> intersects with any obstacles. The intersection condition can be expressed as:<disp-formula><mml:math id="mm138" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02229;</mml:mo><mml:msubsup><mml:mi mathvariant="bold">obs</mml:mi><mml:mi>k</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02260;</mml:mo><mml:mo>&#x02205;</mml:mo><mml:mspace width="1.em"/><mml:mo>&#x02200;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The visibility graph <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is constructed where <italic toggle="yes">V</italic> contains all obstacle vertices, <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. <italic toggle="yes">E</italic> contains edges between vertices that are mutually visible.</p><p>Two vertices <inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are mutually visible if:<disp-formula id="FD29-sensors-25-01176"><label>(29)</label><mml:math id="mm144" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02229;</mml:mo><mml:msubsup><mml:mi mathvariant="bold">obs</mml:mi><mml:mi>k</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mo>&#x02205;</mml:mo><mml:mspace width="1.em"/><mml:mo>&#x02200;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, using Dijkstra&#x02019;s algorithm on the visibility graph, we find the shortest collision-free path between <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD30-sensors-25-01176"><label>(30)</label><mml:math id="mm147" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mo form="prefix">arg</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02225;</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, the obstacle avoidance waypoints are integrated into the final path:<disp-formula id="FD31-sensors-25-01176"><label>(31)</label><mml:math id="mm148" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This approach ensures safe navigation while maintaining optimal path efficiency. The visibility graph method is particularly effective in environments with multiple convex obstacles, providing guaranteed collision-free paths with minimal computational overhead.</p></sec></sec><sec id="sec4-sensors-25-01176"><title>4. Experiments and Discussions</title><p>The experimental platform for algorithm validation consisted of a high-performance computing workstation, assembled by Lenovo in Beijing, China. The workstation was equipped with an AMD Ryzen 9 7945HX CPU (16 cores, 2.5 GHz base frequency) and 16 GB DDR4 RAM. The computational environment was implemented in Visual Studio 2022 running on Windows 11. All algorithms were implemented using the Computational Geometry Algorithms Library [<xref rid="B37-sensors-25-01176" ref-type="bibr">37</xref>] (CGAL, <uri xlink:href="https://doc.cgal.org/latest/Manual/index.html">https://doc.cgal.org/latest/Manual/index.html</uri>, accessed on 5 November 2024) in C++, which provides robust implementations of geometric algorithms including polygon operations, convex hulls, and visibility graphs that are essential for our path planning and obstacle avoidance computations. The user interface was developed using the QT framework, providing an interactive visualization environment for mission planning and results analysis. All data were executed on this platform to ensure consistent and reliable performance metrics.</p><sec id="sec4dot1-sensors-25-01176"><title>4.1. UI Design</title><p>We developed a user-friendly interface for the autonomous mission planning module, as shown in <xref rid="sensors-25-01176-f003" ref-type="fig">Figure 3</xref>, organized into three functional areas from right to left. The right section features input controls for defining mission areas, targets, and no-fly zones, along with options to manage mission parameters such as speed and altitude, and to import/export planning data in XML format. The central section displays a dynamic map with latitude and longitude axes, visually representing the mission area, targets, no-fly zones, and the UAV&#x02019;s planned waypoints. The left section presents a detailed table of the mission&#x02019;s waypoints, including their sequence, coordinates, UAV arrival modes, and payload control instructions. This interface streamlines mission setup, visualization, and result analysis.</p></sec><sec id="sec4dot2-sensors-25-01176"><title>4.2. Autonomous Mission Planning</title><p>BCD stands out for its ability to ensure complete coverage of the area while simplifying the path planning problem. By dividing the workspace into simple, non-overlapping cells, BCD transforms the complex environment into a set of smaller polygons, each free of obstacles. This decomposition not only reduces computational complexity but also enhances the efficiency of path planning algorithms, ensuring that the UAV can systematically cover the entire area with minimal redundant traversal. <xref rid="sensors-25-01176-f004" ref-type="fig">Figure 4</xref> visually represents this process, highlighting how the region is decomposed into discrete cells around no-fly zones.</p><p>In the experimental results depicted in <xref rid="sensors-25-01176-f004" ref-type="fig">Figure 4</xref>a, the BCD method was applied to an area target defined by the vertices <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mn>800</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>800</mml:mn><mml:mo>,</mml:mo><mml:mn>800</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>800</mml:mn><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which encompasses two polygonal no-fly zones with vertices <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>148</mml:mn><mml:mo>,</mml:mo><mml:mn>275</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>418</mml:mn><mml:mo>,</mml:mo><mml:mn>275</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>621</mml:mn><mml:mo>,</mml:mo><mml:mn>148</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm156" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>148</mml:mn><mml:mo>,</mml:mo><mml:mn>418</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm157" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>270</mml:mn><mml:mo>,</mml:mo><mml:mn>82</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm158" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>523</mml:mn><mml:mo>,</mml:mo><mml:mn>82</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm159" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>523</mml:mn><mml:mo>,</mml:mo><mml:mn>224</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm160" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>270</mml:mn><mml:mo>,</mml:mo><mml:mn>224</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. These no-fly zones are represented as black polygons within the diagram. The BCD method successfully divided the complex area into seven distinct cells (marked from left to right), each free of no-fly zones. The decomposition process strategically navigated around the no-fly zones. This not only maintains the integrity of the no-fly zones but also maximizes the coverage of the area target by creating well-defined, accessible regions for the UAV to traverse.</p><p>Assuming the entry point of the area target in <xref rid="sensors-25-01176-f004" ref-type="fig">Figure 4</xref>b is the top left vertex, the optimal order for accessing these cells based on GTSP is as follows: <inline-formula><mml:math id="mm161" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>6</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>4</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>0</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>5</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Assuming <inline-formula><mml:math id="mm162" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the result of the CPP based on BCD is shown in <xref rid="sensors-25-01176-f004" ref-type="fig">Figure 4</xref>c. Through the above numerical experiments, we have validated its effectiveness.</p><p>In the validation phase of the autonomous mission planning module, let us assume a fixed-wing UAV speed of 150 km/h and a flight altitude of 5000 m. The reconnaissance payload requires 70 s from power-on to operational state. The payload has two modes: an SAR Beam Focusing mode and an SAR Strip mode. For the SAR Beam Focusing mode, there are three resolution levels corresponding to three imaging widths, as shown in <xref rid="sensors-25-01176-t001" ref-type="table">Table 1</xref>. Similarly, the characteristics of the SAR Strip mode are shown in <xref rid="sensors-25-01176-t002" ref-type="table">Table 2</xref>. To better demonstrate the obstacle avoidance capability of the module in a large-span mission area, we set the <inline-formula><mml:math id="mm163" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>buffer</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to 2000 m. The input target information includes the required reconnaissance payload mode and resolution for the mission. All position information is represented in latitude and longitude, and the coordinate transformation process during planning will not be elaborated here.</p><p>As shown in <xref rid="sensors-25-01176-f005" ref-type="fig">Figure 5</xref>, in the planning map, within a 300 km &#x000d7; 300 km mission area (blue polygon), we input five area targets (black polygons), three no-fly zones (red polygons), five point targets (gray points), the starting point (green point), and the endpoint (blue point).</p><p>The proposed module was experimentally validated using the SAR Strip mode.The experiment was divided into two different scenarios. <xref rid="sensors-25-01176-f006" ref-type="fig">Figure 6</xref> demonstrates the planning results. Firstly, sequence planning calculates the optimal access order, then mission planning optimizes the coverage path based on payload characteristics, and, finally, power instructions are also considered in the final mission instruction set. The following text will analyze these experimental results.</p><p>In the first area target scenario, the module successfully adjusted the UAV&#x02019;s path to accommodate the different resolution requirements of five area targets. For targets 1, 3, and 5, which required high-resolution imaging (0.5), the UAV followed a finer, closer-offset path to ensure detailed coverage. Target 2, requiring medium resolution (1.0), had a lower path density. For target 4, with the lowest resolution requirement (3.0), the module optimized the path for broader coverage with minimal overlap. In the second scenario, the experiment focused on processing point target scenes, where the resolution requirement for all five point targets was uniformly set to 0.5. This scenario was mainly used to verify the module&#x02019;s switching in various mission scenarios. <xref rid="sensors-25-01176-f007" ref-type="fig">Figure 7</xref>a illustrates the module output path. For the area target scenario, the module successfully adapts to varying resolution requirements, while the point target scenario demonstrates the module&#x02019;s effective switching between scenarios. <xref rid="sensors-25-01176-f007" ref-type="fig">Figure 7</xref>b illustrates an obstacle avoidance method based on visibility graphs. By expanding obstacle vertices according to the <inline-formula><mml:math id="mm164" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>buffer</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, a visibility graph is constructed with waypoint No. 73 (lon: &#x02212;59.5076; lat: &#x02212;34.7782) as the starting point and waypoint No. 75 (lon: &#x02212;58.8323; lat: &#x02212;37.0782) as the endpoint. Utilizing Dijkstra&#x02019;s algorithm, the shortest collision-free path is identified, successfully generating waypoint No. 74 (lon: &#x02212;59.3972; lat: &#x02212;36.1817) as the obstacle avoidance waypoint. The distance between this waypoint and the closest vertex of the rectangular obstacle (longitude: &#x02212;59.368118; latitude: &#x02212;36.192308) satisfies the requirement <inline-formula><mml:math id="mm165" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>buffer</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This method ensures safe and efficient navigation.</p><p>As shown in the mission instruction list on the left side of <xref rid="sensors-25-01176-f007" ref-type="fig">Figure 7</xref>c, the module issued instruction No. 25 for the reconnaissance. Notably, the module inserted instruction No. 24 to power on the payload in advance, ensuring that it was fully operational upon reaching the target. To validate the computational efficiency of the proposed algorithm, we conducted multiple groups of test experiments. Each dataset consisted of 10 targets and three obstacles distributed across a 300 km &#x000d7; 300 km mission area. As shown in <xref rid="sensors-25-01176-f008" ref-type="fig">Figure 8</xref>, the experimental results demonstrate that the proposed method consistently completed calculations within 0.5 s across all test scenarios. The computation time, consistently below 500 ms, ensures compliance with the stringent operational requirements of fixed-wing UAVs.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01176"><title>5. Conclusions</title><p>This paper presented a comprehensive approach to autonomous mission planning for fixed-wing UAVs, specifically tailored to multiscenario reconnaissance missions. By addressing the limitations of existing UAV planning methods, which often overlook the integration of payload characteristics, we developed an advanced autonomous mission planning module that combines flight path planning and payload planning.</p><p>The proposed path planning strategy, grounded in the HTSP, effectively determines the optimal sequence for accessing reconnaissance targets and planning entry waypoints. The GTSP further enhances the coverage path planning, ensuring thorough reconnaissance even in complex environments with no-fly zones.</p><p>Our payload mission planning methodology meticulously considers key payload parameters such as scan resolution, imaging width, and modes. This ensures that each mission instruction is optimized to meet the payload&#x02019;s performance requirements, thereby maximizing reconnaissance effectiveness.</p><p>Experiments demonstrated the practical applicability of the module, allowing for intuitive verification and validation of its effectiveness. Overall, the results affirm the superiority of our integrated mission planning module, providing a robust solution for efficient and effective UAV reconnaissance in diverse scenarios. Future work could extend this framework to accommodate additional mission constraints and enhance real-time adaptability in dynamic environments.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, B.C., J.Y. and Z.Z.; methodology, B.C. and J.Y.; software, J.Y., J.L. and R.L.; writing&#x02014;review and editing, B.C. and J.Y.; visualization, J.L.; supervision, Z.Z.; project administration, R.L. and Z.Z.; funding acquisition, Z.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Authors Bei Chen and Rui Lai were employed by the company AVIC (Chengdu) Unmanned Aerial Vehicle Systems Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01176"><label>1.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sonkar</surname><given-names>S.</given-names></name>
<name><surname>Kumar</surname><given-names>P.</given-names></name>
<name><surname>Philip</surname><given-names>D.</given-names></name>
<name><surname>Ghosh</surname><given-names>A.K.</given-names></name>
</person-group><article-title>Low-cost smart surveillance and reconnaissance using VTOL fixed wing UAV</article-title><source>Proceedings of the 2020 IEEE Aerospace conference</source><conf-loc>Big Sky, MT, USA</conf-loc><conf-date>7 March 2020</conf-date><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="B2-sensors-25-01176"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Huang</surname><given-names>H.</given-names></name>
</person-group><article-title>Occlusion-aware UAV path planning for reconnaissance and surveillance</article-title><source>Drones</source><year>2021</year><volume>5</volume><elocation-id>98</elocation-id><pub-id pub-id-type="doi">10.3390/drones5030098</pub-id></element-citation></ref><ref id="B3-sensors-25-01176"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ranasinghe</surname><given-names>N.D.</given-names></name>
<name><surname>Gunawardana</surname><given-names>W.A.D.L.</given-names></name>
</person-group><article-title>Development of gasoline-electric hybrid propulsion surveillance and reconnaissance VTOL UAV</article-title><source>Proceedings of the 2021 IEEE International Conference on Robotics, Automation and Artificial Intelligence</source><conf-loc>Hong Kong, China</conf-loc><conf-date>21 April 2021</conf-date><fpage>63</fpage><lpage>68</lpage></element-citation></ref><ref id="B4-sensors-25-01176"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liao</surname><given-names>S.L.</given-names></name>
<name><surname>Zhu</surname><given-names>R.M.</given-names></name>
<name><surname>Wu</surname><given-names>N.Q.</given-names></name>
<name><surname>Shaikh</surname><given-names>T.A.</given-names></name>
<name><surname>Sharaf</surname><given-names>M.</given-names></name>
<name><surname>Mostafa</surname><given-names>A.M.</given-names></name>
</person-group><article-title>Path planning for moving target tracking by fixed-wing UAV</article-title><source>Def. Technol.</source><year>2020</year><volume>16</volume><fpage>811</fpage><lpage>824</lpage><pub-id pub-id-type="doi">10.1016/j.dt.2019.10.010</pub-id></element-citation></ref><ref id="B5-sensors-25-01176"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>L.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Yu</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>G.</given-names></name>
<name><surname>Shen</surname><given-names>L.</given-names></name>
</person-group><article-title>Image-based visual servo tracking control of a ground moving target for a fixed-wing unmanned aerial vehicle</article-title><source>J. Intell. Robot. Syst.</source><year>2021</year><volume>102</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1007/s10846-021-01425-y</pub-id></element-citation></ref><ref id="B6-sensors-25-01176"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>Z.</given-names></name>
<name><surname>Garcia de Marina</surname><given-names>H.</given-names></name>
<name><surname>Anderson</surname><given-names>B.D.</given-names></name>
<name><surname>Yu</surname><given-names>C.</given-names></name>
</person-group><article-title>Collaborative target-tracking control using multiple fixed-wing unmanned aerial vehicles with constant speeds</article-title><source>J. Guid. Control. Dyn.</source><year>2021</year><volume>44</volume><fpage>238</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.2514/1.G005092</pub-id></element-citation></ref><ref id="B7-sensors-25-01176"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>Y.</given-names></name>
<name><surname>Leeghim</surname><given-names>H.</given-names></name>
<name><surname>Kim</surname><given-names>D.</given-names></name>
</person-group><article-title>Dubins Path-Oriented Rapidly Exploring Random Tree* for Three-Dimensional Path Planning of Unmanned Aerial Vehicles</article-title><source>Electronics</source><year>2022</year><volume>11</volume><elocation-id>2338</elocation-id><pub-id pub-id-type="doi">10.3390/electronics11152338</pub-id></element-citation></ref><ref id="B8-sensors-25-01176"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aiello</surname><given-names>G.</given-names></name>
<name><surname>Valavanis</surname><given-names>K.P.</given-names></name>
<name><surname>Rizzo</surname><given-names>A.</given-names></name>
</person-group><article-title>Fixed-wing uav energy efficient 3d path planning in cluttered environments</article-title><source>J. Intell. Robot. Syst.</source><year>2022</year><volume>105</volume><fpage>60</fpage><pub-id pub-id-type="doi">10.1007/s10846-022-01608-1</pub-id></element-citation></ref><ref id="B9-sensors-25-01176"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Machmudah</surname><given-names>A.</given-names></name>
<name><surname>Shanmugavel</surname><given-names>M.</given-names></name>
<name><surname>Parman</surname><given-names>S.</given-names></name>
<name><surname>Manan</surname><given-names>T.S.A.</given-names></name>
<name><surname>Dutykh</surname><given-names>D.</given-names></name>
<name><surname>Beddu</surname><given-names>S.</given-names></name>
<name><surname>Rajabi</surname><given-names>A.</given-names></name>
</person-group><article-title>Flight trajectories optimization of fixed-wing UAV by bank-turn mechanism</article-title><source>Drones</source><year>2022</year><volume>6</volume><elocation-id>69</elocation-id><pub-id pub-id-type="doi">10.3390/drones6030069</pub-id></element-citation></ref><ref id="B10-sensors-25-01176"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Cui</surname><given-names>Q.</given-names></name>
</person-group><article-title>Multi-target points path planning for fixed-wing unmanned aerial vehicle performing reconnaissance missions</article-title><source>Proceedings of the 5th International Conference on Information Science, Electrical, and Automation Engineering</source><conf-loc>Wuhan, China</conf-loc><conf-date>24 March 2023</conf-date><volume>Volume 12748</volume><fpage>713</fpage><lpage>723</lpage></element-citation></ref><ref id="B11-sensors-25-01176"><label>11.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Ding</surname><given-names>Z.</given-names></name>
<name><surname>Huang</surname><given-names>Y.</given-names></name>
<name><surname>Yuan</surname><given-names>H.</given-names></name>
<name><surname>Dong</surname><given-names>H.</given-names></name>
</person-group><article-title>Introduction to reinforcement learning</article-title><source>Deep Reinforcement Learning: Fundamentals, Research and Applications</source><person-group person-group-type="editor">
<name><surname>Dong</surname><given-names>H.</given-names></name>
<name><surname>Ding</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
</person-group><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2020</year><fpage>47</fpage><lpage>123</lpage></element-citation></ref><ref id="B12-sensors-25-01176"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Ling</surname><given-names>F.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>You</surname><given-names>T.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Du</surname><given-names>X.</given-names></name>
</person-group><article-title>Coverage path planning of heterogeneous unmanned aerial vehicles based on ant colony system</article-title><source>Swarm Evol. Comput.</source><year>2022</year><volume>69</volume><fpage>101005</fpage><pub-id pub-id-type="doi">10.1016/j.swevo.2021.101005</pub-id></element-citation></ref><ref id="B13-sensors-25-01176"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Theile</surname><given-names>M.</given-names></name>
<name><surname>Bayerlein</surname><given-names>H.</given-names></name>
<name><surname>Nai</surname><given-names>R.</given-names></name>
<name><surname>Gesbert</surname><given-names>D.</given-names></name>
<name><surname>Caccamo</surname><given-names>M.</given-names></name>
</person-group><article-title>UAV coverage path planning under varying power constraints using deep reinforcement learning</article-title><source>Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>24 October 2020</conf-date><fpage>1444</fpage><lpage>1449</lpage></element-citation></ref><ref id="B14-sensors-25-01176"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jia</surname><given-names>Y.</given-names></name>
<name><surname>Zhou</surname><given-names>S.</given-names></name>
<name><surname>Zeng</surname><given-names>Q.</given-names></name>
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Chen</surname><given-names>D.</given-names></name>
<name><surname>Zhang</surname><given-names>K.</given-names></name>
<name><surname>Liu</surname><given-names>L.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
</person-group><article-title>The UAV path coverage algorithm based on the greedy strategy and ant colony optimization</article-title><source>Electronics</source><year>2022</year><volume>17</volume><elocation-id>2667</elocation-id><pub-id pub-id-type="doi">10.3390/electronics11172667</pub-id></element-citation></ref><ref id="B15-sensors-25-01176"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Majeed</surname><given-names>A.</given-names></name>
<name><surname>Hwang</surname><given-names>S.O.</given-names></name>
</person-group><article-title>A multi-objective coverage path planning algorithm for UAVs to cover spatially distributed regions in urban environments</article-title><source>Aerospace</source><year>2021</year><volume>11</volume><elocation-id>343</elocation-id><pub-id pub-id-type="doi">10.3390/aerospace8110343</pub-id></element-citation></ref><ref id="B16-sensors-25-01176"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vazquez-Carmona</surname><given-names>E.V.</given-names></name>
<name><surname>Vasquez-Gomez</surname><given-names>J.I.</given-names></name>
<name><surname>Herrera-Lozada</surname><given-names>J.C.</given-names></name>
<name><surname>Antonio-Cruz</surname><given-names>M.</given-names></name>
</person-group><article-title>Coverage path planning for spraying drones</article-title><source>Comput. Ind. Eng.</source><year>2022</year><volume>168</volume><fpage>108125</fpage><pub-id pub-id-type="doi">10.1016/j.cie.2022.108125</pub-id><pub-id pub-id-type="pmid">35370350</pub-id>
</element-citation></ref><ref id="B17-sensors-25-01176"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Galceran</surname><given-names>E.</given-names></name>
<name><surname>Carreras</surname><given-names>M.</given-names></name>
</person-group><article-title>A survey on coverage path planning for robotics</article-title><source>Robot. Auton. Syst.</source><year>2013</year><volume>61</volume><fpage>1258</fpage><lpage>1276</lpage><pub-id pub-id-type="doi">10.1016/j.robot.2013.09.004</pub-id></element-citation></ref><ref id="B18-sensors-25-01176"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vasquez-Gomez</surname><given-names>J.I.</given-names></name>
<name><surname>Marciano-Melchor</surname><given-names>M.</given-names></name>
<name><surname>Valentin</surname><given-names>L.</given-names></name>
<name><surname>Herrera-Lozada</surname><given-names>J.C.</given-names></name>
</person-group><article-title>Coverage path planning for 2d convex regions</article-title><source>J. Intell. Robot. Syst.</source><year>2020</year><volume>97</volume><fpage>81</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1007/s10846-019-01024-y</pub-id></element-citation></ref><ref id="B19-sensors-25-01176"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>C.S.</given-names></name>
<name><surname>Mohd-Mokhtar</surname><given-names>R.</given-names></name>
<name><surname>Arshad</surname><given-names>M.R.</given-names></name>
</person-group><article-title>A comprehensive review of coverage path planning in robotics using classical and heuristic algorithms</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>119310</fpage><lpage>119342</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2021.3108177</pub-id></element-citation></ref><ref id="B20-sensors-25-01176"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Barrientos</surname><given-names>A.</given-names></name>
<name><surname>Colorado</surname><given-names>J.</given-names></name>
<name><surname>Cerro</surname><given-names>J.D.</given-names></name>
<name><surname>Martinez</surname><given-names>A.</given-names></name>
<name><surname>Rossi</surname><given-names>C.</given-names></name>
<name><surname>Sanz</surname><given-names>D.</given-names></name>
<name><surname>Valente</surname><given-names>J.</given-names></name>
</person-group><article-title>Aerial remote sensing in agriculture: A practical approach to area coverage and path planning for fleets of mini aerial robots</article-title><source>J. Field Robot.</source><year>2011</year><volume>28</volume><fpage>667</fpage><lpage>689</lpage><pub-id pub-id-type="doi">10.1002/rob.20403</pub-id></element-citation></ref><ref id="B21-sensors-25-01176"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hasan</surname><given-names>K.M.</given-names></name>
<name><surname>Reza</surname><given-names>K.J.</given-names></name>
</person-group><article-title>Path planning algorithm development for autonomous vacuum cleaner robots</article-title><source>Proceedings of the 2014 International Conference on Informatics, Electronics &#x00026; Vision</source><conf-loc>Dhaka, Bangladesh</conf-loc><conf-date>23 May 2014</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B22-sensors-25-01176"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>B&#x000e4;hnemann</surname><given-names>R.</given-names></name>
<name><surname>Lawrance</surname><given-names>N.</given-names></name>
<name><surname>Chung</surname><given-names>J.J.</given-names></name>
<name><surname>Pantic</surname><given-names>M.</given-names></name>
<name><surname>Siegwart</surname><given-names>R.</given-names></name>
<name><surname>Nieto</surname><given-names>J.</given-names></name>
</person-group><article-title>Revisiting boustrophedon coverage path planning as a generalized traveling salesman problem</article-title><source>Proceedings of the Field and Service Robotics: Results of the 12th International Conference</source><conf-loc>Tokyo, Japan</conf-loc><conf-date>29 August 2019</conf-date><fpage>277</fpage><lpage>290</lpage></element-citation></ref><ref id="B23-sensors-25-01176"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kumar</surname><given-names>K.</given-names></name>
<name><surname>Kumar</surname><given-names>N.</given-names></name>
</person-group><article-title>Region coverage-aware path planning for unmanned aerial vehicles: A systematic review</article-title><source>Phys. Commun.</source><year>2023</year><volume>59</volume><fpage>102073</fpage><pub-id pub-id-type="doi">10.1016/j.phycom.2023.102073</pub-id></element-citation></ref><ref id="B24-sensors-25-01176"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Choset</surname><given-names>H.</given-names></name>
<name><surname>Pignon</surname><given-names>P.</given-names></name>
</person-group><article-title>Coverage path planning: The boustrophedon cellular decomposition</article-title><source>Proceedings of the Field and Service Robotics</source><conf-loc>Canberra, Australia</conf-loc><conf-date>8&#x02013;10 December 1997</conf-date><fpage>203</fpage><lpage>209</lpage></element-citation></ref><ref id="B25-sensors-25-01176"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Choset</surname><given-names>H.</given-names></name>
</person-group><article-title>Coverage of known spaces: The boustrophedon cellular decomposition</article-title><source>Auton. Robot.</source><year>2000</year><volume>9</volume><fpage>247</fpage><lpage>253</lpage><pub-id pub-id-type="doi">10.1023/A:1008958800904</pub-id></element-citation></ref><ref id="B26-sensors-25-01176"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Apostolidis</surname><given-names>S.D.</given-names></name>
<name><surname>Kapoutsis</surname><given-names>P.C.</given-names></name>
<name><surname>Kapoutsis</surname><given-names>A.C.</given-names></name>
<name><surname>Kosmatopoulos</surname><given-names>E.B.</given-names></name>
</person-group><article-title>Cooperative multi-UAV coverage mission planning platform for remote sensing applications</article-title><source>Auton. Robot.</source><year>2022</year><volume>46</volume><fpage>373</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1007/s10514-021-10028-3</pub-id></element-citation></ref><ref id="B27-sensors-25-01176"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Stecz</surname><given-names>W.</given-names></name>
<name><surname>Gromada</surname><given-names>K.</given-names></name>
</person-group><article-title>UAV mission planning with SAR application</article-title><source>Sensors</source><year>2020</year><volume>4</volume><elocation-id>1080</elocation-id><pub-id pub-id-type="doi">10.3390/s20041080</pub-id></element-citation></ref><ref id="B28-sensors-25-01176"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Yan</surname><given-names>Y.</given-names></name>
<name><surname>Jiang</surname><given-names>F.</given-names></name>
<name><surname>Gao</surname><given-names>X.</given-names></name>
</person-group><article-title>Research on single target cognitive electronic reconnaissance strategy for unmanned aerial vehicle</article-title><source>IET Radar Sonar Navig.</source><year>2023</year><volume>11</volume><fpage>1711</fpage><lpage>1727</lpage><pub-id pub-id-type="doi">10.1049/rsn2.12461</pub-id></element-citation></ref><ref id="B29-sensors-25-01176"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Stecz</surname><given-names>W.</given-names></name>
<name><surname>Gromada</surname><given-names>K.</given-names></name>
</person-group><article-title>Determining UAV flight trajectory for target recognition using EO/IR and SAR</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>5712</elocation-id><pub-id pub-id-type="doi">10.3390/s20195712</pub-id><pub-id pub-id-type="pmid">33049975</pub-id>
</element-citation></ref><ref id="B30-sensors-25-01176"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Asiyabi</surname><given-names>R.M.</given-names></name>
<name><surname>Ghorbanian</surname><given-names>A.</given-names></name>
<name><surname>Tameh</surname><given-names>S.N.</given-names></name>
<name><surname>Amani</surname><given-names>M.</given-names></name>
<name><surname>Jin</surname><given-names>S.</given-names></name>
<name><surname>Mohammadzadeh</surname><given-names>A.</given-names></name>
</person-group><article-title>Synthetic aperture radar (SAR) for ocean: A review</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens.</source><year>2023</year><volume>16</volume><fpage>9106</fpage><lpage>9138</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2023.3310363</pub-id></element-citation></ref><ref id="B31-sensors-25-01176"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Luomei</surname><given-names>Y.</given-names></name>
<name><surname>Xu</surname><given-names>F.</given-names></name>
</person-group><article-title>Segmental aperture imaging algorithm for multirotor UAV-borne MiniSAR</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2023</year><volume>61</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2023.3238166</pub-id></element-citation></ref><ref id="B32-sensors-25-01176"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>P.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Qiu</surname><given-names>J.</given-names></name>
<name><surname>Zhao</surname><given-names>Q.</given-names></name>
<name><surname>Deng</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Yu</surname><given-names>W.</given-names></name>
<etal/>
</person-group><article-title>Digital beamforming synthetic aperture radar (DBSAR): Experiments and performance analysis in support of 16-channel airborne X-band SAR data</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2020</year><volume>59</volume><fpage>6784</fpage><lpage>6798</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2020.3027691</pub-id></element-citation></ref><ref id="B33-sensors-25-01176"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Garc&#x000ed;a-Fern&#x000e1;ndez</surname><given-names>M.</given-names></name>
<name><surname>&#x000c1;lvarez-Narciandi</surname><given-names>G.</given-names></name>
<name><surname>L&#x000f3;pez</surname><given-names>Y.</given-names></name>
<name><surname>Las-Heras</surname><given-names>F.</given-names></name>
</person-group><article-title>Array-based ground penetrating synthetic aperture radar on board an unmanned aerial vehicle for enhanced buried threats detection</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2023</year><volume>61</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2023.3272982</pub-id></element-citation></ref><ref id="B34-sensors-25-01176"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yuan</surname><given-names>Y.</given-names></name>
<name><surname>Cattaruzza</surname><given-names>D.</given-names></name>
<name><surname>Ogier</surname><given-names>M.</given-names></name>
<name><surname>Semet</surname><given-names>F.</given-names></name>
</person-group><article-title>A branch-and-cut algorithm for the generalized traveling salesman problem with time windows</article-title><source>Eur. J. Oper. Res.</source><year>2020</year><volume>286</volume><fpage>849</fpage><lpage>866</lpage><pub-id pub-id-type="doi">10.1016/j.ejor.2020.04.024</pub-id></element-citation></ref><ref id="B35-sensors-25-01176"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nourmohammadzadeh</surname><given-names>A.</given-names></name>
<name><surname>Sarhani</surname><given-names>M.</given-names></name>
<name><surname>Vo&#x000df;</surname><given-names>S.</given-names></name>
</person-group><article-title>A matheuristic approach for the family traveling salesman problem</article-title><source>J. Heuristics</source><year>2023</year><volume>29</volume><fpage>435</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1007/s10732-023-09516-9</pub-id></element-citation></ref><ref id="B36-sensors-25-01176"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Helsgaun</surname><given-names>K.</given-names></name>
</person-group><article-title>Solving the equality generalized traveling salesman problem using the Lin&#x02013;Kernighan&#x02013;Helsgaun Algorithm</article-title><source>Math. Prog. Comp.</source><year>2015</year><volume>7</volume><fpage>269</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.1007/s12532-015-0080-8</pub-id></element-citation></ref><ref id="B37-sensors-25-01176"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Fabri</surname><given-names>A.</given-names></name>
<name><surname>Pion</surname><given-names>S.</given-names></name>
</person-group><article-title>CGAL: The computational geometry algorithms library</article-title><source>Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>4&#x02013;6 November 2009</conf-date><fpage>538</fpage><lpage>539</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01176-f001"><label>Figure 1</label><caption><p>Computational workflow for UAV payload mission planning.</p></caption><graphic xlink:href="sensors-25-01176-g001" position="float"/></fig><fig position="float" id="sensors-25-01176-f002"><label>Figure 2</label><caption><p>Visualization of the visibility graph algorithm for UAV obstacle avoidance. The algorithm identifies potential intersections between the planned path segment <inline-formula><mml:math id="mm166" overflow="scroll"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, represented by the red line segment, and obstacle polygons. Obstacles are expanded by a buffer distance <inline-formula><mml:math id="mm167" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>buffer</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to ensure safety, and alternate paths, such as through <inline-formula><mml:math id="mm168" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mi>avoid</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, are generated to bypass obstacles, as illustrated by the green line.</p></caption><graphic xlink:href="sensors-25-01176-g002" position="float"/></fig><fig position="float" id="sensors-25-01176-f003"><label>Figure 3</label><caption><p>UI for autonomous mission planning module.</p></caption><graphic xlink:href="sensors-25-01176-g003" position="float"/></fig><fig position="float" id="sensors-25-01176-f004"><label>Figure 4</label><caption><p>Area target with no-fly zones and its Boustrophedon Cell Decomposition. (<bold>a</bold>) An area target with no-fly zones. (<bold>b</bold>) Boustrophedon Cell Decomposition of an area target with no-fly zones.The area target is decomposed into 6 cells without no-fly zones, marked from 0 to 6 respectively. (<bold>c</bold>) Results of coverage path planning with an area target.</p></caption><graphic xlink:href="sensors-25-01176-g004" position="float"/></fig><fig position="float" id="sensors-25-01176-f005"><label>Figure 5</label><caption><p>Planning map.</p></caption><graphic xlink:href="sensors-25-01176-g005" position="float"/></fig><fig position="float" id="sensors-25-01176-f006"><label>Figure 6</label><caption><p>Planning results of autonomous mission planning module.</p></caption><graphic xlink:href="sensors-25-01176-g006" position="float"/></fig><fig position="float" id="sensors-25-01176-f007"><label>Figure 7</label><caption><p>Path planning analysis, visibility graph-based obstacle avoidance, and payload power instruction output. (<bold>a</bold>) Path planning for different target scenarios, illustrating the planned waypoint set under varying conditions. (<bold>b</bold>) Visibility graph-based obstacle avoidance, demonstrating the generated path that navigates around no-fly zones and obstacles. (<bold>c</bold>) The output of payload power instructions, showing the power-on and work instructions of the payload during the mission. (<bold>a</bold>) Path planning for different targets. (<bold>b</bold>) Visibility graph-based obstacle avoidance for path planning. (<bold>c</bold>) The output of payload power instructions.</p></caption><graphic xlink:href="sensors-25-01176-g007" position="float"/></fig><fig position="float" id="sensors-25-01176-f008"><label>Figure 8</label><caption><p>Comparison of algorithm execution times under three different test datasets. The results demonstrate that the computation time is within 0.5 s. (<bold>a</bold>) Time consumption results of the first set of test data. (<bold>b</bold>) Time consumption results of the second set of test data. (<bold>c</bold>) Time consumption results of the third set of test data.</p></caption><graphic xlink:href="sensors-25-01176-g008" position="float"/></fig><table-wrap position="float" id="sensors-25-01176-t001"><object-id pub-id-type="pii">sensors-25-01176-t001_Table 1</object-id><label>Table 1</label><caption><p>The characteristics of the SAR Beam Focusing mode.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Default Work Altitude</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Imaging Resolution</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Range</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Imaging Width</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">5000 m</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5 m</td><td align="center" valign="middle" rowspan="1" colspan="1">16&#x02013;80 km</td><td align="center" valign="middle" rowspan="1" colspan="1">5 km &#x000d7; 5 km</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.3 m</td><td align="center" valign="middle" rowspan="1" colspan="1">12&#x02013;60 km</td><td align="center" valign="middle" rowspan="1" colspan="1">3 km &#x000d7; 3 km</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12&#x02013;30 km</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 km &#x000d7; 1 km</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01176-t002"><object-id pub-id-type="pii">sensors-25-01176-t002_Table 2</object-id><label>Table 2</label><caption><p>The characteristics of the SAR Strip mode.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Default Work Altitude</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Imaging Resolution</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Range</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Imaging Width</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">5000 m</td><td align="center" valign="middle" rowspan="1" colspan="1">5 m</td><td align="center" valign="middle" rowspan="1" colspan="1">72&#x02013;120 km</td><td align="center" valign="middle" rowspan="1" colspan="1">405 km</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">1 m</td><td align="center" valign="middle" rowspan="1" colspan="1">60&#x02013;100 km</td><td align="center" valign="middle" rowspan="1" colspan="1">203 km</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48&#x02013;80 km</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12 km</td></tr></tbody></table></table-wrap></floats-group></article>