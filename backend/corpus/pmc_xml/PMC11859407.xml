<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006285</article-id><article-id pub-id-type="pmc">PMC11859407</article-id><article-id pub-id-type="doi">10.3390/s25041057</article-id><article-id pub-id-type="publisher-id">sensors-25-01057</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Research on Upper Limb Motion Intention Classification and Rehabilitation Robot Control Based on sEMG</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2319-1682</contrib-id><name><surname>Song</surname><given-names>Tao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-01057" ref-type="aff">1</xref><xref rid="af2-sensors-25-01057" ref-type="aff">2</xref><xref rid="c1-sensors-25-01057" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Kunpeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-01057" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Yan</surname><given-names>Zhe</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-sensors-25-01057" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Yuwen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af1-sensors-25-01057" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Guo</surname><given-names>Shuai</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-sensors-25-01057" ref-type="aff">1</xref><xref rid="af3-sensors-25-01057" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0524-2469</contrib-id><name><surname>Li</surname><given-names>Xianhua</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af4-sensors-25-01057" ref-type="aff">4</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Tamura</surname><given-names>Toshiyo</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Tran</surname><given-names>Yvonne</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01057"><label>1</label>Shanghai Key Laboratory of Intelligent Manufacturing and Robotics, School of Mechatronic Engineering and Automation, Shanghai University, Shanghai 200444, China; <email>zhangkunpeng@shu.edu.cn</email> (K.Z.); </aff><aff id="af2-sensors-25-01057"><label>2</label>Shanghai Golden Arrow Robot Technology Co., Ltd., 701, Building 3, No. 377 Shanlian Road, Baoshan District, Shanghai 200444, China</aff><aff id="af3-sensors-25-01057"><label>3</label>National Demonstration Center for Experimental Engineering Training Education, Shanghai University, Shanghai 200444, China</aff><aff id="af4-sensors-25-01057"><label>4</label>School of Mechatronics Engineering, Anhui University of Science and Technology, Huainan 232001, China</aff><author-notes><corresp id="c1-sensors-25-01057"><label>*</label>Correspondence: <email>songtao43467226@shu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>10</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1057</elocation-id><history><date date-type="received"><day>02</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>22</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>07</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>sEMG is a non-invasive biomedical engineering technique that can detect and record electrical signals generated by muscles, reflecting both motor intentions and the degree of muscle contraction. This study aims to classify and recognize nine types of upper limb motor intentions based on surface electromyography (sEMG) and apply them to the interactive control of an end-effector rehabilitation robot. The research begins with selecting muscles and data preprocessing, incorporating the generation mechanism of sEMG along with the anatomical and kinesiological principles of upper limb muscles. Next, a musculoskeletal model of the upper limb is established and validated through simulations in OpenSim. To avoid the drawbacks of modeling methods, traditional machine learning and deep learning methods are employed to perform a nine-class classification task on the sEMG data, comparing the classification accuracy of different approaches. Finally, the motor intentions extracted using a multi-stream convolutional neural network (MLCNN) are utilized to control the iReMo<sup>&#x000ae;</sup> end-effector rehabilitation robot, with the system&#x02019;s motion smoothness and accuracy evaluated through tests involving different trajectories.</p></abstract><kwd-group><kwd>stroke</kwd><kwd>surface myoelectricity</kwd><kwd>upper limb rehabilitation robot</kwd><kwd>interactive control</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>82227807</award-id></award-group><award-group><funding-source>Shanghai Municipal of Science and Technology Commission</funding-source><award-id>24S11901600</award-id></award-group><award-group><funding-source>Special fund for Digital Transformation of Shanghai</funding-source><award-id>202202004</award-id></award-group><award-group><funding-source>Key Research and Development Program of Anhui Province</funding-source><award-id>2022i01020015</award-id></award-group><award-group><funding-source>Open Project of the Medical and Industrial Integration Laboratory of Jiangning Hospital Affiliated to Nanjing Medical University</funding-source><award-id>JNYYZXKY202218</award-id></award-group><funding-statement>This work was supported by the National Natural Science Foundation of China (82227807); the Shanghai Municipal of Science and Technology Commission (24S11901600); the Special fund for Digital Transformation of Shanghai (202202004); the Key Research and Development Program of Anhui Province (2022i01020015); and the Open Project of the Medical and Industrial Integration Laboratory of Jiangning Hospital Affiliated to Nanjing Medical University (JNYYZXKY202218).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01057"><title>1. Introduction</title><p>Surface electromyography (sEMG) captures bioelectric signals generated by muscle contractions, reflecting muscle activity [<xref rid="B1-sensors-25-01057" ref-type="bibr">1</xref>]. These signals can be processed and translated into control commands for robots, enabling them to respond in real time or perform specific actions based on muscle activity. The control methods for sEMG-based robots can be categorized into three types: fully autonomous continuous control, semi-autonomous assistive control, and discrete control. Fully autonomous control can mimic human movements in real time and has seen significant advancements in numerous studies. For instance, Vogel et al. utilized a Gaussian regression process to decode sEMG signals into directional velocity commands for robotic hands, facilitating control in microgravity environments [<xref rid="B2-sensors-25-01057" ref-type="bibr">2</xref>]. Similarly, Hagengruber applied a machine learning approach to convert EMG signals from SMA patients into force and velocity commands for robotic hands [<xref rid="B3-sensors-25-01057" ref-type="bibr">3</xref>]. Other strategies, such as Weitschat&#x02019;s spherical linear interpolation motion planning and joint angle mapping by Artemiadis and Kyriakopoulos, allow robots to accurately mimic human movements with minimal delay [<xref rid="B4-sensors-25-01057" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01057" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01057" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-01057" ref-type="bibr">7</xref>]. Semi-autonomous assistive control methods typically rely on pre-set motion templates to perform specific actions, simplifying complex task execution. For example, Vogel combined visual perception with these templates to guide a robotic arm in tasks like drinking water and opening doors using sEMG signals, while Shenoy et al. extracted movement direction from sEMG signals to control robotic arm movement using predefined parameters, making it suitable for rehabilitation and daily assistive tasks [<xref rid="B8-sensors-25-01057" ref-type="bibr">8</xref>]. In discrete control, these methods primarily depend on simple gesture recognition. Murillo recorded sEMG signals with a Myo armband to identify gestures for controlling a multi-axis robotic arm. Hassan compared various classification algorithms for gesture recognition in a 5-DOF robotic arm [<xref rid="B9-sensors-25-01057" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01057" ref-type="bibr">10</xref>].</p><p>Overall, the development of sEMG-controlled robotic technology has matured, with different methods offering varying levels of autonomy, precision, and applicability, showcasing a wide range of promising potential. However, despite significant advancements in sEMG control technology, many rehabilitation robots cannot accurately assess a patient&#x02019;s muscle condition, making it challenging to identify weak muscles or joint movements. While professional physicians can recognize issues and provide targeted single-joint training recommendations, they face challenges in continuously monitoring each patient&#x02019;s rehabilitation progress. They cannot promptly correct movement abnormalities caused by trunk compensation. Additionally, end-effector rehabilitation robots are relatively disadvantaged when addressing multi-joint rehabilitation training and trunk compensation issues. In clinical rehabilitation, active patient participation is crucial for training effectiveness; however, standard rehabilitation devices often focus only on completing training plans and achieving goals, leading to less than ideal rehabilitation outcomes.</p></sec><sec id="sec2-sensors-25-01057"><title>2. Problem Formulation</title><p>This study combines surface electromyography (sEMG) technology to improve the deficiencies of the six-degree-of-freedom upper limb rehabilitation robot iReMo<sup>&#x000ae;</sup> (Shanghai Golden Arrow Robot, Shanghai, China). Currently, upper limb robots are not sufficiently accurate in recognizing the movement intentions of patients. For some patients with weak muscles, the rehabilitation training performed by the robot may be inadequate. Since sEMG signals can be generated before muscle movement, utilizing sEMG can more efficiently and accurately identify the patient&#x02019;s movement intentions [<xref rid="B11-sensors-25-01057" ref-type="bibr">11</xref>]. This approach can provide better patient motion rehabilitation training and more personalized training methods tailored to the individual. Therefore, this paper proposes a control method for driving the robot based on sEMG signals [<xref rid="B12-sensors-25-01057" ref-type="bibr">12</xref>], as shown in <xref rid="sensors-25-01057-f001" ref-type="fig">Figure 1</xref>. First, it is necessary to identify the muscles driving upper limb movement and use the sEMG signals generated by these muscles to recognize the patient&#x02019;s intentions and classify limb movements. As shown in <xref rid="sensors-25-01057-f002" ref-type="fig">Figure 2</xref>, considering the limitations of existing training methods, this study adopts the CO-PTP (center-out point-to-point) task for training. This task helps to improve the range of motion, enhance muscle strength and flexibility, and improve coordination and accuracy of movement [<xref rid="B13-sensors-25-01057" ref-type="bibr">13</xref>]. Finally, the classification results will be applied to the control of the robotic arm, thereby enabling rehabilitation training for the patient through the robotic arm.</p></sec><sec id="sec3-sensors-25-01057"><title>3. Classification of Upper Limb Movements Based on sEMG</title><sec id="sec3dot1-sensors-25-01057"><title>3.1. Upper Limb Muscle Selection</title><p>Computational models are a powerful tool in biomechanics research, and the open-source software OpenSim4.5 provides a modeling platform for human and environmental interactions and is widely used for motion simulation. Holzbaur et al. developed an upper limb model that includes 15 degrees of freedom and 50 muscle compartments to estimate muscle forces and joint moments, aiding the study of neuromuscular control and surgical simulations [<xref rid="B14-sensors-25-01057" ref-type="bibr">14</xref>]. Saul et al. constructed the same upper limb model on both OpenSim and SIMM platforms, conducting simulations with electromyographic (EMG) driving, CMC driving, and gravity driving, comparing the simulation results and speed differences for the wrist, elbow, and shoulder joints, thereby validating the performance differences between the two platforms [<xref rid="B15-sensors-25-01057" ref-type="bibr">15</xref>]. Liu et al. created a musculoskeletal model driven by EMG signals, providing real-time stiffness control for a bilateral upper limb exoskeleton rehabilitation device by integrating muscle activation with the Hill model [<xref rid="B16-sensors-25-01057" ref-type="bibr">16</xref>]. David G. investigated a knee joint musculoskeletal model driven by surface EMG (sEMG) signals, accurately predicting the knee joint moment [<xref rid="B17-sensors-25-01057" ref-type="bibr">17</xref>]. Through these representative studies, the application of OpenSim in simulating muscle forces and motion has been extensively validated and developed.</p><p>The simplified musculoskeletal model (<xref rid="sensors-25-01057-f003" ref-type="fig">Figure 3</xref>) consists of three bones&#x02014;the scapula, humerus, and forearm&#x02014;and eleven muscles, simulating shoulder and elbow joint functions. It allows for adduction, abduction, flexion, extension, rotation, and elbow flexion and extension.</p><p>The Hill model describes muscle as a system of three elements, providing a foundational framework for understanding muscle contraction dynamics [<xref rid="B18-sensors-25-01057" ref-type="bibr">18</xref>]. This model is essential for creating musculoskeletal simulations in OpenSim, which enables the analysis of upper limb movements, including shoulder and elbow joint functions. Using the Hill model, OpenSim can simulate various motions such as flexion, extension, adduction, and rotation, enhancing the study of human biomechanics [<xref rid="B19-sensors-25-01057" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01057" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-01057" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01057" ref-type="bibr">22</xref>].</p><p>This study&#x02019;s model is based on Saul et al.&#x02019;s research in OpenSim and includes the shoulder, elbow, wrist, and hand joints and muscles, simplified for this study (<xref rid="sensors-25-01057-f004" ref-type="fig">Figure 4</xref>).</p><p>In forward dynamics, the acceleration of the coordinates is described based on Newton&#x02019;s second law, which considers the inertia and the forces applied to the skeletal structure treated as a set of rigid bodies:<disp-formula id="FD1-sensors-25-01057"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>&#x000a8;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>M</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this equation, <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>&#x000a8;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the acceleration caused by the joint torque <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> represents the Coriolis and centrifugal forces, which are functions of position and velocity; <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the gravitational force; <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the net force acting on the model; and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>M</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the inverse of the mass matrix.</p><p>The joint torque <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the sum of the net torques produced by the relevant muscles:<disp-formula id="FD2-sensors-25-01057"><label>(2)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The net torque <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> generated by a single muscle force can be expressed as<disp-formula id="FD3-sensors-25-01057"><label>(3)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>&#x000b7;</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The net muscle torque <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the result of the moment arm <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> multiplied by the muscle force <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, where the muscle force <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is a function of muscle activation <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, muscle fiber length <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and muscle fiber contraction velocity <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>. The muscle fiber contraction velocity <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> can be expressed as<disp-formula id="FD4-sensors-25-01057"><label>(4)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="sans-serif">&#x0039b;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD5-sensors-25-01057"><label>(5)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The simulation integrates dynamic equations from the initial state using a 5th-order Runge&#x02013;Kutta&#x02013;Fehlberg integrator to calculate muscle activation and contraction velocities. This activation is based on abduction and adduction movements with a 5 kg dumbbell, starting from an upright position and moving to 90 degrees. The experimental and simulation conditions are shown in <xref rid="sensors-25-01057-f005" ref-type="fig">Figure 5</xref> and <xref rid="sensors-25-01057-f006" ref-type="fig">Figure 6</xref>.</p><p>The subject held a 5 kg dumbbell, starting from an upright position for 2 s, flexing to 90 degrees for 5 s, and returning to the upright position for another 2 s, totaling 9 s. The model&#x02019;s initial position in OpenSim matched the experiment, with shoulder elevation, elbow flexion at 0&#x000b0;, and shoulder rotation at 90&#x000b0;. <xref rid="sensors-25-01057-f007" ref-type="fig">Figure 7</xref> and <xref rid="sensors-25-01057-f008" ref-type="fig">Figure 8</xref> illustrate the experimental and simulation movements, joint angles, and muscle activation outputs.</p><p>The subject held a 5 kg dumbbell, starting upright for 3 s and then flexing the forearm to 130&#x000b0; for 2 s before returning to the upright position for another 2 s, totaling 7 s. In OpenSim, the initial position matched the experiment, with shoulder elevation at 10&#x000b0; and elbow flexion at 0&#x000b0;. <xref rid="sensors-25-01057-f009" ref-type="fig">Figure 9</xref> and <xref rid="sensors-25-01057-f010" ref-type="fig">Figure 10</xref> show the experimental and simulation movements, including joint angles and muscle activation outputs.</p><p>In summary, based on the simulation results, the musculoskeletal model established in this section, consisting of two joints and eleven muscles, is essentially capable of performing muscle contraction-driven shoulder abduction and adduction in the coronal plane; flexion and extension in the sagittal plane; and elbow flexion and extension.</p></sec><sec id="sec3dot2-sensors-25-01057"><title>3.2. Data Collection and Preprocessing</title><p>This study used Delsys<sup>&#x000ae;</sup> Trigno sEMG sensors(Delsys Inc., Massachusetts, USA), transmitting data wirelessly to a base station at a sampling frequency of 2000 Hz. The base station connects to a PC for real-time data processing. In the preprocessing and noise reduction of sEMG signals, sensor placement was optimized, wireless RF technology was used to reduce motion artifacts, a 4th-order Butterworth band-pass filter (20 Hz to 450 Hz) was applied to remove noise, and a notch filter was used to eliminate 50 Hz power line interference. Finally, rectification and envelope processing were performed to extract clear electromyographic signals. This study primarily focuses on the movement of the shoulder and elbow joints of the upper limb. Based on anatomical knowledge, the muscle functions related to the shoulder and elbow joint movements, and the measurability of sEMG signals, the main muscles responsible for driving the movement of these two joints were selected for signal collection. Muscles such as the extensor digitorum, extensor carpi radialis brevis, and palmaris longus muscle were excluded. This study focuses on shoulder and elbow movements, selecting eleven key muscles for data collection: latissimus dorsi, supraspinatus, infraspinatus, triceps brachii, biceps brachii, anterior deltoid, posterior deltoid, pectoralis major, brachialis, brachioradialis, and middle deltoid. The sensor placement is shown in <xref rid="sensors-25-01057-f011" ref-type="fig">Figure 11</xref>.</p><p>First, data collection for the isometric contraction experiment was conducted. After placing the sensors, as shown in <xref rid="sensors-25-01057-f011" ref-type="fig">Figure 11</xref>, subjects sat about 0.3 m in front of the robot, with their right hand grasping the end effector and forearm resting on the support plate. They then performed movements prompted by the interface (<xref rid="sensors-25-01057-f012" ref-type="fig">Figure 12</xref>). The robot restricted movement to a horizontal area of 0.6 m &#x000d7; 0.3 m and applied a maximum resistance of 25 N against the movement. Subjects were instructed to move smoothly to simulate isotonic contraction.</p><p>The position of the robot&#x02019;s end effector was indicated on the screen as a red circular cursor (<xref rid="sensors-25-01057-f013" ref-type="fig">Figure 13</xref>). Movements started at the center point (0) and followed directional cues to black circular points (1&#x02013;8) arranged clockwise. Returning from point (8) to the center completed one set. Each subject performed five sets, with 1&#x02013;2 min of rest between sets. During each set, the end effector&#x02019;s position coordinates (<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) and six-dimensional force measurements (<italic toggle="yes">F</italic><italic toggle="yes">x</italic>, <italic toggle="yes">F</italic><italic toggle="yes">y</italic>) were recorded at 60 Hz, while sEMG data were collected at 2000 Hz. Raw data were accessible via the data analysis interface.</p><p>Next, data collection for the isometric contraction experiment was conducted. <xref rid="sensors-25-01057-f014" ref-type="fig">Figure 14</xref> shows the equipment and setup for collecting isometric contraction data. The hardware system mainly consists of a handheld grip, a six-dimensional force sensor, and a base plate, among other components.</p><p>During data collection, subjects placed the sensors, as shown in <xref rid="sensors-25-01057-f011" ref-type="fig">Figure 11</xref>, and sat in front of a table, approximately 0.2 m from the grip. They performed isometric contractions in eight directions (coded 1 to 8), with rest intervals (coded 0) in between. Each movement sequence followed a predefined pattern (<xref rid="sensors-25-01057-f015" ref-type="fig">Figure 15</xref> and <xref rid="sensors-25-01057-f016" ref-type="fig">Figure 16</xref>) and lasted 85 s, with rest and contraction phases at 5 s each. Seven subjects completed five sets of actions at force levels of 5 N, 10 N, 15 N, 20 N, and 25 N, resting for 1 min between sets.</p><p>Finally, data collection was collected for the maximum voluntary contraction of the eleven muscles used in this study. Subjects were required to perform designated movements or contract their muscles with maximum force at specified joint angles and positions, maintaining this for 3 to 5 s. To ensure the consistency and reliability of the results, each action was repeated five times, with a rest interval of 1 to 2 min between each repetition to avoid the effects of fatigue accumulation. Normalizing the sEMG signal amplitude to the signal level in MVC allows the force level between individuals to be compared more fairly, and the influence of individual differences can be reduced.</p><p>During sEMG signal collection, various noise sources can negatively impact signal quality. Optimizing sensor placement is essential to reduce crosstalk from ECG and unrelated muscle activity. Additionally, wireless data transmission can minimize motion artifacts caused by cables. While these measures improve signal quality, further filtering is necessary to extract reliable sEMG signals. The most valuable information is found between 20 Hz and 500 Hz, so band-pass filtering is applied. This study uses a fourth-order Butterworth filter from 20 Hz to 450 Hz and a notch filter to eliminate 50 Hz power line interference [<xref rid="B23-sensors-25-01057" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-01057" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-01057" ref-type="bibr">25</xref>]. Processed sEMG signals undergo rectification and envelope detection to assess muscle activation [<xref rid="B26-sensors-25-01057" ref-type="bibr">26</xref>]. <xref rid="sensors-25-01057-f017" ref-type="fig">Figure 17</xref> shows the raw sEMG data, band-pass filtered data, rectified data, and the envelope.</p></sec><sec id="sec3dot3-sensors-25-01057"><title>3.3. Structure and Training of the Classification Model</title><p>A CNN consists of critical components: convolutional layers, activation functions, pooling layers, and fully connected layers (<xref rid="sensors-25-01057-f018" ref-type="fig">Figure 18</xref>a). An MLCNN is a specialized architecture that integrates multi-stream convolution operations with large pooling windows to enhance performance by parallel processing features at different scales. This approach improves robustness in capturing signal features, allowing the model to retain critical information even if some features are fragmented. Its structure is shown in <xref rid="sensors-25-01057-f018" ref-type="fig">Figure 18</xref>b.</p><p>Based on the deep learning framework Keras in Python, an alternating convolutional CNN and MLCNN are constructed, as shown in <xref rid="sensors-25-01057-f019" ref-type="fig">Figure 19</xref>a,b, respectively [<xref rid="B27-sensors-25-01057" ref-type="bibr">27</xref>]. The numbers in the figures represent the dimensions of each tensor.</p><p>Based on data from 105 isometric contraction experiments with Subject 2, the data are divided into seven groups by experiment dates. The original data are segmented into 450-point segments for model input. For each training session, six groups are selected, with 80% as the training set and 20% as the validation set. The seventh group serves as an external cross-validation set. <xref rid="sensors-25-01057-f020" ref-type="fig">Figure 20</xref>a shows the accuracy and loss during training, while <xref rid="sensors-25-01057-f020" ref-type="fig">Figure 20</xref>b presents the confusion matrix. The model performs well, with most data classified correctly. Confusion primarily arises between Category 3 (force applied to the right) and Category 4 (force applied to the right and backward) due to the proximity of muscle force application sites for these movements.</p><p>The results of the seven training sessions and cross-validation are shown in <xref rid="sensors-25-01057-f021" ref-type="fig">Figure 21</xref>. Among the models obtained from the seven training sessions, the average accuracy of the CNN model is 96.62%, with an average accuracy of 94.44% on the cross-validation set. The average accuracy of the MLCNN model is 97.13%, with an average accuracy of 95.04% on the cross-validation set.</p><p>In summary, the performance of the MLCNN model is superior to that of the CNN model. Therefore, based on the experimental space and data division, the MLCNN models trained in regions L, M, and R are obtained, and the accuracy of these models is shown in <xref rid="sensors-25-01057-f022" ref-type="fig">Figure 22</xref>.</p><p>Optuna, proposed by Takuya Akiba et al. in 2019, is an automated hyperparameter optimization framework specifically designed for machine learning tasks [<xref rid="B28-sensors-25-01057" ref-type="bibr">28</xref>]. It is widely used in tuning hyperparameters for both deep learning and traditional machine learning models, effectively exploring and determining near-optimal model configurations. Additionally, it improves optimization efficiency through pruning techniques, reducing the waste of computational resources. Optuna also provides visualization tools for the hyperparameter tuning process, making it easier for users to understand and analyze the optimization workflow.</p><p>In Optuna, users need to define an objective function, which is typically the model&#x02019;s loss function or accuracy metric. Through multiple trials in a study, Optuna searches for the hyperparameter combinations that optimize the objective function, i.e., minimizing the loss function or maximizing accuracy. Using Optuna, the existing MLCNN model is tuned by setting the return value of the objective function as the accuracy, making the goal of optimization to maximize accuracy. A total of 50 optimization trials are conducted. The seven hyperparameters and their optimization ranges are shown in <xref rid="sensors-25-01057-t001" ref-type="table">Table 1</xref>.</p></sec></sec><sec id="sec4-sensors-25-01057"><title>4. Research on Interactive Control of Rehabilitation Robot Based on Motion Intention</title><sec id="sec4dot1-sensors-25-01057"><title>4.1. Kinematics of iReMo<sup>&#x000ae;</sup></title><p>In robotics kinematics, the velocity describes the relationship between the linear and angular velocities of the end effector and the joint velocities. This relationship is represented by the Jacobian matrix in forward kinematics. The Jacobian matrix can generally be derived using either analytical or geometric methods [<xref rid="B29-sensors-25-01057" ref-type="bibr">29</xref>]. The analytical Jacobian matrix <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is derived directly by differentiating the forward kinematics, while the geometric Jacobian matrix <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is obtained from geometric properties, where each joint velocity is related to the changes in the linear and angular velocities of the end effector. The relationship between these two representations can be expressed as<disp-formula id="FD6-sensors-25-01057"><label>(6)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>I</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, the Euler angles <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mi>&#x003c6;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>&#x003c8;</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>&#x003c8;</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>&#x003c8;</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:mtd><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>&#x003c8;</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. Assuming that <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is a non-singular matrix, this chapter primarily focuses on the velocity relationship between the workspace and the joint space. Therefore, the geometric Jacobian matrix <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> will be used for the subsequent derivations.</p><p>The geometric Jacobian matrix can be divided into a linear velocity part and an angular velocity part [<xref rid="B30-sensors-25-01057" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-01057" ref-type="bibr">31</xref>]. These parts, <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (linear velocity part) and <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c9;</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (angular velocity part), are determined by whether the joint is a rotational joint or a prismatic joint.<disp-formula id="FD7-sensors-25-01057"><label>(7)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c9;</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>&#x000d7;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The robot consists of six rotational joints, so its Jacobian matrix can be expressed as<disp-formula id="FD8-sensors-25-01057"><label>(8)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c9;</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>&#x000d7;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The Jacobian matrix of the robotic arm relates the joint velocity vector to the end-effector velocity vector <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003be;</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The skew-symmetric matrix <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003c9;</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be used to determine the angular velocity vector <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> in the Jacobian matrix of the robotic arm. The definition of the skew-symmetric matrix <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003c9;</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is as follows:<disp-formula id="FD9-sensors-25-01057"><label>(9)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The linear velocity of the end effector can be directly obtained from the following equation:<disp-formula id="FD10-sensors-25-01057"><label>(10)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Therefore, the Jacobian matrix <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>J</mml:mi><mml:mi>&#x003f5;</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> of the robotic arm can be defined separately by the linear velocity part and the angular velocity part:<disp-formula id="FD11-sensors-25-01057"><label>(11)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-01057"><label>(12)</label><mml:math id="mm41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:msub><mml:mi>&#x003f5;</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, the Jacobian matrix can be expressed as<disp-formula id="FD13-sensors-25-01057"><label>(13)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003be;</mml:mi><mml:mo>=</mml:mo><mml:mi>J</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003be;</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>In iReMo<sup>&#x000ae;</sup>, the robotic arm is controlled via joint velocity inputs; therefore, it is necessary to solve for the velocities in inverse kinematics. Assuming that the Jacobian matrix <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is non-singular, it is evident that in the robotic arm studied, <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a square matrix. Thus, the joint velocities of the robotic arm can be solved using the inverse Jacobian matrix obtained through LU decomposition:<disp-formula id="FD14-sensors-25-01057"><label>(14)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>&#x003be;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>From this, the joint velocities of the robotic arm can be solved and controlled based on the desired Cartesian velocity of the end effector in the workspace.</p></sec><sec id="sec4dot2-sensors-25-01057"><title>4.2. Control System and Implementation Methods</title><p>The system comprises the iReMo<sup>&#x000ae;</sup> lower computer, a server, a host computer, and sEMG sensors, as illustrated in <xref rid="sensors-25-01057-f023" ref-type="fig">Figure 23</xref>. When the user performs active movements, muscle contractions generate EMG signals, which are captured by the sensors and sent to the host computer. The host computer uses the MLCNN model to classify these signals and determine the user&#x02019;s movement intention. It then generates control commands for the iReMo<sup>&#x000ae;</sup> end effector to respond accordingly. Both the host computer and iReMo<sup>&#x000ae;</sup> are connected to the same local area network, using the server for message forwarding to ensure real-time synchronization of commands and status information.</p><p>The system features open-loop and closed-loop training based on isometric and isotonic exercises. In isometric training, patients control iReMo<sup>&#x000ae;</sup> using sEMG signals while holding the handle fixed, allowing the end effector to mimic their movements. In isotonic training, patients hold the end effector directly, creating a closed-loop system with interaction forces. The host computer recognizes nine motion intentions (0 for resting, 1&#x02013;8 for directional movements). If resting, iReMo<sup>&#x000ae;</sup> generates no virtual force; otherwise, it calculates a target point (30 mm offset) and produces a 4 N virtual force. This force is converted into desired Cartesian velocity using a PD controller, and joint velocities are computed with the inverse Jacobian matrix, enabling the end effector to mimic the patient&#x02019;s movements. To avoid vibrations of the robot caused by abrupt changes in virtual force when the motion intention changes, the virtual force is smoothed during transitions to ensure that the movement of iReMo<sup>&#x000ae;</sup> is as smooth as possible. The specific method for smoothing the virtual force is as follows:<disp-formula id="FD15-sensors-25-01057"><label>(15)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the smoothed virtual interaction force, while <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the virtual interaction forces before and after the change in motion intention, respectively. <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the smoothing duration, which is set to 0.4 s in this study. <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the remaining time and the elapsed time during the smoothing period <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>; therefore, the relation <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is established.</p><p>In isotonic training, if the motion intention is resting, no virtual force is generated; if it is active, a virtual interaction force of 6.5 N is directed toward the target point. Actual interaction forces between the hand and the end effector may arise due to the control method not adapting the end effector&#x02019;s speed based on the sEMG signal amplitude, which can create forces when movement speeds differ with an angle of less than 22.5&#x000b0; between them. Additionally, occasional model misidentification can result in inconsistent movement directions, leading to further interaction forces. The resultant interaction force is formed by combining the actual and virtual forces from sEMG, which is then converted into the desired Cartesian velocity using a PD controller. The inverse Jacobian matrix is used to calculate the joint velocities for execution, achieving closed-loop control. These methods are implemented in the iReMo<sup>&#x000ae;</sup> controller using C++. The following pseudocode (Algorithm 1) illustrates the control method and PID controller implementation, where the proportional gain Kp is set to 0.15, the integral coefficient Ki is set to 0, and the derivative coefficient Kd is set to 0.5.</p><array><tbody><tr><td colspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1"><bold>Algorithm 1:</bold> Pseudo code for iReMo<sup>&#x000ae;</sup> joint speed control. iReMo<sup>&#x000ae;</sup> Speed Control</td></tr><tr><td colspan="2" align="left" valign="middle" rowspan="1">Input: F_end_effector: The three-dimensional resultant force applied to the end effector<break/> &#x02003;&#x02003;&#x02003;T_end_effector: The three-dimensional resultant torque applied to the end effector<break/> &#x02003;&#x02003;&#x02003;Q_position_current: The current position of each joint</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="left" valign="middle" rowspan="1" colspan="1">while(running){</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="left" valign="middle" rowspan="1" colspan="1">// 3D Force Velocity Control</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="left" valign="middle" rowspan="1" colspan="1">F_measured = gravity_compensation(Q_position_current, F_end_effector);</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="left" valign="middle" rowspan="1" colspan="1">F_error = F_reference &#x02212; F_measured;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="left" valign="middle" rowspan="1" colspan="1">F_integrator = F_integrator + F_error;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="left" valign="middle" rowspan="1" colspan="1">F_derivative = F_error &#x02212; F_previous_error;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="left" valign="middle" rowspan="1" colspan="1">u_F = Kp * F_error + Ki * F_integrator + Kd * F_derivative;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">9</td><td align="left" valign="middle" rowspan="1" colspan="1">// Angular velocity control of 3D torque</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="left" valign="middle" rowspan="1" colspan="1">T_measured = T_patient;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">11</td><td align="left" valign="middle" rowspan="1" colspan="1">T_error = T_reference &#x02212; T_measured;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">12</td><td align="left" valign="middle" rowspan="1" colspan="1">T_integrator = T_integrator + T_error;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">13</td><td align="left" valign="middle" rowspan="1" colspan="1">T_derivative = T_error &#x02212; T_previous_error;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">14</td><td align="left" valign="middle" rowspan="1" colspan="1">u_T = Kp * T_error + Ki * T_integrator + Kd * T_derivative;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">16</td><td align="left" valign="middle" rowspan="1" colspan="1">vw[6] = [u_Fx, u_Fy, u_Fz, u_Tx, u_Ty, u_Tz];</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">17</td><td align="left" valign="middle" rowspan="1" colspan="1">Q_velocity_desired = inverse_Jacobian(Q_position_current, vw);</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">18</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">19</td><td align="left" valign="middle" rowspan="1" colspan="1">UR5.send_speed_command(Q_velocity_desired);</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">}</td></tr></tbody></array><p>To comprehensively test the system&#x02019;s motion functionality, four target paths of varying complexity were designed on a horizontal plane measuring 60 cm &#x000d7; 30 cm: a circular path, a CO-PTP path, and two sine paths with different curvatures. The center of the circular path is located at the center of the motion area, with a radius of 0.1 m. The center of the CO-PTP path coincides with the center of the circular path, and its outer points are evenly distributed along the circular path. Sine Path 1 is defined as <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="italic">sin</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>10</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>0.525</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>[</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, while Sine Path 2 is defined as <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="italic">sin</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>5</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>0.525</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>[</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in <xref rid="sensors-25-01057-f024" ref-type="fig">Figure 24</xref>.</p><p>During the experiment, the placement of the EMG sensors was consistent with the data collection process, with 11 channels of sensors attached to the corresponding muscles, while the upper body remained upright and still. The subjects followed the paths displayed on the testing software interface one by one, receiving feedback through the real-time position displayed on the interface and actively correcting any deviations in their movements, as shown in <xref rid="sensors-25-01057-f025" ref-type="fig">Figure 25</xref>.</p><p>To assess the smoothness of movement, two indicators are used:</p><p>(1) Smoothness (<inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>): This metric is used to describe the coherence of movement. The calculation method is as follows:<disp-formula id="FD16-sensors-25-01057"><label>(16)</label><mml:math id="mm63" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the average speed during a segment of movement, while <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the maximum speed within the same segment. Therefore, the smoothness value ranges from 0 to 1, where a higher value closer to 1 indicates a greater capability demonstrated by the subject during the movement.</p><p>(2) Range Deviation (<inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>): This metric is used to describe the positional deviation from the standard trajectory during movement. The calculation method is as follows:<disp-formula id="FD17-sensors-25-01057"><label>(17)</label><mml:math id="mm67" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD18-sensors-25-01057"><label>(18)</label><mml:math id="mm68" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x000b7;</mml:mo><mml:msqrt><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>&#x02206;</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <italic toggle="yes">L</italic> represents the actual length of the standard trajectory, and <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02206;</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the distance from the end effector&#x02019;s center to the standard trajectory at time <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Therefore, the range deviation value ranges from 0 to 1, where a higher value closer to 1 indicates stronger control abilities demonstrated by the subject during the movement.</p><p>To assess the precision of movement, the normalized path length <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is used. This metric represents the ratio of the length of the subject&#x02019;s actual movement trajectory to the length of the standard trajectory.<disp-formula id="FD19-sensors-25-01057"><label>(19)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the length of the actual trajectory. When <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is less than the standard trajectory length <italic toggle="yes">L</italic>, the value is less than 1; conversely, when <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is greater than <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the value is greater than l. Thus, the value is generally distributed around 1. However, since the normalized path length cannot describe the similarity between the actual trajectory and the target trajectory, it cannot be used alone to assess movement ability. When the range deviation indicator is close to 1, a normalized path length closer to 1 indicates stronger movement control abilities in the subject.</p></sec></sec><sec sec-type="results" id="sec5-sensors-25-01057"><title>5. Results</title><sec id="sec5dot1-sensors-25-01057"><title>5.1. Classification Model and Optimization Results</title><p>The information about the subjects is shown in <xref rid="sensors-25-01057-t002" ref-type="table">Table 2</xref>. The changes in the optimization target accuracy during the model optimization process in region M are shown in <xref rid="sensors-25-01057-f026" ref-type="fig">Figure 26</xref>. In the 50 trials, the accuracy remained above 70%, with the highest accuracy of 96.87% achieved in trial 45. The hyperparameter combinations selected by Optuna during the trials are illustrated in <xref rid="sensors-25-01057-f027" ref-type="fig">Figure 27</xref>a, while the importance of the hyperparameters is depicted in <xref rid="sensors-25-01057-f027" ref-type="fig">Figure 27</xref>b. Among the seven hyperparameters, the learning rate and batch size had the most significant impact on model performance, each with an importance score of 0.34; the combined importance of the other five parameters was 0.32.</p><p>For the models in regions L and R, optimization was conducted, and through multiple studies, the optimal parameters for models in the three regions were obtained. The model accuracy under the optimal parameters is shown in <xref rid="sensors-25-01057-t003" ref-type="table">Table 3</xref>. The model accuracy for regions M and R improved by 1.56% and 0.33%, respectively, compared to before optimization, while the model accuracy for region L was lower than that of the original model. Ultimately, the accuracies of the models for regions L, M, and R were 89.5%, 96.87%, and 88.01%, respectively, as shown in <xref rid="sensors-25-01057-f028" ref-type="fig">Figure 28</xref>. The lower accuracies in regions L and R compared to the central region may be related to the muscle exertion patterns and experimental design.</p><p>The classification performance of the isotonic experiments was poor due to unclear muscle contractions and instability, making it unsuitable for classification.</p></sec><sec id="sec5dot2-sensors-25-01057"><title>5.2. Robot Interaction Control Experiment Results</title><sec id="sec5dot2dot1-sensors-25-01057"><title>5.2.1. Comparison of Different Control Methods Under the Same Path</title><p>Subject 2 performed seven experiments on isotonic and isometric contractions, as shown in <xref rid="sensors-25-01057-f029" ref-type="fig">Figure 29</xref>. The red dashed line indicates the standard circular path, while the brown lines depict the experimental trajectories. Under open-loop control, trajectories were irregular and scattered (inner radius: 0.0045 m, outer radius: 0.2460 m). In contrast, closed-loop control yielded more circular and concentrated trajectories (inner radius: 0.0618 m, outer radius: 0.1384 m), demonstrating improved alignment with the standard path.</p><p><xref rid="sensors-25-01057-f030" ref-type="fig">Figure 30</xref> and <xref rid="sensors-25-01057-f031" ref-type="fig">Figure 31</xref> show the motion smoothness and range deviation results. Under open-loop control, the average smoothness was 0.5388, with a range deviation of 0.8885. In contrast, closed-loop control had a smoothness of 0.5381 and a range deviation of 0.8711. While both methods had similar smoothness, open-loop control was less stable, exhibiting greater fluctuations than the more consistent closed-loop performance.</p><p><xref rid="sensors-25-01057-f032" ref-type="fig">Figure 32</xref> displays the normalized path length indicators, showing averages of 1.37 for open-loop control and 1.10 for closed-loop control. In <xref rid="sensors-25-01057-f033" ref-type="fig">Figure 33</xref>, the standard CO-PTP path is marked by a red dashed line, while the brown lines represent actual trajectories. Open-loop control exhibited significant divergence from the intended path, complicating the completion of all segments. Conversely, closed-loop control achieved accurate movement but often overshot segment endpoints, indicating challenges in precise stopping.</p><p><xref rid="sensors-25-01057-f034" ref-type="fig">Figure 34</xref>, <xref rid="sensors-25-01057-f035" ref-type="fig">Figure 35</xref> and <xref rid="sensors-25-01057-f036" ref-type="fig">Figure 36</xref> show the results for smoothness, range deviation, and normalized path length in closed-loop control experiments, with averages of 0.48 for smoothness, 0.03 for range deviation, and 1.41 for normalized path length. The consistent smoothness indicates uniformity, while the low range deviation highlights divergence from the standard path during CO-PTP movements.</p><p><xref rid="sensors-25-01057-f037" ref-type="fig">Figure 37</xref> illustrates the results for Sinusoidal Path 2 under open-loop control and Sinusoidal Path 1 under closed-loop control. The red dashed line represents the standard path, while the brown lines depict actual trajectories from the seven experiments. Under open-loop control, the trajectories were mostly straight, leading to significant deviations from the standard path. In contrast, closed-loop control generally followed the sine shape but showed discrepancies at the troughs, with none of the experiments fully matching the standard path.</p><p><xref rid="sensors-25-01057-f038" ref-type="fig">Figure 38</xref> and <xref rid="sensors-25-01057-f039" ref-type="fig">Figure 39</xref> show that open-loop control has an average smoothness of 0.48 and a range deviation of 0.32, while closed-loop control averages 0.54 in smoothness and 0.19 in deviation. Thus, motion smoothness is similar, but open-loop control better handles range deviation. <xref rid="sensors-25-01057-f040" ref-type="fig">Figure 40</xref> presents standardized path length, with open-loop averaging 1.27 and closed-loop at 0.99. Open-loop control exceeds the target length with mostly straight paths, while closed-loop control takes a shorter, more direct route, indicating better trajectory control.</p></sec><sec id="sec5dot2dot2-sensors-25-01057"><title>5.2.2. Comparison of Different Paths Under the Same Control Mode</title><p>Due to the poor performance of open-loop control under the CO-PTP path, this paper focuses on comparing the circular path and Sine Path 2. In <xref rid="sensors-25-01057-f041" ref-type="fig">Figure 41</xref>, the circular trajectory exhibits a smoother speed variation, with its smoothness distribution concentrated around 1.0, while the sine trajectory is more dispersed. For range deviation, the circular path shows effective error control with values between 0.8 and 1.0, whereas the sine trajectory averages around 0.2, indicating substantial deviations from the target path. Additionally, the circular path length ranges from 1.25 to 1.5, reflecting longer actual movement paths than theoretical lengths, while the sine trajectory ranges from 0.75 to 1.0, suggesting slightly shorter actual paths.</p><p>In the closed-loop control experiments, box plots in <xref rid="sensors-25-01057-f042" ref-type="fig">Figure 42</xref> analyze smoothness, range deviation, and standardized path length for three paths. Motion smoothness centers around 0.5 for all trajectories, indicating a maximum speed approximately twice the average speed. For range deviation, values from 0 to 1 show that higher values indicate smaller deviations from expected trajectories, with the ranking as follows: CO-PTP path &#x0003e; Sine Path 1 &#x0003e; circular path. The CO-PTP path requires the highest control capability due to its straight segments, while the sine trajectory shows lower accuracy at trough positions.</p><p>All trajectories exhibit standardized path lengths close to or exceeding 1, indicating that actual movement paths generally exceed the theoretically shortest lengths, aligning with typical upper limb motion patterns in humans.</p></sec></sec></sec><sec sec-type="discussion" id="sec6-sensors-25-01057"><title>6. Discussion</title><p>This study develops an interactive control method based on surface electromyography (sEMG) signals, aimed at providing rehabilitation training for stroke patients with upper limb movement disorders, filling a certain gap in existing technology. The innovation of this study lies in the selection of upper limb muscles based on OpenSim, using sEMG signals for movement intention recognition, and employing the MLCNN model to improve classification accuracy. However, this method still faces numerous challenges and limitations. Compared to existing research in the field, this study has made progress in the classification and recognition of movement intentions and the implementation of control systems, especially by utilizing deep learning models (such as MLCNN) to improve classification accuracy, showing promising results. However, many areas still require improvement. Firstly, due to alterations in muscle electrical activity under varying load and fatigue states, leading to increased non-linearity and instability in the signals, classification accuracy decreases under different force levels and muscle fatigue states, reflecting the model&#x02019;s inadequate adaptability to complex real-world situations. Secondly, due to limitations in obtaining anatomical parameters, the established musculoskeletal model for the upper limb fails to accurately simulate the dynamic movement of the shoulder joint, leading to discrepancies between the simulation results and the actual conditions. Compared to other studies, such as those that have established complete musculoskeletal models and employed more complex algorithms, this study appears relatively basic, especially in terms of the model&#x02019;s robustness and adaptability. Other research may demonstrate greater maturity in data processing, signal classification, and model simulation. Therefore, although this study has foundational significance in advancing related fields, its practical applicability and effectiveness require further optimization.</p><p>Future research can focus on improving the model&#x02019;s adaptability and accuracy, particularly in processing sEMG signals under varying muscle force levels and fatigue states. Introducing more experimental data, advanced model optimization techniques, and real-time feedback mechanisms will help enhance the performance of the designed control system, better serving the rehabilitation needs of stroke patients.</p></sec><sec sec-type="conclusions" id="sec7-sensors-25-01057"><title>7. Conclusions</title><p>The upper limb rehabilitation robot interactive control method based on surface electromyographic (sEMG) signals developed in this study can effectively support isometric and isotonic training for stroke patients with upper limb motor impairments. Although this study utilizes machine learning and deep learning methods to process sEMG signals and classify movement intentions of the upper limb in the horizontal plane, successfully using electromyographic signals to control the upper limb rehabilitation robot, and achieving isometric and isotonic training for stroke patients with upper limb motor impairments, some shortcomings have been revealed. These include insufficient classification accuracy of the model under different force levels and muscle fatigue states and limitations in the anatomical parameter acquisition of the established musculoskeletal model. Future research should focus on enhancing the robustness of the classification model, optimizing the accuracy of movement intention recognition, and improving the processing of surface electromyographic signals under fatigue conditions. This will lay the foundation for improving rehabilitation outcomes for stroke patients and promote the development of this field.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization: T.S. and K.Z.; Methodology: T.S.; Software: T.S.; Validation: T.S., K.Z. and Z.Y.; Formal analysis: T.S.; Investigation: K.Z.; Resources: Z.Y.; Data curation: Y.L.; Writing&#x02014;original draft preparation: S.G.; Writing&#x02014;review and editing: T.S.; Visualization: S.G.; Supervision: X.L.; Project administration: Y.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The dataset is not publicly available.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The company was not involved in the study design, collection, analysis, interpretation of data, the writing of this article or the decision to submit it for publication. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01057"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Artemiadis</surname><given-names>P.K.</given-names></name>
</person-group><article-title>EMG-based robot control interfaces: Past, present and future</article-title><source>Adv. Robot. Autom.</source><year>2012</year><volume>1</volume><fpage>1</fpage><lpage>3</lpage><pub-id pub-id-type="doi">10.4172/2168-9695.1000e107</pub-id></element-citation></ref><ref id="B2-sensors-25-01057"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Blaabjerg</surname><given-names>F.</given-names></name>
</person-group><article-title>Artificial intelligence-aided thermal model considering cross-coupling effects</article-title><source>Trans. Power Electron.</source><year>2020</year><volume>35</volume><fpage>9998</fpage><lpage>10002</lpage><pub-id pub-id-type="doi">10.1109/TPEL.2020.2980240</pub-id></element-citation></ref><ref id="B3-sensors-25-01057"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Vogel</surname><given-names>J.</given-names></name>
<name><surname>Hagengruber</surname><given-names>A.</given-names></name>
</person-group><article-title>An sEMG-based interface to give people with severe muscular atrophy control over assistive devices</article-title><source>Proceedings of the 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>18&#x02013;21 July 2018</conf-date><fpage>2136</fpage><lpage>2141</lpage><pub-id pub-id-type="doi">10.1109/EMBC.2018.8512689</pub-id></element-citation></ref><ref id="B4-sensors-25-01057"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Weitschat</surname><given-names>R.</given-names></name>
<name><surname>Dietrich</surname><given-names>A.</given-names></name>
<name><surname>Vogel</surname><given-names>J.</given-names></name>
</person-group><article-title>Online motion generation for mirroring human arm motion</article-title><source>Proceedings of the 2016 IEEE International Conference on Robotics and Automation</source><conf-loc>Stockholm, Sweden</conf-loc><conf-date>16&#x02013;21 May 2016</conf-date><fpage>4245</fpage><lpage>4250</lpage><pub-id pub-id-type="doi">10.1109/ICRA.2016.7487620</pub-id></element-citation></ref><ref id="B5-sensors-25-01057"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Artemiadis</surname><given-names>P.K.</given-names></name>
<name><surname>Kyriakopoulos</surname><given-names>K.J.</given-names></name>
</person-group><article-title>An EMG-based robot control scheme robust to time-varying EMG signal features</article-title><source>Trans. Inf. Technol. Biomed.</source><year>2010</year><volume>14</volume><fpage>582</fpage><lpage>588</lpage><pub-id pub-id-type="doi">10.1109/TITB.2010.2040832</pub-id><pub-id pub-id-type="pmid">20172839</pub-id>
</element-citation></ref><ref id="B6-sensors-25-01057"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Artemiadis</surname><given-names>P.K.</given-names></name>
<name><surname>Kyriakopoulos</surname><given-names>K.J.</given-names></name>
</person-group><article-title>EMG-based control of a robot arm using low-dimensional embeddings</article-title><source>IEEE Trans. Robot.</source><year>2010</year><volume>26</volume><fpage>393</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1109/TRO.2009.2039378</pub-id></element-citation></ref><ref id="B7-sensors-25-01057"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Artemiadis</surname><given-names>P.K.</given-names></name>
<name><surname>Kyriakopoulos</surname><given-names>K.J.</given-names></name>
</person-group><article-title>Estimating arm motion and force using EMG signals: On the control of exoskeletons</article-title><source>Proceedings of the 2008 IEEE/RSJ International Conference on Intelligent Robots and Systems</source><conf-loc>Nice, France</conf-loc><conf-date>22&#x02013;26 September 2008</conf-date><fpage>279</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1109/IROS.2008.4650949</pub-id></element-citation></ref><ref id="B8-sensors-25-01057"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shenoy</surname><given-names>P.</given-names></name>
<name><surname>Miller</surname><given-names>K.J.</given-names></name>
<name><surname>Crawford</surname><given-names>B.</given-names></name>
<name><surname>Rao</surname><given-names>R.P.</given-names></name>
</person-group><article-title>Online electromyographic control of a robotic prosthesis</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2008</year><volume>55</volume><fpage>1128</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1109/TBME.2007.909536</pub-id><pub-id pub-id-type="pmid">18334405</pub-id>
</element-citation></ref><ref id="B9-sensors-25-01057"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Murillo</surname><given-names>P.U.</given-names></name>
<name><surname>Moreno</surname><given-names>R.J.</given-names></name>
<name><surname>Avil&#x000e9;s</surname><given-names>O.</given-names></name>
</person-group><article-title>Individual robotic arms manipulator control employing electromyographic signals acquired by myo armbands</article-title><source>Int. J. Appl. Eng. Re</source><year>2016</year><volume>11</volume><fpage>11241</fpage><lpage>11249</lpage></element-citation></ref><ref id="B10-sensors-25-01057"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Morais</surname><given-names>G.D.</given-names></name>
<name><surname>Neves</surname><given-names>L.C.</given-names></name>
<name><surname>Masiero</surname><given-names>A.A.</given-names></name>
<name><surname>de Castro</surname><given-names>M.C.F.</given-names></name>
</person-group><article-title>Application of Myo Armband System to Control a Robot Interface</article-title><source>BIOSIGNALS</source><year>2016</year><volume>4</volume><fpage>227</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.5220/0005706302270231</pub-id></element-citation></ref><ref id="B11-sensors-25-01057"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aviles</surname><given-names>M.</given-names></name>
<name><surname>S&#x000e1;nchez-Reyes</surname><given-names>L.M.</given-names></name>
<name><surname>Fuentes-Aguilar</surname><given-names>R.Q.</given-names></name>
<name><surname>Toledo-P&#x000e9;rez</surname><given-names>D.C.</given-names></name>
<name><surname>Rodr&#x000ed;guez-Res&#x000e9;ndiz</surname><given-names>J.</given-names></name>
</person-group><article-title>A novel methodology for classifying EMG movements based on SVM and genetic algorithms</article-title><source>Micromachines</source><year>2022</year><volume>13</volume><elocation-id>2108</elocation-id><pub-id pub-id-type="doi">10.3390/mi13122108</pub-id><pub-id pub-id-type="pmid">36557408</pub-id>
</element-citation></ref><ref id="B12-sensors-25-01057"><label>12.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Tahmid</surname><given-names>S.</given-names></name>
<name><surname>Font-Llagunes</surname><given-names>J.M.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
</person-group><article-title>Upper extremity joint torque estimation through an EMG-driven model</article-title><source>International Design Engineering Technical Conferences and Computers and Information in Engineering Conference</source><publisher-name>American Society of Mechanical Engineers</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2022</year><volume>Volume 86212</volume><fpage>V002T02A026</fpage><pub-id pub-id-type="doi">10.1115/DETC2022-89952</pub-id></element-citation></ref><ref id="B13-sensors-25-01057"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>e Silva</surname><given-names>S.A.V.</given-names></name>
<name><surname>Coch</surname><given-names>V.B.</given-names></name>
<name><surname>de Oliveira Pinto</surname><given-names>M.B.</given-names></name>
<name><surname>de Oliveira</surname><given-names>V.M.</given-names></name>
</person-group><article-title>Assistive Robotics: Robotic Trajectory Planning for Upper Extremity Rehabilitation in Patients with Hemiparesis</article-title><source>Proceedings of the IECON 2021&#x02013;47th Annual Conference of the IEEE Industrial Electronics Society</source><conf-loc>Toronto, ON, Canada</conf-loc><conf-date>13&#x02013;16 October 2021</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/IECON48115.2021.9589760</pub-id></element-citation></ref><ref id="B14-sensors-25-01057"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Holzbaur KR</surname><given-names>S.</given-names></name>
<name><surname>Murray</surname><given-names>W.M.</given-names></name>
<name><surname>Delp</surname><given-names>S.L.</given-names></name>
</person-group><article-title>A model of the upper extremity for simulating musculoskeletal surgery and analyzing neuromuscular control</article-title><source>Ann. Biomed. Eng.</source><year>2005</year><volume>33</volume><fpage>829</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1007/s10439-005-3320-7</pub-id><pub-id pub-id-type="pmid">16078622</pub-id>
</element-citation></ref><ref id="B15-sensors-25-01057"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Saul</surname><given-names>K.R.</given-names></name>
<name><surname>Hu</surname><given-names>X.</given-names></name>
<name><surname>Goehler</surname><given-names>C.M.</given-names></name>
<name><surname>Vidt</surname><given-names>M.E.</given-names></name>
<name><surname>Daly</surname><given-names>M.</given-names></name>
<name><surname>Velisar</surname><given-names>A.</given-names></name>
<name><surname>Murray</surname><given-names>W.M.</given-names></name>
</person-group><article-title>Benchmarking of dynamic simulation predictions in two software platforms using an upper limb musculoskeletal model</article-title><source>Comput. Methods Biomech. Biomed. Eng.</source><year>2015</year><volume>18</volume><fpage>1445</fpage><lpage>1458</lpage><pub-id pub-id-type="doi">10.1080/10255842.2014.916698</pub-id></element-citation></ref><ref id="B16-sensors-25-01057"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Guo</surname><given-names>S.</given-names></name>
<name><surname>Yang</surname><given-names>Z.</given-names></name>
<name><surname>Hirata</surname><given-names>H.</given-names></name>
<name><surname>Tamiya</surname><given-names>T.</given-names></name>
</person-group><article-title>A home-based bilateral rehabilitation system with sEMG-based real-time variable stiffness</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2020</year><volume>25</volume><fpage>1529</fpage><lpage>1541</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2020.3027303</pub-id></element-citation></ref><ref id="B17-sensors-25-01057"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lloyd</surname><given-names>D.G.</given-names></name>
<name><surname>Besier</surname><given-names>T.F.</given-names></name>
</person-group><article-title>An EMG-driven musculoskeletal model to estimate muscle forces and knee joint moments in vivo</article-title><source>J. Biomech.</source><year>2003</year><volume>36</volume><fpage>765</fpage><lpage>776</lpage><pub-id pub-id-type="doi">10.1016/S0021-9290(03)00010-1</pub-id><pub-id pub-id-type="pmid">12742444</pub-id>
</element-citation></ref><ref id="B18-sensors-25-01057"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hill</surname><given-names>A.V.</given-names></name>
</person-group><article-title>The heat of shortening and the dynamic constants of muscle. Proceedings of the Royal Society of London</article-title><source>Ser. B-Biol. Sci.</source><year>1938</year><volume>126</volume><fpage>136</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1098/rspb.1938.0050</pub-id></element-citation></ref><ref id="B19-sensors-25-01057"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>Y.</given-names></name>
<name><surname>Jung</surname><given-names>Y.</given-names></name>
<name><surname>Choi</surname><given-names>W.</given-names></name>
<name><surname>Lee</surname><given-names>K.</given-names></name>
<name><surname>Koo</surname><given-names>S.</given-names></name>
</person-group><article-title>Similarities and differences between musculoskeletal simulations of OpenSim and AnyBody modeling system</article-title><source>J. Mech. Sci. Technol.</source><year>2018</year><volume>32</volume><fpage>6037</fpage><lpage>6044</lpage><pub-id pub-id-type="doi">10.1007/s12206-018-1154-0</pub-id></element-citation></ref><ref id="B20-sensors-25-01057"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Gastaldi</surname><given-names>L.</given-names></name>
<name><surname>Panero</surname><given-names>E.</given-names></name>
<name><surname>Rosso</surname><given-names>V.</given-names></name>
<name><surname>Pastorelli</surname><given-names>S.</given-names></name>
<name><surname>Vieira</surname><given-names>T.</given-names></name>
<name><surname>Botter</surname><given-names>A.</given-names></name>
</person-group><article-title>Upper limbs musculoskeletal Opensim model: Customization and assessment</article-title><source>The International Conference of IFToMM ITALY</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>162</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-55807-9_19</pub-id></element-citation></ref><ref id="B21-sensors-25-01057"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Green</surname><given-names>M.</given-names></name>
<name><surname>Hong</surname><given-names>Y.N.G.</given-names></name>
<name><surname>Roh</surname><given-names>J.</given-names></name>
<name><surname>Fregly</surname><given-names>B.J.</given-names></name>
</person-group><article-title>Computational modeling and simulation of closed chain arm-robot multibody dynamic systems in OpenSim</article-title><source>Multibody Syst. Dyn.</source><year>2022</year><volume>56</volume><fpage>313</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1007/s11044-022-09847-8</pub-id></element-citation></ref><ref id="B22-sensors-25-01057"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Ruggiero</surname><given-names>A.</given-names></name>
<name><surname>Sicilia</surname><given-names>A.</given-names></name>
</person-group><article-title>Development of a multibody biomechanical model of the human upper limb</article-title><source>Biotribology</source><publisher-name>CRC Press</publisher-name><publisher-loc>Boca Raton, FL, USA</publisher-loc><year>2021</year><fpage>91</fpage><lpage>121</lpage></element-citation></ref><ref id="B23-sensors-25-01057"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Atzori</surname><given-names>M.</given-names></name>
<name><surname>Gijsberts</surname><given-names>A.</given-names></name>
<name><surname>Kuzborskij</surname><given-names>I.</given-names></name>
<name><surname>Elsig</surname><given-names>S.</given-names></name>
<name><surname>Hager</surname><given-names>A.G.M.</given-names></name>
<name><surname>Deriaz</surname><given-names>O.</given-names></name>
<name><surname>Castellini</surname><given-names>C.</given-names></name>
<name><surname>M&#x000fc;ller</surname><given-names>H.</given-names></name>
<name><surname>Caputo</surname><given-names>B.</given-names></name>
</person-group><article-title>Characterization of a benchmark database for myoelectric movement classification</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2014</year><volume>23</volume><fpage>73</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2014.2328495</pub-id><pub-id pub-id-type="pmid">25486646</pub-id>
</element-citation></ref><ref id="B24-sensors-25-01057"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Potvin</surname><given-names>J.R.</given-names></name>
<name><surname>Norman</surname><given-names>R.W.</given-names></name>
<name><surname>McGill</surname><given-names>S.M.</given-names></name>
</person-group><article-title>Mechanically corrected EMG for the continuous estimation of erector spinae muscle loading during repetitive lifting</article-title><source>Eur. J. Appl. Physiol. Occup. Physiol.</source><year>1996</year><volume>74</volume><fpage>119</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1007/BF00376504</pub-id><pub-id pub-id-type="pmid">8891510</pub-id>
</element-citation></ref><ref id="B25-sensors-25-01057"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>K.</given-names></name>
<name><surname>Chen</surname><given-names>F.</given-names></name>
</person-group><article-title>Research on sEMG Gesture Recognition Based on Hybrid Dilated Convolutional Neural Network Combining Bidirectional Gated Recurrent Unit and Attention Mechanism</article-title><source>Proceedings of the 2021 China Automation Congress</source><conf-loc>Beijing, China</conf-loc><conf-date>22&#x02013;24 October 2021</conf-date><fpage>3760</fpage><lpage>3763</lpage><pub-id pub-id-type="doi">10.1109/CAC53003.2021.9727886</pub-id></element-citation></ref><ref id="B26-sensors-25-01057"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Seo</surname><given-names>N.J.</given-names></name>
<name><surname>Barry</surname><given-names>A.</given-names></name>
<name><surname>Ghassemi</surname><given-names>M.</given-names></name>
<name><surname>Triandafilou</surname><given-names>K.M.</given-names></name>
<name><surname>Stoykov</surname><given-names>M.E.</given-names></name>
<name><surname>Vidakovic</surname><given-names>L.</given-names></name>
<name><surname>Roth</surname><given-names>E.</given-names></name>
<name><surname>Kamper</surname><given-names>D.G.</given-names></name>
</person-group><article-title>Use of an EMG-controlled game as a therapeutic tool to retrain hand muscle activation patterns following stroke: A pilot study</article-title><source>J. Neurol. Phys. Ther.</source><year>2022</year><volume>46</volume><fpage>198</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1097/NPT.0000000000000398</pub-id><pub-id pub-id-type="pmid">35320135</pub-id>
</element-citation></ref><ref id="B27-sensors-25-01057"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Clarke</surname><given-names>A.K.</given-names></name>
<name><surname>Atashzar</surname><given-names>S.F.</given-names></name>
<name><surname>Del Vecchio</surname><given-names>A.</given-names></name>
<name><surname>Barsakcioglu</surname><given-names>D.</given-names></name>
<name><surname>Muceli</surname><given-names>S.</given-names></name>
<name><surname>Bentley</surname><given-names>P.</given-names></name>
<name><surname>Urh</surname><given-names>F.</given-names></name>
<name><surname>Holobar</surname><given-names>A.</given-names></name>
<name><surname>Farina</surname><given-names>D.</given-names></name>
</person-group><article-title>Deep learning for robust decomposition of high-density surface EMG signals</article-title><source>Trans. Biomed. Eng.</source><year>2020</year><volume>68</volume><fpage>526</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.1109/TBME.2020.3006508</pub-id><pub-id pub-id-type="pmid">32746049</pub-id>
</element-citation></ref><ref id="B28-sensors-25-01057"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Akiba</surname><given-names>T.</given-names></name>
<name><surname>Sano</surname><given-names>S.</given-names></name>
<name><surname>Yanase</surname><given-names>T.</given-names></name>
<name><surname>Ohta</surname><given-names>T.</given-names></name>
<name><surname>Koyama</surname><given-names>M.</given-names></name>
</person-group><article-title>Optuna: A next-generation hyperparameter optimization framework</article-title><source>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &#x00026; Data Mining</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>4&#x02013;8 August 2019</conf-date><fpage>2623</fpage><lpage>2631</lpage></element-citation></ref><ref id="B29-sensors-25-01057"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>J.</given-names></name>
<name><surname>Ye</surname><given-names>C.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Zhao</surname><given-names>H.</given-names></name>
<name><surname>Ding</surname><given-names>H.</given-names></name>
<name><surname>Luo</surname><given-names>M.</given-names></name>
</person-group><article-title>Contour error-based optimization of the end-effector pose of a 6 degree-of-freedom serial robot in milling operation</article-title><source>Robot. Comput.-Integr. Manuf.</source><year>2022</year><volume>73</volume><fpage>102257</fpage><pub-id pub-id-type="doi">10.1016/j.rcim.2021.102257</pub-id></element-citation></ref><ref id="B30-sensors-25-01057"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>T.</given-names></name>
<name><surname>Song</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>Q.</given-names></name>
</person-group><article-title>A novel method to identify DH parameters of the rigid serial-link robot based on a geometry mode</article-title><source>Ind. Robot Int. J. Robot. Res. Appl.</source><year>2021</year><volume>48</volume><fpage>157</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1108/IR-05-2020-0103</pub-id></element-citation></ref><ref id="B31-sensors-25-01057"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Guo</surname><given-names>S.</given-names></name>
<name><surname>Sun</surname><given-names>Q.</given-names></name>
</person-group><article-title>Development and assist-as-needed control of an end-effector upper limb rehabilitation robot</article-title><source>Appl. Sci.</source><year>2020</year><volume>10</volume><elocation-id>6684</elocation-id><pub-id pub-id-type="doi">10.3390/app10196684</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01057-f001"><label>Figure 1</label><caption><p>iReMo<sup>&#x000ae;</sup> upper limb rehabilitation robot system.</p></caption><graphic xlink:href="sensors-25-01057-g001" position="float"/></fig><fig position="float" id="sensors-25-01057-f002"><label>Figure 2</label><caption><p>CO-PTP task path.</p></caption><graphic xlink:href="sensors-25-01057-g002" position="float"/></fig><fig position="float" id="sensors-25-01057-f003"><label>Figure 3</label><caption><p>Upper limb shoulder and elbow joint musculoskeletal model content.</p></caption><graphic xlink:href="sensors-25-01057-g003" position="float"/></fig><fig position="float" id="sensors-25-01057-f004"><label>Figure 4</label><caption><p>Musculoskeletal model in OpenSim.</p></caption><graphic xlink:href="sensors-25-01057-g004" position="float"/></fig><fig position="float" id="sensors-25-01057-f005"><label>Figure 5</label><caption><p>Experimental and simulation of shoulder joint abduction and adduction.</p></caption><graphic xlink:href="sensors-25-01057-g005" position="float"/></fig><fig position="float" id="sensors-25-01057-f006"><label>Figure 6</label><caption><p>Simulation results of shoulder joint abduction and adduction.</p></caption><graphic xlink:href="sensors-25-01057-g006" position="float"/></fig><fig position="float" id="sensors-25-01057-f007"><label>Figure 7</label><caption><p>Experimental and simulation of shoulder flexion and extension.</p></caption><graphic xlink:href="sensors-25-01057-g007" position="float"/></fig><fig position="float" id="sensors-25-01057-f008"><label>Figure 8</label><caption><p>Simulation results of shoulder joint flexion and extension.</p></caption><graphic xlink:href="sensors-25-01057-g008" position="float"/></fig><fig position="float" id="sensors-25-01057-f009"><label>Figure 9</label><caption><p>Experimental and simulation of elbow flexion and extension.</p></caption><graphic xlink:href="sensors-25-01057-g009" position="float"/></fig><fig position="float" id="sensors-25-01057-f010"><label>Figure 10</label><caption><p>Simulation results of elbow flexion and extension.</p></caption><graphic xlink:href="sensors-25-01057-g010" position="float"/></fig><fig position="float" id="sensors-25-01057-f011"><label>Figure 11</label><caption><p>Sensor placement diagram.</p></caption><graphic xlink:href="sensors-25-01057-g011a" position="float"/><graphic xlink:href="sensors-25-01057-g011b" position="float"/></fig><fig position="float" id="sensors-25-01057-f012"><label>Figure 12</label><caption><p>Data acquisition.</p></caption><graphic xlink:href="sensors-25-01057-g012" position="float"/></fig><fig position="float" id="sensors-25-01057-f013"><label>Figure 13</label><caption><p>Screenshot of the interface.</p></caption><graphic xlink:href="sensors-25-01057-g013" position="float"/></fig><fig position="float" id="sensors-25-01057-f014"><label>Figure 14</label><caption><p>Isometric contraction experiment.</p></caption><graphic xlink:href="sensors-25-01057-g014" position="float"/></fig><fig position="float" id="sensors-25-01057-f015"><label>Figure 15</label><caption><p>Muscle contraction sequence.</p></caption><graphic xlink:href="sensors-25-01057-g015" position="float"/></fig><fig position="float" id="sensors-25-01057-f016"><label>Figure 16</label><caption><p>Examples of isometric contraction settings.</p></caption><graphic xlink:href="sensors-25-01057-g016" position="float"/></fig><fig position="float" id="sensors-25-01057-f017"><label>Figure 17</label><caption><p>Comparison between raw data and preprocessed data.</p></caption><graphic xlink:href="sensors-25-01057-g017" position="float"/></fig><fig position="float" id="sensors-25-01057-f018"><label>Figure 18</label><caption><p>Common CNN and MLCNN structure.</p></caption><graphic xlink:href="sensors-25-01057-g018" position="float"/></fig><fig position="float" id="sensors-25-01057-f019"><label>Figure 19</label><caption><p>The CNN and MLCNN structures in this study.</p></caption><graphic xlink:href="sensors-25-01057-g019a" position="float"/><graphic xlink:href="sensors-25-01057-g019b" position="float"/></fig><fig position="float" id="sensors-25-01057-f020"><label>Figure 20</label><caption><p>Accuracy and loss of the training and confusion matrix.</p></caption><graphic xlink:href="sensors-25-01057-g020" position="float"/></fig><fig position="float" id="sensors-25-01057-f021"><label>Figure 21</label><caption><p>Accuracy of alternating convolutional CNN and MLCNN in cross-validation.</p></caption><graphic xlink:href="sensors-25-01057-g021" position="float"/></fig><fig position="float" id="sensors-25-01057-f022"><label>Figure 22</label><caption><p>Accuracy of MLCNN models in different areas.</p></caption><graphic xlink:href="sensors-25-01057-g022" position="float"/></fig><fig position="float" id="sensors-25-01057-f023"><label>Figure 23</label><caption><p>Schematic diagram of system control.</p></caption><graphic xlink:href="sensors-25-01057-g023" position="float"/></fig><fig position="float" id="sensors-25-01057-f024"><label>Figure 24</label><caption><p>Four standard test paths.</p></caption><graphic xlink:href="sensors-25-01057-g024" position="float"/></fig><fig position="float" id="sensors-25-01057-f025"><label>Figure 25</label><caption><p>Experimental interface and description.</p></caption><graphic xlink:href="sensors-25-01057-g025" position="float"/></fig><fig position="float" id="sensors-25-01057-f026"><label>Figure 26</label><caption><p>Accuracy of the model M during the optimization process.</p></caption><graphic xlink:href="sensors-25-01057-g026" position="float"/></fig><fig position="float" id="sensors-25-01057-f027"><label>Figure 27</label><caption><p>Hyperparameter combinations and their importance.</p></caption><graphic xlink:href="sensors-25-01057-g027" position="float"/></fig><fig position="float" id="sensors-25-01057-f028"><label>Figure 28</label><caption><p>Accuracy of the best model in each area.</p></caption><graphic xlink:href="sensors-25-01057-g028" position="float"/></fig><fig position="float" id="sensors-25-01057-f029"><label>Figure 29</label><caption><p>Circular trajectory distribution diagram of closed-loop control and open-loop system.</p></caption><graphic xlink:href="sensors-25-01057-g029" position="float"/></fig><fig position="float" id="sensors-25-01057-f030"><label>Figure 30</label><caption><p>Smoothness of circular path in open-loop and closed-loop system.</p></caption><graphic xlink:href="sensors-25-01057-g030" position="float"/></fig><fig position="float" id="sensors-25-01057-f031"><label>Figure 31</label><caption><p>Deviation of circular path in open-loop and closed-loop system.</p></caption><graphic xlink:href="sensors-25-01057-g031" position="float"/></fig><fig position="float" id="sensors-25-01057-f032"><label>Figure 32</label><caption><p>Standardized path length of circular path in open-loop and closed-loop system.</p></caption><graphic xlink:href="sensors-25-01057-g032" position="float"/></fig><fig position="float" id="sensors-25-01057-f033"><label>Figure 33</label><caption><p>CO-PTP trajectories distribution map.</p></caption><graphic xlink:href="sensors-25-01057-g033" position="float"/></fig><fig position="float" id="sensors-25-01057-f034"><label>Figure 34</label><caption><p>Smoothness of CO-PTP path in closed-loop system.</p></caption><graphic xlink:href="sensors-25-01057-g034" position="float"/></fig><fig position="float" id="sensors-25-01057-f035"><label>Figure 35</label><caption><p>Deviation of CO-PTP in closed-loop system.</p></caption><graphic xlink:href="sensors-25-01057-g035" position="float"/></fig><fig position="float" id="sensors-25-01057-f036"><label>Figure 36</label><caption><p>Standardized path length of CO-PTP path in closed-loop system.</p></caption><graphic xlink:href="sensors-25-01057-g036" position="float"/></fig><fig position="float" id="sensors-25-01057-f037"><label>Figure 37</label><caption><p>Sinusoidal trajectories distribution map.</p></caption><graphic xlink:href="sensors-25-01057-g037" position="float"/></fig><fig position="float" id="sensors-25-01057-f038"><label>Figure 38</label><caption><p>Smoothness of sinusoidal path in open-loop and closed-loop control.</p></caption><graphic xlink:href="sensors-25-01057-g038" position="float"/></fig><fig position="float" id="sensors-25-01057-f039"><label>Figure 39</label><caption><p>Deviation of sinusoidal path in open-loop and closed-loop control.</p></caption><graphic xlink:href="sensors-25-01057-g039" position="float"/></fig><fig position="float" id="sensors-25-01057-f040"><label>Figure 40</label><caption><p>Standardized path length of sinusoidal path in open-loop and closed-loop control.</p></caption><graphic xlink:href="sensors-25-01057-g040" position="float"/></fig><fig position="float" id="sensors-25-01057-f041"><label>Figure 41</label><caption><p>Box plots of two trajectory indicators in open-loop control.</p></caption><graphic xlink:href="sensors-25-01057-g041" position="float"/></fig><fig position="float" id="sensors-25-01057-f042"><label>Figure 42</label><caption><p>Box plots of three trajectory indicators in close-loop control.</p></caption><graphic xlink:href="sensors-25-01057-g042" position="float"/></fig><table-wrap position="float" id="sensors-25-01057-t001"><object-id pub-id-type="pii">sensors-25-01057-t001_Table 1</object-id><label>Table 1</label><caption><p>Tunable parameters table.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hyperparameters</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter Definition</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">OMV</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Optimization Range</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">learning_rate</td><td align="center" valign="middle" rowspan="1" colspan="1">Learning Rate</td><td align="center" valign="middle" rowspan="1" colspan="1">1 &#x000d7; 10<sup>&#x02212;3</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">[1 &#x000d7; 10<sup>&#x02212;5</sup>, 1 &#x000d7; 10<sup>&#x02212;2</sup>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">num_filters_1</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of Filters in Convolutional Layer 1</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td><td align="center" valign="middle" rowspan="1" colspan="1">[10, 128]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">num_filters_2</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of Filters in Convolutional Layer 2</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td><td align="center" valign="middle" rowspan="1" colspan="1">[20, 256]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">dropout_rate</td><td align="center" valign="middle" rowspan="1" colspan="1">Dropout Rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">[0.1, 0.5]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">dense_units</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of Neurons in Fully Connected Layer</td><td align="center" valign="middle" rowspan="1" colspan="1">128</td><td align="center" valign="middle" rowspan="1" colspan="1">[64, 512]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">batch_size</td><td align="center" valign="middle" rowspan="1" colspan="1">Batch Size</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td><td align="center" valign="middle" rowspan="1" colspan="1">[32, 128]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">l2_reg</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L2 Regularization Coefficient</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[1 &#x000d7; 10<sup>&#x02212;7</sup>, 1 &#x000d7; 10<sup>&#x02212;3</sup>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01057-t002"><object-id pub-id-type="pii">sensors-25-01057-t002_Table 2</object-id><label>Table 2</label><caption><p>Subject Information.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Subjects</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Age</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Gender</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Height (cm)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Weight (kg)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">26</td><td align="center" valign="middle" rowspan="1" colspan="1">man</td><td align="center" valign="middle" rowspan="1" colspan="1">172</td><td align="center" valign="middle" rowspan="1" colspan="1">66</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">28</td><td align="center" valign="middle" rowspan="1" colspan="1">man</td><td align="center" valign="middle" rowspan="1" colspan="1">170</td><td align="center" valign="middle" rowspan="1" colspan="1">60</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">24</td><td align="center" valign="middle" rowspan="1" colspan="1">man</td><td align="center" valign="middle" rowspan="1" colspan="1">175</td><td align="center" valign="middle" rowspan="1" colspan="1">62</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">24</td><td align="center" valign="middle" rowspan="1" colspan="1">man</td><td align="center" valign="middle" rowspan="1" colspan="1">176</td><td align="center" valign="middle" rowspan="1" colspan="1">58</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">24</td><td align="center" valign="middle" rowspan="1" colspan="1">man</td><td align="center" valign="middle" rowspan="1" colspan="1">192</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">24</td><td align="center" valign="middle" rowspan="1" colspan="1">man</td><td align="center" valign="middle" rowspan="1" colspan="1">173</td><td align="center" valign="middle" rowspan="1" colspan="1">60</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">man</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">185</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01057-t003"><object-id pub-id-type="pii">sensors-25-01057-t003_Table 3</object-id><label>Table 3</label><caption><p>The hyperparameters and accuracy of the optimized models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Regions</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">L</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">M</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">R</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">learning_rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0016036142</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0007133843</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0014916109</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">num_filters_1</td><td align="center" valign="middle" rowspan="1" colspan="1">113</td><td align="center" valign="middle" rowspan="1" colspan="1">106</td><td align="center" valign="middle" rowspan="1" colspan="1">62</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">num_filters_2</td><td align="center" valign="middle" rowspan="1" colspan="1">219</td><td align="center" valign="middle" rowspan="1" colspan="1">44</td><td align="center" valign="middle" rowspan="1" colspan="1">74</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">dropout_rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4680741832</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2770565315</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4362810779</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">dense_units</td><td align="center" valign="middle" rowspan="1" colspan="1">466</td><td align="center" valign="middle" rowspan="1" colspan="1">261</td><td align="center" valign="middle" rowspan="1" colspan="1">484</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">batch_size</td><td align="center" valign="middle" rowspan="1" colspan="1">75</td><td align="center" valign="middle" rowspan="1" colspan="1">39</td><td align="center" valign="middle" rowspan="1" colspan="1">128</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">l2_reg</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2438425879</td><td align="center" valign="middle" rowspan="1" colspan="1">4.7615135451</td><td align="center" valign="middle" rowspan="1" colspan="1">1.9060718778</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.26%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.87%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.01%</td></tr></tbody></table></table-wrap></floats-group></article>