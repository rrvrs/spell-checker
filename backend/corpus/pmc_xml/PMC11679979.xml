<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11679979</article-id><article-id pub-id-type="doi">10.3390/s24247904</article-id><article-id pub-id-type="publisher-id">sensors-24-07904</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Personalized Shared Control for Automated Vehicles Considering Driving Capability and Styles</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7927-0772</contrib-id><name><surname>Sun</surname><given-names>Bohua</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-24-07904" ref-type="aff">1</xref><xref rid="c1-sensors-24-07904" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Shan</surname><given-names>Yingjie</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-24-07904" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Guanpu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-24-07904" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Shuai</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af2-sensors-24-07904" ref-type="aff">2</xref><xref rid="af3-sensors-24-07904" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Xie</surname><given-names>Fei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-24-07904" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Mateo Sanguino</surname><given-names>Tom&#x000e1;s</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-24-07904"><label>1</label>College of Automotive Engineering, the National Key Laboratory of Automotive Chassis Integration and Bionics, Jilin University, Changchun 130025, China; <email>shanyj23@mails.jlu.edu.cn</email> (Y.S.); <email>xiefei@jlu.edu.cn</email> (F.X.)</aff><aff id="af2-sensors-24-07904"><label>2</label>College of Intelligence and Computing, Tianjin University, Tianjin 300354, China; <email>zhaoshuai@catarc.ac.cn</email></aff><aff id="af3-sensors-24-07904"><label>3</label>Automotive Data Center, CATARC, Tianjin 300000, China</aff><author-notes><corresp id="c1-sensors-24-07904"><label>*</label>Correspondence: <email>sunbohua@jlu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>11</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2024</year></pub-date><volume>24</volume><issue>24</issue><elocation-id>7904</elocation-id><history><date date-type="received"><day>26</day><month>10</month><year>2024</year></date><date date-type="rev-recd"><day>05</day><month>12</month><year>2024</year></date><date date-type="accepted"><day>06</day><month>12</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 by the authors.</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>The shared control system has been a key technology framework and trend, with its advantages in overcoming the performance shortage of safety and comfort in automated vehicles. Understanding human drivers&#x02019; driving capabilities and styles is the key to improving system performance, in particular, the acceptance by and adaption of shared control vehicles to human drivers. In this research, personalized shared control considering drivers&#x02019; main human factors is proposed. A simulated scenario generation method for human factors was established. Drivers&#x02019; driving capabilities were defined and evaluated to improve the rationality of the driving authority allocation. Drivers&#x02019; driving styles were analyzed, characterized, and evaluated in a field test for the intention-aware personalized automated subsystem. A personalized shared control framework is proposed based on the driving capabilities and styles, and its evaluation criteria were established, including driving safety, comfort, and workload. The personalized shared control system was evaluated in a human-in-the-loop simulation platform and a field test based on an automated vehicle. The results show that the proposed system could achieve better performances in terms of different driving capabilities, styles, and complex scenarios than those only driven by human drivers or automated systems.</p></abstract><kwd-group><kwd>automated vehicle</kwd><kwd>driving capability</kwd><kwd>driving style</kwd><kwd>human&#x02013;machine hybrid</kwd><kwd>decision-making</kwd><kwd>shared control</kwd></kwd-group><funding-group><award-group><funding-source>National Nature Science Foundation of China</funding-source><award-id>52102457</award-id><award-id>52394261</award-id></award-group><award-group><funding-source>Science and technology research project of the education department of Jilin Province</funding-source><award-id>JJKH20241266KJ</award-id></award-group><award-group><funding-source>Natural Science Foundation of Jilin Province</funding-source><award-id>20220101213JC</award-id></award-group><award-group><funding-source>Natural Science Foundation of Sichuan Province</funding-source><award-id>23NSFSC4461</award-id></award-group><award-group><funding-source>Science and Technology Development Project of Jilin Province</funding-source><award-id>202302013</award-id></award-group><funding-statement>This research was funded partly by the National Nature Science Foundation of China under grant 52102457, in part by the Science and technology research project of the education department of Jilin Province under grant JJKH20241266KJ, in part by the Natural Science Foundation of Jilin Province under grant 20220101213JC, in part by the Natural Science Foundation of Sichuan Province under grant 23NSFSC4461, in part by the National Nature Science Foundation of China under grant 52394261, and in part by the Science and Technology Development Project of Jilin Province under grant 202302013.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-24-07904"><title>1. Introduction</title><p>Automated vehicles have become a dominant trend in improving traffic safety and transportation efficiency, given their advantages in reducing drivers&#x02019; driving loads and predicting traffic situations [<xref rid="B1-sensors-24-07904" ref-type="bibr">1</xref>,<xref rid="B2-sensors-24-07904" ref-type="bibr">2</xref>]. However, some obstacles still exist in enhancing the safety and adaptability of fully automated vehicles. Social dilemmas, represented by the tram issue and driver complacency, are technical problems that human society and ethics present to artificial intelligence theory in automated vehicles [<xref rid="B3-sensors-24-07904" ref-type="bibr">3</xref>]. The technique limitations of automated vehicles, such as poor perception accuracy and decision-making ability, restrict the transition process as well [<xref rid="B4-sensors-24-07904" ref-type="bibr">4</xref>,<xref rid="B5-sensors-24-07904" ref-type="bibr">5</xref>]. Therefore, the use of highly automated vehicles with a human-in-the-loop, which is called shared control, is likely to last for a long time and has been attracting increasing attention. Shared control can be defined as a safe, efficient, friendly, and stable driving mode formed by overcoming the decision-making conflict between the driver with social attributes and the automated system with logical attributes. The human&#x02013;machine cooperation mechanism constitutes the theory foundation of the shared control system, and the human factors are required to be evaluated in detail.</p><p>As the key technique to overcoming the human&#x02013;machine conflict in shared control, the driving authority allocation strategy (DAAS) needs to be analyzed in depth. The DAAS can be defined as the weight distribution method between the driver and the automated system, with the aim of achieving a safe, efficient, and stable system configuration [<xref rid="B6-sensors-24-07904" ref-type="bibr">6</xref>]. The DAAS consists of a switched type and a shared type, and the latter is subdivided into direct and indirect shared controls [<xref rid="B7-sensors-24-07904" ref-type="bibr">7</xref>]. With the development of by-wire and intelligent sensing techniques, the basic functionality of the DAAS has been improved, and indirect shared control has developed rapidly by virtue of its framework advantage, signal identification, velocity optimization, and H&#x0221e; control theories [<xref rid="B8-sensors-24-07904" ref-type="bibr">8</xref>,<xref rid="B9-sensors-24-07904" ref-type="bibr">9</xref>]. In recent years, in order to improve system safety, adaptability, and driver acceptability, the DAAS has gradually focused on the cognitive mechanism of human factor attributes, the cognitive capability of complex scenarios, and switching smoothness [<xref rid="B10-sensors-24-07904" ref-type="bibr">10</xref>,<xref rid="B11-sensors-24-07904" ref-type="bibr">11</xref>]. Drivers&#x02019; driving skills are modeled and evaluated based on the optimal driver preview model, overtaking model, and accident assessment model and the drivers&#x02019; learning processes and levels are revealed, mainly based on typical physical models [<xref rid="B12-sensors-24-07904" ref-type="bibr">12</xref>,<xref rid="B13-sensors-24-07904" ref-type="bibr">13</xref>]. Drivers&#x02019; driving statuses focus on drivers&#x02019; fatigue, sleepiness, distraction, and mood and are identified accurately based on biological signals, vehicle states, and driving actions, such as eye and head movements. Image detection and machine learning theories are used to analyze drivers&#x02019; take-over and reaction abilities, and they reveal the inducement and externalization of the driving status [<xref rid="B14-sensors-24-07904" ref-type="bibr">14</xref>,<xref rid="B15-sensors-24-07904" ref-type="bibr">15</xref>]. Drivers&#x02019; driving styles are classified and identified based on physical modeling or a data-driven approach, and the technique details are discussed in the next section. It can be seen that key human factors are analyzed and evaluated in a decoupled way, and few studies were conducted on the comprehensive representation of driving skills, statuses, and styles. The rationality of the DAAS still needs to be improved due to the lack of comprehensive human factors reflecting drivers&#x02019; time-varying abilities for vehicle control.</p><p>As the main component of a human-like automated driving pattern, a personalized driving pattern is produced by a driver&#x02019;s driving style [<xref rid="B16-sensors-24-07904" ref-type="bibr">16</xref>]. Understanding the drivers&#x02019; driving styles that make the shared control more human-like is the key to improving system performance, in particular, driver adaption and acceptance [<xref rid="B17-sensors-24-07904" ref-type="bibr">17</xref>]. Four human-like degrees, namely, none, low, medium, and high levels, were researched, and the medium degree received more attention based on its advantage of high computational efficiency and distinct personalized results [<xref rid="B18-sensors-24-07904" ref-type="bibr">18</xref>]). The classification and identification methods were mainly developed to obtain up to six typical driving styles [<xref rid="B19-sensors-24-07904" ref-type="bibr">19</xref>,<xref rid="B20-sensors-24-07904" ref-type="bibr">20</xref>]. The classification mode of driving styles is the primary task used to reflect the intrinsic mechanism of the human-like driving pattern and is mainly affected by scenarios, modal datasets, and characteristic parameter sets [<xref rid="B21-sensors-24-07904" ref-type="bibr">21</xref>]. No more than five types of driving styles produce optimal computational efficiency but are mainly used for fuel economy instead of drivers&#x02019; acceptability [<xref rid="B22-sensors-24-07904" ref-type="bibr">22</xref>]. Subjective and objective classification methods were researched individually by online or offline methods, and the average accuracy was about 85%, which was based on data-driven algorithms, and 80% was based on a questionnaire [<xref rid="B23-sensors-24-07904" ref-type="bibr">23</xref>]. The identification process of driving styles takes the moment-based array or time-series-based high-dimensional data as the model inputs, and the identification method is mainly based on machine learning and system modeling [<xref rid="B19-sensors-24-07904" ref-type="bibr">19</xref>,<xref rid="B24-sensors-24-07904" ref-type="bibr">24</xref>]. Data-driven methods have advantages in processing the human factor data with high-order nonlinearity and uncertainty, and the framework of the system model has low-order nonlinearity and certain physical interpretations. Considering the model&#x02019;s complexity, related methods are mainly conducted using offline methods to obtain analysis results [<xref rid="B25-sensors-24-07904" ref-type="bibr">25</xref>]. Furthermore, few studies focused on scenario generation methods specific to human factors, which are the main factors restricting the evaluation accuracy of human factors as well. Therefore, a combination of classification and identification processes for driving styles needs to be developed in depth, and its evaluation method combining online and offline methods and simulated scenarios for the personalized system needs to be established to improve evaluation accuracy.</p><p>The decision-making process is required to generate efficient and stable driving tasks in complex scenarios and becomes the core-level component in shared control [<xref rid="B26-sensors-24-07904" ref-type="bibr">26</xref>]. The DAAS and driving styles constitute internal constraints and determine the system features together with the external ones, such as scenarios or vehicle states [<xref rid="B27-sensors-24-07904" ref-type="bibr">27</xref>]. Thus, the system features of the decision-making process consist of self-learning, high-order nonlinearity, high data dependency, and personality, and the key challenge is how to deal with the uncertainties and improve driver acceptance [<xref rid="B28-sensors-24-07904" ref-type="bibr">28</xref>]. Two typical methods, the physical modeling-based and data-driven-based, are researched for decision-making logic. The physical modeling-based methods, such as the car-following model, the Pipes and Forbes model, and the overtaking maneuver model, aim to reveal the decision-making mechanisms of the human&#x02013;vehicle closed loop system [<xref rid="B29-sensors-24-07904" ref-type="bibr">29</xref>]. The model&#x02019;s accuracy is improved with the gradual refinement of driving skill sub-models, such as the driver preview model, the unified driver model, or the driver control model [<xref rid="B30-sensors-24-07904" ref-type="bibr">30</xref>]. The physical modeling-based decision-making models have clear physical meaning and can realize real-time and strong, robust decision-making tasks. However, these models poorly adapt to scenarios with high uncertainty due to the deterministic internal model structure and the neglect of data support. Data-driven-based methods, such as the multi-criteria, Bayesian network, or deep learning decision-making, are trained and operated based on datasets for the online decision-making of automated vehicles [<xref rid="B31-sensors-24-07904" ref-type="bibr">31</xref>,<xref rid="B32-sensors-24-07904" ref-type="bibr">32</xref>]. Artificial intelligence and research theories, such as game theory, the Markov decision-making process (MDP), or deep learning, are used to examine high-order nonlinearity and personality characteristics occurring in the decision-making process based on large-scale datasets [<xref rid="B33-sensors-24-07904" ref-type="bibr">33</xref>]. However, few studies combine intention-aware uncertainty and personalized factors into the decision-making process in shared control, which affects the acceptance and adaption of shared control to a certain extent. As a result, the data-driven-based decision-making paradigm needs to be developed in depth, where the personality of a driver should be merged.</p><p>Motivated by the above-mentioned observations, a personalized shared control framework considering a person&#x02019;s driving capability and style is proposed. The contributions are as follows.</p><p>(1) Drivers&#x02019; driving capability is defined and evaluated to improve the rationality of the DAAS. Driving styles combining classification and identification processes are analyzed, characterized, and evaluated in both online and offline ways. The simulated scenario generation method for human factors is established.</p><p>(2) The personalized framework of shared control for the automated vehicle is proposed, considering drivers&#x02019; driving capabilities and style, and its evaluation criteria are established considering driving safety, comfort, and workload. The intention-aware decision-making logic is proposed based on the mixed observable Markov decision process (MOMDP).</p></sec><sec id="sec2-sensors-24-07904"><title>2. Analysis and Evaluation of Drivers&#x02019; Driving Capability and Styles</title><sec id="sec2dot1-sensors-24-07904"><title>2.1. Simulated Scenario Generation Methods for Human Factors</title><p>The system&#x02019;s simulation and scenario generation method have important significance in improving the rationality and accuracy of evaluation methods and reveal the internal mechanism of human factors, in particular, driving capability and styles. In order to motivate human factors to a maximum extent, the single source-based simulation and the random vehicle&#x02013;road field (RVRF)-based scenario generation method are proposed for drivers&#x02019; driving capability and styles, respectively.</p><p>The single source-based simulation consists of signal features, physical mapping, and signal dimension and can be designed as the velocity function of background vehicles based on the signal periodicity, mutability, and spectral characteristics, such as the aperiodic mutation signal and periodic gradient signal as follows:<disp-formula id="FD1-sensors-24-07904"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mi>sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003c9;</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="sans-serif">&#x00393;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">v<sub>L</sub></italic> is the velocity of the background vehicle, <italic toggle="yes">A<sub>p</sub></italic> is the velocity amplitude, <italic toggle="yes">&#x003c9;</italic> is the signal frequency, <italic toggle="yes">&#x003c6;</italic> is the initial velocity phase, and &#x00393; is the velocity offset.</p><p>Micro-scenarios analyze the physical mechanism and random effect of vehicle&#x02013;road coupling to reveal a general pattern of microscopic traffic scenarios. Therefore, they are suitable for the evaluation of driving capability, reflecting drivers&#x02019; time-varying features. The RVRF can be defined as a strong, random, and steady field formed by the vehicle&#x02013;road coupling in the simulation environment, and the system framework of RVRF is shown in <xref rid="sensors-24-07904-f001" ref-type="fig">Figure 1</xref>.</p><p>The field strength <italic toggle="yes">E<sub>vef</sub></italic> of RVRF consists of the field strength <italic toggle="yes">E<sub>kef</sub></italic> of the kinetic energy field composed of moving objects, the <italic toggle="yes">E<sub>ve</sub></italic><sub>f</sub> of the potential field composed of stationary objects, and the <italic toggle="yes">E<sub>vef</sub></italic> of the intention field composed of drivers&#x02019; uncertainty. Three components, generated by the transportation participant P at the spatial point (<italic toggle="yes">x<sub>q</sub></italic>, <italic toggle="yes">y<sub>q</sub></italic>), can be expressed as follows:<disp-formula id="FD2-sensors-24-07904"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD3-sensors-24-07904"><label>(3)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>G</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:msub><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD4-sensors-24-07904"><label>(4)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>G</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:msub><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD5-sensors-24-07904"><label>(5)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>G</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:msub><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="sans-serif">&#x003a6;</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where (<italic toggle="yes">G</italic>, <italic toggle="yes">k</italic><sub>1</sub>, <italic toggle="yes">k</italic><sub>2</sub>) are constants, <italic toggle="yes">v<sub>P</sub></italic> is the longitudinal velocity of P, <italic toggle="yes">r<sub>Pq</sub></italic> is the distance vector at (<italic toggle="yes">x<sub>q</sub></italic>, <italic toggle="yes">y<sub>q</sub></italic>), &#x003a6;<italic toggle="yes"><sub>D</sub></italic> is the intention factor, <italic toggle="yes">M<sub>P</sub></italic> is the equivalent mass, and <italic toggle="yes">R<sub>P</sub></italic> is road field index.</p><p>The RVRF map &#x003a9; at the specific moment can be obtained and expressed as follows after slicing the natural scenario data:<disp-formula id="FD6-sensors-24-07904"><label>(6)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003a9;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b6;</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>&#x003b6;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">E<sub>Svef,&#x003b6;</sub></italic> is the field strength of the vehicle&#x02013;road field (<italic toggle="yes">x<sub>&#x003b6;</sub></italic>, <italic toggle="yes">y<sub>&#x003b6;</sub></italic>). Thus, the process of spatial situation clustering is that of extracting the statistical characteristics of the field map and clustering the field map.</p><p>Considering the spatial variation feature of the field strength and its neighborhood correlation, the mean neighborhood two-dimensional histogram theory is proposed to extract the statistical characteristics of the field strength. The mean field strength of the N &#x000d7; N neighborhood is centered in <italic toggle="yes">(x<sub>&#x003b6;</sub></italic>, <italic toggle="yes">y<sub>&#x003b6;</sub></italic>).
<disp-formula id="FD7-sensors-24-07904"><label>(7)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b6;</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">G<sub>E</sub></italic>(<italic toggle="yes">x<sub>&#x003b6;</sub></italic>, <italic toggle="yes">y<sub>&#x003b6;</sub></italic>) is the average field strength at (<italic toggle="yes">x<sub>&#x003b6;</sub></italic>, <italic toggle="yes">y<sub>&#x003b6;</sub></italic>), and <italic toggle="yes">Q<sub>x</sub></italic> and <italic toggle="yes">Q<sub>y</sub></italic> are field map boundaries. The probability of (<italic toggle="yes">E<sub>Svef,&#x003b6;</sub></italic>(<italic toggle="yes">x<sub>&#x003b6;</sub></italic>, <italic toggle="yes">y<sub>&#x003b6;</sub></italic>) = <italic toggle="yes">E<sub>m</sub></italic>, <italic toggle="yes">G<sub>E</sub></italic>(<italic toggle="yes">x<sub>&#x003b6;</sub></italic>, <italic toggle="yes">y<sub>&#x003b6;</sub></italic>) = <italic toggle="yes">E<sub>n</sub></italic>) can be expressed as follows:<disp-formula id="FD9-sensors-24-07904"><label>(8)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b6;</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b6;</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">H<sub>2D_mean</sub></italic>(<italic toggle="yes">E<sub>m</sub></italic>, <italic toggle="yes">E<sub>n</sub></italic>) is the mean neighborhood histogram. The similarity index <italic toggle="yes">S<sub>I</sub></italic> is the sum of the minimum values of <italic toggle="yes">H<sub>2D_mean</sub></italic>, and the spatial situation database <italic toggle="yes">D<sub>S</sub></italic> is as follows, where <italic toggle="yes">w</italic> is the situation number, and <italic toggle="yes">u</italic> is the field map number.
<disp-formula id="FD10-sensors-24-07904"><label>(9)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="sans-serif">&#x003a9;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The spatial position of the ego vehicle obeys the probability distribution P(<italic toggle="yes">S<sub>t</sub></italic> = (<italic toggle="yes">S<sub>tx</sub></italic>, <italic toggle="yes">S<sub>ty</sub></italic>)), and its set <italic toggle="yes">&#x003be;</italic> is as follows:<disp-formula id="FD11-sensors-24-07904"><label>(10)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003be;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>~</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">S<sub>tx</sub></italic> and <italic toggle="yes">S<sub>ty</sub></italic> are the numbers of ego vehicle position, and <italic toggle="yes">&#x003be;</italic> is the completely observable set with a high density, fragmentation, and strong randomness in the naturalistic scenario data. Therefore, the growing neutral gas (GNG) algorithm [<xref rid="B34-sensors-24-07904" ref-type="bibr">34</xref>] is proposed to extract the vehicle&#x02019;s continuous topology in <italic toggle="yes">D<sub>S</sub></italic>, as follows:<disp-formula id="FD12-sensors-24-07904"><label>(11)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>N</mml:mi><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>&#x003c7;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b5;</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">&#x003c7;</italic> and <italic toggle="yes">&#x003b5;</italic> are the node set and the boundary set, and the submanifold consists of {<italic toggle="yes">&#x003c7;</italic>[<italic toggle="yes">i</italic>], <italic toggle="yes">i</italic> = 1, &#x02026;, <italic toggle="yes">G</italic>} and its eigenvector <italic toggle="yes">ev[i]</italic> = [<italic toggle="yes">&#x003c7;</italic>[<italic toggle="yes">i</italic>]&#x000b7;<italic toggle="yes">S<sub>tx</sub>, &#x003c7;</italic>[<italic toggle="yes">i</italic>]&#x000b7;<italic toggle="yes">S<sub>ty</sub></italic>]<sup>T</sup> &#x02208; <italic toggle="yes">R<sup>g</sup></italic>. <italic toggle="yes">&#x003be;</italic> is taken as the input, and the learning process of <italic toggle="yes">&#x003c7;</italic> is that of searching the minimum offset as follows, where <italic toggle="yes">D<sub>e</sub></italic> is the Euclidean distance between <italic toggle="yes">S<sub>t</sub></italic> and <italic toggle="yes">e<sub>v</sub></italic>.
<disp-formula id="FD13-sensors-24-07904"><label>(12)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>&#x003c7;</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mi>min</mml:mi><mml:mi>&#x003c7;</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>G</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The velocity of the ego vehicle obeys the specific motion pattern in typical DS, and its spatio-temporal variation features can be generalized by Gaussian process regression (GPR) as follows, where <italic toggle="yes">f</italic>(<italic toggle="yes">X</italic>) is the function of the mean value <italic toggle="yes">&#x003b7;</italic>(<italic toggle="yes">X</italic>) and the covariance <italic toggle="yes">&#x003d1;</italic>(<italic toggle="yes">X</italic>, <italic toggle="yes">X&#x02032;</italic>).
<disp-formula id="FD14-sensors-24-07904"><label>(13)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>~</mml:mo><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003d1;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The generation process of the vehicle motion pattern is that of the position searching and the related velocity mapping MP(<italic toggle="yes">x</italic>), and MP:(<italic toggle="yes">S<sub>tx</sub></italic>, <italic toggle="yes">S<sub>ty</sub></italic>)<sup>T</sup> &#x02192; (<italic toggle="yes">v<sub>tx</sub></italic>, <italic toggle="yes">v<sub>ty</sub></italic>)<sup>T</sup> can be expressed as follows:<disp-formula id="FD15-sensors-24-07904"><label>(14)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mo>&#x003a3;</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x02200;</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:mi>&#x003be;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The RVRF model H<italic toggle="yes"><sub>ru</sub></italic> can be abstracted as the probabilistic model consisting of the mutually exclusive region <italic toggle="yes">R<sub>e</sub></italic>, vehicle&#x02013;road topology {<italic toggle="yes">X<sub>Re</sub></italic>, <italic toggle="yes">&#x003b5;<sub>Re</sub></italic>}, and the state transition matrix <italic toggle="yes">T<sub>m</sub></italic> as follows:<disp-formula id="FD16-sensors-24-07904"><label>(15)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003c7;</mml:mi><mml:mrow><mml:mi>Re</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b5;</mml:mi><mml:mrow><mml:mi>Re</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>Re</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>&#x003d6;</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>&#x02229;</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003c6;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02200;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>T</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>|</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where the subscript <italic toggle="yes">ru</italic> indicates traffic rules, and the spatial probability distribution P(<italic toggle="yes">S<sub>t</sub></italic><sub>+<italic toggle="yes">&#x00394;t</italic></sub>|<italic toggle="yes">R<sub>e</sub></italic>,<italic toggle="yes"><sub>t</sub></italic> = Re[<italic toggle="yes">i</italic>]) at the (<italic toggle="yes">t</italic> + &#x00394;<italic toggle="yes">t</italic>) moment can be expressed as follows:<disp-formula id="FD17-sensors-24-07904"><label>(16)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="sans-serif">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="3.33333pt"/><mml:mo>|</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="sans-serif">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="3.33333pt"/><mml:mo>|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:munder><mml:munder><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="sans-serif">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="3.33333pt"/><mml:mo>|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">&#x0fe38;</mml:mo></mml:munder><mml:mrow><mml:mi>Velocity</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>integral</mml:mi></mml:mrow></mml:munder><mml:munder><mml:munder><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">&#x0fe38;</mml:mo></mml:munder><mml:mrow><mml:mi>G</mml:mi><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:munder><mml:munder><mml:munder><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">&#x0fe38;</mml:mo></mml:munder><mml:mrow><mml:mi>Observed</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>value</mml:mi></mml:mrow></mml:munder></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The observation set can be estimated by predicting the vehicle&#x02019;s spatial distribution, and the maximum likelihood function <italic toggle="yes">u<sub>R,j</sub></italic> can be expressed as follows, where <italic toggle="yes">M<sub>j</sub></italic> is the observed value number corresponding to <italic toggle="yes">&#x003c8;<sub>t</sub></italic><sub>+&#x00394;<italic toggle="yes">t</italic></sub><sup>[<italic toggle="yes">j</italic>]</sup> = 1.
<disp-formula id="FD18-sensors-24-07904"><label>(17)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02200;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>&#x003d6;</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot2-sensors-24-07904"><title>2.2. The Mechanism Analysis and Evaluation Method for Driving Capability</title><p>In order to improve the rationality of the DAAS, coupling relationships among human factors are analyzed and shown in <xref rid="sensors-24-07904-f002" ref-type="fig">Figure 2</xref>. The comprehensive human factor consisting of the driving style, skill, and status is proposed as the driver&#x02019;s driving capability, which can be defined as the driver&#x02019;s maneuverability to the vehicle with time-varying nonlinear dynamic features.</p><p>The mechanism analysis and evaluation framework of the driver&#x02019;s driving capability is established and shown in <xref rid="sensors-24-07904-f003" ref-type="fig">Figure 3</xref>. The characteristics and mechanism of the driving capability are analyzed offline, and its evaluation method is conducted based on the online Gaussian mixture model (GMM) and introduced in detail in the DAAS in <xref rid="sec3-sensors-24-07904" ref-type="sec">Section 3</xref>.</p><p>The Hammerstein identification process is proposed as the driving capability identification model (DCIM) in view of its conforming with the time-varying, high-order nonlinear, and dynamic features of the driving capability, as shown in <xref rid="sensors-24-07904-f004" ref-type="fig">Figure 4</xref>. As the set of longitudinal and lateral identification models (LnDCIM and LtDCIM), the DCIM consists of the static nonlinear and dynamic linear elements in series.</p><p>The static nonlinear and the dynamic linear elements can be expressed as follows:<disp-formula id="FD19-sensors-24-07904"><label>(18)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>&#x022c5;</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD20-sensors-24-07904"><label>(19)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">C<sub>p</sub></italic>(<italic toggle="yes">k</italic>) consists of the pedal signal <italic toggle="yes">P<sub>d</sub>(k)</italic> and the steering angle signal <italic toggle="yes">S<sub>w</sub></italic>(<italic toggle="yes">k</italic>)<italic toggle="yes">, S</italic>(<italic toggle="yes">k</italic>) is the set {<italic toggle="yes">S<sub>ln</sub></italic>(<italic toggle="yes">k</italic>)<italic toggle="yes">, S<sub>lt</sub></italic>(<italic toggle="yes">k</italic>)} in the static nonlinear element of DCIM, <italic toggle="yes">q</italic> and <italic toggle="yes">n</italic> are orders in the dynamic linear element, and d is the input delay order defined as the integer multiple of the sampling time.</p><p>As the key data to reveal the intrinsic attributes of driving capability, the parameters of the DCIM model need to be decoupled to avoid data oversaturation in the regression fitting. The principal component analysis (PCA) method [<xref rid="B35-sensors-24-07904" ref-type="bibr">35</xref>] is adopted to decouple and reduce the key parameter dimension in the DCIM. The dataset <italic toggle="yes">P<sub>r</sub></italic> with <italic toggle="yes">&#x003b6;</italic>-dimension parameters and U-dimension observation variables can be expressed as follows:<disp-formula id="FD21-sensors-24-07904"><label>(20)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c2;</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c2;</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c2;</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The contribution rate of the principal component is defined as the percentage of the sum of the eigenvalues of the first <italic toggle="yes">q</italic> principal components and the sum of all ones, and the cumulative contribution rate <italic toggle="yes">Q<sub>q</sub></italic> is as follows:<disp-formula id="FD22-sensors-24-07904"><label>(21)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>&#x003c8;</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">/</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>U</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The <italic toggle="yes">&#x003c8;</italic> corresponding to <italic toggle="yes">Q<sub>&#x003c8;</sub></italic> &#x02265; 85% is taken as the independent component number, and the principal component matrix <italic toggle="yes">P<sub>L</sub></italic> with <italic toggle="yes">&#x003c8;</italic> &#x000d7; <italic toggle="yes">U</italic> dimensions can be obtained. The driving capability needs to be classified as the following model set based on the objective and subjective methods.
<disp-formula id="FD23-sensors-24-07904"><label>(22)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The objective classification method consists of the particle clustering and the mapping process from the clustering results to <italic toggle="yes">D<sub>cap</sub></italic>, and the subjective method is based on scale analysis. The element in the clustering set <italic toggle="yes">O<sub>P,ru</sub></italic> that has the largest intersection with the specific one in the scale analysis set <italic toggle="yes">S<sub>P,ij</sub></italic>, which is the driving capability element of the same type.
<disp-formula id="FD24-sensors-24-07904"><label>(23)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi><mml:mn>5</mml:mn></mml:munderover><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:mstyle><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD25-sensors-24-07904"><label>(24)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>max</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The driving capability evaluation equation (DCEE) is as follows:<disp-formula id="FD26-sensors-24-07904"><label>(25)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>L</mml:mi></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the subset of <italic toggle="yes">P<sub>L</sub></italic>, and <italic toggle="yes">&#x003c1;</italic> is the regression coefficient.
<disp-formula id="FD27-sensors-24-07904"><label>(26)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>;</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>}</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mi>u</mml:mi><mml:mo>;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>u</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>U</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot3-sensors-24-07904"><title>2.3. The Characterization and Evaluation Method for Driving Styles</title><p>The characterization and evaluation framework of drivers&#x02019; driving styles is shown in <xref rid="sensors-24-07904-f005" ref-type="fig">Figure 5</xref>. Accounting for classification application and computation complexities, the driving styles can be labeled as the offline database <italic toggle="yes">D<sub>Sty</sub></italic> and are proposed to be classified into three types as follows:<disp-formula id="FD28-sensors-24-07904"><label>(27)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The driving style classification model (DSCM) combines the subjective and objective methods to approximate the truth value of the driving styles. Particle swarm clustering (PSC) is proposed as the objective classification method, and features of drivers&#x02019; driving styles are extracted as the physical property and model inputs, which can be expressed as follows:<disp-formula id="FD29-sensors-24-07904"><label>(28)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">a<sub>&#x003c9;</sub></italic> is the root mean square of acceleration, <italic toggle="yes">f</italic><sub>0</sub> and <italic toggle="yes">F</italic> are the initial frequency and the integrating frequency range, and <italic toggle="yes">PSD</italic>(<italic toggle="yes">f</italic>) is the power spectral density of acceleration of the ego vehicle. The driver reaction time <italic toggle="yes">T<sub>s</sub></italic> represents the driver&#x02019;s sensitivity to the stimuli of the current situation and can be expressed as follows:<disp-formula id="FD30-sensors-24-07904"><label>(29)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="sans-serif">&#x00394;</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x02265;</mml:mo><mml:mi mathvariant="sans-serif">&#x00394;</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">t<sub>sp</sub></italic> is the sampling period, <italic toggle="yes">n<sub>p</sub></italic> is the sampling number, <italic toggle="yes">v<sub>ego</sub></italic> is the velocity of the ego vehicle, &#x00394;<italic toggle="yes">v</italic><sub>0</sub> is the velocity threshold, and <italic toggle="yes">T</italic><sub>0</sub> is the instant that <italic toggle="yes">v<sub>ego</sub></italic> achieves &#x00394;<italic toggle="yes">v</italic><sub>0</sub> for the first time. The time headway <italic toggle="yes">T<sub>f</sub></italic> represents the approaching degree of the ego vehicle relative to the surrounding situation and is as follows:<disp-formula id="FD31-sensors-24-07904"><label>(30)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>ln</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">D<sub>avg</sub></italic> and <italic toggle="yes">D<sub>ins</sub></italic> are the average relative distance and the instant relative distance when the ego vehicle steers, and <italic toggle="yes">V<sub>avg</sub></italic> and <italic toggle="yes">V<sub>ins</sub></italic> are the corresponding relative velocities.</p><p>Each element in <italic toggle="yes">D<sub>Sty</sub></italic> can be modeled as the DSCM, which can be designed as the function consisting of the mean and variance of the <italic toggle="yes">a<sub>&#x003c9;</sub></italic>, <italic toggle="yes">T<sub>s</sub>,</italic> and <italic toggle="yes">T<sub>f</sub></italic> as follows:<disp-formula id="FD32-sensors-24-07904"><label>(31)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>&#x003c9;</mml:mi><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>&#x003c9;</mml:mi><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
where variables with subscript <italic toggle="yes">M</italic> are those of their mean, and the ones with subscript <italic toggle="yes">V</italic> are those of their variance. The DSCM consists of the PSC and the mapping process between clustering centers and <italic toggle="yes">D<sub>Sty</sub></italic>. The number of the clustering center is 3, and the mapping relation is as follows:<disp-formula id="FD33-sensors-24-07904"><label>(32)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>&#x003c9;</mml:mi><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>&#x003c9;</mml:mi><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where [<italic toggle="yes">C<sub>a&#x003c9;M</sub></italic>, <italic toggle="yes">C<sub>TsM</sub></italic>, <italic toggle="yes">C<sub>TfM</sub></italic>, <italic toggle="yes">C<sub>a&#x003c9;V</sub></italic>, <italic toggle="yes">C<sub>TsV</sub></italic>, <italic toggle="yes">C<sub>TfV</sub></italic>] is the clustering center of the DSCM, <italic toggle="yes">&#x003c9;</italic><sub>1</sub> and <italic toggle="yes">&#x003c9;</italic><sub>2</sub> are the weight coefficients, and <italic toggle="yes">D<sub>ag</sub></italic> is the radicalization factor. The clustering center with the maximum of <italic toggle="yes">D<sub>ag</sub></italic> is that of the radical type, and that with the minimum is that of the steady type.</p><p>The questionnaire method [<xref rid="B36-sensors-24-07904" ref-type="bibr">36</xref>] is proposed as the subjective classification method and consists of five comfort degrees when respondents are passengers or drivers, respectively. The Cronbach &#x003b1; is used as the scale of confidence to verify the reliability and stability of the results as follows:<disp-formula id="FD34-sensors-24-07904"><label>(33)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>K</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">K</italic> is the question number, <italic toggle="yes">&#x003c3;<sub>i</sub></italic> and <italic toggle="yes">&#x003c3;<sub>total</sub></italic> are standard deviations of the score of the <italic toggle="yes">ith</italic> question and the total score of all the questions, respectively. The questionnaire for driving styles is shown in <xref rid="sensors-24-07904-t001" ref-type="table">Table 1</xref>.</p><p>The traffic situation assessment will be introduced in detail in the intention-aware MOMDP framework in <xref rid="sec3-sensors-24-07904" ref-type="sec">Section 3</xref>. Drivers&#x02019; operating signals, all states of the ego vehicle, and their relative states to the surrounding traffic are continuous time series and affect the states at the next moment. Therefore, the multi-dimension Gaussian hidden Markov process (MGHMP) with a set of hidden states <italic toggle="yes">q<sub>t</sub></italic> and a corresponding set of <italic toggle="yes">&#x003ba;</italic> possible observations is proposed as the driving style evaluation model (DSEM) as follows:<disp-formula id="FD35-sensors-24-07904"><label>(34)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c0;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD36-sensors-24-07904"><label>(35)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">&#x003c0;</italic> is the initial state distribution, and the state transition probability matrix A is as follows:<disp-formula id="FD37-sensors-24-07904"><label>(36)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD38-sensors-24-07904"><label>(37)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The set of observable sequences <italic toggle="yes">O</italic> = {<italic toggle="yes">V<sub>i</sub></italic>, <italic toggle="yes">i</italic> = 1, 2,&#x02026;, <italic toggle="yes">&#x003ba;</italic>}, where <italic toggle="yes">V</italic> is the possible observation. The observation probability matrix of the <italic toggle="yes">j</italic><sup>th</sup> state is <italic toggle="yes">B</italic> = {<italic toggle="yes">b<sub>j</sub></italic>(<italic toggle="yes">O</italic>)} and <italic toggle="yes">b<sub>j</sub></italic>(<italic toggle="yes">O</italic>) is as follows:<disp-formula id="FD39-sensors-24-07904"><label>(38)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>O</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mo>&#x003a3;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD40-sensors-24-07904"><label>(39)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo>&#x0221e;</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mo>&#x0221e;</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>O</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">c<sup>jk</sup></italic> is the <italic toggle="yes">k</italic><sup>th</sup> mixed-weight coefficient in the <italic toggle="yes">j</italic><sup>th</sup> state, and <italic toggle="yes">M</italic> is the Gaussian mixture number. N(<italic toggle="yes">O</italic>|<italic toggle="yes">&#x003bc;<sub>jk</sub></italic>, <italic toggle="yes">&#x003a3;<sub>jk</sub></italic>) is the Gaussian probability density function with mean <italic toggle="yes">&#x003bc;</italic> and covariance &#x003a3;. <italic toggle="yes">c<sub>jk</sub></italic> is as follows:<disp-formula id="FD41-sensors-24-07904"><label>(40)</label><mml:math id="mm41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>DSEM can be defined as a tuple <italic toggle="yes">&#x003bb;</italic> with <italic toggle="yes">N</italic> states.
<disp-formula id="FD42-sensors-24-07904"><label>(41)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003c0;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x003a3;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Each DSEM is calculated as the logarithmic likelihood as follows, and the maximum likelihood in DSEM is mapped to the corresponding driving style.
<disp-formula id="FD43-sensors-24-07904"><label>(42)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo>|</mml:mo><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec3-sensors-24-07904"><title>3. Personalized Shared Control Strategy for the Automated Vehicle</title><p>Personalized shared control aims at improving driving safety and comfort and minimizing the driving load, which should be the optimal driving control match between the driver and the autonomous system. The personalized paradigm and its evaluation criteria are established for the proposed shared control system.</p><sec id="sec3dot1-sensors-24-07904"><title>3.1. The Framework of the Personalized Shared Control</title><p>The strong coupling between the shared control and human&#x02013;vehicle&#x02013;scenario system is determined by the system function of the shared control. Therefore, the mechanism analysis of the holonomic system X with the human&#x02013;controller&#x02013;vehicle&#x02013;scenario system is the development infrastructure of shared control, which can be expressed as follows:<disp-formula id="FD44-sensors-24-07904"><label>(43)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">E</italic> is the stimulating system, <italic toggle="yes">D</italic> is the driver system, <italic toggle="yes">V</italic> is the vehicle system, <italic toggle="yes">M</italic> is the shared control system, which consists of the personalized system <italic toggle="yes">H<sub>s</sub></italic> and driving authority allocation system <italic toggle="yes">D<sub>as</sub></italic>, and <italic toggle="yes">f</italic>(<italic toggle="yes">E</italic>, <italic toggle="yes">V</italic>, <italic toggle="yes">D</italic>, <italic toggle="yes">M</italic>) is the coupling function.</p><p><italic toggle="yes">D</italic> and <italic toggle="yes">M</italic> comprehend the current driving situation of <italic toggle="yes">E</italic> and adjust the operation mode applied to <italic toggle="yes">V</italic> based on their status feedback and form the cooperative mode by overcoming the inconsistent understanding of <italic toggle="yes">E</italic>. The framework of the personalized shared control consisting of the system, strategy, and data configurations is developed based on system features of X and large-scale data acquisition and training methods, as shown in <xref rid="sensors-24-07904-f006" ref-type="fig">Figure 6</xref>. The <italic toggle="yes">H<sub>s</sub></italic> outputs personalized operation signals based on the <italic toggle="yes">D<sub>sty</sub></italic> type, and operation and status from both <italic toggle="yes">D</italic> and <italic toggle="yes">H<sub>s</sub></italic> are sent to <italic toggle="yes">D,</italic> and the driving authority is obtained based on the DAAS. The closed loop X is formed when states of <italic toggle="yes">V</italic> are changed and sent to <italic toggle="yes">E</italic>. The <italic toggle="yes">M</italic> outputs the online result of the <italic toggle="yes">D<sub>sty</sub></italic> type, the real-time result of <italic toggle="yes">D<sub>cap</sub></italic>, and the MOMDP-based decision-making result, which is based on the offline raw dataset, the offline database, and the online system states.</p></sec><sec id="sec3dot2-sensors-24-07904"><title>3.2. The DAAS and Evaluation Criteria for Shared Control</title><p>The <italic toggle="yes">D<sub>as</sub></italic> arbitrates the driving authority between <italic toggle="yes">D</italic> and <italic toggle="yes">H<sub>s</sub></italic> and outputs the desired control signals to <italic toggle="yes">V</italic>. The <italic toggle="yes">D<sub>as</sub></italic> can be expressed as follows:<disp-formula id="FD45-sensors-24-07904"><label>(44)</label><mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="sans-serif">&#x0039b;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="sans-serif">&#x0039b;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">D<sub>f</sub></italic> is the human factor in <italic toggle="yes">D</italic>, <italic toggle="yes">E<sub>f</sub></italic> is the situation factor in <italic toggle="yes">E</italic>, which is affected by the observability factor <italic toggle="yes">O<sub>Ef</sub></italic> and the key situation factor <italic toggle="yes">T<sub>Ef</sub></italic>, and &#x0039b; is the coupling effect function.</p><p>The <italic toggle="yes">D<sub>as</sub></italic> in the indirect shared control has advantages in overcoming structure conflict of human&#x02013;machine operation and improving human comfort, and is proposed as the DAAS in <italic toggle="yes">M as</italic> follows:<disp-formula id="FD46-sensors-24-07904"><label>(45)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003d1;</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003d1;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where the control signal <italic toggle="yes">C</italic> consists of steer signal <italic toggle="yes">&#x003b4;<sub>c</sub></italic>, driving signal <italic toggle="yes">P<sub>a,c</sub></italic>, and braking signal <italic toggle="yes">P<sub>b,c</sub></italic>; <italic toggle="yes">C<sub>d</sub></italic> and <italic toggle="yes">C<sub>m</sub></italic> are operation signals of <italic toggle="yes">D</italic> and <italic toggle="yes">H<sub>s</sub></italic>, respectively; and <italic toggle="yes">&#x003d1;</italic> is the allocation coefficient, which determines the driving authority. The online evaluation result of driving capability is taken as <italic toggle="yes">&#x003d1;</italic>, given its maneuverability to determine the control of <italic toggle="yes">D</italic>. The mapping relation from <italic toggle="yes">D<sub>cap</sub></italic> = {1, 2, 3, 4, 5} to <italic toggle="yes">&#x003d1;</italic> &#x02208; [0, 1] is <italic toggle="yes">&#x003d1;</italic> = 0.25<italic toggle="yes">D<sub>cap</sub></italic> &#x02212; 0.25.</p><p>The DAAS is based on the GMM with an offline classification database of driving capability. The GMM consists of multiple single-Gaussian probability distribution models, and their probability density can be described by the Gaussian density function <italic toggle="yes">g</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">&#x003bc;</italic>, <italic toggle="yes">&#x003a3;</italic>) as follows:<disp-formula id="FD47-sensors-24-07904"><label>(46)</label><mml:math id="mm47" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x003a3;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>d</mml:mi></mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x003a3;</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle><mml:mi>exp</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mo>&#x003a3;</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">x</italic> is the random vector, <italic toggle="yes">&#x003bc;</italic> is the mean vector, &#x003a3; is the covariance matrix, and <italic toggle="yes">d</italic> is the dimension of the random vector. Multidimensional Gaussian probability density functions <italic toggle="yes">p</italic>(<italic toggle="yes">x</italic>) need to be superimposed in the DSIM with multidimensional state inputs as follows:<disp-formula id="FD48-sensors-24-07904"><label>(47)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">x<sub>j</sub></italic>, <italic toggle="yes">&#x003bc;<sub>j</sub></italic>, and <italic toggle="yes">&#x003a3;<sub>j</sub></italic> are the <italic toggle="yes">j</italic><sup>th</sup> random vector, mean vector, and covariance matrix, respectively, and <italic toggle="yes">&#x003b1;<sub>j</sub></italic> is the weight coefficient. The likelihood function of N-dimensional sample <italic toggle="yes">XG</italic> is as follows:<disp-formula id="FD49-sensors-24-07904"><label>(48)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="sans-serif">&#x00398;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mi>log</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">&#x003b8;<sub>j</sub></italic> = (<italic toggle="yes">x<sub>j</sub></italic>, <italic toggle="yes">&#x003bc;<sub>j</sub></italic>, &#x003a3;<italic toggle="yes"><sub>j</sub></italic>), &#x00398; = (<italic toggle="yes">&#x003b8;<sub>1</sub></italic>,&#x02026;, <italic toggle="yes">&#x003b8;<sub>M</sub></italic>). The EM algorithm is proposed to train (<italic toggle="yes">&#x003b1;</italic>, <italic toggle="yes">&#x003bc;</italic>, &#x003a3;) and consists of the E and <italic toggle="yes">M</italic> steps.</p><p>Accurate and objective evaluation criteria need to be established for shared control to evaluate the system&#x02019;s performance. The system performance index of <italic toggle="yes">M</italic> is as follows:<disp-formula id="FD50-sensors-24-07904"><label>(49)</label><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x0039e;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b5;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b5;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b5;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000a0;</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>&#x003b5;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b5;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b5;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
where &#x0039e; is the composite index of shared control, and the subscript <italic toggle="yes">i</italic> is the number of <italic toggle="yes">E</italic>. &#x0039e; consists of the safety index <italic toggle="yes">&#x0014b;<sub>ds</sub></italic>, the driving load index <italic toggle="yes">&#x0014b;<sub>cw</sub></italic>, and comfort index <italic toggle="yes">&#x0014b;<sub>dc</sub></italic>, and <italic toggle="yes">&#x003b5;</italic><sub>1</sub>, <italic toggle="yes">&#x003b5;</italic><sub>2</sub>, and <italic toggle="yes">&#x003b5;</italic><sub>3</sub> are the corresponding weight coefficients. In order to reveal the system&#x02019;s performance, values of <italic toggle="yes">&#x003b5;</italic><sub>1</sub> and <italic toggle="yes">&#x003b5;</italic><sub>3</sub> should be higher than <italic toggle="yes">&#x003b5;<sub>2</sub></italic> and can be set as {<italic toggle="yes">&#x003b5;</italic><sub>1</sub>, <italic toggle="yes">&#x003b5;</italic><sub>2</sub>, <italic toggle="yes">&#x003b5;</italic><sub>3</sub>} = {0.4, 0.2, 0.4}. The <italic toggle="yes">&#x0014b;<sub>ds</sub></italic>, <italic toggle="yes">&#x0014b;<sub>cw</sub></italic>, and <italic toggle="yes">&#x0014b;<sub>dc</sub></italic> in the longitudinal and lateral scenarios are as follows:<disp-formula id="FD51-sensors-24-07904"><label>(50)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>N</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>&#x003c9;</mml:mi><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>&#x003c9;</mml:mi><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>N</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>N</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD52-sensors-24-07904"><label>(51)</label><mml:math id="mm52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover><mml:mi>w</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>N</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>max</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>max</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>N</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>N</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">D<sub>cap</sub></italic><sub>,<italic toggle="yes">ln</italic></sub> and <italic toggle="yes">Dc<sub>ap,lt</sub></italic> are the longitudinal and lateral driving capabilities, <italic toggle="yes">P<sub>ac</sub></italic> and <italic toggle="yes">P<sub>br</sub></italic> are the positions of the gas and braking pedals, and <italic toggle="yes">S<sub>w</sub></italic> is the angle of the steering wheel. The variables corresponding to the subscript <italic toggle="yes">h</italic> are the statistics of the driving features in the case that <italic toggle="yes">&#x003d1;</italic> = 1 and N is the normalization operator, which represents the deviation degree between the statistics and the specified value of the driver. <italic toggle="yes">&#x00196;<sub>max</sub></italic> is the maximum curvature, and the subscript <italic toggle="yes">j</italic> = 1, 2, 3 represents three modes: the shared control, human, and automated driving modes.</p></sec><sec id="sec3dot3-sensors-24-07904"><title>3.3. The Personalized Decision-Making Logic in the Intention-Aware MOMDP Framework</title><p>The personalized decision-making logic is developed based on MOMDP to improve the acceptance and adaptation of shared control, and the intention-aware-based assessment method is proposed for the uncertainty in complex scenarios based on the MGHMP. Vehicles&#x02019; motion intention in the micro-traffic scenarios can be regarded as the set of longitudinal and lateral relative intentions in spatial evolution, and their reaction behaviors can be regarded as the system response to states <italic toggle="yes">s</italic><sub>0</sub> of the background vehicles and their relative states <italic toggle="yes">ds</italic><sub>0</sub> to the ego vehicle. The coupling mechanism of motion intentions in micro-traffic scenarios is shown in <xref rid="sensors-24-07904-f007" ref-type="fig">Figure 7</xref>. The driving motion intention model (DMIM) appears to be extended infinitely with an increase in the coupling region quantity. Therefore, the reactive motion intention model (RMIM) is proposed for adjacent background vehicles to improve model universality and controllability of the model scale.</p><p>The motion intention of background vehicles can be expressed as follows:<disp-formula id="FD53-sensors-24-07904"><label>(52)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>}</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">I<sub>R</sub></italic> is the motion intention set based on the RMIM, and <italic toggle="yes">I<sub>D</sub></italic> is that based on the DMIM. The RMIM is the function of states <italic toggle="yes">s</italic><sub>0</sub> and <italic toggle="yes">ds</italic><sub>0,</sub> and the DMIM is the function of <italic toggle="yes">s<sub>0</sub></italic> as follows:<disp-formula id="FD54-sensors-24-07904"><label>(53)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where {<italic toggle="yes">I<sub>FA</sub></italic>, <italic toggle="yes">I<sub>HT</sub></italic>, <italic toggle="yes">I<sub>NM</sub></italic>, <italic toggle="yes">I<sub>CI</sub></italic>} are the intentions of bearing off, hesitation, maintaining, and approaching, and {<italic toggle="yes">I<sub>LE</sub></italic>, <italic toggle="yes">I<sub>RI</sub></italic>, <italic toggle="yes">I<sub>FO</sub></italic>} are those of turning left, turning right, and keeping straight.</p><p>The observation sequences consisting of states <italic toggle="yes">s</italic><sub>0</sub> and <italic toggle="yes">ds</italic><sub>0</sub> are continuous time series and affect the states at the next moment. Therefore, the MGHMP with a set of hidden states <italic toggle="yes">q<sub>t</sub></italic> and a corresponding set of <italic toggle="yes">&#x003ba;</italic> possible observations is proposed as the online identification model of the traffic situation (TSIM) for both DMIM and RMIM, as shown in <xref rid="sensors-24-07904-f008" ref-type="fig">Figure 8</xref>. The derivation process of the TSIM is similar to that of the DSEM.</p><p>The MOMDP-based personalized decision-making process can be expressed as a tuple as follows:<disp-formula>{<italic toggle="yes">S</italic>, <italic toggle="yes">A</italic>, <italic toggle="yes">T</italic>, <italic toggle="yes">Z</italic>, <italic toggle="yes">O</italic>, <italic toggle="yes">R</italic>, <italic toggle="yes">&#x003b3;</italic>}<label>(54)</label></disp-formula>
where <italic toggle="yes">S</italic> is the state space, <italic toggle="yes">A</italic> is the action space, <italic toggle="yes">T</italic>(<italic toggle="yes">s&#x02032;</italic>, <italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) = Pr(<italic toggle="yes">s&#x02032;</italic>|<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>): S &#x000d7; A &#x000d7; S is the transition function, <italic toggle="yes">Z</italic> is the observation space, and <italic toggle="yes">O</italic>(<italic toggle="yes">z</italic>, <italic toggle="yes">s&#x02032;</italic>, <italic toggle="yes">a</italic>) = Pr(<italic toggle="yes">z</italic>|<italic toggle="yes">s&#x02032;</italic>, <italic toggle="yes">a</italic>): <italic toggle="yes">S</italic> &#x000d7; <italic toggle="yes">A</italic> &#x000d7; <italic toggle="yes">Z</italic> is the observation function. <italic toggle="yes">R</italic>(<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>): <italic toggle="yes">S</italic> &#x000d7; <italic toggle="yes">A</italic> is the reward function, <italic toggle="yes">&#x003b3;</italic> &#x02208; [0,1] is the discount factor. The uncertainty factor of the MOMDP is the motion intention of vehicles, and the belief <italic toggle="yes">b</italic> &#x02208; <italic toggle="yes">B</italic> and the Bayes rule-based updated belief <italic toggle="yes">b&#x02032;</italic> = <italic toggle="yes">&#x003c4;</italic>(<italic toggle="yes">b</italic>, <italic toggle="yes">a</italic>, <italic toggle="yes">z</italic>) is proposed for the uncertainty, where <italic toggle="yes">B</italic> is the belief set, and <italic toggle="yes">&#x003c4;</italic> is the belief updating function, with <italic toggle="yes">a</italic> and observation <italic toggle="yes">z</italic> as follows:<disp-formula id="FD55-sensors-24-07904"><label>(55)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x022c5;</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">&#x0014b;</italic> is the normalization coefficient as follows:<disp-formula id="FD56-sensors-24-07904"><label>(56)</label><mml:math id="mm56" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b7;</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The MOMDP-based personalized decision-making process aims at searching the strategy <italic toggle="yes">&#x003c0;</italic>* corresponding to maximize <italic toggle="yes">R</italic> as follows, where <italic toggle="yes">&#x003c0;</italic> is the mapping strategy with the specified action <italic toggle="yes">a</italic> = <italic toggle="yes">&#x003c0;</italic>(<italic toggle="yes">b</italic>) and <italic toggle="yes">b</italic><sub>0</sub> is the initial belief.
<disp-formula id="FD57-sensors-24-07904"><label>(57)</label><mml:math id="mm57" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>&#x003c0;</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>max</mml:mi></mml:mrow><mml:mi>&#x003c0;</mml:mi></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>&#x0221e;</mml:mo></mml:munderover><mml:mrow><mml:msup><mml:mi>&#x003b3;</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x003c0;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Abundant messages for the decision-making process should be contained in <italic toggle="yes">S</italic>, given the property of the Markov process, and can be expressed as follows:<disp-formula id="FD58-sensors-24-07904"><label>(58)</label><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where [<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, <italic toggle="yes">&#x003b8;</italic>] is the vehicle position and [<italic toggle="yes">V<sub>x</sub></italic>, <italic toggle="yes">V<sub>y</sub></italic>, <italic toggle="yes">a<sub>x</sub></italic>, <italic toggle="yes">a<sub>y</sub></italic>, <italic toggle="yes">Y<sub>aw</sub></italic>] are the velocity and acceleration in longitudinal and lateral directions and yaw velocity, respectively. The current state set <italic toggle="yes">s</italic> = [<italic toggle="yes">s<sub>ego</sub></italic>, <italic toggle="yes">s<sub>t</sub></italic><sub>1</sub>, <italic toggle="yes">s<sub>t</sub></italic><sub>2</sub>,&#x02026;, <italic toggle="yes">s<sub>tN</sub></italic>], where <italic toggle="yes">s<sub>ego</sub></italic> is the state of the ego vehicle, and the others are those of background vehicles. Therefore, the mixed observable MDP is established and consists of the complete observable state set <italic toggle="yes">X<sub>s</sub></italic>, the inference-based state set <italic toggle="yes">Y<sub>s</sub></italic>, and the action space <italic toggle="yes">A</italic> as follows:<disp-formula id="FD59-sensors-24-07904"><label>(59)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo><mml:mo>;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>;</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where [<italic toggle="yes">a<sub>ln</sub></italic>, <italic toggle="yes">a<sub>lt</sub></italic>] are the longitudinal and lateral actions, and the corresponding subscripts [<italic toggle="yes">a</italic>, <italic toggle="yes">d</italic>, <italic toggle="yes">c</italic>] are the acceleration, deceleration, and cruise control, respectively. The observation space <italic toggle="yes">z</italic> = [<italic toggle="yes">z<sub>ego</sub></italic>, <italic toggle="yes">z<sub>t1</sub></italic>, <italic toggle="yes">z<sub>t2</sub></italic>,&#x02026;, <italic toggle="yes">z<sub>tN</sub></italic>], where <italic toggle="yes">z<sub>ego</sub></italic> is that of the ego vehicle, and the others are those of background vehicles. The state transition model <italic toggle="yes">T</italic>(<italic toggle="yes">s&#x02032;</italic>, <italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) = Pr(<italic toggle="yes">s&#x02032;</italic>|<italic toggle="yes">s</italic>, <italic toggle="yes">a</italic>) predicts the dynamic randomness of the system affected by the ego vehicle and background vehicles given the current <italic toggle="yes">s<sub>ego</sub></italic> and <italic toggle="yes">a<sub>ego</sub></italic> as follows:<disp-formula id="FD60-sensors-24-07904"><label>(60)</label><mml:math id="mm60" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mi mathvariant="sans-serif">&#x003a0;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">a<sub>ego</sub></italic> is the current action of the ego vehicle, Pr(<italic toggle="yes">s&#x02032;<sub>ego</sub></italic>|<italic toggle="yes">s<sub>ego</sub></italic>, <italic toggle="yes">a<sub>ego</sub></italic>) is the transition probability from the <italic toggle="yes">s<sub>ego</sub></italic> and <italic toggle="yes">a<sub>ego</sub></italic> to those of the next moment as per Equation (61), and <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the transition probability of background vehicles as per Equation (62).
<disp-formula id="FD61-sensors-24-07904"><label>(61)</label><mml:math id="mm62" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>&#x003b8;</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mi mathvariant="sans-serif">&#x00394;</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mi mathvariant="sans-serif">&#x00394;</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mspace width="12pt"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mi mathvariant="sans-serif">&#x00394;</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mi mathvariant="sans-serif">&#x00394;</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mi mathvariant="sans-serif">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mi mathvariant="sans-serif">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD62-sensors-24-07904"><label>(62)</label><mml:math id="mm63" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> need to be conducted as follows:<disp-formula id="FD63-sensors-24-07904"><label>(63)</label><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be expressed as Equation (59), and Pr(<italic toggle="yes">s&#x02032;<sub>i</sub></italic>|<italic toggle="yes">s<sub>i</sub></italic>, <italic toggle="yes">a<sub>i</sub></italic>) can be expressed as follows when <italic toggle="yes">I<sub>m</sub></italic> is supposed to be maintained in the current moment and changes with the sample of the input data in the next moment.
<disp-formula id="FD64-sensors-24-07904"><label>(64)</label><mml:math id="mm68" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:munder><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be obtained based on Equation (61) and Pr(<italic toggle="yes">a<sub>i</sub></italic>|<italic toggle="yes">x&#x02032;<sub>ego</sub></italic>, <italic toggle="yes">x<sub>i</sub></italic>, <italic toggle="yes">I<sub>m,i</sub></italic>) can be conducted as follows, which is based on the deterministic model.
<disp-formula id="FD65-sensors-24-07904"><label>(65)</label><mml:math id="mm70" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">a<sub>i,low</sub></italic> and <italic toggle="yes">a<sub>i,comft</sub></italic> are the lower and upper limits of acceleration. The observation model is the data sequence in the data acquisition and measurement process as follows:<disp-formula id="FD66-sensors-24-07904"><label>(66)</label><mml:math id="mm71" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mi mathvariant="sans-serif">&#x003a0;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>~</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mo>&#x003a3;</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The reward function <italic toggle="yes">R</italic> is required to be mapped with personalized factors, obey the traffic rules, consider driving safety and comfort, and complete in the shortest time, as follows:<disp-formula id="FD67-sensors-24-07904"><label>(67)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where [<italic toggle="yes">R<sub>saf</sub></italic>, <italic toggle="yes">R<sub>goal</sub></italic>, <italic toggle="yes">R<sub>law</sub></italic>, <italic toggle="yes">R<sub>comf</sub></italic>] are the safety, time, traffic rules, and comfort rewards, and [<italic toggle="yes">&#x003bc;</italic><sub>1</sub>, <italic toggle="yes">&#x003bc;</italic><sub>2</sub>, <italic toggle="yes">&#x003bc;</italic><sub>3</sub>, <italic toggle="yes">&#x003bc;</italic><sub>4</sub>] are the weight coefficients. The policy selection pseudocode is shown in <xref rid="sensors-24-07904-f009" ref-type="fig">Figure 9</xref>.</p></sec></sec><sec id="sec4-sensors-24-07904"><title>4. Experiment Platform and Scenario Generation</title><p>The evaluation process for the personalized shared control aims to verify the shared control performance, the decision-making logic, and the key human factors. The rationality and validity of personalized shared control for automated vehicles are proposed to be evaluated in the human-in-the-loop simulation platform and the field test platform based on the automated vehicle and the related testing scenarios.</p><sec id="sec4dot1-sensors-24-07904"><title>4.1. The Real-Time Human-in-the-Loop Simulation Platform</title><p>The core of the personalized simulation platform is the driver and shared controller of the automated system, and the main components of the simulation platform are by-wire assembly and driving simulator. The human-in-the-loop co-simulation platform with a real-time concurrent pattern is shown in <xref rid="sensors-24-07904-f010" ref-type="fig">Figure 10</xref>, which consists of the virtual simulation environment, driving and real-time simulators, and the real-time controller. The shared control runs in dSPACE MicroAutoBox II from dSPACE GmbH in Paderborn, Germany, in real-time, and the driver operates the G29 driving simulator in a dynamic virtual simulation environment from Panosim 8.1. The vehicle dynamic model and traffic scenarios run in the dSPACE simulator, and the real-time controller receives the driver&#x02019;s operation and system states and outputs the control signals as feedback.</p></sec><sec id="sec4dot2-sensors-24-07904"><title>4.2. The Automated Vehicle Platform for Shared Control</title><p>The driver and on-board controller are key components in the field test. Therefore, the types of equipment in the field test platform for shared control consist of the on-board sensors, the on-board controller, the by-wire actuators, and the V2V communication equipment, as shown in <xref rid="sensors-24-07904-f011" ref-type="fig">Figure 11</xref>. The field test platform consists of an ego vehicle and a background vehicle; the RT3000 IMU measures vehicle states, and the RT-Range is equipped with V2V communication equipment. The on-board controller MicroAutoBox II achieves a real-time process for the shared control. The position sensor of the braking pedal and the angle sensor of the steering wheel are equipped to acquire the driver&#x02019;s braking and steering signals, respectively. The steering robot, electronic throttle, and iBooster are used as the actuators for the shared control based on their by-wire performance, while the signals for the actuators are decoupled with those from the driver and the shared control.</p></sec><sec id="sec4dot3-sensors-24-07904"><title>4.3. Scenario Generation Results of Stimuli and RVRF for Human Factors</title><p>The natural driving scenario database with about 30,000 km and 66,600 scenario sections was established for the training and test sets for the RVRF. A total of 30 drivers, aged 25 to 40 years old with more than 10 driving years and a driving frequency of more than 14 h per week, were selected, as shown in <xref rid="sensors-24-07904-t002" ref-type="table">Table 2</xref>. As shown in <xref rid="sensors-24-07904-t002" ref-type="table">Table 2</xref>, typical driving scenarios were collected based on the road topology, and typical driving behavior was collected during the large-scale data acquisition process. &#x003a9;s under time slices of forthright and their <italic toggle="yes">H<sub>2D_mean</sub></italic>(<italic toggle="yes">E<sub>m</sub></italic>, <italic toggle="yes">E<sub>n</sub></italic>) and <italic toggle="yes">S<sub>I</sub></italic> were calculated. Scenario slices with <italic toggle="yes">S<sub>I</sub></italic> &#x0003e; 95% were clustered as the same types, and the clustering results are shown in <xref rid="sensors-24-07904-t002" ref-type="table">Table 2</xref>.</p><p>Three-lane, forthright scenarios without side parking are taken as an example for the verification of the RVRF. The ego vehicle enters into a low-field strength region of the vehicle&#x02013;road space with particle nature and randomness, as shown in <xref rid="sensors-24-07904-f012" ref-type="fig">Figure 12</xref>a. <xref rid="sensors-24-07904-f012" ref-type="fig">Figure 12</xref>b is the extraction result of the topology structure in vehicle&#x02013;road space, consisting of orange nodes for the scenario samples and a black topology structure. Errors&#x02019; RMSs of GNG with good convergence were calculated, as well.</p><p>A total of 3/5 data were taken as the training set, and the other 2/5 as the test set to achieve the accuracy verification results, and the vehicle&#x02013;road spatio-temporal states based on Voronoi are shown in <xref rid="sensors-24-07904-f012" ref-type="fig">Figure 12</xref>, respectively. The node set and boundary set were obtained, and the position and states of the ego vehicle were determined by the RVRF, as shown in <xref rid="sensors-24-07904-f013" ref-type="fig">Figure 13</xref>; the probability of state transition between mutually exclusive regions is represented by an arrow vector colored with probability values. The imitative effect and volatility of the RVRF are compared with the NaSch model with a cell length of 1.5 m, sample period of 1 s, and vehicle length of 4.5 m. The macroscopic traffic flow results show high consistency between RVRF and NaSch models, as shown in <xref rid="sensors-24-07904-f014" ref-type="fig">Figure 14</xref>. According to the specific combination of <italic toggle="yes">&#x003c7;</italic> and <italic toggle="yes">&#x003b5;</italic>, the RVRF can be applied to different geographical or cultural contexts.</p></sec></sec><sec id="sec5-sensors-24-07904"><title>5. Experiment Verification and Performance Analysis</title><sec id="sec5dot1-sensors-24-07904"><title>5.1. Analysis and Evaluation Results of Driving Capability</title><p>The stimuli with typical topology structures generated from the RVRF based on the scenario occurrence frequency from the naturalistic driving scenario database are proposed to reveal the mechanism of driving capability and to analyze its evaluation effectiveness, as shown in <xref rid="sensors-24-07904-f015" ref-type="fig">Figure 15</xref> and <xref rid="sensors-24-07904-f016" ref-type="fig">Figure 16</xref>. Five drivers aged 25 to 40 years old with more than 10 driving years and a driving frequency of more than 14 h per week were chosen to conduct the continuous cyclic tests consisting of more than 6 h and 10 groups. A single test in the cyclic tests lasted from 10 s to 60 s; the end of the cyclic test ended when four continuous accidents occurred by the driver or the driver was unable to continue driving. The S-type function was taken as the static nonlinear element and orders <italic toggle="yes">q</italic> = 3, <italic toggle="yes">n</italic> = 3, <italic toggle="yes">d</italic> = 1 in the dynamic linear element.</p><p>The results of the DCIM of the NO.1 driver are taken as an example, as shown in <xref rid="sensors-24-07904-f017" ref-type="fig">Figure 17</xref>. <xref rid="sensors-24-07904-f017" ref-type="fig">Figure 17</xref>a is the identification result Aid of the longitudinal driving capability in single test NO.165 of the NO.1 cyclic test. <xref rid="sensors-24-07904-f017" ref-type="fig">Figure 17</xref>b is that of the lateral driving capability in single test NO. 219 of the NO.1 cyclic test. <xref rid="sensors-24-07904-f017" ref-type="fig">Figure 17</xref>c,d are the prediction results for Apr of the single tests NO.166 and NO.223. The average accuracy of both Aid and Apr are above 85%, as shown in <xref rid="sensors-24-07904-t003" ref-type="table">Table 3</xref>.</p><p>The longitudinal <italic toggle="yes">ln</italic> and lateral <italic toggle="yes">lt</italic> parameter configurations of the static nonlinear and dynamic linear elements are as follows:<disp-formula id="FD68-sensors-24-07904"><label>(68)</label><mml:math id="mm73" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>&#x003b3;</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mi>&#x003b3;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:msub><mml:mi>Y</mml:mi><mml:mi>&#x003b3;</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">SF<sub>&#x003b3;</sub></italic> and <italic toggle="yes">DY<sub>&#x003b3;</sub></italic> are the parameter dimensions in the S-type function and dynamic linear element, and <italic toggle="yes">H<sub>&#x003b3;</sub></italic> is the total dimension. <italic toggle="yes">&#x003c8;</italic> is calculated from the DCIM and is shown in <xref rid="sensors-24-07904-t004" ref-type="table">Table 4</xref>.</p><p>The principal component dimensions of the longitudinal and lateral driving capabilities of the five tested drivers are {14(<italic toggle="yes">ln</italic>)/21(<italic toggle="yes">lt</italic>), 16/24, 14/22, 16/23, 15/22}, which are 1/3 to 1/4 of <italic toggle="yes">H<sub>&#x003b3;</sub></italic>. The clear and reasonable classification results of the driving capabilities are obtained by combining the subjective and objective classification methods, as shown in <xref rid="sensors-24-07904-f018" ref-type="fig">Figure 18</xref>.</p><p>Valid classification sets from six sets of the cyclic test were used as the training set, and those of the other four sets were used to verify the DCEE. As an example, the longitudinal and lateral training sets of the NO.1 driver are 1792 and 2710, and the verification set are 1105 and 1623, respectively. The longitudinal and lateral regression results are shown in <xref rid="sensors-24-07904-f019" ref-type="fig">Figure 19</xref>a,b, and the effective regression results are obtained based on the DCEE in <xref rid="sensors-24-07904-f020" ref-type="fig">Figure 20</xref>a,b.</p><p>It can be seen that both longitudinal and lateral driving capabilities represent their nonlinear, time-varying, and gradual features, as well as the volatility and randomness in several adjacent repeatability tests. These reasonable and effective results are obtained based on evaluation methods in conjunction with the DCIM and DCEE.</p></sec><sec id="sec5dot2-sensors-24-07904"><title>5.2. Analysis and Evaluation Results for Driving Styles</title><p>The single source-based stimuli with an ego vehicle and a background vehicle (BV) are proposed in the analysis and evaluation for driving styles, as shown in <xref rid="sensors-24-07904-f021" ref-type="fig">Figure 21</xref> and <xref rid="sensors-24-07904-f022" ref-type="fig">Figure 22</xref>. Based on the periodicity and mutability of the longitudinal velocity of the BV, the BV drives with preset velocity sequences, and the driver in the ego vehicle is required to keep a certain level of tension. The scenario of the urban structured road without side parking and 64 drivers consisting of 41 males and 23 females was chosen for data collection.</p><p>Five data from each driver in a typical stimuli situation were selected, and 320 groups of data were taken as the classification sample. The objective features of the driving styles in typical stimuli are shown in <xref rid="sensors-24-07904-f023" ref-type="fig">Figure 23</xref>. Significant differences exist in <italic toggle="yes">a<sub>&#x003c9;M</sub></italic>, <italic toggle="yes">T<sub>sM</sub></italic>, and <italic toggle="yes">T<sub>fM</sub></italic>; ranges of <italic toggle="yes">a<sub>&#x003c9;M</sub></italic> and <italic toggle="yes">T<sub>sV</sub></italic> are smaller, while <italic toggle="yes">T<sub>fM</sub></italic> is bigger. The weight coefficients <italic toggle="yes">&#x003c9;<sub>1</sub></italic> = 10 and <italic toggle="yes">&#x003c9;<sub>2</sub></italic> = 10<sup>3</sup>, and the classification results of the driving styles are shown in <xref rid="sensors-24-07904-f024" ref-type="fig">Figure 24</xref>.</p><p>The questionnaire results of the subjective classification method are shown in <xref rid="sensors-24-07904-t005" ref-type="table">Table 5</xref>. It can be seen that the more radical the driver is, the more steady he is to perceive others, and his passenger tends to perceive him as the radical one. The Sine 3 and Step 3 are proposed as the stimuli for the DSEM with <italic toggle="yes">A<sub>p</sub></italic> = 30, <italic toggle="yes">&#x003c9;</italic> = 2&#x003c0;/40, and <italic toggle="yes">&#x003c6;</italic> = 0, <italic toggle="yes">&#x00393;</italic> = 45, as well as the velocity sequence 0 &#x02192; 20 &#x02192; 50 &#x02192; 70 &#x02192; 50 &#x02192; 20 &#x02192; 0 km/h. A total of 192 groups of data were selected for model training, the Baum&#x02013;Welch method [<xref rid="B37-sensors-24-07904" ref-type="bibr">37</xref>] was proposed for the model training of the DSEM, and the verification data were the 128 groups. The input sets for the DSEM are shown in <xref rid="sensors-24-07904-t006" ref-type="table">Table 6</xref>.</p><p>The four principal elements of the DSEM, the <italic toggle="yes">M</italic>, <italic toggle="yes">N</italic>, training period <italic toggle="yes">TP</italic>, and identification period <italic toggle="yes">IP</italic>, needed to be optimized, and the orthogonal experimental method is proposed with table L<sub>9</sub>(3<sup>4</sup>), where <italic toggle="yes">M</italic> = {4, 8, 12}, <italic toggle="yes">N</italic> = {4, 5, 6}, <italic toggle="yes">TP</italic> = {80, 90, 100}, and <italic toggle="yes">IP</italic> = {80, 85, 90}. Combining input sets and principal elements of the DSEM, optimal identification results are obtained, as shown in <xref rid="sensors-24-07904-t007" ref-type="table">Table 7</xref>. It can be seen that the optimal input sets are those of 1 + 3 and 1 + 2 + 3 with the optimal principal element set {<italic toggle="yes">M</italic> = 12, <italic toggle="yes">N</italic> = 6, <italic toggle="yes">TP</italic> = 80, <italic toggle="yes">IP</italic> = 80}, and the identification accuracy is above 95%.</p><p>Based on the offline evaluation results, the online evaluation of driving styles was conducted with 50 drivers, and the naturalistic driving scenarios consisted of T-road, roundabouts and forthright with one to four lanes. The average online identification periods of driving styles are 1345 s of steady style, 1034 s of general style, and 762 s of radical style, which achieved an acceptable identification period with high accuracy.</p></sec><sec id="sec5dot3-sensors-24-07904"><title>5.3. Results of Personalized Shared Control for Automated Vehicles</title><p>Personalized shared control was verified in the simulation platform and field test, respectively. The scenario configurations consisted of the car-following scenario in the forthright and taking-over scenario in the double lanes and comprised of 18 drivers, consisting of 9 males and 9 females aged from 24 to 45 years old with more than 14 driving hours per week, as shown in <xref rid="sensors-24-07904-f025" ref-type="fig">Figure 25</xref> and <xref rid="sensors-24-07904-f026" ref-type="fig">Figure 26</xref>.</p><p>The radical male driver NO.1 and steady male driver NO.7 are taken as examples in the car-following simulation scenario under strong and weak driving capabilities, as shown in <xref rid="sensors-24-07904-f027" ref-type="fig">Figure 27</xref> and <xref rid="sensors-24-07904-f028" ref-type="fig">Figure 28</xref>. The car-following effects of the human mode and automated driving mode have significant differences under strong driving capability, and similarities exist in the car-following sequences in the three modes. The drivers obtained high and stable authority from the DAAS. In contrast, the radical driver obtained frequent velocity fluctuations and even conducted emergency braking, while the steady driver showed a tendency to drive away from a BV under weak driving capabilities. In this case, the drivers obtained lower authorities but the personalized system shared more from the DAAS to guarantee driving safety and improve the driver&#x02019;s comfort.</p><p>The composite index &#x0039e; of the NO.1 driver and the subindex are shown in <xref rid="sensors-24-07904-t008" ref-type="table">Table 8</xref>. The shared control mode combines system features from both the human mode and automated driving mode, and the smallest and optimal &#x0039e; can be obtained in different degrees of driving capabilities.</p><p>The results for the NO.1 and NO.7 drivers in the taking-over simulation scenario under strong and weak driving capabilities are shown in <xref rid="sensors-24-07904-f029" ref-type="fig">Figure 29</xref> and <xref rid="sensors-24-07904-f030" ref-type="fig">Figure 30</xref>. Similar results to those in the car-following scenario were obtained, except for a frequent steering wheel angle fluctuation from the radical driver. Shared control can guarantee driving safety, improve driver&#x02019;s comfort, and reduce the workload in both longitudinal and lateral scenarios.</p><p>The &#x0039e; of the NO.1 driver and the subindex are shown in <xref rid="sensors-24-07904-t009" ref-type="table">Table 9</xref>. The smallest and optimal &#x0039e;s were obtained in different degrees of driving capabilities, as well.</p><p>Tests for the NO.1 and NO.7 drivers in the car-following field test under strong and weak driving capabilities were conducted. The preset velocity sequence of a BV is a step stimulate of 0 &#x02192; 40 km/h, and similar results with those in the car-following simulation scenarios were obtained. The &#x0039e; of the NO.1 driver and the subindex are shown in <xref rid="sensors-24-07904-t010" ref-type="table">Table 10</xref>. The smallest and optimal &#x0039e;s were obtained, which is similar to those in the simulation results.</p><p>The radical male driver NO.1 and steady male driver NO.7 in the taking-over field test under strong and weak driving capabilities were conducted. The BV drove at a constant speed of 30 km/h, and the initial speed of the ego vehicle was 40 km/h. Similar results to those in the simulation scenario were obtained. Shared control can guarantee driving safety, improve driver&#x02019;s comfort, and reduce workloads in both longitudinal and lateral scenarios. The &#x0039e; of the NO.1 driver and the subindex are shown in <xref rid="sensors-24-07904-t011" ref-type="table">Table 11</xref>, which shows the optimal performance of shared control. Furthermore, it is worth noting that although humans make moral decisions in principle, there are individual and cultural differences in moral justice [<xref rid="B38-sensors-24-07904" ref-type="bibr">38</xref>], which should be the principle of observance during personalized shared control.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-24-07904"><title>6. Conclusions</title><p>A personalized shared control, while considering drivers&#x02019; driving capabilities and styles, is proposed to improve the acceptance and adaptation of shared control by human drivers. As per theoretical and validation analyses, the following conclusions can be obtained:</p><p>The simulation scenario generation method for human factors was established. The RVRF theory reveals a wave-particle duality of macro- and micro-traffic flows. Stimuli and the RVRF achieved the ability to motivate driving capability and styles to the maximum extent. Drivers&#x02019; driving capability was defined and evaluated, the average accuracy of both Aid and Apr were above 85%, and its physical properties with nonlinearity, time gradient, randomness, and predictive ability were extracted and analyzed. Drivers&#x02019; driving styles were analyzed, characterized, and evaluated, and their accuracy was higher than 95% within a short identification period.</p><p>The MOMDP-based decision-making process shows advantages in dealing with uncertain motion intention and personalized logic. The personalized shared control system obtained excellent performance in both the human-in-the-loop simulation platform and field tests. The proposed system combines the randomness of human factor attributes in a single test, multi-objective optimization, and personalized characteristics of the driver and the automated driving system. Personalized shared control can achieve better performance in driving safety, comfort, and workload, corresponding to different driving capability degrees and driving style types than those only driven by human drivers or automated systems.</p><p>With the advantage of deep mixing decision-making between human and automated systems, personalized shared control will achieve better driver acceptance in future automated driving tasks.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, B.S.; Data curation, B.S. and S.Z.; Investigation, B.S. and S.Z.; Methodology, B.S.; Resources, G.W.; Supervision, G.W.; Writing&#x02014;original draft, B.S.; Writing&#x02014;review and editing, Y.S. and F.X. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The authors do not have permission to share data.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></notes><ref-list><title>References</title><ref id="B1-sensors-24-07904"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>T.</given-names></name>
<name><surname>Pan</surname><given-names>H.</given-names></name>
<name><surname>Sun</surname><given-names>W.</given-names></name>
<name><surname>Gao</surname><given-names>H.</given-names></name>
</person-group><article-title>Sine resistance network-based motion planning approach for autonomous electric vehicles in dynamic environments</article-title><source>IEEE Trans. Transp. Electrif.</source><year>2022</year><volume>8</volume><fpage>2862</fpage><lpage>2873</lpage><pub-id pub-id-type="doi">10.1109/TTE.2022.3151852</pub-id></element-citation></ref><ref id="B2-sensors-24-07904"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>Z.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>C.</given-names></name>
<name><surname>Chen</surname><given-names>P.</given-names></name>
</person-group><article-title>Energy-Efficient Task Offloading for Distributed Edge Computing in Vehicular Networks</article-title><source>IEEE Trans. Veh. Technol.</source><year>2024</year><volume>73</volume><fpage>14056</fpage><lpage>14061</lpage><pub-id pub-id-type="doi">10.1109/TVT.2024.3395893</pub-id></element-citation></ref><ref id="B3-sensors-24-07904"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bonnefon</surname><given-names>J.F.</given-names></name>
<name><surname>Shariff</surname><given-names>A.</given-names></name>
<name><surname>Rahwan</surname><given-names>I.</given-names></name>
</person-group><article-title>The social dilemma of autonomous vehicles</article-title><source>Science</source><year>2016</year><volume>352</volume><fpage>1573</fpage><lpage>2576</lpage><pub-id pub-id-type="doi">10.1126/science.aaf2654</pub-id><pub-id pub-id-type="pmid">27339987</pub-id>
</element-citation></ref><ref id="B4-sensors-24-07904"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hakobyan</surname><given-names>G.</given-names></name>
<name><surname>Yang</surname><given-names>B.</given-names></name>
</person-group><article-title>High-performance automotive radar: A review of signal processing algorithms and modulation schemes</article-title><source>IEEE Signal Process. Mag.</source><year>2019</year><volume>36</volume><fpage>32</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1109/MSP.2019.2911722</pub-id></element-citation></ref><ref id="B5-sensors-24-07904"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Karle</surname><given-names>P.</given-names></name>
<name><surname>Geisslinger</surname><given-names>M.</given-names></name>
<name><surname>Betz</surname><given-names>J.</given-names></name>
<name><surname>Lienkamp</surname><given-names>M.</given-names></name>
</person-group><article-title>Scenario understanding and motion prediction for autonomous vehicles&#x02014;Review and comparison</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>23</volume><fpage>16962</fpage><lpage>16982</lpage><pub-id pub-id-type="doi">10.1109/TITS.2022.3156011</pub-id></element-citation></ref><ref id="B6-sensors-24-07904"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Tian</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>L.</given-names></name>
</person-group><article-title>A novel adaptive steering torque control approach for human&#x02013;machine cooperation autonomous vehicles</article-title><source>IEEE Trans. Transp. Electrif.</source><year>2021</year><volume>7</volume><fpage>2516</fpage><lpage>2529</lpage><pub-id pub-id-type="doi">10.1109/TTE.2021.3083679</pub-id></element-citation></ref><ref id="B7-sensors-24-07904"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Saito</surname><given-names>T.</given-names></name>
<name><surname>Wada</surname><given-names>T.</given-names></name>
<name><surname>Sonoda</surname><given-names>K.</given-names></name>
</person-group><article-title>Control authority transfer method for automated-to-manual driving via a shared authority mode</article-title><source>IEEE Trans. Intell. Veh.</source><year>2018</year><volume>3</volume><fpage>198</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1109/TIV.2018.2804167</pub-id></element-citation></ref><ref id="B8-sensors-24-07904"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Anderson</surname><given-names>S.J.</given-names></name>
<name><surname>Karumanchi</surname><given-names>S.B.</given-names></name>
<name><surname>Iagnemma</surname><given-names>K.</given-names></name>
<name><surname>Walker</surname><given-names>J.M.</given-names></name>
</person-group><article-title>The intelligent copilot: A constraint-based approach to shared-adaptive control of ground vehicles</article-title><source>IEEE Intell. Transp. Syst. Mag.</source><year>2013</year><volume>5</volume><fpage>45</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1109/MITS.2013.2247796</pub-id></element-citation></ref><ref id="B9-sensors-24-07904"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Soualmi</surname><given-names>B.</given-names></name>
<name><surname>Sentouh</surname><given-names>C.</given-names></name>
<name><surname>Popieul</surname><given-names>J.C.</given-names></name>
<name><surname>Debernard</surname><given-names>S.</given-names></name>
</person-group><article-title>Automation-driver cooperative driving in presence of undetected obstacles</article-title><source>Control Eng. Pract.</source><year>2014</year><volume>24</volume><fpage>106</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1016/j.conengprac.2013.11.015</pub-id></element-citation></ref><ref id="B10-sensors-24-07904"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>M.</given-names></name>
<name><surname>Gao</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Jiang</surname><given-names>Z.P.</given-names></name>
</person-group><article-title>Data-driven shared steering control of semi-autonomous vehicles</article-title><source>IEEE Trans. Hum.-Mach. Syst.</source><year>2019</year><volume>49</volume><fpage>350</fpage><lpage>361</lpage><pub-id pub-id-type="doi">10.1109/THMS.2019.2900409</pub-id></element-citation></ref><ref id="B11-sensors-24-07904"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ghasemi</surname><given-names>A.H.</given-names></name>
<name><surname>Jayakumar</surname><given-names>P.</given-names></name>
<name><surname>Gillespie</surname><given-names>R.B.</given-names></name>
</person-group><article-title>Shared control architectures for vehicle steering</article-title><source>Cogn. Technol. Work</source><year>2019</year><volume>21</volume><fpage>699</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1007/s10111-019-00560-9</pub-id></element-citation></ref><ref id="B12-sensors-24-07904"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rahman</surname><given-names>M.</given-names></name>
<name><surname>Chowdhury</surname><given-names>M.</given-names></name>
<name><surname>Xie</surname><given-names>Y.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
</person-group><article-title>Review of microscopic lane-changing models and future research opportunities</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2013</year><volume>14</volume><fpage>1942</fpage><lpage>1956</lpage><pub-id pub-id-type="doi">10.1109/TITS.2013.2272074</pub-id></element-citation></ref><ref id="B13-sensors-24-07904"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>H.</given-names></name>
<name><surname>Kim</surname><given-names>H.</given-names></name>
<name><surname>Choi</surname><given-names>S.</given-names></name>
</person-group><article-title>Driving skill modeling using neural networks for performance-based haptic assistance</article-title><source>IEEE Trans. Human-Mach. Syst.</source><year>2021</year><volume>51</volume><fpage>198</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1109/THMS.2021.3061409</pub-id></element-citation></ref><ref id="B14-sensors-24-07904"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Poorna</surname><given-names>S.S.</given-names></name>
<name><surname>Arsha</surname><given-names>V.V.</given-names></name>
<name><surname>Aparna</surname><given-names>P.T.A.</given-names></name>
<name><surname>Gopal</surname><given-names>P.</given-names></name>
<name><surname>Nair</surname><given-names>G.J.</given-names></name>
</person-group><article-title>Drowsiness detection for safe driving using PCA EEG signals</article-title><source>Progress Comput. Anal. Netw.</source><year>2018</year><volume>710</volume><fpage>419</fpage><lpage>428</lpage></element-citation></ref><ref id="B15-sensors-24-07904"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Mao</surname><given-names>S.</given-names></name>
</person-group><article-title>Unsupervised drowsy driving detection with RFID</article-title><source>IEEE Trans. Veh. Technol.</source><year>2020</year><volume>69</volume><fpage>8151</fpage><lpage>8163</lpage><pub-id pub-id-type="doi">10.1109/TVT.2020.2995835</pub-id></element-citation></ref><ref id="B16-sensors-24-07904"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Martinez</surname><given-names>C.M.</given-names></name>
<name><surname>Heucke</surname><given-names>M.</given-names></name>
<name><surname>Wang</surname><given-names>F.Y.</given-names></name>
<name><surname>Gao</surname><given-names>B.</given-names></name>
<name><surname>Cao</surname><given-names>D.</given-names></name>
</person-group><article-title>Driving style recognition for intelligent vehicle control and advanced driver assistance: A survey</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2018</year><volume>19</volume><fpage>666</fpage><lpage>676</lpage><pub-id pub-id-type="doi">10.1109/TITS.2017.2706978</pub-id></element-citation></ref><ref id="B17-sensors-24-07904"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Suzdaleva</surname><given-names>E.</given-names></name>
<name><surname>Nagy</surname><given-names>I.</given-names></name>
</person-group><article-title>An online estimation of driving style using data-dependent pointer model</article-title><source>Transp. Res. Part C Emerg. Technol.</source><year>2018</year><volume>86</volume><fpage>23</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/j.trc.2017.11.001</pub-id></element-citation></ref><ref id="B18-sensors-24-07904"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Xi</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>H.</given-names></name>
</person-group><article-title>Modeling and recognizing driver behavior based on driving data: A survey</article-title><source>Math. Probl. Eng.</source><year>2014</year><volume>2014</volume><fpage>245641</fpage><pub-id pub-id-type="doi">10.1155/2014/245641</pub-id></element-citation></ref><ref id="B19-sensors-24-07904"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>G.</given-names></name>
<name><surname>Zhu</surname><given-names>F.</given-names></name>
<name><surname>Qu</surname><given-names>X.</given-names></name>
<name><surname>Cheng</surname><given-names>B.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Green</surname><given-names>P.</given-names></name>
</person-group><article-title>Driving style classification based on driving operational pictures</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>90180</fpage><lpage>90189</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2926494</pub-id></element-citation></ref><ref id="B20-sensors-24-07904"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Meiring</surname><given-names>G.</given-names></name>
<name><surname>Myburgh</surname><given-names>H.</given-names></name>
</person-group><article-title>A review of intelligent driving style analysis systems and related artificial intelligence algorithms</article-title><source>Sensors</source><year>2015</year><volume>15</volume><fpage>30653</fpage><lpage>30682</lpage><pub-id pub-id-type="doi">10.3390/s151229822</pub-id><pub-id pub-id-type="pmid">26690164</pub-id>
</element-citation></ref><ref id="B21-sensors-24-07904"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Peng</surname><given-names>X.</given-names></name>
<name><surname>Hu</surname><given-names>L.</given-names></name>
<name><surname>Cao</surname><given-names>D.</given-names></name>
</person-group><article-title>Study on the driving style adaptive vehicle longitudinal control strategy</article-title><source>IEEE/CAA J. Autom. Sin.</source><year>2020</year><volume>7</volume><fpage>1107</fpage><lpage>1115</lpage><pub-id pub-id-type="doi">10.1109/JAS.2020.1003261</pub-id></element-citation></ref><ref id="B22-sensors-24-07904"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gilman</surname><given-names>E.</given-names></name>
<name><surname>Keskinarkaus</surname><given-names>A.</given-names></name>
<name><surname>Tamminen</surname><given-names>S.</given-names></name>
<name><surname>Pirttikangas</surname><given-names>S.</given-names></name>
<name><surname>R&#x000f6;ning</surname><given-names>J.</given-names></name>
<name><surname>Riekki</surname><given-names>J.</given-names></name>
</person-group><article-title>Personalised assistance for fuel-efficient driving</article-title><source>Transp. Res. Part C Emerg. Technol.</source><year>2015</year><volume>58</volume><fpage>681</fpage><lpage>705</lpage><pub-id pub-id-type="doi">10.1016/j.trc.2015.02.007</pub-id></element-citation></ref><ref id="B23-sensors-24-07904"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>K.T.</given-names></name>
<name><surname>Chen</surname><given-names>H.Y.W.</given-names></name>
</person-group><article-title>Driving style clustering using naturalistic driving data</article-title><source>Transp. Res. Rec.</source><year>2019</year><volume>2673</volume><fpage>176</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1177/0361198119845360</pub-id></element-citation></ref><ref id="B24-sensors-24-07904"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mudgal</surname><given-names>A.</given-names></name>
<name><surname>Hallmark</surname><given-names>S.</given-names></name>
<name><surname>Carriquiry</surname><given-names>A.</given-names></name>
<name><surname>Gkritza</surname><given-names>K.</given-names></name>
</person-group><article-title>Driving behavior at a roundabout: A hierarchical Bayesian regression analysis</article-title><source>Transp. Res. Part D Transp. Environ.</source><year>2014</year><volume>26</volume><fpage>20</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/j.trd.2013.10.003</pub-id></element-citation></ref><ref id="B25-sensors-24-07904"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qi</surname><given-names>G.</given-names></name>
<name><surname>Du</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Hounsell</surname><given-names>N.</given-names></name>
<name><surname>Jia</surname><given-names>Y.</given-names></name>
</person-group><article-title>What is the appropriate temporal distance range for driving style analysis?</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2016</year><volume>17</volume><fpage>1393</fpage><lpage>1403</lpage><pub-id pub-id-type="doi">10.1109/TITS.2015.2502985</pub-id></element-citation></ref><ref id="B26-sensors-24-07904"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Markkula</surname><given-names>G.</given-names></name>
<name><surname>Romano</surname><given-names>R.</given-names></name>
<name><surname>Madigan</surname><given-names>R.</given-names></name>
<name><surname>Fox</surname><given-names>C.W.</given-names></name>
<name><surname>Giles</surname><given-names>O.T.</given-names></name>
<name><surname>Merat</surname><given-names>N.</given-names></name>
</person-group><article-title>Models of human decision-making as tools for estimating and optimizing impacts of vehicle automation</article-title><source>Transp. Res. Rec.</source><year>2018</year><volume>2672</volume><fpage>153</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1177/0361198118792131</pub-id></element-citation></ref><ref id="B27-sensors-24-07904"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>M.</given-names></name>
</person-group><article-title>Shared control with a novel dynamic authority allocation strategy based on game theory and driving safety field</article-title><source>Mech. Syst. Signal Process.</source><year>2019</year><volume>124</volume><fpage>199</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2019.01.040</pub-id></element-citation></ref><ref id="B28-sensors-24-07904"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>C.</given-names></name>
<name><surname>Naghdy</surname><given-names>F.</given-names></name>
<name><surname>Du</surname><given-names>H.</given-names></name>
<name><surname>Huang</surname><given-names>H.</given-names></name>
</person-group><article-title>Shared control of highly automated vehicles using steer-by-wire systems</article-title><source>IEEE/CAA J. Autom. Sin.</source><year>2019</year><volume>6</volume><fpage>410</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1109/JAS.2019.1911384</pub-id></element-citation></ref><ref id="B29-sensors-24-07904"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Zhu</surname><given-names>X.</given-names></name>
</person-group><article-title>A robust design of hybrid fuzzy controller with fuzzy decision tree for autonomous intelligent parking system</article-title><source>Proceedings of the 2014 American Control Conference</source><conf-loc>Portland, OR, USA</conf-loc><conf-date>4&#x02013;6 June 2014</conf-date><fpage>5282</fpage><lpage>5287</lpage></element-citation></ref><ref id="B30-sensors-24-07904"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Ozguner</surname><given-names>U.</given-names></name>
</person-group><article-title>Human driver model and driver decision making for intersection driving</article-title><source>Proceedings of the 2007 IEEE Intelligent Vehicles Symposium</source><conf-loc>Istanbul, Turkey</conf-loc><conf-date>13&#x02013;15 June 2007</conf-date><fpage>642</fpage><lpage>647</lpage></element-citation></ref><ref id="B31-sensors-24-07904"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gao</surname><given-names>H.</given-names></name>
<name><surname>Shi</surname><given-names>G.</given-names></name>
<name><surname>Wang</surname><given-names>K.</given-names></name>
<name><surname>Xie</surname><given-names>G.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Research on decision-making of autonomous vehicle following based on reinforcement learning method</article-title><source>IR</source><year>2019</year><volume>46</volume><fpage>444</fpage><lpage>452</lpage><pub-id pub-id-type="doi">10.1108/IR-07-2018-0154</pub-id></element-citation></ref><ref id="B32-sensors-24-07904"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>C.</given-names></name>
<name><surname>Zhao</surname><given-names>W.</given-names></name>
<name><surname>Xu</surname><given-names>C.</given-names></name>
</person-group><article-title>Decision-making and planning method for autonomous vehicles based on motivation and risk assessment</article-title><source>IEEE Trans. Veh. Technol.</source><year>2021</year><volume>70</volume><fpage>107</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1109/TVT.2021.3049794</pub-id></element-citation></ref><ref id="B33-sensors-24-07904"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>He</surname><given-names>X.</given-names></name>
<name><surname>Yang</surname><given-names>H.</given-names></name>
<name><surname>Hu</surname><given-names>Z.</given-names></name>
<name><surname>Lv</surname><given-names>C.</given-names></name>
</person-group><article-title>Robust lane change decision making for autonomous vehicles: An observation adversarial reinforcement learning approach</article-title><source>IEEE Trans. Intell. Veh.</source><year>2022</year><volume>8</volume><fpage>184</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1109/TIV.2022.3165178</pub-id></element-citation></ref><ref id="B34-sensors-24-07904"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Heinke</surname><given-names>D.</given-names></name>
<name><surname>Hamker</surname><given-names>F.H.</given-names></name>
</person-group><article-title>Comparing neural networks: A benchmark on growing neural gas, growing cell structures, and fuzzy ARTMAP</article-title><source>IEEE Trans. Neural Netw.</source><year>1998</year><volume>9</volume><fpage>1279</fpage><lpage>1291</lpage><pub-id pub-id-type="doi">10.1109/72.728377</pub-id><pub-id pub-id-type="pmid">18255809</pub-id>
</element-citation></ref><ref id="B35-sensors-24-07904"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rutledge</surname><given-names>D.N.</given-names></name>
</person-group><article-title>Comparison of principal components analysis, independent components analysis and common components analysis</article-title><source>J. Anal. Test.</source><year>2018</year><volume>2</volume><fpage>235</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1007/s41664-018-0065-5</pub-id></element-citation></ref><ref id="B36-sensors-24-07904"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>&#x000d6;zkan</surname><given-names>T.</given-names></name>
<name><surname>Lajunen</surname><given-names>T.</given-names></name>
</person-group><article-title>What causes the differences in driving between young men and women? The effects of gender roles and sex on young drivers&#x02019; driving behaviour and self-assessment of skills</article-title><source>Transp. Res. Part F Traffic Psychol. Behav.</source><year>2006</year><volume>9</volume><fpage>269</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1016/j.trf.2006.01.005</pub-id></element-citation></ref><ref id="B37-sensors-24-07904"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lewis</surname><given-names>M.E.</given-names></name>
<name><surname>Puterman</surname><given-names>M.L.</given-names></name>
</person-group><article-title>A probabilistic analysis of bias optimality in unichain Markov decision processes</article-title><source>IEEE Trans. Autom. Control</source><year>2001</year><volume>46</volume><fpage>96</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1109/9.898698</pub-id></element-citation></ref><ref id="B38-sensors-24-07904"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Awad</surname><given-names>E.</given-names></name>
<name><surname>Dsouza</surname><given-names>S.</given-names></name>
<name><surname>Kim</surname><given-names>R.</given-names></name>
<name><surname>Schulz</surname><given-names>J.</given-names></name>
<name><surname>Henrich</surname><given-names>J.</given-names></name>
<name><surname>Shariff</surname><given-names>A.</given-names></name>
<name><surname>Bonnefon</surname><given-names>J.F.</given-names></name>
<name><surname>Rahwan</surname><given-names>I.</given-names></name>
</person-group><article-title>The moral machine experiment</article-title><source>Nature</source><year>2018</year><volume>563</volume><fpage>59</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0637-6</pub-id><pub-id pub-id-type="pmid">30356211</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-24-07904-f001"><label>Figure 1</label><caption><p>The system framework of RVRF.</p></caption><graphic xlink:href="sensors-24-07904-g001" position="float"/></fig><fig position="float" id="sensors-24-07904-f002"><label>Figure 2</label><caption><p>Coupling relationships among human factors.</p></caption><graphic xlink:href="sensors-24-07904-g002" position="float"/></fig><fig position="float" id="sensors-24-07904-f003"><label>Figure 3</label><caption><p>The analysis and evaluation framework of driver&#x02019;s driving capability.</p></caption><graphic xlink:href="sensors-24-07904-g003" position="float"/></fig><fig position="float" id="sensors-24-07904-f004"><label>Figure 4</label><caption><p>The Hammerstein identification process is based on the offline identification model of driving capability.</p></caption><graphic xlink:href="sensors-24-07904-g004" position="float"/></fig><fig position="float" id="sensors-24-07904-f005"><label>Figure 5</label><caption><p>The characterization and evaluation framework of drivers&#x02019; driving styles.</p></caption><graphic xlink:href="sensors-24-07904-g005" position="float"/></fig><fig position="float" id="sensors-24-07904-f006"><label>Figure 6</label><caption><p>The framework of the personalized shared control.</p></caption><graphic xlink:href="sensors-24-07904-g006" position="float"/></fig><fig position="float" id="sensors-24-07904-f007"><label>Figure 7</label><caption><p>The coupling mechanism of motion intentions in micro-traffic scenarios.</p></caption><graphic xlink:href="sensors-24-07904-g007" position="float"/></fig><fig position="float" id="sensors-24-07904-f008"><label>Figure 8</label><caption><p>The framework of <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>}</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the online TSIM.</p></caption><graphic xlink:href="sensors-24-07904-g008" position="float"/></fig><fig position="float" id="sensors-24-07904-f009"><label>Figure 9</label><caption><p>The policy selection pseudocode.</p></caption><graphic xlink:href="sensors-24-07904-g009" position="float"/></fig><fig position="float" id="sensors-24-07904-f010"><label>Figure 10</label><caption><p>Human-in-the-loop real-time co-simulation platform.</p></caption><graphic xlink:href="sensors-24-07904-g010" position="float"/></fig><fig position="float" id="sensors-24-07904-f011"><label>Figure 11</label><caption><p>Field test platform for shared control.</p></caption><graphic xlink:href="sensors-24-07904-g011" position="float"/></fig><fig position="float" id="sensors-24-07904-f012"><label>Figure 12</label><caption><p>Topology structure results.</p></caption><graphic xlink:href="sensors-24-07904-g012" position="float"/></fig><fig position="float" id="sensors-24-07904-f013"><label>Figure 13</label><caption><p>The result of the vehicle&#x02013;road spatio-temporal states in the virtual micro RVRF.</p></caption><graphic xlink:href="sensors-24-07904-g013" position="float"/></fig><fig position="float" id="sensors-24-07904-f014"><label>Figure 14</label><caption><p>Comparison of traffic flow fluctuation results in the virtual macro RVRF.</p></caption><graphic xlink:href="sensors-24-07904-g014" position="float"/></fig><fig position="float" id="sensors-24-07904-f015"><label>Figure 15</label><caption><p>Configuration of longitudinal stimuli.</p></caption><graphic xlink:href="sensors-24-07904-g015" position="float"/></fig><fig position="float" id="sensors-24-07904-f016"><label>Figure 16</label><caption><p>Configuration of lateral stimuli.</p></caption><graphic xlink:href="sensors-24-07904-g016" position="float"/></fig><fig position="float" id="sensors-24-07904-f017"><label>Figure 17</label><caption><p>The identification and prediction results of typical longitudinal and lateral driving capabilities. (<bold>a</bold>) Aid of the longitudinal driving capability. (<bold>b</bold>) Aid of the lateral driving capability. (<bold>c</bold>) Apr in single tests NO. 166. (<bold>d</bold>) Apr in single tests NO. 223.</p></caption><graphic xlink:href="sensors-24-07904-g017" position="float"/></fig><fig position="float" id="sensors-24-07904-f018"><label>Figure 18</label><caption><p>Classification results of the longitudinal and lateral driving capabilities.</p></caption><graphic xlink:href="sensors-24-07904-g018" position="float"/></fig><fig position="float" id="sensors-24-07904-f019"><label>Figure 19</label><caption><p>Regression results of typical longitudinal and lateral driving capabilities. Longitudinal and lateral prediction results of the 8th cyclic test of the NO.1 driver are shown in (<bold>a</bold>,<bold>b</bold>).</p></caption><graphic xlink:href="sensors-24-07904-g019" position="float"/></fig><fig position="float" id="sensors-24-07904-f020"><label>Figure 20</label><caption><p>Fitting results of the driving capability evaluation equation. (<bold>a</bold>) The longitudinal effective regression results. (<bold>b</bold>) The lateral effective regression results.</p></caption><graphic xlink:href="sensors-24-07904-g020" position="float"/></fig><fig position="float" id="sensors-24-07904-f021"><label>Figure 21</label><caption><p>The configuration of the longitudinal stimuli.</p></caption><graphic xlink:href="sensors-24-07904-g021" position="float"/></fig><fig position="float" id="sensors-24-07904-f022"><label>Figure 22</label><caption><p>The configuration of the lateral stimuli.</p></caption><graphic xlink:href="sensors-24-07904-g022" position="float"/></fig><fig position="float" id="sensors-24-07904-f023"><label>Figure 23</label><caption><p>Extraction results of driving style features in typical longitudinal stimuli.</p></caption><graphic xlink:href="sensors-24-07904-g023" position="float"/></fig><fig position="float" id="sensors-24-07904-f024"><label>Figure 24</label><caption><p>Classification results of driving styles in typical longitudinal stimuli.</p></caption><graphic xlink:href="sensors-24-07904-g024" position="float"/></fig><fig position="float" id="sensors-24-07904-f025"><label>Figure 25</label><caption><p>Configuration of the car-following forthright scenario.</p></caption><graphic xlink:href="sensors-24-07904-g025" position="float"/></fig><fig position="float" id="sensors-24-07904-f026"><label>Figure 26</label><caption><p>The configuration of the taking-over scenario in double lanes.</p></caption><graphic xlink:href="sensors-24-07904-g026" position="float"/></fig><fig position="float" id="sensors-24-07904-f027"><label>Figure 27</label><caption><p>Simulation results of three modes corresponding to strong driving capability in the forthright.</p></caption><graphic xlink:href="sensors-24-07904-g027" position="float"/></fig><fig position="float" id="sensors-24-07904-f028"><label>Figure 28</label><caption><p>Simulation results of three modes corresponding to weak driving capability in the forthright.</p></caption><graphic xlink:href="sensors-24-07904-g028" position="float"/></fig><fig position="float" id="sensors-24-07904-f029"><label>Figure 29</label><caption><p>Simulation results of three modes corresponding to strong driving capability in double lanes.</p></caption><graphic xlink:href="sensors-24-07904-g029a" position="float"/><graphic xlink:href="sensors-24-07904-g029b" position="float"/></fig><fig position="float" id="sensors-24-07904-f030"><label>Figure 30</label><caption><p>Simulation results of three modes corresponding to weak driving capability in double lanes.</p></caption><graphic xlink:href="sensors-24-07904-g030" position="float"/></fig><table-wrap position="float" id="sensors-24-07904-t001"><object-id pub-id-type="pii">sensors-24-07904-t001_Table 1</object-id><label>Table 1</label><caption><p>Questionnaire for driving styles.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Questionnaire Content</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">1 Point</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">2 Point</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">3 Point</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">4 Point</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">5 Point</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Feeling for the driver when as the passenger</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Very soft</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Soft</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Comfort</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Relatively radical</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Radical</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Feeling for passenger when as the driver</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Very soft</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Soft</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Comfort</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Relatively radical</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Radical</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07904-t002"><object-id pub-id-type="pii">sensors-24-07904-t002_Table 2</object-id><label>Table 2</label><caption><p>Clustering results of vehicle&#x02013;road space situation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Main Case</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Subcase</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Results</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Forthright</td><td align="center" valign="middle" rowspan="1" colspan="1">Lane number</td><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">(1) Side parking<break/>(2) Sparseness<break/>degree of the<break/>Surrounding<break/>traffic</td><td align="center" valign="middle" rowspan="1" colspan="1">182</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Curve road</td><td align="center" valign="middle" rowspan="1" colspan="1">Curve and lanes</td><td align="center" valign="middle" rowspan="1" colspan="1">519</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">T-road</td><td align="center" valign="middle" rowspan="1" colspan="1">Angle and lanes</td><td align="center" valign="middle" rowspan="1" colspan="1">198</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Intersection</td><td align="center" valign="middle" rowspan="1" colspan="1">Traffic lights</td><td align="center" valign="middle" rowspan="1" colspan="1">341</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Roundabout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Curvature and driving actions</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">447</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07904-t003"><object-id pub-id-type="pii">sensors-24-07904-t003_Table 3</object-id><label>Table 3</label><caption><p>Identification and prediction accuracies of DCIM.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Driver NO.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Aid</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Apr</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.26%/95.92%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.31%/95.16%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.47%/96.77%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.44%/95.93%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.23%/94.81%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.93%/93.44%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.37%/96.42%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.62%/96.21%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.22%/93.01%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.94%/92.89%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.710%/95.386%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.048%/94.726%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07904-t004"><object-id pub-id-type="pii">sensors-24-07904-t004_Table 4</object-id><label>Table 4</label><caption><p>Parameter dimensions of DCIM.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dimensions</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><italic toggle="yes">&#x003b3;</italic> = <italic toggle="yes">ln</italic></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><italic toggle="yes">&#x003b3;</italic> = <italic toggle="yes">lt</italic></th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">SF<sub>&#x003b3;</sub></italic>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">DY<sub>&#x003b3;</sub></italic>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07904-t005"><object-id pub-id-type="pii">sensors-24-07904-t005_Table 5</object-id><label>Table 5</label><caption><p>Results of questionnaire classification method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Driving Styles</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean (Q1/Q2)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Variance (Q1/Q2)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Radical type</td><td align="center" valign="middle" rowspan="1" colspan="1">2.04/4.35</td><td align="center" valign="middle" rowspan="1" colspan="1">0.23/0.18</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">General type</td><td align="center" valign="middle" rowspan="1" colspan="1">3.32/3.53</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39/0.19</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Steady type</td><td align="center" valign="middle" rowspan="1" colspan="1">4.12/1.58</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07/0.12</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" rowspan="1" colspan="1">3.44/3.34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.29/0.17</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">The total variance is 0.79, and Cronbach &#x003b1; = 0.835 &#x0003e; 0.6</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07904-t006"><object-id pub-id-type="pii">sensors-24-07904-t006_Table 6</object-id><label>Table 6</label><caption><p>Input sets for the DSEM.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">NO.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Input Set</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">States</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">Driver operation set</td><td align="center" valign="middle" rowspan="1" colspan="1"><italic toggle="yes">P<sub>ac</sub></italic>, <italic toggle="yes">P<sub>ma</sub></italic></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">State set of the ego vehicle</td><td align="center" valign="middle" rowspan="1" colspan="1"><italic toggle="yes">V<sub>x</sub></italic>, <italic toggle="yes">a<sub>x</sub></italic></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Relative states</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><italic toggle="yes">D<sub>Vln</sub></italic>, <italic toggle="yes">D<sub>Aln</sub></italic></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07904-t007"><object-id pub-id-type="pii">sensors-24-07904-t007_Table 7</object-id><label>Table 7</label><caption><p>Identification and accuracies in types stimuli.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Stimuli</th><th colspan="6" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Input Set to DSEM(%)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 + 2</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 + 3</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 + 3</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 + 2 + 3</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sine3</td><td align="center" valign="middle" rowspan="1" colspan="1">92.2</td><td align="center" valign="middle" rowspan="1" colspan="1">85.9</td><td align="center" valign="middle" rowspan="1" colspan="1">93.8</td><td align="center" valign="middle" rowspan="1" colspan="1">99.2</td><td align="center" valign="middle" rowspan="1" colspan="1">96.9</td><td align="center" valign="middle" rowspan="1" colspan="1">98.4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Step3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.2</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07904-t008"><object-id pub-id-type="pii">sensors-24-07904-t008_Table 8</object-id><label>Table 8</label><caption><p>Results of &#x0039e; and normalization subindex.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Modes</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Car-Following in Single Lane</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">&#x0014b;<sub>ds</sub></italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">&#x0014b;<sub>cw</sub></italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">&#x0014b;<sub>dc</sub></italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0039e;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Human</td><td align="center" valign="middle" rowspan="1" colspan="1">0.54</td><td align="center" valign="middle" rowspan="1" colspan="1">0.83</td><td align="center" valign="middle" rowspan="1" colspan="1">0.31</td><td align="center" valign="middle" rowspan="1" colspan="1">0.506</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Automated driving</td><td align="center" valign="middle" rowspan="1" colspan="1">0.16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.52</td><td align="center" valign="middle" rowspan="1" colspan="1">0.36</td><td align="center" valign="middle" rowspan="1" colspan="1">0.312</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shared control</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.298</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07904-t009"><object-id pub-id-type="pii">sensors-24-07904-t009_Table 9</object-id><label>Table 9</label><caption><p>Results of &#x0039e; and normalization subindex.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Modes</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Taking over in Double Lanes</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">&#x0014b;<sub>ds</sub></italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">&#x0014b;<sub>cw</sub></italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">&#x0014b;<sub>dc</sub></italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0039e;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Human</td><td align="center" valign="middle" rowspan="1" colspan="1">0.53</td><td align="center" valign="middle" rowspan="1" colspan="1">0.85</td><td align="center" valign="middle" rowspan="1" colspan="1">0.32</td><td align="center" valign="middle" rowspan="1" colspan="1">0.510</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Automated driving</td><td align="center" valign="middle" rowspan="1" colspan="1">0.18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" rowspan="1" colspan="1">0.43</td><td align="center" valign="middle" rowspan="1" colspan="1">0.336</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shared control</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.314</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07904-t010"><object-id pub-id-type="pii">sensors-24-07904-t010_Table 10</object-id><label>Table 10</label><caption><p>Results of &#x0039e; and normalization subindex.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Modes</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Car-Following in Single Lane</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">&#x0014b;<sub>ds</sub></italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">&#x0014b;<sub>cw</sub></italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">&#x0014b;<sub>dc</sub></italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0039e;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Human</td><td align="center" valign="middle" rowspan="1" colspan="1">0.63</td><td align="center" valign="middle" rowspan="1" colspan="1">0.77</td><td align="center" valign="middle" rowspan="1" colspan="1">0.28</td><td align="center" valign="middle" rowspan="1" colspan="1">0.518</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Automated driving</td><td align="center" valign="middle" rowspan="1" colspan="1">0.15</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39</td><td align="center" valign="middle" rowspan="1" colspan="1">0.36</td><td align="center" valign="middle" rowspan="1" colspan="1">0.282</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shared control</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.256</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07904-t011"><object-id pub-id-type="pii">sensors-24-07904-t011_Table 11</object-id><label>Table 11</label><caption><p>Results of &#x0039e; and normalization subindex.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Modes</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Taking over in Double Lanes</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">&#x0014b;<sub>ds</sub></italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">&#x0014b;<sub>cw</sub></italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">&#x0014b;<sub>dc</sub></italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0039e;</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Human</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.510</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Automated driving</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.31</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.274</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shared control</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.254</td></tr></tbody></table></table-wrap></floats-group></article>