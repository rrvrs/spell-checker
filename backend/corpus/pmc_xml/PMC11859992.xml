<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006314</article-id><article-id pub-id-type="pmc">PMC11859992</article-id><article-id pub-id-type="doi">10.3390/s25041086</article-id><article-id pub-id-type="publisher-id">sensors-25-01086</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Object Recognition and Positioning with Neural Networks: Single Ultrasonic Sensor Scanning Approach</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0644-5938</contrib-id><name><surname>Karagoz</surname><given-names>Ahmet</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="c1-sensors-25-01086" ref-type="corresp">*</xref><xref rid="fn1-sensors-25-01086" ref-type="author-notes">&#x02020;</xref><xref rid="fn2-sensors-25-01086" ref-type="author-notes">&#x02021;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5642-7212</contrib-id><name><surname>Dindis</surname><given-names>Gokhan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="fn2-sensors-25-01086" ref-type="author-notes">&#x02021;</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Guldiken</surname><given-names>Rasim</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01086">Department of Electrical and Electronics Engineering, Faculty of Engineering and Architecture, Eskisehir Osmangazi University, Eskisehir 26040, T&#x000fc;rkiye; <email>gdindis@ogu.edu.tr</email></aff><author-notes><corresp id="c1-sensors-25-01086"><label>*</label>Correspondence: <email>ahmet.karagoz@tubitak.gov.tr</email></corresp><fn id="fn1-sensors-25-01086"><label>&#x02020;</label><p>Current address: The Scientific and Technological Research Council of Turkiye, Defense Industries Research and Development Institute, Ankara 06261, T&#x000fc;rkiye.</p></fn><fn id="fn2-sensors-25-01086"><label>&#x02021;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>11</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1086</elocation-id><history><date date-type="received"><day>27</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>29</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>10</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Ultrasonic sensing may become a useful technique for distance measurement and object detection when optical visibility is not available. However, the research on detecting multiple target objects and locating their coordinates is limited. This makes it a valuable topic. Reflection signal data obtained from a single ultrasonic sensor may be just enough for the measurements of distance and reflection strength. On the other hand, if extracted properly, a scanned set of signal data by the same sensor holds a significant amount of information about the surrounding geometries. Evaluating this dataset from a single sensor scanning can be a perfect application for convolutional neural networks (CNNs). This study proposes an imaging technique based on a scanned dataset obtained by a single low-cost ultrasonic sensor. So that images are suitable for desired outputs in a CNN, a 3D printer is converted to an ultrasonic image scanner and automated to perform as a data acquisition system for the desired datasets. A deep learning model demonstrated by this work extracts object features using convolutional neural networks (CNNs) and performs coordinate estimation using regression layers. With the proposed solution, by training a reasonable amount of obtained data, 90% accuracy was achieved in the classification and position estimation of multiple objects with the CNN algorithm as a result of converting the signals obtained from ultrasonic sensors into images.</p></abstract><kwd-group><kwd>ultrasonic sensors</kwd><kwd>signal classification</kwd><kwd>convolutional neural networks</kwd><kwd>signal processing</kwd><kwd>object recognition</kwd><kwd>machine learning</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01086"><title>1. Introduction</title><p>Object recognition and localization play a critical role in many applications, from&#x000a0;industrial robotics to autonomous vehicles, and from&#x000a0;security systems to logistics management. Ultrasonic sensors are widely used in these areas due to their low cost, reliability and&#x000a0;wide detection&#x000a0;range.</p><p>There are many studies in the literature using cameras in object classification and detection [<xref rid="B1-sensors-25-01086" ref-type="bibr">1</xref>]. The&#x000a0;optical systems used in computer vision algorithms used for object recognition are sensitive to the amount of light in the air. This directly affects the object recognition process in real-life applications. The&#x000a0;use of data obtained from ultrasonic sensors for object recognition has an important place in the literature. The&#x000a0;use of these systems for object recognition and positioning in places where camera systems cannot be used is an important research area. Object recognition systems performed with ultrasonic sensors are known for their advantages in areas such as cost-effectiveness, ability to work in dark environments, low energy consumption and ease of integration compared with optical systems [<xref rid="B2-sensors-25-01086" ref-type="bibr">2</xref>].</p><p>Ultrasonic sensors are used as an effective technology in various applications such as environment sensing, object classification and material recognition. These sensors provide information about both static and dynamic objects by processing time, frequency and time&#x02013;frequency features obtained from echo signals [<xref rid="B3-sensors-25-01086" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01086" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01086" ref-type="bibr">5</xref>]. Related studies have focused on various applications, covering a wide range of classification from simple shapes to complex structural obstacles, different types of materials and surface conditions [<xref rid="B6-sensors-25-01086" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-01086" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01086" ref-type="bibr">8</xref>]. Various signal processing techniques such as Hilbert transform, continuous wavelet transform (CWT), fast Fourier transform (FFT) and empirical mode decomposition (EMD) have been used for the analysis of echo signals [<xref rid="B5-sensors-25-01086" ref-type="bibr">5</xref>,<xref rid="B9-sensors-25-01086" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01086" ref-type="bibr">10</xref>]. These techniques have supported the classification and recognition processes by enabling the extraction of features such as phase, amplitude, delay and frequency from the signals [<xref rid="B11-sensors-25-01086" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01086" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01086" ref-type="bibr">13</xref>]. These data have been processed with machine learning algorithms (e.g., CNN, SVM, KNN, decision trees) and have provided high accuracy in tasks such as human detection, material detection and ground type classification [<xref rid="B14-sensors-25-01086" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01086" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-01086" ref-type="bibr">16</xref>]. Methods inspired by biological systems increase the performance of ultrasonic sensors. In&#x000a0;particular, they have been inspired by the echolocation mechanisms of animals such as bats and dolphins [<xref rid="B8-sensors-25-01086" ref-type="bibr">8</xref>,<xref rid="B17-sensors-25-01086" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01086" ref-type="bibr">18</xref>]. These approaches have optimized environmental sensing and material recognition processes in robotic applications and have been effective in difficult environmental conditions (dust, darkness, environments full of toxic gases) [<xref rid="B7-sensors-25-01086" ref-type="bibr">7</xref>,<xref rid="B19-sensors-25-01086" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01086" ref-type="bibr">20</xref>]. As a result, ultrasonic sensors offer an important solution in environmental sensing and classification tasks in autonomous systems, robotic applications and industrial processes with their low-cost, energy-efficient and durable features [<xref rid="B21-sensors-25-01086" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01086" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-01086" ref-type="bibr">23</xref>].</p><p>With the proposed method, data acquisition processes are performed by scanning multiple objects with a single ultrasonic sensor integrated into the 3D printer, and the obtained signals are converted into images. The&#x000a0;images are given as input to the proposed CNN-based deep learning model, and,&#x000a0;in addition to the classification of objects, position estimation is made, unlike studies in the literature. The&#x000a0;CNN-based multi-output model proposed for object detection and recognition with ultrasonic sensors has the ability to simultaneously estimate object types (A, B, C) or multiple object types and location information (X, Y coordinates) with high&#x000a0;accuracy.</p><p>The contributions of this paper, which aims to provide an end-to-end solution to the problem of multiple object classification and location estimation with ultrasound-based sensors using a CNN-based deep learning model, and&#x000a0;the motivation for our work are as&#x000a0;follows.</p><p>The main difference of the proposed method compared with the studies in the literature is that errors are minimized by collecting data with a single ultrasonic sensor on multiple objects with single axis scanning. In&#x000a0;this way, an&#x000a0;end-to-end reliable solution is provided for the recognition of objects and the estimation of their coordinates. Again, unlike the studies in the literature, instead of using the envelope, amplitude, statistical, time, frequency or echo features of the signal, this method stands out by visualizing the signal analyses in a single image, with the integration provided by single axis scanning and performing the classification processes with image processing. As&#x000a0;an output of this, it aims to perform multiple object classification, object recognition and coordinate estimation as in&#x000a0;ultrasonic.</p><p>The proposed method forms the basis of a system that can perform object recognition and coordinate determination operations integrated with human or robotic applications by scanning objects in real time, together with the algorithm and sensor design to be developed within the scope of future studies in environments without&#x000a0;visibility.</p><p>In the literature, there is no automated data collection system integrated with ultrasonic sensors for multiple objects. With&#x000a0;this developed system, it is now possible to produce datasets very quickly for different object types and to test different scenarios on these datasets. The&#x000a0;designed ultrasonic sensor circuit is low cost and provides more accurate results than measurements taken with commercially available and unstable ultrasonic sensors. Applications used for object recognition in the literature mainly require different data pre-processing and feature extraction processes. This situation causes loss of time and more mathematical&#x000a0;operations.</p><p>The introduction part of the article is followed by a literature review (<xref rid="sec2-sensors-25-01086" ref-type="sec">Section 2</xref>) including evaluations from different perspectives regarding ultrasonic sensors for object recognition and detection. The&#x000a0;automated data collection system, generated dataset, designed sensor, proposed method and&#x000a0;the data pre-processing process are mentioned in the material and methods (<xref rid="sec3-sensors-25-01086" ref-type="sec">Section 3</xref>) title. The&#x000a0;proposed CNN deep learning model is explained in detail in <xref rid="sec4-sensors-25-01086" ref-type="sec">Section 4</xref>. The&#x000a0;study is followed by experimental results and discussion (<xref rid="sec5-sensors-25-01086" ref-type="sec">Section 5</xref>) and conclusions (<xref rid="sec6-sensors-25-01086" ref-type="sec">Section 6</xref>).</p></sec><sec id="sec2-sensors-25-01086"><title>2. Related&#x000a0;Work</title><p>A review of the literature reveals that object detection and recognition using ultrasonic sensors remains a significant area of research, with&#x000a0;numerous studies conducted in this domain. Simone&#x000a0;et&#x000a0;al. investigate the use of ultrasonic sensors for dynamic obstacle avoidance. The&#x000a0;system aims to increase agricultural precision by retrofitting existing agricultural machinery for autonomous functionality [<xref rid="B3-sensors-25-01086" ref-type="bibr">3</xref>]. P&#x000f6;pperli&#x000a0;et&#x000a0;al. present an effective method for object height classification using low-cost automotive ultrasonic sensors. The&#x000a0;proposed capsule neural network architecture exhibits improved performance over traditional CNNs for automotive detection tasks by achieving high accuracy (99%) and low runtime (0.2&#x000a0;ms) [<xref rid="B11-sensors-25-01086" ref-type="bibr">11</xref>]. Shi&#x000a0;et&#x000a0;al. propose a CNN-based method for automatic classification of ultrasonic signals, focusing on defect detection in mixed stainless steel welds. Higher accuracy is achieved compared with manual methods and traditional feature extraction [<xref rid="B6-sensors-25-01086" ref-type="bibr">6</xref>]. Meng&#x000a0;et&#x000a0;al. apply deep CNNs to classify ultrasonic signals from composite materials. The&#x000a0;approach achieves high accuracy in defect detection and visualization via C-scan imaging by integrating wavelet transform features and deep learning [<xref rid="B9-sensors-25-01086" ref-type="bibr">9</xref>]. Bystrov&#x000a0;et&#x000a0;al. investigate the use of ultrasonic sensors to classify road surfaces under various environmental conditions. They emphasize the extraction of signal features for segmentation and apply neural networks for reliable classification, even in difficult terrains such as gravel and soil [<xref rid="B17-sensors-25-01086" ref-type="bibr">17</xref>]. This study compares object shape recognition methods using ultrasonic sensor arrays and neural networks. It emphasizes the integration of sensor arrays with machine learning for robust shape recognition under various conditions [<xref rid="B19-sensors-25-01086" ref-type="bibr">19</xref>]. Hwasser evaluates the performance of machine learning algorithms including CNNs and CapsNet to classify objects using raw ultrasonic sensor data. The&#x000a0;study compares input data types and achieves a classification accuracy of 94% for six object classes using CNNs [<xref rid="B12-sensors-25-01086" ref-type="bibr">12</xref>]. Sadh and Huber use high-frequency ultrasonic sensors to detect and classify materials, focusing on water quality and material classification. Fourier transforms and machine learning models are used to identify object features [<xref rid="B14-sensors-25-01086" ref-type="bibr">14</xref>].</p><p>Ohtani and Baba propose a system using ultrasonic sensor arrays and neural networks for material recycling. The&#x000a0;system classifies objects based on shape and material properties without physical contact [<xref rid="B21-sensors-25-01086" ref-type="bibr">21</xref>]. Zhang and others apply deep CNNs, including a lightweight architecture called LWTNet, to&#x000a0;recognize material textures based on ultrasonic C-scan images [<xref rid="B24-sensors-25-01086" ref-type="bibr">24</xref>]. Yan&#x000a0;et&#x000a0;al. use deep learning techniques to analyze ultrasonic signals to detect cracks in gas pipeline welds. Better accuracy rates are achieved by using the CNN and SVM together, compared with traditional methods [<xref rid="B25-sensors-25-01086" ref-type="bibr">25</xref>]. Latete&#x000a0;et&#x000a0;al. use convolutional neural networks to detect and classify faults in ultrasonic imaging. Data augmentation has been shown to increase the model&#x02019;s ability to identify flat-bottomed and side-drilled holes in production [<xref rid="B26-sensors-25-01086" ref-type="bibr">26</xref>]. This study uses ultrasonic sensors to monitor the structural integrity of wind turbine blades and addresses the challenges encountered in various fault conditions through machine learning and digital signal processing [<xref rid="B27-sensors-25-01086" ref-type="bibr">27</xref>]. This paper investigates the use of ultrasonic sensors with CNN and MLP models to classify objects in environments that are not suitable for traditional cameras for object recognition [<xref rid="B20-sensors-25-01086" ref-type="bibr">20</xref>]. The study evaluates the importance of phase data for ultrasonic object classification in vehicles. It has been observed that phase features significantly increase the classification rate in complex environments [<xref rid="B28-sensors-25-01086" ref-type="bibr">28</xref>]. A&#x000a0;scalogram-based signal processing method for ultrasonic detection has been presented and successful results have been achieved with CNN algorithms [<xref rid="B22-sensors-25-01086" ref-type="bibr">22</xref>]. Bouhamed&#x000a0;et&#x000a0;al. propose to use ultrasonic sensing for staircase detection using machine learning techniques considering environmental use for robotic technologies [<xref rid="B18-sensors-25-01086" ref-type="bibr">18</xref>]. In this review, the&#x000a0;authors discuss the application of neural network-based deep learning to detect acoustic events [<xref rid="B29-sensors-25-01086" ref-type="bibr">29</xref>].</p><p>Bianco&#x000a0;et&#x000a0;al. examine the transformative applications of machine learning in acoustics, examining advances in environmental sounds, bioacoustics and&#x000a0;source localization [<xref rid="B30-sensors-25-01086" ref-type="bibr">30</xref>]. This article discusses material classification using non-contact ultrasonic echo signals for robotic navigation and autonomous vehicle applications. The&#x000a0;signal envelope was extracted with the Hilbert transform, and materials such as glass, wood, metal, sponge and fabric were classified with 96% accuracy with a 1-dimensional convolutional neural network (1D-CNN). The&#x000a0;work provides high accuracy with low-cost sensors and automatic feature extraction, offering applicability to broader sets of materials in the future [<xref rid="B15-sensors-25-01086" ref-type="bibr">15</xref>]. Kroh&#x000a0;et&#x000a0;al. perform target classification with ultrasonic sonar sensors according to geometric shapes and sizes. In&#x000a0;experiments with narrow-band and wide-band signals, artificial neural networks (ANNs) showed over 95% accuracy. They can be used in target geometry identification, navigation and obstacle detection [<xref rid="B31-sensors-25-01086" ref-type="bibr">31</xref>]. This study, inspired by the echolocation principles of bats, classified various ground types (grass, concrete, sand, gravel) with over 97% accuracy using ultrasonic sensors. Support vector machines (SVMs) and time&#x02013;frequency features were analyzed [<xref rid="B16-sensors-25-01086" ref-type="bibr">16</xref>]. Kalliris&#x000a0;et&#x000a0;al. use machine learning algorithms to detect wet surfaces with acoustic measurements. In&#x000a0;this study, surface conditions were determined via acoustic echo signals, and high success rates were achieved in wet floor perception [<xref rid="B23-sensors-25-01086" ref-type="bibr">23</xref>]. In&#x000a0;this study, using an ultrasonic sensor and statistical methods, indoor objects were classified into four classes (edge, flat surface, small cylinder, corner). Linear and Quadratic Discriminant Analysis (LDA/QDA) was applied for feature selection and classification [<xref rid="B4-sensors-25-01086" ref-type="bibr">4</xref>].</p><p>Another study focuses on identifying humans using ultrasonic sensors with single-class classifiers. A&#x000a0;fuzzy-based model distinguished between humans and inanimate objects based on time and frequency features. The&#x000a0;results showed higher accuracy compared with the SVM method [<xref rid="B7-sensors-25-01086" ref-type="bibr">7</xref>]. Sabatini suggests modeling narrow-band ultrasonic signals with Laguerre polynomials. Objects were classified using echo envelope signals and the robustness of the model to noise was tested [<xref rid="B10-sensors-25-01086" ref-type="bibr">10</xref>]. Dror&#x000a0;et&#x000a0;al. have studied three-dimensional target recognition in different orientations with an echolocation-based neural network model. The&#x000a0;effects of time, frequency and time&#x02013;frequency features were analyzed and spectrogram-based approaches provided the highest accuracy [<xref rid="B13-sensors-25-01086" ref-type="bibr">13</xref>]. Ecemis&#x000a0;et&#x000a0;al. aim to classify objects using spectral information with a sonar-based system. Objects were recognized with 96% accuracy with the Fuzzy ARTMAP neural network. Both frequency and envelope signals were analyzed [<xref rid="B5-sensors-25-01086" ref-type="bibr">5</xref>]. In&#x000a0;this study, the&#x000a0;features obtained by empirical mode decomposition (EMD) were processed with machine learning algorithms such as KNN, SVM and decision trees. The&#x000a0;proposed method increased the material detection capabilities of robots in dark, dusty or hazardous environments [<xref rid="B8-sensors-25-01086" ref-type="bibr">8</xref>].</p><p>The proposed method enables the recognition of objects of varying sizes and distances using a single sensor in zero optic visibility environments, as&#x000a0;well as the extraction of their coordinates. As&#x000a0;evident from the developed automatic data collection system, the&#x000a0;system scans in the single <italic toggle="yes">x</italic>-axis direction and the obtained signals are converted into a single image by the computer. Then, the&#x000a0;obtained images are pre-processed and the CNN-based deep learning model, which is customized and optimized by extracting CNN-based features, enables the classification of objects and determination of their positions. First of all, the&#x000a0;performance of the system with single axis scanning has been tested with the studies conducted. At&#x000a0;the same time, the&#x000a0;problems and errors that arise have been&#x000a0;analyzed.</p><p>In this form, the&#x000a0;system constitutes an important step for real-world applications. With&#x000a0;the developed system, it will be possible to determine objects in zero-visibility environments in real time by integrating it into a helmet that a person will wear on their head or into robotic systems. For&#x000a0;real-time applications, signals can be obtained and processed very quickly sequentially with a powerful local processor integrated into the sensor. Depending on the system it is integrated with, it can be widely used in different areas by performing rotary or spherical scanning as well as single axis&#x000a0;scanning.</p><p><xref rid="sensors-25-01086-t001" ref-type="table">Table 1</xref> provides a comparative overview of studies on object detection and recognition using ultrasonic&#x000a0;signals.</p></sec><sec id="sec3-sensors-25-01086"><title>3. Materials and&#x000a0;Methods</title><sec id="sec3dot1-sensors-25-01086"><title>3.1. Automated Data Collection System with Ultrasonic&#x000a0;Sensor</title><p>An automated data collection system was built by connecting the Creality CR-10 S4 model 3D printer. All printing heads were replaced with our ultrasonic sensor, thus generating the dataset efficiently and accurately, making a significant contribution to the literature. In&#x000a0;this system, a&#x000a0;single ultrasonic sensor was mounted in place of the 3D printer, and measurements were taken by scanning in the X direction of the 3D printer with 2 mm steps.The data collection process was recorded by creating 116 different scenarios. This number can be increased for different object types. In&#x000a0;this context, the&#x000a0;G-code language was used for the 3D printer controller. This code determines the necessary movements and commands for the 3D printer to perform operations in accordance with the coordinates and procedures given. Even though it is cylindrical and spherical, similar results can be obtained with different scanners. Its main purpose is to control the movements and operations of a physical machine. The image of the established mechanism is included in <xref rid="sensors-25-01086-f001" ref-type="fig">Figure 1</xref>.</p><p>For automation, an&#x000a0;easily operable graphical user interface (GUI) was prepared in Python 3.9.1. This program includes functions whose features are briefly explained in the buttons in its interface. The&#x000a0;screenshot of the developed interface is included in <xref rid="sensors-25-01086-f002" ref-type="fig">Figure 2</xref>.</p></sec><sec id="sec3dot2-sensors-25-01086"><title>3.2. Dataset</title><p>This dataset contains 116 labeled cylindrical object images obtained by positioning objects in front of the ultrasonic sensor using one of each, two of each, three of each and mixed up combinations. A maximum 3 objects are used for classification. The&#x000a0;details of the 3&#x000a0;classes are as follows: Large-Diameter Object (40 mm), Medium-Diameter Object (20&#x000a0;mm), Narrow-Diameter Object (10 mm). The&#x000a0;objects are made of PLA materials printed by 3D printers. The classified objects and the developed ultrasonic sensor are included in <xref rid="sensors-25-01086-f003" ref-type="fig">Figure 3</xref>.</p><p>The ultrasonic data recorder circuit was specifically designed with unique modifications. The SRF04 Sonar Rangefinder (Devantech, Attleborough, UK) module was adapted for data recording purposes by removing the PIC12C508 microcontroller and LP311 comparator chips (Microchip Technology Inc., Chandler, AZ, USA) from its original printed circuit board. Instead, an&#x000a0;STM32F103 (STMicroelectronics, Geneva, Switzerland) carrier board was integrated into the design to handle data recording tasks. Ultrasonic transducers used in this research, 400ST/SR160 (Prowave, Queensland, Australia), have been used in our several projects before [<xref rid="B32-sensors-25-01086" ref-type="bibr">32</xref>]. They were driven with square wave burst at their 40&#x000a0;KHz (+/&#x02212;) 1&#x000a0;KHz center resonant frequency. Their main beam angle has 30&#x000a0;degree between their &#x02212;3&#x000a0;dB points. The&#x000a0;STM32F103 microcontroller is well suited for such applications, featuring a 32-bit architecture and capable of operating at a clock speed of up to 72&#x000a0;MHz, offering significantly more computational power compared with the PIC12C508. Additionally, it includes two multi-channel analog-to-digital converter (ADC) modules, each capable of sampling analog signals at a rate of up to 1&#x000a0;mega-sample per second (MSPS). Since the ultrasonic transducers operate at a frequency of 40 kHz, achieving a sampling rate close to 1 MSPS ensures approximately 25 samples per period, providing adequate resolution for the application. Analog input has following specifications: resolution = 12 bit, Vref(&#x02212;) = 0 V, Vref(+) = 3.3 V, DC offset level of the signal is 1.6 V. A&#x000a0;sampling period of 1.66 microseconds was selected, optimized to align with the operating clock frequency and memory constraints, allowing the storage of sufficient data for effective echo distance calculations. This corresponds to 15 samples per cycle of the ultrasonic signal, providing accurate representation. The&#x000a0;STM32F103 also features built-in Universal Serial Bus (USB) hardware. By&#x000a0;enabling a Virtual Communication Port (VCP), the&#x000a0;system facilitates USB communication with a host computer. The&#x000a0;unit is powered directly through the USB connection, enhancing simplicity. Once the appropriate firmware was integrated, the&#x000a0;entire unit was enclosed in a custom-designed 3D-printed case, making it portable and user-friendly. The&#x000a0;interface supports signal acquisition and comparison across four separate channels, allowing for the analysis of signal characteristics from various object types. The&#x000a0;recorded data are then collected for use in training neural network. The&#x000a0;system includes advanced processing functions, such as subtracting a specific reference signal from the acquired signal to highlight variations, and&#x000a0;calculating the envelope of the resulting signal for further analysis. The block diagram of the ultrasonic sensor circuit is given in <xref rid="sensors-25-01086-f004" ref-type="fig">Figure 4</xref>.</p></sec><sec id="sec3dot3-sensors-25-01086"><title>3.3. Proposed Method&#x000a0;Architecture</title><p>The proposed model is a CNN-based deep learning model that provides multiple outputs. The&#x000a0;block diagram of the proposed method is given in <xref rid="sensors-25-01086-f005" ref-type="fig">Figure 5</xref>. Additionally, the&#x000a0;algorithm of the proposed method has been extracted in&#x000a0;detail. See Algorithm 1.
<array><tbody><tr><td style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold> Object Classification and Localization Using Processed Image&#x000a0;Data.</td></tr><tr><td style="border-bottom:solid thin" rowspan="1" colspan="1"><list list-type="simple"><list-item><label><bold>Require:</bold>&#x000a0;</label><p>Zipped image dataset <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label><bold>Ensure:</bold>&#x000a0;</label><p>Trained multi-output CNN model</p></list-item><list-item><label>&#x000a0;&#x000a0;1:</label><p>Extract images from <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and load as dataset <italic toggle="yes">D</italic></p></list-item><list-item><label>&#x000a0;&#x000a0;2:</label><p>Parse filenames to extract object types and coordinates, storing in DataFrame <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;3:</label><p>Preprocess images in <italic toggle="yes">D</italic> and extract labels, resulting in feature array <italic toggle="yes">X</italic>, type labels <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>type</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and&#x000a0;coordinate labels <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>coords</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;4:</label><p>Split <italic toggle="yes">X</italic>, <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>type</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and&#x000a0;<inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>coords</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> into training and testing sets</p></list-item><list-item><label>&#x000a0;&#x000a0;5:</label><p>Convert <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>type</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to one-hot encoded labels for each object type</p></list-item><list-item><label>&#x000a0;&#x000a0;6:</label><p>Define CNN model with convolutional layers for feature extraction</p></list-item><list-item><label>&#x000a0;&#x000a0;7:</label><p>Add classification and coordinate output layers</p></list-item><list-item><label>&#x000a0;&#x000a0;8:</label><p>Compile model with loss functions for classification and coordinate regression</p></list-item><list-item><label>&#x000a0;&#x000a0;9:</label><p>Train model on training data, using testing data for validation</p></list-item><list-item><label>10:</label><p><bold>Return</bold> trained model</p></list-item></list></td></tr></tbody></array></p></sec><sec id="sec3dot4-sensors-25-01086"><title>3.4. Data&#x000a0;Preprocessing</title><p>In the signal information obtained from ultrasonic sensors, the vertical axis is the magnitude of the signal and the horizontal axis is the sampling point. An example of the signal data obtained in each scan is given in <xref rid="sensors-25-01086-f006" ref-type="fig">Figure 6</xref>.</p><p>In the <xref rid="sensors-25-01086-f006" ref-type="fig">Figure 6</xref>, the first reflection was from the object and the other reflections were from the background wall. In <xref rid="sensors-25-01086-f007" ref-type="fig">Figure 7</xref>, an image line is obtained by extracting the envelope of the signal, which is called an A-Scan, and then converting the vertical value to the pixel color for the corresponding sample (lighter pixel color for the higher vertical value).</p><p>When the image lines are combined for each progressive scan, the final image is obtained after all the image lines are combined, as in <xref rid="sensors-25-01086-f008" ref-type="fig">Figure 8</xref>; in the bottom image are what are called C-Scan images in the literature.</p><p>The obtained data were divided into separately labeled groups according to object type, distance and coordinate information by the developed software with Python 3.11.5. Following data integration, arrangements were made to read the data in these groups and give them as input to the classification algorithm.</p><p>Objects are placed anywhere between 0 and 40 cm on the <italic toggle="yes">x</italic>-axis and in a certain position on the <italic toggle="yes">y</italic>-axis, and while the object is stationary the ultrasonic sensor moves 40 cm on the <italic toggle="yes">x</italic>-axis and takes 201 measurements every 2 mm. These measurements are then converted into images.</p><p>The reflections from single objects were compared with the reflections from multiple objects. Images obtained from a singular object and multiple objects are shown in <xref rid="sensors-25-01086-f009" ref-type="fig">Figure 9</xref> and <xref rid="sensors-25-01086-f010" ref-type="fig">Figure 10</xref>, respectively. In images obtained this way, light color regions indicate peaks and dark colors indicate valleys. We should note that images obtained from singular objects have distinct patterns. They are usually in a hyperboloid curve with a continuous color pattern. On the other hand, images obtained from multiple objects have interleaved hyperboloid curve shapes with discrete gaps instead of continuous solid colors.</p><p>Within the scope of the study, it is aimed to perform object recognition using a single ultrasonic sensor. However, in the case of multiple object problems, we see that in some signal shots reflections disappear, lose their amplitude where reflections come from the object in different faces and cancel each other. Because of this effect, signal trace with a single sensor depending on location cannot classify the objects correctly and the object recognition success rate is negatively affected.</p><p>In the upper signal, the trough formed due to the interference of two signals due to the phase difference between the signals reflected from multiple objects is marked. In the lower signal, the peak formed due to the overlapping of signals reflected from multiple objects is marked. This situation reveals that there are limitations in measuring a single signal belonging to objects in the problem of multiple object recognition. Therefore, scanning is good idea to obtain these features so that multiple object recognition processes give a more accurate output. <xref rid="sensors-25-01086-f011" ref-type="fig">Figure 11</xref> demonstrates that sometimes 10 mm intervals make a big difference in reflection. Using this scanning method, this problem can be converted to a beneficial feature. In order to capture these features more precisely, it is more appropriate to take measurements every 2 mm. If measurements were taken every 5 mm, we would miss some features. As a result of these measurements, each scan is converted into a picture. Instead of manual single measurements, objects are placed anywhere between 0 and 40 cm on the <italic toggle="yes">x</italic>-axis and a certain position on the <italic toggle="yes">y</italic>-axis, and the ultrasonic sensor moves 40 cm on the <italic toggle="yes">x</italic>-axis and takes 201 measurements every 2 mm. <xref rid="sensors-25-01086-f012" ref-type="fig">Figure 12</xref> shows this much better for each progressive scan.</p><p>In <xref rid="sensors-25-01086-f013" ref-type="fig">Figure 13</xref>, it is clearly seen that objects create different curved effects on the image depending on their diameters. By taking advantage of these differences, distinctive information can be extracted on the diameters and surface widths of objects.</p></sec><sec id="sec3dot5-sensors-25-01086"><title>3.5. Normalization&#x000a0;Process</title><p>The normalization process performed here aims to convert the pixel values of the images from the range of 0 to 255 to the range of 0 to 1. This normalization is performed to enable the neural network to process the inputs better. It prevents high pixel values from excessively affecting the model parameters and allows the model to learn in a more balanced way. It also speeds up the optimization process and facilitates the convergence of the model.</p><p>The ultrasonic image data used in the model were obtained compressed in ZIP format. Object types (A = Narrow-Diameter Cylinder (10 mm), B = Medium-Diameter Cylinder (20 mm), C = Large-Diameter Cylinder (40 mm)) and coordinates (X, Y) were extracted from the file names using regex patterns, and this information about each image was converted into a data frame (DataFrame).</p><p>Each image was converted to grayscale and normalized according to the input size of the model (128 &#x000d7; 128 pixels). These pre-processing steps optimized the structure of the images to allow the model to predict object types and coordinate information more accurately. Assuming that there are a maximum of three objects in the images, missing objects were filled with the label &#x02212;1 and coordinates [0, 0].</p></sec></sec><sec id="sec4-sensors-25-01086"><title>4. The Used Deep Learning&#x000a0;Model</title><sec id="sec4dot1-sensors-25-01086"><title>4.1. CNN&#x000a0;Algorithm</title><p>Convolutional Neural Networks (CNNs) are a deep learning model that combines fully connected layers with convolutional layers. Mathematically based on the convolution operation in the field of signal processing, CNNs work with the principle of cross-correlation and thus extract important features from data. The basic structure of CNNs consists of two main layers. These are feature extraction and classification layers. While feature extraction is usually performed by convolutional layers and pooling layers, the classification process is performed through fully connected layers. Convolutional layers are not directly connected to each node in the input layer. Instead, they focus on specific regions through small windows called filters or convolutional kernels. This structure allows the network to learn low-level features, and these features are combined to create higher-level patterns. Another important feature of the CNN algorithm is that it can generalize the features it learns in one region to other regions. This is made possible by sharing parameters in the filters and allows the model to produce a similar output at different locations. Pooling layers reduce the computational load and help prevent the overfitting problem of the model. Thus, CNN structures provide a more effective and efficient learning process [<xref rid="B6-sensors-25-01086" ref-type="bibr">6</xref>,<xref rid="B33-sensors-25-01086" ref-type="bibr">33</xref>]. The general block diagram of the CNN architecture is given in <xref rid="sensors-25-01086-f014" ref-type="fig">Figure 14</xref>.</p><p>The CNN model is a pivotal type of neural network that is widely applied in robotics applications for object and target recognition and detection, especially when processing image data [<xref rid="B22-sensors-25-01086" ref-type="bibr">22</xref>,<xref rid="B30-sensors-25-01086" ref-type="bibr">30</xref>].</p><p>Moreover, this algorithm plays an important role in the literature on feature extraction from sound signals and acoustic measurement data in ultrasonics. In essence, the CNN model works as a convolutional neural network layer that extracts features from the input data and transforms them into feature maps. By shifting the kernels along the input, the algorithm calculates the output by multiplying the kernel weights with the input data element-wise. Unlike traditional neural networks, CNNs stand out due to the reduced number of trainable parameters and the faster training process, making them more efficient for complex tasks [<xref rid="B34-sensors-25-01086" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-01086" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-01086" ref-type="bibr">36</xref>].</p></sec><sec id="sec4dot2-sensors-25-01086"><title>4.2. Implementing CNN&#x000a0;Model</title><p>The model was divided into training sets and test sets to evaluate its generalizability. The training and test sets were divided into 80% training and 20% test ratios.</p><p>The Adam optimization algorithm was used in the training of the model, and the categorical_crossentropy loss function was applied for the classification outputs and the mean_squared_error (mse) loss function was applied for the coordinate estimation. This combination is a suitable choice to optimize both the classification accuracy and the coordinate estimation accuracy [<xref rid="B37-sensors-25-01086" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-01086" ref-type="bibr">38</xref>].</p><p>During the training process of the model, the accuracy and loss values were monitored for 1000 epochs, and techniques such as early stopping were applied to prevent the model from overfitting [<xref rid="B37-sensors-25-01086" ref-type="bibr">37</xref>]. Pre-trained weights were not loaded into the model. All layers and weights were randomly initialized and included in the system.</p><p>The model was used to classify the images in the test dataset and estimate their coordinates.</p><p>The object types predicted by the model were drawn on the image using matplotlib with circle drawings in accordance with the diameter value of the object.</p><p>Object types and location estimates are shown in the correct order. Thus, the ability of the model to correctly determine the object type as well as its ability to correctly estimate the coordinates was evaluated.</p><p>The classification performance of the model was measured with metrics such as confusion_matrix, accuracy_score, precision, recall and f1_score. The performance, sensitivity and specificity were calculated for each object type and analyzed in more detail [<xref rid="B37-sensors-25-01086" ref-type="bibr">37</xref>]. The coordinate estimation performance was analyzed with mse loss. The accuracy rate was evaluated by calculating the mean square error between the estimated values and the real values in the X- and Y-axes for each type of object.</p><p>This section details the components of the model. In <xref rid="sensors-25-01086-f015" ref-type="fig">Figure 15</xref>, the flowchart shows the convolution and pooling layers of the model, as well as the classification and coordinate estimation layers. The model produces three separate classification outputs to determine the type of objects. For each object type, the output layers class_output_1, class_output_2 and class_output_3 are created using the Softmax activation function in a Dense layer. This structure allows the model to classify each object independently. A linear regression layer called coord_output is used for coordinate estimation. This layer of the model directly outputs the coordinate values to estimate the positions of the objects on the <italic toggle="yes">x</italic>- and <italic toggle="yes">y</italic>-axes.</p><p>A shared convolutional base in the proposed CNN model extracts features for all tasks (classification and regression), improving efficiency and reducing overfitting by leveraging shared information. Images are normalized and resized for consistent input, improving model performance and stability. Predictions are sorted by coordinates, providing important outputs for evaluation and visualization.</p><p>The proposed CNN model takes images with 128 &#x000d7; 128 dimensions and 1 channel (black/white) as input to the model. The input dimension represents the height, width and color channels of the image. Convolutional layers allow features such as edges and objects to be extracted from the image. Conv2d layers apply filters by performing convolution on the image. Filters recognize patterns in the image.</p><p>The number of filters used in the proposed CNN model is as follows. Layer 1: 32 filters, each with a size of 3 &#x000d7; 3. Layer 2: 64 filters, each with a size of 3 &#x000d7; 3. Layer 3: 64 filters, each with a size of 3 &#x000d7; 3. MaxPooling2d layers take the maximum value in the 2 &#x000d7; 2 matrix to reduce the size of the image. The flatten layer converts the 16 &#x000d7; 16 &#x000d7; 64 feature map into a one-dimensional vector, resulting in a series of 16,384 features.</p><p>The model produces four different outputs. Three of them are for object classification and one for coordinate estimation. The goal here is to predict what each object in the image is (A, B, C). In dense layers, the first layer is 64 neurons and is used to learn more features. Output layer: three neurons (three object types). The coordinate output allows the x, y coordinates of three objects in the image to be estimated. The first layer in dense layers is 64 neurons and is again used to learn features. The output layer consists of six neurons for a total of three objects, with x, y coordinates for each object. The activation function is not used in this part because this is a regression, i.e., continuous value estimation problem. Label ordering is performed according to the order of coordinate information on the <italic toggle="yes">x</italic>-axis of the objects in the model&#x02019;s training dataset from smallest to largest. The first object with the smallest X coordinate in the image is assigned to Class Output 1. The second object is assigned to Class Output 2. The third object with the largest X coordinate is assigned to Class Output 3. If the X values are equal, labeling is performed according to the order of coordinate information on the <italic toggle="yes">y</italic>-axis from smallest to largest.</p></sec></sec><sec id="sec5-sensors-25-01086"><title>5. Experimental Results and&#x000a0;Discussion</title><p><xref rid="sensors-25-01086-f016" ref-type="fig">Figure 16</xref> shows representation of the classification and determination of coordinates of objects in a test image containing multiple objects with the proposed method.</p><p><xref rid="sensors-25-01086-t002" ref-type="table">Table 2</xref> provides a comparative demonstration of the classification of objects with performance metrics commonly used in the literature in classification algorithms.</p><p>The model performs best for OBJECT1. In this class, precision, recall and sensitivity are very high, and false positive and false negative rates are very low. OBJECT2 performs weaker than OBJECT1, but still the performance rates are acceptable. OBJECT3 is the class where the model struggles the most. For this class, both precision and recall are low, indicating that both false positives and false negatives are higher. This indicates that the model has difficulty recognizing objects in less distinct, smaller or complex scenes.</p><p><xref rid="sensors-25-01086-f017" ref-type="fig">Figure 17</xref> shows the k-fold cross validation accuracies across different folds during the evaluation of the proposed CNN model. Each vertical column in the graph represents the model accuracy for a particular fold during cross-validation. The model shows consistent performance across most folds, with an accuracy ranging from approximately 83% to 90%. The dashed red line represents the average accuracy across all folds, which is approximately 84.55%. This provides a general indication of the model performance stability in different data splits. The fourth fold has a significantly lower accuracy compared with the other folds. This can be attributed to the cases of complex images and lack of clarity during multiple object recognition. The model shows stability and relatively high accuracy across most folds, which is a good sign of generalization.</p><p>In <xref rid="sensors-25-01086-f018" ref-type="fig">Figure 18</xref>, a significant portion of the coordinate detection errors are concentrated around 0 cm. This indicates that the estimated coordinates are mostly very close to the true coordinates. The majority of the distribution is between &#x02212;2 and +2 units (cm). This indicates that the coordinate estimates are generally quite successful and the errors remain within a small range. A few errors beyond &#x000b1;4 units (cm) were observed. These outliers represent cases where the model has difficulty in estimating the coordinates. For example, these errors were caused by objects being very close to each other, not being able to make a clear separation on the shape, or making classification errors. The high frequency of 0 indicates that many of the model&#x02019;s estimates match the true coordinates exactly or almost exactly. This is a positive result in terms of model performance in coordinate estimation.</p><p><xref rid="sensors-25-01086-f019" ref-type="fig">Figure 19</xref> shows the mean errors for each set of coordinates. For some coordinates, the mean error is positive (e.g., around +2 cm) and for others it is negative (e.g., around &#x02212;2 cm). The mean errors are generally small, indicating that the model is not making large errors. If the errors are not close to zero, this may indicate a slight bias in the model&#x02019;s predictions. In some cases, the model can be retrained with more data to reduce the bias. Methods such as adding noise can be further diversified to increase the generalization ability of the model.</p><p>In order to test the validity of the proposed method, configurations with errors of &#x000b1;4 cm were analyzed in order to apply the trained NN to difficult scenarios. In the analysis, it was seen that some of the images with coordinate estimation errors of &#x000b1;4 cm and above and incorrect classification were not clear, which made the detection process difficult. Examples of these images are given in <xref rid="sensors-25-01086-f020" ref-type="fig">Figure 20</xref>.</p><p>In order to test the performance of the model close to real-world applications, Gaussian noise and salt-and-pepper noise were separately included in the dataset, and the system performance under noise was tested comparatively. Accordingly, the model is more robust to Gaussian noise. Using data with Gaussian noise added during training can provide the model with more robust performance in real-world conditions. Salt-and-pepper noise degraded the performance of the model more than Gaussian noise. This is because salt-and-pepper noise usually causes sharp contrast changes in the image, making feature extraction difficult. The metrics for the performance of the model in the case of adding Gaussian noise and salt-and-pepper noise are given in <xref rid="sensors-25-01086-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-01086-t004" ref-type="table">Table 4</xref>, respectively.</p><p>The comparison of images with Gaussian noise and salt-and-pepper noise added with the original image is shown in <xref rid="sensors-25-01086-f021" ref-type="fig">Figure 21</xref>.</p><p>By incorporating different analysis methods into the experimental results, important inferences regarding the limitations and performance of the proposed method have been provided. In this section, the output regarding the system behavior in cases where the objects are closest to each other is analyzed. <xref rid="sensors-25-01086-f022" ref-type="fig">Figure 22</xref> and <xref rid="sensors-25-01086-f023" ref-type="fig">Figure 23</xref> shows the location and position estimation of objects as a result of classification in images with two objects. In this part of the study, for two objects, measurements were taken at y = 30 cm and the distance between them on the <italic toggle="yes">x</italic>-axis was 3, 5, 7, 9 cm, respectively, and classification studies were carried out. In addition, similar measurements were taken at close range by changing the positions of B and C. The results obtained in <xref rid="sensors-25-01086-t005" ref-type="table">Table 5</xref> show that, even in the closest situations, the classification success is at an acceptable level.</p><p>In this section, the performance of the proposed method in cases with four and five objects is analyzed. The classification results obtained in <xref rid="sensors-25-01086-t006" ref-type="table">Table 6</xref> and <xref rid="sensors-25-01086-t007" ref-type="table">Table 7</xref> for cases with four and five objects show that the proposed method can effectively respond to cases with more than three objects. However, as the complexity increases, the classification performance decreases. Including more cases with four and five objects in the training database will contribute significantly to the classification success. Thus, more information will be extracted from the characteristics of these cases. In the case of four objects in <xref rid="sensors-25-01086-f024" ref-type="fig">Figure 24</xref>, the classification results are quite successful, as in the case of three objects. In <xref rid="sensors-25-01086-f025" ref-type="fig">Figure 25</xref>, the classification performance increased because the B object in the middle was more isolated from the other four objects. The proximity of the B and C objects at the top of the figure had a negative effect on the classification performance. As a result, the system performance and classification success will increase with an effective learning process.</p></sec><sec sec-type="conclusions" id="sec6-sensors-25-01086"><title>6. Conclusions</title><p>The study aims to accurately classify multiple objects and determine their coordinates using raw ultrasonic signals of objects of various types and shapes obtained from ultrasonic sensors for autonomous vehicles, robots and other robotic applications. In addition, the object recognition study of the proposed method can be performed with repeated data that can be produced thanks to the automated data acquisition system developed with a single sensor without using a sensor array. The automated data acquisition system built using a 3D printer was a good idea to ensure the validity of the proposed method.</p><p>The results achieved, together with the CNN-based model of ultrasonic sensors, are important and can be distinguished from the studies in the literature on multiple object classification and coordinate determination. Within the scope of the study, objects were classified and position estimation was performed with a CNN model that automatically extracts features from input data using three different cylindrical objects and a low-cost sensor.</p><p>This study demonstrates both the advantages and limitations of deep learning techniques in extracting object type and location information from multiple-object ultrasonic sensor data. The proposed model was able to predict object types and locations simultaneously with high accuracy.</p><p>In the case of multiple object situations, instead of continuous distinct shapes, diffraction patterns are obtained because of the ultrasonic wave length. Discontinuities are observed in the ultrasonic images obtained during multiple object recognition. When these discontinuities are analyzed as a separate parameter, they can serve as valuable information for feature extraction.</p><p>Experimental studies have been conducted with different methods in order to reveal the limits and performance of the proposed method in a more effective and understandable way. The images that give the highest coordinate error in the detection of objects have been analyzed. In order to see the situation regarding the system performance in noisy environments, classification has been performed by adding various noises to the dataset. The performance of recognition and coordinate estimation has been examined by taking measurements at the distances where the objects are closest to each other. In addition, the situations experienced in object classification and position estimation in cases where there are four and five objects at the same time have been addressed. The results show that the proposed method has proven itself against the contributions and difficulties provided by the different analysis methods applied. However, there are certainly aspects of the study that need to be improved in order to minimize errors, and increase performance and classification success. More strengthened datasets and different deep learning techniques will constitute the main methodology of our future studies.</p><p>In the future studies, the dataset will be expanded, improvements will be made in the image pre-processing section for objects whose coordinates and classes are determined incorrectly, methods for detailed analysis of CNN-based features will be applied, more detailed error analysis will be performed, and comparative evaluations will be carried out with different deep neural network models. At the same time, it is aimed to bring different perspectives to the fore by preparing an experimental setup where rotary and spherical measurements can be made. In real-time applications, scanning can be performed faster by using more than one transmitter and receiver. The system is open to development for different applications and can be improved with a finite number of transmitters and/or receivers.</p><p>Furthermore, in environments with zero optic visibility, a real-time imaging system capable of object recognition and coordinate estimation can be developed. This system can be integrated into a helmet worn by a human or robot-integrated, supported by enhanced algorithms and sensor designs.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.K. and G.D.; methodology, A.K. and G.D.; software, A.K. and G.D.; validation, A.K. and G.D.; investigation, G.D. and A.K.; writing&#x02014;original draft preparation, A.K.; writing&#x02014;review and editing, A.K. and G.D.; visualization, A.K. and G.D.; supervision, G.D. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data will be made available by the corresponding author upon reasonable request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01086"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pandharipande</surname><given-names>A.</given-names></name>
<name><surname>Cheng</surname><given-names>C.-H.</given-names></name>
<name><surname>Dauwels</surname><given-names>J.</given-names></name>
<name><surname>Gurbuz</surname><given-names>S.Z.</given-names></name>
<name><surname>Ibanez-Guzman</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>G.</given-names></name>
<name><surname>Piazzoni</surname><given-names>A.</given-names></name>
<name><surname>Wang</surname><given-names>P.</given-names></name>
<name><surname>SantraShape</surname><given-names>A.</given-names></name>
</person-group><article-title>Sensing and machine learning for automotive perception: A review</article-title><source>IEEE Sens.</source><year>2023</year><volume>23</volume><fpage>11097</fpage><lpage>11115</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2023.3262134</pub-id></element-citation></ref><ref id="B2-sensors-25-01086"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ohtani</surname><given-names>K.</given-names></name>
<name><surname>Baba</surname><given-names>M.</given-names></name>
</person-group><article-title>Shape Recognition and Position Measurement of an Object Using an Ultrasonic Sensor Array</article-title><source>Sens. Array</source><year>2012</year><volume>4</volume><fpage>53</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.5772/36115</pub-id></element-citation></ref><ref id="B3-sensors-25-01086"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Simone</surname><given-names>M.</given-names></name>
<name><surname>Rivera</surname><given-names>Z.</given-names></name>
<name><surname>Guida</surname><given-names>D.</given-names></name>
</person-group><article-title>Obstacle avoidance system for unmanned ground vehicles by using ultrasonic sensors</article-title><source>Machines</source><year>2018</year><volume>6</volume><elocation-id>18</elocation-id><pub-id pub-id-type="doi">10.3390/machines6020018</pub-id></element-citation></ref><ref id="B4-sensors-25-01086"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Barat</surname><given-names>C.</given-names></name>
<name><surname>Ait Oufroukh</surname><given-names>N.</given-names></name>
</person-group><article-title>Classification of indoor environment using only one ultrasonic sensor</article-title><source>Proceedings of the IMTC 2001: Proceedings of the 18th IEEE Instrumentation and Measurement Technology Conference: Rediscovering Measurement in the Age of Informatics</source><conf-loc>Budapest, Hungary</conf-loc><conf-date>21&#x02013;23 May 2001</conf-date><fpage>1750</fpage><lpage>1755</lpage></element-citation></ref><ref id="B5-sensors-25-01086"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ecemis</surname><given-names>M.I.</given-names></name>
<name><surname>Gaudiano</surname><given-names>P.</given-names></name>
</person-group><article-title>Object recognition with ultrasonic sensors</article-title><source>Proceedings of the 1999 IEEE International Symposium on Computational Intelligence in Robotics and Automation: CIRA&#x02019;99</source><conf-loc>Monterey, CA, USA</conf-loc><conf-date>8&#x02013;9 November 1999</conf-date><fpage>250</fpage><lpage>255</lpage></element-citation></ref><ref id="B6-sensors-25-01086"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shi</surname><given-names>Y.</given-names></name>
<name><surname>Xu</surname><given-names>W.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
</person-group><article-title>Automated classification of ultrasonic signal via a convolutional neural network</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>4179</elocation-id><pub-id pub-id-type="doi">10.3390/app12094179</pub-id></element-citation></ref><ref id="B7-sensors-25-01086"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sharma</surname><given-names>S.</given-names></name>
<name><surname>Tripathi</surname><given-names>A.M.</given-names></name>
<name><surname>Baruah</surname><given-names>R.D.</given-names></name>
<name><surname>Nair</surname><given-names>S.B.</given-names></name>
</person-group><article-title>Ultrasonic sensorbased human detector using one-class classifiers</article-title><source>Proceedings of the 2015 IEEE International Conference on Evolving and Adaptive Intelligent Systems (EAIS)</source><conf-loc>Douai, France</conf-loc><conf-date>1&#x02013;3 December 2015</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B8-sensors-25-01086"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>B.</given-names></name>
<name><surname>Geng</surname><given-names>T.</given-names></name>
<name><surname>Jiang</surname><given-names>G.</given-names></name>
<name><surname>Guan</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Yun</surname><given-names>X.</given-names></name>
</person-group><article-title>Surrounding object material detection and identification method for robots based on ultrasonic echo signals</article-title><source>Appl. Bionics Biomech.</source><year>2023</year><volume>2023</volume><elocation-id>1998218</elocation-id><pub-id pub-id-type="doi">10.1155/2023/1998218</pub-id></element-citation></ref><ref id="B9-sensors-25-01086"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Meng</surname><given-names>M.</given-names></name>
<name><surname>Chua</surname><given-names>Y.J.</given-names></name>
<name><surname>Wouterson</surname><given-names>E.</given-names></name>
<name><surname>Ong</surname><given-names>C.P.K.</given-names></name>
</person-group><article-title>Ultrasonic signal classification and imaging system for composite materials via deep convolutional neural networks</article-title><source>Neurocomputing</source><year>2017</year><volume>257</volume><fpage>128</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2016.11.066</pub-id></element-citation></ref><ref id="B10-sensors-25-01086"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sabatini</surname><given-names>A.M.</given-names></name>
</person-group><article-title>A digital-signal-processing technique for ultrasonic signal modeling and classification</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2001</year><volume>50</volume><fpage>15</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1109/19.903873</pub-id></element-citation></ref><ref id="B11-sensors-25-01086"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>P&#x000f6;pperli</surname><given-names>M.</given-names></name>
<name><surname>Gulagundi</surname><given-names>R.</given-names></name>
<name><surname>Yogamini</surname><given-names>R.</given-names></name>
<name><surname>Milz</surname><given-names>S.</given-names></name>
</person-group><article-title>Capsule neural network based height classification using low-cost automotive ultrasonic sensors</article-title><source>Proceedings of the 2019 IEEE Intelligent Vehicles Symposium (IV)</source><conf-loc>Paris, France</conf-loc><conf-date>9&#x02013;12 June 2019</conf-date><volume>Volume 43</volume><fpage>661</fpage><lpage>666</lpage><pub-id pub-id-type="doi">10.1109/IVS.2019.8813879</pub-id></element-citation></ref><ref id="B12-sensors-25-01086"><label>12.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Hwasser</surname><given-names>R.</given-names></name>
</person-group><article-title>Machine Learning Classification Based on Ultrasonic Analog Data</article-title><source>Master&#x02019;s Thesis</source><publisher-name>Chalmers University of Technology Department of Electrical Engineering</publisher-name><publisher-loc>G&#x000f6;teborg, Sweden</publisher-loc><year>2020</year></element-citation></ref><ref id="B13-sensors-25-01086"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dror</surname><given-names>I.E.</given-names></name>
<name><surname>Zagaeski</surname><given-names>M.</given-names></name>
<name><surname>Moss</surname><given-names>C.F.</given-names></name>
</person-group><article-title>Three-dimensional target recognition via sonar: A neural network model</article-title><source>Neural Netw.</source><year>1995</year><volume>8</volume><fpage>149</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1016/0893-6080(94)00057-S</pub-id></element-citation></ref><ref id="B14-sensors-25-01086"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Sadh</surname><given-names>M.V.</given-names></name>
</person-group><article-title>Detection and Classification of Object Presence and Characteristics in a Water Container Using High Frequency Ultrasound</article-title><source>Ph.D. Thesis</source><publisher-name>The University of Texas at Arlington</publisher-name><publisher-loc>Arlington, TX, USA</publisher-loc><year>2022</year></element-citation></ref><ref id="B15-sensors-25-01086"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sahoo</surname><given-names>A.K.</given-names></name>
<name><surname>Udgata</surname><given-names>S.K.</given-names></name>
</person-group><article-title>Material Classification based on Non-contact Ultrasonic Echo Signal Using Deep Learning Approach</article-title><source>Procedia Comput. Sci.</source><year>2023</year><volume>235</volume><fpage>606</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2024.04.060</pub-id></element-citation></ref><ref id="B16-sensors-25-01086"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Riopelle</surname><given-names>N.</given-names></name>
<name><surname>Caspers</surname><given-names>P.</given-names></name>
<name><surname>Sofge</surname><given-names>D.</given-names></name>
</person-group><article-title>Terrain classification for autonomous vehicles using bat-inspired echolocation</article-title><source>Proceedings of the 2018 International Joint Conference on Neural Networks (IJCNN)</source><conf-loc>Rio de Janeiro, Brazil</conf-loc><conf-date>8&#x02013;13 July 2018</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B17-sensors-25-01086"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bystrov</surname><given-names>A.</given-names></name>
<name><surname>Hoare</surname><given-names>E.</given-names></name>
<name><surname>Tran</surname><given-names>E.</given-names></name>
<name><surname>Clarke</surname><given-names>T.Y.N.</given-names></name>
<name><surname>Gashinova</surname><given-names>M.</given-names></name>
<name><surname>Cherniakov</surname><given-names>M.</given-names></name>
</person-group><article-title>Road surface classification using automotive ultrasonic sensor</article-title><source>Procedia Eng.</source><year>2016</year><volume>168</volume><fpage>19</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.proeng.2016.11.119</pub-id></element-citation></ref><ref id="B18-sensors-25-01086"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bouhamed</surname><given-names>S.A.</given-names></name>
<name><surname>Khanfir</surname><given-names>K.K.</given-names></name>
<name><surname>Dorra</surname><given-names>S.M.</given-names></name>
</person-group><article-title>Stair case detection and recognition using ultrasonic signal</article-title><source>Proceedings of the 36th International Conference on Telecommunications and Signal Processing (TSP)</source><conf-loc>Rome, Italy</conf-loc><conf-date>2&#x02013;4 July 2013</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2013</year><pub-id pub-id-type="doi">10.1109/TSP.2013.6614021</pub-id></element-citation></ref><ref id="B19-sensors-25-01086"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Barua</surname><given-names>S.</given-names></name>
<name><surname>Saha</surname><given-names>A.</given-names></name>
<name><surname>Khan</surname><given-names>A.A.S.</given-names></name>
<name><surname>Chowdhury</surname><given-names>R.H.</given-names></name>
</person-group><article-title>Comparative Study of Object Shape Recognition using Ultrasonic Sensor Arrays with Artificial Neural Network</article-title><source>Proceedings of the 2nd International Conference on Innovation in Engineering and Technology (ICIET)</source><conf-loc>Dhaka, Bangladesh</conf-loc><conf-date>23&#x02013;24 December 2019</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/ICIET48527.2019.9290607</pub-id></element-citation></ref><ref id="B20-sensors-25-01086"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Karagoz</surname><given-names>A.</given-names></name>
<name><surname>Dindis</surname><given-names>G.</given-names></name>
</person-group><article-title>Object Recognition Using Deep Learning Algorithms and Ultrasonic Signals</article-title><source>Proceedings of the 32nd Signal Processing and Communications Applications Conference (SIU)</source><conf-loc>Mersin, Turkey</conf-loc><conf-date>15&#x02013;18 May 2024</conf-date><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/SIU61531.2024.10600754</pub-id></element-citation></ref><ref id="B21-sensors-25-01086"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ohtani</surname><given-names>K.</given-names></name>
<name><surname>Baba</surname><given-names>M.</given-names></name>
</person-group><article-title>A simple identification method for object shapes and materials using an ultrasonic sensor array</article-title><source>Proceedings of the 2006 IEEE Instrumentation and Measurement Technology Conference Proceedings</source><conf-loc>Sorrento, Italy</conf-loc><conf-date>24&#x02013;27 April 2006</conf-date><fpage>2138</fpage><lpage>2143</lpage><pub-id pub-id-type="doi">10.1109/IMTC.2006.328525</pub-id></element-citation></ref><ref id="B22-sensors-25-01086"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Eisele</surname><given-names>J.</given-names></name>
<name><surname>Gerlach</surname><given-names>A.</given-names></name>
<name><surname>Maeder</surname><given-names>M.</given-names></name>
<name><surname>Marburg</surname><given-names>S.</given-names></name>
</person-group><article-title>Convolutional neural network with data augmentation for object classification in automotive ultrasonic sensings</article-title><source>J. Acoust. Soc. Am.</source><year>2023</year><volume>153</volume><fpage>2447</fpage><pub-id pub-id-type="doi">10.1121/10.0017922</pub-id><pub-id pub-id-type="pmid">37092949</pub-id>
</element-citation></ref><ref id="B23-sensors-25-01086"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kalliris</surname><given-names>M.</given-names></name>
<name><surname>Kanarachos</surname><given-names>S.</given-names></name>
<name><surname>Kotsakis</surname><given-names>R.</given-names></name>
<name><surname>Haas</surname><given-names>O.</given-names></name>
<name><surname>Blundell</surname><given-names>M.</given-names></name>
</person-group><article-title>Machine learning algorithms for wet road surface detection using acoustic measurements</article-title><source>Proceedings of the 2019 IEEE International Conference on Mechatronics (ICM)</source><conf-loc>Ilmenau, Germany</conf-loc><conf-date>18&#x02013;20 March 2019</conf-date><fpage>265</fpage><lpage>270</lpage></element-citation></ref><ref id="B24-sensors-25-01086"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>B.</given-names></name>
<name><surname>Saniie</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep convolutional neural networks applied to ultrasonic images for material texture recognition</article-title><source>Proceedings of the IEEE International Ultrasonics Symposium (IUS)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>7&#x02013;11 September 2020</conf-date><fpage>1</fpage><lpage>3</lpage><pub-id pub-id-type="doi">10.1109/IUS46767.2020.9251734</pub-id></element-citation></ref><ref id="B25-sensors-25-01086"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yan</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>D.</given-names></name>
<name><surname>Gao</surname><given-names>B.</given-names></name>
<name><surname>Tian</surname><given-names>G.Y.</given-names></name>
<name><surname>Cai</surname><given-names>Z.</given-names></name>
</person-group><article-title>A deep learning-based ultrasonic pattern recognition method for inspecting girth weld cracking of gas pipeline</article-title><source>IEEE Sens. J.</source><year>2020</year><volume>20</volume><fpage>7997</fpage><lpage>8006</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2020.2982680</pub-id></element-citation></ref><ref id="B26-sensors-25-01086"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Latete</surname><given-names>T.</given-names></name>
<name><surname>Gauthier</surname><given-names>B.</given-names></name>
<name><surname>Belanger</surname><given-names>P.</given-names></name>
</person-group><article-title>Towards using convolutional neural network to locate, identify and size defects in phased array ultrasonic testing</article-title><source>Ultrasonics</source><year>2021</year><volume>115</volume><fpage>106436</fpage><pub-id pub-id-type="doi">10.1016/j.ultras.2021.106436</pub-id><pub-id pub-id-type="pmid">33873024</pub-id>
</element-citation></ref><ref id="B27-sensors-25-01086"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Oliveira</surname><given-names>M.A.</given-names></name>
<name><surname>Simas Filho</surname><given-names>E.F.</given-names></name>
<name><surname>Albuquerque</surname><given-names>M.C.</given-names></name>
<name><surname>Santos</surname><given-names>Y.T.</given-names></name>
<name><surname>Da Silva</surname><given-names>L.C.</given-names></name>
<name><surname>Farias</surname><given-names>C.T.</given-names></name>
</person-group><article-title>Ultrasound-based identification of damage in wind turbine blades using novelty detection</article-title><source>Ultrasonics</source><year>2020</year><volume>108</volume><fpage>106166</fpage><pub-id pub-id-type="doi">10.1016/j.ultras.2020.106166</pub-id><pub-id pub-id-type="pmid">32526526</pub-id>
</element-citation></ref><ref id="B28-sensors-25-01086"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Eisele</surname><given-names>J.</given-names></name>
<name><surname>Gerlach</surname><given-names>A.</given-names></name>
<name><surname>Maeder</surname><given-names>M.</given-names></name>
<name><surname>Marburg</surname><given-names>S.</given-names></name>
</person-group><article-title>Relevance of phase information for object classification in automotive ultrasonic sensing using convolutional neural networks</article-title><source>J. Acoust. Soc. Am.</source><year>2024</year><volume>155</volume><fpage>1060</fpage><lpage>1070</lpage><pub-id pub-id-type="doi">10.1121/10.0024753</pub-id><pub-id pub-id-type="pmid">38341735</pub-id>
</element-citation></ref><ref id="B29-sensors-25-01086"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xia</surname><given-names>X.</given-names></name>
<name><surname>Togneri</surname><given-names>R.</given-names></name>
<name><surname>Sohel</surname><given-names>F.</given-names></name>
<name><surname>Zhao</surname><given-names>Y.</given-names></name>
<name><surname>Huang</surname><given-names>D.</given-names></name>
</person-group><article-title>A Survey: Neural Network-Based Deep Learning for Acoustic Event Detection</article-title><source>Circuits Syst. Signal Process.</source><year>2019</year><volume>38</volume><fpage>3433</fpage><lpage>3453</lpage><pub-id pub-id-type="doi">10.1007/s00034-019-01094-1</pub-id></element-citation></ref><ref id="B30-sensors-25-01086"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bianco</surname><given-names>M.J.</given-names></name>
<name><surname>Gerstoft</surname><given-names>P.</given-names></name>
<name><surname>Traer</surname><given-names>J.</given-names></name>
<name><surname>Ozanich</surname><given-names>E.</given-names></name>
<name><surname>Roch</surname><given-names>M.A.</given-names></name>
<name><surname>Gannot</surname><given-names>S.</given-names></name>
</person-group><article-title>Machine learning in acoustics: Theory and applications</article-title><source>J. Acoust. Soc. Am.</source><year>2019</year><volume>146</volume><fpage>3590</fpage><lpage>3628</lpage><pub-id pub-id-type="doi">10.1121/1.5133944</pub-id><pub-id pub-id-type="pmid">31795641</pub-id>
</element-citation></ref><ref id="B31-sensors-25-01086"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kroh</surname><given-names>P.</given-names></name>
<name><surname>Simon</surname><given-names>R.</given-names></name>
<name><surname>Rupitsch</surname><given-names>S.</given-names></name>
</person-group><article-title>Classification of sonar targets in air: A neural network approach</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>1176</elocation-id><pub-id pub-id-type="doi">10.3390/s19051176</pub-id><pub-id pub-id-type="pmid">30866574</pub-id>
</element-citation></ref><ref id="B32-sensors-25-01086"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Dindis</surname><given-names>G.</given-names></name>
<name><surname>Karamancioglu</surname><given-names>A.</given-names></name>
</person-group><article-title>A Testbench for Directivity and Attenuation Patterns Of Ultrasonic Sensor Arrays</article-title><source>Proceedings of the ISPEC 7 th International Conference On Engineering &#x00026; Natural Sciences</source><conf-loc>Izmir, Turkey</conf-loc><conf-date>8&#x02013;10 May 2020</conf-date></element-citation></ref><ref id="B33-sensors-25-01086"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Douglass</surname><given-names>M.J.J.</given-names></name>
</person-group><source>Hands-on Machine Learning with Scikit-Learn, Keras, and Tensorflow</source><edition>2nd ed.</edition><publisher-name>O&#x02019;Reilly Media, Inc.</publisher-name><publisher-loc>Newton, MA, USA</publisher-loc><year>2020</year><volume>Volume 43</volume><fpage>1135</fpage><lpage>1136</lpage></element-citation></ref><ref id="B34-sensors-25-01086"><label>34.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Goodfellow</surname><given-names>I.</given-names></name>
<name><surname>Bengio</surname><given-names>Y.</given-names></name>
<name><surname>Courville</surname><given-names>A.</given-names></name>
</person-group><source>Applications in Deep Learning</source><publisher-name>MIT</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2016</year><fpage>443</fpage><lpage>485</lpage></element-citation></ref><ref id="B35-sensors-25-01086"><label>35.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Goodfellow</surname><given-names>I.</given-names></name>
<name><surname>Bengio</surname><given-names>Y.</given-names></name>
<name><surname>Courville</surname><given-names>A.</given-names></name>
</person-group><source>Convolutional Networks in Deep Learning</source><publisher-name>MIT</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2016</year><fpage>330</fpage><lpage>372</lpage></element-citation></ref><ref id="B36-sensors-25-01086"><label>36.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Habibi Aghdam</surname><given-names>H.</given-names></name>
<name><surname>Jahani Heravi</surname><given-names>E.</given-names></name>
</person-group><source>Convolutional Neural Networks in Guide to Convolutional Neural Networks</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2017</year><fpage>85</fpage><lpage>130</lpage></element-citation></ref><ref id="B37-sensors-25-01086"><label>37.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Hovden</surname><given-names>I.</given-names></name>
</person-group><source>Optimizing Artificial Neural Network Hyperparameters and Architecture</source><publisher-name>University of Oslo</publisher-name><publisher-loc>Oslo, Norway</publisher-loc><year>2019</year></element-citation></ref><ref id="B38-sensors-25-01086"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kingma</surname><given-names>D.P.</given-names></name>
<name><surname>Ba</surname><given-names>J.</given-names></name>
</person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id><pub-id pub-id-type="arxiv">1412.6980</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01086-f001"><label>Figure 1</label><caption><p>Automated data collection mechanism.</p></caption><graphic xlink:href="sensors-25-01086-g001" position="float"/></fig><fig position="float" id="sensors-25-01086-f002"><label>Figure 2</label><caption><p>Improved ultrasonic scanner user interface.</p></caption><graphic xlink:href="sensors-25-01086-g002" position="float"/></fig><fig position="float" id="sensors-25-01086-f003"><label>Figure 3</label><caption><p>(<bold>a</bold>) Classified objects; (<bold>b</bold>) designed ultrasonic sensor.</p></caption><graphic xlink:href="sensors-25-01086-g003" position="float"/></fig><fig position="float" id="sensors-25-01086-f004"><label>Figure 4</label><caption><p>Ultrasonic sensor block diagram.</p></caption><graphic xlink:href="sensors-25-01086-g004" position="float"/></fig><fig position="float" id="sensors-25-01086-f005"><label>Figure 5</label><caption><p>Method architecture flowchart.</p></caption><graphic xlink:href="sensors-25-01086-g005" position="float"/></fig><fig position="float" id="sensors-25-01086-f006"><label>Figure 6</label><caption><p>Typical signal information obtained in each scan.</p></caption><graphic xlink:href="sensors-25-01086-g006" position="float"/></fig><fig position="float" id="sensors-25-01086-f007"><label>Figure 7</label><caption><p>Extracting signal envelope.</p></caption><graphic xlink:href="sensors-25-01086-g007" position="float"/></fig><fig position="float" id="sensors-25-01086-f008"><label>Figure 8</label><caption><p>Sample signal information obtained (<bold>top</bold>), and combined with the others to make one image (<bold>bottom</bold>).</p></caption><graphic xlink:href="sensors-25-01086-g008" position="float"/></fig><fig position="float" id="sensors-25-01086-f009"><label>Figure 9</label><caption><p>(<bold>a</bold>) Example of object positioning (single object); (<bold>b</bold>) representation of single object on the image.</p></caption><graphic xlink:href="sensors-25-01086-g009" position="float"/></fig><fig position="float" id="sensors-25-01086-f010"><label>Figure 10</label><caption><p>(<bold>a</bold>) Example of object positioning (Object A, Object B and Object C); (<bold>b</bold>) representation of objects on the image (Object A, Object B and Object C).</p></caption><graphic xlink:href="sensors-25-01086-g010" position="float"/></fig><fig position="float" id="sensors-25-01086-f011"><label>Figure 11</label><caption><p>The superposition of the waves by acquired signals from multiple objects. It should be noted that instances are at the 10 mm apart travel distances.</p></caption><graphic xlink:href="sensors-25-01086-g011" position="float"/></fig><fig position="float" id="sensors-25-01086-f012"><label>Figure 12</label><caption><p>In the process of multiple object recognition, signals obtained as a result of ultrasonic scanning and formation of a single image.</p></caption><graphic xlink:href="sensors-25-01086-g012" position="float"/></fig><fig position="float" id="sensors-25-01086-f013"><label>Figure 13</label><caption><p>(<bold>a</bold>) Sample location map of objects; (<bold>b</bold>) placement of objects in pictures.</p></caption><graphic xlink:href="sensors-25-01086-g013" position="float"/></fig><fig position="float" id="sensors-25-01086-f014"><label>Figure 14</label><caption><p>CNN architecture.</p></caption><graphic xlink:href="sensors-25-01086-g014" position="float"/></fig><fig position="float" id="sensors-25-01086-f015"><label>Figure 15</label><caption><p>Object recognition-CNN flowchart.</p></caption><graphic xlink:href="sensors-25-01086-g015" position="float"/></fig><fig position="float" id="sensors-25-01086-f016"><label>Figure 16</label><caption><p>Object and coordinate estimates in test data for 3 objects.</p></caption><graphic xlink:href="sensors-25-01086-g016" position="float"/></fig><fig position="float" id="sensors-25-01086-f017"><label>Figure 17</label><caption><p>k-fold cross validation accuracies.</p></caption><graphic xlink:href="sensors-25-01086-g017" position="float"/></fig><fig position="float" id="sensors-25-01086-f018"><label>Figure 18</label><caption><p>Distribution of coordinate prediction errors in cm.</p></caption><graphic xlink:href="sensors-25-01086-g018" position="float"/></fig><fig position="float" id="sensors-25-01086-f019"><label>Figure 19</label><caption><p>Distribution of mean errors for coordinates in cm.</p></caption><graphic xlink:href="sensors-25-01086-g019" position="float"/></fig><fig position="float" id="sensors-25-01086-f020"><label>Figure 20</label><caption><p>Analysis of images with errors greater than 4 cm.</p></caption><graphic xlink:href="sensors-25-01086-g020" position="float"/></fig><fig position="float" id="sensors-25-01086-f021"><label>Figure 21</label><caption><p>Comparison of original and noisy images.</p></caption><graphic xlink:href="sensors-25-01086-g021" position="float"/></fig><fig position="float" id="sensors-25-01086-f022"><label>Figure 22</label><caption><p>The situation where objects are very close to each other (B (x = 13 cm), C (x = 20 cm)).</p></caption><graphic xlink:href="sensors-25-01086-g022" position="float"/></fig><fig position="float" id="sensors-25-01086-f023"><label>Figure 23</label><caption><p>The situation where objects are very close to each other (B (x = 17 cm), C (x = 20 cm)).</p></caption><graphic xlink:href="sensors-25-01086-g023" position="float"/></fig><fig position="float" id="sensors-25-01086-f024"><label>Figure 24</label><caption><p>Object and coordinate estimates in test data for 4 objects.</p></caption><graphic xlink:href="sensors-25-01086-g024" position="float"/></fig><fig position="float" id="sensors-25-01086-f025"><label>Figure 25</label><caption><p>Object and coordinate estimates in test data for 5 objects.</p></caption><graphic xlink:href="sensors-25-01086-g025" position="float"/></fig><table-wrap position="float" id="sensors-25-01086-t001"><object-id pub-id-type="pii">sensors-25-01086-t001_Table 1</object-id><label>Table 1</label><caption><p>Related&#x000a0;works.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ref.</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Used Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object Type</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy Rate</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B3-sensors-25-01086" ref-type="bibr">3</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neural Networks</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cylinder, Cone, Parallelepiped</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B11-sensors-25-01086" ref-type="bibr">11</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CapsNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Object Height Estimation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B6-sensors-25-01086" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neural Networks</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Circumferential Composed of Mat.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B9-sensors-25-01086" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN and SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Composite Materials</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2017</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B17-sensors-25-01086" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MDC, ANN-MLP, KNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Road Surface Classification</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2016</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B19-sensors-25-01086" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ANN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Triangle, Rectangular, Circle</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B12-sensors-25-01086" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CapsNet and CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Triangle, Rectangular, Square, Circle</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B14-sensors-25-01086" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PCA with NN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Water, Rock, Soap Bar, Sand</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B21-sensors-25-01086" ref-type="bibr">21</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Material Identification</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2006</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B24-sensors-25-01086" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Material Texture Recognition</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.58</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B25-sensors-25-01086" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gas Pipeline</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.75</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B26-sensors-25-01086" ref-type="bibr">26</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Phased Array Defects</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.75</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B27-sensors-25-01086" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PCA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wind Turbine Blades</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B20-sensors-25-01086" ref-type="bibr">20</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN-MLP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cylinders and Triangular Prisms</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B28-sensors-25-01086" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CWT-CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bag, Objects, Curb, Tree/Pole, Pedestrian</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B22-sensors-25-01086" ref-type="bibr">22</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CWT-CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bag, Objects, Curb, Tree/Pole, Pedestrian</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B18-sensors-25-01086" ref-type="bibr">18</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stair Detection and Recognition</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2013</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.41</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B29-sensors-25-01086" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN, RNN, CRNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic Event Detection</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02013;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B30-sensors-25-01086" ref-type="bibr">30</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Different Algorithms</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic Review</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02013;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B15-sensors-25-01086" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1D-CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Glass, Wood, Metal Plate, Sponge, Cloth</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B31-sensors-25-01086" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ANN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Disc, Cylinder and Hollow Hemisphere</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High&#x000a0;Rate</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B16-sensors-25-01086" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PCA-SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Grass, Concrete, Sand, Gravel Terrain Substrates</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B23-sensors-25-01086" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Decision Tree, SVM, kNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Road Surface Classification</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B4-sensors-25-01086" ref-type="bibr">4</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LDA, QDA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Edge, Plan, Small Cylinder, Corner</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2001</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High&#x000a0;Rate</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B7-sensors-25-01086" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Door, Chair, Glass, Human Being Signal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2015</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B10-sensors-25-01086" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FLDA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Signal Classification</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2001</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High&#x000a0;Rate</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B13-sensors-25-01086" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D Target Recognition</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1995</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B5-sensors-25-01086" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fuzzy ARTMAP neural networks</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bottle,
Metal Trash Can, Styrofoam Sheet, Lego</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1999</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B8-sensors-25-01086" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">kNN, SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Material Classification</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cylindrical Objects of Different Diameters</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01086-t002"><object-id pub-id-type="pii">sensors-25-01086-t002_Table 2</object-id><label>Table 2</label><caption><p>Performance metrics for Object1, Object2 and Object3 results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Performance Metrics</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object2</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object3</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">92%</td><td align="center" valign="middle" rowspan="1" colspan="1">83%</td><td align="center" valign="middle" rowspan="1" colspan="1">79%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">93%</td><td align="center" valign="middle" rowspan="1" colspan="1">82%</td><td align="center" valign="middle" rowspan="1" colspan="1">79%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">92%</td><td align="center" valign="middle" rowspan="1" colspan="1">82%</td><td align="center" valign="middle" rowspan="1" colspan="1">79%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" rowspan="1" colspan="1">92%</td><td align="center" valign="middle" rowspan="1" colspan="1">82%</td><td align="center" valign="middle" rowspan="1" colspan="1">79%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Sensitivity</td><td align="center" valign="middle" rowspan="1" colspan="1">99%</td><td align="center" valign="middle" rowspan="1" colspan="1">99%</td><td align="center" valign="middle" rowspan="1" colspan="1">83%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Specificity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01086-t003"><object-id pub-id-type="pii">sensors-25-01086-t003_Table 3</object-id><label>Table 3</label><caption><p>Performance metrics with Gaussian noise.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Performance Metrics</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object2</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object3</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01086-t004"><object-id pub-id-type="pii">sensors-25-01086-t004_Table 4</object-id><label>Table 4</label><caption><p>Performance metrics with salt-and-pepper noise.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Performance Metrics</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object2</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object3</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01086-t005"><object-id pub-id-type="pii">sensors-25-01086-t005_Table 5</object-id><label>Table 5</label><caption><p>Performance results when objects are very close to each other (B (x = 17 cm), C (x = 20 cm)).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Performance Metrics</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object2</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">88%</td><td align="center" valign="middle" rowspan="1" colspan="1">79%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">90%</td><td align="center" valign="middle" rowspan="1" colspan="1">84%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">88%</td><td align="center" valign="middle" rowspan="1" colspan="1">79%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01086-t006"><object-id pub-id-type="pii">sensors-25-01086-t006_Table 6</object-id><label>Table 6</label><caption><p>Performance metrics for four objects.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Performance Metrics</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object2</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object3</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object4</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">87%</td><td align="center" valign="middle" rowspan="1" colspan="1">72%</td><td align="center" valign="middle" rowspan="1" colspan="1">88%</td><td align="center" valign="middle" rowspan="1" colspan="1">82%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">83%</td><td align="center" valign="middle" rowspan="1" colspan="1">71%</td><td align="center" valign="middle" rowspan="1" colspan="1">88%</td><td align="center" valign="middle" rowspan="1" colspan="1">79%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">82%</td><td align="center" valign="middle" rowspan="1" colspan="1">71%</td><td align="center" valign="middle" rowspan="1" colspan="1">86%</td><td align="center" valign="middle" rowspan="1" colspan="1">80%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01086-t007"><object-id pub-id-type="pii">sensors-25-01086-t007_Table 7</object-id><label>Table 7</label><caption><p>Performance metrics for five objects.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Performance Metrics</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object2</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object3</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object4</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Object5</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">84%</td><td align="center" valign="middle" rowspan="1" colspan="1">73%</td><td align="center" valign="middle" rowspan="1" colspan="1">88%</td><td align="center" valign="middle" rowspan="1" colspan="1">68%</td><td align="center" valign="middle" rowspan="1" colspan="1">75%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">83%</td><td align="center" valign="middle" rowspan="1" colspan="1">71%</td><td align="center" valign="middle" rowspan="1" colspan="1">85%</td><td align="center" valign="middle" rowspan="1" colspan="1">67%</td><td align="center" valign="middle" rowspan="1" colspan="1">75%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" rowspan="1" colspan="1">83%</td><td align="center" valign="middle" rowspan="1" colspan="1">71%</td><td align="center" valign="middle" rowspan="1" colspan="1">86%</td><td align="center" valign="middle" rowspan="1" colspan="1">67%</td><td align="center" valign="middle" rowspan="1" colspan="1">75%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79%</td></tr></tbody></table></table-wrap></floats-group></article>