<!--

File produced by pipelineRunner package (for JATS 2 SCJATS with pipeline SCJATS)
At: 2025-05-22T10:16:48.928Z

Version        : 1.16.1
Last update    : 2024-08-27
Modified by    : dunnm

-->
<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="review-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Natl Sci Rev</journal-id><journal-id journal-id-type="iso-abbrev">Natl Sci Rev</journal-id><journal-id journal-id-type="publisher-id">nsr</journal-id><journal-title-group><journal-title>National Science Review</journal-title></journal-title-group><issn pub-type="ppub">2095-5138</issn><issn pub-type="epub">2053-714X</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40191253</article-id><article-id pub-id-type="pmc">PMC11970245</article-id>
<article-id pub-id-type="doi">10.1093/nsr/nwaf050</article-id><article-id pub-id-type="publisher-id">nwaf050</article-id><article-categories><subj-group subj-group-type="heading"><subject>Review</subject><subj-group subj-group-type="category-toc-heading"><subject>Computer Science</subject></subj-group></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>Nsr/2</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/MED00010</subject><subject>AcademicSubjects/SCI00010</subject></subj-group></article-categories><title-group><article-title>Generative artificial intelligence: a historical perspective</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>He</surname><given-names>Ran</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="lead">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision" degree-contribution="lead">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft" degree-contribution="equal">Writing - original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="equal">Writing - review &#x00026; editing</role><!--rhe@nlpr.ia.ac.cn--><aff>
<institution>New Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences</institution>, <addr-line>Beijing 100190</addr-line>, <country country="CN">China</country></aff><aff>
<institution>School of Intelligence Science and Technology, Nanjing University</institution>, <addr-line>Nanjing 210008</addr-line>, <country country="CN">China</country></aff><xref rid="cor1" ref-type="corresp"/></contrib><contrib contrib-type="author"><name><surname>Cao</surname><given-names>Jie</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation" degree-contribution="equal">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="supporting">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization" degree-contribution="lead">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft" degree-contribution="equal">Writing - original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="supporting">Writing - review &#x00026; editing</role><aff>
<institution>New Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences</institution>, <addr-line>Beijing 100190</addr-line>, <country country="CN">China</country></aff></contrib><contrib contrib-type="author"><name><surname>Tan</surname><given-names>Tieniu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="lead">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision" degree-contribution="lead">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation" degree-contribution="equal">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft" degree-contribution="equal">Writing - original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="equal">Writing - review &#x00026; editing</role><aff>
<institution>New Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences</institution>, <addr-line>Beijing 100190</addr-line>, <country country="CN">China</country></aff><aff>
<institution>School of Intelligence Science and Technology, Nanjing University</institution>, <addr-line>Nanjing 210008</addr-line>, <country country="CN">China</country></aff></contrib></contrib-group><author-notes><corresp id="cor1">
<bold>Corresponding author.</bold> E-mail: <email>rhe@nlpr.ia.ac.cn</email></corresp></author-notes><pub-date pub-type="collection"><month>5</month><year>2025</year></pub-date><pub-date pub-type="epub" iso-8601-date="2025-02-21"><day>21</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>21</day><month>2</month><year>2025</year></pub-date><volume>12</volume><issue>5</issue><elocation-id>nwaf050</elocation-id><history><date date-type="received"><day>11</day><month>10</month><year>2024</year></date><date date-type="rev-recd"><day>05</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>09</day><month>2</month><year>2025</year></date><date date-type="corrected-typeset"><day>27</day><month>3</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025. Published by Oxford University Press on behalf of China Science Publishing &#x00026; Media Ltd.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="nwaf050.pdf"/><abstract><title>ABSTRACT</title><p>Generative artificial intelligence (GAI) has recently achieved significant success, enabling anyone to create texts, images, videos and even computer codes while providing insights that might not be possible with traditional tools. To stimulate future research, this work provides a brief summary of the ongoing and historical developments in GAI over the past 70&#x000a0;years. The achievements are grouped into four categories: (i) rule-based generative systems that follow specialized rules and instructions, (ii) model-based generative algorithms that produce new content based on statistical or graphical models, (iii) deep generative methodologies that utilize deep neural networks to learn how to generate new content from data and (iv) foundation models that are trained on extensive datasets and capable of performing a variety of generative tasks. This paper also reviews successful generative applications and identifies open challenges posed by remaining issues. In addition, this paper describes potential research directions aimed at better utilizing, understanding and harnessing GAI technologies.</p></abstract><abstract abstract-type="teaser"><p>This paper reviews the historical milestones, successful applications, and remaining challenges in generative artificial intelligence over the past seven decades.</p></abstract><kwd-group><kwd>artificial intelligence</kwd><kwd>foundation model</kwd><kwd>generative method</kwd></kwd-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>National Natural Science Foundation of China</institution><institution-id institution-id-type="DOI">10.13039/501100001809</institution-id></institution-wrap>
</funding-source><award-id>62206277</award-id><award-id>62425606</award-id><award-id>32341009</award-id></award-group></funding-group><counts><page-count count="15"/></counts></article-meta></front><body><sec sec-type="intro" id="sec1"><title>INTRODUCTION</title><p>Generative artificial intelligence (GAI) refers to a group of AI algorithms and models that are capable of producing new content, including texts, images, videos and problem-solving strategies, with human-like creativity and adaptability. The past few years have witnessed unprecedented advancements in GAI. Notably, the AI system ChatGPT&#x000a0;[<xref rid="bib1" ref-type="bibr">1</xref>] can communicate with humans in over 80 languages, and it can be used to perform almost any task for which text responses are appropriate. The capabilities of ChatGPT facilitate its use for generating visual, audio and even multimodal content. This success stems from the development of GAI over half a century. For instance, representative events include the rise of deep learning, transformer architectures and foundation models.</p><p>This work presents a systematic review of GAI from a historical perspective. The scope of the work includes modern GAI, which is realized through programmable computers. We review the history from the origin to the present, highlighting milestone events and organizing them into four stages.</p><list list-type="order"><list-item><p>
<italic toggle="yes">Rule-based generative systems</italic>. Computerized methods for autonomous generation emerged in the 1950s, followed by computer programs that are capable of generating data. These programs typically generate data by following the rules designed by human experts. During this period, expert systems achieved early success in some specific tasks.</p></list-item><list-item><p>
<italic toggle="yes">Model-based generative algorithms</italic>. Researchers designed generative algorithms based on statistical or physical models. Hence, GAI came to include studies in machine learning, neural networks, computer graphics, computer vision, etc. Then, various generative applications built upon these studies were introduced. Among these examples, technologies such as computer animation generation became reliable for practical use, and have started to replace human efforts in content generation.</p></list-item><list-item><p>
<italic toggle="yes">Deep generative methodologies</italic>. Benefiting from the growth in computational power and data resources, deep neural networks&#x000a0;[<xref rid="bib2" ref-type="bibr">2</xref>,<xref rid="bib3" ref-type="bibr">3</xref>] have demonstrated superior power in content generation&#x000a0;[<xref rid="bib4" ref-type="bibr">4</xref>]. Then, deep generative models, including autoregressive-based&#x000a0;[<xref rid="bib5" ref-type="bibr">5</xref>] and diffusion-based&#x000a0;[<xref rid="bib6" ref-type="bibr">6</xref>] models, have been introduced and served as the basis for numerous practical applications until the present. Moreover, researchers of computer graphics developed deep learning&#x02013;based approaches&#x000a0;[<xref rid="bib7" ref-type="bibr">7</xref>] that show improved capability and scalability in open environments.</p></list-item><list-item><p>
<italic toggle="yes">Foundation models</italic>. The advent of generative pretrained transformers (GPTs)&#x000a0;[<xref rid="bib8" ref-type="bibr">8&#x02013;10</xref>], which are a prominent family of foundation models, represents a significant revolution in GAI. Such models leverage deep learning techniques, but they are characterized by their large scale in terms of model size and training data. The strategy of scaling up yields unprecedented advantages, including high-quality content generation, natural interactions and versatility across tasks. Consequently, foundation models have become the driving force of content generation across various applications.</p></list-item></list><p>The rise of GAI has revolutionized the production of content and services to create multimedia data and other content types, such as plans, codes and proteins. The number of industries adopting GAI technologies has been increasing rapidly, especially since foundation models became popular. Today, traditional sectors such as manufacturing, developing industries such as autonomous driving&#x000a0;[<xref rid="bib11" ref-type="bibr">11</xref>] and emerging fields such as molecular design&#x000a0;[<xref rid="bib12" ref-type="bibr">12</xref>] have seen successful implementations based on generative approaches.</p><p>A representative timeline is shown in Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1</xref>, tracing the development trajectory of GAI methods and applications. In the remaining parts of this paper, we detail representative approaches, discuss the strengths and limitations of different kinds of generative technologies, and introduce successful generative applications in various fields. In addition, we summarize the open challenges and possible future directions.</p><fig position="float" id="fig1"><label>Figure 1.</label><caption><p>Timeline of the development of GAI methods and applications.</p></caption><graphic xlink:href="nwaf050fig1" position="float"/></fig></sec><sec id="sec2"><title>RULE-BASED GENERATIVE SYSTEMS</title><p>Studies of automatic data generation can be traced back to the 1950s, when symbolic AI emerged. During that period, as shown in Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2a</xref>, researchers designed rules based on their expertise and implemented programs to execute generative tasks according to such rules. Generally, such a program&#x000a0;[<xref rid="bib13" ref-type="bibr">13</xref>] consisted of two primary components, namely, a generation engine and an interpreter.</p><fig position="float" id="fig2"><label>Figure 2.</label><caption><p>Evolution of design principles in GAI.</p></caption><graphic xlink:href="nwaf050fig2" position="float"/></fig><p>The function of the generation engine is to generate data through various formulaic operations. It is structured around a knowledge base that includes rules and facts. Human experts design different types of rules and formulate them with distinct antecedent and consequent components&#x000a0;[<xref rid="bib14" ref-type="bibr">14</xref>]. Then, they embed the rules into the generation engine through coding in the form of symbolic descriptions. Notably, these rules are effective for one specific task. Therefore, subsequent research&#x000a0;[<xref rid="bib15" ref-type="bibr">15</xref>] has developed rules for more tasks, such as dialogue, translation, etc. On the other hand, the facts in the generation engine provide factual information that conveys assertions about propositions&#x000a0;[<xref rid="bib13" ref-type="bibr">13</xref>], properties&#x000a0;[<xref rid="bib14" ref-type="bibr">14</xref>], relations&#x000a0;[<xref rid="bib16" ref-type="bibr">16</xref>], etc. When an input signal that contains factual information is received, the generation engine traverses all the rules. Then, after the engine has identified a rule that matches the current fact, it takes actions following the rules and generates new facts. This process runs in a loop until the rules for concluding the generation process are satisfied.</p><p>The interpreter in the generative programs ensures that humans can understand the reasons behind the operations made by the generation engine. To achieve this goal, the interpreter translates the rules and all possible actions into explanations in a human-readable language&#x000a0;[<xref rid="bib17" ref-type="bibr">17</xref>]. The translation involves mapping the logical structure of the rules to descriptions that can be read by humans, ensuring that users can easily understand the reasoning behind the system&#x02019;s decisions. Notably, the interpreter is also rule based, but these special rules are not directly involved in the generation process. Hence, the rule-based generative program is a &#x02018;white box&#x02019;. The interpreter can always provide straightforward explanations for the generated results even if the data generation process is complex. In addition, debugging and modification of generative programs are also interpretable.</p><p>Expert systems, designed based on user-defined rules, were widely applied in generative tasks that required specialized knowledge from the 1950s to the 1990s. Successful applications included but were not limited to chatbots, machine translation systems and speech synthesis systems. We examine each of these representative applications as follows.</p><p>One of the first AI chatbots, ELIZA, was introduced in 1966. It acts as a psychotherapist that responds to patients. ELIZA processes the text inputs via pattern matching and then seeks predesigned responses based on the rules. The success of this pioneering chatbot is rooted in the limited scope of discussion topics, where rules are quite effective for simulating human conversations. Subsequent studies&#x000a0;[<xref rid="bib18" ref-type="bibr">18</xref>] designed chatbots for more roles such as a schizophrenia patient. However, these rule-based chatbots had a limited ability to understand context and were only applicable to a specific task.</p><p>A machine translation system was first proposed in the late 1950s. It contained detailed linguistic and grammatical rules, as well as a fact base composed of linguistic knowledge. Subsequently, translation systems supported by rules from other computational experts and computational linguists were proposed. For instance, SYSTRAN&#x000a0;[<xref rid="bib19" ref-type="bibr">19</xref>], which was developed in 1968, served as a translation tool for web browsers until 2007.</p><p>The introduction of speech synthesis systems can be attributed to the system proposed by Fant <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib20" ref-type="bibr">20</xref>] in the 1960s. This system involved using linguistic and phonetic rules to model the characteristics of speech and connect speech segments. The synthesized speech fragments have a noticeable mechanical tone and are not fluent, differing significantly from natural human speech. Nevertheless, they are clear and easy to understand, thus meeting the requirements for certain practical applications.</p><p>Although rule-based generative programs have achieved notable applications, they always face an inherent challenge: scenarios outside predefined rules. For real-world applications, manually designed rules cannot consider all possible situations; thus, generative programs inevitably encounter situations beyond their capabilities. Moreover, in highly complex scenarios, the number of generative rules increases substantially, making the design and update processes prohibitively expensive.</p></sec><sec id="sec3"><title>MODEL-BASED GENERATIVE ALGORITHMS</title><p>To overcome the abovementioned inherent issues of the rule-based approach, researchers have explored generative algorithms based on models grounded in certain principles. The concept behind this design, shown in Fig&#x000a0;<xref rid="fig2" ref-type="fig">2b</xref>, continues to be the de facto standard in GAI. Below we review the algorithms based on statistical machine learning and computer graphics due to their significant contributions to GAI.</p><sec id="sec3-1"><title>Statistical machine learning models</title><p>Statistical machine learning aims to design algorithms that can learn from data how to complete tasks, instead of following explicit rules. Generally, these approaches can be categorized into discriminative and generative models&#x000a0;[<xref rid="bib21" ref-type="bibr">21</xref>]. The former focus on learning to make predictions and decisions from data. In contrast, generative models aim to model the data distribution and then synthesize data through inference or sampling. Since the 1960s, several research approaches to data generation have emerged for generative modeling or approximate inference.</p><p>Generative modeling methods capture the characteristics of data distributions to construct statistical models explicitly or implicitly. In general, the most typical explicit approach entails probabilistic graphical models [<xref rid="bib22" ref-type="bibr">22</xref>]. These models build graphs where the nodes are random variables that describe data, and the edges represent probabilistic relationships between variables. Building on this concept, generating data can be interpreted as the process of inferring the unknown part of nodes in the graph. The optimization process followed in applying these models uses likelihood maximization algorithms, most notably the expectation&#x02013;maximization algorithm [<xref rid="bib23" ref-type="bibr">23</xref>]. Moreover, these models typically postulate the Markov property&#x000a0;[<xref rid="bib24" ref-type="bibr">24</xref>], which states that the future state of data depends solely on its current state. Hidden Markov models [<xref rid="bib25" ref-type="bibr">25</xref>] introduce latent variables when generating data, that is, the observable variables depend on the latent variables. Following this approach, Harris <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib26" ref-type="bibr">26</xref>] constructed probabilistic graphical models where each node is independent of non-neighbors given that its neighboring nodes are determined. This type of approach significantly reduces the complexity of modeling, making the data generation process computationally feasible. Subsequent studies&#x000a0;[<xref rid="bib27" ref-type="bibr">27</xref>] have incorporated latent nodes into the graph model and successfully generated sequential data. Li <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib28" ref-type="bibr">28</xref>] introduced Markov networks that enforce bidirectional and symmetric edges defined by potential functions. Such networks are widely utilized for generating high-dimensional data, such as images. Friedman <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib29" ref-type="bibr">29</xref>] developed Bayesian networks that use direct edges to represent dependencies between nodes, which is effective for generating new content based on partially known data.</p><p>Autoregressive models&#x000a0;[<xref rid="bib5" ref-type="bibr">5</xref>] constitute another type of explicit modeling approach, particularly suited for data consisting of sequential elements. The generation of elements follows a one-by-one approach, where the probability distribution of each element is estimated based on the previously generated values. Given the sequential nature of language and speech, autoregressive models appeared for their generation&#x000a0;[<xref rid="bib30" ref-type="bibr">30</xref>] in the 1980s. In the 1990s, autoregressive neural networks capable of processing and generating sequential data were subsequently proposed&#x000a0;[<xref rid="bib31" ref-type="bibr">31</xref>]. More recently, autoregressive generation approaches&#x000a0;[<xref rid="bib8" ref-type="bibr">8</xref>] have been extended to large-scale neural networks, paving the way for the emergence of foundation models such as GPTs [<xref rid="bib9" ref-type="bibr">9</xref>].</p><p>Other studies have explored approaches to implicit data modeling. For example, normalizing flows&#x000a0;[<xref rid="bib32" ref-type="bibr">32</xref>] use a series of invertible transformations, i.e.&#x000a0;flows, to convert a prior distribution into a complex data distribution. These methods do not explicitly estimate the data distribution, but instead present a probability density function for data generation. When stochastic transformations are applied, the generation process is considered to be the evolution of a diffusion model. The concept of the diffusion model was introduced by Jarzynski&#x000a0;[<xref rid="bib6" ref-type="bibr">6</xref>] in a study of non-equilibrium systems. Stein <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib33" ref-type="bibr">33</xref>] proposed a probabilistic approach to learn the diffusion process using a parameterized model. It has been demonstrated theoretically [<xref rid="bib34" ref-type="bibr">34</xref>,<xref rid="bib35" ref-type="bibr">35</xref>] that flow-based models and diffusion-based models can be amalgamated into a collection of differential equations&#x000a0;[<xref rid="bib36" ref-type="bibr">36</xref>]. Although the two types of approaches were not mainstream at that time, their successors, probabilistic diffusion models [<xref rid="bib37" ref-type="bibr">37</xref>,<xref rid="bib38" ref-type="bibr">38</xref>] and flow matching&#x000a0;[<xref rid="bib39" ref-type="bibr">39</xref>], gained significant prominence in the deep learning era.</p><p>In addition, another group of approaches leverages artificial neural networks for generative modeling. The term &#x02018;artificial neural network&#x02019; originates from studies of nerve cells [<xref rid="bib40" ref-type="bibr">40</xref>], while its mathematical inception is rooted in hierarchical linear regression&#x000a0;[<xref rid="bib41" ref-type="bibr">41</xref>,<xref rid="bib42" ref-type="bibr">42</xref>]. In subsequent developments, artificial neural networks have advanced through the integration of non-linear transformations and the design of complex architectures, now known as deep learning. The basic computational unit in these networks is a neuron [<xref rid="bib43" ref-type="bibr">43</xref>]; neurons connect to each other through nonlinear activation functions. Such nonlinearity allows these models to theoretically approximate any distribution given a sufficient number of neurons. Hence, many studies leverage artificial neural networks to model data distributions for generative tasks.</p><p>Many studies have explored the architectures of artificial neural networks to specify how the components of networks are organized and interact. Several classic network architectures have been proposed, and their design principles continue to influence modern deep neural networks. Feed-forward neural networks, introduced in the 1950s&#x000a0;[<xref rid="bib44" ref-type="bibr">44</xref>], were widely adopted due to their simplicity and effectiveness. Convolutional neural networks (CNNs) emerged in the 1970s [<xref rid="bib45" ref-type="bibr">45</xref>]. Two-dimensional CNNs, as described by Zhang <italic toggle="yes">et&#x000a0;al.</italic> [<xref rid="bib46" ref-type="bibr">46</xref>], have become fundamental to image processing and generation with neural networks. Recurrent neural networks (RNNs) incorporate recurrent connections and internal memory, making them well suited for modeling and generating sequential data. The origin of RNNs can be traced back to the mathematical model in statistical mechanics introduced by Lenz&#x000a0;[<xref rid="bib47" ref-type="bibr">47</xref>] and Ising&#x000a0;[<xref rid="bib48" ref-type="bibr">48</xref>]. Kleene [<xref rid="bib49" ref-type="bibr">49</xref>] conducted a formal analysis of RNNs and framed them within the context of neural networks. Subsequently, the Amari&#x02013;Hopfield network [<xref rid="bib50" ref-type="bibr">50</xref>,<xref rid="bib51" ref-type="bibr">51</xref>] introduced the ability to learn and associatively recall data patterns. It provided a core architecture for storing and generating diverse types of data. Hochreiter and Schmidhuber [<xref rid="bib52" ref-type="bibr">52</xref>] introduced long short-term memory to better manage the memory and forgetting mechanisms, leading to significant improvements in generating textual data.</p><p>Furthermore, several studies [<xref rid="bib53" ref-type="bibr">53</xref>,<xref rid="bib54" ref-type="bibr">54</xref>] extend the Amari&#x02013;Hopfield network framework to explicitly model probability distributions. These models learn an energy function based on data, where lower values correspond to more probable data configurations. The restricted Boltzmann machine [<xref rid="bib55" ref-type="bibr">55</xref>] was introduced in the 1980s. Researchers applied a two-layer neural network for hierarchical data modeling and used an energy function to determine the probability of neural states.</p><p>Backpropagation is the core technique used to train modern neural networks, including those designed for generative tasks. In the 1970s, Linnainmaa&#x000a0;[<xref rid="bib56" ref-type="bibr">56</xref>] developed a method to optimize the parameters of neural network-like models by recursively applying the chain rule to compute derivatives. Werbos [<xref rid="bib57" ref-type="bibr">57</xref>] proposed applying this method to train artificial neural networks. Starting in the 1980s, this optimization method was commonly referred to by its current name, &#x02018;backpropagation&#x02019;&#x000a0;[<xref rid="bib58" ref-type="bibr">58</xref>]. Through a series of studies [<xref rid="bib58" ref-type="bibr">58</xref>,<xref rid="bib59" ref-type="bibr">59</xref>], backpropagation has proven to be a general method enabling neural networks to learn useful representations.</p><p>The inference process of generative models produces data from trained models. In practice, direct inference is often impractical due to its mathematical intractability or excessive computational demands&#x000a0;[<xref rid="bib60" ref-type="bibr">60</xref>]. Consequently, existing methods generate data through approximate inference based on empirical priors and observed historical data. Such methods typically fall into two categories: stochastic approximation and variational inference.</p><p>In the realm of generative models, stochastic approximation aims to estimate the probability distribution of data gradually by random sampling. The most representative method is the Markov chain Monte Carlo algorithm [<xref rid="bib61" ref-type="bibr">61</xref>,<xref rid="bib62" ref-type="bibr">62</xref>], which constructs a Markov chain where each state corresponds to a data point embedded in the sample space. A carefully designed probability between the states ensures that the stationary distribution of a Markov chain approaches the data distribution. Brooks <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib63" ref-type="bibr">63</xref>] introduced efficient sampling strategies assuming that the data distributions are almost independent. Subsequent research has focused on improving the strategies of step size selection&#x000a0;[<xref rid="bib64" ref-type="bibr">64</xref>], sample selection&#x000a0;[<xref rid="bib65" ref-type="bibr">65</xref>] and efficient computing&#x000a0;[<xref rid="bib66" ref-type="bibr">66</xref>].</p><p>Variational inference was systematically introduced into machine learning in the 1990s [<xref rid="bib67" ref-type="bibr">67</xref>,<xref rid="bib68" ref-type="bibr">68</xref>]. The core concept is utilizing a tractable parameterized distribution, known as a variational distribution, to approximate the real-world distributions of data. The approximation error between the variational distribution and the true distribution can be effectively measured by an evidence lower bound [<xref rid="bib67" ref-type="bibr">67</xref>]. Because of its flexibility, variational inference facilitates various generative tasks in complex scenarios.</p><p>When applied to generative tasks, the approaches based on statistical machine learning demonstrate better generalizability than rule-based generative systems. However, for these approaches, adapting to real-world situations remains challenging due to practical issues such as the curse of dimensionality. Nonetheless, some applications have achieved notable success. For instance, the rise of statistical machine translation, which replaced rule-based translation systems, occurred in the 1980s. This implies that the focus of GAI shifted from pursuing expert knowledge to collecting large-scale datasets. Later, in the 1990s, statistical approaches became mainstream for speech translation and synthesis until they were replaced by deep learning models. Moreover, statistical approaches have also been applied to visual content generation. But these applications are limited to specific tasks, such as texture synthesis and image fusion.</p></sec><sec id="sec3-2"><title>Graphics-based models</title><p>Graphics-based methods focus on creating visual content through physical modeling. These approaches stem from Marr&#x02019;s theory of vision&#x000a0;[<xref rid="bib69" ref-type="bibr">69</xref>], a paradigm for reconstructing the shape and appearance of real-world scenes. Within this framework, content generation is achieved through rendering, which refers to the process of combining materials, textures, lighting and other elements to produce visual effects. Recent studies&#x000a0;[<xref rid="bib7" ref-type="bibr">7</xref>,<xref rid="bib70" ref-type="bibr">70</xref>] have incorporated deep generative learning into these methods, thus making them a part of GAI.</p><p>Graphics-based methods reconstruct three-dimensional (3D) scenes either explicitly or implicitly. Explicit representations, such as lines, point clouds&#x000a0;[<xref rid="bib71" ref-type="bibr">71</xref>] and voxels&#x000a0;[<xref rid="bib72" ref-type="bibr">72</xref>], are intuitive to humans and were widely adopted in the early stages of research. In contrast, implicit reconstruction methods use deep neural networks to encode scene information, enabling the rendering of images at arbitrary resolutions.</p><p>Rendering techniques can be broadly categorized into two types, namely, rasterization and ray tracing, both of which play crucial roles in GAI methods. These two rendering approaches offer complementary advantages. Rasterization algorithms&#x000a0;[<xref rid="bib73" ref-type="bibr">73&#x02013;75</xref>] are highly efficient in utilizing hardware for fast rendering, whereas ray tracing algorithms&#x000a0;[<xref rid="bib76" ref-type="bibr">76&#x02013;78</xref>] provide superior image quality at the cost of intensive computation.</p><p>There are also notable studies&#x000a0;[<xref rid="bib79" ref-type="bibr">79</xref>,<xref rid="bib80" ref-type="bibr">80</xref>] of computing equipment customized for graphics-based methods. The first graphics processing unit (GPU) was presented in 1999, providing an accelerated framework for rasterization rendering. In 2004, Oh and Jung&#x000a0;[<xref rid="bib81" ref-type="bibr">81</xref>] proposed a GPU-based implementation of artificial neural networks. After two decades of development, GPUs have become foundational hardware devices for GAI. Moreover, researchers have focused on developing hardware-agnostic programming interfaces, such as OpenGL&#x000a0;[<xref rid="bib79" ref-type="bibr">79</xref>] and Direct3D&#x000a0;[<xref rid="bib80" ref-type="bibr">80</xref>]. These studies have standardized the pipeline of rasterization rendering used presently.</p><p>Graphics-based generative methods have led to widespread applications in computer animation. The first computer-animated film was released in 1958, marking the beginning of a gradual shift in film production, with generative methods gradually replacing manual techniques. In the mid-1990s, <italic toggle="yes">Toy Story</italic>, the first feature film created entirely with computer graphics, achieved significant commercial success. Since then, graphics-based generative methods have continued to evolve, becoming the core technology in video games, and enabling the production of highly realistic graphics and complex visual effects.</p></sec></sec><sec id="sec4"><title>DEEP GENERATIVE METHODOLOGIES</title><p>Notable achievements of GAI stem from the renaissance of deep learning&#x000a0;[<xref rid="bib82" ref-type="bibr">82</xref>,<xref rid="bib83" ref-type="bibr">83</xref>]. In 2011, researchers [<xref rid="bib2" ref-type="bibr">2</xref>] showed that increasing the depth of neural networks significantly improved their capacity to learn data representations, achieving superhuman performance in classification tasks. In the following year, subsequent studies [<xref rid="bib3" ref-type="bibr">3</xref>,<xref rid="bib84" ref-type="bibr">84</xref>] further corroborated the effectiveness of deep neural networks. Deep neural networks have since been widely applied to generative tasks&#x000a0;[<xref rid="bib4" ref-type="bibr">4</xref>,<xref rid="bib85" ref-type="bibr">85</xref>]. These networks demonstrate powerful capabilities for understanding data distributions and yield breakthroughs in producing realistic results.</p><sec id="sec4-1"><title>Deep network architectures</title><p>The studies on the architectures of deep neural networks can be divided into two categories: improvements over RNN-based architectures and developments built upon attention mechanisms.</p><p>Gers <italic toggle="yes">et&#x000a0;al.</italic> [<xref rid="bib86" ref-type="bibr">86</xref>] proposed a variant of RNNs with forget gates to process long data sequences. In subsequent studies, this mechanism was further elaborated in the form of gated recurrent units [<xref rid="bib87" ref-type="bibr">87</xref>], which included update gates and reset gates to control information flow. The update gate controls how much information is preserved, and the reset gate determines how much of the accumulated memory should be discarded. Networks that utilize the gated recurrent unit typically have fewer parameters and are computationally efficient; however, they are limited in handling long-term dependencies. Nonetheless, these characteristics make them suitable for real-time generative tasks.</p><p>Transformers&#x000a0;[<xref rid="bib88" ref-type="bibr">88</xref>] are the most influential deep architecture at present. They process input as tokens, which represent the basic units of data. In the case of textual data, tokens can be words, characters or bytes, depending on the tokenization method used. Using different tokenization methods&#x000a0;[<xref rid="bib89" ref-type="bibr">89&#x02013;91</xref>], transformer-based models are capable of handling data from different modalities. Moreover, they can leverage large-scale training datasets, establishing themselves as state-of-the-art approaches for diverse applications, including generative tasks.</p><p>Transformers process sequential data through two key techniques: positional encoding and the self-attention mechanism. Positional encoding adds position information to the input embeddings, enabling the transformer to capture the relationships in sequential data. The self-attention mechanism assigns weights to different data elements and helps the transformer focus on the most useful parts of the data. This design allows transformers to capture long-range dependencies. Moreover, transformers are well suited for GPU-optimized operations, such as parallel computation, which leads to fast training and inference processes. Subsequent studies&#x000a0;[<xref rid="bib90" ref-type="bibr">90</xref>,<xref rid="bib92" ref-type="bibr">92</xref>] effectively adapted transformers to vision tasks. Vision transformers divide images into fixed-size patches and utilize a linear mapping technique to convert the patches into a sequence, thus unifying the backbone architecture for visual and textual data. Other efforts have focused on improving the computational efficiency of transformers; examples include linearized self-attention&#x000a0;[<xref rid="bib93" ref-type="bibr">93</xref>,<xref rid="bib94" ref-type="bibr">94</xref>], sparse transformers&#x000a0;[<xref rid="bib95" ref-type="bibr">95</xref>] and approximation approaches&#x000a0;[<xref rid="bib96" ref-type="bibr">96</xref>].</p><p>The attention mechanism has also inspired other network architectures in addition to transformers. For instance, the attention-based method introduced in&#x000a0;[<xref rid="bib97" ref-type="bibr">97</xref>] effectively models graph-structured data, including protein interactions. In addition, there are studies of deep architectures beyond the attention mechanism, such as capsule networks&#x000a0;[<xref rid="bib98" ref-type="bibr">98</xref>] and state space models&#x000a0;[<xref rid="bib99" ref-type="bibr">99</xref>].</p></sec><sec id="sec4-2"><title>Deep generative models</title><p>Deep generative models refer to machine learning models based on deep networks. These models originate from various generative theories, and the most representative categories include generative adversarial networks, variational autoencoders and probabilistic diffusion models.</p><p>Generative adversarial networks (GANs) [<xref rid="bib100" ref-type="bibr">100</xref>,<xref rid="bib101" ref-type="bibr">101</xref>] have been widely applied to various generative tasks due to their capability to produce realistic data. GANs engage in a minimax game, where the generator aims to produce realistic samples, while the discriminator aims to differentiate between generated and real samples. According to game theory, both networks improve their performance through the adversarial training process, until they reach a dynamic equilibrium where the discriminator cannot distinguish between generated and real samples. This provides theoretical support for the superiority of GANs in terms of generation quality. GAN-based generative models&#x000a0;[<xref rid="bib102" ref-type="bibr">102&#x02013;105</xref>] have further advantages, especially in terms of the controllability of generated content. For example, StyleGAN&#x000a0;[<xref rid="bib104" ref-type="bibr">104</xref>] can perform semantic editing on images at the pixel level. Additionally, the training and inference of GAN models are very fast, particularly in comparison to graphics-based methods. However, GANs suffer from model collapse, which means that the generator fails to fully capture the complexity of the data distribution, resulting in a restricted variety of generated data. Although some studies&#x000a0;[<xref rid="bib106" ref-type="bibr">106</xref>] have aimed to alleviate this problem, the training of GANs is still prone to collapse, particularly when the model is scaled up.</p><p>The variational autoencoder (VAE)&#x000a0;[<xref rid="bib107" ref-type="bibr">107</xref>] is another typical type of deep generative model. A VAE learns the distribution characteristics of high-dimensional data in a latent space. It utilizes an encoder network to map high-dimensional data to latent representations and a decoder network to reconstruct the data with resampled representations. During training, the VAE optimizes the reconstruction error while ensuring that the distribution of latent representations approaches a prior distribution. These approaches exhibit strengths that complement the capabilities of GANs. Theoretically, VAE-based models&#x000a0;[<xref rid="bib108" ref-type="bibr">108</xref>,<xref rid="bib109" ref-type="bibr">109</xref>] can capture the entire distribution of data. Thus, sampling representations from the latent space offers diverse unseen data points. However, the generated data tend to be blurry, and thus lack realistic appearances.</p><p>Probabilistic diffusion models&#x000a0;[<xref rid="bib37" ref-type="bibr">37</xref>,<xref rid="bib38" ref-type="bibr">38</xref>] describe data generation as a stochastic process. These models involve two processes: the forward diffusion process and the reverse process. During the forward process, prior noise is progressively added to the real data, and the model learns to predict the noise. Then, during the reverse process, the model transforms the sampled noise into data. Studies have focused on improving the speed of the reverse process, which includes introducing latent space generation&#x000a0;[<xref rid="bib110" ref-type="bibr">110</xref>], incorporating discriminative priors&#x000a0;[<xref rid="bib111" ref-type="bibr">111</xref>], combining model distillation techniques&#x000a0;[<xref rid="bib112" ref-type="bibr">112</xref>], etc. Diffusion-based approaches can utilize large-scale training data effectively and generalize well across various generative tasks. In particular, these methods have demonstrated unprecedented performance in zero-shot generative tasks, producing impressive scenes that do not exist in the real world. However, the training and inference of probabilistic diffusion models are computationally intensive. This results in computational demands that are orders of magnitude greater than those of GANs.</p><p>Deep generative models have gradually replaced traditional machine learning models since the late 2010s. The ability to utilize large-scale training data allows deep generative models significant flexibility in handling high-dimensional generative tasks. By that time, GAI-generated content had become realistic and was sometimes even indistinguishable from real content. Respective applications use transformer-based models to translate languages or generate various types of textual data, including documents, web pages and code. Moreover, deep generative models such as WaveNet [<xref rid="bib113" ref-type="bibr">113</xref>] can synthesize realistic and comprehensible audio content, thus supporting multiple applications such as music generation and speech synthesis. FaceSwap [<xref rid="bib114" ref-type="bibr">114</xref>], which manipulates media by replacing one person&#x02019;s appearance with that of another, was created in 2017.</p></sec><sec id="sec4-3"><title>Deep generative learning for graphics</title><p>The successful application of deep neural networks to 3D perception and understanding has inspired efforts to integrate these networks with rendering techniques. However, the traditional rendering process does not ensure differentiability with respect to model parameters. Therefore, some studies have developed differentiable rendering and end-to-end algorithms, which allow gradient-based parameter optimization and direct editing of 3D scenes.</p><p>The neural radiance field [<xref rid="bib7" ref-type="bibr">7</xref>] is a typical differentiable rendering technique based on implicit representations. The cited study is based on the overall framework of ray tracing, uses a multilayer network to model the volumetric scene function and applies volume rendering algorithms to simulate the process of light travel. Moreover, Fridovich <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib115" ref-type="bibr">115</xref>] used sparse voxel representations to achieve computationally efficient rendering. The recently proposed 3D Gaussian splatting [<xref rid="bib116" ref-type="bibr">116</xref>] utilizes a rasterization pipeline and employs neural point clouds as scene representations. Three-dimensional Gaussian splatting methods can meet the requirements of real-time rendering while generating realistic images.</p><p>Ray tracing algorithms have become popular in generative applications based on computer graphics due to improvements in computing equipment. In the 2010s, these algorithms brought realistic avatars to commercial films. The techniques of overlaying animated scenes with live-action footage were also developed, enabling high-fidelity interactive rendering. In addition, deep learning&#x02013;based supersampling technologies enabled real-time ray tracing at the 4K resolution on a consumer GPU. The availability of these technologies led to the emergence of various virtual, mixed and augmented reality devices, depicting lifelike digital worlds.</p></sec></sec><sec id="sec5"><title>FOUNDATION MODELS</title><p>The term &#x02018;foundational model&#x02019; was introduced in the report of Bommasani <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib117" ref-type="bibr">117</xref>]. It refers to a base model that is trained on broad data and can be adapted to a wide range of downstream tasks. Such a model is also known as a large X model, for example, a large language model.</p><p>Constructing foundation models aligns with the conceptual framework of classic model-based approaches, but does not mandate adherence to any particular type of generative model. The common approach today involves the use of deep neural networks, particularly transformers. As illustrated in Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2c</xref>, the training schemes typically include generative pretraining and fine-tuning&#x000a0;[<xref rid="bib8" ref-type="bibr">8</xref>], with specific details varying depending on the input and output data modalities. Foundation models represent a significant shift in GAI, achieving extraordinary performance in the generation of texts, images and contents of other modalities.</p><sec id="sec5-1"><title>Large language models</title><p>Foundation models were first applied in the language domain. Devlin <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib118" ref-type="bibr">118</xref>] attempted to pretrain a model on large-scale unlabeled corpora and then fine-tune the network according to specific downstream tasks. Interestingly, researchers find that scaling pretrained language models often leads to emergent abilities on downstream tasks [<xref rid="bib119" ref-type="bibr">119</xref>]. For example, a 175B-parameter model [<xref rid="bib9" ref-type="bibr">9</xref>] can solve few-shot tasks through in-context learning, whereas a 1.5B-parameter model [<xref rid="bib120" ref-type="bibr">120</xref>] cannot do so well. However, even fine-tuning such a large model is computationally expensive. Hence, Wei <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib121" ref-type="bibr">121</xref>] proposed instruction tuning to fine-tune foundation models on a collection of datasets described via instructions, substantially improving zero-shot performance on unseen tasks. In addition, some studies&#x000a0;[<xref rid="bib122" ref-type="bibr">122</xref>] have combined instruction tuning with human preferences and feedback. Notably, ChatGPT&#x000a0;[<xref rid="bib1" ref-type="bibr">1</xref>], which was developed based on the large language model of the GPT type [<xref rid="bib9" ref-type="bibr">9</xref>], presents an amazing ability to converse with humans. Subsequently, interest in large language models has continued to surge, giving rise to numerous influential studies&#x000a0;[<xref rid="bib123" ref-type="bibr">123&#x02013;126</xref>]. Among them, Gemini is a remarkable family of large models [<xref rid="bib123" ref-type="bibr">123</xref>], which demonstrates state-of-the-art capabilities in reasoning and understanding across various benchmarks. The recently released DeepSeek models [<xref rid="bib127" ref-type="bibr">127</xref>] have demonstrated remarkable capabilities, particularly in reasoning tasks, while significantly reducing computational costs.</p><p>Prompt engineering is an important technique for working with foundation models. It helps foundation models adapt to specific problems without a change of model parameters. There are typically two types of prompt engineering. One involves carefully designing good prompts for a specific problem. For example, context learning provides additional context, such as exemplars, to help models understand the problem. Kojima <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib128" ref-type="bibr">128</xref>] simply added &#x02018;Let&#x02019;s think step by step&#x02019; before each answer, which can achieve better performance. The other type of approach compels models to imitate the reasoning process of humans. For instance, the method in [<xref rid="bib129" ref-type="bibr">129</xref>] provides a few chains of thought demonstrations as exemplars in prompting. The least-to-most method [<xref rid="bib130" ref-type="bibr">130</xref>] breaks down a complex problem into a series of simpler subproblems and then solves them in sequence.</p><p>The success of large language models has advanced vision-language understanding, where the main point is aligning and fusing vision and language features. Researchers have proposed various methods with different architectural designs. The dual-encoder architecture [<xref rid="bib131" ref-type="bibr">131</xref>] uses a parallel visual and language encoder with aligned representations. The encoder-decoder architecture [<xref rid="bib132" ref-type="bibr">132</xref>] applies joint feature encoding and decoding sequentially. In addition, Alayrac <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib133" ref-type="bibr">133</xref>] used a large language model as an adapter, harnessing its superior capacity through visual prompts.</p></sec><sec id="sec5-2"><title>Large text-to-image models</title><p>Numerous large text-to-image models have been built based on foundation models, achieving unprecedented breakthroughs. The text-to-image models are typically based on GANs&#x000a0;[<xref rid="bib134" ref-type="bibr">134</xref>], autoregressive models&#x000a0;[<xref rid="bib135" ref-type="bibr">135</xref>] or probabilistic diffusion models&#x000a0;[<xref rid="bib136" ref-type="bibr">136</xref>,<xref rid="bib137" ref-type="bibr">137</xref>]. Their supervision for aligning the text-image features is obtained from large language models [<xref rid="bib138" ref-type="bibr">138</xref>] or vision-language models [<xref rid="bib131" ref-type="bibr">131</xref>]. Stable Diffusion&#x000a0;[<xref rid="bib136" ref-type="bibr">136</xref>] and FLUX&#x000a0;[<xref rid="bib137" ref-type="bibr">137</xref>] models are among the most commonly used text-to-image models and are known for their outstanding performance in generation and following textual instructions.</p><p>The text-to-image models facilitate various downstream tasks, including style transfer, personalization, semantic editing, image restoration, image enhancement, etc. These methods can be divided into three types: training-based, testing-time tuning and training-free approaches. The training-based approaches collect additional data and fine-tune the model. These methods include domain-specific editing with weak supervision [<xref rid="bib139" ref-type="bibr">139</xref>], reference and attribute guidance via self-supervision [<xref rid="bib140" ref-type="bibr">140</xref>] and instructional editing via full supervision [<xref rid="bib141" ref-type="bibr">141</xref>]. Testing-time tuning methods optimize the model parameters during model inference; such methods include embedding optimization [<xref rid="bib142" ref-type="bibr">142</xref>], hypernetwork guidance [<xref rid="bib143" ref-type="bibr">143</xref>], latent variable optimization [<xref rid="bib144" ref-type="bibr">144</xref>] and hybrid fine-tuning [<xref rid="bib145" ref-type="bibr">145</xref>]. Training-free methods use off-the-shelf models without changing any model parameters. These approaches [<xref rid="bib146" ref-type="bibr">146</xref>] refine the input texts or masks or alter the inverted latent code to generate outputs tailored to the specified task.</p></sec><sec id="sec5-3"><title>Large text-to-video models</title><p>Some research efforts&#x000a0;[<xref rid="bib110" ref-type="bibr">110</xref>,<xref rid="bib147" ref-type="bibr">147</xref>] have led to the development of large models for generating video content. These models are derived from text-to-image models, making them sub-derivatives of foundational models. Despite the variety of designs, most of the approaches follow the pipeline proposed in Sora&#x000a0;[<xref rid="bib147" ref-type="bibr">147</xref>]. This pipeline first generates low-dimensional videos or latent codes&#x000a0;[<xref rid="bib110" ref-type="bibr">110</xref>], which are then refined using temporal and spatial super-resolution techniques.</p><p>Current video generation models are extremely data intensive due to the staggering complexity of the state space in video data. As a result, effectively utilizing limited real-world data is crucial. Existing studies can be broadly categorized into two approaches: spatial-temporal compression&#x000a0;[<xref rid="bib148" ref-type="bibr">148</xref>] and efficient transfer learning&#x000a0;[<xref rid="bib149" ref-type="bibr">149</xref>].</p><p>Some studies&#x000a0;[<xref rid="bib147" ref-type="bibr">147</xref>,<xref rid="bib150" ref-type="bibr">150</xref>] adhere to the scaling law and increase investments in data and computational resources, leading to superior video generation quality and early commercial successes. For example, the aforementioned Sora model&#x000a0;[<xref rid="bib147" ref-type="bibr">147</xref>] can simulate real-world object interactions and generate corresponding videos lasting several minutes. Video models&#x000a0;[<xref rid="bib150" ref-type="bibr">150</xref>] from Runway AI are capable of generating visual storytelling scenes. Produced without human intervention, these synthetic results achieve remarkably realistic effects.</p></sec><sec id="sec5-4"><title>Expanding the use of foundation models</title><p>Foundation models have made prominent contributions to GAI. Moreover, applications based on such models can generate content beyond text and images. Because of their ability to generate textual instructions, large language models can help humans interact with computational software and even physical tools by natural language. This makes it possible for humans to use generative technologies without requiring specialized knowledge. For instance, Suno AI&#x000a0;[<xref rid="bib151" ref-type="bibr">151</xref>] allows users to generate realistic music through language descriptions, including customized voices and sound effects, while also designing album covers; First <italic toggle="yes">et&#x000a0;al.</italic>&#x000a0;[<xref rid="bib152" ref-type="bibr">152</xref>] applied large language models to automated reasoning, using them to generate proofs. This effort revitalizes the domain of formal software verification and mathematical problem solving. Additionally, research communities have explored using large language models for specialized tasks, such as automated manufacturing&#x000a0;[<xref rid="bib153" ref-type="bibr">153</xref>], algorithm design&#x000a0;[<xref rid="bib154" ref-type="bibr">154</xref>] and molecular discovery&#x000a0;[<xref rid="bib12" ref-type="bibr">12</xref>].</p><p>Because of their generalized capabilities for generation, foundation models provide an intuitive way to simulate real-world scenarios. By learning patterns in complex environments, foundation models can predict environmental dynamics and generate decision-making strategies. In this context, they serve as world models [<xref rid="bib155" ref-type="bibr">155</xref>] within a specific system. This facilitates the expansion of human comprehension, as the generated content can offer predictions about future events.</p><p>Foundation models have been applied to building world models in practical systems. For instance, studies on integrating foundation models into transportation systems [<xref rid="bib156" ref-type="bibr">156</xref>] focus on addressing practical challenges, such as vehicle navigation and communication. By fulfilling personalized demand through automated content generation, these approaches enhance both the service quality and efficiency of transportation systems. Approaches that integrate foundation models into physical entities have also emerged&#x000a0;[<xref rid="bib157" ref-type="bibr">157</xref>,<xref rid="bib158" ref-type="bibr">158</xref>]. These efforts leverage foundation models for action control. In addition, research on intelligent agents powered by foundation models is actively advancing [<xref rid="bib159" ref-type="bibr">159</xref>]. Such generative technologies mitigate the need for highly detailed physical modeling, which is expensive in real-world scenarios.</p></sec><sec id="sec5-5"><title>Limitations of foundation models</title><p>However, current GAI technologies are far from perfect. While foundation models are driving commercial success at an unprecedented pace, they still make mistakes in problems that are trivial for humans, much like the AI systems from 70&#x000a0;years ago did. For instance, as of this writing, an issue reported in the OpenAI community revealed that ChatGPT still incorrectly assumed that 9.11 was greater than 9.9 in a dialogue.</p><p>The example above illustrates the hallucination problem in foundation models, where they occasionally generate irrelevant, inconsistent or incorrect content. Such hallucinations can be highly misleading, causing users to believe that the provided information is accurate. In other instances, the generated content may be nonsensical, resulting in confusion among users. Additionally, foundation models are susceptible to reasoning errors, as demonstrated in the numeric comparison example, as well as factual inaccuracies, where some of the information is fabricated.</p><p>Diagnosing and fixing foundation models is challenging due to their black-box nature. The generative processes are not interpretable, as foundation models consist of complex neural networks trained on vast datasets. As a result, diagnosis is primarily based on outputs, without direct access to the decision-making processes. Such opacity leads to the current approach of applying case-by-case patches to address specific issues, while a general fix remains essentially difficult.</p><p>The bottleneck of computational resources presents another significant challenge. The hardware costs for developing foundation models are prohibitively high for individuals, academic institutions and even some AI research organizations. While some tiny versions of foundation models can be deployed on commercial-grade devices, the cost of training a foundation model with billions of parameters is measured in millions of dollars. Running foundation models on portable computing devices presents more challenges, particularly in resource management, computation offloading and mobility management, among others [<xref rid="bib160" ref-type="bibr">160</xref>]. OpenAI reports that updating a large model can take several months due to limited computational power, meaning that such models cannot incorporate real-time information or receive timely updates.</p><p>Harnessing the capabilities of foundation models, which aims to avoid generating illegal, immoral, biased or inaccurate information, is becoming increasingly challenging. Several serious issues have been identified in applications built on these models. For instance, AI chatbots can be tricked by carefully crafted prompts into leaking sensitive data, such as individuals&#x02019; names, phone numbers and addresses. Additionally, the content generated by large models can be misused, but preventing such misuse is difficult due to gaps in current security policies.</p></sec></sec><sec id="sec6"><title>FUTURE DIRECTIONS</title><p>GAI applications based on foundation models have become the current mainstream practice, but the vulnerabilities of foundation models have also been inherited. Currently, research on GAI safety lags behind its technological development. We summarize several critical security issues that urgently require further attention and development.</p><p>
<italic toggle="yes">Value alignment.</italic> GAI should understand human intentions and adhere to human values, ensuring that the generated content is helpful while preventing misuse for inappropriate purposes. This goal promotes the practices of responsible GAI and requires a deeper understanding of evaluating alignment capabilities&#x000a0;[<xref rid="bib161" ref-type="bibr">161</xref>]. It also necessitates the development of more comprehensive guidelines that accurately reflect human preferences, which presents a significant challenge to current statistics-based evaluation paradigms.</p><p>
<italic toggle="yes">Source identification.</italic> Current GAI-generated content is convincing and easy to manipulate. This makes it necessary to ensure that the origin of generated content is traceable to prevent intellectual property disputes. Therefore, techniques such as invisible watermarks and signatures should be further studied to verify the integrity and ownership of the generated content. However, since GAI-generated content is highly malleable, imposing unalterable identifiers without negatively affecting usability presents a significant challenge.</p><p>
<italic toggle="yes">Security regulations.</italic> GAI developers should adhere to a necessary consensus, ensuring that their products do not harm humanity. On the one hand, standards organizations should require that the development process follows legal and ethical guidelines, similarly to the review process in scientific research. On the other hand, a correction mechanism must be established to prevent the dissemination of harmful GAI models or generated content. Depending on the circumstances, this mechanism should mandate that developers publicly verify, correct or retract any released GAI technologies that fail to meet these standards.</p><p>Despite safety concerns, the current wave of interest in GAI is likely to persist. Here, we discuss several promising directions with the potential to result in breakthrough improvements and address a broader range of human needs. We note that other dimensions, which may not yet have received broad attention, are also worthy of further exploration.</p><p>
<italic toggle="yes">Unification of modalities.</italic> Although there have been some achievements in bridging the textual and visual domains, research on the interactions among multiple systems [<xref rid="bib162" ref-type="bibr">162</xref>] is still in its early stages. How to align and fuse multiple modalities, including text, images, videos and structured data from different systems, remains an open challenge.</p><p>
<italic toggle="yes">Deciphering GAI models.</italic> Explaining how GAI models work, particularly the decision-making process, in a way that is understandable to humans, is both challenging and indispensable. Current efforts&#x000a0;[<xref rid="bib163" ref-type="bibr">163</xref>] have provided heuristic understanding based on phenomenological approaches. Moreover, introducing interpretable principles based on theories from domains such as thermodynamics&#x000a0;[<xref rid="bib38" ref-type="bibr">38</xref>] and electrodynamics&#x000a0;[<xref rid="bib164" ref-type="bibr">164</xref>], into AI modeling offers promising directions for enhancing the transparency of GAI, with initial successes already demonstrated. However, the reliability of this understanding should be grounded in a computational logic framework to ensure more accurate and dependable interpretations.</p><p>
<italic toggle="yes">Learning from GAI-generated content.</italic> Synthetic data are becoming increasingly accessible and important. While current studies assume that synthetic data primarily represent interpolations of patterns from existing data, GAI models have demonstrated the ability to make nontrivial inferences for specialized tasks. Moreover, GAI models can reduce labeling costs, augment existing datasets and facilitate the learning and understanding process of humans. Consequently, more extensive research is anticipated to explore the use of synthetic data, potentially making such data a dominant resource in the future.</p><p>
<italic toggle="yes">Supervision beyond human capability.</italic> Throughout the history of GAI, researchers have utilized knowledge familiar to humans to develop generative models. A recently released model [<xref rid="bib165" ref-type="bibr">165</xref>] trained through reinforcement learning without supervised data has demonstrated reasoning capabilities on par with top-performing models that rely on human supervision. This finding suggests that foundation models may develop superhuman capabilities through self-enhancement. Assuming that these models eventually achieve such capabilities, how can humans effectively regulate them? It is thus necessary to change the learning paradigm while adhering to the principles of serving humanity.</p></sec><sec sec-type="conclusions" id="sec7"><title>CONCLUSION</title><p>This work summarizes the historical and ongoing developments of GAI. We divide the methodologies into rule-based generative systems, model-based generative algorithms, deep generative methodologies and foundation models, and introduce their characteristics and applications. The focus is not on reviewing all the relevant literature, but rather on providing a brief summary of representative methodologies, emphasizing general principles and strategies rather than specific algorithms. Many strategies and ideas mentioned in this work can be realized in various forms, possibly with additional advantages in the future. We also discuss the remaining issues in the context of existing approaches. Moreover, we introduce some potential research directions to address the risks that may undermine the development of GAI.</p></sec></body><back><sec id="sec8"><title>FUNDING</title><p>This work was partially supported by the National Natural Science Foundation of China (62206277, 62425606 and 32341009).</p></sec><sec id="sec9"><title>AUTHOR CONTRIBUTIONS</title><p>T.T. and R.H. led the project&#x02019;s initiation and design. All authors contributed to the paper&#x02019;s structure and organization.</p></sec><sec><p>
<bold>
<italic toggle="yes">Conflict of interest statement.</italic>
</bold> None declared.</p></sec><ref-list id="ref1"><title>REFERENCES</title><ref id="bib1"><label>1.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>ChatGPT</collab>
</person-group>. <source>OpenAI</source>. <ext-link xlink:href="https://chatgpt.com" ext-link-type="uri">https://chatgpt.com</ext-link> &#x000a0;<comment>(15 January 2025, date last accessed)</comment>.</mixed-citation></ref><ref id="bib2"><label>2.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Ciresan</surname> &#x000a0;<given-names>DC</given-names></string-name>, <string-name><surname>Meier</surname> &#x000a0;<given-names>U</given-names></string-name>, <string-name><surname>Masci</surname> &#x000a0;<given-names>J</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Flexible, high performance convolutional neural networks for image classification</article-title>. In: <source>Proceedings of the 22nd International Joint Conference on Artificial Intelligence</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>AAAI Press</publisher-name>, <year>2011</year>, <fpage>1237</fpage>&#x02013;<lpage>42</lpage>.</mixed-citation></ref><ref id="bib3"><label>3.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Krizhevsky</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname> &#x000a0;<given-names>I</given-names></string-name>, <string-name><surname>Hinton</surname> &#x000a0;<given-names>GE</given-names></string-name></person-group>. <article-title>ImageNet classification with deep convolutional neural networks</article-title>. In: <source>Proceedings of the 26th International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2012</year>, <fpage>1097</fpage>&#x02013;<lpage>105</lpage>.</mixed-citation></ref><ref id="bib4"><label>4.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Radford</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Metz</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Chintala</surname> &#x000a0;<given-names>S</given-names></string-name></person-group>. <article-title>Unsupervised representation learning with deep convolutional generative adversarial networks</article-title>. <comment>International Conference on Learning Representations, San Juan, Puerto Rico, 2&#x02013;4 May 2016</comment>.</mixed-citation></ref><ref id="bib5"><label>5.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Yule</surname> &#x000a0;<given-names>GU</given-names></string-name>
</person-group>. <article-title>On a method of investigating periodicities disturbed series, with special reference to Wolfer&#x02019;s sunspot numbers</article-title>. <source>Philos Trans R Soc Lond</source> &#x000a0;<year>1927</year>; <volume>226</volume>: <fpage>267</fpage>&#x02013;<lpage>98</lpage>.<pub-id pub-id-type="doi">10.1098/rsta.1927.0007</pub-id></mixed-citation></ref><ref id="bib6"><label>6.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Jarzynski</surname> &#x000a0;<given-names>C.</given-names></string-name>
</person-group> &#x000a0;<article-title>Equilibrium free-energy differences from nonequilibrium measurements: a master-equation approach</article-title>. <source>Phys Rev E</source> &#x000a0;<year>1997</year>; <volume>56</volume>: <fpage>5018</fpage>.<pub-id pub-id-type="doi">10.1103/PhysRevE.56.5018</pub-id></mixed-citation></ref><ref id="bib7"><label>7.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Mildenhall</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Srinivasan</surname> &#x000a0;<given-names>PP</given-names></string-name>, <string-name><surname>Tancik</surname> &#x000a0;<given-names>M</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>NeRF: representing scenes as neural radiance fields for view synthesis</article-title>. <source>Commun ACM</source> &#x000a0;<year>2021</year>; <volume>65</volume>: <fpage>99</fpage>&#x02013;<lpage>106</lpage>.<pub-id pub-id-type="doi">10.1145/3503250</pub-id></mixed-citation></ref><ref id="bib8"><label>8.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>OpenAI</collab>
</person-group>. <article-title>Improving language understanding by generative pre-training</article-title>. <ext-link xlink:href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" ext-link-type="uri">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</ext-link> &#x000a0;<comment>(15 January 2025, date last accessed)</comment>.</mixed-citation></ref><ref id="bib9"><label>9.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Brown</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Mann</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Ryder</surname> &#x000a0;<given-names>N</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Language models are few-shot learners</article-title>. In: <source>Proceedings of the 34th International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2020</year>, <fpage>1877</fpage>&#x02013;<lpage>901</lpage>.</mixed-citation></ref><ref id="bib10"><label>10.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Achiam</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Adler</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Agarwal</surname> &#x000a0;<given-names>S</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>GPT-4 technical report</article-title>. arXiv: 2303.08774.</mixed-citation></ref><ref id="bib11"><label>11.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Dickmanns</surname> &#x000a0;<given-names>ED</given-names></string-name>
</person-group>. <source>Dynamic Vision for Perception and Control of Motion</source>. <publisher-loc>Heidelberg</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2007</year>.</mixed-citation></ref><ref id="bib12"><label>12.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Liu</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Yang</surname> &#x000a0;<given-names>Z</given-names></string-name>, <string-name><surname>Yu</surname> &#x000a0;<given-names>Z</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Generative artificial intelligence and its applications in materials science: current situation and future perspectives</article-title>. <source>J Mater</source> &#x000a0;<year>2023</year>; <volume>9</volume>: <fpage>798</fpage>&#x02013;<lpage>816</lpage>.<pub-id pub-id-type="doi">10.1016/j.jmat.2023.05.001</pub-id></mixed-citation></ref><ref id="bib13"><label>13.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Buchanan</surname> &#x000a0;<given-names>BG</given-names></string-name>, <string-name><surname>Duda</surname> &#x000a0;<given-names>RO</given-names></string-name></person-group>. <source>Advances in Computers</source>. <publisher-loc>Amsterdam</publisher-loc>: <publisher-name>Elsevier</publisher-name>, <year>1983</year>.</mixed-citation></ref><ref id="bib14"><label>14.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hayes-Roth</surname> &#x000a0;<given-names>F.</given-names></string-name>
</person-group> &#x000a0;<article-title>Rule-based systems</article-title>. <source>Commun ACM</source> &#x000a0;<year>1985</year>; <volume>28</volume>: <fpage>921</fpage>&#x02013;<lpage>32</lpage>.<pub-id pub-id-type="doi">10.1145/4284.4286</pub-id></mixed-citation></ref><ref id="bib15"><label>15.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Grosan</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Abraham</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Grosan</surname> &#x000a0;<given-names>C</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Rule-based expert systems</article-title>. <source>Intell Syst Mod Approach</source> &#x000a0;<year>2011</year>; <fpage>149</fpage>&#x02013;<lpage>85</lpage>.<pub-id pub-id-type="doi">10.1007/978-3-642-21004-4_7</pub-id></mixed-citation></ref><ref id="bib16"><label>16.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Masri</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Sultan</surname> &#x000a0;<given-names>YA</given-names></string-name>, <string-name><surname>Akkila</surname> &#x000a0;<given-names>AN</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Survey of rule-based systems</article-title>. <source>Int J Acad Inf Syst Res</source> &#x000a0;<year>2019</year>; <volume>3</volume>: <fpage>1</fpage>&#x02013;<lpage>22</lpage>.</mixed-citation></ref><ref id="bib17"><label>17.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Buchanan</surname> &#x000a0;<given-names>BG.</given-names></string-name>
</person-group> &#x000a0;<article-title>Rule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuristic Programming Project</article-title>. <publisher-loc>Boston</publisher-loc>: <publisher-name>Addison-Wesley</publisher-name>, <year>1984</year>.</mixed-citation></ref><ref id="bib18"><label>18.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Norvig</surname> &#x000a0;<given-names>P.</given-names></string-name>
</person-group> &#x000a0;<source>Paradigms of Artificial Intelligence Programming: Case Studies in Common LISP</source>. <publisher-loc>Burlington</publisher-loc>: <publisher-name>Morgan Kaufmann Publishers</publisher-name>, <year>2014</year>.</mixed-citation></ref><ref id="bib19"><label>19.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>SYSTRAN Software Inc. SYSTRAN</collab>
</person-group>. <ext-link xlink:href="https://www.systransoft.com" ext-link-type="uri">https://www.systransoft.com</ext-link> &#x000a0;<comment>(15 January 2025, date last accessed)</comment>.</mixed-citation></ref><ref id="bib20"><label>20.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Fant</surname> &#x000a0;<given-names>G.</given-names></string-name>
</person-group> &#x000a0;<source>The Modern Educational Treatment of Deafness</source>. <publisher-loc>Manchester</publisher-loc>: <publisher-name>Manchester University Press</publisher-name>, <year>1960</year>.</mixed-citation></ref><ref id="bib21"><label>21.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Jaakkola</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Haussler</surname> &#x000a0;<given-names>D.</given-names></string-name></person-group> &#x000a0;<article-title>Exploiting generative models in discriminative classifiers</article-title>. In: <source>Proceedings of the 12th International Conference on Neural Information Processing Systems</source>, <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <year>1998</year>, <fpage>487</fpage>&#x02013;<lpage>93</lpage>.</mixed-citation></ref><ref id="bib22"><label>22.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Jordan</surname> &#x000a0;<given-names>MI.</given-names></string-name>
</person-group> &#x000a0;<source>Learning in Graphical Models</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <year>1999</year>.</mixed-citation></ref><ref id="bib23"><label>23.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Dempster</surname> &#x000a0;<given-names>AP</given-names></string-name>, <string-name><surname>Laird</surname> &#x000a0;<given-names>NM</given-names></string-name>, <string-name><surname>Rubin</surname> &#x000a0;<given-names>DB.</given-names></string-name></person-group> &#x000a0;<article-title>Maximum likelihood from incomplete data via the EM algorithm</article-title>. <source>J R Stat Soc Ser B</source> &#x000a0;<year>1977</year>; <volume>39</volume>: <fpage>1</fpage>&#x02013;<lpage>22</lpage>.<pub-id pub-id-type="doi">10.1111/j.2517-6161.1977.tb01600.x</pub-id></mixed-citation></ref><ref id="bib24"><label>24.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Stratonovich</surname> &#x000a0;<given-names>RL.</given-names></string-name>
</person-group> &#x000a0;<article-title>Conditional Markov processes</article-title>. <source>Theory Probab Appl</source> &#x000a0;<year>1960</year>; <volume>5</volume>: <fpage>156</fpage>&#x02013;<lpage>78</lpage>.<pub-id pub-id-type="doi">10.1137/1105015</pub-id></mixed-citation></ref><ref id="bib25"><label>25.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Baum</surname> &#x000a0;<given-names>LE</given-names></string-name>, <string-name><surname>Petrie</surname> &#x000a0;<given-names>T.</given-names></string-name></person-group> &#x000a0;<article-title>Statistical inference for probabilistic functions of finite state Markov chains</article-title>. <source>Ann Math Stat</source> &#x000a0;<year>1966</year>; <volume>37</volume>: <fpage>1554</fpage>&#x02013;<lpage>63</lpage>.<pub-id pub-id-type="doi">10.1214/aoms/1177699147</pub-id></mixed-citation></ref><ref id="bib26"><label>26.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Harris</surname> &#x000a0;<given-names>TE.</given-names></string-name>
</person-group> &#x000a0;<article-title>Additive set-valued Markov processes and graphical methods</article-title>. <source>Ann Probab</source> &#x000a0;<year>1978</year>; <volume>6</volume>: <fpage>355</fpage>&#x02013;<lpage>78</lpage>.<pub-id pub-id-type="doi">10.1214/aop/1176995523</pub-id></mixed-citation></ref><ref id="bib27"><label>27.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Darroch</surname> &#x000a0;<given-names>JN</given-names></string-name>, <string-name><surname>Lauritzen</surname> &#x000a0;<given-names>SL</given-names></string-name>, <string-name><surname>Speed</surname> &#x000a0;<given-names>TP.</given-names></string-name></person-group> &#x000a0;<article-title>Markov fields and log-linear interaction models for contingency tables</article-title>. <source>Ann Stat</source> &#x000a0;<year>1980</year>; <volume>8</volume>: <fpage>522</fpage>&#x02013;<lpage>39</lpage>.<pub-id pub-id-type="doi">10.1214/aos/1176345006</pub-id></mixed-citation></ref><ref id="bib28"><label>28.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Li</surname> &#x000a0;<given-names>SZ</given-names></string-name>
</person-group>. <source>Markov random field models in computer vision</source>. In: <person-group person-group-type="editor"><string-name><surname>Eklundh</surname> &#x000a0;<given-names>JO</given-names></string-name></person-group> (ed) <source>Computer Vision&#x02014;ECCV &#x02019;94</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>1994</year>, <fpage>361</fpage>&#x02013;<lpage>70</lpage>.</mixed-citation></ref><ref id="bib29"><label>29.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Friedman</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Linial</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Nachman</surname> &#x000a0;<given-names>I</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Using Bayesian networks to analyze expression data</article-title>. <source>J Comput Biol</source> &#x000a0;<year>2000</year>; <volume>7</volume>: <fpage>601</fpage>&#x02013;<lpage>20</lpage>.<pub-id pub-id-type="doi">10.1089/106652700750050961</pub-id><pub-id pub-id-type="pmid">11108481</pub-id>
</mixed-citation></ref><ref id="bib30"><label>30.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Jurafsky</surname> &#x000a0;<given-names>D.</given-names></string-name>
</person-group> &#x000a0;<source>Speech and Language Processing</source>. <publisher-loc>Upper Saddle River</publisher-loc>: <publisher-name>Prentice-Hall</publisher-name>, <year>2000</year>.</mixed-citation></ref><ref id="bib31"><label>31.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schmidhuber</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Heil</surname> &#x000a0;<given-names>S.</given-names></string-name></person-group> &#x000a0;<article-title>Sequential neural text compression</article-title>. <source>IEEE Trans Neural Netw</source> &#x000a0;<year>1996</year>; <volume>7</volume>: <fpage>142</fpage>&#x02013;<lpage>6</lpage>.<pub-id pub-id-type="doi">10.1109/72.478398</pub-id><pub-id pub-id-type="pmid">18255564</pub-id>
</mixed-citation></ref><ref id="bib32"><label>32.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Abresch</surname> &#x000a0;<given-names>U</given-names></string-name>, <string-name><surname>Langer</surname> &#x000a0;<given-names>J.</given-names></string-name></person-group> &#x000a0;<article-title>The normalized curve shortening flow and homothetic solutions</article-title>. <source>J Differ Geom</source> &#x000a0;<year>1986</year>; <volume>23</volume>: <fpage>175</fpage>&#x02013;<lpage>96</lpage>.<pub-id pub-id-type="doi">10.4310/jdg/1214440025</pub-id></mixed-citation></ref><ref id="bib33"><label>33.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Stein</surname> &#x000a0;<given-names>C.</given-names></string-name>
</person-group> &#x000a0;<article-title>A bound for the error in the normal approximation to the distribution of a sum of dependent random variables</article-title>. <source>Berkeley Symp Math Stat Prob</source> &#x000a0;<year>1972</year>; <volume>2</volume>: <fpage>583</fpage>&#x02013;<lpage>602</lpage>.</mixed-citation></ref><ref id="bib34"><label>34.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Song</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Sohl-Dickstein</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Kingma</surname> &#x000a0;<given-names>DP</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Score-based generative modeling through stochastic differential equations</article-title>. <comment>International Conference on Learning Representations, Kigali, Rwanda, 1&#x02013;5 May 2023</comment>.</mixed-citation></ref><ref id="bib35"><label>35.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Zhang</surname> &#x000a0;<given-names>Q</given-names></string-name>, <string-name><surname>Chen</surname> &#x000a0;<given-names>Y</given-names></string-name></person-group>. <article-title>Diffusion normalizing flow</article-title>. In: <source>Proceedings of the 35th International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2021</year>, <fpage>16280</fpage>&#x02013;<lpage>91</lpage>.</mixed-citation></ref><ref id="bib36"><label>36.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Van Kampen</surname> &#x000a0;<given-names>NG</given-names></string-name>
</person-group>. <article-title>Stochastic differential equations</article-title>. <source>Phys Rep</source> &#x000a0;<year>1976</year>; <volume>24</volume>: <fpage>171</fpage>&#x02013;<lpage>228</lpage>.<pub-id pub-id-type="doi">10.1016/0370-1573(76)90029-6</pub-id></mixed-citation></ref><ref id="bib37"><label>37.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Song</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Ermon</surname> &#x000a0;<given-names>S</given-names></string-name></person-group>. <article-title>Generative modeling by estimating gradients of the data distribution</article-title>. In: <source>Proceedings of the 33rd International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2019</year>, <fpage>11918</fpage>&#x02013;<lpage>30</lpage>.</mixed-citation></ref><ref id="bib38"><label>38.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Ho</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Jain</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Abbeel</surname> &#x000a0;<given-names>P</given-names></string-name></person-group>. <article-title>Denoising diffusion probabilistic models</article-title>. In: <source>Proceedings of the 34th International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2020</year>, <fpage>6840</fpage>&#x02013;<lpage>51</lpage>.</mixed-citation></ref><ref id="bib39"><label>39.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lipman</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname> &#x000a0;<given-names>RT</given-names></string-name>, <string-name><surname>Ben-Hamu</surname> &#x000a0;<given-names>H</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Flow matching for generative modeling</article-title>. <comment>International Conference on Learning Representations, Kigali, Rwanda, 1&#x02013;5 May 2023</comment>.</mixed-citation></ref><ref id="bib40"><label>40.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>McCulloch</surname> &#x000a0;<given-names>WS</given-names></string-name>, <string-name><surname>Pitts</surname> &#x000a0;<given-names>W.</given-names></string-name></person-group> &#x000a0;<article-title>A logical calculus of the ideas immanent in nervous activity</article-title>. <source>Bull Math Biophys</source> &#x000a0;<year>1943</year>; <volume>5</volume>: <fpage>115</fpage>&#x02013;<lpage>33</lpage>.<pub-id pub-id-type="doi">10.1007/BF02478259</pub-id></mixed-citation></ref><ref id="bib41"><label>41.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Stigler</surname> &#x000a0;<given-names>SM.</given-names></string-name>
</person-group> &#x000a0;<article-title>Gauss and the invention of least squares</article-title>. <source>Ann Stat</source> &#x000a0;<year>1981</year>; <volume>9</volume>: <fpage>465</fpage>&#x02013;<lpage>74</lpage>.<pub-id pub-id-type="doi">10.1214/aos/1176345451</pub-id></mixed-citation></ref><ref id="bib42"><label>42.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Merriman</surname> &#x000a0;<given-names>M.</given-names></string-name>
</person-group> &#x000a0;<source>A List of Writings Relating to the Method of Least Squares: With Historical and Critical Notes</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Academy Press</publisher-name>, <year>1877</year>.</mixed-citation></ref><ref id="bib43"><label>43.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Jenkin</surname> &#x000a0;<given-names>N.</given-names></string-name>
</person-group> &#x000a0;<article-title>Affective processes in perception</article-title>. <source>Psychol Bull</source> &#x000a0;<year>1957</year>; <volume>54</volume>: <fpage>100</fpage>&#x02013;<lpage>27</lpage>.<pub-id pub-id-type="doi">10.1037/h0045230</pub-id><pub-id pub-id-type="pmid">13420272</pub-id>
</mixed-citation></ref><ref id="bib44"><label>44.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Rosenblatt</surname> &#x000a0;<given-names>F.</given-names></string-name>
</person-group> &#x000a0;<article-title>The perceptron: a probabilistic model for information storage and organization in the brain</article-title>. <source>Psychol Rev</source> &#x000a0;<year>1958</year>; <volume>65</volume>: <fpage>386</fpage>&#x02013;<lpage>408</lpage>.<pub-id pub-id-type="doi">10.1037/h0042519</pub-id><pub-id pub-id-type="pmid">13602029</pub-id>
</mixed-citation></ref><ref id="bib45"><label>45.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Fukushima</surname> &#x000a0;<given-names>K.</given-names></string-name>
</person-group> &#x000a0;<article-title>Neocognitron: a self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</article-title>. <source>Biol Cybern</source> &#x000a0;<year>1980</year>; <volume>36</volume>: <fpage>193</fpage>&#x02013;<lpage>202</lpage>.<pub-id pub-id-type="doi">10.1007/BF00344251</pub-id><pub-id pub-id-type="pmid">7370364</pub-id>
</mixed-citation></ref><ref id="bib46"><label>46.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Zhang</surname> &#x000a0;<given-names>W</given-names></string-name>, <string-name><surname>Tanida</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Itoh</surname> &#x000a0;<given-names>K</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Shift-invariant pattern recognition neural network and its optical architecture</article-title>. In: <source>Proceedings of Annual Conference of the Japan Society of Applied Physics</source>, <comment>Montreal, Canada: Japan Society of Applied Physics</comment>, <year>1988</year>, <fpage>2147</fpage>&#x02013;<lpage>51</lpage>.</mixed-citation></ref><ref id="bib47"><label>47.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lenz</surname> &#x000a0;<given-names>W.</given-names></string-name>
</person-group> &#x000a0;<article-title>Beitrag zum Verst&#x000e4;ndnis der magnetischen Erscheinungen in festen K&#x000f6;rpern</article-title>. <source>Z Phys</source> &#x000a0;<year>1920</year>; <volume>21</volume>: <fpage>613</fpage>&#x02013;<lpage>5</lpage>.</mixed-citation></ref><ref id="bib48"><label>48.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Ising</surname> &#x000a0;<given-names>E.</given-names></string-name>
</person-group> &#x000a0;<comment>Beitrag zur theorie des ferro-und paramagnetismus. Doctoral Thesis</comment>. <publisher-name>University of Hamburg</publisher-name>, <year>1924</year>.</mixed-citation></ref><ref id="bib49"><label>49.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kleene</surname> &#x000a0;<given-names>SC.</given-names></string-name>
</person-group> &#x000a0;<article-title>Representation of events in nerve nets and finite automata</article-title>. <source>Automata Stud</source> &#x000a0;<year>1951</year>; <volume>1</volume>: <fpage>3</fpage>&#x02013;<lpage>103</lpage>.</mixed-citation></ref><ref id="bib50"><label>50.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Amari</surname> &#x000a0;<given-names>SI.</given-names></string-name>
</person-group> &#x000a0;<article-title>Learning patterns and pattern sequences by self-organizing nets of threshold elements</article-title>. <source>IEEE Trans Comput</source> &#x000a0;<year>1972</year>; <volume>100</volume>: <fpage>1197</fpage>&#x02013;<lpage>206</lpage>.<pub-id pub-id-type="doi">10.1109/T-C.1972.223477</pub-id></mixed-citation></ref><ref id="bib51"><label>51.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hopfield</surname> &#x000a0;<given-names>JJ.</given-names></string-name>
</person-group> &#x000a0;<article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proc Natl Acad Sci USA</source> &#x000a0;<year>1982</year>; <volume>79</volume>: <fpage>2554</fpage>&#x02013;<lpage>8</lpage>.<pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id><pub-id pub-id-type="pmid">6953413</pub-id>
</mixed-citation></ref><ref id="bib52"><label>52.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hochreiter</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Schmidhuber</surname> &#x000a0;<given-names>J.</given-names></string-name></person-group> &#x000a0;<article-title>Long short-term memory</article-title>. <source>Neural Comput</source> &#x000a0;<year>1997</year>; <volume>9</volume>: <fpage>1735</fpage>&#x02013;<lpage>80</lpage>.<pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id>
</mixed-citation></ref><ref id="bib53"><label>53.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Heilmann</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Rigney</surname> &#x000a0;<given-names>D.</given-names></string-name></person-group> &#x000a0;<article-title>An energy-based model of friction and its application to coated systems</article-title>. <source>Wear</source> &#x000a0;<year>1981</year>; <volume>72</volume>: <fpage>195</fpage>&#x02013;<lpage>217</lpage>.<pub-id pub-id-type="doi">10.1016/0043-1648(81)90367-7</pub-id></mixed-citation></ref><ref id="bib54"><label>54.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>BakIr</surname> &#x000a0;<given-names>G.</given-names></string-name>
</person-group> &#x000a0;<source>Predicting Structured Data</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <year>2007</year>.</mixed-citation></ref><ref id="bib55"><label>55.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Rumelhart</surname> &#x000a0;<given-names>DE</given-names></string-name>, <string-name><surname>McClelland</surname> &#x000a0;<given-names>JL.</given-names></string-name></person-group> &#x000a0;<source>Information Processing in Dynamical Systems: Foundations of Harmony Theory</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <year>1986</year>.</mixed-citation></ref><ref id="bib56"><label>56.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Linnainmaa</surname> &#x000a0;<given-names>S.</given-names></string-name>
</person-group> &#x000a0;<article-title>The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors</article-title>. <comment>Master&#x02019;s Thesis</comment>. <publisher-name>The University of Helsinki</publisher-name>, <year>1970</year>.</mixed-citation></ref><ref id="bib57"><label>57.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Werbos</surname> &#x000a0;<given-names>PJ</given-names></string-name>
</person-group>. Applications of advances in nonlinear sensitivity analysis. In: <person-group person-group-type="editor"><string-name><surname>Drenick</surname> &#x000a0;<given-names>RF</given-names></string-name>, <string-name><surname>Kozin</surname> &#x000a0;<given-names>F</given-names></string-name></person-group> (eds.). <source>System Modeling and Optimization</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>1982</year>, <fpage>762</fpage>&#x02013;<lpage>70</lpage>.</mixed-citation></ref><ref id="bib58"><label>58.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Rumelhart</surname> &#x000a0;<given-names>DE</given-names></string-name>, <string-name><surname>Hinton</surname> &#x000a0;<given-names>GE</given-names></string-name>, <string-name><surname>Williams</surname> &#x000a0;<given-names>RJ.</given-names></string-name></person-group> &#x000a0;<article-title>Learning representations by back-propagating errors</article-title>. <source>Nature</source> &#x000a0;<year>1986</year>; <volume>323</volume>: <fpage>533</fpage>&#x02013;<lpage>6</lpage>.<pub-id pub-id-type="doi">10.1038/323533a0</pub-id></mixed-citation></ref><ref id="bib59"><label>59.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>LeCun</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Touresky</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Hinton</surname> &#x000a0;<given-names>G</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>A theoretical framework for back-propagation</article-title>. In: <source>Proceedings of the 1988 Connectionist Models Summer School</source>. <publisher-loc>Burlington</publisher-loc>: <publisher-name>Morgan Kaufmann</publisher-name>, <year>1988</year>, <fpage>21</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="bib60"><label>60.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Mitchell</surname> &#x000a0;<given-names>T.</given-names></string-name>
</person-group> &#x000a0;<source>Machine Learning</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>McGraw-Hill</publisher-name>, <year>1997</year>.</mixed-citation></ref><ref id="bib61"><label>61.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Peskun</surname> &#x000a0;<given-names>PH.</given-names></string-name>
</person-group> &#x000a0;<article-title>Optimum Monte-Carlo sampling using Markov chains</article-title>. <source>Biometrika</source> &#x000a0;<year>1973</year>; <volume>60</volume>: <fpage>607</fpage>&#x02013;<lpage>12</lpage>.<pub-id pub-id-type="doi">10.1093/biomet/60.3.607</pub-id></mixed-citation></ref><ref id="bib62"><label>62.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Geyer</surname> &#x000a0;<given-names>CJ.</given-names></string-name>
</person-group> &#x000a0;<article-title>Practical Markov chain Monte Carlo</article-title>. <source>Stat Sci</source> &#x000a0;<year>1992</year>; <volume>7</volume>: <fpage>473</fpage>&#x02013;<lpage>83</lpage>.<pub-id pub-id-type="doi">10.1214/ss/1177011137</pub-id></mixed-citation></ref><ref id="bib63"><label>63.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Brooks</surname> &#x000a0;<given-names>S.</given-names></string-name>
</person-group> &#x000a0;<article-title>Markov chain Monte Carlo method and its application</article-title>. <source>J R Stat Soc Ser D</source> &#x000a0;<year>1998</year>; <volume>47</volume>: <fpage>69</fpage>&#x02013;<lpage>100</lpage>.<pub-id pub-id-type="doi">10.1111/1467-9884.00117</pub-id></mixed-citation></ref><ref id="bib64"><label>64.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chib</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Greenberg</surname> &#x000a0;<given-names>E.</given-names></string-name></person-group> &#x000a0;<article-title>Understanding the Metropolis-Hastings algorithm</article-title>. <source>Am Stat</source> &#x000a0;<year>1995</year>; <volume>49</volume>: <fpage>327</fpage>&#x02013;<lpage>35</lpage>.<pub-id pub-id-type="doi">10.1080/00031305.1995.10476177</pub-id></mixed-citation></ref><ref id="bib65"><label>65.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hoffman</surname> &#x000a0;<given-names>MD</given-names></string-name>, <string-name><surname>Gelman</surname> &#x000a0;<given-names>A</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo</article-title>. <source>J Mach Learn Res</source> &#x000a0;<year>2014</year>; <volume>15</volume>: <fpage>1593</fpage>&#x02013;<lpage>623</lpage>.</mixed-citation></ref><ref id="bib66"><label>66.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Chen</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Fox</surname> &#x000a0;<given-names>E</given-names></string-name>, <string-name><surname>Guestrin</surname> &#x000a0;<given-names>C</given-names></string-name></person-group>. <article-title>Stochastic gradient Hamiltonian Monte Carlo</article-title>. In: <source>Proceedings of the 31st International Conference on Machine Learning</source>. <publisher-name>PMLR</publisher-name>, <year>2014</year>, <fpage>1683</fpage>&#x02013;<lpage>91</lpage>.</mixed-citation></ref><ref id="bib67"><label>67.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Jordan</surname> &#x000a0;<given-names>MI</given-names></string-name>, <string-name><surname>Ghahramani</surname> &#x000a0;<given-names>Z</given-names></string-name>, <string-name><surname>Jaakkola</surname> &#x000a0;<given-names>TS</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>An introduction to variational methods for graphical models</article-title>. <source>Mach Learn</source> &#x000a0;<year>1999</year>; <volume>37</volume>: <fpage>183</fpage>&#x02013;<lpage>233</lpage>.<pub-id pub-id-type="doi">10.1023/A:1007665907178</pub-id></mixed-citation></ref><ref id="bib68"><label>68.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wainwright</surname> &#x000a0;<given-names>MJ</given-names></string-name>, <string-name><surname>Jordan</surname> &#x000a0;<given-names>MI.</given-names></string-name></person-group> &#x000a0;<article-title>Graphical models, exponential families, and variational inference</article-title>. <source>Found Trends Mach Learn</source> &#x000a0;<year>2008</year>; <volume>1</volume>: <fpage>1</fpage>&#x02013;<lpage>305</lpage>.<pub-id pub-id-type="doi">10.1561/2200000001</pub-id></mixed-citation></ref><ref id="bib69"><label>69.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Marr</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Thach</surname> &#x000a0;<given-names>WT</given-names></string-name></person-group>. <article-title>A theory of cerebellar cortex</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Vaina</surname> &#x000a0;<given-names>L</given-names></string-name></person-group> (ed.). <source>From the Retina to the Neocortex</source>. <publisher-loc>Boston</publisher-loc>: <publisher-name>Birkhauser</publisher-name>, <year>1991</year>, <fpage>11</fpage>&#x02013;<lpage>50</lpage>.<pub-id pub-id-type="doi">10.1007/978-1-4684-6775-8_3</pub-id></mixed-citation></ref><ref id="bib70"><label>70.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Lindell</surname> &#x000a0;<given-names>DB</given-names></string-name>, <string-name><surname>Martel</surname> &#x000a0;<given-names>JN</given-names></string-name>, <string-name><surname>Wetzstein</surname> &#x000a0;<given-names>G</given-names></string-name></person-group>. <article-title>Autoint: automatic integration for fast neural volume rendering</article-title>. In: <source>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, <year>2021</year>, <fpage>14551</fpage>&#x02013;<lpage>60</lpage>.<pub-id pub-id-type="doi">10.1109/CVPR46437.2021.01432</pub-id></mixed-citation></ref><ref id="bib71"><label>71.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>V&#x000e1;rady</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Benko</surname> &#x000a0;<given-names>P</given-names></string-name></person-group>. <article-title>Reverse engineering B-rep models from multiple point clouds</article-title>. In: <source>Proceedings Geometric Modeling and Processing 2000. Theory and Applications</source>. <publisher-loc>Piscataway, NJ</publisher-loc>: <publisher-name>IEEE Press</publisher-name>, <year>2000</year>, <fpage>3</fpage>&#x02013;<lpage>12</lpage>.<pub-id pub-id-type="doi">10.1109/GMAP.2000.838234</pub-id></mixed-citation></ref><ref id="bib72"><label>72.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Caon</surname> &#x000a0;<given-names>M.</given-names></string-name>
</person-group> &#x000a0;<article-title>Voxel-based computational models of real human anatomy: a review</article-title>. <source>Radiat Environ Biophys</source> &#x000a0;<year>2004</year>; <volume>42</volume>: <fpage>229</fpage>&#x02013;<lpage>35</lpage>.<pub-id pub-id-type="doi">10.1007/s00411-003-0221-8</pub-id><pub-id pub-id-type="pmid">14730450</pub-id>
</mixed-citation></ref><ref id="bib73"><label>73.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Blinn</surname> &#x000a0;<given-names>JF</given-names></string-name>
</person-group>. <article-title>Models of light reflection for computer synthesized pictures</article-title>. In: <source>Proceedings of the 4th Annual Conference on Computer Graphics and Interactive Techniques</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>, <year>1977</year>, <fpage>192</fpage>&#x02013;<lpage>8</lpage>.<pub-id pub-id-type="doi">10.1145/563858.563893</pub-id></mixed-citation></ref><ref id="bib74"><label>74.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Bartell</surname> &#x000a0;<given-names>FO</given-names></string-name>, <string-name><surname>Dereniak EL and Wolfe</surname> &#x000a0;<given-names>WL</given-names></string-name></person-group>. <article-title>The theory and measurement of bidirectional reflectance distribution function (BRDF) and bidirectional transmittance distribution function (BTDF)</article-title>. In: <source>Radiation Scattering in Optical Systems</source>. <publisher-name>SPIE</publisher-name>, <year>1981</year>, <fpage>154</fpage>&#x02013;<lpage>60</lpage>.<pub-id pub-id-type="doi">10.1117/12.959611</pub-id></mixed-citation></ref><ref id="bib75"><label>75.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Akenine-Moller</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Haines</surname> &#x000a0;<given-names>E</given-names></string-name>, <string-name><surname>Hoffman</surname> &#x000a0;<given-names>N.</given-names></string-name></person-group> &#x000a0;<source>Real-time Rendering</source>. <publisher-loc>Boca Raton</publisher-loc>: <publisher-name>A K Peters/CRC Press</publisher-name>, <year>2019</year>.</mixed-citation></ref><ref id="bib76"><label>76.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Appel</surname> &#x000a0;<given-names>A</given-names></string-name>
</person-group>. <article-title>Some techniques for shading machine renderings of solids</article-title>. In: <source>Proceedings of the April 30&#x02013;May 2, 1968, Spring Joint Computer Conference</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>, <year>1968</year>, <fpage>37</fpage>&#x02013;<lpage>45</lpage>.<pub-id pub-id-type="doi">10.1145/1468075.1468082</pub-id></mixed-citation></ref><ref id="bib77"><label>77.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Whitted</surname> &#x000a0;<given-names>T.</given-names></string-name>
</person-group> &#x000a0;<article-title>An improved illumination model for shaded display</article-title>. In: <source>Proceedings of the 6th Annual Conference on Computer Graphics and Interactive Techniques</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>, <year>1979</year>.</mixed-citation></ref><ref id="bib78"><label>78.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Cline</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Talbot</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Egbert</surname> &#x000a0;<given-names>P.</given-names></string-name></person-group> &#x000a0;<article-title>Energy redistribution path tracing</article-title>. <source>ACM Trans Graph</source> &#x000a0;<year>2005</year>; <volume>24</volume>: <fpage>1186</fpage>&#x02013;<lpage>95</lpage>.<pub-id pub-id-type="doi">10.1145/1073204.1073330</pub-id></mixed-citation></ref><ref id="bib79"><label>79.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Woo</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Neider</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Davis</surname> &#x000a0;<given-names>T</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<source>OpenGL Programming Guide: The Official Guide to Learning OpenGL, Version 1.2</source>. <publisher-loc>Boston</publisher-loc>: <publisher-name>Addison-Wesley</publisher-name>, <year>1999</year>.</mixed-citation></ref><ref id="bib80"><label>80.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Blythe</surname> &#x000a0;<given-names>D.</given-names></string-name>
</person-group> &#x000a0;<article-title>The Direct3D 10 system</article-title>. <comment>Conference on Computer Graphics and Interactive Techniques, Boston, MA, USA, July 30&#x02013;3 August 2006</comment>.</mixed-citation></ref><ref id="bib81"><label>81.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Oh</surname> &#x000a0;<given-names>KS</given-names></string-name>, <string-name><surname>Jung</surname> &#x000a0;<given-names>K.</given-names></string-name></person-group> &#x000a0;<article-title>GPU implementation of neural networks</article-title>. <source>Pattern Recognit</source> &#x000a0;<year>2004</year>; <volume>37</volume>: <fpage>1311</fpage>&#x02013;<lpage>4</lpage>.<pub-id pub-id-type="doi">10.1016/j.patcog.2004.01.013</pub-id></mixed-citation></ref><ref id="bib82"><label>82.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ivakhnenko</surname> &#x000a0;<given-names>AG.</given-names></string-name>
</person-group> &#x000a0;<article-title>Polynomial theory of complex systems</article-title>. <source>IEEE Trans Syst Man Cybern</source> &#x000a0;<year>1971</year>; <fpage>364</fpage>&#x02013;<lpage>78</lpage>.<pub-id pub-id-type="doi">10.1109/TSMC.1971.4308320</pub-id></mixed-citation></ref><ref id="bib83"><label>83.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>LeCun</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Bengio</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Hinton</surname> &#x000a0;<given-names>G.</given-names></string-name></person-group> &#x000a0;<article-title>Deep learning</article-title>. <source>Nature</source> &#x000a0;<year>2015</year>; <volume>521</volume>: <fpage>436</fpage>&#x02013;<lpage>44</lpage>.<pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id>
</mixed-citation></ref><ref id="bib84"><label>84.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Le</surname> &#x000a0;<given-names>QV</given-names></string-name>, <string-name><surname>Ranzato</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Monga</surname> &#x000a0;<given-names>R</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Building high-level features using large scale unsupervised learning</article-title>. In: <source>Proceedings of the 29th International Coference on International Conference on Machine Learning</source>. <publisher-loc>Madison, WI</publisher-loc>: <publisher-name>Omnipress</publisher-name>, <year>2012</year>, <fpage>507</fpage>&#x02013;<lpage>14</lpage>.</mixed-citation></ref><ref id="bib85"><label>85.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Kingma</surname> &#x000a0;<given-names>DP</given-names></string-name>, <string-name><surname>Welling</surname> &#x000a0;<given-names>M.</given-names></string-name></person-group> &#x000a0;<article-title>Auto-encoding variational Bayes</article-title>. <comment>International Conference on Learning Representations, Banff, Canada, 14&#x02013;16 April 2014</comment>.</mixed-citation></ref><ref id="bib86"><label>86.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gers</surname> &#x000a0;<given-names>FA</given-names></string-name>, <string-name><surname>Schmidhuber</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Cummins</surname> &#x000a0;<given-names>F.</given-names></string-name></person-group> &#x000a0;<article-title>Learning to forget: continual prediction with LSTM</article-title>. <source>Neural Comput</source> &#x000a0;<year>2000</year>; <volume>12</volume>: <fpage>2451</fpage>&#x02013;<lpage>71</lpage>.<pub-id pub-id-type="doi">10.1162/089976600300015015</pub-id><pub-id pub-id-type="pmid">11032042</pub-id>
</mixed-citation></ref><ref id="bib87"><label>87.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Cho</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>van Merrienboer</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Gulcehre</surname> &#x000a0;<given-names>C</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Learning phrase representations using RNN encoder-decoder for statistical machine translation</article-title>. In: <source>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</source>. <publisher-loc>Kerrville, TX</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>, <year>2014</year>, <fpage>1724</fpage>&#x02013;<lpage>34</lpage>.<pub-id pub-id-type="doi">10.3115/v1/D14-1179</pub-id></mixed-citation></ref><ref id="bib88"><label>88.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Vaswani</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Shazeer</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname> &#x000a0;<given-names>N</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Attention is all you need</article-title>. In: <source>Proceedings of the 31st International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2017</year>, <fpage>6000</fpage>&#x02013;<lpage>10</lpage>.</mixed-citation></ref><ref id="bib89"><label>89.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Webster</surname> &#x000a0;<given-names>JJ</given-names></string-name>, <string-name><surname>Kit</surname> &#x000a0;<given-names>C</given-names></string-name></person-group>. <article-title>Tokenization as the initial phase in NLP</article-title>. In: <source>Proceedings of the 14th Conference on Computational Linguistics</source>. <publisher-loc>Kerrville, TX</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>, <year>1992</year>, <fpage>1106</fpage>&#x02013;<lpage>10</lpage>.<pub-id pub-id-type="doi">10.3115/992424.992434</pub-id></mixed-citation></ref><ref id="bib90"><label>90.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Dosovitskiy</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Beyer</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Kolesnikov</surname> &#x000a0;<given-names>A</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>An image is worth 16x16 words: transformers for image recognition at scale</article-title>. <comment>International Conference on Learning Representations, Virtual, 3&#x02013;7 May 2021</comment>.</mixed-citation></ref><ref id="bib91"><label>91.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Perozzi</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Al-Rfou</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Skiena</surname> &#x000a0;<given-names>S</given-names></string-name></person-group>. <article-title>DeepWalk: online learning of social representations</article-title>. In: <source>Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>, <year>2014</year>, <fpage>701</fpage>&#x02013;<lpage>10</lpage>.<pub-id pub-id-type="doi">10.1145/2623330.2623732</pub-id></mixed-citation></ref><ref id="bib92"><label>92.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Liu</surname> &#x000a0;<given-names>Z</given-names></string-name>, <string-name><surname>Lin</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Cao</surname> &#x000a0;<given-names>Y</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Swin transformer: hierarchical vision transformer using shifted windows</article-title>. In: <source>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</source>. <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, <year>2021</year>, <fpage>9992</fpage>&#x02013;<lpage>10002</lpage>.<pub-id pub-id-type="doi">10.1109/ICCV48922.2021.00986</pub-id></mixed-citation></ref><ref id="bib93"><label>93.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schmidhuber</surname> &#x000a0;<given-names>J</given-names></string-name>
</person-group>. <article-title>Learning to control fast-weight memories: an alternative to dynamic recurrent networks</article-title>. <source>Neural Comput</source> &#x000a0;<year>1992</year>; <volume>4</volume>: <fpage>131</fpage>&#x02013;<lpage>9</lpage>.<pub-id pub-id-type="doi">10.1162/neco.1992.4.1.131</pub-id></mixed-citation></ref><ref id="bib94"><label>94.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Katharopoulos</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Vyas</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Pappas</surname> &#x000a0;<given-names>N</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Transformers are RNNs: fast autoregressive transformers with linear attention</article-title>. In: <source>Proceedings of the 37th International Conference on Machine Learning</source>. <publisher-name>JMLR</publisher-name>, <year>2020</year>, <fpage>5156</fpage>&#x02013;<lpage>65</lpage>.</mixed-citation></ref><ref id="bib95"><label>95.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Tay</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Bahri</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Yang</surname> &#x000a0;<given-names>L</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Sparse sinkhorn attention</article-title>. In: <source>Proceedings of the 37th International Conference on Machine Learning</source>. <publisher-name>JMLR</publisher-name>, <year>2020</year>, <fpage>9438</fpage>&#x02013;<lpage>47</lpage>.</mixed-citation></ref><ref id="bib96"><label>96.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Yun</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Bhojanapalli</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Rawat</surname> &#x000a0;<given-names>AS</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Are transformers universal approximators of sequence-to-sequence functions?</article-title> &#x000a0;<comment>International Conference on Learning Representations, New Orleans, LA, 6&#x02013;9 May 2019</comment>.</mixed-citation></ref><ref id="bib97"><label>97.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Veli&#x0010d;kovi&#x00107;</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Cucurull</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Casanova</surname> &#x000a0;<given-names>A</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Graph attention networks</article-title>. <comment>International Conference on Learning Representations, Vancouver, Canada, 30 April&#x02013;3 May 2018</comment>.</mixed-citation></ref><ref id="bib98"><label>98.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Sabour</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Frosst</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Hinton</surname> &#x000a0;<given-names>GE</given-names></string-name></person-group>. <article-title>Dynamic routing between capsules</article-title>. In: <source>Proceedings of the 31st International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2017</year>, <fpage>3859</fpage>&#x02013;<lpage>69</lpage>.</mixed-citation></ref><ref id="bib99"><label>99.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Gu</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Dao</surname> &#x000a0;<given-names>T.</given-names></string-name></person-group> &#x000a0;<article-title>Mamba: Linear-time sequence modeling with selective state spaces</article-title>. <comment>Conference on Language Modeling, Philadelphia, PA, USA, 7&#x02013;9 October 2024</comment>.</mixed-citation></ref><ref id="bib100"><label>100.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Schmidhuber</surname> &#x000a0;<given-names>J</given-names></string-name>
</person-group>. <article-title>A possibility for implementing curiosity and boredom in model-building neural controllers</article-title>. In: <source>Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats</source>, <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <year>1991</year>, <fpage>222</fpage>&#x02013;<lpage>7</lpage>.<pub-id pub-id-type="doi">10.7551/mitpress/3115.003.0030</pub-id></mixed-citation></ref><ref id="bib101"><label>101.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Goodfellow</surname> &#x000a0;<given-names>IJ</given-names></string-name>, <string-name><surname>Pouget-Abadie</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Mirza</surname> &#x000a0;<given-names>M</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Generative adversarial networks</article-title>. In: <source>Proceedings of the 28th International Conference on Neural Information Processing Systems</source>, Vol. <volume>2</volume>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <year>2014</year>, <fpage>2672</fpage>&#x02013;<lpage>80</lpage>.</mixed-citation></ref><ref id="bib102"><label>102.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Brock</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Donahue</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Simonyan</surname> &#x000a0;<given-names>K.</given-names></string-name></person-group> &#x000a0;<article-title>Large scale GAN training for high fidelity natural image synthesis</article-title>. <comment>International Conference on Learning Representations, New Orleans, LA, 6&#x02013;9 May 2019</comment>.</mixed-citation></ref><ref id="bib103"><label>103.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Karras</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Aila</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Laine</surname> &#x000a0;<given-names>S</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Progressive growing of GANs for improved quality, stability, and variation</article-title>. <comment>International Conference on Learning Representations, Vancouver, Canada, 30 April&#x02013;3 May 2018</comment>.</mixed-citation></ref><ref id="bib104"><label>104.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Karras</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Laine</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Aila</surname> &#x000a0;<given-names>T</given-names></string-name></person-group>. <article-title>A style-based generator architecture for generative adversarial networks</article-title>. In: <source>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, <year>2019</year>, <fpage>4396</fpage>&#x02013;<lpage>405</lpage>.</mixed-citation></ref><ref id="bib105"><label>105.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Skorokhodov</surname> &#x000a0;<given-names>I</given-names></string-name>, <string-name><surname>Tulyakov</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Elhoseiny</surname> &#x000a0;<given-names>M</given-names></string-name></person-group>. <article-title>StyleGAN-V: a continuous video generator with the price, image quality and perks of StyleGAN2</article-title>. In: <source>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, <year>2022</year>, <fpage>3616</fpage>&#x02013;<lpage>26</lpage>.<pub-id pub-id-type="doi">10.1109/CVPR52688.2022.00361</pub-id></mixed-citation></ref><ref id="bib106"><label>106.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Arjovsky</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Chintala</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Bottou</surname> &#x000a0;<given-names>L</given-names></string-name></person-group>. <article-title>Wasserstein generative adversarial networks</article-title>. In: <source>Proceedings of the 34th International Conference on Machine Learning</source>. <publisher-name>PMLR</publisher-name>, <year>2017</year>, <fpage>214</fpage>&#x02013;<lpage>23</lpage>.</mixed-citation></ref><ref id="bib107"><label>107.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Kingma</surname> &#x000a0;<given-names>DP</given-names></string-name>, <string-name><surname>Welling</surname> &#x000a0;<given-names>M.</given-names></string-name></person-group> &#x000a0;<article-title>Auto-encoding variational Bayes</article-title>. <comment>International Conference on Learning Representations, Banff, Canada, 14&#x02013;16 April 2014</comment>.</mixed-citation></ref><ref id="bib108"><label>108.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Ranganath</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Gerrish</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Blei</surname> &#x000a0;<given-names>D</given-names></string-name></person-group>. <article-title>Black box variational inference</article-title>. In: <source>Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics</source>. <publisher-name>PMLR</publisher-name>, <year>2014</year>, <fpage>814</fpage>&#x02013;<lpage>22</lpage>.</mixed-citation></ref><ref id="bib109"><label>109.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Kucukelbir</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Ranganath</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Gelman</surname> &#x000a0;<given-names>A</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Automatic variational inference in Stan</article-title>. In: <source>Proceedings of the 29th International Conference on Neural Information Processing Systems</source>, Vol. <volume>1</volume>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <year>2015</year>, <fpage>568</fpage>&#x02013;<lpage>76</lpage>.</mixed-citation></ref><ref id="bib110"><label>110.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Rombach</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Blattmann</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Lorenz</surname> &#x000a0;<given-names>D</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>High-resolution image synthesis with latent diffusion models</article-title>. In: <source>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, <year>2022</year>, <fpage>10674</fpage>&#x02013;<lpage>85</lpage>.<pub-id pub-id-type="doi">10.1109/CVPR52688.2022.01042</pub-id></mixed-citation></ref><ref id="bib111"><label>111.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Dhariwal P and Nichol</surname> &#x000a0;<given-names>A</given-names></string-name>
</person-group>. <article-title>Diffusion models beat GANs on image synthesis</article-title>. In: <source>Proceedings of the 35th International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2021</year>, <fpage>8780</fpage>&#x02013;<lpage>94</lpage>.</mixed-citation></ref><ref id="bib112"><label>112.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Salimans</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Ho</surname> &#x000a0;<given-names>J.</given-names></string-name></person-group> &#x000a0;<article-title>Progressive distillation for fast sampling of diffusion models</article-title>. <comment>International Conference on Learning Representations, Virtual, 25&#x02013;29 April 2022</comment>.</mixed-citation></ref><ref id="bib113"><label>113.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Oord</surname> &#x000a0;<given-names>Avd</given-names></string-name>, <string-name><surname>Dieleman</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Zen</surname> &#x000a0;<given-names>H</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Wavenet: a generative model for raw audio</article-title>. In: <source>Proc. 9th ISCA Workshop on Speech Synthesis Workshop, Sunnyvale, CA, USA, 13&#x02013;15 September 2016</source>.</mixed-citation></ref><ref id="bib114"><label>114.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Deepfakes</collab>
</person-group>. <article-title>Deepfakes Software</article-title>. <ext-link xlink:href="https://github.com/deepfakes/faceswap" ext-link-type="uri">https://github.com/deepfakes/faceswap</ext-link> &#x000a0;<comment>(8 May 2024, date last accessed)</comment>.</mixed-citation></ref><ref id="bib115"><label>115.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Fridovich-Keil</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Yu</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Tancik</surname> &#x000a0;<given-names>M</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Plenoxels: radiance fields without neural networks</article-title>. In: <source>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, <year>2022</year>, <fpage>5491</fpage>&#x02013;<lpage>500</lpage>.<pub-id pub-id-type="doi">10.1109/CVPR52688.2022.00542</pub-id></mixed-citation></ref><ref id="bib116"><label>116.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kerbl</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Kopanas</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Leimk&#x000fc;hler</surname> &#x000a0;<given-names>T</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>3D Gaussian splatting for real-time radiance field rendering</article-title>. <source>ACM Trans Graph</source> &#x000a0;<year>2023</year>; <volume>42</volume>: <fpage>1</fpage>&#x02013;<lpage>14</lpage>.<pub-id pub-id-type="doi">10.1145/3592433</pub-id></mixed-citation></ref><ref id="bib117"><label>117.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bommasani</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Hudson</surname> &#x000a0;<given-names>DA</given-names></string-name>, <string-name><surname>Adeli</surname> &#x000a0;<given-names>E</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>On the opportunities and risks of foundation models</article-title>. arXiv: 2108.07258.</mixed-citation></ref><ref id="bib118"><label>118.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Devlin</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Chang</surname> &#x000a0;<given-names>MW</given-names></string-name>, <string-name><surname>Lee</surname> &#x000a0;<given-names>K</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>BERT: pre-training of deep bidirectional transformers for language understanding</article-title>. In: <source>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</source>. <publisher-loc>Kerrville, TX</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>, <year>2019</year>, <fpage>4171</fpage>&#x02013;<lpage>86</lpage>.</mixed-citation></ref><ref id="bib119"><label>119.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Wei</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Tay</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Bommasani</surname> &#x000a0;<given-names>R</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Emergent abilities of large language models</article-title>. arXiv: 2206.07682.</mixed-citation></ref><ref id="bib120"><label>120.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<collab>OpenAI</collab>
</person-group>. <article-title>Language models are unsupervised multitask learners</article-title>. <ext-link xlink:href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" ext-link-type="uri">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</ext-link> &#x000a0;<comment>(15 January 2025, date last accessed)</comment>.</mixed-citation></ref><ref id="bib121"><label>121.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Wei</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Bosma</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Zhao</surname> &#x000a0;<given-names>V</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Finetuned language models are zero-shot learners</article-title>. <comment>International Conference on Learning Representations, Virtual, 25&#x02013;29 April 2022</comment>.</mixed-citation></ref><ref id="bib122"><label>122.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Ouyang</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Wu</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Jiang</surname> &#x000a0;<given-names>X</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Training language models to follow instructions with human feedback</article-title>. In: <source>Proceedings of the 36th International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2022</year>, <fpage>27730</fpage>&#x02013;<lpage>44</lpage>.</mixed-citation></ref><ref id="bib123"><label>123.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Team</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Anil</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Borgeaud</surname> &#x000a0;<given-names>S</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Gemini: a family of highly capable multimodal models</article-title>. arXiv: 2312.11805.</mixed-citation></ref><ref id="bib124"><label>124.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chowdhery</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Narang</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Devlin</surname> &#x000a0;<given-names>J</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>PaLM: Scaling language modeling with pathways</article-title>. <source>J Mach Learn Res</source> &#x000a0;<year>2023</year>; <volume>24</volume>: <fpage>240</fpage>.</mixed-citation></ref><ref id="bib125"><label>125.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Touvron</surname> &#x000a0;<given-names>H</given-names></string-name>, <string-name><surname>Lavril</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Izacard</surname> &#x000a0;<given-names>G</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>LLaMA: open and efficient foundation language models</article-title>. arXiv: 2302.13971.</mixed-citation></ref><ref id="bib126"><label>126.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Du</surname> &#x000a0;<given-names>Z</given-names></string-name>, <string-name><surname>Qian</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Liu</surname> &#x000a0;<given-names>X</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>GLM: general language model pretraining with autoregressive blank infilling</article-title>. In: <source>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</source>. <publisher-loc>Kerrville, TX</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>, <year>2022</year>, <fpage>320</fpage>&#x02013;<lpage>35</lpage>.<pub-id pub-id-type="doi">10.18653/v1/2022.acl-long.26</pub-id></mixed-citation></ref><ref id="bib127"><label>127.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Liu</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Feng</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Xue</surname> &#x000a0;<given-names>B</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<comment>DeepSeek-V3 technical report</comment>. arXiv: 2412.19437.</mixed-citation></ref><ref id="bib128"><label>128.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Kojima</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Gu</surname> &#x000a0;<given-names>SS</given-names></string-name>, <string-name><surname>Reid</surname> &#x000a0;<given-names>M</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Large language models are zero-shot reasoners</article-title>. In: <source>Proceedings of the 36th International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2022</year>, <fpage>22199</fpage>&#x02013;<lpage>213</lpage>.</mixed-citation></ref><ref id="bib129"><label>129.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Wei</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Wang</surname> &#x000a0;<given-names>X</given-names></string-name>, <string-name><surname>Schuurmans</surname> &#x000a0;<given-names>D</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Chain-of-thought prompting elicits reasoning in large language models</article-title>. In: <source>Proceedings of the 36th International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2022</year>, <fpage>24824</fpage>&#x02013;<lpage>37</lpage>.</mixed-citation></ref><ref id="bib130"><label>130.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Zhou</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Sch&#x000e4;rli</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Hou</surname> &#x000a0;<given-names>L</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Least-to-most prompting enables complex reasoning in large language models</article-title>. <comment>International Conference on Learning Representations, Kigali, Rwanda, 1&#x02013;5 May 2023</comment>.</mixed-citation></ref><ref id="bib131"><label>131.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Radford</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Kim</surname> &#x000a0;<given-names>JW</given-names></string-name>, <string-name><surname>Hallacy</surname> &#x000a0;<given-names>C</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Learning transferable visual models from natural language supervision</article-title>. In: <source>Proceedings of the 38th International Conference on Machine Learning</source>. <publisher-name>PMLR</publisher-name>, <year>2021</year>, <fpage>8748</fpage>&#x02013;<lpage>63</lpage>.</mixed-citation></ref><ref id="bib132"><label>132.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Tsimpoukelli</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Menick</surname> &#x000a0;<given-names>JL</given-names></string-name>, <string-name><surname>Cabi</surname> &#x000a0;<given-names>S</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Multimodal few-shot learning with frozen language models</article-title>. In: <source>Proceedings of the 35th International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2021</year>, <fpage>200</fpage>&#x02013;<lpage>12</lpage>.</mixed-citation></ref><ref id="bib133"><label>133.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Alayrac</surname> &#x000a0;<given-names>JB</given-names></string-name>, <string-name><surname>Donahue</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Luc</surname> &#x000a0;<given-names>P</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Flamingo: a visual language model for few-shot learning</article-title>. In: <source>Proceedings of the 36th International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2022</year>, <fpage>23716</fpage>&#x02013;<lpage>36</lpage>.</mixed-citation></ref><ref id="bib134"><label>134.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Kang</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Zhu</surname> &#x000a0;<given-names>JY</given-names></string-name>, <string-name><surname>Zhang</surname> &#x000a0;<given-names>R</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Scaling up GANs for text-to-image synthesis</article-title>. In: <source>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, <year>2023</year>, <fpage>10124</fpage>&#x02013;<lpage>34</lpage>.<pub-id pub-id-type="doi">10.1109/CVPR52729.2023.00976</pub-id></mixed-citation></ref><ref id="bib135"><label>135.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Ramesh</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Pavlov</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Goh</surname> &#x000a0;<given-names>G</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Zero-shot text-to-image generation</article-title>. In: <source>Proceedings of the 38th International Conference on Machine Learning</source>. <publisher-name>PMLR</publisher-name>, <year>2021</year>, <fpage>8821</fpage>&#x02013;<lpage>31</lpage>.</mixed-citation></ref><ref id="bib136"><label>136.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Computer Vision and Learning research group at Ludwig Maximilian University of Munich. Stable Diffusion</collab>
</person-group>. <ext-link xlink:href="https://github.com/CompVis/stable-diffusion" ext-link-type="uri">https://github.com/CompVis/stable-diffusion</ext-link> &#x000a0;<comment>(8 May 2024, date last accessed)</comment>.</mixed-citation></ref><ref id="bib137"><label>137.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Black-forest-labs. FLUX</collab>
</person-group>. <ext-link xlink:href="https://github.com/black-forest-labs/flux" ext-link-type="uri">https://github.com/black-forest-labs/flux</ext-link> &#x000a0;<comment>(8 May 2024, date last accessed)</comment>.</mixed-citation></ref><ref id="bib138"><label>138.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Raffel</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Shazeer</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Roberts</surname> &#x000a0;<given-names>A</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Exploring the limits of transfer learning with a unified text-to-text transformer</article-title>. <source>J Mach Learn Res</source> &#x000a0;<year>2020</year>; <volume>21</volume>: <fpage>140</fpage>.</mixed-citation></ref><ref id="bib139"><label>139.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Kim</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Kwon</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Ye</surname> &#x000a0;<given-names>JC</given-names></string-name></person-group>. <article-title>DiffusionCLIP: text-guided diffusion models for robust image manipulation</article-title>. In: <source>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, <year>2022</year>, <fpage>2416</fpage>&#x02013;<lpage>25</lpage>.<pub-id pub-id-type="doi">10.1109/CVPR52688.2022.00246</pub-id></mixed-citation></ref><ref id="bib140"><label>140.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Yang</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Chen</surname> &#x000a0;<given-names>X</given-names></string-name>, <string-name><surname>Liao</surname> &#x000a0;<given-names>J</given-names></string-name></person-group>. <source>Uni-paint: a unified framework for multimodal image inpainting with pretrained diffusion model</source>. In: <source>Proceedings of the 31st ACM International Conference on Multimedia</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>, <year>2023</year>, <fpage>3190</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="bib141"><label>141.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Brooks</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Holynski</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Efros</surname> &#x000a0;<given-names>AA</given-names></string-name></person-group>. <source>InstructPix2Pix: learning to follow image editing instructions</source>. In: <source>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, <year>2023</year>, <fpage>18392</fpage>&#x02013;<lpage>402</lpage>.</mixed-citation></ref><ref id="bib142"><label>142.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Mokady</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Hertz</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Aberman</surname> &#x000a0;<given-names>K</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Null-text inversion for editing real images using guided diffusion models</article-title>. In: <source>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, <year>2023</year>, <fpage>6038</fpage>&#x02013;<lpage>47</lpage>.<pub-id pub-id-type="doi">10.1109/CVPR52729.2023.00585</pub-id></mixed-citation></ref><ref id="bib143"><label>143.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Zhang</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Huang</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Tang</surname> &#x000a0;<given-names>F</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Inversion-based style transfer with diffusion models</article-title>. In: <source>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, <year>2023</year>, <fpage>10146</fpage>&#x02013;<lpage>56</lpage>.</mixed-citation></ref><ref id="bib144"><label>144.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Mou</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Wang</surname> &#x000a0;<given-names>X</given-names></string-name>, <string-name><surname>Song</surname> &#x000a0;<given-names>J</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>DragonDiffusion: enabling drag-style manipulation on diffusion models</article-title>. <comment>International Conference on Learning Representations, Vienna, Austria, 7&#x02013;11 May 2024</comment>.</mixed-citation></ref><ref id="bib145"><label>145.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Kawar</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Zada</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Lang</surname> &#x000a0;<given-names>O</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Imagic: text-based real image editing with diffusion models</article-title>. In: <source>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, <year>2023</year>, <fpage>6007</fpage>&#x02013;<lpage>17</lpage>.<pub-id pub-id-type="doi">10.1109/CVPR52729.2023.00582</pub-id></mixed-citation></ref><ref id="bib146"><label>146.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Avrahami</surname> &#x000a0;<given-names>O</given-names></string-name>, <string-name><surname>Fried</surname> &#x000a0;<given-names>O</given-names></string-name>, <string-name><surname>Lischinski</surname> &#x000a0;<given-names>D</given-names></string-name></person-group>. <article-title>Blended latent diffusion</article-title>. <source>ACM Trans Graph</source> &#x000a0;<year>2023</year>; <volume>42</volume>: <fpage>149</fpage>.<pub-id pub-id-type="doi">10.1145/3592450</pub-id></mixed-citation></ref><ref id="bib147"><label>147.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>OpenAI. Sora</collab>
</person-group>. <ext-link xlink:href="https://openai.com/sora" ext-link-type="uri">https://openai.com/sora</ext-link> &#x000a0;<comment>(8 May 2024, date last accessed)</comment>.</mixed-citation></ref><ref id="bib148"><label>148.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname> &#x000a0;<given-names>X</given-names></string-name>, <string-name><surname>Ma</surname> &#x000a0;<given-names>X</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Lavie: high-quality video generation with cascaded latent diffusion models</article-title>. <source>Int J Comput Vis</source> &#x000a0;<year>2024</year>; doi: 10.1007/s11263-024-02295-1.<pub-id pub-id-type="doi">10.1007/s11263-024-02295-1</pub-id></mixed-citation></ref><ref id="bib149"><label>149.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>PKU-Yuan Group. Open Sora Plan</collab>
</person-group>. <ext-link xlink:href="https://github.com/PKU-YuanGroup/Open-Sora-Plan" ext-link-type="uri">https://github.com/PKU-YuanGroup/Open-Sora-Plan</ext-link> &#x000a0;<comment>(8 May 2024, date last accessed)</comment>.</mixed-citation></ref><ref id="bib150"><label>150.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>H&#x000f6;ppe</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Mehrjou</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Bauer</surname> &#x000a0;<given-names>S</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Diffusion models for video prediction and infilling</article-title>. <comment>Advances in Neural Information Processing Systems, New Orleans, LA, USA, 2022</comment>.</mixed-citation></ref><ref id="bib151"><label>151.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Suno Inc. Suno AI</collab>
</person-group>. <ext-link xlink:href="https://suno.com" ext-link-type="uri">https://suno.com</ext-link> &#x000a0;<comment>(8 May 2024, date last accessed)</comment>.</mixed-citation></ref><ref id="bib152"><label>152.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>First</surname> &#x000a0;<given-names>E</given-names></string-name>, <string-name><surname>Rabe</surname> &#x000a0;<given-names>MN</given-names></string-name>, <string-name><surname>Ringer</surname> &#x000a0;<given-names>T</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Baldur: whole-proof generation and repair with large language models</article-title>. In: <source>Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>, <year>2023</year>, <fpage>1229</fpage>&#x02013;<lpage>41</lpage>.<pub-id pub-id-type="doi">10.1145/3611643.3616243</pub-id></mixed-citation></ref><ref id="bib153"><label>153.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Rane</surname> &#x000a0;<given-names>NL</given-names></string-name>
</person-group>. <article-title>ChatGPT and similar generative artificial intelligence (AI) for smart industry: role, challenges and opportunities for industry 4.0, industry 5.0 and society 5.0</article-title>. <source>Innov Bus Strateg Manag</source> &#x000a0;<year>2024</year>; <volume>4</volume>: <fpage>10</fpage>&#x02013;<lpage>7</lpage>.<pub-id pub-id-type="doi">10.61577/ibsm.2024.100002</pub-id></mixed-citation></ref><ref id="bib154"><label>154.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Mankowitz</surname> &#x000a0;<given-names>DJ</given-names></string-name>, <string-name><surname>Michi</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Zhernov</surname> &#x000a0;<given-names>A</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Faster sorting algorithms discovered using deep reinforcement learning</article-title>. <source>Nature</source> &#x000a0;<year>2023</year>; <volume>618</volume>: <fpage>257</fpage>&#x02013;<lpage>63</lpage>.<pub-id pub-id-type="doi">10.1038/s41586-023-06004-9</pub-id><pub-id pub-id-type="pmid">37286649</pub-id>
</mixed-citation></ref><ref id="bib155"><label>155.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Ha D and Schmidhuber</surname> &#x000a0;<given-names>J</given-names></string-name>
</person-group>. <source>Recurrent world models facilitate policy evolution</source>. In: <source>Proceedings of the 32nd International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2018</year>, <fpage>2455</fpage>&#x02013;<lpage>67</lpage>.</mixed-citation></ref><ref id="bib156"><label>156.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Zhang</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Xiong</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Du</surname> &#x000a0;<given-names>H</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Generative AI-enabled vehicular networks: fundamentals, framework, and case study</article-title>. <source>IEEE Netw</source> &#x000a0;<year>2024</year>; <volume>38</volume>: <fpage>259</fpage>&#x02013;<lpage>67</lpage>.<pub-id pub-id-type="doi">10.1109/MNET.2024.3391767</pub-id></mixed-citation></ref><ref id="bib157"><label>157.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Brohan</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Brown</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Carbajal</surname> &#x000a0;<given-names>J</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>RT-1: Robotics transformer for real-world control at scale</article-title>. <comment>Robotics: Science and System, Daegu, Republic of Korea, 10&#x02013;14 July, 2023</comment>.</mixed-citation></ref><ref id="bib158"><label>158.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ahn</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Brohan</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Brown</surname> &#x000a0;<given-names>N</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Do as I can, not as I say: grounding language in robotic affordances</article-title>. In: <source>Proceedings of The 6th Conference on Robot Learning</source>, <year>2023</year>; <volume>205</volume>: <fpage>287</fpage>&#x02013;<lpage>318</lpage>.</mixed-citation></ref><ref id="bib159"><label>159.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Xi</surname> &#x000a0;<given-names>Z</given-names></string-name>, <string-name><surname>Chen</surname> &#x000a0;<given-names>W</given-names></string-name>, <string-name><surname>Guo</surname> &#x000a0;<given-names>X</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>The rise and potential of large language model based agents: a survey</article-title>. <source>Sci China Inf Sci</source> &#x000a0;<year>2025</year>; <volume>68</volume>: <fpage>121101</fpage>.<pub-id pub-id-type="doi">10.1007/s11432-024-4222-0</pub-id></mixed-citation></ref><ref id="bib160"><label>160.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Xu</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Du</surname> &#x000a0;<given-names>H</given-names></string-name>, <string-name><surname>Niyato</surname> &#x000a0;<given-names>D</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Unleashing the power of edge-cloud generative AI in mobile networks: a survey of AIGC services</article-title>. <source>IEEE Commun Surv Tutor</source> &#x000a0;<year>2024</year>; <fpage>1127</fpage>&#x02013;<lpage>70</lpage>.<pub-id pub-id-type="doi">10.1109/COMST.2024.3353265</pub-id></mixed-citation></ref><ref id="bib161"><label>161.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Liu</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Duan</surname> &#x000a0;<given-names>P</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Technical countermeasures for security risks of artificial general intelligence</article-title>. <source>Strategic Stud Chin Acad Eng</source> &#x000a0;<year>2021</year>; <volume>23</volume>: <fpage>75</fpage>&#x02013;<lpage>81</lpage>.<pub-id pub-id-type="doi">10.15302/J-SSCAE-2021.03.005</pub-id></mixed-citation></ref><ref id="bib162"><label>162.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Du</surname> &#x000a0;<given-names>H</given-names></string-name>, <string-name><surname>Niyato</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Kang</surname> &#x000a0;<given-names>J</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>The age of generative AI and AI-generated everything</article-title>. <source>IEEE Netw</source> &#x000a0;<year>2024</year>; <volume>38</volume>: <fpage>501</fpage>&#x02013;<lpage>12</lpage>.<pub-id pub-id-type="doi">10.1109/MNET.2024.3422241</pub-id></mixed-citation></ref><ref id="bib163"><label>163.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Tan</surname> &#x000a0;<given-names>Z</given-names></string-name>, <string-name><surname>Chen</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Zhang</surname> &#x000a0;<given-names>Z</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Sparsity-guided holistic explanation for LLMs with interpretable inference-time intervention</article-title>. In: <source>Proceedings of the 38th AAAI Conference on Artificial Intelligence</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>AAAI Press</publisher-name>, <year>2024</year>, <fpage>21619</fpage>&#x02013;<lpage>27</lpage>.<pub-id pub-id-type="doi">10.1609/aaai.v38i19.30160</pub-id></mixed-citation></ref><ref id="bib164"><label>164.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Xu</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Liu</surname> &#x000a0;<given-names>Z</given-names></string-name>, <string-name><surname>Tegmark</surname> &#x000a0;<given-names>M</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>Poisson flow generative models</article-title>. In: <source>Proceedings of the 36th International Conference on Neural Information Processing Systems</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2022</year>, <fpage>16782</fpage>&#x02013;<lpage>95</lpage>.</mixed-citation></ref><ref id="bib165"><label>165.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Guo</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Yang</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Zhang</surname> &#x000a0;<given-names>H</given-names></string-name> &#x000a0;<etal>et al.</etal></person-group> &#x000a0;<article-title>DeepSeek-R1: incentivizing reasoning capability in LLMs via reinforcement learning</article-title>. arXiv: 2501.12948.</mixed-citation></ref></ref-list></back></article>