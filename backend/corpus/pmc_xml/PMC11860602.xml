<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006451</article-id><article-id pub-id-type="pmc">PMC11860602</article-id><article-id pub-id-type="doi">10.3390/s25041222</article-id><article-id pub-id-type="publisher-id">sensors-25-01222</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Bridging Neuroscience and Machine Learning: A Gender-Based Electroencephalogram Framework for Guilt Emotion Identification</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-7778-3563</contrib-id><name><surname>Zaidi</surname><given-names>Saima Raza</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-01222" ref-type="aff">1</xref><xref rid="c1-sensors-25-01222" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Khan</surname><given-names>Najeed Ahmed</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01222" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Hasan</surname><given-names>Muhammad Abul</given-names></name><xref rid="af2-sensors-25-01222" ref-type="aff">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Sato</surname><given-names>Wataru</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01222"><label>1</label>CS &#x00026; IT Department, NED University of Engg &#x00026; Tech, Karachi 75270, Pakistan; <email>najeed@cloud.neduet.edu.pk</email></aff><aff id="af2-sensors-25-01222"><label>2</label>Bio-Medical Enginering Department, NED University of Engg &#x00026; Tech, Karachi 75270, Pakistan; <email>abulhasan@cloud.neduet.edu.pk</email></aff><author-notes><corresp id="c1-sensors-25-01222"><label>*</label>Correspondence: <email>saimarzaidi@cloud.neduet.edu.pk</email></corresp></author-notes><pub-date pub-type="epub"><day>17</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1222</elocation-id><history><date date-type="received"><day>08</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>31</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>03</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>This study explores the link between the emotion &#x0201c;guilt&#x0201d; and human EEG data, and investigates the influence of gender differences on the expression of guilt and neutral emotions in response to visual stimuli. Additionally, the stimuli used in the study were developed to ignite guilt and neutral emotions. Two emotions, &#x0201c;guilt&#x0201d; and &#x0201c;neutral&#x0201d;, were recorded from 16 participants after these emotions were induced using storyboards as pictorial stimuli. These storyboards were developed based on various guilt-provoking events shared by another group of participants. In the pre-processing step, collected data were de-noised using bandpass filters and ICA, then segmented into smaller sections for further analysis. Two approaches were used to feed these data to the SVM classifier. First, the novel approach employed involved feeding the data to SVM classifier without computing any features. This method provided an average accuracy of 83%. In the second approach, data were divided into Alpha, Beta, Gamma, Theta and Delta frequency bands using Discrete Wavelet Decomposition. Afterward, the computed features, including entropy, Hjorth parameters and Band Power, were fed to SVM classifiers. This approach achieved an average accuracy of 63%. The findings of both classification methodologies indicate that females are more expressive in response to depicted stimuli and that their brain cells exhibit higher feature values. Moreover, females displayed higher accuracy than males in all bands except the Delta band.</p></abstract><kwd-group><kwd>emotion recognition</kwd><kwd>brain&#x02013;computer interaction</kwd><kwd>machine learning</kwd><kwd>guilt</kwd><kwd>EEG</kwd></kwd-group><funding-group><funding-statement>We acknowledge funding from the Most Endowment Fund, NED University of Engineering &#x00026; Technology and Higher Education Commission Pakistan.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01222"><title>1. Introduction</title><p>Acceptance and amelioration of emotion-aware computing requires computers to be capable of interpreting and understanding a variety of human emotions. This understanding enables computers to respond and act more effectively, thereby maximizing user satisfaction. Emotions have a major role due to their significant effect on human moods, decisions and regular communication. Therefore, it is critical for computers to efficiently recognize human emotions to facilitate better integration into daily life. This capability helps machines and computers to be more attentive to human emotional needs. Due to the rapid growth of affective computing, emotion-aware systems are extensively used in real-time systems in health facilities [<xref rid="B1-sensors-25-01222" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01222" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01222" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01222" ref-type="bibr">4</xref>], smart driving systems [<xref rid="B5-sensors-25-01222" ref-type="bibr">5</xref>], voice recognition systems [<xref rid="B6-sensors-25-01222" ref-type="bibr">6</xref>], humanoids [<xref rid="B7-sensors-25-01222" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01222" ref-type="bibr">8</xref>], intelligent tutoring systems and education [<xref rid="B9-sensors-25-01222" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01222" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01222" ref-type="bibr">11</xref>]. Therefore, extensive research is required in the field of affective computing to design and build user-friendly, effective, intelligent and reliable emotion identification systems. These emotion identification systems should ensure accuracy, robustness and adaptability to real-time applications. More than 90 definitions of human emotion [<xref rid="B12-sensors-25-01222" ref-type="bibr">12</xref>] have been reported in the literature, and emotions are classified into primary and secondary emotions [<xref rid="B13-sensors-25-01222" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01222" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01222" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-01222" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01222" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01222" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-01222" ref-type="bibr">19</xref>]. Significant studies of emotion recognition have focused on primary emotions. However, few researchers have paid attention to secondary complex emotions such as guilt, terror, or confusion. These secondary emotions are understudied compared to the primary ones.</p><p>The analysis of human emotion identification systems has gained remarkable importance in recent years [<xref rid="B20-sensors-25-01222" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-01222" ref-type="bibr">21</xref>]. Researchers have explored various dimensions of human&#x02013;machine interaction to address potential challenges in communication with hominids. Researchers have tried to enable computers to handle the affective needs of human beings by observing different modalities. These modalities include human facial expressions [<xref rid="B9-sensors-25-01222" ref-type="bibr">9</xref>,<xref rid="B22-sensors-25-01222" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-01222" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-01222" ref-type="bibr">24</xref>], handwriting style [<xref rid="B25-sensors-25-01222" ref-type="bibr">25</xref>], associated physiological signals or brain activities [<xref rid="B26-sensors-25-01222" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01222" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-01222" ref-type="bibr">28</xref>], gesture recognition [<xref rid="B29-sensors-25-01222" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-01222" ref-type="bibr">30</xref>] and human speech [<xref rid="B31-sensors-25-01222" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-01222" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-01222" ref-type="bibr">33</xref>]. Nonetheless, these factors may be unreal, posed, disguised or camouflaged, easily excluding physiological signals or brain activities. Lindquist et al. [<xref rid="B34-sensors-25-01222" ref-type="bibr">34</xref>] suggest that the human brain plays a pivotal role in the human emotional paradigm. EEG signals are titled as a &#x0201c;window on the mind&#x0201d; [<xref rid="B35-sensors-25-01222" ref-type="bibr">35</xref>] due to their genuine nature as they cannot be falsified. This supports the application of brain&#x02013;computer interaction technology to enhance emotion-aware computing. In recent years, brain signal acquisition devices have attracted researchers, such as EEG signals, Galvanic Skin Response (GSR) and many others. EEG is one of the most prominent and reliable modalities to enhance affective computing studies [<xref rid="B22-sensors-25-01222" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-01222" ref-type="bibr">23</xref>,<xref rid="B36-sensors-25-01222" ref-type="bibr">36</xref>].</p><p>The proposed research investigates the EEG signals linked to humans&#x02019; guilt state and explores the impact of gender differences on the expression of guilt and neutral emotions in response to depicted stimuli. As per our knowledge, EEG data related to guilt and its gender differences have never been explored or analyzed before this contribution. The proposed methodology contributes to the fields of human emotions, behavior and human&#x02013;machine interaction by highlighting insights of a secondary emotion [<xref rid="B18-sensors-25-01222" ref-type="bibr">18</xref>] and the physiological signals linked to it. Guilt is a secondary emotion as shown in <xref rid="sensors-25-01222-f001" ref-type="fig">Figure 1</xref>, initiated by transgressions [<xref rid="B37-sensors-25-01222" ref-type="bibr">37</xref>] and still needs attention from scholars despite its high involvement in psychiatric disorders.</p><p>Guilt plays an important role in self-improvement and strengthens multiple relationship-enhancing factors. It is also critical for improving interpersonal relationships. People experiencing guilt often isolate themselves from their community, which may result in society losing their significant contributions. If a machine automatically detects a person feeling guilty, it may aim to boost their morale and support self-improvement. Guilt is directly linked with feelings of depression and anxiety. Machines&#x02019; ability to identify guilt-related EEG patterns may aid an individual&#x02019;s well-being. Guilt shapes the future behavior of a person; thus, a machine that detects guilt-related EEG signals may provide better responses to individuals.</p><p>This study provides a state-of-the-art contribution to emotion-aware computing specifically toward intelligent tutorial systems and clinical psychology. The remaining sections of the paper are presented as follows. <xref rid="sec2-sensors-25-01222" ref-type="sec">Section 2</xref> presents a description of existing emotion recognition systems. <xref rid="sec3-sensors-25-01222" ref-type="sec">Section 3</xref> demonstrates the proposed emotion identification system. <xref rid="sec4-sensors-25-01222" ref-type="sec">Section 4</xref> shows the performance and evaluation of the proposed methodology. Finally, in <xref rid="sec5-sensors-25-01222" ref-type="sec">Section 5</xref>, the conclusion and future research directions are given.</p></sec><sec id="sec2-sensors-25-01222"><title>2. Literature Review</title><p>Prior research advocates for using numerous machine learning algorithms for emotion identification from EEG signals. This section evaluates existing AI methods for recognizing emotions from EEG physiological signals. Gao et al. [<xref rid="B38-sensors-25-01222" ref-type="bibr">38</xref>] employed the Phase Locking Value (PLV) method and developed the brain functional adjacency matrix of the EEG brain signal of each subject. For the decomposition of signals, the tucker decomposition technique was applied. Differential entropy and energy features were combined with dynamic brain functional connections and fed to the SVM classifier. The proposed system was validated on the ERN and DEAP data sets. Goshvarpour et al. [<xref rid="B39-sensors-25-01222" ref-type="bibr">39</xref>] validated the proposed system on the DEAP data set. The proposed system fed cross-information potential (CIP) between two electrodes to SVM and kNN classifiers. kNN outperformed the SVM classifier with a maximum frequency of 90%, and the Gamma frequency band played a pivotal role in low&#x02013;high valance and arousal classification. Liu et al. [<xref rid="B40-sensors-25-01222" ref-type="bibr">40</xref>] demonstrated an EEG-based emotion identification technique on a DEAP data set using a pre-trained convolution capsule network developed on an attention mechanism. The EEG features were extracted with pre-trained Mobile Net from pre-processed EEG data. The proposed methodology was evaluated in two dimensions: subject-dependent and subject-independent. This algorithm identified the valence and arousal of the data set. Tuncer et al. [<xref rid="B41-sensors-25-01222" ref-type="bibr">41</xref>] applied a fractal pattern approach and Q-factor Wavelet Transform (TQWT) for feature extraction and signal decomposition, respectively. The calculated features were tested on the GAEMO EEG data set using Linear Discriminant Analysis (LDA), K-Nearest Neighborhood (k-NN) and support vector machine (SVM). The results presented that SVM outperformed the LDA and KNN. Li et al. [<xref rid="B42-sensors-25-01222" ref-type="bibr">42</xref>] developed EEG-based emotion recognition by first extracting meaningful information from EEG data by learning them simultaneously through MIL (multi-task learning) and then usin a capsule network combined with an attention mechanism to identify arousal, valence and dominance in the DEAP and DREAMER data sets. Joshi et al. [<xref rid="B43-sensors-25-01222" ref-type="bibr">43</xref>] extracted Power Spectral Density, Hjorth parameters, differential entropy and linear formulation of differential entropy from DEAP, SEED and the collected data set. The extracted features were then fed to a deep RNN model based on a bidirectional long short-term memory (BiLSTM) network, Multilayer perceptron (MLP), K-Nearest Neighbors (k-NN) and support vector machine (SVM). As per the evaluation, optimized BiLSTM performed the best among all the classifiers. Gao et al. [<xref rid="B44-sensors-25-01222" ref-type="bibr">44</xref>] calculated both time-domain features listed as Hjorth, differential and sample entropy, and frequency-related features as Power Spectral Density. These extracted features were fused with fully connected layers and the SVM classifier was applied to classify the EEG data. The proposed algorithm was evaluated on a publically available data set, DEAP, achieving accuracies of 80.52% for valence and 75.22% for arousal. Yuvaraj et al. [<xref rid="B45-sensors-25-01222" ref-type="bibr">45</xref>] tested two classifiers, a support vector machine (SVM) and a Classification and Regression Tree (CART), on five publicly available data sets&#x02014;MAHNOB-HCI, DEAP, SEED, AMIGOS and DREAMER. The result showed that the FD-CART feature classification method outperformed the SVM classifier. Subasi et al. [<xref rid="B46-sensors-25-01222" ref-type="bibr">46</xref>] applied Tunable-Q Wavelet Transform (TQWT) to find the required features from a publically available data set (SEED). Data were de-noised by applying a Symlets-4 filter. In the classification phase, the rotation forest ensemble (RFE) classifier was utilized with multiple algorithms such as k-NN, SVM, ANN and many more. The result demonstrated that the combination of REF and SVM classifiers performed the best.</p></sec><sec sec-type="methods" id="sec3-sensors-25-01222"><title>3. Proposed Methodology</title><p>The proposed methodology separates guilt and neutral emotion EEG data. The emotions can be induced among participants by using different emotion induction techniques. Due to the unavailability of stimuli for guilt and neutral emotion induction, guilt stimuli images were prepared in the form of storyboards. The design and development of these stimuli images were based on a literature review of Bhushan et al. [<xref rid="B47-sensors-25-01222" ref-type="bibr">47</xref>] study, that developed comic sketches after collecting stories from participants.</p><sec id="sec3dot1-sensors-25-01222"><title>3.1. Stimuli Set Construction for Target Emotion Induction</title><p>For Stimuli Set Construction, storylines were developed with the help of 52 volunteer participants. The age group of participants was 19&#x02013;22 (mean = 20.5 years, SD = 1.6). They were Pakistanis from middle-class backgrounds. All the participants were healthy and university students. Participants were required to fill out a consent form that was reviewed and accepted by the Ethical Committee of NED University of Engineering and Technology. The block diagram illustrating the Stimuli Set Construction phase, along with the Data Acquisition phase, is presented <xref rid="sensors-25-01222-f002" ref-type="fig">Figure 2</xref>.</p><sec id="sec3dot1dot1-sensors-25-01222"><title>3.1.1. Phase I: Development of Storyline</title><p>Before the storyline development phase, participants were guided about guilt and neutral emotions in detail. The difference between guilt and shame was also discussed in detail. Additionally, they were also guided about feelings of regret and remorse, as remorse is very much connected to guilt. This paper follows the definition of guilt as &#x0201c;An emotion which compels the wrongdoer to improve their actions in future and repair their crashed relations&#x0201d; [<xref rid="B48-sensors-25-01222" ref-type="bibr">48</xref>,<xref rid="B49-sensors-25-01222" ref-type="bibr">49</xref>]. This is in contrast to shame, where a person tries to withdraw from social contacts after wrong deeds [<xref rid="B48-sensors-25-01222" ref-type="bibr">48</xref>,<xref rid="B50-sensors-25-01222" ref-type="bibr">50</xref>,<xref rid="B51-sensors-25-01222" ref-type="bibr">51</xref>,<xref rid="B52-sensors-25-01222" ref-type="bibr">52</xref>]. With shame, a person considers themselves a bad person. The link between guilt, shame, remorse and regret is illustrated in <xref rid="sensors-25-01222-f003" ref-type="fig">Figure 3</xref>.</p><p>After answering all their queries, some sample stories taken from the studies [<xref rid="B47-sensors-25-01222" ref-type="bibr">47</xref>,<xref rid="B53-sensors-25-01222" ref-type="bibr">53</xref>,<xref rid="B54-sensors-25-01222" ref-type="bibr">54</xref>] were shared with them. This was to give them a basic idea of the type of stories they could share with us. These instructions were explained to the participants together; however, the story-sharing process was conducted separately with a group of 15 participants at a time. During the story-writing phase, participants were seated at a distance from one another. The story collection process was completed in a single day.</p><p>Participants were asked to recall an event or decision that might ignite feelings of guilt in them. They were guided to write about this in as much detail as they felt comfortable. This phase provided a total set 52 guilt-related events shared by participants.</p></sec><sec id="sec3dot1dot2-sensors-25-01222"><title>3.1.2. Phase II: Selection of Events for Storyboard Development</title><p>The shared events were evaluated based on the probability of inducing the target emotion and the frequency of occurrence, so another person can relate to it easily; it was ensured that selected events did not reveal anything personal about the storyteller. These measurement criteria were related to the instructions given to the participants earlier before the writing events. At the end of this phase, almost 10 storylines were selected for further development (see <xref rid="sensors-25-01222-t001" ref-type="table">Table 1</xref>).</p></sec><sec id="sec3dot1dot3-sensors-25-01222"><title>3.1.3. Phase III: Storyboard Construction for Guilt and Neutral Emotion Induction</title><p>After the selection of storylines, 10 story boards were developed based on the guilt-provoking events. These events were shared by the participants in the previous &#x0201c;Development of Storyline Phase&#x0201d;. These 10 storyboards were developed using CANVA software available online for graphics designing. Developed storyboards (See <xref rid="sensors-25-01222-f004" ref-type="fig">Figure 4</xref>) were shown to participants during the data collection phase to induce guilt and neutral emotions among participants.</p><p>As per prior research, neutral emotion means the absence of all emotion. This is when a person feels no emotion, neither happy nor sad. In accordance with past studies, guilt has been widely explored with neutral or control conditions as suggested in [<xref rid="B26-sensors-25-01222" ref-type="bibr">26</xref>,<xref rid="B48-sensors-25-01222" ref-type="bibr">48</xref>,<xref rid="B55-sensors-25-01222" ref-type="bibr">55</xref>,<xref rid="B56-sensors-25-01222" ref-type="bibr">56</xref>,<xref rid="B57-sensors-25-01222" ref-type="bibr">57</xref>,<xref rid="B58-sensors-25-01222" ref-type="bibr">58</xref>,<xref rid="B59-sensors-25-01222" ref-type="bibr">59</xref>]. Therefore, 10 events were selected for storyboard development for neutral emotion induction. A few of these events were selected from prior research [<xref rid="B55-sensors-25-01222" ref-type="bibr">55</xref>,<xref rid="B56-sensors-25-01222" ref-type="bibr">56</xref>,<xref rid="B60-sensors-25-01222" ref-type="bibr">60</xref>], with a little description added that was required to prepare a complete story. A few events were based on daily routine life experiences. These events did not produce any positive or negative affect on the perceiver, e.g., going for shopping or preparing supper. Afterward, 10 storyboards were developed based on neutral events as shown in <xref rid="sensors-25-01222-t002" ref-type="table">Table 2</xref> in a similar way as they had been created earlier for guilt emotion induction.</p></sec></sec><sec id="sec3dot2-sensors-25-01222"><title>3.2. Data Acquisition</title><p>The experiment was carried out in a controlled environment with low light and a noise-free environment. The workflow of the proposed methodology from Stimuli Set building to data acquisition is shown in <xref rid="sensors-25-01222-f002" ref-type="fig">Figure 2</xref>.</p><sec id="sec3dot2dot1-sensors-25-01222"><title>3.2.1. Subject Details</title><p>There were a total of 16 participants, 8 females and 8 males, in the 18 to 24 age group, who participated in the study. Alarcao et al. [<xref rid="B36-sensors-25-01222" ref-type="bibr">36</xref>] recommends that the median for the number of participants is 15 after exploring almost one hundred papers of EEG emotion recognition systems. In light of this finding and to ensure equal representation of both genders, the number of participants was set to 16. All were native healthy Pakistanis from middle-class backgrounds. They were required to fill out the consent form, with the ratification of the Ethical Committee of NED University of Engineering and Tech. The data of one participant were collected at a time. Only two participants were engaged in the data collection process on a single day. For better data acquisition, participants were requested to wash their hair before the data collection phases.</p></sec><sec id="sec3dot2dot2-sensors-25-01222"><title>3.2.2. Experiment Instructions</title><p>Subjects were given detailed guidance about guilt and neutral emotions. A similar explanation was provided to another group of participants during the storyline development phase. They were briefly guided about EEG equipment. Some sample storyboards were shown to them to help them feel at ease. Afterward, they were instructed regarding EEG recording as follows:<list list-type="bullet"><list-item><p>To remain mentally and physically relaxed.</p></list-item><list-item><p>To avoid blinking their eyes.</p></list-item><list-item><p>To try to keep their eyes open.</p></list-item></list></p></sec><sec id="sec3dot2dot3-sensors-25-01222"><title>3.2.3. Experimental Protocol</title><p>The step-by-step experiment procedure is written below:<list list-type="bullet"><list-item><p>First of all, a slide was exposed to the user for 5 s as a reminder message about performing fewer physical and eye movements.</p></list-item><list-item><p>Afterwards, a baseline EEG was recorded for 65 s in which the first 30 s participants kept their eyes open. For the next 30 s, participants were prompted to keep their eyes closed. Between the eyes-open and eyes-closed sessions, a &#x0201c;+&#x0201d; sign was displayed for 5 s.</p></list-item><list-item><p>On the next slide, a countdown timer from 5 to 1 s with a decrement of 1 s with each count was displayed. The timer was applied to boost the alertness level of the subject.</p></list-item><list-item><p>Finally, the storyboards were displayed on the screen with a duration of 45 s for each storyboard. The first three seconds title of the story were shown with an instruction for viewers to immerse themselves in the mentioned character. After 3 s, a story with three to four scenes was depicted on screen for 42 s as shown in <xref rid="sensors-25-01222-f004" ref-type="fig">Figure 4</xref>.</p></list-item><list-item><p>There were a total of 20 trials of storyboard depiction, with 10 storyboards for guilt emotion induction and 10 storyboards for neutral emotion induction. These storyboards were displayed in random orders using the randperm function of Matlab.</p></list-item><list-item><p>After each trial, participants were asked to report the emotions they felt while looking at the depicted storyboards. As a result, collected data could be labeled as guilt, neutral and other, as illustrated in <xref rid="sensors-25-01222-f005" ref-type="fig">Figure 5</xref>. All emotion induction procedures include an emotion assessment feedback questionnaire to label emotions. This is due to the probability that the target emotion and similar situations may cause several emotions that vary from person to person [<xref rid="B26-sensors-25-01222" ref-type="bibr">26</xref>,<xref rid="B61-sensors-25-01222" ref-type="bibr">61</xref>]. SAM (Self-Assessment Manikin system) [<xref rid="B62-sensors-25-01222" ref-type="bibr">62</xref>] is widely used in all emotion induction methodologies.</p></list-item><list-item><p>In the end, after completing 20 trials, participants were again prompted for baseline recording for 30 s with their eyes closed and next 30 s with open eyes.</p></list-item></list></p></sec></sec></sec><sec id="sec4-sensors-25-01222"><title>4. Data Processing</title><sec id="sec4dot1-sensors-25-01222"><title>4.1. Data Sampling</title><p>In total, 16 participants were employed and each participant had twenty trials. In each trial, a stimulus storyboard was shown on screen for 45 s. There were 3 s for the title of the story and 42 s for the actual story board. Therefore, the total number of samples for each participant is as follows:
<disp-formula><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>42</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mo>(</mml:mo><mml:mi>story</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>depiction</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>time</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>in</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>sec</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>20</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mo>(</mml:mo><mml:mi>num</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>trials</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>16</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mo>(</mml:mo><mml:mi>num</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>channels</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
The emotion assessment phase had three classes of emotion: guilt, control and none of the above. These categories were based on the participants&#x02019; selection of their felt emotion. After selecting the guilt and control classes only, total samples of both classes were:
<disp-formula><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>146</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi>samples</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>guilt</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>86</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi>samples</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>control</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where the dimension of each sample is 5209 by 16 as data are collected from 16 channel devices for 42 s with a frequency of 124 Hz,
<disp-formula><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>where</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mn>42</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>124</mml:mn><mml:mo>=</mml:mo><mml:mn>5209</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
</p></sec><sec id="sec4dot2-sensors-25-01222"><title>4.2. Data Pre-Processing and Segmentation</title><p>The recorded EEG data always have a high probability of being contaminated with unwanted data from numerous sources [<xref rid="B63-sensors-25-01222" ref-type="bibr">63</xref>,<xref rid="B64-sensors-25-01222" ref-type="bibr">64</xref>]. It is a challenging task to differentiate between artifacts and the original brain. Thus, proper data cleaning steps must be applied. At the first pre-processing stage, a minimum-order low-pass FIR filter was designed using the Kaiser Window method. The filter had a passband frequency of 0.15 <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mo>&#x003a0;</mml:mo></mml:mrow></mml:math></inline-formula> rad/sample and a stopband frequency of 0.45 <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mo>&#x003a0;</mml:mo></mml:mrow></mml:math></inline-formula> rad/sample. The passband ripple was set at 0.15, and the stopband attenuation was 65. Furthermore, the ICA technique was also applied to remove the eye artifacts, which are the primary noise source from recorded EEG data. ICA provides promising results when data sources are independent [<xref rid="B65-sensors-25-01222" ref-type="bibr">65</xref>]. The major objective of ICA is to reduce the statistical dependence of the components on each other [<xref rid="B66-sensors-25-01222" ref-type="bibr">66</xref>,<xref rid="B67-sensors-25-01222" ref-type="bibr">67</xref>].</p><p>The filtered and ICA-pruned data set was then split into equal segments of a length of six seconds. Candra et al. [<xref rid="B68-sensors-25-01222" ref-type="bibr">68</xref>] evaluated the emotion classification of EEG data using different time window lengths. The findings showed that time window lengths between 3 s to 12 s performed better than other window lengths. Before segmentation, the dimension of each sample was 5209 by 16; the last row of the data set was ignored to avoid complex calculation. A total of 86 control samples of dimensions 5208 by 16 were segmented. After dividing each sample into six segments of equal length, the total number of control samples was
<disp-formula><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>86</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>6</mml:mn><mml:mo>=</mml:mo><mml:mn>512</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi>samples</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>dimension</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mn>868</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi>by</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mn>16</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where 512 is the number of samples, 868 is the number of rows and 16 is the number of channels. Similarly, in the case of the guilt class, after dividing each of the 146 samples into six segments of equal length 868 by 16, the total number of guilt samples was
<disp-formula><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>146</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>6</mml:mn><mml:mo>=</mml:mo><mml:mn>876</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi>samples</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>dimension</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mn>868</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi>by</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mn>16</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec4dot3-sensors-25-01222"><title>4.3. Training Model</title><p>To train the model, data were fed to train the SVM classifier in two different approaches, Data Engineering and Feature Computation, as shown in <xref rid="sensors-25-01222-f006" ref-type="fig">Figure 6</xref>.</p><sec id="sec4dot3dot1-sensors-25-01222"><title>4.3.1. First Approach&#x02014;Data Engineering</title><p>In the first approach, filtered, ICA-pruned and segmented data were loaded into the SVM classifier without extracting any features from it. This is a novel approach by using Data Engineering in an emotion classification system using machine learning. It treated chunks of data rows as feature sets themselves. This technique did not empower any single value feature to represent a large data set. In this model, all samples of EEG data linked to guilt emotions are called &#x0201c;Guilt samples (GSs)&#x0201d;. All samples of neutral or control emotions are called &#x0201c;Control Samples (CSs)&#x0201d;. The dimension of each sample is 868 by 16. Random samples from each emotion class were selected. These selected samples were then concatenated, forming data sets consisting of concatenated rows for each emotion class. After horizontally concatenating the selected samples, the rows for the guilt emotion class were combined and represented as &#x0201c;GAHC&#x0201d;. After horizontally concatenating the selected samples, the rows for the control emotion class were combined and represented as &#x0201c;CAHC&#x0201d;. The dimensions GAHC and CAHC depend upon the number of samples randomly selected as &#x0201c;R &#x000d7; 868 by 16&#x0201d;, where R represents the number of randomly selected samples. Overview of prepared data sets with their dimensions in <xref rid="sensors-25-01222-t003" ref-type="table">Table 3</xref>.</p><p><bold>Classification to Recognize Emotions:</bold> The developed GAHC and CAHC were then fed to the SVM classifier and trained. SVM is an extensively used supervised learning algorithm for emotion classification purposes [<xref rid="B38-sensors-25-01222" ref-type="bibr">38</xref>,<xref rid="B69-sensors-25-01222" ref-type="bibr">69</xref>,<xref rid="B70-sensors-25-01222" ref-type="bibr">70</xref>]. SVM changes input data onto a higher dimensional feature space using the kernel transfer function; thus, the data become easy to separate in contrast to the original feature space. Here, the Radial Basis Function kernel function was utilized to map data onto a higher dimensional space. After training and learning the SVM classifier, a 5-fold technique was utilized. Prior research witnessed numerous values of &#x0201c;k&#x0201d; ranging from 2 to 10 in emotion classification systems. However, the proposed methodology chose k = 5, which has been applied in past research for emotion identification [<xref rid="B71-sensors-25-01222" ref-type="bibr">71</xref>]. Moreover, Kusumaningrum et al. [<xref rid="B72-sensors-25-01222" ref-type="bibr">72</xref>] demonstrated that 5-fold provided the best accuracy with multiple classifiers.</p></sec><sec id="sec4dot3dot2-sensors-25-01222"><title>4.3.2. Second Approach&#x02014;Feature Computation</title><p>This traditional approach involved first applying wavelet decomposition to EEG signals. Features were then calculated from the detail coefficients and used as input for the classifier.</p><p><bold>Discrete Wavelet Transform:</bold> At first, segmented data were decomposed into different frequency bands using Discrete Wavelet Transform (DWT). DWT serves as a reliable tool to analyze time-frequency domain information. DWT outperforms continuous wavelet transform in cleaning the redundant data [<xref rid="B73-sensors-25-01222" ref-type="bibr">73</xref>]. DWT was utilized in several prior EEG emotion recognition systems [<xref rid="B74-sensors-25-01222" ref-type="bibr">74</xref>,<xref rid="B75-sensors-25-01222" ref-type="bibr">75</xref>]. The mathematical representation of DWT is expressed as:<disp-formula id="FD1-sensors-25-01222"><label>(1)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo>&#x0221e;</mml:mo></mml:mrow><mml:mo>&#x0221e;</mml:mo></mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:msup><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:msup></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo>&#x003a8;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>b</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:msup></mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:msup></mml:mfrac></mml:mstyle></mml:mfenced><mml:mi mathvariant="italic">dy</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mo>&#x003a8;</mml:mo></mml:mrow></mml:math></inline-formula> is the mother wavelet, <italic toggle="yes">a</italic> and <italic toggle="yes">b</italic> are the scaling and shifting factors, respectively, and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the Discrete Wavelet Transform (DWT) of <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>In this study, db3 was employed as the mother wavelet. The decomposition level was set to 6 to calculate detail coefficients at each level and the approximate coefficient with a sampling frequency is 124 Hz.</p><p><bold>Feature Extraction:</bold> The feature extraction process represents raw data in numeral forms while preserving the meaning full information of actual data. This numerical information is further used to classify the data. As this is the first ever reported study on EEG signals and guilt emotion, the proposed methodology explores a wide range of EEG features to identify links between guilt and EEG signals. For the proposed methodology, selected features are written below.</p><p>
<bold>1. Entropy:</bold>
</p><p>Entropy is a measure of energy distribution for each band as shown below:<disp-formula id="FD2-sensors-25-01222"><label>(2)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">Entropy</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi mathvariant="italic">pilog</mml:mi><mml:mn>10</mml:mn><mml:mi mathvariant="italic">pi</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where pi is the probability of state i.</p><p>
<bold>2. Band Power:</bold>
</p><p>The Band Power value of signals was employed to classify the data, consistent with its application in prior research [<xref rid="B76-sensors-25-01222" ref-type="bibr">76</xref>]. The Band Power (BP) is calculated as the addition of squares of total time domain samples divided by the total signal length.</p><p>
<bold>3. Hjorth Activity:</bold>
</p><p>Hjorth parameters were extracted from the segmented and pruned data set, following their common extraction in previous EEG-based emotion recognition systems [<xref rid="B1-sensors-25-01222" ref-type="bibr">1</xref>,<xref rid="B77-sensors-25-01222" ref-type="bibr">77</xref>]. Hjorth Activity is the squared standard deviation of the signal amplitude, represents variations in the signal power and is calculated as shown below:<disp-formula id="FD3-sensors-25-01222"><label>(3)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">HjorthActivity</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mover><mml:mi>a</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>
<bold>4. Hjorth Complexity:</bold>
</p><p>This feature represents deviation in the signal frequency as shown below:<disp-formula id="FD4-sensors-25-01222"><label>(4)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">HjorthMobility</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Mobility</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Mobility</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>
<bold>5. Hjorth Mobility:</bold>
</p><p>This finds out the deviation in the power spectrum, as given below:<disp-formula id="FD5-sensors-25-01222"><label>(5)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">HjorthMobility</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">var</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">var</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, a&#x02019; is a derivate of signal a.</p><p><bold>Classification to Recognize Emotions:</bold> All features were calculated separately for each frequency band, which were Alpha, Beta, Theta, Gamma and Delta. The calculated features were fed into the SVM classifier. The SVM classifier utilized the RBF kernel. The RBF kernel is much more efficient in separating non-linear relationships present in the data. The data set was input to the classifier using the k-fold technique. The k-fold technique splits data into a subset of approximately equal size. In each fold, one subset is used as the testing data set and the remaining four subsets serve as training sets. This classification process was repeated five times, taking each subset as the testing set only once.</p></sec></sec><sec id="sec4dot4-sensors-25-01222"><title>4.4. Classification Using Deep Learning Approach</title><p><bold>Data Pre-Processing and Segmentation:</bold> In the first step, a bandpass filter was applied on the data to eliminate noise. Low- and high-frequency artifacts were removed using a band bass filter of range 0.5 Hz and 50 Hz. Further data were normalized to zero mean and unit variance as the samples size was very small to be fed to deep learning. To make the data set sufficient for deep learning, EEG signals were segmented into smaller windows using a sliding window technique with a window length of 2 s, providing 248 samples per window given the 124 Hz sampling rate. The windows overlapped by 50%, in which each consecutive window shared 1 s of data with the previous window. This overlap provides more training samples from the original data. This enhances the model&#x02019;s ability to generalize by providing it large numbers of samples.</p><p><bold>Model Architecture:</bold> A 2D Convolutional Neural Network (CNN) was utilized for the classification. CNNs are efficient in learning both spatial and temporal patterns of EEG signals. The data were first converted into a 2D data model&#x02019;s input shape. The input shape was defined as n channels, window size, 1, where n channels is the number of EEG channels, window size represents the number of samples per window and the depth is 1 as it is a single time series per window. To prevent overfitting, L1_L2 regularization was applied to the model&#x02019;s weights. Moreover, to randomly eliminate half of the neurons, a 50% dropout rate was applied to further minimize the probability of overfitting. The model&#x02019;s output layer utilized a softmax activation function that converted the network&#x02019;s predictions into probabilities, enabling it to perform a binary classification task with two output classes (guilt and neutral).</p><p><bold>lModel Compilation:</bold> Adam optimizer was utilized to compile the model with a loss function of sparse categorical cross entropy. The efficiency of the model is evaluated through the acquired accuracy to analyze the performance of model.</p></sec></sec><sec id="sec5-sensors-25-01222"><title>5. Result and Discussion</title><sec id="sec5dot1-sensors-25-01222"><title>5.1. Classifier Performance&#x02014;Data Engineering Approach</title><p>This methodology employed an engineered data set as a feature set and fed it to the SVM classifier. Several trials with different samples were performed to validate the model. For each trial, different sets of samples were randomly selected and concatenated. The findings of the initial trial are written below:</p><p><bold>Initial trial:</bold> At first, a total of 16 random samples were selected from GAHC and CAHC, using the Matlab rand function. The number of samples selected was 16 because of the limited processing capacity of the hardware used for data processing. The selected samples for both CAHC and GAHC are shown in <xref rid="sensors-25-01222-t004" ref-type="table">Table 4</xref>.</p><p>The selected samples were referred to as the data set after concatenation. These data sets were then fed into the SVM classifier. It provided promising accuracy in separating data of two separate classes. SVM employed a 5-fold method for this classification problem.</p><p><xref rid="sensors-25-01222-t005" ref-type="table">Table 5</xref> and the ROC curve in <xref rid="sensors-25-01222-f007" ref-type="fig">Figure 7</xref>, highlight performance assessments of the first trial. In the ROC curve, false positive (x-axis) defines the proportion of neutral emotions incorrectly identified as guilt. True Positive describes the proportion of guilt emotions correctly identified as guilt.</p></sec><sec id="sec5dot2-sensors-25-01222"><title>5.2. Classifier Performance of Multiple Trials&#x02014;Data Engineering Approach:</title><p>Multiple classification trials using 16 randomly selected samples were performed using the classifier. It provided an average accuracy of 83% across four trials. The performance of four trials is described in <xref rid="sensors-25-01222-f008" ref-type="fig">Figure 8</xref>.</p><p>The accuracy value indicates correctly classified instances and F1 Score describes a balance between precision and recall. Precision defines the values identified as positive and which are actually positive. Additionally, recall indicates the proportion of actual positive values identified correctly by the classifier. This metric presents a comprehensive view of the algorithm&#x02019;s efficiency in multiple aspects.</p><p>The classifier&#x02019;s results remained consistent across all four trials when employed using the provided data set. The algorithm efficiently and reliably classified data into two distinct emotional classes: guilt and neutral. These stable results indicate the classifier&#x02019;s robustness and ability to maintain accuracy despite variations across multiple experimental trials.</p></sec><sec id="sec5dot3-sensors-25-01222"><title>5.3. Classifier Performance of Gender-Specific Data Engineering Approach</title><p>The proposed methodology evaluated the effect of stimuli and compared the differences in EEG data signals of male and female participants separately. Numerous studies have explored gender-related differences in emotion identification systems [<xref rid="B78-sensors-25-01222" ref-type="bibr">78</xref>,<xref rid="B79-sensors-25-01222" ref-type="bibr">79</xref>,<xref rid="B80-sensors-25-01222" ref-type="bibr">80</xref>]. A total of 16 participants were employed in this study, 8 males and 8 females. A similar data processing and data-engineering approach was applied to both groups separately. The classification results provided an average accuracy of 88% for female participants and 80% for male participants, indicating a difference in classifier performance between genders. The cause of this difference may be subject to the more sensitive nature of the females surveyed. The Receiver Operating Characteristic (ROC) curves in <xref rid="sensors-25-01222-f009" ref-type="fig">Figure 9</xref> presents the classification findings for both groups, highlighting a visual representation of the classifier&#x02019;s discriminative ability across both groups. In the ROC curve, false positive (x-axis) defines the proportion of neutral emotions incorrectly identified as guilt. True Positive describes the proportion of guilt emotions correctly identified as guilt. Aligning with prior research, females display greater expressiveness in response to emotional stimuli.</p></sec><sec id="sec5dot4-sensors-25-01222"><title>5.4. Classifier Performance&#x02014;Feature Extraction Approach</title><p>The second processing approach, feature extraction, provided a reasonable classification accuracy. The results show the model&#x02019;s accuracy under two different feature fusion strategies: first, when features were calculated separately for each frequency band, and second, when extracted features from all frequency bands were combined into a single feature set. This traditional approach was evaluated using four performance metrics and is shown in <xref rid="sensors-25-01222-t006" ref-type="table">Table 6</xref>. It achieved an average accuracy of 63%; precision was measured at 65%. The model&#x02019;s recall reached 83% and F1 was calculated as 72%. This score indicates that the model maintained a good balance, favoring high recall while managing reasonable precision. The summary of classification accuracy achieved for each feature type (Hjorth parameters, Band Power, entropy and their combination) across different EEG frequency bands (Gamma, Beta, Alpha, Theta, Delta and All) is presented in <xref rid="sensors-25-01222-t007" ref-type="table">Table 7</xref>. Hjorth Complexity, Mobility and entropy present a consistent accuracy of 62.9%, while Hjorth Activity and Band Power display more variability. Combining all features provides slightly better and more stable accuracy, demonstrating their complementary strengths.</p></sec><sec sec-type="subjects" id="sec5dot5-sensors-25-01222"><title>5.5. Classifier Performance of Male and Female Participants&#x02014;Feature Extraction Approach</title><p>When a similar approach was applied to the data of male participants and female participants, the findings demonstrated a notable variation in classification accuracy between the two groups. The method achieved an average accuracy of 60% for male participants and 68% for female participants. This difference was observed; however, the experimental procedure was identical for all participants. These findings are similar to prior studies [<xref rid="B81-sensors-25-01222" ref-type="bibr">81</xref>,<xref rid="B82-sensors-25-01222" ref-type="bibr">82</xref>] that females are more expressive and sensitive than males.</p><p>When the accuracies of both genders were compared for each band separately, it was concluded that female participants showed higher accuracies across all bands, except the Delta bandas shown in <xref rid="sensors-25-01222-t008" ref-type="table">Table 8</xref>. This displays a distinct brain activity between males and females, specifically in lower frequency bands. Females&#x02019; brains might behave differently in ways that impacts Delta band activity, particularly in the case of complex emotions like guilt.</p><p>This suggests that female participants were more engaged and involved in the experiment than male participants, thus providing better accuracy.</p></sec><sec id="sec5dot6-sensors-25-01222"><title>5.6. Insights into Accuracy Across Data Engineering and Feature Extraction Approaches</title><p>A comprehensive overview of accuracies acquired using both data training approaches is presented in <xref rid="sensors-25-01222-f010" ref-type="fig">Figure 10</xref>. These two approaches include the data engineering approach and feature extraction approach. The accuracies are highlighted on different subsets of data categorized as male and female participants. The respective strengths and weaknesses of data training in handling specific subsets of the data are highlighted in this figure given below. This comparison provides insights into the effectiveness of both approaches under different conditions and participant characteristics.</p><p>The findings of the emotion classification system demonstrate that the classifier provides better accuracy with an engineered data set, bypassing the feature extraction process from EEG data. The probable reasons behind the accuracy difference are as follows:<list list-type="bullet"><list-item><p>A single value or a combination of a few values of calculated features represents a whole large data set of at least a 2 by 2 matrix or may be of more dimensions.</p></list-item><list-item><p>These features may fail to preserve valuable information present in collected data and be unable to provide improved accuracy and results. As Rani et al. [<xref rid="B83-sensors-25-01222" ref-type="bibr">83</xref>] discussed, feature extraction is a core challenge in affective computing.</p></list-item><list-item><p>Due to phenomena of personal stereotypes where a system may rely on generalized assumptions about individuals [<xref rid="B84-sensors-25-01222" ref-type="bibr">84</xref>], it is hard to extract features that perform efficiently over a large number of subjects. There is a probability that extracted features are unable to represent critical data points and information hidden in the EEG data set.</p></list-item><list-item><p>Human physiological signals are complex with unidentified ranges, making it difficult to predict or standardize the data.</p></list-item><list-item><p>The high probability of variation across several parameters further complicates the analysis, making it more challenging that few features can cover that variability.</p></list-item><list-item><p>Decomposed frequency bands represent specific brain activities, and focusing only on these bands may lose critical information present outside these frequencies.</p></list-item></list></p><p>The aforementioned reasons could contribute to the significant improvement of 20% in classification accuracy that was achieved using the first approach&#x02014;Data Engineering&#x02014;where engineered data chunks were provided to the classifier.</p></sec><sec id="sec5dot7-sensors-25-01222"><title>5.7. Classifier Performance via CNN</title><p>To ensure the model&#x02019;s robustness and prevent overfitting, k-fold cross-validation was utilized. K-fold splits the data set into five folds. In each fold, 80% of the data was used for training, and 20% was used for validation. This approach was repeated for all five folds, providing a reliable measure of model performance across different subsets of the data. The average accuracy achieved was 63%. There was no significant difference in accuracy achieved from deep learning.</p></sec><sec id="sec5dot8-sensors-25-01222"><title>5.8. Comparison with Existing Studies</title><p>It was challenging to compare the proposed algorithm with existing emotion recognition systems. Only two researchers have explored guilt emotion identification systems, each using different modalities. Differences in the data sets, stimuli used, recording devices, extracted features and classification methods made direct comparisons more challenging.</p><p>However, an overview of the existing studies investigating guilt, with details of the findings for each method, is demonstrated in <xref rid="sensors-25-01222-t009" ref-type="table">Table 9</xref>. Key aspects of each study, such as data modalities and findings, are summarized in this table to highlight how the proposed methodology aligns with or varies from prior research.</p><p>Another approach to compare the proposed research with previous studies is to explore contributions within similar modalities (EEG signals), but investigating different emotions. By analyzing the similarities and variances in techniques and findings, meaningful insights can be drawn. <xref rid="sensors-25-01222-t010" ref-type="table">Table 10</xref> provides a comparative analysis of the proposed model with previous studies, that collected their own datasets. This ensures that the objective of proposed methodology is appropriately aligned with theirs. As a result, a more robust and direct comparison can be made. The studies shown in <xref rid="sensors-25-01222-t010" ref-type="table">Table 10</xref> utilized numerous classifiers for emotion identification; here, only those classifiers are mentioned that provided the best accuracy results among all. It is evident that SVM outperforms all classifiers.</p><p>As the proposed methodology also aimed to investigate the gender difference in response to stimuli, the findings of the proposed method can be compared with previous research on gender differences in emotion recognition systems; <xref rid="sensors-25-01222-t011" ref-type="table">Table 11</xref> presents a summary of studies that evaluated gender differences in EEG emotion systems. These studies demonstrate various aspects of gender-specific responses in emotional and cognitive processes. These findings are compared with the results obtained through the proposed methodology. Several investigations have been conducted on the gender differences in emotional responses; however, no consistent findings have concluded which gender is more emotional. Studies [<xref rid="B80-sensors-25-01222" ref-type="bibr">80</xref>,<xref rid="B89-sensors-25-01222" ref-type="bibr">89</xref>] reported only a slight gender variation in regulation. Nolen et al. [<xref rid="B90-sensors-25-01222" ref-type="bibr">90</xref>] demonstrated that women have higher usage of reshaping the situation and there is no significant gender difference in situations of suppression. In contrast, Parvaz et al. [<xref rid="B91-sensors-25-01222" ref-type="bibr">91</xref>] proposed that no gender difference exists in LPP and Alpha frontal activity during cognitive reappraisal.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-01222"><title>6. Conclusions and New Research Directions</title><p>The proposed methodology takes the first step, aiming to observe EEG data linked to guilt using a bottom-up experimental approach. This study addresses the research question of whether guilt and neutral emotions are related to human brain signals and investigates gender-based differences in responses to guilt and neutral emotional stimuli. The findings reveal that the brain not only has a connection to these emotions but also behaves distinctly in response to guilt and neutral emotions, with females showing greater expressiveness in response to emotional stimuli than male participants. Additionally, this contribution includes the development of stimuli to ignite guilt, further enhancing the understanding of its neural correlates. The findings of the proposed methodology illustrate the significance of EEG brain signals for the detection of complex emotions.</p><p>The classification results reveal that the classifier provides better accuracy with an engineered data set by bypassing the feature extraction process from EEG data. With the traditional approach, few EEG features seem to be specifically linked with the emotion of guilt. These features can be utilized to distinguish between guilt and neutral EEG data. Additionally all five major frequency bands have a pivotal role in distinguishing between guilt and non-guilt EEG data.Not only are the five major frequency bands significant, but critical information may also be present outside these bands, particularly in the case of complex emotions, as evident from better accuracy via the Data Engineering approach.</p><p>Moreover, the accuracies of both genders were analyzed for each band separately. It was revealed that female participants displayed higher accuracies across all bands except the Delta band. However, the limited sample size constrains the generalization of the findings, although it highlights that females are more attentive and expressive to emotional stimuli.</p><p>In the future, we plan to analyze more data processing techniques to compare the performance of proposed methodology. Finally, special efforts will made to make the collected data set available for further research to support human&#x02013;computer interaction technology. Further, the proposed methodology will be improved by incorporating more data sets for training and testing.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Methodology, S.R.Z.; Validation, M.A.H.; Formal analysis, N.A.K.; Investigation, S.R.Z. and M.A.H.; Data curation, S.R.Z.; Writing&#x02014;original draft, S.R.Z.; Writing&#x02014;review &#x00026; editing, N.A.K. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in this study are included in the article. Further inquiries can be directed to the corresponding authors.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01222"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mehmood</surname><given-names>R.</given-names></name>
<name><surname>Du</surname><given-names>R.</given-names></name>
<name><surname>Lee</surname><given-names>H.</given-names></name>
</person-group><article-title>Optimal Feature Selection and Deep Learning Ensembles Method for Emotion Recognition from Human Brain EEG Sensors</article-title><source>IEEE Access</source><year>2017</year><volume>5</volume><fpage>14797</fpage><lpage>14806</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2017.2724555</pub-id></element-citation></ref><ref id="B2-sensors-25-01222"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Luneski</surname><given-names>A.</given-names></name>
<name><surname>Konstantinidis</surname><given-names>E.</given-names></name>
<name><surname>Bamidis</surname><given-names>P.</given-names></name>
</person-group><article-title>Affective Medicine: A Review of Affective Computing Efforts in Medical Informatics</article-title><source>Methods Inf. Med.</source><year>2010</year><volume>49</volume><fpage>207</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.3414/ME0617</pub-id><pub-id pub-id-type="pmid">20411209</pub-id>
</element-citation></ref><ref id="B3-sensors-25-01222"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aung</surname><given-names>M.S.</given-names></name>
<name><surname>Kaltwang</surname><given-names>S.</given-names></name>
<name><surname>Romera-Paredes</surname><given-names>B.</given-names></name>
<name><surname>Martinez</surname><given-names>B.</given-names></name>
<name><surname>Singh</surname><given-names>A.</given-names></name>
<name><surname>Cella</surname><given-names>M.</given-names></name>
<name><surname>Bianchi-Berthouze</surname><given-names>N.</given-names></name>
</person-group><article-title>The Automatic Detection of Chronic Pain-Related Expression: Requirements, Challenges and the Multimodal EmoPain Dataset</article-title><source>IEEE Trans. Affect. Comput.</source><year>2015</year><volume>7</volume><fpage>435</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2015.2462830</pub-id><pub-id pub-id-type="pmid">30906508</pub-id>
</element-citation></ref><ref id="B4-sensors-25-01222"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Guan</surname><given-names>L.</given-names></name>
</person-group><article-title>An Investigation of Speech-Based Human Emotion Recognition</article-title><source>Proceedings of the IEEE 6th Workshop on Multimedia Signal Processing</source><conf-loc>Sienna, Italy</conf-loc><conf-date>29 September&#x02013;1 October 2004</conf-date><fpage>15</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1109/MMSP.2004.1436403</pub-id></element-citation></ref><ref id="B5-sensors-25-01222"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nakisa</surname><given-names>B.</given-names></name>
<name><surname>Rastgoo</surname><given-names>M.N.</given-names></name>
<name><surname>Tjondronegoro</surname><given-names>D.</given-names></name>
<name><surname>Chandran</surname><given-names>V.</given-names></name>
</person-group><article-title>Evolutionary Computation Algorithms for Feature Selection of EEG-Based Emotion Recognition Using Mobile Sensors</article-title><source>Expert Syst. Appl.</source><year>2018</year><volume>93</volume><fpage>143</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2017.09.062</pub-id></element-citation></ref><ref id="B6-sensors-25-01222"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.J.</given-names></name>
<name><surname>Yu</surname><given-names>M.</given-names></name>
<name><surname>Zhao</surname><given-names>G.</given-names></name>
<name><surname>Song</surname><given-names>J.</given-names></name>
<name><surname>Ge</surname><given-names>Y.</given-names></name>
<name><surname>Shi</surname><given-names>Y.</given-names></name>
</person-group><article-title>Real-Time Movie-Induced Discrete Emotion Recognition from EEG Signals</article-title><source>IEEE Trans. Affect. Comput.</source><year>2017</year><volume>9</volume><fpage>550</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2017.2660485</pub-id></element-citation></ref><ref id="B7-sensors-25-01222"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dai</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>P.</given-names></name>
<name><surname>Zhang</surname><given-names>W.</given-names></name>
</person-group><article-title>Wearable Biosensor Network Enabled Multimodal Daily-Life Emotion Recognition Employing Reputation-Driven Imbalanced Fuzzy Classification</article-title><source>Measurement</source><year>2017</year><volume>109</volume><fpage>408</fpage><lpage>424</lpage><pub-id pub-id-type="doi">10.1016/j.measurement.2017.06.006</pub-id></element-citation></ref><ref id="B8-sensors-25-01222"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cid</surname><given-names>F.</given-names></name>
<name><surname>Moreno</surname><given-names>J.</given-names></name>
<name><surname>Bustos</surname><given-names>P.</given-names></name>
<name><surname>N&#x000fa;nez</surname><given-names>P.</given-names></name>
</person-group><article-title>Muecas: A Multi-Sensor Robotic Head for Affective Human Robot Interaction and Imitation</article-title><source>Sensors</source><year>2014</year><volume>14</volume><elocation-id>7711&#x02013;7737</elocation-id><pub-id pub-id-type="doi">10.3390/s140507711</pub-id><pub-id pub-id-type="pmid">24787636</pub-id>
</element-citation></ref><ref id="B9-sensors-25-01222"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Grafsgaard</surname><given-names>J.</given-names></name>
<name><surname>Wiggins</surname><given-names>J.B.</given-names></name>
<name><surname>Boyer</surname><given-names>K.E.</given-names></name>
<name><surname>Wiebe</surname><given-names>E.N.</given-names></name>
<name><surname>Lester</surname><given-names>J.</given-names></name>
</person-group><article-title>Automatically Recognizing Facial Expression: Predicting Engagement and Frustration</article-title><source>Proceedings of the Educational Data Mining 2013</source><conf-loc>Memphis, TN, USA</conf-loc><conf-date>6&#x02013;9 July 2013</conf-date></element-citation></ref><ref id="B10-sensors-25-01222"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yadegaridehkordi</surname><given-names>E.</given-names></name>
<name><surname>Noor</surname><given-names>N.F.B.M.</given-names></name>
<name><surname>Ayub</surname><given-names>M.N.B.</given-names></name>
<name><surname>Affal</surname><given-names>H.B.</given-names></name>
<name><surname>Hussin</surname><given-names>N.B.</given-names></name>
</person-group><article-title>Affective Computing in Education: A Systematic Review and Future Research</article-title><source>Comput. Educ.</source><year>2019</year><volume>142</volume><fpage>103649</fpage><pub-id pub-id-type="doi">10.1016/j.compedu.2019.103649</pub-id></element-citation></ref><ref id="B11-sensors-25-01222"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Duo</surname><given-names>S.</given-names></name>
<name><surname>Song</surname><given-names>L.X.</given-names></name>
</person-group><article-title>An E-Learning System Based on Affective Computing</article-title><source>Phys. Procedia</source><year>2012</year><volume>24</volume><fpage>1893</fpage><lpage>1898</lpage><pub-id pub-id-type="doi">10.1016/j.phpro.2012.02.278</pub-id></element-citation></ref><ref id="B12-sensors-25-01222"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>C.</given-names></name>
</person-group><article-title>Recognition of Psychological Emotion by EEG Features</article-title><source>Netw. Model. Anal. Health Inform. Bioinform.</source><year>2021</year><volume>10</volume><fpage>12</fpage><pub-id pub-id-type="doi">10.1007/s13721-020-00283-2</pub-id></element-citation></ref><ref id="B13-sensors-25-01222"><label>13.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
</person-group><article-title>Facial Action Coding System</article-title><source>Environmental Psychology &#x00026; Nonverbal Behavior</source><publisher-name>American Psychological Association</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>1977</year></element-citation></ref><ref id="B14-sensors-25-01222"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
<name><surname>Cordaro</surname><given-names>D.</given-names></name>
</person-group><article-title>What is meant by calling emotions basic</article-title><source>Emot. Rev.</source><year>2011</year><volume>3</volume><fpage>364</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1177/1754073911410740</pub-id></element-citation></ref><ref id="B15-sensors-25-01222"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Izard</surname><given-names>C.E.</given-names></name>
</person-group><article-title>Forms and functions of emotions: Matters of emotion&#x02013;cognition interactions</article-title><source>Emot. Rev.</source><year>2011</year><volume>3</volume><fpage>371</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1177/1754073911410737</pub-id></element-citation></ref><ref id="B16-sensors-25-01222"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Levenson</surname><given-names>R.W.</given-names></name>
</person-group><article-title>Basic emotion questions</article-title><source>Emot. Rev.</source><year>2011</year><volume>3</volume><fpage>379</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.1177/1754073911410743</pub-id></element-citation></ref><ref id="B17-sensors-25-01222"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Panksepp</surname><given-names>J.</given-names></name>
<name><surname>Watt</surname><given-names>D.</given-names></name>
</person-group><article-title>What is basic about basic emotions? Lasting lessons from affective neuroscience</article-title><source>Emot. Rev.</source><year>2011</year><volume>3</volume><fpage>387</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1177/1754073911410741</pub-id></element-citation></ref><ref id="B18-sensors-25-01222"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Russell</surname><given-names>A.</given-names></name>
</person-group><article-title>A circumplex model of affect</article-title><source>J. Personal. Soc. Psychol.</source><year>1980</year><volume>39</volume><fpage>1161</fpage><lpage>1178</lpage><pub-id pub-id-type="doi">10.1037/h0077714</pub-id></element-citation></ref><ref id="B19-sensors-25-01222"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Plutchik</surname><given-names>R.</given-names></name>
</person-group><article-title>The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice</article-title><source>Am. Sci.</source><year>2001</year><volume>89</volume><fpage>344</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1511/2001.28.344</pub-id></element-citation></ref><ref id="B20-sensors-25-01222"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Peng</surname><given-names>W.</given-names></name>
<name><surname>Hong</surname><given-names>X.</given-names></name>
<name><surname>Zhao</surname><given-names>G.</given-names></name>
</person-group><article-title>Adaptive modality distillation for separable multimodal sentiment analysis</article-title><source>IEEE Intell. Syst.</source><year>2021</year><volume>36</volume><fpage>82</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1109/MIS.2021.3057757</pub-id></element-citation></ref><ref id="B21-sensors-25-01222"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sailunaz</surname><given-names>K.</given-names></name>
<name><surname>Alhajj</surname><given-names>R.</given-names></name>
</person-group><article-title>Emotion and sentiment analysis from Twitter text</article-title><source>J. Comput. Sci.</source><year>2019</year><volume>36</volume><fpage>101003</fpage><pub-id pub-id-type="doi">10.1016/j.jocs.2019.05.009</pub-id></element-citation></ref><ref id="B22-sensors-25-01222"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Koelstra</surname><given-names>S.</given-names></name>
<name><surname>Muhl</surname><given-names>C.</given-names></name>
<name><surname>Soleymani</surname><given-names>M.</given-names></name>
<name><surname>Lee</surname><given-names>J.S.</given-names></name>
<name><surname>Yazdani</surname><given-names>A.</given-names></name>
<name><surname>Ebrahimi</surname><given-names>T.</given-names></name>
<name><surname>Patras</surname><given-names>I.</given-names></name>
</person-group><article-title>DEAP: A database for emotion analysis using physiological signals</article-title><source>IEEE Trans. Affect. Comput.</source><year>2011</year><volume>3</volume><fpage>18</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1109/T-AFFC.2011.15</pub-id></element-citation></ref><ref id="B23-sensors-25-01222"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Soleymani</surname><given-names>M.</given-names></name>
<name><surname>Lichtenauer</surname><given-names>J.</given-names></name>
<name><surname>Pun</surname><given-names>T.</given-names></name>
<name><surname>Pantic</surname><given-names>M.</given-names></name>
</person-group><article-title>A multimodal database for affect recognition and implicit tagging</article-title><source>IEEE Trans. Affect. Comput.</source><year>2011</year><volume>3</volume><fpage>42</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1109/T-AFFC.2011.25</pub-id></element-citation></ref><ref id="B24-sensors-25-01222"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ko</surname><given-names>B.C.</given-names></name>
</person-group><article-title>A brief review of facial emotion recognition based on visual information</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>401</elocation-id><pub-id pub-id-type="doi">10.3390/s18020401</pub-id><pub-id pub-id-type="pmid">29385749</pub-id>
</element-citation></ref><ref id="B25-sensors-25-01222"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rodriguez</surname><given-names>L.M.</given-names></name>
<name><surname>Young</surname><given-names>C.M.</given-names></name>
<name><surname>Neighbors</surname><given-names>C.</given-names></name>
<name><surname>Campbell</surname><given-names>M.T.</given-names></name>
<name><surname>Lu</surname><given-names>Q.</given-names></name>
</person-group><article-title>Evaluating guilt and shame in an expressive writing alcohol intervention</article-title><source>Alcohol</source><year>2015</year><volume>49</volume><fpage>491</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1016/j.alcohol.2015.05.001</pub-id><pub-id pub-id-type="pmid">26074424</pub-id>
</element-citation></ref><ref id="B26-sensors-25-01222"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rebega</surname><given-names>O.L.</given-names></name>
<name><surname>Apostol</surname><given-names>L.</given-names></name>
<name><surname>Benga</surname><given-names>O.</given-names></name>
<name><surname>Miclea</surname><given-names>M.</given-names></name>
</person-group><article-title>Inducing guilt: A literature review</article-title><source>Procedia-Soc. Behav. Sci.</source><year>2013</year><volume>78</volume><fpage>536</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1016/j.sbspro.2013.04.346</pub-id></element-citation></ref><ref id="B27-sensors-25-01222"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Greco</surname><given-names>A.</given-names></name>
<name><surname>Valenza</surname><given-names>G.</given-names></name>
<name><surname>Citi</surname><given-names>L.</given-names></name>
<name><surname>Scilingo</surname><given-names>E.P.</given-names></name>
</person-group><article-title>Arousal and valence recognition of affective sounds based on electrodermal activity</article-title><source>IEEE Sens. J.</source><year>2016</year><volume>17</volume><fpage>716</fpage><lpage>725</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2016.2623677</pub-id></element-citation></ref><ref id="B28-sensors-25-01222"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Xu</surname><given-names>C.</given-names></name>
<name><surname>Xue</surname><given-names>W.</given-names></name>
<name><surname>Hu</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
<name><surname>Gao</surname><given-names>M.</given-names></name>
</person-group><article-title>Emotion recognition based on multichannel physiological signals with comprehensive nonlinear processing</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>3886</elocation-id><pub-id pub-id-type="doi">10.3390/s18113886</pub-id><pub-id pub-id-type="pmid">30423894</pub-id>
</element-citation></ref><ref id="B29-sensors-25-01222"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Piana</surname><given-names>S.</given-names></name>
<name><surname>Stagliano</surname><given-names>A.</given-names></name>
<name><surname>Odone</surname><given-names>F.</given-names></name>
<name><surname>Verri</surname><given-names>A.</given-names></name>
<name><surname>Camurri</surname><given-names>A.</given-names></name>
</person-group><article-title>Real-time automatic emotion recognition from body gestures</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1402.5047</pub-id></element-citation></ref><ref id="B30-sensors-25-01222"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Camurri</surname><given-names>A.</given-names></name>
<name><surname>Lagerl&#x000f6;f</surname><given-names>I.</given-names></name>
<name><surname>Volpe</surname><given-names>G.</given-names></name>
</person-group><article-title>Recognizing emotion from dance movement: Comparison of spectator recognition and automated techniques</article-title><source>Int. J. Hum.-Comput. Stud.</source><year>2003</year><volume>59</volume><fpage>213</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1016/S1071-5819(03)00050-8</pub-id></element-citation></ref><ref id="B31-sensors-25-01222"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schuller</surname><given-names>B.</given-names></name>
<name><surname>Rigoll</surname><given-names>G.</given-names></name>
<name><surname>Lang</surname><given-names>M.</given-names></name>
</person-group><article-title>Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture</article-title><source>Proc. IEEE Int. Conf. Acoust. Speech Signal Process</source><year>2004</year><volume>1</volume><fpage>I-577</fpage><pub-id pub-id-type="doi">10.1109/ICASSP.2004.1326622</pub-id></element-citation></ref><ref id="B32-sensors-25-01222"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Miji&#x00107;</surname><given-names>I.</given-names></name>
<name><surname>&#x00160;arlija</surname><given-names>M.</given-names></name>
<name><surname>Petrinovi&#x00107;</surname><given-names>D.</given-names></name>
</person-group><article-title>MMOD-COG: A database for multimodal cognitive load classification</article-title><source>Proceedings of the 2019 11th International Symposium on Image and Signal Processing and Analysis (ISPA)</source><conf-loc>Dubrovnik, Croatia</conf-loc><conf-date>23&#x02013;25 September 2019</conf-date><fpage>15</fpage><lpage>20</lpage></element-citation></ref><ref id="B33-sensors-25-01222"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>T.Y.</given-names></name>
<name><surname>Li</surname><given-names>J.L.</given-names></name>
<name><surname>Chang</surname><given-names>C.M.</given-names></name>
<name><surname>Lee</surname><given-names>C.C.</given-names></name>
</person-group><article-title>A Dual-Complementary Acoustic Embedding Network Learned from Raw Waveform for Speech Emotion Recognition</article-title><source>Proceedings of the 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)</source><conf-loc>Cambridge, UK</conf-loc><conf-date>3&#x02013;6 September 2019</conf-date><fpage>83</fpage><lpage>88</lpage></element-citation></ref><ref id="B34-sensors-25-01222"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lindquist</surname><given-names>K.A.</given-names></name>
<name><surname>Wager</surname><given-names>T.D.</given-names></name>
<name><surname>Kober</surname><given-names>H.</given-names></name>
<name><surname>Bliss-Moreau</surname><given-names>E.</given-names></name>
<name><surname>Barrett</surname><given-names>L.F.</given-names></name>
</person-group><article-title>The brain basis of emotion: A meta-analytic review</article-title><source>Behav. Brain Sci.</source><year>2012</year><volume>35</volume><fpage>121</fpage><pub-id pub-id-type="doi">10.1017/S0140525X11000446</pub-id><pub-id pub-id-type="pmid">22617651</pub-id>
</element-citation></ref><ref id="B35-sensors-25-01222"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nunez</surname><given-names>P.L.</given-names></name>
<name><surname>Srinivasan</surname><given-names>R.</given-names></name>
</person-group><article-title>A theoretical basis for standing and traveling brain waves measured with human EEG with implications for an integrated consciousness</article-title><source>Clin. Neurophysiol.</source><year>2006</year><volume>117</volume><fpage>2424</fpage><lpage>2435</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2006.06.754</pub-id><pub-id pub-id-type="pmid">16996303</pub-id>
</element-citation></ref><ref id="B36-sensors-25-01222"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alarc&#x000e3;o</surname><given-names>S.M.</given-names></name>
<name><surname>Fonseca</surname><given-names>M.J.</given-names></name>
</person-group><article-title>Emotion&#x02019;s recognition using EEG signals: A survey</article-title><source>IEEE Trans. Affective Comput.</source><year>2017</year><volume>10</volume><fpage>374</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2017.2714671</pub-id></element-citation></ref><ref id="B37-sensors-25-01222"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tangney</surname><given-names>J.P.</given-names></name>
<name><surname>Miller</surname><given-names>R.S.</given-names></name>
<name><surname>Flicker</surname><given-names>L.</given-names></name>
<name><surname>Barlow</surname><given-names>D.H.</given-names></name>
</person-group><article-title>Are shame, guilt, and embarrassment distinct emotions?</article-title><source>J. Personal. Soc. Psychol.</source><year>1996</year><volume>70</volume><fpage>1256</fpage><pub-id pub-id-type="doi">10.1037/0022-3514.70.6.1256</pub-id><pub-id pub-id-type="pmid">8667166</pub-id>
</element-citation></ref><ref id="B38-sensors-25-01222"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gao</surname><given-names>Y.</given-names></name>
<name><surname>Zhu</surname><given-names>Z.</given-names></name>
<name><surname>Fang</surname><given-names>F.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Meng</surname><given-names>M.</given-names></name>
</person-group><article-title>EEG emotion recognition based on data-driven signal auto-segmentation and feature fusion</article-title><source>J. Affect. Disord.</source><year>2024</year><volume>361</volume><fpage>356</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1016/j.jad.2024.06.042</pub-id><pub-id pub-id-type="pmid">38885847</pub-id>
</element-citation></ref><ref id="B39-sensors-25-01222"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Goshvarpour</surname><given-names>A.</given-names></name>
<name><surname>Goshvarpour</surname><given-names>A.</given-names></name>
</person-group><article-title>EEG emotion recognition based on an innovative information potential index</article-title><source>Cogn. Neurodyn.</source><year>2024</year><volume>18</volume><fpage>2177</fpage><lpage>2191</lpage><pub-id pub-id-type="doi">10.1007/s11571-024-10077-1</pub-id><pub-id pub-id-type="pmid">39555291</pub-id>
</element-citation></ref><ref id="B40-sensors-25-01222"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>An</surname><given-names>Y.</given-names></name>
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Zhao</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.D.</given-names></name>
</person-group><article-title>EEG emotion recognition based on the attention mechanism and pre-trained convolution capsule network</article-title><source>Knowl.-Based Syst.</source><year>2023</year><volume>265</volume><fpage>110372</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2023.110372</pub-id></element-citation></ref><ref id="B41-sensors-25-01222"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tuncer</surname><given-names>T.</given-names></name>
<name><surname>Dogan</surname><given-names>S.</given-names></name>
<name><surname>Subasi</surname><given-names>A.</given-names></name>
</person-group><article-title>A new fractal pattern feature generation function-based emotion recognition method using EEG</article-title><source>Chaos Solitons Fractals</source><year>2021</year><volume>144</volume><fpage>110671</fpage><pub-id pub-id-type="doi">10.1016/j.chaos.2021.110671</pub-id></element-citation></ref><ref id="B42-sensors-25-01222"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>B.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Song</surname><given-names>R.</given-names></name>
<name><surname>Cheng</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
</person-group><article-title>Emotion recognition from EEG based on multi-task learning with capsule network and attention mechanism</article-title><source>Comput. Biol. Med.</source><year>2022</year><volume>143</volume><elocation-id>105303</elocation-id><pub-id pub-id-type="doi">10.1016/j.compbiomed.2022.105303</pub-id><pub-id pub-id-type="pmid">35217341</pub-id>
</element-citation></ref><ref id="B43-sensors-25-01222"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Joshi</surname><given-names>V.M.</given-names></name>
<name><surname>Ghongade</surname><given-names>R.B.</given-names></name>
<name><surname>Joshi</surname><given-names>A.M.</given-names></name>
<name><surname>Kulkarni</surname><given-names>R.V.</given-names></name>
</person-group><article-title>Deep BiLSTM neural network model for emotion detection using cross-dataset approach</article-title><source>Biomed. Signal Process. Control</source><year>2022</year><volume>73</volume><elocation-id>103407</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2021.103407</pub-id></element-citation></ref><ref id="B44-sensors-25-01222"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gao</surname><given-names>Q.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
<name><surname>Kang</surname><given-names>Q.</given-names></name>
<name><surname>Tian</surname><given-names>Z.</given-names></name>
<name><surname>Song</surname><given-names>Y.</given-names></name>
</person-group><article-title>EEG-based emotion recognition with feature fusion networks</article-title><source>Int. J. Mach. Learn. Cybern.</source><year>2022</year><volume>13</volume><fpage>421</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1007/s13042-021-01414-5</pub-id></element-citation></ref><ref id="B45-sensors-25-01222"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yuvaraj</surname><given-names>R.</given-names></name>
<name><surname>Thagavel</surname><given-names>P.</given-names></name>
<name><surname>Thomas</surname><given-names>J.</given-names></name>
<name><surname>Fogarty</surname><given-names>J.</given-names></name>
<name><surname>Ali</surname><given-names>F.</given-names></name>
</person-group><article-title>Comprehensive analysis of feature extraction methods for emotion recognition from multichannel EEG recordings</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>915</elocation-id><pub-id pub-id-type="doi">10.3390/s23020915</pub-id><pub-id pub-id-type="pmid">36679710</pub-id>
</element-citation></ref><ref id="B46-sensors-25-01222"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Subasi</surname><given-names>A.</given-names></name>
<name><surname>Tuncer</surname><given-names>T.</given-names></name>
<name><surname>Dogan</surname><given-names>S.</given-names></name>
<name><surname>Tanko</surname><given-names>D.</given-names></name>
<name><surname>Sakoglu</surname><given-names>U.</given-names></name>
</person-group><article-title>EEG-based emotion recognition using tunable Q wavelet transform and rotation forest ensemble classifier</article-title><source>Biomed. Signal Process. Control</source><year>2021</year><volume>68</volume><elocation-id>102648</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2021.102648</pub-id></element-citation></ref><ref id="B47-sensors-25-01222"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bhushan</surname><given-names>B.</given-names></name>
<name><surname>Basu</surname><given-names>S.</given-names></name>
<name><surname>Dutta</surname><given-names>S.</given-names></name>
</person-group><article-title>Revisiting guilt, shame, and remorse</article-title><source>Psychol. Stud.</source><year>2020</year><volume>65</volume><fpage>247</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1007/s12646-020-00561-z</pub-id></element-citation></ref><ref id="B48-sensors-25-01222"><label>48.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Julle-Dani&#x000e8;re</surname><given-names>E.C.C.</given-names></name>
</person-group><article-title>The Expression, Experience, and Social Consequences of Guilt: A Cross-Cultural Study</article-title><source>Ph.D. Thesis</source><publisher-name>University of Portsmouth</publisher-name><publisher-loc>Portsmouth, UK</publisher-loc><year>2019</year></element-citation></ref><ref id="B49-sensors-25-01222"><label>49.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Lewis</surname><given-names>M.</given-names></name>
</person-group><article-title>Self-conscious emotions: Embarrassment, pride, shame, and guilt</article-title><source>Handbook of Emotions</source><person-group person-group-type="editor">
<name><surname>Lewis</surname><given-names>M.</given-names></name>
<name><surname>Haviland</surname><given-names>J.M.</given-names></name>
</person-group><publisher-name>Guilford Press</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>1993</year><fpage>563</fpage><lpage>573</lpage></element-citation></ref><ref id="B50-sensors-25-01222"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cryder</surname><given-names>C.E.</given-names></name>
<name><surname>Springer</surname><given-names>S.</given-names></name>
<name><surname>Morewedge</surname><given-names>C.K.</given-names></name>
</person-group><article-title>Guilty feelings, targeted actions</article-title><source>Personal. Soc. Psychol. Bull.</source><year>2012</year><volume>38</volume><fpage>607</fpage><lpage>618</lpage><pub-id pub-id-type="doi">10.1177/0146167211435796</pub-id><pub-id pub-id-type="pmid">22337764</pub-id>
</element-citation></ref><ref id="B51-sensors-25-01222"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tignor</surname><given-names>S.M.</given-names></name>
<name><surname>Colvin</surname><given-names>C.R.</given-names></name>
</person-group><article-title>The meaning of guilt: Reconciling the past to inform the future</article-title><source>J. Pers. Soc. Psychol.</source><year>2019</year><volume>116</volume><fpage>989</fpage><pub-id pub-id-type="doi">10.1037/pspp0000216</pub-id><pub-id pub-id-type="pmid">30359067</pub-id>
</element-citation></ref><ref id="B52-sensors-25-01222"><label>52.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Parrott</surname><given-names>W.G.</given-names></name>
<name><surname>Harre</surname><given-names>R.</given-names></name>
</person-group><source>The Emotions: Social, Cultural and Biological Dimensions</source><publisher-name>Sage Publications</publisher-name><publisher-loc>Thousand Oaks, CA, USA</publisher-loc><year>1996</year></element-citation></ref><ref id="B53-sensors-25-01222"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>T.R.</given-names></name>
<name><surname>Wolf</surname><given-names>S.T.</given-names></name>
<name><surname>Panter</surname><given-names>A.T.</given-names></name>
<name><surname>Insko</surname><given-names>C.A.</given-names></name>
</person-group><article-title>Introducing the GASP scale: A new measure of guilt and shame proneness</article-title><source>J. Pers. Soc. Psychol.</source><year>2011</year><volume>100</volume><fpage>947</fpage><pub-id pub-id-type="doi">10.1037/a0022641</pub-id><pub-id pub-id-type="pmid">21517196</pub-id>
</element-citation></ref><ref id="B54-sensors-25-01222"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tangney</surname><given-names>W.</given-names></name>
<name><surname>Wagner</surname><given-names>R.</given-names></name>
<name><surname>Gramzow</surname><given-names>R.</given-names></name>
</person-group><article-title>TOSCA: Test of Self-Conscious Affect</article-title><source>Psychol. Assess.</source><year>1989</year><volume>1</volume><fpage>291</fpage><lpage>308</lpage></element-citation></ref><ref id="B55-sensors-25-01222"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Marci</surname><given-names>C.D.</given-names></name>
<name><surname>Glick</surname><given-names>D.M.</given-names></name>
<name><surname>Loh</surname><given-names>R.</given-names></name>
<name><surname>Dougherty</surname><given-names>D.D.</given-names></name>
</person-group><article-title>Autonomic and prefrontal cortex responses to autobiographical recall of emotions</article-title><source>Cogn. Affect. Behav. Neurosci.</source><year>2007</year><volume>7</volume><fpage>243</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.3758/CABN.7.3.243</pub-id><pub-id pub-id-type="pmid">17993210</pub-id>
</element-citation></ref><ref id="B56-sensors-25-01222"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Siedlecka</surname><given-names>E.</given-names></name>
<name><surname>Capper</surname><given-names>M.M.</given-names></name>
<name><surname>Denson</surname><given-names>T.F.</given-names></name>
</person-group><article-title>Negative emotional events that people ruminate about feel closer in time</article-title><source>PLoS ONE</source><year>2015</year><volume>10</volume><elocation-id>e0117105</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0117105</pub-id><pub-id pub-id-type="pmid">25714395</pub-id>
</element-citation></ref><ref id="B57-sensors-25-01222"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bushman</surname><given-names>B.J.</given-names></name>
<name><surname>Baumeister</surname><given-names>R.F.</given-names></name>
</person-group><article-title>Threatened egotism, narcissism, self-esteem, and direct and displaced aggression: Does self-love or self-hate lead to violence?</article-title><source>J. Pers. Soc. Psychol.</source><year>1998</year><volume>75</volume><fpage>219</fpage><pub-id pub-id-type="doi">10.1037/0022-3514.75.1.219</pub-id><pub-id pub-id-type="pmid">9686460</pub-id>
</element-citation></ref><ref id="B58-sensors-25-01222"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>De Hooge</surname><given-names>I.E.</given-names></name>
<name><surname>Nelissen</surname><given-names>R.</given-names></name>
<name><surname>Breugelmans</surname><given-names>S.M.</given-names></name>
<name><surname>Zeelenberg</surname><given-names>M.</given-names></name>
</person-group><article-title>What is moral about guilt? Acting &#x02018;prosocially&#x02019; at the disadvantage of others</article-title><source>J. Pers. Soc. Psychol.</source><year>2011</year><volume>100</volume><fpage>462</fpage><pub-id pub-id-type="doi">10.1037/a0021459</pub-id><pub-id pub-id-type="pmid">21244173</pub-id>
</element-citation></ref><ref id="B59-sensors-25-01222"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Janssen</surname><given-names>J.H.</given-names></name>
<name><surname>Tacken</surname><given-names>P.</given-names></name>
<name><surname>de Vries</surname><given-names>J.J.G.</given-names></name>
<name><surname>van den Broek</surname><given-names>E.L.</given-names></name>
<name><surname>Westerink</surname><given-names>J.H.</given-names></name>
<name><surname>Haselager</surname><given-names>P.</given-names></name>
<name><surname>IJsselsteijn</surname><given-names>W.A.</given-names></name>
</person-group><article-title>Machines outperform laypersons in recognizing emotions elicited by autobiographical recollection</article-title><source>Hum.-Comput. Interact.</source><year>2013</year><volume>28</volume><fpage>479</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1080/07370024.2012.755421</pub-id></element-citation></ref><ref id="B60-sensors-25-01222"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bond</surname><given-names>C.V.A.</given-names></name>
</person-group><article-title>Personal relevance is an important dimension for visceral reactivity in emotional imagery</article-title><source>Cogn. Emot.</source><year>1998</year><volume>12</volume><fpage>231</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1080/026999398379736</pub-id></element-citation></ref><ref id="B61-sensors-25-01222"><label>61.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Chanel</surname><given-names>G.</given-names></name>
<name><surname>Kronegg</surname><given-names>J.</given-names></name>
<name><surname>Grandjean</surname><given-names>D.</given-names></name>
<name><surname>Pun</surname><given-names>T.</given-names></name>
</person-group><article-title>Emotion assessment: Arousal evaluation using EEG&#x02019;s and peripheral physiological signals</article-title><source>International Workshop on Multimedia Content Representation, Classification and Security</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2006</year><fpage>530</fpage><lpage>537</lpage><pub-id pub-id-type="doi">10.1007/11848035_70</pub-id></element-citation></ref><ref id="B62-sensors-25-01222"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bradley</surname><given-names>M.M.</given-names></name>
<name><surname>Lang</surname><given-names>P.J.</given-names></name>
</person-group><article-title>Measuring emotion: The self-assessment manikin and the semantic differential</article-title><source>J. Behav. Ther. Exp. Psychiatry</source><year>1994</year><volume>25</volume><fpage>49</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1016/0005-7916(94)90063-9</pub-id><pub-id pub-id-type="pmid">7962581</pub-id>
</element-citation></ref><ref id="B63-sensors-25-01222"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>M&#x000fc;ller-Gerking</surname><given-names>J.</given-names></name>
<name><surname>Pfurtscheller</surname><given-names>G.</given-names></name>
<name><surname>Flyvbjerg</surname><given-names>H.</given-names></name>
</person-group><article-title>Designing optimal spatial filters for single-trial EEG classification in a movement task</article-title><source>Clin. Neurophysiol.</source><year>1999</year><volume>110</volume><fpage>787</fpage><lpage>798</lpage><pub-id pub-id-type="doi">10.1016/S1388-2457(98)00038-8</pub-id><pub-id pub-id-type="pmid">10400191</pub-id>
</element-citation></ref><ref id="B64-sensors-25-01222"><label>64.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Perrin</surname><given-names>X.</given-names></name>
</person-group><article-title>Semi-Autonomous Navigation of an Assistive Robot Using Low Throughput Interfaces</article-title><source>Ph.D. Dissertation</source><publisher-name>Eidgen&#x000f6;ssische Technische Hochschule Z&#x000fc;rich</publisher-name><publisher-loc>Z&#x000fc;rich, Switzerland</publisher-loc><year>2009</year></element-citation></ref><ref id="B65-sensors-25-01222"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guerrero-Mosquera</surname><given-names>C.</given-names></name>
<name><surname>Navia-V&#x000e1;zquez</surname><given-names>A.</given-names></name>
</person-group><article-title>Automatic removal of ocular artefacts using adaptive filtering and independent component analysis for electroencephalogram data</article-title><source>IET Signal Process.</source><year>2012</year><volume>6</volume><fpage>99</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1049/iet-spr.2010.0135</pub-id></element-citation></ref><ref id="B66-sensors-25-01222"><label>66.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>L.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Beadle</surname><given-names>P.J.</given-names></name>
</person-group><article-title>Independent component analysis of EEG signals</article-title><source>Proceedings of the 2005 IEEE International Workshop on VLSI Design and Video Technology</source><conf-loc>Suzhou, China</conf-loc><conf-date>28&#x02013;30 May 2005</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2005</year><fpage>219</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1109/IWVDVT.2005.1504590</pub-id></element-citation></ref><ref id="B67-sensors-25-01222"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ungureanu</surname><given-names>M.</given-names></name>
<name><surname>Bigan</surname><given-names>C.</given-names></name>
<name><surname>Strungaru</surname><given-names>R.</given-names></name>
<name><surname>Lazarescu</surname><given-names>V.</given-names></name>
</person-group><article-title>Independent component analysis applied in biomedical signal processing</article-title><source>Meas. Sci. Rev.</source><year>2004</year><volume>4</volume><fpage>18</fpage><pub-id pub-id-type="doi">10.2478/msr-2020-0029</pub-id></element-citation></ref><ref id="B68-sensors-25-01222"><label>68.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Candra</surname><given-names>H.</given-names></name>
<name><surname>Yuwono</surname><given-names>M.</given-names></name>
<name><surname>Chai</surname><given-names>R.</given-names></name>
<name><surname>Handojoseno</surname><given-names>A.</given-names></name>
<name><surname>Elamvazuthi</surname><given-names>I.</given-names></name>
<name><surname>Nguyen</surname><given-names>H.T.</given-names></name>
<name><surname>Su</surname><given-names>S.</given-names></name>
</person-group><article-title>Investigation of window size in classification of EEG-emotion signal with wavelet entropy and support vector machine</article-title><source>Proceedings of the 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source><conf-loc>Milan, Italy</conf-loc><conf-date>25&#x02013;29 August 2015</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2015</year><fpage>7250</fpage><lpage>7253</lpage><pub-id pub-id-type="doi">10.1109/EMBC.2015.7320065</pub-id></element-citation></ref><ref id="B69-sensors-25-01222"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Luu</surname><given-names>P.</given-names></name>
<name><surname>Tucker</surname><given-names>D.M.</given-names></name>
<name><surname>Makeig</surname><given-names>S.</given-names></name>
</person-group><article-title>Frontal midline theta and the error-related negativity: Neurophysiological mechanisms of action regulation</article-title><source>Clin. Neurophysiol.</source><year>2004</year><volume>115</volume><fpage>1821</fpage><lpage>1835</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2004.03.031</pub-id><pub-id pub-id-type="pmid">15261861</pub-id>
</element-citation></ref><ref id="B70-sensors-25-01222"><label>70.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>M.</given-names></name>
<name><surname>Lu</surname><given-names>B.L.</given-names></name>
</person-group><article-title>Emotion classification based on gamma-band EEG</article-title><source>Proceedings of the 2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society</source><conf-loc>Minneapolis, MN, USA</conf-loc><conf-date>3&#x02013;6 September 2009</conf-date><fpage>1223</fpage><lpage>1226</lpage><pub-id pub-id-type="doi">10.1109/IEMBS.2009.5333290</pub-id></element-citation></ref><ref id="B71-sensors-25-01222"><label>71.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Matlovic</surname><given-names>T.</given-names></name>
<name><surname>Gaspar</surname><given-names>P.</given-names></name>
<name><surname>Moro</surname><given-names>R.</given-names></name>
<name><surname>Simko</surname><given-names>J.</given-names></name>
<name><surname>Bielikova</surname><given-names>M.</given-names></name>
</person-group><article-title>Emotions detection using facial expressions recognition and EEG</article-title><source>Proceedings of the 2016 11th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP)</source><conf-loc>Thessaloniki, Greece</conf-loc><conf-date>20&#x02013;21 October 2016</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2016</year><fpage>18</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1109/SMAP.2016.7753378</pub-id></element-citation></ref><ref id="B72-sensors-25-01222"><label>72.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kusumaningrum</surname><given-names>T.D.</given-names></name>
<name><surname>Faqih</surname><given-names>A.</given-names></name>
<name><surname>Kusumoputro</surname><given-names>B.</given-names></name>
</person-group><article-title>Emotion recognition based on DEAP database using EEG time-frequency features and machine learning methods</article-title><source>J. Phys. Conf. Ser.</source><year>2020</year><volume>1501</volume><fpage>012020</fpage><pub-id pub-id-type="doi">10.1088/1742-6596/1501/1/012020</pub-id></element-citation></ref><ref id="B73-sensors-25-01222"><label>73.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kumar</surname><given-names>N.</given-names></name>
<name><surname>Alam</surname><given-names>K.</given-names></name>
<name><surname>Siddiqi</surname><given-names>A.H.</given-names></name>
</person-group><article-title>Wavelet transform for classification of EEG signal using SVM and ANN</article-title><source>Biomed. Pharmacol. J.</source><year>2017</year><volume>10</volume><fpage>2061</fpage><lpage>2069</lpage><pub-id pub-id-type="doi">10.13005/bpj/1328</pub-id></element-citation></ref><ref id="B74-sensors-25-01222"><label>74.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Jalilifard</surname><given-names>A.</given-names></name>
<name><surname>Pizzolato</surname><given-names>E.B.</given-names></name>
<name><surname>Islam</surname><given-names>M.K.</given-names></name>
</person-group><article-title>Emotion classification using single-channel scalp-EEG recording</article-title><source>Proceedings of the 2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>16&#x02013;20 August 2016</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2016</year><fpage>845</fpage><lpage>849</lpage><pub-id pub-id-type="doi">10.1109/EMBC.2016.7591165</pub-id></element-citation></ref><ref id="B75-sensors-25-01222"><label>75.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gajbhiye</surname><given-names>P.</given-names></name>
<name><surname>Mingchinda</surname><given-names>N.</given-names></name>
<name><surname>Chen</surname><given-names>W.</given-names></name>
<name><surname>Mukhopadhyay</surname><given-names>S.C.</given-names></name>
<name><surname>Wilaiprasitporn</surname><given-names>T.</given-names></name>
<name><surname>Tripathy</surname><given-names>R.K.</given-names></name>
</person-group><article-title>Wavelet domain optimized Savitzky&#x02013;Golay filter for the removal of motion artifacts from EEG recordings</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2020</year><volume>70</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1109/TIM.2020.3041099</pub-id><pub-id pub-id-type="pmid">33776080</pub-id>
</element-citation></ref><ref id="B76-sensors-25-01222"><label>76.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Topic</surname><given-names>A.</given-names></name>
<name><surname>Russo</surname><given-names>M.</given-names></name>
</person-group><article-title>Emotion recognition based on EEG feature maps through deep learning network</article-title><source>Eng. Sci. Technol. Int. J.</source><year>2021</year><volume>24</volume><fpage>1442</fpage><lpage>1454</lpage><pub-id pub-id-type="doi">10.1016/j.jestch.2021.03.012</pub-id></element-citation></ref><ref id="B77-sensors-25-01222"><label>77.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>G.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
</person-group><article-title>Emotion feature analysis and recognition based on reconstructed EEG sources</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>11907</fpage><lpage>11916</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.2966144</pub-id></element-citation></ref><ref id="B78-sensors-25-01222"><label>78.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mahaldar</surname><given-names>O.</given-names></name>
<name><surname>Aditya</surname><given-names>S.</given-names></name>
</person-group><article-title>Gender differences in brain activity during exposure to emotional film clips: An EEG study</article-title><source>Cogn. Brain Behav.</source><year>2017</year><volume>21</volume><fpage>29</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.24193/cbb.2017.21.03</pub-id></element-citation></ref><ref id="B79-sensors-25-01222"><label>79.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Stevens</surname><given-names>J.S.</given-names></name>
<name><surname>Hamann</surname><given-names>S.</given-names></name>
</person-group><article-title>Sex differences in brain activation to emotional stimuli: A meta-analysis of neuroimaging studies</article-title><source>Neuropsychologia</source><year>2012</year><volume>50</volume><fpage>1578</fpage><lpage>1593</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.03.011</pub-id><pub-id pub-id-type="pmid">22450197</pub-id>
</element-citation></ref><ref id="B80-sensors-25-01222"><label>80.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>K.</given-names></name>
<name><surname>Hai</surname><given-names>M.</given-names></name>
<name><surname>Cai</surname><given-names>P.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Preliminary Study on Gender Differences in EEG-Based Emotional Responses in Virtual Architectural Environments</article-title><source>Buildings</source><year>2024</year><volume>14</volume><elocation-id>2884</elocation-id><pub-id pub-id-type="doi">10.3390/buildings14092884</pub-id></element-citation></ref><ref id="B81-sensors-25-01222"><label>81.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Romeo</surname><given-names>Z.</given-names></name>
<name><surname>Angrilli</surname><given-names>A.</given-names></name>
<name><surname>Spironelli</surname><given-names>C.</given-names></name>
</person-group><article-title>Gender effect in affective processing: Alpha EEG source analysis on emotional slides and film-clips</article-title><source>Psychophysiology</source><year>2024</year><volume>61</volume><fpage>e14568</fpage><pub-id pub-id-type="doi">10.1111/psyp.14568</pub-id><pub-id pub-id-type="pmid">38467579</pub-id>
</element-citation></ref><ref id="B82-sensors-25-01222"><label>82.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kring</surname><given-names>A.M.</given-names></name>
<name><surname>Gordon</surname><given-names>A.H.</given-names></name>
</person-group><article-title>Sex differences in emotion: Expression, experience, and physiology</article-title><source>J. Personal. Soc. Psychol.</source><year>1998</year><volume>74</volume><fpage>686</fpage><lpage>703</lpage><pub-id pub-id-type="doi">10.1037/0022-3514.74.3.686</pub-id></element-citation></ref><ref id="B83-sensors-25-01222"><label>83.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rani</surname><given-names>P.</given-names></name>
<name><surname>Liu</surname><given-names>C.</given-names></name>
<name><surname>Sarkar</surname><given-names>N.</given-names></name>
<name><surname>Vanman</surname><given-names>E.</given-names></name>
</person-group><article-title>An empirical study of machine learning techniques for affect recognition in human&#x02013;robot interaction</article-title><source>Pattern Anal. Appl.</source><year>2006</year><volume>9</volume><fpage>58</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1007/s10044-006-0025-y</pub-id></element-citation></ref><ref id="B84-sensors-25-01222"><label>84.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Sohaib</surname><given-names>A.T.</given-names></name>
<name><surname>Qureshi</surname><given-names>S.</given-names></name>
<name><surname>Hagelb&#x000e4;ck</surname><given-names>J.</given-names></name>
<name><surname>Hilborn</surname><given-names>O.</given-names></name>
<name><surname>Jer&#x0010d;i&#x00107;</surname><given-names>P.</given-names></name>
</person-group><article-title>Evaluating Classifiers for Emotion Recognition Using EEG</article-title><source>Foundations of Augmented Cognition, Proceedings of the 7th International Conference, AC 2013, Held as Part of HCI International 2013, Las Vegas, NV, USA, 21&#x02013;26 July 2013</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2013</year><pub-id pub-id-type="doi">10.1007/978-3-642-39454-6_53</pub-id></element-citation></ref><ref id="B85-sensors-25-01222"><label>85.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.W.</given-names></name>
<name><surname>Nie</surname><given-names>D.</given-names></name>
<name><surname>Lu</surname><given-names>B.L.</given-names></name>
</person-group><article-title>EEG-based Emotion Recognition Using Frequency Domain Features and Support Vector Machines</article-title><source>Neural Information Processing, Proceedings of the 18th International Conference, ICONIP 2011,Shanghai, China, 13&#x02013;17 November 2011</source><comment>Proceedings, Part I</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2011</year><fpage>734</fpage><lpage>743</lpage></element-citation></ref><ref id="B86-sensors-25-01222"><label>86.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Sourina</surname><given-names>O.</given-names></name>
</person-group><article-title>EEG databases for emotion recognition</article-title><source>Proceedings of the 2013 International Conference on Cyberworlds</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>21&#x02013;23 October 2013</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2013</year><fpage>302</fpage><lpage>309</lpage><pub-id pub-id-type="doi">10.1109/CW.2013.52</pub-id></element-citation></ref><ref id="B87-sensors-25-01222"><label>87.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Ro</surname><given-names>T.</given-names></name>
<name><surname>Zhu</surname><given-names>Z.</given-names></name>
</person-group><article-title>Emotion recognition with audio, video, EEG, and EMG: A dataset and baseline approaches</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>13229</fpage><lpage>13242</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3146729</pub-id></element-citation></ref><ref id="B88-sensors-25-01222"><label>88.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Suhaimi</surname><given-names>N.S.</given-names></name>
<name><surname>Mountstephens</surname><given-names>J.</given-names></name>
<name><surname>Teo</surname><given-names>J.</given-names></name>
</person-group><article-title>A dataset for emotion recognition using virtual reality and EEG (DER-VREEG): Emotional state classification using low-cost wearable VR-EEG headsets</article-title><source>Big Data Cogn. Comput.</source><year>2022</year><volume>6</volume><elocation-id>16</elocation-id><pub-id pub-id-type="doi">10.3390/bdcc6010016</pub-id></element-citation></ref><ref id="B89-sensors-25-01222"><label>89.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Gross</surname><given-names>J.J.</given-names></name>
<name><surname>Richards</surname><given-names>J.M.</given-names></name>
<name><surname>John</surname><given-names>O.P.</given-names></name>
</person-group><article-title>Emotion regulation in everyday life</article-title><source>Emotion Regulation in Couples and Families: Pathways to Dysfunction and Health</source><person-group person-group-type="editor">
<name><surname>Snyder</surname><given-names>D.K.</given-names></name>
<name><surname>Simpson</surname><given-names>J.</given-names></name>
<name><surname>Hughes</surname><given-names>J.N.</given-names></name>
</person-group><publisher-name>American Psychological Association</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2006</year><fpage>13</fpage><lpage>35</lpage></element-citation></ref><ref id="B90-sensors-25-01222"><label>90.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nolen-Hoeksema</surname><given-names>S.</given-names></name>
</person-group><article-title>Emotion regulation and psychopathology: The role of gender</article-title><source>Annu. Rev. Clin. Psychol.</source><year>2012</year><volume>8</volume><fpage>161</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1146/annurev-clinpsy-032511-143109</pub-id><pub-id pub-id-type="pmid">22035243</pub-id>
</element-citation></ref><ref id="B91-sensors-25-01222"><label>91.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Parvaz</surname><given-names>M.A.</given-names></name>
<name><surname>MacNamara</surname><given-names>A.</given-names></name>
<name><surname>Goldstein</surname><given-names>R.Z.</given-names></name>
<name><surname>Hajcak</surname><given-names>G.</given-names></name>
</person-group><article-title>Event-related induced frontal alpha as a marker of lateral prefrontal cortex activation during cognitive reappraisal</article-title><source>Cogn. Affect. Behav. Neurosci.</source><year>2012</year><volume>12</volume><fpage>730</fpage><lpage>740</lpage><pub-id pub-id-type="doi">10.3758/s13415-012-0107-9</pub-id><pub-id pub-id-type="pmid">22773414</pub-id>
</element-citation></ref><ref id="B92-sensors-25-01222"><label>92.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>W.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>P.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
<name><surname>Liu</surname><given-names>S.</given-names></name>
</person-group><article-title>Gender Differences in Hierarchical Cognitive Control of Emotion: An EEG Study</article-title><source>Proceedings of the 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>15&#x02013;19 July 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/EMBC53108.2024.10782118</pub-id></element-citation></ref><ref id="B93-sensors-25-01222"><label>93.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tolegenova</surname><given-names>A.A.</given-names></name>
<name><surname>Kustubayeva</surname><given-names>A.M.</given-names></name>
<name><surname>Matthews</surname><given-names>G.</given-names></name>
</person-group><article-title>Trait meta-mood, gender and EEG response during emotion-regulation</article-title><source>Personal. Individ. Differ.</source><year>2014</year><volume>65</volume><fpage>75</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.paid.2014.01.028</pub-id></element-citation></ref><ref id="B94-sensors-25-01222"><label>94.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Im</surname><given-names>S.</given-names></name>
<name><surname>Jin</surname><given-names>G.</given-names></name>
<name><surname>Jeong</surname><given-names>J.</given-names></name>
<name><surname>Yeom</surname><given-names>J.</given-names></name>
<name><surname>Jekal</surname><given-names>J.</given-names></name>
<name><surname>Cho</surname><given-names>J.A.</given-names></name>
<name><surname>Lee</surname><given-names>C.H.</given-names></name>
</person-group><article-title>Gender differences in aggression-related responses on EEG and ECG</article-title><source>Exp. Neurobiol.</source><year>2018</year><volume>27</volume><elocation-id>526</elocation-id><pub-id pub-id-type="doi">10.5607/en.2018.27.6.526</pub-id><pub-id pub-id-type="pmid">30636903</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01222-f001"><label>Figure 1</label><caption><p>Russel Circumplex Emotion Model [<xref rid="B18-sensors-25-01222" ref-type="bibr">18</xref>].</p></caption><graphic xlink:href="sensors-25-01222-g001" position="float"/></fig><fig position="float" id="sensors-25-01222-f002"><label>Figure 2</label><caption><p>Block Diagram of Experimental Process: From Stimuli Development to Data Acquisition.</p></caption><graphic xlink:href="sensors-25-01222-g002" position="float"/></fig><fig position="float" id="sensors-25-01222-f003"><label>Figure 3</label><caption><p>Illustration of Guilt, Shame, Regret and Remorse [<xref rid="B48-sensors-25-01222" ref-type="bibr">48</xref>].</p></caption><graphic xlink:href="sensors-25-01222-g003" position="float"/></fig><fig position="float" id="sensors-25-01222-f004"><label>Figure 4</label><caption><p>Title Page and Stimuli Storyboard.</p></caption><graphic xlink:href="sensors-25-01222-g004" position="float"/></fig><fig position="float" id="sensors-25-01222-f005"><label>Figure 5</label><caption><p>Emotion Assessment Dialogue Box Displayed After Each Stimulus.</p></caption><graphic xlink:href="sensors-25-01222-g005" position="float"/></fig><fig position="float" id="sensors-25-01222-f006"><label>Figure 6</label><caption><p>Block Diagram Illustration for Data Training.</p></caption><graphic xlink:href="sensors-25-01222-g006" position="float"/></fig><fig position="float" id="sensors-25-01222-f007"><label>Figure 7</label><caption><p>ROC Curve of Initial Trial.</p></caption><graphic xlink:href="sensors-25-01222-g007" position="float"/></fig><fig position="float" id="sensors-25-01222-f008"><label>Figure 8</label><caption><p>Performance Analysis of Multiple trials via Data Engineering Approach.</p></caption><graphic xlink:href="sensors-25-01222-g008" position="float"/></fig><fig position="float" id="sensors-25-01222-f009"><label>Figure 9</label><caption><p>ROC curves. (<bold>a</bold>) ROC curve of Male Participants Data using Data Engineering Approach. (<bold>b</bold>) ROC Curve of Female Participants Data using Data Engineering Approach.</p></caption><graphic xlink:href="sensors-25-01222-g009" position="float"/></fig><fig position="float" id="sensors-25-01222-f010"><label>Figure 10</label><caption><p>Average Accuracies Acquired by Data Engineering and Feature Extraction Approaches.</p></caption><graphic xlink:href="sensors-25-01222-g010" position="float"/></fig><table-wrap position="float" id="sensors-25-01222-t001"><object-id pub-id-type="pii">sensors-25-01222-t001_Table 1</object-id><label>Table 1</label><caption><p>Guilt-Inducing Events Collected from Participants.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Event</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Friend Suffered Because of Me:</bold> Feeling problems in an exam, I asked my friend for help. She got caught, her marks were deducted, and she lost her scholarship.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Didn&#x02019;t Perform Well in a Group Assignment:</bold> I was unable to perform well in a group assignment, which was submitted late. As a result, every group member suffered because of me.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Missed my Grandfather after His Demise:</bold> I always thought of visiting him but kept delaying it to finish my chores. Unfortunately, he passed away while I kept postponing.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Lied to My Mother Regarding Test Scores:</bold> I got bad marks in my test and lied to my mother about them. She believed me.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Damaged Someone&#x02019;s Property:</bold> At the mall, I played a shooting game and accidentally broke the gun. I returned it to the shopkeeper without admitting the damage.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Bad Career Choice:</bold> I got admission to two universities but chose one because my best friend joined it. Now, I regret not choosing the other option.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Misbehaved with a Poor Person:</bold> A beggar knocked on my door while I was watching a movie. Without listening to him, I scolded him.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Being Rude to Parents:</bold> My mother asked for my help, but I spoke to her very rudely.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Refused to Help a Friend in Need:</bold> A friend urgently needed my bike, but I lied about it having fuel issues to avoid helping.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Prioritized Fun over Family Responsibilities:</bold> My mother managed chaos at home alone because she thought I was studying at a friend&#x02019;s place, but I was actually watching a movie.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01222-t002"><object-id pub-id-type="pii">sensors-25-01222-t002_Table 2</object-id><label>Table 2</label><caption><p>Neutral-Emotion Inducing Events.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Event</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Going for a walk:</bold> I ate a lot today, feeling that my abdomen was full. I went for a walk [<xref rid="B55-sensors-25-01222" ref-type="bibr">55</xref>].</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>An interaction with a stranger</bold> [<xref rid="B56-sensors-25-01222" ref-type="bibr">56</xref>].</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Riding a bicycle</bold> [<xref rid="B60-sensors-25-01222" ref-type="bibr">60</xref>].</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Went for shopping:</bold> It was too cold. I didn&#x02019;t have enough socks. I left for shopping [<xref rid="B56-sensors-25-01222" ref-type="bibr">56</xref>].</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Having dinner at home:</bold> I came late at home and asked for dinner.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>How to avoid COVID-19:</bold> Wash your hands regularly.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Having fun with a sibling:</bold> Please don&#x02019;t tell mother I broke the glass. Ok, then say &#x02018;I am the best sibling in the world&#x02019;.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Hiring Bykea to go home:</bold> I was done with my work then hired Bykea to go home.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Discussion with a friend about yesterday&#x02019;s lecture:</bold> I was absent, please share the previous lecture with me.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Daily routine life:</bold> Start the day with exercise, then listening to music and mopping the house, and at the end some yoga.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01222-t003"><object-id pub-id-type="pii">sensors-25-01222-t003_Table 3</object-id><label>Table 3</label><caption><p>Data Samples&#x02019; Description for Classification&#x02014;Data Engineering.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Data Samples</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total Guilt Sample (GS)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">876 samples</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GS Dimension</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">868 by 16</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GAHC Dimension</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R &#x000d7; 868 by 16</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total Control Sample (CS)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">516 samples</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CS Dimension</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">868 by 16</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CAHC Dimension</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R &#x000d7; 868 by 16</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01222-t004"><object-id pub-id-type="pii">sensors-25-01222-t004_Table 4</object-id><label>Table 4</label><caption><p>Randomly Selected Data Samples for Classification: Initial trial.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sample</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Randomly Selected Row</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sample Dimension</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GAHC</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32, 743, 817, 593, 661, 648, 342, 570, 149, 613, 28, 240, 40, 84, 710, 599</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13,888 by 16</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CAHC</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">144, 282, 492, 495, 81, 496, 488, 247, 406, 72, 213, 462, 399, 482, 329</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13,888 by 16</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01222-t005"><object-id pub-id-type="pii">sensors-25-01222-t005_Table 5</object-id><label>Table 5</label><caption><p>Initial trial Performance Metrics with 5-fold Cross-Validation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">5-Fold</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 Score</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Average</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.76</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01222-t006"><object-id pub-id-type="pii">sensors-25-01222-t006_Table 6</object-id><label>Table 6</label><caption><p>Performance Metrics: Second Approach&#x02014;Feature Extraction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 Score</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.63</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.72</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.83</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01222-t007"><object-id pub-id-type="pii">sensors-25-01222-t007_Table 7</object-id><label>Table 7</label><caption><p>Acquired Accuracy via Second Approach&#x02014;Feature Extraction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Calculated Features</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Gamma <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi mathvariant="sans-serif-bold-italic">&#x003b3;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Beta <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi mathvariant="sans-serif-bold-italic">&#x003b2;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Alpha <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi mathvariant="sans-serif-bold-italic">&#x003b1;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Theta <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi mathvariant="sans-serif-bold-italic">&#x003b8;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Delta <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi mathvariant="sans-serif-bold-italic">&#x003b4;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">All</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hjorth Activity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hjorth Complexity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.9%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.9%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hjorth Mobility</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.9%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.9%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Band Power</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Entropy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.9%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.9%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.9%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.5%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All Features</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.4%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01222-t008"><object-id pub-id-type="pii">sensors-25-01222-t008_Table 8</object-id><label>Table 8</label><caption><p>Achieved Accuracy Across Different Frequency Bands for Males and Females using Feature Extraction Approach.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Band</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Men (Accuracy)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Women (Accuracy)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Gamma</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.55399&#x02013;0.6763</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6115&#x02013;0.7338</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Beta</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5755&#x02013;0.6286</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6115&#x02013;0.7338</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Alpha</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5612&#x02013;0.6763</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6043&#x02013;0.6906</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Theta</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5899&#x02013;0.6331</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5683&#x02013;0.7143</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Delta</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5612&#x02013;0.6691</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5783&#x02013;0.6244</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01222-t009"><object-id pub-id-type="pii">sensors-25-01222-t009_Table 9</object-id><label>Table 9</label><caption><p>Summary of Studies Analyzing Guilt using Different Modalities.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Studies</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Emotions</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Modalities</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Findings</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bhushan et al. [<xref rid="B47-sensors-25-01222" ref-type="bibr">47</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Guilt</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Facial clues</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neck touching (a gesture not a<break/>
confirmed facial clue)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Julle et al. [<xref rid="B48-sensors-25-01222" ref-type="bibr">48</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Guilt, Shame, Remorse</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thermal signals</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Difference of 0.5 &#x000b0;C or more<break/>
in face regions when guilt was<break/>
felt, compared to shame<break/>
and remorse.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Emotion Recognition System<break/>
(1st Data Processing Approach)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Guilt</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEG signals</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy of 63%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Emotion Recognition System<break/>
(2nd Data Processing Approach)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Guilt</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEG signals</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy of 83%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01222-t010"><object-id pub-id-type="pii">sensors-25-01222-t010_Table 10</object-id><label>Table 10</label><caption><p>Overview of Emotion Classification Systems Utilizing the Self-Collected Data Set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Study</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Emotions <break/>
Explored</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number of <break/>
Participants</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Classifier</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Findings</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sohaib et al. [<xref rid="B84-sensors-25-01222" ref-type="bibr">84</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Valence, Arousal,<break/>
Dominance</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15<break/>
05</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM<break/>
SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56 %<break/>
66 %</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wang et al. [<xref rid="B85-sensors-25-01222" ref-type="bibr">85</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Joy, Relaxation, Sadness, Fear</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">05</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.51%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Liu et al. [<xref rid="B86-sensors-25-01222" ref-type="bibr">86</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Protection, Satisfaction, Delight,<break/>
Surprise, Unhappiness, Feeling Unconcerned,<break/>
Feeling Petrified, Annoyance</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.7%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Chen et al. [<xref rid="B87-sensors-25-01222" ref-type="bibr">87</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anger, Contempt, Sadness<break/>
Disgust, Fear, Joy, Amazement</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM<break/>
KNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35%<break/>
36%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Suhaimi et al. [<xref rid="B88-sensors-25-01222" ref-type="bibr">88</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Calm, Happiness, Fear, Boredom</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Emotion Recognition System<break/>
(1st Data Processing Approach)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Guilt</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEG signals</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Emotion Recognition System<break/>
(2nd Data Processing Approach)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Guilt</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEG signals</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01222-t011"><object-id pub-id-type="pii">sensors-25-01222-t011_Table 11</object-id><label>Table 11</label><caption><p>Findings of Studies Investigating Gender Differences in Emotional Responses within EEG Emotion Systems.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Author</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Finding</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Li et al. [<xref rid="B80-sensors-25-01222" ref-type="bibr">80</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Males are more responsive to positive environments, and females are more sensitive to discomforting environments.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Zhang et al. [<xref rid="B92-sensors-25-01222" ref-type="bibr">92</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Females respond more quickly than males in low-level tasks, but not in high-level tasks.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tolegenova et al. [<xref rid="B93-sensors-25-01222" ref-type="bibr">93</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Females showed higher Theta values in the reappraisal condition.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Im et al. [<xref rid="B94-sensors-25-01222" ref-type="bibr">94</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Methodology</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Female participants displayed higher accuracies across all bands except the Delta band.</td></tr></tbody></table></table-wrap></floats-group></article>