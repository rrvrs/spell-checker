<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Audiol Res</journal-id><journal-id journal-id-type="iso-abbrev">Audiol Res</journal-id><journal-id journal-id-type="publisher-id">audiolres</journal-id><journal-title-group><journal-title>Audiology Research</journal-title></journal-title-group><issn pub-type="ppub">2039-4330</issn><issn pub-type="epub">2039-4349</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40407673</article-id><article-id pub-id-type="pmc">PMC12101244</article-id>
<article-id pub-id-type="doi">10.3390/audiolres15030059</article-id><article-id pub-id-type="publisher-id">audiolres-15-00059</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Preliminary Investigation of a Novel Measure of Speech Recognition in Noise</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Thibodeau</surname><given-names>Linda</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><xref rid="c1-audiolres-15-00059" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Freeman</surname><given-names>Emma</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role></contrib><contrib contrib-type="author"><name><surname>Kronenberger</surname><given-names>Kristin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role></contrib><contrib contrib-type="author"><name><surname>Suarez</surname><given-names>Emily</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role></contrib><contrib contrib-type="author"><name><surname>Kim</surname><given-names>Hyun-Woong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role></contrib><contrib contrib-type="author"><name><surname>Qi</surname><given-names>Shuang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role></contrib><contrib contrib-type="author"><name><surname>Lee</surname><given-names>Yune Sang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Zhang</surname><given-names>Yang</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-audiolres-15-00059">Department of Speech, Language, and Hearing, School of Behavioral and Brain Sciences, University of Texas at Dallas, Richardson, TX 75080, USA</aff><author-notes><corresp id="c1-audiolres-15-00059"><label>*</label>Correspondence: <email>thib@utdallas.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>13</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>6</month><year>2025</year></pub-date><volume>15</volume><issue>3</issue><elocation-id>59</elocation-id><history><date date-type="received"><day>16</day><month>2</month><year>2025</year></date><date date-type="rev-recd"><day>25</day><month>4</month><year>2025</year></date><date date-type="accepted"><day>25</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p><bold>Background/Objectives:</bold> Previous research has shown that listeners may use acoustic cues for speech processing that are perceived during brief segments in the noise when there is an optimal signal-to-noise ratio (SNR). This &#x0201c;glimpsing&#x0201d; effect requires higher cognitive skills than the speech tasks used in typical audiometric evaluations. Purpose: The aim of this study was to investigate the use of an online test of speech processing in noise in listeners with typical hearing sensitivity (TH, defined as thresholds &#x02264; 25 dB HL) who were asked to determine the gender of the subject in sentences that were presented in increasing levels of continuous and interrupted noise. <bold>Methods:</bold> This was a repeated-measures design with three factors (SNR, noise type, and syntactic complexity). Study Sample: Participants with self-reported TH (N = 153, ages 18&#x02013;39 years, mean age = 20.7 years) who passed an online hearing screening were invited to complete an online questionnaire. Data Collection and Analysis: Participants completed a sentence recognition task under four SNRs (&#x02212;6, &#x02212;9, &#x02212;12, and &#x02212;15 dB), two syntactic complexity settings (subjective-relative and objective-relative center-embedded), and two noise types (interrupted and continuous). They were asked to listen to 64 sentences through their own headphones/earphones that were presented in an online format at a user-selected comfortable listening level. Their task was to identify the gender of the person performing the action in each sentence. <bold>Results:</bold> Significant main effects of all three factors as well as the SNR by noise-type two-way interaction were identified (<italic toggle="yes">p</italic> &#x0003c; 0.05). This interaction indicated that the effect of SNR on sentence comprehension was more pronounced in the continuous noise compared to the interrupted noise condition. <bold>Conclusions:</bold> Listeners with self-reported TH benefited from the glimpsing effect in the interrupted noise even under low SNRs (i.e., &#x02212;15 dB). The evaluation of glimpsing may be a sensitive measure of auditory processing beyond the traditional word recognition used in clinical evaluations in persons who report hearing challenges and may hold promise for the development of auditory training programs.</p></abstract><kwd-group><kwd>speech recognition</kwd><kwd>hearing tests</kwd><kwd>signal-to-noise ratio</kwd><kwd>syntactic complexity</kwd><kwd>auditory training</kwd><kwd>aural rehabilitation</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-audiolres-15-00059"><title>1. Introduction</title><p>In noisy environments, such as restaurants, a crowded room, or an elementary classroom, many factors may negatively impact speech perception. Physical room characteristics may include detrimental effects of reverberation caused by high ceilings and hard floors. Multiple speakers talking simultaneously increases background noise or sudden loud impulse noises can occur in the surrounding environment. These characteristics decrease the signal-to-noise ratio (SNR), making the speaker&#x02019;s voice less intense than the background noise. A more challenging SNR makes it difficult to communicate and hampers speech perception in those with typical hearing (TH, defined as thresholds &#x02264; 25 dB HL) and can make it more difficult for those with hearing loss [<xref rid="B1-audiolres-15-00059" ref-type="bibr">1</xref>]. It is known that speech recognition improves as the SNR increases [<xref rid="B2-audiolres-15-00059" ref-type="bibr">2</xref>]. However, in the traditional clinical audiological evaluation, speech recognition testing is performed in quiet and using single words that listeners are asked to repeat. Furthermore, in the real world, background noise is often fluctuating, which may allow listeners to receive useful cues during the weaker components of the competition. These cues may be held in short-term memory and combined over time to determine meaning. This skill has been referred to as &#x0201c;glimpsing&#x0201d; and has not been included as part of routine clinical audiological assessments.</p><p>Glimpsing is a technique that allows an individual to gain meaning from a speech signal by utilizing speech cues that occur in the silent periods or gaps within the noise. When listening in challenging noisy environments such as a crowded restaurant, persons typically use glimpsing when the nearby competing conversation periodically subsides in order to hear the acoustic cues from their communication partners. As the glimpsing process continues, persons will combine the fragments received in the gaps in noise to gain meaning. This process was first described by Miller and Licklider in 1950, where they determined whether interrupting acoustic speech signal in different manners influenced the perception for the listener. It was concluded that in regularly spaced bursts of noise, the higher the SNR, the higher the accuracy of recognizing speech in the presence of background noise [<xref rid="B3-audiolres-15-00059" ref-type="bibr">3</xref>]. This foundational work also indicated a negative relationship between speech recognition and the frequency of interruptions. When the interruptions occurred with greater frequency, the gaps of silence became smaller, and speech recognition declined. Many studies have since been conducted to determine if glimpsing is an effective way to improve speech recognition scores. Cooke [<xref rid="B4-audiolres-15-00059" ref-type="bibr">4</xref>] conducted a study of 12 different conditions with various SNRs from &#x02212;7 to +8 dB, frequency modulations, and amplitude modulations and demonstrated the proportion of glimpsing could work as a good predictor of intelligibility. It was suggested that the best fit model could predict behavioral data when regions in the acoustic signal with a local SNR &#x0003e; &#x02212;5 dB were considered potential opportunities for glimpsing. Evaluation of the ability to benefit from glimpsing at even more demanding SNRs (&#x02212;6, &#x02212;9, &#x02212;12, and &#x02212;15 dB) would contribute to our understanding of this effect in listeners with TH.</p><p>This ability was studied in persons with hearing loss by Fogerty et al. [<xref rid="B5-audiolres-15-00059" ref-type="bibr">5</xref>] who showed differences in speech recognition scores in the presence of background noise depending on whether the noise was continuous (i.e., being played during the entire duration of the speech signal) or interrupted (i.e., glimpsing). The interrupted signal contained gaps in the noise that allowed parts of the speech signal to be perceived in the absence of noise. Consistently poorer speech recognition performance was reported in older listeners with hearing loss compared to those age-matched peers with TH. They proposed that a decline might be related to impaired temporal processing abilities.</p><p>In addition, &#x0201c;glimpsing&#x0201d; is reported as a skill that can be improved as a learning effect [<xref rid="B6-audiolres-15-00059" ref-type="bibr">6</xref>,<xref rid="B7-audiolres-15-00059" ref-type="bibr">7</xref>]. Specifically, Rhebergen et al. [<xref rid="B7-audiolres-15-00059" ref-type="bibr">7</xref>] found the speech reception threshold improved by about 3&#x02013;4 dB SNR after five replications in interrupted noise relative to the initial threshold. Sullivan et al. [<xref rid="B8-audiolres-15-00059" ref-type="bibr">8</xref>] also reported improvements in glimpsing following three months of auditory training in interrupted noise compared to training in continuous noise in 24 children with moderate-to-severe hearing loss. Similar speech recognition improvement was found for those trained in interrupted noise when tested in both interrupted and continuous noise conditions while only modest improvements existed under interrupted noise tests for those trained in continuous noise. These findings confirmed the benefits of auditory training under interrupted noise on speech recognition in noise for children with hearing loss. It was suggested that glimpsing was a skill that could be improved through training and may serve as a promising clinical application in auditory rehabilitation.</p><p>The ability to use acoustic speech cues available during silent intervals may lead to more sensitive measures of auditory processing. Such results could ultimately result in beneficial training protocols. Such training might involve practicing speech recognition of sentences presented simultaneously with interrupted noise in which the gaps become progressively smaller. Currently, speech-in-noise (SIN) tests are typically performed in continuous background noise and are thought to reflect the individual&#x02019;s capability in the real world [<xref rid="B9-audiolres-15-00059" ref-type="bibr">9</xref>,<xref rid="B10-audiolres-15-00059" ref-type="bibr">10</xref>]. However, there has been limited attention paid to audiometric tests that assess auditory processing beyond mere recognition of a stream of words at the sentence level. Hearing loss is indeed more vulnerable to spoken sentences with a more complex syntactic structure, suggesting interactions between acoustic processing and higher-order cognitive operations [<xref rid="B11-audiolres-15-00059" ref-type="bibr">11</xref>,<xref rid="B12-audiolres-15-00059" ref-type="bibr">12</xref>].</p><p>In contrast to using word repetition, Lee et al. [<xref rid="B13-audiolres-15-00059" ref-type="bibr">13</xref>] used a task that required a rapid analysis of syntactic relations within a sentence. The stimuli were always spoken by a male, but the complexity of sentence stimuli was manipulated by using either subject-relative or object-relative sentences. In a sentence with both a subject and an object, sentence relative (SR) clauses involve the action being performed by the subject, i.e., &#x0201c;Girls that comfort dads are kind&#x0201d;. In contrast, object relative (OR) clauses are object-centered, with the action being performed by the object, resulting in higher syntactic complexity, i.e., &#x0201c;Dads that girls comfort are kind&#x0201d;. For each sentence, the listeners were instructed to indicate whether a male or a female performed the action. This paradigm has been used to demonstrate a robust syntactic effect with a higher-level of difficulty for the OR than the SR sentences. This is presumably due to the increasing working memory load and/or temporal complexity [<xref rid="B14-audiolres-15-00059" ref-type="bibr">14</xref>,<xref rid="B15-audiolres-15-00059" ref-type="bibr">15</xref>,<xref rid="B16-audiolres-15-00059" ref-type="bibr">16</xref>,<xref rid="B17-audiolres-15-00059" ref-type="bibr">17</xref>,<xref rid="B18-audiolres-15-00059" ref-type="bibr">18</xref>].</p><p>Using these syntactically complex sentences in conditions where glimpsing is possible (i.e., interrupted noise) compared to conditions where it is not possible may provide insights into the difficulties that people experience in real-world noisy conditions where they must not only recognize the speech but also cognitively process what they hear. It is expected that as SNR increases from negative to positive values, more acoustic information is received. In addition, if there are gaps in the noise, additional cues may be received which will lead to greater accuracy. By using the cognitively demanding task involving varying sentence structure, additional skills such as short-term memory and syntactic knowledge can be assessed and may more accurately reflect the real-world communication difficulties.</p><p>The aim of this study was to evaluate a novel, auditory speech perception-in-noise test administered online in adults with TH. This test consisted of the SR/OR sentences played in continuous/interrupted noise under four SNRs and required the participant to process the syntax to determine if the action was performed by a male or female. This task involved the retention of important information from each sentence, which is a higher-level task targeting working memory than simple sentence repetition. In addition, the cognitive load increased as the listening became more difficult at the lower SNRs, in the continuous versus interrupted noise, and as the sentence type changed from SR to OR. It is hypothesized that listeners with TH would perform better with higher SNRs and when listening in the interrupted versus continuous noise conditions. Furthermore, listeners might benefit more from the &#x0201c;glimpsing&#x0201d; as shown by the gap between the two noise conditions as the SNR decreased.</p></sec><sec id="sec2-audiolres-15-00059"><title>2. Materials and Methods</title><sec sec-type="subjects" id="sec2dot1-audiolres-15-00059"><title>2.1. Participants</title><p>A total of 158 participants were involved in this study which agreed with the estimated sample size needed to obtain a medium-effect size with an error probability of 0.05 [<xref rid="B19-audiolres-15-00059" ref-type="bibr">19</xref>]. Five responses were deleted due to incomplete information. Responses included 153 adults, aged 18&#x02013;39 years (mean = 20.7 years). Participants were recruited through a university research participant-recruiting platform and email distribution to various student groups. Informed consent was obtained by each participant at the beginning of the survey which was approved by the Institutional Review Board (IRB). All participants had to self-report TH bilaterally as determined with an online hearing screening and complete all test questions. Exclusion criteria included self-reported hearing loss or identified through a hearing screener or inability to complete this study due to unforeseen circumstances and technical difficulties.</p></sec><sec id="sec2dot2-audiolres-15-00059"><title>2.2. Stimuli</title><p>The stimuli were taken from those developed by Lee et al. [<xref rid="B13-audiolres-15-00059" ref-type="bibr">13</xref>] and included a total of 80 sentences (the first 16 were practice). Stimuli from the Lee et al. [<xref rid="B13-audiolres-15-00059" ref-type="bibr">13</xref>] study were mixed with babble noise using Adobe Audition and uploaded to Qualtrics as MP3 files. There were three difficulty variables which included SNR, glimpsing (interrupted vs. continuous noise), and sentence type (SR vs. OR). An example of an SR sentence is &#x0201c;Daughters that charm fathers are joyful&#x0201d; and an example of an OR sentence is &#x0201c;Boys that women scold are mean&#x0201d;. The sentences had been normalized by root-mean-square values (RMS) to minimize the use of intensity variations as perceptual cues. The following four sentence structures were used: OR spoken by a male or female and SR spoken by a male or female.</p><p>To familiarize the participants with the task, a practice set of 16 sentences was presented in quiet and at easy SNRs (0 and +3 dB SNR). Eight sentences were played in continuous and eight in interrupted noise with SR/OR type evenly distributed. The actual test session consisted of four SNRs at &#x02212;6, &#x02212;9, &#x02212;12, and &#x02212;15 dB, with 16 sentences played in each SNR. Among these 16 sentences, 8 were presented in continuous noise (4 SR and 4 OR) and 8 were presented in interrupted noise (4 SR and 4 OR). The stimuli (SNR, noise, and sentence type) were randomized within this study using a random number generator.</p><p>Multi-talker babble noise was used for the background noise stimuli in both continuous and interrupted conditions [<xref rid="B20-audiolres-15-00059" ref-type="bibr">20</xref>]. Using Adobe Audition for audio editing, the original noise file was first decreased in amplitude to avoid peak clipping and was matched to the RMS of the sentences to create the 0 dB SNR condition. The background noise, either continuous or interrupted, was increased systematically and mixed with the sentence stimuli at a steady intensity to create the four SNRs. To avoid identical noise samples mixed with the sentences, a random section of noise was selected to mix with each sentence such that there was one second of noise before and after the sentence. The interrupted noise files were made by taking the noise file and randomly segmented it by sections of 20, 50, 75, or 95 ms of silence to create silent gaps. Sentences were mixed with the interrupted noise files in the same way as for the continuous noise. When the interrupted noise files were played, it created a &#x0201c;choppy&#x0201d; effect that allowed the listener to hear parts of the sentence through the noise.</p></sec><sec id="sec2dot3-audiolres-15-00059"><title>2.3. Procedure</title><p>Anonymous responses were collected through the online web-based platform Qualtrics supported by the University of Texas at Dallas in the order shown in <xref rid="audiolres-15-00059-f001" ref-type="fig">Figure 1</xref>. Participants were allowed to complete the survey at their home or in any quiet location with minimal interruptions. They were asked to use wired or wireless stereo headphones or earphones. After completing the consent and basic demographic information, the participants were redirected by a link to the online hearing screener ShoeboxOnline/Signia (<uri xlink:href="https://www.signia.net/en-us/service/hearing-test/">https://www.signia.net/en-us/service/hearing-test/</uri> (accessed on 1 September 2023)). Following some questions regarding general hearing abilities, the user was asked to use headphones (wired or wireless) and to adjust the volume of the computer to max level. First, continuous speech was presented followed by tones with each ear tested separately. The user adjusted the volume buttons on the screen to the highest comfortable level and the lowest level that speech could be understood. The final step was the volume adjustment for barely audible warbled tones. The screener would record a hearing level as &#x0201c;normal&#x0201d;, &#x0201c;loss&#x0201d;, or &#x0201c;severe loss&#x0201d;. Participants were instructed to discontinue this study if they reported their hearing acuity as a &#x0201c;loss&#x0201d; or &#x0201c;severe loss&#x0201d; for either ear.</p><p>Prior to listening to the stimuli, participants were asked to play a single clip of multi-talker babble and adjust the volume of their headphones/earphones to a comfortable listening level using the volume slider on the device they were using. They were instructed to leave their volume here for the remainder of this study. In total, this study took approximately 1 h to complete. After the volume was adjusted, a practice section was completed first to ensure the participants understood the task and timing of the questions. Correct answer feedback was provided during these 16 questions. Participants were instructed to remain in a quiet environment without interruptions; if this was not possible for the length of the entire task, they were instructed to please exit and try again at another time. Each screen was timed and would automatically advance to the next question after 10 s to prevent the participants from playing the sentence more than once. The practice questions were also timed to allow the practice of both listening to the stimuli and selecting the answer choice they heard in the appropriate amount of time. Participants were required to meet the practice criteria of 10/16 questions correctly answered to advance to the main study questions. Because the task of identifying the subject is not as common as repeating words, performance at 63% accuracy was selected as an indication that participants understood and could attend to the task.</p><p>The practice and main survey questions took about one hour to complete and involved listening to the sentences spoken by a male with particular attention to who was performing the action, a male or female. The instructions were &#x0201c;Please play the following clip and identify the gender of the people performing the action: male or female&#x0201d;. For example, the answer to this OR sentence, &#x0201c;Girls that dads comfort are kind&#x0201d;, was male, as the dads are performing the action (comfort). Each sentence was made into both OR and SR syntax. For example, the above OR sentence was also presented in SR form as &#x0201c;Girls that comfort dads are kind&#x0201d; with a female conducting the action in this sentence. This required cognitive processing and working memory as the participants did not know which sentence structure they would receive each time.</p><p>Four SR and four OR sentences were used per SNR level for continuous noise and also for interrupted noise. These stimuli were randomly placed within the Qualtrics study. Participants were given ten seconds to answer the survey questions before the page auto-advanced. If a question was not answered in the allotted time, then the trial was scored as incorrect. Questions were automatically scored into categories based on four SNR levels (&#x02212;6, &#x02212;9, &#x02212;12, &#x02212;15), two noise types (continuous and interrupted), and two sentence types (SR and OR).</p></sec><sec id="sec2dot4-audiolres-15-00059"><title>2.4. Data Analysis</title><p>The accuracy of individual trials was entered into a mixed-effect logistic regression model using the glmer function of the lme4 package [<xref rid="B21-audiolres-15-00059" ref-type="bibr">21</xref>], to analyze participants&#x02019; comprehension performance. The model included sentence type (SR and OR), noise type (continuous and interrupted), SNR (&#x02212;6, &#x02212;9, &#x02212;12, and &#x02212;15 dB), the respective two-way and three-way interactions as fixed factors, and item and participant as random intercepts. Effect sizes were determined according to Cohen&#x02019;s <italic toggle="yes">d</italic> values.</p></sec></sec><sec sec-type="results" id="sec3-audiolres-15-00059"><title>3. Results</title><p>To examine the results of the online measurement of speech recognition in noise, the performance for the two sentence types (SR vs. OR) presented in two types of noise (interrupted and continuous) at four SNR levels (&#x02212;6, &#x02212;9, &#x02212;12, &#x02212;15 dB) was reviewed. As expected, all three main effects were significant. Interactions were non-significant except for the noise type by SNR. The group-averaged accuracy for each combination of stimulus conditions is shown in <xref rid="audiolres-15-00059-f002" ref-type="fig">Figure 2</xref> and <xref rid="audiolres-15-00059-t001" ref-type="table">Table 1</xref>. The main effect of sentence type was significant (<italic toggle="yes">b</italic> = 0.50, <italic toggle="yes">SE</italic> = 0.09, <italic toggle="yes">z</italic> = 5.53, <italic toggle="yes">p</italic> &#x0003c; 0.001, small Cohen&#x02019;s <italic toggle="yes">d</italic> = 0.47), indicating lower accuracy for the OR than SR sentences. We also found the main effects of noise type (<italic toggle="yes">b</italic> = 0.80, <italic toggle="yes">SE</italic> = 0.09, <italic toggle="yes">z</italic> = 8.74, <italic toggle="yes">p</italic> &#x0003c; 0.001, large Cohen&#x02019;s <italic toggle="yes">d</italic> = 0.90) and SNR (<italic toggle="yes">b</italic> = 0.51, <italic toggle="yes">SE</italic> = 0.08, <italic toggle="yes">z</italic> = 6.21, <italic toggle="yes">p</italic> &#x0003c; 0.001), indicating that participants performed worse with a lower SNR and when the noise was continuous compared to interrupted. Importantly, the noise type significantly interacted with the SNR factor (<italic toggle="yes">b</italic> = &#x02212;0.37, <italic toggle="yes">SE</italic> = 0.08, <italic toggle="yes">z</italic> = &#x02212;4.57, <italic toggle="yes">p</italic> &#x0003c; 0.001), indicating that the effect of SNR on sentence comprehension was more pronounced in the continuous noise compared to the interrupted noise condition (<xref rid="audiolres-15-00059-f002" ref-type="fig">Figure 2</xref>). However, the two-way interactions including the sentence type as well as the three-way interaction were not significant (sentence type &#x000d7; noise type: <italic toggle="yes">b</italic> = &#x02212;0.14, <italic toggle="yes">SE</italic> = 0.09, <italic toggle="yes">z</italic> = 1.55, <italic toggle="yes">p</italic> = 0.121, sentence type &#x000d7; SNR: <italic toggle="yes">b</italic> = &#x02212;0.07, <italic toggle="yes">SE</italic> = 0.08, <italic toggle="yes">z</italic> = &#x02212;0.82, <italic toggle="yes">p</italic> = 0.414, and sentence type &#x000d7; noise type &#x000d7; SNR: <italic toggle="yes">b</italic> = &#x02212;0.02, <italic toggle="yes">SE</italic> = 0.08, <italic toggle="yes">z</italic> = &#x02212;0.23, <italic toggle="yes">p</italic> = 0.818). The non-significant two-way interactions with sentence type were not surprising given the previous research showing the greater difficulty with OR relative to SR sentences. With this consistent pattern of OR performance lower than SR performance, the lack of a significant three-way interaction was also not surprising.</p></sec><sec sec-type="discussion" id="sec4-audiolres-15-00059"><title>4. Discussion</title><p>In this study, the key findings were the significant main effects of SNR, the noise type, as well as their two-way interactions. As shown in <xref rid="audiolres-15-00059-f002" ref-type="fig">Figure 2</xref>, poorer recognition accuracy was identified with a lower SNR setting, but only when tested with continuous noise. The accuracy score remained high across the four SNRs when tested in interrupted noise. The significant impact of SNR level found in this study was in accordance with the previous studies [<xref rid="B9-audiolres-15-00059" ref-type="bibr">9</xref>,<xref rid="B10-audiolres-15-00059" ref-type="bibr">10</xref>] as well as the glimpsing effect as indicated by the noise type [<xref rid="B4-audiolres-15-00059" ref-type="bibr">4</xref>]. This study extended the range of glimpsing effect to a lower SNR (i.e., &#x02212;15 dB). The significant SNR by noise-type two-way interaction indicated the benefits of the glimpsing effect despite adverse listening conditions. Specifically, even under a demanding listening condition of &#x02212;15 dB SNR, the performance remained over 80%. Relative to the decreased accuracy with lower SNRs, when tested with continuous noise, the benefit of a glimpsing effect increased as the SNR decreased. This suggests that as the perceptual task becomes more difficult, persons with TH can rely more on the cues they receive during the gaps in the interrupted noise.</p><p>Some implications of these findings include the effects of the challenging listening task on the individual such as increased listening effort and the sensitivity of this task to reflect differences in auditory processing. Our results suggested the need to include listening effort measures (i.e., cognitive demands when completing a listening task) in future investigations. In a pilot study conducted in the lab setting, the same stimuli were tested by another 11 participants with TH and all participants were required to report their subject listening effort using the NASA-Task-Load-Index [<xref rid="B22-audiolres-15-00059" ref-type="bibr">22</xref>] after each noise type. Across the four SNRs and two clause structures, listeners reported lower listening effort (52.61%) when tested in interrupted noise relative to continuous noise (70.70%). Although this testing was conducted in the lab, the pilot data imply potentially reduced listening effort by the glimpsing effect.</p><p>The current study also showed a significant impact of syntactic complexity on individual auditory processing performance. As expected, participants had greater accuracy when recognizing the SR-structured sentences compared to OR-structured sentences. Although the perceptual difficulty was set differently (noise type/SNR vs. speech rate/aging/hearing loss), our findings were parallel to Wingfield et al. [<xref rid="B12-audiolres-15-00059" ref-type="bibr">12</xref>]. The performance for the easy conditions in the current study (i.e., interrupted noise/high SNR) was similar to the performance in the Wingfield et al. [<xref rid="B12-audiolres-15-00059" ref-type="bibr">12</xref>] study for the slower rate condition by the younger group/typical hearing. Likewise, the performance in the hard condition in the current study (i.e., continuous noise/lower SNR) was similar to that in the Wingfield et al. study [<xref rid="B12-audiolres-15-00059" ref-type="bibr">12</xref>] for the fast-rate condition by the older group/hearing loss. This suggests that the SR/OR stimuli may be a sensitive measure of differences in auditory processing. There may be additional factors that influence performance such as linguistic proficiency, working memory, or syntactic familiarity. Future studies could include additional measures of these abilities to determine the contributions of each that should be considered when determining rehabilitative programs and/or technology. If there is a significant contribution of working memory to the task performance, training to improve that skill may be considered. However, if linguistic proficiency showed a stronger contribution, then training may focus more on that ability. In either case, the use of remote microphone technology may provide a more immediate benefit to persons with HD despite TH.</p><p>The greatest limitations of this study relate to the lack of control during online testing. It is expected with online audio testing that variability will naturally occur with the use of various headphones and allowing the user to adjust their volume settings. To determine the range of these variations, a pilot study was conducted. Using the digits-in-noise task at the same SNR values, there was no significant difference when the online stimuli were presented in the lab with one set of headphones versus through individual computers at home with a variety of headphones. Though the home setting could provide the participants with a more comfortable testing environment, it could undermine the data quality that may occur with uncontrolled transducers (headphones, earphones), intensity (selected volume levels were not reported), extraneous environmental sounds, etc. As part of the pilot study, the online screening measure was also compared to traditional audiometric testing in a sound booth and there was agreement for 91% of the participants. A final limitation is that the sensitivity of the measures was constrained by the high probability of guessing the correct answer because there was only two choices (male/female).</p><p>These findings might be beneficial in future clinical applications. It is possible that some training in temporal processing could result in rehabilitation benefits for those with hearing loss who have poor temporal processing. For example, Weissgerber and Baumann [<xref rid="B23-audiolres-15-00059" ref-type="bibr">23</xref>] reported an 8.7 dB decrease in the speech reception threshold using amplitude-modulated masking noise (i.e., &#x0201c;Fastl-noise&#x0201d;) relative to continuous broadband noise (i.e., &#x0201c;Olnoise&#x0201d;). This training effect might be dependent on residual hearing levels. Several researchers have reported an absence of the glimpsing effect in bilateral cochlear implant users or subjects using electric-acoustic stimulation (i.e., vocoded speech) [<xref rid="B23-audiolres-15-00059" ref-type="bibr">23</xref>,<xref rid="B24-audiolres-15-00059" ref-type="bibr">24</xref>,<xref rid="B25-audiolres-15-00059" ref-type="bibr">25</xref>]; meanwhile, others have reported the potential glimpsing effect may benefit bimodal listeners [<xref rid="B24-audiolres-15-00059" ref-type="bibr">24</xref>]. This suggests the need to fill a gap in the literature regarding the role of temporal processing and speech recognition in noise. Examination of the findings in groups with different degrees of residual hearing may provide insights into determining candidacy for temporal processing training.</p><p>Sullivan et al. [<xref rid="B8-audiolres-15-00059" ref-type="bibr">8</xref>] confirmed the potential benefits of a three-month auditory training program under interrupted noise of 0-, 6-, and 12-dB SNR on SIN recognition for children with hearing loss. A 1 dB improvement on the HINT is equivalent to improvements in speech intelligibility by 8.9% [<xref rid="B26-audiolres-15-00059" ref-type="bibr">26</xref>]. Sullivan et al. [<xref rid="B8-audiolres-15-00059" ref-type="bibr">8</xref>] reported an average post-training improvement of 7.46&#x02009;and 7.13 dB when evaluated in interrupted and continuous noise, respectively. This corresponds to a predicted 63% recognition improvement based on the 1 dB to 8.9% transformation. However, both the training and test conditions were at SNRs &#x0003e; 0 dB, which do not reflect the common negative SNR conditions present in daily life. The current findings suggest the extension of the glimpsing benefit to a lower SNR although testing was conducted with listeners with TH. Working memory training may also be an effective way of improving speech perception in noise [<xref rid="B27-audiolres-15-00059" ref-type="bibr">27</xref>]. Following the rationale of Sullivan et al. [<xref rid="B8-audiolres-15-00059" ref-type="bibr">8</xref>], such training may be more effective in interrupted noise compared to continuous noise. When taken together, the results suggest possible promising benefits of auditory training in interrupted noise in lower SNR settings. Similarly, the parallel findings with Wingfield et al. [<xref rid="B12-audiolres-15-00059" ref-type="bibr">12</xref>] also suggest that the perceptual difficulty is impacted by other factors including aging and speech rate.</p><p>Future research may focus on the exploration of the combination of factors including noise type and syntactic complexity for auditory training in clinical rehabilitation programs. Compared to control groups with no auditory training to improve glimpsing skills, individuals who enroll in training programs that are designed to increase working auditory skills during specifically designed noise may show significantly better speech recognition in noise. The training hierarchy might begin with monosyllabic words spaced exactly during the gaps in the background noise and progress to conditions with greater word length and overlapping intermittent noise. The ultimate goal would be to enhance speech recognition in more naturalistic situations such as increasing one&#x02019;s ability to follow a story presented in typical multi-talker babble akin to that encountered in a restaurant. Following the conclusive evidence of the training benefits, the application of training protocols could be implemented by a manufacturer into the corresponding hearing aid application on a smartphone. The application would include a sequence of training levels that would be presented to the user via the personal hearing technology and scored based on responses spoken by the user into the smartphone.</p></sec><sec sec-type="conclusions" id="sec5-audiolres-15-00059"><title>5. Conclusions</title><p>Individuals may encounter communication deficits when listening with background noise despite TH sensitivity. To explore the impact of different factors on individual auditory processing performance in noise, 153 listeners with TH were tested under three perceptual challenges (SNR level, sentence syntax complexity, and noise type). Significant main effects of each difficulty factor as well as the SNR by noise-type two-way interaction were found in the regression model, while the other two-way interactions and three-way interactions were nonsignificant. Auditory processing skills involve higher-level abilities beyond the simple repetition of what was heard especially under adverse listening conditions. Listeners with TH show benefit from the glimpsing effect even under low SNRs (i.e., &#x02212;15 dB). Evaluation of one&#x02019;s ability to use glimpsing with syntactically complex sentences may lead to more sensitive clinical measures of speech processing beyond the typical task of simple repetition. It could be that two persons with the same degree of hearing loss might perform very differently on a task involving glimpsing. Those individuals who score the same on speech processing tasks presented in continuous versus interrupted noise (i.e., limited glimpsing) may be candidates for auditory training. Such training may focus on progressively complex tasks that vary in gap duration in the competing noise, memory load of the sentence length, and/or syntactic complexity. However, further research is needed to establish a paradigm involving glimpsing that may be used as a clinical measure of auditory processing and possible training program in auditory rehabilitation.</p></sec></body><back><ack><title>Acknowledgments</title><p>Appreciation is expressed to the subject recruitment program, SONA, at the University of Texas at Dallas.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, L.T. and Y.S.L.; methodology, L.T, E.F., E.S. and K.K.; software, E.S. and K.K.; validation, K.K., H.-W.K. and S.Q.; formal analysis, H.-W.K. and S.Q.; investigation, L.T. and S.Q.; resources, L.T.; data curation, H.-W.K.; writing&#x02014;original draft preparation, E.F., E.S., K.K., L.T. and S.Q.; writing&#x02014;review and editing, L.T., Y.S.L. and S.Q.; visualization, H.-W.K. and S.Q.; supervision, L.T.; project administration, L.T.; All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>The study was conducted in accordance with the Declaration of Helsinki, and approved by the Institutional Review Board of the University of Texas at Dallas (IRB 21-155 and 2-5-2024; date: 11 March 2024).</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available on request from the corresponding author due to privacy concern.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">dB</td><td align="left" valign="middle" rowspan="1" colspan="1">Decibel</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OR</td><td align="left" valign="middle" rowspan="1" colspan="1">Object relative</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SR</td><td align="left" valign="middle" rowspan="1" colspan="1">Subject relative</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">TH</td><td align="left" valign="middle" rowspan="1" colspan="1">Typical Hearing</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-audiolres-15-00059"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Plomp</surname><given-names>R.</given-names></name>
</person-group><article-title>A signal-to-noise ratio model for the speech-reception threshold of the hearing impaired</article-title><source>J. Speech Lang. Hear. Res.</source><year>1986</year><volume>29</volume><fpage>146</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1044/jshr.2902.146</pub-id><pub-id pub-id-type="pmid">3724108</pub-id>
</element-citation></ref><ref id="B2-audiolres-15-00059"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hornsby</surname><given-names>B.W.Y.</given-names></name>
<name><surname>Ricketts</surname><given-names>T.A.</given-names></name>
</person-group><article-title>The effects of compression ratio, signal-to-noise ratio, and level on speech recognition in normal-hearing listeners</article-title><source>J. Acoust. Soc. Am.</source><year>2001</year><volume>109</volume><fpage>2964</fpage><lpage>2973</lpage><pub-id pub-id-type="doi">10.1121/1.1369105</pub-id><pub-id pub-id-type="pmid">11425138</pub-id>
</element-citation></ref><ref id="B3-audiolres-15-00059"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Miller</surname><given-names>G.A.</given-names></name>
<name><surname>Licklider</surname><given-names>J.C.R.</given-names></name>
</person-group><article-title>The intelligibility of interrupted speech</article-title><source>J. Acoust. Soc. Am.</source><year>1950</year><volume>22</volume><fpage>167</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1121/1.1906584</pub-id></element-citation></ref><ref id="B4-audiolres-15-00059"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cooke</surname><given-names>M.</given-names></name>
</person-group><article-title>A glimpsing model of speech perception in noise</article-title><source>J. Acoust. Soc. Am.</source><year>2006</year><volume>119</volume><fpage>1562</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1121/1.2166600</pub-id><pub-id pub-id-type="pmid">16583901</pub-id>
</element-citation></ref><ref id="B5-audiolres-15-00059"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fogerty</surname><given-names>D.</given-names></name>
<name><surname>Ahlstrom</surname><given-names>J.B.</given-names></name>
<name><surname>Bologna</surname><given-names>W.J.</given-names></name>
<name><surname>Dubno</surname><given-names>J.R.</given-names></name>
</person-group><article-title>Sentence intelligibility during segmental interruption and masking by speech-modulated noise: Effects of age and hearing loss</article-title><source>J. Acoust. Soc. Am.</source><year>2015</year><volume>137</volume><fpage>3487</fpage><lpage>3501</lpage><pub-id pub-id-type="doi">10.1121/1.4921603</pub-id><pub-id pub-id-type="pmid">26093436</pub-id>
</element-citation></ref><ref id="B6-audiolres-15-00059"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rhebergen</surname><given-names>K.S.</given-names></name>
<name><surname>Versfeld</surname><given-names>N.J.</given-names></name>
<name><surname>Dreschler</surname><given-names>W.</given-names></name>
</person-group><article-title>Extended speech intelligibility index for the prediction of the speech reception threshold in fluctuating noise</article-title><source>J. Acoust. Soc. Am.</source><year>2006</year><volume>120</volume><fpage>3988</fpage><lpage>3997</lpage><pub-id pub-id-type="doi">10.1121/1.2358008</pub-id><pub-id pub-id-type="pmid">17225425</pub-id>
</element-citation></ref><ref id="B7-audiolres-15-00059"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rhebergen</surname><given-names>K.S.</given-names></name>
<name><surname>Versfeld</surname><given-names>N.J.</given-names></name>
<name><surname>Dreschler</surname><given-names>W.A.</given-names></name>
</person-group><article-title>Learning effect observed for the speech reception threshold in interrupted noise with normal hearing listeners</article-title><source>Int. J. Audiol.</source><year>2008</year><volume>47</volume><fpage>185</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1080/14992020701883224</pub-id><pub-id pub-id-type="pmid">18389414</pub-id>
</element-citation></ref><ref id="B8-audiolres-15-00059"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sullivan</surname><given-names>J.R.</given-names></name>
<name><surname>Thibodeau</surname><given-names>L.M.</given-names></name>
<name><surname>Assmann</surname><given-names>P.F.</given-names></name>
</person-group><article-title>Auditory training of speech recognition with interrupted and continuous noise maskers by children with hearing impairment</article-title><source>J. Acoust. Soc. Am.</source><year>2013</year><volume>133</volume><fpage>495</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1121/1.4770247</pub-id><pub-id pub-id-type="pmid">23297921</pub-id>
</element-citation></ref><ref id="B9-audiolres-15-00059"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Killion</surname><given-names>M.C.</given-names></name>
<name><surname>Niquette</surname><given-names>P.A.</given-names></name>
<name><surname>Gudmundsen</surname><given-names>G.I.</given-names></name>
<name><surname>Revit</surname><given-names>L.J.</given-names></name>
<name><surname>Banerjee</surname><given-names>S.</given-names></name>
</person-group><article-title>Development of a quick speech-in-noise test for measuring signal-to-noise ratio loss in normal-hearing and hearing-impaired listeners</article-title><source>J. Acoust. Soc. Am.</source><year>2004</year><volume>116</volume><fpage>2395</fpage><lpage>2405</lpage><pub-id pub-id-type="doi">10.1121/1.1784440</pub-id><pub-id pub-id-type="pmid">15532670</pub-id>
</element-citation></ref><ref id="B10-audiolres-15-00059"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Portnuff</surname><given-names>C.</given-names></name>
<name><surname>Bell</surname><given-names>B.</given-names></name>
</person-group><article-title>Effective use of speech-in-noise testing in the clinic</article-title><source>Hear. J.</source><year>2019</year><volume>72</volume><fpage>40</fpage><pub-id pub-id-type="doi">10.1097/01.HJ.0000559502.51932.b1</pub-id></element-citation></ref><ref id="B11-audiolres-15-00059"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wingfield</surname><given-names>A.</given-names></name>
<name><surname>Peelle</surname><given-names>J.E.</given-names></name>
<name><surname>Grossman</surname><given-names>M.</given-names></name>
</person-group><article-title>Speech rate and syntactic complexity as multiplicative factors in speech comprehension by young and older adults</article-title><source>Aging Neuropsychol. Cogn.</source><year>2003</year><volume>10</volume><fpage>310</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1076/anec.10.4.310.28974</pub-id></element-citation></ref><ref id="B12-audiolres-15-00059"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wingfield</surname><given-names>A.</given-names></name>
<name><surname>McCoy</surname><given-names>S.L.</given-names></name>
<name><surname>Peelle</surname><given-names>J.E.</given-names></name>
<name><surname>Tun</surname><given-names>P.A.</given-names></name>
<name><surname>Cox</surname><given-names>C.L.</given-names></name>
</person-group><article-title>Effects of adult aging and hearing loss on comprehension of rapid speech varying in syntactic complexity</article-title><source>J. Am. Acad. Audiol.</source><year>2006</year><volume>17</volume><fpage>487</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.3766/jaaa.17.7.4</pub-id><pub-id pub-id-type="pmid">16927513</pub-id>
</element-citation></ref><ref id="B13-audiolres-15-00059"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>Y.S.</given-names></name>
<name><surname>Ahn</surname><given-names>S.</given-names></name>
<name><surname>Holt</surname><given-names>R.F.</given-names></name>
<name><surname>Schellenberg</surname><given-names>E.G.</given-names></name>
</person-group><article-title>Rhythm and syntax processing in school-age children</article-title><source>Dev. Psychol.</source><year>2020</year><volume>56</volume><fpage>1632</fpage><lpage>1641</lpage><pub-id pub-id-type="doi">10.1037/dev0000969</pub-id><pub-id pub-id-type="pmid">32700950</pub-id>
</element-citation></ref><ref id="B14-audiolres-15-00059"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>H.W.</given-names></name>
<name><surname>Kovar</surname><given-names>J.</given-names></name>
<name><surname>Bajwa</surname><given-names>J.S.</given-names></name>
<name><surname>Mian</surname><given-names>Y.</given-names></name>
<name><surname>Ahmad</surname><given-names>A.</given-names></name>
<name><surname>Moreno</surname><given-names>M.M.</given-names></name>
<name><surname>Price</surname><given-names>T.J.</given-names></name>
<name><surname>Lee</surname><given-names>Y.S.</given-names></name>
</person-group><article-title>Rhythmic motor behavior explains individual differences in grammar skills in adults</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>3710</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-53382-9</pub-id><pub-id pub-id-type="pmid">38355855</pub-id>
</element-citation></ref><ref id="B15-audiolres-15-00059"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>J.</given-names></name>
<name><surname>Kim</surname><given-names>H.W.</given-names></name>
<name><surname>Kovar</surname><given-names>J.</given-names></name>
<name><surname>Lee</surname><given-names>Y.S.</given-names></name>
</person-group><article-title>Neural consequences of binaural beat stimulation on auditory sentence comprehension: An EEG study</article-title><source>Cereb. Cortex.</source><year>2024</year><volume>34</volume><fpage>bhad459</fpage><pub-id pub-id-type="doi">10.1093/cercor/bhad459</pub-id><pub-id pub-id-type="pmid">38044462</pub-id>
</element-citation></ref><ref id="B16-audiolres-15-00059"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>Y.S.</given-names></name>
<name><surname>Wingfield</surname><given-names>A.</given-names></name>
<name><surname>Min</surname><given-names>N.E.</given-names></name>
<name><surname>Kotloff</surname><given-names>E.</given-names></name>
<name><surname>Grossman</surname><given-names>M.</given-names></name>
<name><surname>Peelle</surname><given-names>J.E.</given-names></name>
</person-group><article-title>Differences in hearing acuity among &#x0201c;normal-hearing&#x0201d; young adults modulate the neural basis for speech comprehension</article-title><source>eNeuro</source><year>2018</year><volume>5</volume><elocation-id>e0263-17.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0263-17.2018</pub-id></element-citation></ref><ref id="B17-audiolres-15-00059"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>Y.S.</given-names></name>
<name><surname>Rogers</surname><given-names>C.S.</given-names></name>
<name><surname>Grossman</surname><given-names>M.</given-names></name>
<name><surname>Wingfield</surname><given-names>A.</given-names></name>
<name><surname>Peelle</surname><given-names>J.E.</given-names></name>
</person-group><article-title>Hemispheric dissociations in regions supporting auditory sentence comprehension in older adults</article-title><source>Aging Brain</source><year>2022</year><volume>2</volume><fpage>100051</fpage><pub-id pub-id-type="doi">10.1016/j.nbas.2022.100051</pub-id><pub-id pub-id-type="pmid">36908889</pub-id>
</element-citation></ref><ref id="B18-audiolres-15-00059"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>Y.S.</given-names></name>
<name><surname>Min</surname><given-names>N.E.</given-names></name>
<name><surname>Wingfield</surname><given-names>A.</given-names></name>
<name><surname>Grossman</surname><given-names>M.</given-names></name>
<name><surname>Peelle</surname><given-names>J.E.</given-names></name>
</person-group><article-title>Acoustic richness modulates the neural networks supporting intelligible speech processing</article-title><source>Hear. Res.</source><year>2016</year><volume>333</volume><fpage>108</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2015.12.008</pub-id><pub-id pub-id-type="pmid">26723103</pub-id>
</element-citation></ref><ref id="B19-audiolres-15-00059"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Faul</surname><given-names>F.</given-names></name>
<name><surname>Erdfelder</surname><given-names>E.</given-names></name>
<name><surname>Lang</surname><given-names>A.G.</given-names></name>
<name><surname>Buchner</surname><given-names>A.</given-names></name>
</person-group><article-title>G*power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title><source>Behav. Res. Methods</source><year>2007</year><volume>39</volume><fpage>175</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.3758/BF03193146</pub-id><pub-id pub-id-type="pmid">17695343</pub-id>
</element-citation></ref><ref id="B20-audiolres-15-00059"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schafer</surname><given-names>E.C.</given-names></name>
<name><surname>Thibodeau</surname><given-names>L.M.</given-names></name>
</person-group><article-title>Speech recognition in noise in children with cochlear implants while listening in bilateral, bimodal, and FM-system arrangements</article-title><source>Am. J. Audiol.</source><year>2006</year><volume>15</volume><fpage>114</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1044/1059-0889(2006/015)</pub-id><pub-id pub-id-type="pmid">17182876</pub-id>
</element-citation></ref><ref id="B21-audiolres-15-00059"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bates</surname><given-names>D.</given-names></name>
<name><surname>Kliegl</surname><given-names>R.</given-names></name>
<name><surname>Vasishth</surname><given-names>S.</given-names></name>
<name><surname>Baayen</surname><given-names>H.</given-names></name>
</person-group><article-title>Parsimonious mixed models</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1506.04967</pub-id><pub-id pub-id-type="doi">10.48550/arXiv.1506.04967</pub-id></element-citation></ref><ref id="B22-audiolres-15-00059"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Hart</surname><given-names>S.G.</given-names></name>
<name><surname>Staveland</surname><given-names>L.E.</given-names></name>
</person-group><article-title>Development of NASA-TLX (task load index): Results of empirical and theoretical research</article-title><source>Advances in Psychology</source><person-group person-group-type="editor">
<name><surname>Hancock</surname><given-names>P.A.</given-names></name>
<name><surname>Meshkati</surname><given-names>N.</given-names></name>
</person-group><publisher-name>Human Mental Workload</publisher-name><publisher-loc>North-Holland, The Netherlands</publisher-loc><year>1988</year><volume>Volume 52</volume><fpage>139</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1016/S0166-4115(08)62386-9</pub-id></element-citation></ref><ref id="B23-audiolres-15-00059"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Weissgerber</surname><given-names>T.</given-names></name>
<name><surname>Rader</surname><given-names>T.</given-names></name>
<name><surname>Baumann</surname><given-names>U.</given-names></name>
</person-group><article-title>Impact of a moving noise masker on speech perception in cochlear implant users</article-title><source>PLoS ONE</source><year>2015</year><volume>10</volume><elocation-id>e0126133</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0126133</pub-id><pub-id pub-id-type="pmid">25970594</pub-id>
</element-citation></ref><ref id="B24-audiolres-15-00059"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>H.</given-names></name>
<name><surname>Dietz</surname><given-names>M.</given-names></name>
<name><surname>Williges</surname><given-names>B.</given-names></name>
<name><surname>Ewert</surname><given-names>S.D.</given-names></name>
</person-group><article-title>Better-ear glimpsing with symmetrically-placed interferers in bilateral cochlear implant users</article-title><source>J. Acoust. Soc. Am.</source><year>2018</year><volume>143</volume><fpage>2128</fpage><lpage>2141</lpage><pub-id pub-id-type="doi">10.1121/1.5030918</pub-id><pub-id pub-id-type="pmid">29716260</pub-id>
</element-citation></ref><ref id="B25-audiolres-15-00059"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gibbs</surname><given-names>B.E.I.I.</given-names></name>
<name><surname>Bernstein</surname><given-names>J.G.W.</given-names></name>
<name><surname>Brungart</surname><given-names>D.S.</given-names></name>
<name><surname>Goupell</surname><given-names>M.J.</given-names></name>
</person-group><article-title>Effects of better-ear glimpsing, binaural unmasking, and spectral resolution on spatial release from masking in cochlear-implant users</article-title><source>J. Acoust. Soc. Am.</source><year>2022</year><volume>152</volume><fpage>1230</fpage><lpage>1246</lpage><pub-id pub-id-type="doi">10.1121/10.0013746</pub-id><pub-id pub-id-type="pmid">36050186</pub-id>
</element-citation></ref><ref id="B26-audiolres-15-00059"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nilsson</surname><given-names>M.</given-names></name>
<name><surname>Soli</surname><given-names>S.D.</given-names></name>
<name><surname>Sullivan</surname><given-names>J.A.</given-names></name>
</person-group><article-title>Development of the hearing in noise test for the measurement of speech reception thresholds in quiet and in noise</article-title><source>J. Am. Acad. Audiol.</source><year>1994</year><volume>95</volume><fpage>1085</fpage><lpage>1099</lpage><pub-id pub-id-type="doi">10.1121/1.408469</pub-id></element-citation></ref><ref id="B27-audiolres-15-00059"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ingvalson</surname><given-names>E.M.</given-names></name>
<name><surname>Dhar</surname><given-names>S.</given-names></name>
<name><surname>Wong</surname><given-names>P.C.M.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
</person-group><article-title>Working memory training to improve speech perception in noise across languages</article-title><source>J. Acoust. Soc. Am.</source><year>2015</year><volume>137</volume><fpage>3477</fpage><lpage>3486</lpage><pub-id pub-id-type="doi">10.1121/1.4921601</pub-id><pub-id pub-id-type="pmid">26093435</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="audiolres-15-00059-f001"><label>Figure 1</label><caption><p>Sequence of online survey. Note: SNR = signal-to-noise ratio; SR = subject-relative; OR = object-relative.</p></caption><graphic xlink:href="audiolres-15-00059-g001" position="float"/></fig><fig position="float" id="audiolres-15-00059-f002"><label>Figure 2</label><caption><p>Average speech recognition accuracy across sentence type, noise type, and signal-to-noise ratio. Note: SNR = signal-to-noise ratio; Intr = interrupted noise; Cont = continuous noise; SR = subject-relative; OR = object-relative.</p></caption><graphic xlink:href="audiolres-15-00059-g002" position="float"/></fig><table-wrap position="float" id="audiolres-15-00059-t001"><object-id pub-id-type="pii">audiolres-15-00059-t001_Table 1</object-id><label>Table 1</label><caption><p>Average speech recognition accuracy across the noise and sentence-type conditions as a function of SNR.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Continuous</th><th colspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Interrupted</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
SNR (dB)
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
SR
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
OR
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
SR
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
OR
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02212;6</td><td align="left" valign="middle" rowspan="1" colspan="1">0.90</td><td align="left" valign="middle" rowspan="1" colspan="1">0.86</td><td align="left" valign="middle" rowspan="1" colspan="1">0.95</td><td align="left" valign="middle" rowspan="1" colspan="1">0.92</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02212;9</td><td align="left" valign="middle" rowspan="1" colspan="1">0.78</td><td align="left" valign="middle" rowspan="1" colspan="1">0.69</td><td align="left" valign="middle" rowspan="1" colspan="1">0.96</td><td align="left" valign="middle" rowspan="1" colspan="1">0.76</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02212;12</td><td align="left" valign="middle" rowspan="1" colspan="1">0.75</td><td align="left" valign="middle" rowspan="1" colspan="1">0.45</td><td align="left" valign="middle" rowspan="1" colspan="1">0.91</td><td align="left" valign="middle" rowspan="1" colspan="1">0.78</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02212;15</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.46</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.35</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.95</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.83</td></tr></tbody></table><table-wrap-foot><fn><p>Note: SNR = signal-to-noise ratio.</p></fn></table-wrap-foot></table-wrap></floats-group></article>