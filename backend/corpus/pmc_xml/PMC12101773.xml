<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="iso-abbrev">PLoS Comput Biol</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40367239</article-id><article-id pub-id-type="pmc">PMC12101773</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1013054</article-id><article-id pub-id-type="publisher-id">PCOMPBIOL-D-24-01340</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Classical Mechanics</subject><subj-group><subject>Momentum</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Human Performance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Human Performance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Classical Mechanics</subject><subj-group><subject>Motion</subject><subj-group><subject>Inertia</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Mathematical and Statistical Techniques</subject><subj-group><subject>Mathematical Models</subject><subj-group><subject>Random Walk</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Optimization</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Building momentum: A computational account of persistence toward long-term goals</article-title><alt-title alt-title-type="running-head">Momentum and persistence toward long-term goals</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9329-2863</contrib-id><name><surname>Aenugu</surname><given-names>Sneha</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff"/><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>O&#x02019;Doherty</surname><given-names>John P.</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff"/></contrib></contrib-group><aff id="aff001">
<addr-line>Division of the Humanities and Social Sciences, California Institute of Technology, Pasadena, California, United States of America</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter E.</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>UCL, UNITED KINGDOM OF GREAT BRITAIN AND NORTHERN IRELAND</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>saenugu@caltech.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>14</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>5</month><year>2025</year></pub-date><volume>21</volume><issue>5</issue><elocation-id>e1013054</elocation-id><history><date date-type="received"><day>9</day><month>8</month><year>2024</year></date><date date-type="accepted"><day>14</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Aenugu, O&#x02019;Doherty</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Aenugu, O&#x02019;Doherty</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pcbi.1013054.pdf">
</self-uri><abstract><p>Extended goals necessitate extended commitment. We address how humans select between multiple goals in a temporally extended setting. We probe whether humans engage in prospective valuation of goals by estimating which goals are likely to yield future success and choosing those, or whether they rely on a less optimal retrospective strategy, favoring goals with greater accumulated progress even if less likely to result in success. To address this, we introduce a novel task in which goals need to be persistently selected until a set target is reached to earn an overall reward. In a series of experiments, we show that human goal selection involves a mix of prospective and retrospective influences, with an undue bias in favor of retrospective valuation. We show that a goal valuation model utilizing the concept of &#x02018;momentum&#x02019;, where progress accrued toward a goal builds value and persists across trials, successfully explains human behavior better than alternative frameworks. Our findings thus suggest an important role for momentum in explaining the valuation process underpinning human goal selection.</p></abstract><abstract abstract-type="summary"><title>Author summary</title><p>Goals take time to accomplish and require commitment over extended periods of time. However, over time, some goals may become less attainable than others, necessitating switching between goals as contexts and circumstances change. A fundamental question concerns how humans switch goals across extended intervals. One possibility is that humans prospectively commit to specific goals in a way that is maximally sensitive to the likelihood of achieving that goal in the near future. Alternatively, humans might retrospectively persist in goals that they have previously worked toward. Consistent with this latter possibility, we found evidence that humans are retrospectively biased towards goals that they have spent time building progress in, even when it is more optimal to switch. We account for such a preference for accrued progress using a computational model incorporating the concept of momentum borrowed from classical mechanics. We show that momentum steadily integrates reinforcement throughout the goal progress and maintains stable goal commitment toward the goal despite environmental shifts. Although this results in suboptimal performance in our experimental paradigm, we show that momentum computations for goal commitment can have adaptive advantages for goal pursuit in real-world scenarios.</p></abstract><funding-group><award-group id="award001"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000183</institution-id><institution>Army Research Office</institution></institution-wrap>
</funding-source><award-id>W911NF2110328</award-id><principal-award-recipient>
<name><surname>O&#x02019;Doherty</surname><given-names>John P.</given-names></name>
</principal-award-recipient></award-group><award-group id="award002"><funding-source><institution>Tianqiao and Chrissy Chen Center for Social and Decision Neuroscience</institution>
</funding-source><principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9329-2863</contrib-id>
<name><surname>Aenugu</surname><given-names>Sneha</given-names></name>
</principal-award-recipient></award-group><funding-statement>This work was supported by a grant from the ARO MURI project W911NF2110328 to JO&#x02019;D. SA and JO&#x02019;D have received salary support from this funding. SA&#x02019;s salary is also supported by the Tianqiao &#x00026; Chrissy Chen Center for social and decision neuroscience. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="11"/><table-count count="2"/><page-count count="39"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>Data and code used in the study is available at <ext-link xlink:href="https://github.com/asneha213/goal-switching/tree/main" ext-link-type="uri">https://github.com/asneha213/goal-switching/tree/main</ext-link>.</meta-value></custom-meta><custom-meta><meta-name>PLOS Publication Stage</meta-name><meta-value>vor-update-to-uncorrected-proof</meta-value></custom-meta><custom-meta><meta-name>Publication Update</meta-name><meta-value>2025-05-23</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>Data and code used in the study is available at <ext-link xlink:href="https://github.com/asneha213/goal-switching/tree/main" ext-link-type="uri">https://github.com/asneha213/goal-switching/tree/main</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Humans often direct sustained effort to reach rewarding states that are distant from them. We often refer to such distant setpoints as goals &#x02013; desired future states for our future selves. Whether it is climbing Mount Whitney or landing a dream job, these goals require sustained effort to ensure gradual progress toward the respective goals [<xref rid="pcbi.1013054.ref001" ref-type="bibr">1</xref>,<xref rid="pcbi.1013054.ref002" ref-type="bibr">2</xref>]. We often toggle between multiple such end states. For example, we wish to finish our project deadlines, while also contemplating unfinished construction projects at home. How do we select which long-term goal to pursue at any given point, and how do we assign value to multiple goals at the same time?</p><p>While a desired end state can be a source of motivation, maintaining such a long term goal could require intermediate reinforcement, signaling whether a target is achievable or not worth pursuing. Progress towards the target is a potential reinforcement signal that serves to validate our efforts toward goal pursuit. Sustained progress that draws us closer to the target motivates us to expend more effort in the pursuit, whereas a slump in progress lowers our drive and induces us to pursue another target that is more attainable. [<xref rid="pcbi.1013054.ref003" ref-type="bibr">3</xref>] found that even progress not explicitly signaled in a task can, when implicitly ruling out alternative options, influence autonomous goal selection by limiting exploration [<xref rid="pcbi.1013054.ref004" ref-type="bibr">4</xref>].</p><p>Here, we introduce a computational model of momentum in progress, to capture the notion of a sustained drive towards a distant goal. We propose that intermittent reinforcement signaling progress builds up in value and drives persistence towards goals. According to this framework, a goal that yielded sustained progress towards a target in the recent past can be perceived as more valuable than an alternative goal which made no progress in the past, but yet has a greater probability of reaching the target in the near future. Our proposed model of persistent valuation predicts an inertial effect in the willingness to change targets even when the rate of progress toward a goal comes to a standstill [<xref rid="pcbi.1013054.ref005" ref-type="bibr">5</xref>].</p><p>Momentum is a concept deeply entrenched in the domain of Newtonian physics that formalizes inertia or resistance to change that is fundamental to all physical quantities embodied with mass. In physics, momentum is defined as a product of mass and velocity, <inline-formula id="pcbi.1013054.e001"><alternatives><graphic xlink:href="pcbi.1013054.e001.jpg" id="pcbi.1013054.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>*</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>v</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, and within its formulation there is an evident corollary that the greater the momentum of an object, the greater is its resistance to change. In other words, more force is required to stop an object with higher momentum, which is a function of both its mass and velocity of progress.</p><p>Several applications of this concept in cognitive and computational science indicate that this construct might have implications beyond rigid body dynamics. In the perceptual decision-making literature, it has been observed that memory for the location of a moving target is often displaced in the direction of the target&#x02019;s motion, which has led to the use of the term <italic toggle="yes">representational momentum</italic> alongside the hypothesis that the perceptual system evolved to function in the regime of Newtonian physics [<xref rid="pcbi.1013054.ref006" ref-type="bibr">6</xref>&#x02013;<xref rid="pcbi.1013054.ref008" ref-type="bibr">8</xref>]. In operant conditioning, the persistence of learned operant behavior in the face of altered conditions was termed <italic toggle="yes">behavioral momentum</italic> [<xref rid="pcbi.1013054.ref009" ref-type="bibr">9</xref>&#x02013;<xref rid="pcbi.1013054.ref013" ref-type="bibr">13</xref>]. Momentum is also a key concept in machine learning utilized for stability in learning and optimization [<xref rid="pcbi.1013054.ref014" ref-type="bibr">14</xref>]. Momentum induces inertia in gradient updates by maintaining a running average of recent updates instead of fluctuating momentary gradients to update expectations/policies. This rewards consistency in signed gradient errors, thereby maintaining stability in the direction of updates. The same concept was proposed as a possible explanation for the effect of mood on decision making [<xref rid="pcbi.1013054.ref015" ref-type="bibr">15</xref>,<xref rid="pcbi.1013054.ref016" ref-type="bibr">16</xref>].</p><p>Psychologists have previously linked the concept of momentum to the phenomenology of goal pursuit [<xref rid="pcbi.1013054.ref017" ref-type="bibr">17</xref>&#x02013;<xref rid="pcbi.1013054.ref019" ref-type="bibr">19</xref>]. However, most of this previous work has been limited to qualitative formulations, introducing momentum as a potential account for the tendency of dispositional states to carry forward through time; task-set inertia, mental simulation, and rumination are a few examples. Moreover, momentum, as a term, has been a part of the popular vernacular relating to the pursuit of goals (&#x0201c;this project needs more momentum", &#x0201c;this fight is gathering momentum every day"). However, its quantitative formalization in the decision-making literature is lacking.</p><p>Decision inertia and persistence of actions are extensively studied in psychology, albeit in the context of intermittent reinforcements. Notable studies in the foraging literature have used the concept of the opportunity cost of time as a means to decide whether to continue foraging in the current patch or move to a new one with a delay in time [<xref rid="pcbi.1013054.ref020" ref-type="bibr">20</xref>&#x02013;<xref rid="pcbi.1013054.ref022" ref-type="bibr">22</xref>]. Differently from this approach, in the present study we focus on scenarios where rewards are distant, which requires the maintenance of a persistent drive toward goal pursuit.</p><p>Suboptimal human persistence has been extensively documented in prior literature where there are sunk costs [<xref rid="pcbi.1013054.ref023" ref-type="bibr">23</xref>&#x02013;<xref rid="pcbi.1013054.ref025" ref-type="bibr">25</xref>]. Humans are known to overpersist on goals that have incurred sunk costs of time, effort, and resources, even when it is optimal to abandon them. The sunk cost fallacy demonstrates similar persistence effects in relation to extended goals, the neural basis of which was recently investigated in humans [<xref rid="pcbi.1013054.ref026" ref-type="bibr">26</xref>]. But not all goal-selection decisions made in daily life occur in the presence of explicit sunk costs. For instance, we can decide to stop working on debugging code and rest instead, with the full knowledge that our work can be resumed later. In the present study, we explicitly emphasized investigating human goal persistence patterns in a multitasking setting rather than a sunk-cost one. Therefore, we allowed goals to retain accrued progress having switched away from them. This was done to mitigate the influence of sunk costs in our experiments related to abandoned goals completely losing their value. Consequently, the present study deviates from previous work [<xref rid="pcbi.1013054.ref026" ref-type="bibr">26</xref>], as sunk costs are likely to exert a potent influence on the long-term pursuit of the goal. Moreover, in [<xref rid="pcbi.1013054.ref026" ref-type="bibr">26</xref>], while overpersistence was observed in behavior in that study, providing a computational account for the overpersistence effect was not their focus. Here, our overall goal is to understand the nature of the overpersistence effect, particularly after the effects of sunk costs explicitly related to a loss of prior goal progress have been reduced.</p><p>We make two significant contributions through the study described here. Firstly, we introduce a novel paradigm to investigate the temporal nature of the valuation of extended goals in the presence of competing alternatives, a hitherto unexplored question. Secondly, we provide an algorithmic account for persistence in extended goals. We introduce momentum as a key computational variable driving inertia in goal valuation, providing a fresh perspective on human persistence and decision inertia.</p></sec><sec sec-type="results" id="sec002"><title>Results</title><sec id="sec003"><title>Suits task: A paradigm for studying extended goal pursuit</title><p>We introduce a suit collection paradigm to simulate extended goal pursuit inspired by traditional card games. A suit in the game comprises a specific number (say seven) of tokens of the same kind. The objective for the participants is to collect as many suits as they can and win points in the game (which ultimately translates to a monetary bonus). There are three different types of suits available (<italic toggle="yes">cat</italic>, <italic toggle="yes">hat</italic>, <italic toggle="yes">car</italic>) and participants can freely switch between collecting different suits without penalty to earn points in the game. Participants only receive points when a suit is completed without any partial credit for any incomplete suits.</p></sec><sec id="sec004"><title>Gameplay</title><p>The game is played in blocks with each block having 30 rounds. A sample round of the gameplay is shown in <xref rid="pcbi.1013054.g001" ref-type="fig">Fig 1</xref>. Participants start with an empty set of slots (7 for each token type as shown in <xref rid="pcbi.1013054.g001" ref-type="fig">Fig 1A</xref>).</p><fig position="float" id="pcbi.1013054.g001"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.g001</object-id><label>Fig 1</label><caption><title>Suits task.</title><p><bold>A.</bold> Participants start with empty slots to collect tokens for each of the three available suits (<italic toggle="yes">cat, hat, car</italic>). <bold>B.</bold> Each suit type is mapped to two distinct cards which serve to stand in for the suits in the game. <bold>C.</bold> Panel shows a sample round of the task: 1. Participants see suit collection progress of all three goals. 2. Participants pick a card corresponding to one of the three goals. 3. Participants sees the outcome of their action 4. If a token is received, slots are updated. 5. Periodically, participants report the suit they are pursuing. <bold>D. Conditional probabilities</bold>. <inline-formula id="pcbi.1013054.e005"><alternatives><graphic xlink:href="pcbi.1013054.e005" id="pcbi.1013054.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for different token-card combinations are shown for the 80&#x02013;20 block. Selecting the <italic toggle="yes">basket</italic> or the <italic toggle="yes">mat</italic> card gives you the <italic toggle="yes">cat</italic> token with a high probability of 0.8, making <italic toggle="yes">cat</italic> the dominant suit in the block. <bold>E. Block shifts</bold> Each block has fixed probabilities with one dominant suit and two inferior suits of identical probabilities. The dominant suit is switched across adjacent blocks (<italic toggle="yes">Cat</italic> is most abundant in block N while <italic toggle="yes">hat</italic> is the most abundant in the next one). <bold>F. Token probabilities.</bold> Two experimental variants (experiment 1 and 2) with probability configurations specified for each suit. <bold>Figure credits.</bold> We used images from <ext-link xlink:href="https://openclipart.org/" ext-link-type="uri">https://openclipart.org/</ext-link> to generate the figures. See the Task design section in Methods for full image credits.</p></caption><graphic xlink:href="pcbi.1013054.g001" position="float"/></fig><p><bold>State:</bold> The game state is indicated by the current slot configuration which indicates progress made toward different suits. An example slot configuration occurring a few rounds into the game is shown in <xref rid="pcbi.1013054.g001" ref-type="fig">Fig 1C</xref>.</p><p><bold>Actions:</bold> There are 6 cards in the game, each corresponding to a specific suit type (two cards are allotted for each suit, see <xref rid="pcbi.1013054.g001" ref-type="fig">Fig 1B</xref>). On each round, a participant is shown three cards (one card for each suit). Participants can choose to flip between one of the three cards shown to receive tokens for a suit (see <xref rid="pcbi.1013054.g001" ref-type="fig">Fig 1C</xref>). For example, if a participant wishes to acquire a <italic toggle="yes">car</italic> token to collect the <italic toggle="yes">car</italic> suit, they would pick the <italic toggle="yes">key</italic> or the <italic toggle="yes">baggage</italic> card (whichever is available in the round) to try to acquire the token.</p><p><bold>Outcomes:</bold> If the participant flips the <italic toggle="yes">car</italic>-related card (say), they receive the <italic toggle="yes">car</italic> token with a specific (non-zero) probability and the <italic toggle="yes">hat</italic> and the <italic toggle="yes">cat</italic> tokens with zero probability. If the participant receives the <italic toggle="yes">car</italic> token, one of the empty <italic toggle="yes">car</italic> slots is filled and the participant proceeds to the next round of token collection. If a suit&#x02019;s target is reached (say 7 <italic toggle="yes">tokens</italic> of one type are collected), the participant will receive 10 points for finishing the suit and all of those 7 slots are emptied for a fresh bout of collection.</p><p><bold>Game blocks:</bold> Gameplay is continuous, and participants retain any tokens from incomplete suits across the entire game. However, the game is structured into blocks, and participants are informed that each block introduces a new probability distribution over the three suits. For example, in an 80-20 block, the probabilities might be 0.8 for <italic toggle="yes">cat</italic>, and 0.2 each for <italic toggle="yes">hat</italic> and <italic toggle="yes">car</italic>. The conditional probabilities of receiving a token given a card-action pairing in such a block are shown in <xref rid="pcbi.1013054.t001" ref-type="table">Table 1</xref> (see also <xref rid="pcbi.1013054.g001" ref-type="fig">Fig 1D</xref>). When a new block begins, the suit probabilities shift. For instance, in a 70-30 block, the dominant suit might now be hat, such that <inline-formula id="pcbi.1013054.e002"><alternatives><graphic xlink:href="pcbi.1013054.e002.jpg" id="pcbi.1013054.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="italic">hat</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="normal">hat</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">related card</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, while <inline-formula id="pcbi.1013054.e003"><alternatives><graphic xlink:href="pcbi.1013054.e003.jpg" id="pcbi.1013054.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="italic">cat</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="normal">cat</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">related card</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1013054.e004"><alternatives><graphic xlink:href="pcbi.1013054.e004.jpg" id="pcbi.1013054.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="italic">car</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi/><mml:mrow><mml:mi mathvariant="normal">car</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">related card</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> (see <xref rid="pcbi.1013054.g001" ref-type="fig">Fig 1E</xref>). Participants are explicitly informed about the block structure: each block lasts 30 rounds, different suits have different probabilities within a block, and these probabilities change when a new block begins.</p><table-wrap position="float" id="pcbi.1013054.t001"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.t001</object-id><label>Table 1</label><caption><title><inline-formula id="pcbi.1013054.e006"><alternatives><graphic xlink:href="pcbi.1013054.e006" id="pcbi.1013054.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for the 80-20 block. The dominant token is <inline-formula id="pcbi.1013054.e007"><alternatives><graphic xlink:href="pcbi.1013054.e007" id="pcbi.1013054.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">cat</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> with <inline-formula id="pcbi.1013054.e008"><alternatives><graphic xlink:href="pcbi.1013054.e008" id="pcbi.1013054.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="normal">cat</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">related card</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></title></caption><alternatives><graphic xlink:href="pcbi.1013054.t001" id="pcbi.1013054.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">
<inline-formula id="pcbi.1013054.e009">
<alternatives><graphic xlink:href="pcbi.1013054.e009" id="pcbi.1013054.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">cat</italic>
</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">hat</italic>
</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">car</italic>
</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1"><italic toggle="yes">cat</italic>-related card</td><td align="left" rowspan="1" colspan="1">0.8</td><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">0</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic toggle="yes">hat</italic>-related card</td><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">0.2</td><td align="left" rowspan="1" colspan="1">0</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic toggle="yes">car</italic>-related card</td><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">0.2</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec005"><title>Experimental manipulations</title><p><bold>There is a dominant suit in each block.</bold> We designed blocks in the game so that there is a dominant suit on each block (counterbalanced across all suit types), such that flipping its cards gives tokens with the highest probability (in <xref rid="pcbi.1013054.g001" ref-type="fig">Fig 1D</xref>, the <italic toggle="yes">cat</italic> suit is the dominant one with 0.8 probability). The other two suits have identically inferior token probabilities (both <italic toggle="yes">hat</italic> and <italic toggle="yes">car</italic> with 0.2 probability). We instructed the participants of this task manipulation, explaining that within each block there is a specific suit which has the highest chance of token collection. Pursuing the dominant suit type in each block gives participants the best chance at success in suit completion and we investigated how often participants chose the dominant option in a block in relation to their progress on various suits.</p><p><bold>Probability disparity between suits is modulated across blocks.</bold> We also varied the probability disparity between the dominant and the inferior suits to ascertain how it modulates goal selection. We expected that selection of the dominant suit would be high in the high disparity blocks (where the probability difference between the dominant and the inferior suits is highest) and that this would be reflected in participants&#x02019; goal selection patterns.</p><p><bold>The dominant suit is switched across adjacent blocks.</bold> We implemented dominant token switches across blocks so as to enable us to investigate conflicts in goal selection between retrospective and prospective influences, as this is critical to assess how individuals change their choices of goal in the face of such a conflict. If a participant pursues a dominant suit in a block and is close to finishing it, and they encounter a block shift when the hitherto dominant suit is no longer dominant, then it is relatively more difficult to make further progress towards its target. A goal conflict arises in these situations, concerning whether to persist in the suit that is close to the target, or to switch over to the new dominant suit. Such scenarios occur frequently during the game, and the optimal strategy is to switch over to the dominant suit in the new block. We did not instruct the participants that the dominant token in one block becomes an inferior token in the adjacent block.</p><p><bold>Testing the effect of instructions.</bold> We varied the instructions the participants received on the precise conditional probabilites across blocks. We investigated the extent to which knowledge of absolute probabilities of dominant and inferior tokens (without disclosing the identity of the dominant token) would influence goal selection in the game.</p><p><bold>Effect of non-uniform targets on goal selection.</bold> We set non-uniform targets for the three suits to ascertain how that influences extended-valuation of the suits in relation to their progress and task-conditions.</p><p>We include four experiments to implement the above mentioned design manipulations. Experiments 1-3 have uniform targets for all suits, with each suit comprising of seven tokens of one kind. Experiment 1 has 18 blocks with 30 rounds each, with three different probability structures: 80-20, 70-30, 60-40. In the 80-20 block (say), the dominant suit is available with 80% chance and the two inferior tokens are available with 20% chance. Experiment 2 has 12 blocks (30 rounds each), with two different block types: 75-25 and 55-45. In both experiments 1 and 2, our instructions are limited in that, within each block, we explain that one specific suit has the highest probability, and that the probabilities change with each block switch. In experiment 3 (which is a repeat of experiment 1), we gave participants more information about their current block context. Before each block began, we specifically informed them that they are in an 80-20 context, without telling them which specific suit is available with 80% chance (more details on task design and instructions are elaborated in the Methods section). In experiment 4, we employed non-uniform targets for different suits and explored its impact on validating our computational hypothesis.</p></sec><sec id="sec006"><title>Participants demonstrate a retrospective bias in temporally extended goal selection</title><p>We analyze goal selection patterns in human participants influenced by the current context (suit probabilities in a block) and slot configuration. In a given round, participants can choose to collect the dominant suit (with the highest probability of receiving tokens), or choose to pursue the suit with the most progress (max progress suit) according to their slot configuration. In <xref rid="pcbi.1013054.g002" ref-type="fig">Fig 2A</xref>, we show the participants&#x02019; probability of selecting cards related to the dominant suit, the max progress suit, and the third suit. Participants preferred to select either the dominant or the max progress suit across all conditions, picking the remaining suit well below the chance level. This indicates that the primary conflict in goal selection is between the accrued goal progress and the current rate of goal progress.</p><fig position="float" id="pcbi.1013054.g002"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.g002</object-id><label>Fig 2</label><caption><title>A. Participants show a preference for the progress accrued over the current rate of progress.</title><p>Participants primarily chose between the dominant suit (one with the highest probability within the block) and the max progress suit (one which made greatest progress towards the target), while choosing the third option relatively infrequently. <bold>B.</bold> The proportion of times participants chose the max progress suit over the dominant suit (when the two diverge) is broken down by block type. The preference for the max progress option increases with the difference in progress between the dominant and max progress suits. <bold>C.</bold> Participants show a preference for the retrospective suit over the prospective suit (when the two choices diverge). Participants chose the retrospective suit (with the greatest progress) over the prospective suit (optimal choice prescribed the task-optimized prospective agent). <bold>D. Learning effects within blocks.</bold> Panel shows the dominant suit choice probability in the first and second halves of the block (15 rounds each). The proportion of dominant choices increases in the second half indicating learning effects within blocks. The choice patterns are contrasted with that of the task-optimized prospective agent, wherein, preference for the dominant suit is significantly lower in participants in both halves of the blocks, indicating suboptimal retrospective bias in goal selection.</p></caption><graphic xlink:href="pcbi.1013054.g002" position="float"/></fig><p>Moreover, the preference toward the dominant/max progress suit shows an interaction with the block condition. Participants show a higher preference for the dominant suit when the disparity between different suits is large (80-20/75-25 conditions) and a lower preference when the disparity is small (60&#x02013;40/55&#x02013;45 conditions), indicating sensitivity to the current rate of progress (repeated measures ANOVA; Experiment 1: F(2, 86) = 20.23, <italic toggle="yes">p</italic>&#x0003c;0.00001; Experiment 2: F(1, 49) = 17.32, <italic toggle="yes">p</italic>&#x0003c;0.00001). Likewise, preference toward the max progress suit increases with decreases in disparity between the suits (repeated measures ANOVA; Experiment 1: F(2, 86) = 19.67, <italic toggle="yes">p</italic>&#x0003c;0.00001; Experiment 2: F(1, 49) = 7.1, <italic toggle="yes">p</italic>&#x0003c;0.01). However, in all conditions, participants preferred to pick the maximum progress suit over the dominant suit, indicating an increased preference for the accrued progress over the current rate of progress.</p><p><xref rid="pcbi.1013054.g002" ref-type="fig">Fig 2A</xref> provides a partial examination of the choice patterns, highlighting rounds where the dominant suit and the max progress suit are separate. However, in a fraction of rounds, the dominant suit is the same as the max-progress suit because maximum progress is made toward the dominant suit. In 62% of the rounds in Experiment 1 and 63% of the rounds in Experiment 2, the dominant suit is separate from the max progress suit, while in the rest of the rounds the two options converge. We noted that in the rounds where the dominant and the max progress suits are the same, both the accrued progress and the current rate of progress for the suit are high, mitigating conflict in goal selection. In those rounds, participants predominantly chose to pursue the dominant option (or the max progress option), picking the other two options occasionally and well below the chance level (see <xref rid="pcbi.1013054.s002" ref-type="supplementary-material">S1 Fig</xref>).</p><p>Limiting our analyses to the rounds where the dominant and max progress suits are separate (with a goal selection conflict), we further analyzed how progress differences between the two suits influence goal selection. <xref rid="pcbi.1013054.g002" ref-type="fig">Fig 2B</xref> shows the probability of choosing the max progress suit as a function of the difference in progress between the two suits. We show that participants&#x02019; preference toward the max progress suit monotonically increases with the increasing progress difference between the suits. This preference is highest when the max progress suit is at 86% progress (6/7 tokens completed), and when the dominant suit has 0/7 tokens collected.</p><p>The pattern of increased preference for accrued progress in human participants is not necessarily suboptimal. If a suit is close to completion, it might be optimal to persist with it and get a reward sooner than to switch to the dominant suit that might have a high rate of progress but an equally high trials-to-completion estimate. In fact, humans are known to prefer sooner rewards over later ones, a tendency due to the incorporation of a discount factor in the estimation of the reward value incurred over a delay [<xref rid="pcbi.1013054.ref027" ref-type="bibr">27</xref>]. We investigated whether the goal selection patterns of participants are optimal under this assumption.</p><p>We constructed a prospective agent which prioritizes faster suit completion and reward accumulation. This agent, rather than optimizing for discounted returns accumulated at the end of the task, focuses on the immediate reward delivered at the end of suit completion. The agent thus places the highest value on goals that have the lowest estimates of trials-to-completion. To estimate values, the agent needs to maintain beliefs about the current probability estimates of various suits and roll out expectations into the future until the termination condition (collecting 7 tokens) is reached. We constructed the agent as a partially observable Markov decision process, with the current slot configuration as the observed state, selection of cards as actions, and an estimate of the current suit probabilities as the belief state. The agent updates the belief about token-outcome probability of a suit using a delta learning rule:</p><disp-formula id="pcbi.1013054.e010"><alternatives><graphic xlink:href="pcbi.1013054.e010.jpg" id="pcbi.1013054.e010g" position="anchor"/><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(1)</label></disp-formula><p>where <italic toggle="yes">M</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>) is the belief at time <italic toggle="yes">t</italic>, <italic toggle="yes">I</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>) is the indicator function of the token outcome (1 if the token is received in a round), and <inline-formula id="pcbi.1013054.e011"><alternatives><graphic xlink:href="pcbi.1013054.e011.jpg" id="pcbi.1013054.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the learning rate. Using the estimated probability of belief, the prospective agent rolls out expectations one step in the future for two scenarios: receiving a token with probability <italic toggle="yes">M</italic><sub><italic toggle="yes">g</italic></sub> and moving one token closer to the target (<inline-formula id="pcbi.1013054.e012"><alternatives><graphic xlink:href="pcbi.1013054.e012.jpg" id="pcbi.1013054.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>); and not receiving the token with probability 1 &#x02212; <italic toggle="yes">M</italic><sub><italic toggle="yes">g</italic></sub>, and staying in the same slot state (<inline-formula id="pcbi.1013054.e013"><alternatives><graphic xlink:href="pcbi.1013054.e013.jpg" id="pcbi.1013054.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>). The rolled-out future states are discounted by a factor of <inline-formula id="pcbi.1013054.e014"><alternatives><graphic xlink:href="pcbi.1013054.e014.jpg" id="pcbi.1013054.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</p><disp-formula id="pcbi.1013054.e015"><alternatives><graphic xlink:href="pcbi.1013054.e015.jpg" id="pcbi.1013054.e015g" position="anchor"/><mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mi>&#x003b3;</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(2)</label></disp-formula><p>where <italic toggle="yes">Q</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>) is the value of the goal suit <italic toggle="yes">g</italic>, and <italic toggle="yes">s</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>) is the current slot state of suit <italic toggle="yes">g</italic>. Specifically, <italic toggle="yes">s</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>) is a counter for the progress of the goal, with the above recursive estimation terminating when the current state reaches the target (say, 7 tokens). Recursion is also terminated with a horizon of 20 steps.</p><p>The prospective agent, as defined above, can favor pursuing the maximum progress suit over the dominant suit. When the maximum progress suit is close to completion (say, 5 out of 7 tokens), the agent has to unroll fewer steps ahead to receive rewards compared to the dominant suit that is further away from the target (say, 2 out of 7 tokens). However, the prospective agent is sensitive to block conditions: When placed in the 80&#x02013;20 block, the chance of collecting two additional tokens with a chance of 20% is lower than that of collecting five tokens with a chance of 80%, and the agent prefers to follow the dominant suit.</p><p>We optimized the parameters of the prospective agent (parameters and action selection process documented in the Computational Modeling section of Methods) for task performance (total number of suits completed). The optimal discount factor for this agent is <inline-formula id="pcbi.1013054.e016"><alternatives><graphic xlink:href="pcbi.1013054.e016.jpg" id="pcbi.1013054.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for Experiment 1 and <inline-formula id="pcbi.1013054.e017"><alternatives><graphic xlink:href="pcbi.1013054.e017.jpg" id="pcbi.1013054.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.93</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for Experiment 2. The prospective agent, thus optimized for the task, significantly outperforms the human participants (performance distinctions are elaborated in a later section).</p><p>For each slot configuration seen by human participants, we computed the response exhibited by the prospective agent and investigated the extent of the participants&#x02019; deviation from this response. For generating the prospective responses, we assumed the true state of suit probabilities to be the agent&#x02019;s belief (not accounting for learning effects). We define the suit selected by the prospective agent as the prospective suit, and the suit with the maximum progress in a round as the retrospective suit. <xref rid="pcbi.1013054.g002" ref-type="fig">Fig 2C</xref> provides a revised analysis of <xref rid="pcbi.1013054.g002" ref-type="fig">Fig 2A</xref>, indicating the goal selection patterns to choose between the prospective suit, the retrospective suit, and the third suit (when the identities of the prospective and retrospective suit are different). The revised goal selection patterns show a trend similar to that shown in <xref rid="pcbi.1013054.g002" ref-type="fig">Fig 2A</xref>, with the preference for the retrospective suit being higher than that of the prospective suit in all conditions. Moreover, the prospective suit shows the same interaction profile with the block condition (repeated measures ANOVA: Experiment 1: F(86) = 7.04, <italic toggle="yes">p</italic>&#x0003c;0.001; Experiment 2: F(49) = 17.79, <italic toggle="yes">p</italic>&#x0003c;0.0001), and also with the retrospective suit (Experiment 1: F(86) = 2.14, <italic toggle="yes">p</italic>&#x02009;=&#x02009;0.12; Experiment 2: F(49) = 8.88, <italic toggle="yes">p</italic>&#x0003c;0.005). The proportion of rounds in which there is a conflict of goal selection in the current classification (prospective suit versus retrospective suit) is now reduced to 40% on average across all conditions (as opposed to 60% previously in the max progress suit vs. dominant suit classification), indicating that a fraction of max progress choices are indeed prospective in nature after accounting for discounting effects. <xref rid="pcbi.1013054.s002" ref-type="supplementary-material">S1 Fig</xref> shows preferences when the prospective and the retrospective suit identities are the same (when the prospective agent prefers the maximum progress option), with participants predominantly choosing the prospective/retrospective option devoid of the goal selection conflict.</p><p>In summary, the patterns of goal selection in human participants reveal a retrospective bias for accrued progress in the task. Further quantifying the extent of this bias, <xref rid="pcbi.1013054.g002" ref-type="fig">Fig 2D</xref> shows the overall proportion of the choice of dominant suit in participants compared to that of the prospective agent optimized for the task. We also split the choices as occurring in the first and second halves of blocks (averaged over all blocks) to account for any learning effects. Both participants and the prospective agent show learning effects within blocks, with the proportion of dominant choice increasing in the latter half of the block. However, the dominant choice of the participants remains significantly lower than that of the prospective agent in both halves of the blocks, causing suboptimal human performance in the task (elaborated in a later section). Lower learning rates and increased discounting in participants could potentially explain suboptimality in task performance in humans, which we consider to be a potential explanation for the goal selection patterns seen in humans. This account is tested in later sections by fitting the prospective agent to the participants&#x02019; choices. Furthermore, there are no significant effects of learning or performance changes between blocks. <xref rid="pcbi.1013054.s004" ref-type="supplementary-material">S3 Fig</xref> shows the proportion of dominant choice between blocks, where we find no significant changes in the selection effects as the task progresses.</p></sec><sec id="sec007"><title>Switching patterns reveal greater persistence towards the retrospective suit</title><p>We further investigated temporal persistence patterns with regard to suit selection. Although overall goal selection is biased toward the retrospective suit, we further aimed to determine whether we could find evidence of a bias toward temporal persistence once a suit is selected. For this, we calculated the proportion of rounds in which participants switched away from the selected option in the subsequent round. We isolated these switches by suit type: separating rounds where they switched away from the prospective suit from those where they switched from the retrospective suit (similarly defined as in the previous section). <xref rid="pcbi.1013054.g003" ref-type="fig">Fig 3A</xref> shows the switch probabilities of the participants. We also separated the conditions where participants received a token in the previous round from those where they did not receive a token.</p><fig position="float" id="pcbi.1013054.g003"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.g003</object-id><label>Fig 3</label><caption><title>A. Participants show greater persistence towards the retrospective suit.</title><p>Participants make more switches away from the prospective option than the retrospective option. Switches are split by suit type (prospective/retrospective), block condition, and whether the round elicits a token or not. Significance markers indicate paired-sample t-tests (<inline-formula id="pcbi.1013054.e018"><alternatives><graphic xlink:href="pcbi.1013054.e018" id="pcbi.1013054.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mo>*</mml:mo><mml:mo>*</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.001</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mo>*</mml:mo><mml:mo>*</mml:mo><mml:mo>*</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>). <bold>B. Switches split by progress differences between the suits.</bold> H diff, L diff correspond to higher and lower progress difference between the suits. Increased persistence is observed with the retrospective suit when progress difference between the suits is high.</p></caption><graphic xlink:href="pcbi.1013054.g003" position="float"/></fig><p>We found that participants made a significantly greater number of switches away from the prospective suit regardless of the token outcome in the previous round (paired-sample t-test; Experiment 1: t(86) = &#x02013;2.22, <italic toggle="yes">p</italic>&#x0003c;0.01; Experiment 2: t(96) = &#x02013;2.79, <italic toggle="yes">p</italic>&#x0003c;0.001). We found that the proportion of prospective switches interacts with the block types (repeated measures ANOVA; Experiment 1: F(2, 144) = 12.56, <italic toggle="yes">p</italic>&#x0003c;0.0001; Experiment 2: F(1, 49) = 19.55, <italic toggle="yes">p</italic>&#x0003c;0.0001). Participants are most likely to persist with the prospective suit in the high-disparity blocks (80&#x02013;20/75&#x02013;25), where the adaptive value of persisting with the prospective token is maximal, compared to the low-disparity blocks (60&#x02013;40/55&#x02013;25). However, overall levels of persistence with the prospective suit are lower than those of the retrospective suit.</p><p>Retrospective switch propensities are also sensitive to the relative progress differences between the two suits and are lower when the difference is high. <xref rid="pcbi.1013054.g003" ref-type="fig">Fig 3B</xref> shows switching proportions away from prospective and retrospective suits split by high (&#x0003e;50%) and low (<inline-formula id="pcbi.1013054.e019"><alternatives><graphic xlink:href="pcbi.1013054.e019.jpg" id="pcbi.1013054.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>%) progress difference between the two suits. Participants demonstrate increasing levels of commitment toward the retrospective suit with its increasing distance (progress) from the prospective suit. This is reflected in lowered switching from the retrospective suit and increased switching from the prospective suit. Increased commitment toward the retrospective choice is thus evidenced not only by resistance to switching from the choice but also by increased rates of revisiting it after having switched away from it.</p><p>This points out an interesting facet of goal persistence which can be dissociated from mere stickiness in choice driven by task-switching costs. The persistence in goal selection that we describe here tends to involve a tendency to revisit the retrospective option even after it has been previously switched away from, whereas choice stickiness typically refers to a tendency to refrain from switching away from a particular action in the first place. Thus, accrued progress in a particular goal seems to exert a retrospective pull on subsequent decisions, clearly highlighting persistent retrospective influences on goal selection.</p></sec><sec id="sec008"><title>Human performance falls short of the prospective agent optimizing for discounted future rewards</title><p>We showed evidence that human participants exhibit a bias towards selecting and persisting with the retrospective suit. Here, we analyze how bias translates into suboptimal performance in the task.</p><p>We compare the performance of human participants with that of the task-optimized prospective agent. For further contrast, we also constructed a retrospective agent that always prefers the suit that made the most progress toward the target through the following value estimation process.</p><disp-formula id="pcbi.1013054.e020"><alternatives><graphic xlink:href="pcbi.1013054.e020.jpg" id="pcbi.1013054.e020g" position="anchor"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>=</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(3)</label></disp-formula><p>The retrospective agent also applies a recursive estimate similar to that of the prospective agent to discount future rewards (with a factor <inline-formula id="pcbi.1013054.e021"><alternatives><graphic xlink:href="pcbi.1013054.e021.jpg" id="pcbi.1013054.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>), but without incorporating the belief state about the current suit probabilities (Equation 3). In effect, the agent always values the suit that is closest to the target.</p><p>We incorporated the same goal selection algorithm as that of the prospective agent (see Computational Modeling section in Methods), and likewise, the parameters of the retrospective agent are optimized for task performance.</p><p>In general, human performance does not match the prospective agent in both experiments (Experiment 1: t(86) = &#x02013;9.98, <italic toggle="yes">p</italic>&#x0003c;10<sup>&#x02212;15</sup>; Experiment 2: t(98) = &#x02013;9.60, <italic toggle="yes">p</italic>&#x0003c;10<sup>&#x02212;15</sup>), as shown in <xref rid="pcbi.1013054.g004" ref-type="fig">Fig 4A</xref>. The contrast is particularly striking on conditions where there is a high disparity in the probabilities of token outcome (80-20 and 75-25 blocks). However, the performance contrast is not significant in the blocks where the benefit of prospecting is minimal (60-40 and 55-45 blocks). The prospective agent shows a significant interaction of performance with the block type (Experiment 1: F(2, 86) = 49.85, <italic toggle="yes">p</italic>&#x0003c;10<sup>&#x02212;14</sup>; Experiment 2: F(1, 49) = 147.8, <italic toggle="yes">p</italic>&#x0003c;10<sup>&#x02212;15</sup>), whereas human agents show a diminished yet significant interaction with block type in Experiment 1 (F(2, 86) = 3.81, <italic toggle="yes">p</italic>&#x0003c;0.05) and no significant interaction in Experiment 2. Human performance beats the retrospective agent marginally in Experiment 1 (t(86) = 3.28, <italic toggle="yes">p</italic>&#x0003c;0.01), while the difference in performance distributions is not significant in Experiment 2. Task performance mirrors the heterogeneity evident in participants&#x02019; retrospective choices, with only a few participants attaining performance close to that of the optimal prospective agent.</p><fig position="float" id="pcbi.1013054.g004"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.g004</object-id><label>Fig 4</label><caption><title>A, B. Human performance contrasted against task-optimized prospective and retrospective agents.</title><p>Human behavior significantly falls short of optimal prospective agent and marginally beats the retrospective agent in task performance (total number of suits collected). Agents&#x02019; parameters are optimized for performance in the task. Performance is broken down by task condition and the distribution of performance is shown. <bold>B. Preference towards accrued progress shown by the two agents contrasted with human participants.</bold> The retrospective agent has the highest proportion of retrospective choice and the prospective agent the lowest, while human retrospective preferences lies between these extremes.</p></caption><graphic xlink:href="pcbi.1013054.g004" position="float"/></fig><p><xref rid="pcbi.1013054.g004" ref-type="fig">Fig 4B</xref> compares the proportion of choice of the max progress option of the two agents with that of the human participants. The retrospective agent, in the spirit of its formulation, demonstrates the highest preference for accrued progress, whereas the prospective agent falls at the other extreme; human participants fall part-way between the two agents, indicating an inflation of retrospective choice beyond what is optimal for the task.</p></sec><sec id="sec009"><title>Explicit instruction about contingencies within a block alleviates but does not eliminate the inflation of retrospective choice</title><p>Although we explicitly designed the block conditions so that there is a dominant suit, it is likely that participants did not anticipate how effective the pursuit of the prospective suit over the retrospective choice would be, thus reducing the exploration of alternative suits. If information regarding the abundance of the prospective suit is made explicit, will it lower retrospective influences on the task?</p><p>In Experiments 1 and 2, we simply alerted the participant when a block shift occurred. We now consider a variant of the first experiment, which we call Experiment 3, where we gave explicit instructions to participants about the contingencies within each block. Before the start of the game, we instructed the participants on the types of blocks present in the task and also alerted the participants regarding the available contingencies at the beginning of each block. For example, we told them that they are in an 80-20 block condition. We nevertheless refrained from telling the participants which specific suit is the dominant one in a block to avoid explicitly priming a goal switch.</p><p><xref rid="pcbi.1013054.g005" ref-type="fig">Fig 5A</xref> plots the performance of the task with explicit contingency information alongside the variant without contingency information, compared to the prospective agent. Task performance improved with contingency (independent samples t-test: t (106) = &#x02013; 2.00, <italic toggle="yes">p</italic>&#x0003c;0.05) - evident in the 80-20 block condition - yet it is still significantly short of the prospective agent.</p><fig position="float" id="pcbi.1013054.g005"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.g005</object-id><label>Fig 5</label><caption><title>A, B. Effects on explicit instruction about prospects of alternative goals on behavior.</title><p><bold>A.</bold> Task performance improves with instructions yet significantly falls short of the optimal prospective agent. <bold>B.</bold> Retrospective choice proportion decreases with instructions yet is significantly inflated when contrasted with the prospective agent. <bold>C. Learning effects within blocks.</bold> Panel shows the dominant suit choice probability in the first and second halves of the block (15 rounds each) in the presence and absence of explicit instructions. Preference towards the dominant choice increases in both halves of the blocks indicating a shift towards optimal goal selection in the presence of instructions.</p></caption><graphic xlink:href="pcbi.1013054.g005" position="float"/></fig><p><xref rid="pcbi.1013054.g005" ref-type="fig">Fig 5B</xref> shows the contrast in the choice proportion of the max progress suit found in versions with and without instructions. Explicit instructions reduced the retrospective influence on goal selection, yet did not eliminate its effect relative to an optimal prospective agent.</p><p><xref rid="pcbi.1013054.g005" ref-type="fig">Fig 5C</xref> contrasts the proportion of choice of dominant suit in the first and second halves of the blocks in the version with instructions (Experiment 3), without instructions (Experiment 1), and the prospective agent optimized for the task. Although the proportion of dominant choices increased in the version with instructions, it remained significantly lower than the task-optimized prospective agent, indicating suboptimal goal selection patterns in human participants even in the presence of explicit instructions on contextual goal prospects.</p></sec><sec id="sec010"><title>Momentum as a key computational variable driving the persistence of temporally extended goals</title><p>We previously described an optimal prospective agent as a benchmark for human performance. A potential account for suboptimal human performance could be that humans incorporate token-outcome contingencies of the suits inefficiently, or alternatively, humans could be using an entirely different mechanism to estimate the likelihood of achieving future goals.</p><p>As an alternative account, we formalized momentum as a measure of the goal value that drives persistence and is defined as the product of progress made toward the goal and its speed toward the goal.</p><disp-formula id="pcbi.1013054.e022"><alternatives><graphic xlink:href="pcbi.1013054.e022.jpg" id="pcbi.1013054.e022g" position="anchor"/><mml:math id="M22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>goal&#x000a0;momentum</mml:mtext><mml:mo>=</mml:mo><mml:mtext>progress&#x000a0;accrued</mml:mtext><mml:mo>&#x000d7;</mml:mo><mml:mtext>current&#x000a0;rate&#x000a0;of&#x000a0;reinforcement</mml:mtext></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(4)</label></disp-formula><p>In the case of multiple goals with different rewards, the above value can be scaled appropriately with the associated rewards.</p><p>This equation is similar in essence to the construct of &#x02018;momentum&#x02019;, defined as the product of mass and velocity in Newtonian physics, which explains the resistance of an object to change. The larger the momentum, the greater its resistance to change in value. The analogy of goal momentum with the physical interpretation of momentum goes only as far as it comprises two factors: accrued progress (analogous to behavioral mass) and the current rate of progress (analogous to velocity), which together create resistance to change in valuation. A longer history of reinforcement with an option (accumulated behavioral mass) and a higher current rate of reinforcement both increase momentum and facilitate persistence with an option. This analogy was previously used in the definition of behavioral momentum [<xref rid="pcbi.1013054.ref009" ref-type="bibr">9</xref>], where the rate at which the free operant responds in the presence of a stimulus is likened to the velocity of a moving body, and the resistance to change of the response is likened to its inertial mass. We further extend this metaphor to include extended goals, where the momentum of the goal in reaching its desired setpoint determines its value.</p><p>Momentum, in the purview of goal pursuit, is an estimate of the drive toward a specific goal that is dependent on both the history of reinforcement (previous progress) and the current rate of reinforcement. We define the progress accrued in a fractional format (say 40%) that facilitates a relative comparison between goals with distinct objectives. Formulated in this way, the progress accrued is also the inverse of proximity to the target. From this perspective, goals can be viewed as setpoints in the distant future, and the value of the goal is its momentum in reaching the setpoint.</p><p>With the above definition, our aim was to capture the two effects that we found in behavior. As progress accrued in a suit increases, the resistance to switch away from the suit is higher, and as the speed of progress reduces (as a result of shifting token contingencies), the resistance to switching is lowered. Finding parallels to the definition of physics, we equate the progress accrued as a measure of mass causing inertial effects, in line with the endowment effects seen with physical objects in behavioral economics [<xref rid="pcbi.1013054.ref028" ref-type="bibr">28</xref>]. While progress accrued increases inertia in switching, a sufficiently lowered speed of progress can lower momentum, and thus facilitate switching away from the option.</p><sec id="sec011"><title>TD-momentum</title><p>We use the aforementioned conceptualization of momentum, a product of progress accrued and the speed of progress, as a measure of the goal value.</p><disp-formula id="pcbi.1013054.e023"><alternatives><graphic xlink:href="pcbi.1013054.e023.jpg" id="pcbi.1013054.e023g" position="anchor"/><mml:math id="M23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(5)</label></disp-formula><p>where <italic toggle="yes">Q</italic><sub><italic toggle="yes">g</italic></sub> is the estimate of the goal value of a suit <italic toggle="yes">g</italic> in a specific slot configuration state <inline-formula id="pcbi.1013054.e024"><alternatives><graphic xlink:href="pcbi.1013054.e024.jpg" id="pcbi.1013054.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">m</italic> is the fractional progress made toward the goal in a given instant <italic toggle="yes">t</italic>, <inline-formula id="pcbi.1013054.e025"><alternatives><graphic xlink:href="pcbi.1013054.e025.jpg" id="pcbi.1013054.e025g" position="anchor"/><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is the parameter for the current rate of progress and <italic toggle="yes">b</italic><sub><italic toggle="yes">g</italic></sub> is a goal-specific bias for <italic toggle="yes">g</italic>.</p><p>We formulate an algorithm, TD-momentum, to facilitate updating of momentum estimates for different goals. Here, the product of the progress accrued and the rate of progress serve as a function approximator of the goal value, with speed as a free parameter and progress as an indicator of the state of the goal [<xref rid="pcbi.1013054.ref029" ref-type="bibr">29</xref>]. The estimation of speed is done through the temporal difference learning rule, with its value updated through gradient descent on the prediction error of the expectation of receiving a token in the next round (and progressing one step ahead).</p><p>The temporal difference error (<inline-formula id="pcbi.1013054.e026"><alternatives><graphic xlink:href="pcbi.1013054.e026.jpg" id="pcbi.1013054.e026g" position="anchor"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b4;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>) is estimated as follows:</p><disp-formula id="pcbi.1013054.e027"><alternatives><graphic xlink:href="pcbi.1013054.e027.jpg" id="pcbi.1013054.e027g" position="anchor"/><mml:math id="M27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(6)</label></disp-formula><p>The TD momentum parameters <inline-formula id="pcbi.1013054.e028"><alternatives><graphic xlink:href="pcbi.1013054.e028.jpg" id="pcbi.1013054.e028g" position="anchor"/><mml:math id="M28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <italic toggle="yes">b</italic><sub><italic toggle="yes">g</italic></sub> are updated with a learning rate <inline-formula id="pcbi.1013054.e029"><alternatives><graphic xlink:href="pcbi.1013054.e029.jpg" id="pcbi.1013054.e029g" position="anchor"/><mml:math id="M29" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> through a gradient descent in the value function <italic toggle="yes">Q</italic><sub><italic toggle="yes">g</italic></sub>.</p><disp-formula id="pcbi.1013054.e030"><alternatives><graphic xlink:href="pcbi.1013054.e030.jpg" id="pcbi.1013054.e030g" position="anchor"/><mml:math id="M30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mi>&#x003b4;</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(7)</label></disp-formula><disp-formula id="pcbi.1013054.e031"><alternatives><graphic xlink:href="pcbi.1013054.e031.jpg" id="pcbi.1013054.e031g" position="anchor"/><mml:math id="M31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mi>&#x003b4;</mml:mi><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(8)</label></disp-formula><disp-formula id="pcbi.1013054.e032"><alternatives><graphic xlink:href="pcbi.1013054.e032.jpg" id="pcbi.1013054.e032g" position="anchor"/><mml:math id="M32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mi>&#x003b4;</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(9)</label></disp-formula><p>The goal-specific bias term (<italic toggle="yes">b</italic><sub><italic toggle="yes">g</italic></sub>) is included to ensure that the goal value is updated even when progress is at <italic toggle="yes">m</italic>&#x02009;=&#x02009;0. When one chooses the goal when its progress is zero and repeatedly fails to achieve the outcome, its value is depreciated through the bias term. The bias term also ensures that the goal value is retained when the goal target is accomplished (upon which the progress <italic toggle="yes">m</italic> resets to zero), and can increase the chance of choosing the same goal again even though the value component of the term <inline-formula id="pcbi.1013054.e033"><alternatives><graphic xlink:href="pcbi.1013054.e033.jpg" id="pcbi.1013054.e033g" position="anchor"/><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mi>m</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is zero.</p><p>Here are the key features of momentum computations for goal valuation.</p><list list-type="order"><list-item><p>
<bold>The relation between goal momentum and fractional progress is modulated by the discount factor, the step size of progress, and the current rate of progress</bold>
</p><p>If no unit progress is made in a round, <inline-formula id="pcbi.1013054.e034"><alternatives><graphic xlink:href="pcbi.1013054.e034.jpg" id="pcbi.1013054.e034g" position="anchor"/><mml:math id="M34" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1013054.e035"><alternatives><graphic xlink:href="pcbi.1013054.e035.jpg" id="pcbi.1013054.e035g" position="anchor"/><mml:math id="M35" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b4;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is always negative for <inline-formula id="pcbi.1013054.e036"><alternatives><graphic xlink:href="pcbi.1013054.e036.jpg" id="pcbi.1013054.e036g" position="anchor"/><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.</p><p>If unit progress (<inline-formula id="pcbi.1013054.e037"><alternatives><graphic xlink:href="pcbi.1013054.e037.jpg" id="pcbi.1013054.e037g" position="anchor"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>) is made in the next round, <inline-formula id="pcbi.1013054.e038"><alternatives><graphic xlink:href="pcbi.1013054.e038.jpg" id="pcbi.1013054.e038g" position="anchor"/><mml:math id="M38" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>&#x003b7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. Assuming no goal-specific bias,<disp-formula id="pcbi.1013054.e039"><alternatives><graphic xlink:href="pcbi.1013054.e039.jpg" id="pcbi.1013054.e039g" position="anchor"/><mml:math id="M39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(10)</label></disp-formula></p><p>
<disp-formula id="pcbi.1013054.e040"><alternatives><graphic xlink:href="pcbi.1013054.e040.jpg" id="pcbi.1013054.e040g" position="anchor"/><mml:math id="M40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(11)</label></disp-formula>
<disp-formula id="pcbi.1013054.e041"><alternatives><graphic xlink:href="pcbi.1013054.e041.jpg" id="pcbi.1013054.e041g" position="anchor"/><mml:math id="M41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x0003e;</mml:mo><mml:mn>0</mml:mn><mml:mo>&#x027fa;</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003b7;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(12)</label></disp-formula>
</p><p>With unit progress (<inline-formula id="pcbi.1013054.e042"><alternatives><graphic xlink:href="pcbi.1013054.e042.jpg" id="pcbi.1013054.e042g" position="anchor"/><mml:math id="M42" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>), prediction errors (<inline-formula id="pcbi.1013054.e043"><alternatives><graphic xlink:href="pcbi.1013054.e043.jpg" id="pcbi.1013054.e043g" position="anchor"/><mml:math id="M43" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b4;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>) are positive if and only if the condition <inline-formula id="pcbi.1013054.e044"><alternatives><graphic xlink:href="pcbi.1013054.e044.jpg" id="pcbi.1013054.e044g" position="anchor"/><mml:math id="M44" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003b7;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> is satisfied (Equation 12).</p><p>For example, given <inline-formula id="pcbi.1013054.e045"><alternatives><graphic xlink:href="pcbi.1013054.e045.jpg" id="pcbi.1013054.e045g" position="anchor"/><mml:math id="M45" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, the unit progress from <italic toggle="yes">m</italic><sup>(<italic toggle="yes">t</italic>)</sup>&#x02009;=&#x02009;0.2 to <italic toggle="yes">m</italic><sup>(<italic toggle="yes">t</italic>&#x02009;+&#x02009;1)</sup>&#x02009;=&#x02009;0.25 will yield a positive prediction error, while the unit progress from <italic toggle="yes">m</italic><sup>(<italic toggle="yes">t</italic>)</sup>&#x02009;=&#x02009;0.9 to <italic toggle="yes">m</italic><sup>(<italic toggle="yes">t</italic>&#x02009;+&#x02009;1)</sup>&#x02009;=&#x02009;0.95 produces a negative prediction error. Therefore, for a fixed <inline-formula id="pcbi.1013054.e046"><alternatives><graphic xlink:href="pcbi.1013054.e046.jpg" id="pcbi.1013054.e046g" position="anchor"/><mml:math id="M46" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, a small enough unit of progress can result in negative speed updates when the fractional progress (<italic toggle="yes">m</italic>) is close to 1. This accounts for the phenomenon of diminishing marginal utility [<xref rid="pcbi.1013054.ref030" ref-type="bibr">30</xref>], whose main feature is that the marginal utility of a unit increase in a good decreases as its quantity increases.</p><p>When no progress is made in a round (<inline-formula id="pcbi.1013054.e047"><alternatives><graphic xlink:href="pcbi.1013054.e047.jpg" id="pcbi.1013054.e047g" position="anchor"/><mml:math id="M47" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>), <inline-formula id="pcbi.1013054.e048"><alternatives><graphic xlink:href="pcbi.1013054.e048.jpg" id="pcbi.1013054.e048g" position="anchor"/><mml:math id="M48" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> &#x02212; <inline-formula id="pcbi.1013054.e049"><alternatives><graphic xlink:href="pcbi.1013054.e049.jpg" id="pcbi.1013054.e049g" position="anchor"/><mml:math id="M49" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>&#x0003c;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, and updates for the speed of progress are always negative. On the other hand, if unit progress (<inline-formula id="pcbi.1013054.e050"><alternatives><graphic xlink:href="pcbi.1013054.e050.jpg" id="pcbi.1013054.e050g" position="anchor"/><mml:math id="M50" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003b7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>) made in a round, the changes to the speed term are positive, but only if <inline-formula id="pcbi.1013054.e051"><alternatives><graphic xlink:href="pcbi.1013054.e051.jpg" id="pcbi.1013054.e051g" position="anchor"/><mml:math id="M51" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003b7;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. Thus, TD-momentum value estimates do not need to increase monotonically with progress; their relation to progress is determined by the discount factor <inline-formula id="pcbi.1013054.e052"><alternatives><graphic xlink:href="pcbi.1013054.e052.jpg" id="pcbi.1013054.e052g" position="anchor"/><mml:math id="M52" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, the step size of unit progress <inline-formula id="pcbi.1013054.e053"><alternatives><graphic xlink:href="pcbi.1013054.e053.jpg" id="pcbi.1013054.e053g" position="anchor"/><mml:math id="M53" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, and the current estimate of the rate of progress (<inline-formula id="pcbi.1013054.e054"><alternatives><graphic xlink:href="pcbi.1013054.e054.jpg" id="pcbi.1013054.e054g" position="anchor"/><mml:math id="M54" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>).</p><p><xref rid="pcbi.1013054.g006" ref-type="fig">Fig 6A</xref> demonstrates this feature of momentum computations. We simulated TD-momentum value estimates for several discount factors, with a step size of unit progress of 0.14, and probability of making unit progress as 0.6 (binary 0s and 1s drawn with probabilities 0.4 and 0.6, respectively). We note that for <inline-formula id="pcbi.1013054.e055"><alternatives><graphic xlink:href="pcbi.1013054.e055.jpg" id="pcbi.1013054.e055g" position="anchor"/><mml:math id="M55" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, TD-momentum exhibits diminished value with increasing progress when the fractional progress is close to one.</p><p>However, for most participants in our task, the TD-momentum <inline-formula id="pcbi.1013054.e056"><alternatives><graphic xlink:href="pcbi.1013054.e056.jpg" id="pcbi.1013054.e056g" position="anchor"/><mml:math id="M56" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> estimates through behavioral fits indicate a discount factor &#x0003e;0.9, so nonmonotonic behavior with increasing progress is not observed in the task. However, we expect that with sufficiently lowered step sizes of progress, the TD-momentum can predict diminished utility closer to goal completion.</p></list-item><list-item><p><bold>Goal momentum provides stable value estimates against fluctuations in reinforcements</bold> We contrast the TD-momentum value estimates with that of the prospective agent in <xref rid="pcbi.1013054.g006" ref-type="fig">Fig 6B</xref>. In the simulation, we introduced a change in the probability of unit progress from 0.6 to 0.2 after 15 rounds of the task. We note that the value estimates from the prospective agent exhibit fluctuations corresponding to individual reinforcements of unit progress, whereas momentum builds up value steadily and gradually decays when encountered with a shift in probability. We believe that this retention of stable value estimates enables persistence beyond a context shift, which could facilitate sustained commitment toward long-term goals.</p></list-item></list><fig position="float" id="pcbi.1013054.g006"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.g006</object-id><label>Fig 6</label><caption><title>A, B. Features of the TD-momentum algorithm.</title><p><bold>A.</bold> The relation of TD-momentum value estimates with progress is determined by the discount factor, step size of progress, and the probability of making unit progress. In the simulations, the probability of unit progress is 0.6, and the step size of unit progress is 0.14. <bold>B.</bold> Comparison between the value computations from prospective and TD-momentum algorithms. The black line indicates the shift in probability of unit progress from 0.6 to 0.2 after 15 trials. TD-momentum provides stable value estimates, as opposed to the prospective model which exhibits fluctuations in response to individual reinforcements. Step size of unit progress is 0.05, learning rate is 0.4, and <inline-formula id="pcbi.1013054.e057"><alternatives><graphic xlink:href="pcbi.1013054.e057" id="pcbi.1013054.e057g" position="anchor"/><mml:math id="M57" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is 0.9 for both algorithms.</p></caption><graphic xlink:href="pcbi.1013054.g006" position="float"/></fig></sec><sec id="sec012"><title>Alternative models.</title><p>We constructed several alternative models to enable us to test several alternative hypotheses to the momentum explanation in accounting for human behavior on the task.</p><p><bold>Perturbed prospection.</bold> Firstly, we devised parameterized variants of the prospective agent that learns the token outcome probabilities inefficiently, employing both symmetric and asymmetric learning rates [<xref rid="pcbi.1013054.ref031" ref-type="bibr">31</xref>] for positive and negative prediction errors. We expected that a prospective agent acting on noisy/inertial/biased probabilities could account for suboptimality in choices. We experimented with several variants of the prospective model and observed that the simple variant with symmetric learning rates captures the data better than the others (see <xref rid="pcbi.1013054.s007" ref-type="supplementary-material">S6 Fig</xref> and <xref rid="pcbi.1013054.s001" ref-type="supplementary-material">S1 Appendix</xref>).</p><p><bold>Hybrid.</bold> Secondly, we conceived a hybrid model that incorporates weighted contributions from prospective and retrospective agents, which could potentially account for retrospective bias in goal switching.</p><p><bold>TD-persistence.</bold> Lastly, we constructed an agent &#x02013; TD persistence &#x02013; that learns the token outcome expectations of each goal using a temporal difference learning rule and acts guided by those expectations, modulated by persistence parameters in action policy. As we instructed the participants that every block has different probabilities, a dominant strategy would be to estimate which goal has the highest token-outcome expectation and persevere with it until the end of the block. We expected the influence of this strategy to be greater in the experimental variant (Experiment 3), where we provided explicit instructions about the type of their current block. TD persistence, having an optimal performance similar to that of the prospective agent (see <xref rid="pcbi.1013054.s008" ref-type="supplementary-material">S7 Fig</xref>), is one of the better strategies in the task. However, by its very nature, it cannot reproduce either the temporal patterns of retrospective bias or the switching trends seen in behavior.</p><p>We incorporated a goal perseveration parameter in the common action selection policy (see Methods)&#x02014;a choice kernel that increments each time a goal is selected and decays each time it is not&#x02014;so model comparisons elicit differences beyond any choice persistence or putative &#x0201c;sunk cost" mechanisms.</p><p>We ensured that the competing hypotheses elucidated above are individually recoverable (see Methods on parameter and model recovery, also <xref rid="pcbi.1013054.s005" ref-type="supplementary-material">S4 Fig</xref> and <xref rid="pcbi.1013054.s006" ref-type="supplementary-material">S5 Fig</xref>).</p><p><bold>Comparison to simpler reinforcement learning models.</bold> We also considered the possibility that participants did not play the game within the framework of extended goals, but rather selected goals by treating them as individual unconnected events in which one simply receives or does not receive a token reward. In such a scenario, behavior could be captured by a simple reinforcement learning (RL) model utilizing a standard delta learning rule such as in the trial-based Rescorla-Wagner model [<xref rid="pcbi.1013054.ref032" ref-type="bibr">32</xref>]. This simplified RL model could potentially predict an increase in retrospective choice due to prior reinforcements increasing expected values of tokens. However, participants were alerted during block shifts to ensure minimal carryover of such expectations of token probabilities. Nevertheless, to rule out this possibility, we fit participants&#x02019; choices to this simplified RL model [<xref rid="pcbi.1013054.ref033" ref-type="bibr">33</xref>] (while also including a choice kernel), and we generated simulated data from the fitted parameters (see Methods). The model-generated data capture neither the preference for accrued progress nor the sensitivity to relative progress differences between the suits (see <xref rid="pcbi.1013054.s014" ref-type="supplementary-material">S13 Fig</xref>). That is to be expected, because this simplified RL model has no notion of commitment toward extended goals, supporting our presumption that the reinforcement schedule imposed by extended goals inflates retrospective choice over and beyond the predictions of such a simplified RL model.</p><p><bold>Time-inconsistent discounting.</bold> We employed exponential discounting to design the task-optimized prospective agent. However, given that human behavior is better accounted for by hyperbolic discounting [<xref rid="pcbi.1013054.ref027" ref-type="bibr">27</xref>], we also implemented a prospective agent with hyperbolic discounting. We estimated trials to completion (delay to reward <italic toggle="yes">D</italic>) for each goal by rolling out the results into the future and used the hyperbolic discounting factor (<inline-formula id="pcbi.1013054.e058"><alternatives><graphic xlink:href="pcbi.1013054.e058.jpg" id="pcbi.1013054.e058g" position="anchor"/><mml:math id="M58" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1013054.e059"><alternatives><graphic xlink:href="pcbi.1013054.e059.jpg" id="pcbi.1013054.e059g" position="anchor"/><mml:math id="M59" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) to discount the rewards. However, we found no significant difference in behavioral fits between the variants of the prospective agent with exponential and hyperbolic discounting (see <xref rid="pcbi.1013054.s007" ref-type="supplementary-material">S6 Fig</xref>).</p></sec><sec id="sec013"><title>The TD-momentum model offers a better fit to human choice data.</title><p>We fit each of the above models to the participants&#x02019; choices of stay/switch choices from actions and goals reported from the previous round and compared the models using the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) metrics.</p><disp-formula id="pcbi.1013054.e060">
<alternatives><graphic xlink:href="pcbi.1013054.e060.jpg" id="pcbi.1013054.e060g" position="anchor"/><mml:math id="M60" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>AIC</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mi>ln</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><disp-formula id="pcbi.1013054.e061">
<alternatives><graphic xlink:href="pcbi.1013054.e061.jpg" id="pcbi.1013054.e061g" position="anchor"/><mml:math id="M61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>BIC</mml:mtext><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mi>ln</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mi>ln</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>Here, <italic toggle="yes">n</italic> is the number of data points, <italic toggle="yes">k</italic> is the number of parameters in the model, and <inline-formula id="pcbi.1013054.e062"><alternatives><graphic xlink:href="pcbi.1013054.e062.jpg" id="pcbi.1013054.e062g" position="anchor"/><mml:math id="M62" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> is the maximum value of the likelihood function. The lower the AIC/BIC metrics, the better the fit of the model. Moreover, we computed a 3-fold cross-validation (CV) metric for each participant by fitting the model with two-thirds of the task blocks and computing the negative log-likelihood on the held out one-third data (mean metric computed over 30 random splits of the data). <xref rid="pcbi.1013054.g007" ref-type="fig">Fig 7A</xref> and <xref rid="pcbi.1013054.g007" ref-type="fig">7B</xref> show that the TD-momentum offers a better account of the data according to the AIC, BIC, and cross-validation metrics. Beyond just aggregate likelihood comparisons between models, we examined whether TD-momentum provided superior fits consistently across individual subjects [<xref rid="pcbi.1013054.ref031" ref-type="bibr">31</xref>]. We performed paired sample t-tests between AIC, BIC, and CV metrics, and TD-momentum consistently offers better fits between participants in experiments 1 and 2 compared to other models.</p><fig position="float" id="pcbi.1013054.g007"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.g007</object-id><label>Fig 7</label><caption><title>A, B. Model comparisons.</title><p>TD-momentum accounts for the switching choices in the task better than the other prospective, hybrid, and TD-persistence models as per AIC, BIC, and 3-fold cross validation metrics in experiments 1 and 2. Mean and standard error bars of the metrics shown in the figure. Significance markers correspond to two-tailed paired sample <italic toggle="yes">t</italic>-tests (<inline-formula id="pcbi.1013054.e063"><alternatives><graphic xlink:href="pcbi.1013054.e063" id="pcbi.1013054.e063g" position="anchor"/><mml:math id="M63" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>). <bold>C.</bold> TD-momentum also offers the best account of behavior in the variant with explicit instructions. <bold>D.</bold> Incidence of the competing hypothesis of TD-persistence increases in the version with instructions.</p></caption><graphic xlink:href="pcbi.1013054.g007" position="float"/></fig><p><xref rid="pcbi.1013054.g007" ref-type="fig">Fig 7C</xref> shows a comparison between the behavioral predictions of the different models in the presence of explicit instructions on the types of contingency (experiment 3), contrasting with that of the variant without instructions (experiment 1). In both versions, TD-momentum offers a better fit to the data. However, we observed an increased contribution of TD-persistence at the level of individual participants in the version with explicit instructions. <xref rid="pcbi.1013054.g007" ref-type="fig">Fig 7D</xref> shows the difference in BIC between the two algorithms in the individual participants who contrast the two variants. In experiment 1, TD-momentum offers a better fit for 72% of the participant pool (loglikelihood ratio test with <italic toggle="yes">p</italic>&#x02009;=&#x02009;0.05), whereas the number drops to 58% in experiment 3. We predicted that 42% of participants who were better accounted for by the TD-persistence model in the version with instructions would perform better on the task than those of participants accounted for by the TD-momentum model. In fact, there was a significant difference in performance between the two groups (independent samples t-test: t(63)=&#x02212;3.69, <italic toggle="yes">p</italic>&#x0003c;10<sup>&#x02212;3</sup>). Retrospective bias patterns in the two participant groups (TD-momentum and TD-persistence) follow the same patterns as those obtained from simulations of respective algorithms (see <xref rid="pcbi.1013054.s009" ref-type="supplementary-material">S8 Fig</xref>).</p></sec><sec id="sec014"><title>The TD-momentum model reproduces the behavioral effects of the aggregated participant pool.</title><p>Using the parameters derived from individual participant fits of TD-momentum, we generated synthetic data imitating the participant pool to validate whether the model can replicate key behavioral effects that capture retrospective influences in experiments 1 and 2. One sample of synthetic data per real participant is generated using the fitted parameters, and the aggregate choice patterns of the population of model-simulated participants are contrasted with those of the real participants. Simulated participant data reproduces interaction effects of preference for accrued progress with the block type and also captures the pattern of increase in preference for accrued progress with increasing progress difference between the maximum progress suit and the dominant suit (see <xref rid="pcbi.1013054.g008" ref-type="fig">Fig 8A</xref> and <xref rid="pcbi.1013054.g008" ref-type="fig">8B</xref>). The simulated data also captures the trend of greater persistence in favor of the retrospective suit over the prospective one in switching patterns (see <xref rid="pcbi.1013054.g008" ref-type="fig">Fig 8C</xref> and <xref rid="pcbi.1013054.g008" ref-type="fig">8D</xref>). The prospective and hybrid agent also captures the behavioral trends reasonably well (see <xref rid="pcbi.1013054.s011" ref-type="supplementary-material">S10 Fig</xref> and <xref rid="pcbi.1013054.s012" ref-type="supplementary-material">S11 Fig</xref>), while the TD-persistence agent fails to capture them (see <xref rid="pcbi.1013054.s013" ref-type="supplementary-material">S12 Fig</xref>).</p><fig position="float" id="pcbi.1013054.g008"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.g008</object-id><label>Fig 8</label><caption><title>TD-momentum: model predictions</title><p>TD-momentum agent captures general trends in preferences towards maximum progress suit and switching patterns in the task.</p></caption><graphic xlink:href="pcbi.1013054.g008" position="float"/></fig><p>We also plotted how individually fitted model parameters correlate with participants&#x02019; task measures. The momentum learning rate correlates with the performance in the task (Experiment 1: R=0.78, <italic toggle="yes">p</italic>&#x0003c;10<sup>&#x02212;9</sup>; Experiment 2: R=0.45, <italic toggle="yes">p</italic>&#x0003c;0.0008; Experiment 3: R = 0.68, <italic toggle="yes">p</italic>&#x0003c;10<sup>&#x02212;10</sup>), that is, participants who exhibited greater learning of the speed of progress of different goals collected more suits in the task. The cost of switching is correlated with the number of action switches made by the participant (Experiment 1: R=-0.78, <italic toggle="yes">p</italic>&#x0003c;10<sup>&#x02212;9</sup>; Experiment 2: R=-0.81, <italic toggle="yes">p</italic>&#x0003c;10<sup>&#x02212;12</sup>; Experiment 3: R = &#x02212;0.62, <italic toggle="yes">p</italic>&#x0003c;0.001), that is, participants with lower switching costs made more switches under all conditions. Lastly, the softmax temperature parameter correlates with the preference for accrued progress (Experiment 1: R=0.73, <italic toggle="yes">p</italic>&#x0003c;10<sup>&#x02212;7</sup>; Experiment 2: R=0.75, <italic toggle="yes">p</italic>&#x0003c;10<sup>&#x02212;10</sup>; Experiment 3: R = 0.60, <italic toggle="yes">p</italic>&#x0003c;10<sup>&#x02212;7</sup>), that is, the more the participant relies on the value disparity between the goals generated by the momentum model, the greater is their preference for accrued progress. TD-momentum also reproduced participants&#x02019; learning effects within a block and captured the increase in dominant suit choice proportions as the block progresses (see <xref rid="pcbi.1013054.s010" ref-type="supplementary-material">S9 Fig</xref>).</p></sec><sec id="sec015"><title>Non uniform targets for goals delineate distinctions between TD-momentum and prospective agents.</title><p>Although the TD-momentum model outperforms all other models in explaining the likelihood of participants&#x02019; choices in the task, three of the agents, TD-momentum, prospective, and hybrid, qualitatively reproduce key behavioral signatures in the task. In order to more definitively differentiate between which of these models best explains human behavior, we turned to an additional experiment.</p><p>A key distinction between the TD-momentum and the prospective agent is that the former relies on a measure of fractional progress, and the latter relies on the absolute distance to the target. In experiments 1 and 2, both measures converge, as targets for all suits are the same with no distinctions between fractional progress and absolute proximity to the target.</p><p>In experiment 4, we introduce variable target token lengths for the suits. The suit <italic toggle="yes">cat</italic> can be completed by collecting 4 cat tokens while the suits <italic toggle="yes">hat</italic> and <italic toggle="yes">car</italic> can be completed by collecting 6 and 8 tokens, respectively. Thus, if the <italic toggle="yes">cat</italic> suit has 2/4 tokens completed and the car suit has 6/8 tokens completed, the prospective agent has to unroll two steps ahead to predict rewards for both suits, however, TD-momentum considers that the <italic toggle="yes">cat</italic> suit is 50% done and that the <italic toggle="yes">car</italic> suit is 75% done. Thus, this design emphasizes the distinctions between the two algorithms.</p><p><xref rid="pcbi.1013054.g009" ref-type="fig">Fig 9A</xref> shows the slot configuration for experiment 4. We adjusted the token probabilities so that there is one dominant prospective suit in each block and so that the dominant prospective suit is counterbalanced across suits. We introduced high- and low- disparity block conditions (<italic toggle="yes">H disp</italic> and <italic toggle="yes">L disp</italic>). In both conditions, the dominant suit takes 8 rounds (on average) to complete, while in the <italic toggle="yes">H disp</italic> and <italic toggle="yes">L disp</italic> conditions, the two inferior tokens take 15 and 12 rounds (on average) to complete. <xref rid="pcbi.1013054.g009" ref-type="fig">Fig 9B</xref> shows the participants&#x02019; goal selection probabilities divided by the dominant suit condition. The participants selected the <italic toggle="yes">cat</italic> suit with the highest chance in the <italic toggle="yes">cat</italic> dominant blocks, etc. for the other suits, indicating sensitivity to token contingencies. Participants also showed sensitivity to relative distinctions between suits with a lower proportion of dominant suit choice in the low-disparity blocks compared to the high-disparity blocks (<xref rid="pcbi.1013054.g009" ref-type="fig">Fig 9C</xref>). Moreover, goal selections are uniform with slight biases towards specific suits on account of them having variable-length targets. For example, participants selected the hat suit significantly lower than the suits <italic toggle="yes">cat</italic> or <italic toggle="yes">car</italic> in the low disparity condition, perhaps accounted for by preferences for the options at extremes (in terms of target length). <xref rid="pcbi.1013054.g009" ref-type="fig">Fig 9C</xref>,<xref rid="pcbi.1013054.g009" ref-type="fig">9D</xref>, and <xref rid="pcbi.1013054.g009" ref-type="fig">9E</xref> show behavioral signatures from experiments 1 and 2 replicated in the new task.</p><fig position="float" id="pcbi.1013054.g009"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.g009</object-id><label>Fig 9</label><caption><title>Experiment 4: Variable targets for goals</title><p><bold>A.</bold> Slot configuration for experiment 4. <bold>B.</bold> Suit selections divided by high disparity (H disp) and low disparity (L disp) conditions and sub divided by the dominant token type (cat/hat/car). <bold>C, D, E.</bold> Behavioral signatures (suit selections, retrospective choice, switching patterns) replicated from experiments 1 and 2. <bold>Figure credits.</bold> We used images from <ext-link xlink:href="https://openclipart.org/" ext-link-type="uri">https://openclipart.org/</ext-link> to generate this figure. See the Task Design section in Methods for full image credits.</p></caption><graphic xlink:href="pcbi.1013054.g009" position="float"/></fig><p>Model comparisons for all models are shown in <xref rid="pcbi.1013054.g010" ref-type="fig">Fig 10</xref>. TD momentum provides the best account of participants&#x02019; choices in the task using AIC, BIC, and cross-validation measures, as shown in <xref rid="pcbi.1013054.g010" ref-type="fig">Fig 10A</xref>. <xref rid="pcbi.1013054.g010" ref-type="fig">Fig 10B</xref> and <xref rid="pcbi.1013054.g010" ref-type="fig">10C</xref> show behavioral signatures derived from model-generated datasets contrasted with participants&#x02019; behavior. The suit selections in <xref rid="pcbi.1013054.g010" ref-type="fig">Fig 10B</xref> show an interesting distinction between TD-momentum and prospective agents. The prospective agent prefers the <italic toggle="yes">cat</italic> suit more than other suits across all conditions, as it has the smallest target of all suits and thus the lowest absolute distance to the target compared to the others. The hybrid agent, despite accounting for retrospective influences, shows the same pattern of preferences as that of the prospective agent (see <xref rid="pcbi.1013054.s015" ref-type="supplementary-material">S14 Fig</xref>). However, an optimal prospective agent shows no such preference for the smallest target suit as it perfectly incorporates token probabilities while estimating future prospects (see <xref rid="pcbi.1013054.s016" ref-type="supplementary-material">S15 Fig</xref>). The prospective agent fitted to the behavior of the participants shows the bias that indicates the inefficacy of the model in predicting the goal selections of the participants. Human participants and the TD-momentum model fitted to behavior do not show such a bias. Likewise, in <xref rid="pcbi.1013054.g010" ref-type="fig">Fig 10C</xref>, the TD-momentum model shows retrospective choice patterns that more closely resemble human behavior relative to the prospective agent. The TD-momentum model did not fully capture the specific bias against the <italic toggle="yes">hat</italic> suit displayed in the <italic toggle="yes">L disp</italic> condition; nevertheless, its distribution of goal selections appears to be closer to the pattern found in the choices of human participants. Finer deviations in goal selection patterns between the TD-momentum model and human behavior can potentially be attributed to idiosyncratic goal selection patterns in humans that are not factored into the model. The deviations could also arise from systematic biases (such as a preference for options at the extremes), which could be included in the model as inductive bias components.</p><fig position="float" id="pcbi.1013054.g010"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.g010</object-id><label>Fig 10</label><caption><title>Experiment 4: Model comparisons</title><p><bold>A.</bold> TD-momentum outperforms other models in AIC, BIC, cross validation metrics. <bold>B.</bold> Prospective agent shows bias towards the cat suit, while TD-momentum closely captures distribution of goal selections. <bold>C.</bold> TD-momentum captures retrospective choice patterns better than the prospective agent.</p></caption><graphic xlink:href="pcbi.1013054.g010" position="float"/></fig></sec></sec><sec id="sec016"><title>Algorithmic complexity and a normative account of momentum.</title><p>We provided evidence that incorporating the momentum of a goal as a proxy for its value can account for suboptimal switching and undue retrospective valuation in temporally-extended goal pursuit. However, is momentum-based decision making necessarily maladaptive?</p><p>Evidently, there are scenarios where reliance on momentum as a measure of goal value is suboptimal. When a goal with a considerable history of progress is abruptly decreased in value, its drop in prospective value is drastic, however, its momentum is sustained for a period beyond its decrease in value, whereupon reliance on momentum leads to suboptimal persistence. Conversely, when alternative goals are enhanced in value, their prospects are suppressed as their momentum is lower because of their lack of history.</p><p>If we revert to the interpretation of the term as it is used to describe the physical properties of objects, it is rare that we observe such drastic changes in momentum in a world where continuity is the norm. Consider the momentum of a car driving on a freeway, which is a continuous variable with occasional rises and dips; only in rare cases of collision do we notice an abrupt drop in momentum.</p><p>In a framework of continuity, where the recent past greatly predicts the near future, the momentum model offers a more efficient mode of computation and thereby an adaptive advantage. TD-momentum and the prospective agent both have identical parametric complexity (6 free parameters), yet their algorithmic complexities are different. Prospection relies on expensive model-based computations&#x02014;keeping track of token-outcome expectations of each goal and recursively extrapolating them into the future. Momentum, on the other hand, maintains a model-free account of the past and uses it to extrapolate it to the future. The momentum model, without incorporating task contingencies, leads to suboptimalities, especially when there are jumps and discontinuities in prospects, as was the case in our task. The optimal performance attained by the TD momentum in the task is significantly lower than that of the prospective agent (see <xref rid="pcbi.1013054.s008" ref-type="supplementary-material">S7 Fig</xref>).</p><p>To test this account further, we simulated several variants of the task in which the prospects of the three goals undergo gradual shifts through Gaussian random walks. We estimated the optimal performance achieved in each of these variants by both modes of decision-making&#x02014;momentum and prospection&#x02014;and found that there is little difference in performance between the two algorithms (see <xref rid="pcbi.1013054.g011" ref-type="fig">Fig 11</xref>). However, when sudden reversals of the option probabilities are incorporated, the momentum model falls behind the prospective agent in performance.</p><fig position="float" id="pcbi.1013054.g011"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.g011</object-id><label>Fig 11</label><caption><title>Normative account of momentum in goal pursuit.</title><p>Optimal performance attained by the TD-momentum agent contrasted with that of the prospective agent when the token-outcome probabilities change in Gaussian random walks (left) and when they change in abrupt probability reversals (right). Each point in the plot refers to a variant of the task with randomly generated token contingencies; a sample task design was illustrated for both random walks and probability reversals.</p></caption><graphic xlink:href="pcbi.1013054.g011" position="float"/></fig><p>Momentum computations can be efficient, yet in scenarios with sudden upheavals of fortunes, as is common in quotidian experiences, they can lead to suboptimal decisions. In addition, momentum computations and concomitant goal persistence can have adaptive features when there is uncertainty in the environment. Even when an option is devalued, one might be uncertain when it might be valued again, or whether viability of alternative options would remain stable enough to warrant a switch. Furthermore, persistence in goals can have other advantages in social settings, such as stable inference of one&#x02019;s goals by peers [<xref rid="pcbi.1013054.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1013054.ref034" ref-type="bibr">34</xref>], all the while avoiding the inefficiency of incurring cognitive costs of goal switching.</p></sec></sec><sec sec-type="conclusions" id="sec017"><title>Discussion</title><p>Using a novel paradigm in which participants are free to pursue any of three temporally extended goals, we observed that participants exhibited a strong bias toward retrospectively persisting with goals with greater accumulated progress at the expense of goals with higher prospective benefits. The nature of this bias, which we call a retrospective bias&#x02014;was highly heterogeneous in our pool of participants, with individuals manifesting varying degrees of retrospective influence on the selection of extended goals.</p><p>We manipulated block contingencies in the task so that there is a dominant goal that is most favorable to pursue in a given block that is not viable to pursue once the block shifted. This manipulation created situations in which participants made partial progress toward specific goals only to discover that they were no longer feasible shortly afterward. The optimal strategy would be to switch to an alternative option. However, we observed that participants demonstrated undue resistance in making such switches, which compromised performance on the task. Persistence towards a goal increased with an increase in accumulated progress. Participants also demonstrated sensitivity to prospective values, with the proportion of retrospective choice diminishing with an increase in the disparity of prospective values between options.</p><p>We conducted a variant of the task giving explicit instructions on the contextual prospects of different goals. By alerting the participants about the probability structure within a block, we expected that the inference of the identity of the dominant suit would be relatively easier and more salient. We noted that such explicit instructions served to diminish retrospective influences. However, they remained potent in driving goal selection. In this variant, participants continued to show suboptimal persistence toward the goals with greater accumulated progress despite having explicit knowledge about the prospects of alternative goals.</p><p>We explain this phenomenon by turning to the metaphor of momentum, aided by the simple fact that the momentum of a goal changes more slowly than its expected value. Momentum&#x02014;a variable derived from physics defined as a product of mass and velocity&#x02014;changes its magnitude with changes in velocity. Inertial mass acts as a moderating factor, tempering the impact of velocity on momentum. In this context, we draw a parallel by associating the velocity variable with the speed of progress toward the target and the mass variable with the prior progress accumulated in the task. When the accrued progress is substantial, any variation in the progress speed has a minor impact on the momentum&#x02019;s magnitude compared to when the progress is minimal. If participants prioritize the momentum of the goal over its expected value, this preference could explain their increased persistence towards the goals that have made significant progress towards the target. To give a computational account of this phenomenon, we introduced a novel TD-momentum algorithm that adjusts participants&#x02019; momentum expectations for various goals using a temporal difference learning rule.</p><p>We compared the TD momentum algorithm with alternative accounts, including an inefficient prospective agent, a hybrid of prospective and retrospective agents, and a simple persistence-guided reinforcement learning of goal expectations. We hypothesized that momentum lends a better account of human persistence in goal valuation, as being driven by retrospective influences of past reinforcements accrued in goal pursuit, extending beyond mere choice repetition or sunk-cost considerations. In fact, through our modeling, we demonstrate that incorporating momentum as a measure of goal value best accounts for behavior in the task better than its alternatives. This computational model outperforms alternative hypotheses in explaining the choice data and reproduces several behavioral patterns at both the population and individual levels.</p><p>The phenomenon of persistence in retrospective choice goes beyond task-switching costs or sunk costs, which are also potential drivers of goal persistence. Participants faced no explicit penalty for switching and can retain progress made in goals until the game&#x02019;s end and revisit them, hence the choice between stay or quit does not arise, except perhaps at the very end of the task. In addition, participants frequently revisited retrospective choices throughout the task, despite having switched away from them, indicating persistent goal valuation that goes beyond task switching costs. Nevertheless, to explain away persistence generated by task switching and/or sunk costs in a more formal way, we also included a choice kernel for goals in each of the computational models that we tested, whereby the tendency to stick with a particular goal is increased the more that goal is chosen in previous trials, acting as a proxy for a cumulative task switching effect. Our model fitting results show that momentum is critical to capture behavior above and beyond that explained by a cumulative task-switching cost effect. Thus, while our data do not rule out the contribution of one or another of these phenomena to goal selection and switching behavior in general, our findings show that such accounts are not sufficient to account for the bias toward retrospective choices we observed.</p><p>Our study goes beyond existing work on human goal commitment by providing a computational basis of overpersistence. [<xref rid="pcbi.1013054.ref026" ref-type="bibr">26</xref>] observed patterns of overpersistence during the pursuit of extended goals in a low-cost setting. However, their best-fitting model was not designed to account for the persistence biases displayed by the participants in the task. Instead, they quantified an individual persistence bias factor measured as a deviation from their prescribed goal selection model. Our model provides a plausible mechanistic account for participants&#x02019; persistence patterns during goal selection. In support of this claim, the model parameters we fit to individual participants were found to correlate with the extent of individual retrospective preferences manifested in the task. Hence, our momentum model provides a computational account of human persistence in goal selection.</p><p>Although the momentum-based model accounts for cognitive biases manifested in the pursuit of temporally extended goals, it begs the question of why humans employ a suboptimal algorithm in pursuing extended goals. We also provide a normative explanation for the role of momentum in the pursuit of goals, indicating through simulations that such a strategy is not always maladaptive. In situations involving gradual shifts in the viability of the goal, interpolating future value through momentum is parsimonious, with little depreciation in expected rewards compared to an optimal strategy. However, when shifts in goal viability are more abrupt, as is our task, the momentum-based strategy of goal selection can lead to suboptimal persistence with goals of high retrospective value. Moreover, other adaptive effects of persistence in partially observed environments with uncertainty in goal prospects could also favor momentum computations.</p><p>The approach we have developed here can be applied to the broader question of how humans adaptively self-regulate, since the ability to disengage from goals that are unattainable in the current context is a hallmark of self-regulation [<xref rid="pcbi.1013054.ref035" ref-type="bibr">35</xref>,<xref rid="pcbi.1013054.ref036" ref-type="bibr">36</xref>]. An inability to disengage from goals is routinely observed in healthy individuals, with more pronounced effects seen in psychiatric conditions such as impulsivity [<xref rid="pcbi.1013054.ref037" ref-type="bibr">37</xref>,<xref rid="pcbi.1013054.ref038" ref-type="bibr">38</xref>], depression [<xref rid="pcbi.1013054.ref039" ref-type="bibr">39</xref>,<xref rid="pcbi.1013054.ref040" ref-type="bibr">40</xref>], attention deficit hyperactivity disorder [<xref rid="pcbi.1013054.ref041" ref-type="bibr">41</xref>], and generalized anxiety disorder [<xref rid="pcbi.1013054.ref042" ref-type="bibr">42</xref>,<xref rid="pcbi.1013054.ref043" ref-type="bibr">43</xref>]. The present findings have potential relevance for understanding the failures of behavioral persistence seen in psychiatric disorders.</p><p>There are several limitations of the present work. One is that we introduced a number of simplifying assumptions into our analysis which, when discarded, could open up a realm of other potential algorithmic accounts. For example, in formulating the prospective agent, we assumed that the agent optimizes for immediate rewards delivered upon suit completion. However, it is possible that humans could employ subroutines that are pre-planned according to the selection of hierarchical options [<xref rid="pcbi.1013054.ref044" ref-type="bibr">44</xref>], such as committing to a goal for a fixed set of rounds. We also observed significant heterogeneity in human goal selection with evidence of the prevalence of other strategies (such as the TD-persistence model) in some individuals. Furthermore, we found evidence that instructions can moderate the influence of momentum computations on goal selection behavior. Thus, the momentum model serves as a coarse descriptor of human goal selection behavior, but not necessarily as a fully comprehensive account for the goal selection tendencies of each individual.</p><p>In conclusion, the present work provides insight into the computational mechanisms underlying flexible goal pursuit. Characterizing how humans assign subjective value to goals over time and determining how that assignment is influenced by multiple factors, such as trial history, context, reinforcement rates, and the presence of alternative goals, is a crucial first step toward understanding how humans set and commit to goals in everyday decision-making contexts. Such insights can ultimately be used to implement novel behavioral interventions and nudges to improve the efficiency of goal-directed decision making within and between individuals.</p></sec><sec sec-type="materials|methods" id="sec018"><title>Methods</title><sec id="sec019"><title>Ethics statement</title><p>The research described here was reviewed by the Caltech Institutional Review Board and deemed exempt from ongoing in-depth review (IR18-0796) due to the fact that the data were collected anonymously through the online platform Prolific and because the research was considered of no more than minimal risk. All participants provided consent for their online participation by ticking a box on an online consent form, but did not sign the consent form, as the data collection is anonymized.</p></sec><sec id="sec020"><title>Experimental protocol</title><sec id="sec021"><title>Task Design.</title><p>We have three different suits in the game: <italic toggle="yes">cat, hat, car</italic>. There are six cards, two for each suit. The mappings are as follows: the <italic toggle="yes">car</italic> suit is assigned to <italic toggle="yes">mat</italic> and <italic toggle="yes">basket</italic> cards; the <italic toggle="yes">hat</italic> is mapped to <italic toggle="yes">wardrobe</italic> and <italic toggle="yes">table</italic> cards; the <italic toggle="yes">car</italic> is mapped to <italic toggle="yes">key</italic> and <italic toggle="yes">baggage</italic> cards. We inform the participants about the above mapping at the beginning of the game. We chose the card symbols to be semantically related to the token types and we conveyed those semantic descriptions to the participant for their ease of retrieval (for instance, &#x0201c;places where you would like to keep a <italic toggle="yes">hat</italic>"). We further tested participants&#x02019; knowledge of card-token mappings through a quiz before the game began. The participants continued with the game only if they answered all the questions correctly; if not, they were sent to the beginning of the instructions for revision.</p><p>In each round of the game, we presented the participants with three cards (one for each suit type). The exact card displayed for a particular token type is shuffled across the rounds. We also shuffled the locations of the cards on the screen at random every round to prevent any mappings between goals and action key presses.</p><p>Throughout the game, cards serve to stand in for the suits, and to collect tokens for a specific suit, participants need to flip the cards associated with the suit. Through this design, our objective was to eliminate the stimulus-related value mapping, which would likely emerge if we were to directly display the suit stimuli (say images of <italic toggle="yes">cat</italic>, <italic toggle="yes">hat</italic>, <italic toggle="yes">car</italic>) and to elicit the construct of goals in participants. If, say, a participant persists in the goal of pursuing the <italic toggle="yes">cat</italic> suit, they would choose the relevant cards, the <italic toggle="yes">basket</italic> card when it is available and the <italic toggle="yes">mat</italic> card when it is not. By making the associated cards appear at random, we mitigated stimulus-elicited persistence effects in our design.</p><p>In experiment 1, we designed three block conditions: 80-20, 70-30, and 60-40. There are 18 blocks in the game, 6 of each type. Each block has 30 rounds, and the game lasts 540 rounds in total. The block conditions are shuffled throughout the game. However, we ensured that the dominant suit is flipped across adjacent blocks, so that no two adjacent blocks have the same token with the highest availability. As our primary focus lay in investigating flexible goal switching across contexts, we sought to induce conditions where switching is optimal.</p><p>In experiment 2, there are two block conditions: 75-25, 55-45. There are 12 blocks in the game (6 for each type), so 360 rounds in all.</p><p>For experiments 1 and 2, we instructed the participants that suit probabilities are constant within a block but shift across blocks. We included a quiz during the instructions to ensure that they understand block switches and differing suit probabilities. We also emphasized during the instructions that the probabilities are associated with the suit types and that picking either card related to the suit gives the token with the same probability. This is to prevent participants from tracking the value associated with specific card stimuli (whether a specific card gives the token more often) and to prompt them to place values on the goals (suit types) instead. In the experiment, block transitions are signaled with a screen that displays the text &#x0201c;This is the beginning of a new block" for 2 seconds. We also added a notation indicating the current block number out of the total number of blocks (say, block 13/18). We informed the participants that each block lasts 30 rounds and indicated the round number at the beginning of each round (say, round 20).</p><p>In experiment 3, a variant of experiment 1, we provided explicit instructions on the current token contingency faced by the participants. At the start of each block, we specified the type of block (80-20/70-30/60-40) without explicitly informing which token has the highest availability. During signaling the start of a new block, we also indicated that it is a specific type of block (say 80-20). We designed this as a nudge to encourage flexible goal adaptation across blocks.</p><p>In experiment 4, we assigned different targets to different suits. The cat suit can be finished by accruing 4 cat tokens, while the hat and car suits can be finished by collecting 6 and 8 tokens, respectively. Dominant suits are counterbalanced in all token types, and token probabilities are adjusted accordingly. We constructed two block conditions, high disparity and low disparity blocks (<italic toggle="yes">H disp</italic>, <italic toggle="yes">L disp</italic>) where the inferior token takes 15 and 12 rounds (on average) to finish. The dominant token takes 8 rounds (on average) to finish in all blocks.</p><p>We added a goal probe at the end of every third round, to elicit their target suits at different points throughout the game. To avoid a forced choice between three options, we also included an &#x02018;undecided&#x02019; prompt.</p><p><xref rid="pcbi.1013054.g001" ref-type="fig">Figs 1</xref> and <xref rid="pcbi.1013054.g009" ref-type="fig">9</xref> in the manuscript were generated using images from <ext-link xlink:href="https://openclipart.org" ext-link-type="uri">https://openclipart.org</ext-link>.</p><list list-type="order"><list-item><p><ext-link xlink:href="https://openclipart.org/image/800px/1198" ext-link-type="uri">https://openclipart.org/image/800px/1198</ext-link> (cat)</p></list-item><list-item><p><ext-link xlink:href="https://openclipart.org/image/800px/159907" ext-link-type="uri">https://openclipart.org/image/800px/159907</ext-link> (hat),</p></list-item><list-item><p><ext-link xlink:href="https://openclipart.org/image/800px/321286" ext-link-type="uri">https://openclipart.org/image/800px/321286</ext-link> (car),</p></list-item><list-item><p><ext-link xlink:href="https://openclipart.org/image/800px/203250" ext-link-type="uri">https://openclipart.org/image/800px/203250</ext-link> (basket),</p></list-item><list-item><p><ext-link xlink:href="https://openclipart.org/image/800px/153547" ext-link-type="uri">https://openclipart.org/image/800px/153547</ext-link> (mat),</p></list-item><list-item><p><ext-link xlink:href="https://openclipart.org/image/800px/29276" ext-link-type="uri">https://openclipart.org/image/800px/29276</ext-link> (table),</p></list-item><list-item><p><ext-link xlink:href="https://openclipart.org/image/800px/26415" ext-link-type="uri">https://openclipart.org/image/800px/26415</ext-link> (wardrobe),</p></list-item><list-item><p><ext-link xlink:href="https://openclipart.org/image/800px/320342" ext-link-type="uri">https://openclipart.org/image/800px/320342</ext-link> (baggage),</p></list-item><list-item><p><ext-link xlink:href="https://openclipart.org/image/800px/59329" ext-link-type="uri">https://openclipart.org/image/800px/59329</ext-link> (key).</p></list-item></list></sec><sec id="sec022"><title>Participants.</title><p><xref rid="pcbi.1013054.t002" ref-type="table">Table 2</xref> shows the details of the recruitment of the participants for all experiments. Participants were recruited from the Prolific on-line platform and compensated with a base pay of $9 per hour, with a performance-based bonus of up to $3. We instructed the participants that their performance bonus is directly proportional to the number of suits completed by the end of the game.</p><table-wrap position="float" id="pcbi.1013054.t002"><object-id pub-id-type="doi">10.1371/journal.pcbi.1013054.t002</object-id><label>Table 2</label><caption><title>Participant recruitment.</title></caption><alternatives><graphic xlink:href="pcbi.1013054.t002" id="pcbi.1013054.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1">Participants</th><th align="left" rowspan="1" colspan="1">Median time (mins)</th><th align="left" rowspan="1" colspan="1">Age range (mean <inline-formula id="pcbi.1013054.e064"><alternatives><graphic xlink:href="pcbi.1013054.e064" id="pcbi.1013054.e064g" position="anchor"/><mml:math id="M64" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> std)</th><th align="left" rowspan="1" colspan="1">Female/Male</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Experiment 1</td><td align="left" rowspan="1" colspan="1">44</td><td align="left" rowspan="1" colspan="1">31</td><td align="left" rowspan="1" colspan="1">18-64 (28.6 <inline-formula id="pcbi.1013054.e065"><alternatives><graphic xlink:href="pcbi.1013054.e065" id="pcbi.1013054.e065g" position="anchor"/><mml:math id="M65" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 9.5)</td><td align="left" rowspan="1" colspan="1">13/31</td></tr><tr><td align="left" rowspan="1" colspan="1">Experiment 2</td><td align="left" rowspan="1" colspan="1">49</td><td align="left" rowspan="1" colspan="1">24</td><td align="left" rowspan="1" colspan="1">20-68 (29.2 <inline-formula id="pcbi.1013054.e066"><alternatives><graphic xlink:href="pcbi.1013054.e066" id="pcbi.1013054.e066g" position="anchor"/><mml:math id="M66" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 8.8)</td><td align="left" rowspan="1" colspan="1">20/29</td></tr><tr><td align="left" rowspan="1" colspan="1">Experiment 3</td><td align="left" rowspan="1" colspan="1">67</td><td align="left" rowspan="1" colspan="1">33</td><td align="left" rowspan="1" colspan="1">19-52 (28.1 <inline-formula id="pcbi.1013054.e067"><alternatives><graphic xlink:href="pcbi.1013054.e067" id="pcbi.1013054.e067g" position="anchor"/><mml:math id="M67" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 7.6)</td><td align="left" rowspan="1" colspan="1">32/35</td></tr><tr><td align="left" rowspan="1" colspan="1">Experiment 4</td><td align="left" rowspan="1" colspan="1">39</td><td align="left" rowspan="1" colspan="1">26</td><td align="left" rowspan="1" colspan="1">19-57 (31.1 <inline-formula id="pcbi.1013054.e068"><alternatives><graphic xlink:href="pcbi.1013054.e068" id="pcbi.1013054.e068g" position="anchor"/><mml:math id="M68" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> 10.0)</td><td align="left" rowspan="1" colspan="1">18/21</td></tr></tbody></table></alternatives></table-wrap><p>We did not exclude any participants from the study. Chronologically, we conducted experiment 3 as our first study, followed by experiments 2, 1, 4.</p></sec></sec><sec id="sec023"><title>Computational modeling</title><sec id="sec024"><title>Goal probes.</title><p>We elicited goal probes from the participant every third round of the task. We relied on goal probe estimates to account for model predictions of goal persistence that is different from action selection. To ensure reliability in goal probes, we measured goal-action congruence (whether the participant made an action that is congruent to the goal in the round following the goal probe prompt. S2A Fig shows the goal-action congruence density histograms plotted across all experiments. The mean congruence in the target probes is 80%, 78%, 76%, and 73% for experiments 1 to 4.</p><p>We also estimated the proportion of the total number of switches that the participant made in the task that followed a goal probe. This is to verify whether the goal probe prompt itself influenced subsequent goal selection (either prompting a switch/prompting persistence in the specified probe). S2B Fig shows the mean proportion of switches are 31%, 32%, 29%, 32% for experiments 1 to 4. As the goal is probed every third round (in 33% of the rounds), the proportion of switches around the goal probes is in line with the chance expectation of 33%.</p><p>The congruence of goal action is high but not perfect in the participants. We anticipated this mismatch by our hierarchical assumption in our modeling, by dissociating participant goal and action selection. Participants can value a goal above other alternatives, but can temporarily explore other options before revisiting it. We found such patterns of revisiting in participants (<xref rid="pcbi.1013054.g003" ref-type="fig">Fig 3</xref>), indicating persistence in goal valuation. However, we still expect noise in goal probes in a portion of less-engaged participants (perhaps those showing low goal-action congruence) and that is a fundamental limitation in probing intrinsic goal states in participants. This noise could explain why the models do not capture the goal selection patterns in the task perfectly.</p></sec><sec id="sec025"><title>Prospective and retrospective models.</title><p><bold>Prospective.</bold> The prospective model updates the belief states of the current suit probabilities and rolls out the expected outcomes in the future.</p><p>The estimate of the probability of token collection for a suit <italic toggle="yes">g</italic> can be computed using a delta learning rule.</p><disp-formula id="pcbi.1013054.e081"><alternatives><graphic xlink:href="pcbi.1013054.e081.jpg" id="pcbi.1013054.e081g" position="anchor"/><mml:math id="M81" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(13)</label></disp-formula><p>Using the probability of token outcome, the prospective agent rolls out expectations one step into the future, for two scenarios: receiving a token and moving one token closer towards the target, and not receiving the token and staying in the same state. The value estimate for a goal <italic toggle="yes">g</italic> in a specific configuration state of the slots <italic toggle="yes">s</italic><sub><italic toggle="yes">g</italic></sub> is calculated as follows:</p><disp-formula id="pcbi.1013054.e082"><alternatives><graphic xlink:href="pcbi.1013054.e082.jpg" id="pcbi.1013054.e082g" position="anchor"/><mml:math id="M82" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mi>&#x003b3;</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(14)</label></disp-formula><p>where <italic toggle="yes">s</italic><sub><italic toggle="yes">g</italic></sub> is the current state of the goal (number of tokens collected) and <inline-formula id="pcbi.1013054.e083"><alternatives><graphic xlink:href="pcbi.1013054.e083.jpg" id="pcbi.1013054.e083g" position="anchor"/><mml:math id="M83" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the temporal discounting factor that ensures sooner rewards are valued higher over later rewards. This algorithm prefers the suit that can be completed fastest in a given context.</p><p>
<bold>Retrospective.</bold>
</p><p>The retrospective model values the goals with the greatest progress.</p><disp-formula id="pcbi.1013054.e084"><alternatives><graphic xlink:href="pcbi.1013054.e084.jpg" id="pcbi.1013054.e084g" position="anchor"/><mml:math id="M84" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>=</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(15)</label></disp-formula><p>As <italic toggle="yes">s</italic><sub><italic toggle="yes">g</italic></sub> is closer to the target, there is less discounting of rewards and therefore higher value. This formulation is chosen to ensure similar scaling and parameterization for prospective and retrospective models.</p></sec><sec id="sec026"><title>TD-momentum.</title><p>Momentum in the context of goal valuation is defined as the product of the progress made and the speed of progress. Here, the learnable parameter is the speed of progress. The goal value is formulated as a function approximator for momentum as follows.</p><disp-formula id="pcbi.1013054.e085"><alternatives><graphic xlink:href="pcbi.1013054.e085.jpg" id="pcbi.1013054.e085g" position="anchor"/><mml:math id="M85" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(16)</label></disp-formula><p>where <italic toggle="yes">Q</italic><sub><italic toggle="yes">g</italic></sub> is the value of a given suit <italic toggle="yes">g</italic> in a specific slot configuration <inline-formula id="pcbi.1013054.e086"><alternatives><graphic xlink:href="pcbi.1013054.e086.jpg" id="pcbi.1013054.e086g" position="anchor"/><mml:math id="M86" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">m</italic><sup>(<italic toggle="yes">t</italic>)</sup> is the progress (fraction of slots filled) at a given instant <italic toggle="yes">t</italic>, <inline-formula id="pcbi.1013054.e087"><alternatives><graphic xlink:href="pcbi.1013054.e087.jpg" id="pcbi.1013054.e087g" position="anchor"/><mml:math id="M87" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is the parameter for the speed of progress and <italic toggle="yes">b</italic><sub><italic toggle="yes">g</italic></sub> is a bias for the goal.</p><p>Depending on whether a token is received in a given round, the state of the goal is updated accordingly. The speed of progress is learned with the following temporal difference learning rule.</p><disp-formula id="pcbi.1013054.e088"><alternatives><graphic xlink:href="pcbi.1013054.e088.jpg" id="pcbi.1013054.e088g" position="anchor"/><mml:math id="M88" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(17)</label></disp-formula><disp-formula id="pcbi.1013054.e089"><alternatives><graphic xlink:href="pcbi.1013054.e089.jpg" id="pcbi.1013054.e089g" position="anchor"/><mml:math id="M89" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mi>&#x003b4;</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mi>&#x003b4;</mml:mi><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(18)</label></disp-formula><disp-formula id="pcbi.1013054.e090"><alternatives><graphic xlink:href="pcbi.1013054.e090.jpg" id="pcbi.1013054.e090g" position="anchor"/><mml:math id="M90" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mi>&#x003b4;</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(19)</label></disp-formula><p>We used a fixed initialization for <inline-formula id="pcbi.1013054.e091"><alternatives><graphic xlink:href="pcbi.1013054.e091.jpg" id="pcbi.1013054.e091g" position="anchor"/><mml:math id="M91" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> and <italic toggle="yes">b</italic><sub><italic toggle="yes">g</italic></sub>&#x02009;=&#x02009;0.1 for all goals. We tested different initializations to assess their impact on the model fitting and found that they are not significantly impacted by the choice of function approximator initialization.</p><p>We ensured that all the parameters of the model are recoverable through parameter recovery analysis (see <xref rid="pcbi.1013054.s005" ref-type="supplementary-material">S4 Fig</xref>).</p></sec><sec id="sec027"><title>Alternative hypotheses.</title><p>
<bold>TD Persistence.</bold>
</p><p>Our task can be solved by reducing it to a three-armed bandit task and finding the optimal bandit in each block that gives you the most rewards and persists with it until the end of the block. We assumed that this might be a plausible strategy in the experimental variant where we clearly inform the participants of the token contingencies of the block.</p><p>The token-outcome expectations from each goal, calculated using a delta rule, drive the choices in this algorithm. Once the value of a certain token sufficiently exceeds its alternatives, the persistence parameters in the action policy (described later) ensure persistence towards the token.</p><disp-formula id="pcbi.1013054.e092"><alternatives><graphic xlink:href="pcbi.1013054.e092.jpg" id="pcbi.1013054.e092g" position="anchor"/><mml:math id="M92" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(20)</label></disp-formula><disp-formula id="pcbi.1013054.e093"><alternatives><graphic xlink:href="pcbi.1013054.e093.jpg" id="pcbi.1013054.e093g" position="anchor"/><mml:math id="M93" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(21)</label></disp-formula><p>
<bold>Hybrid.</bold>
</p><p>A potential mechanism to incorporate both prospective and retrospective influences is to assume a fixed weight arbitration model that weights contributions from both valuations and their combined influence drives decision making.</p><disp-formula id="pcbi.1013054.e094"><alternatives><graphic xlink:href="pcbi.1013054.e094.jpg" id="pcbi.1013054.e094g" position="anchor"/><mml:math id="M94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo>*</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(22)</label></disp-formula></sec><sec id="sec028"><title>Common mechanism of goal and action selection.</title><p>All competing hypotheses of goal valuation followed a common action selection policy elucidated as follows.</p><p>
<bold>Algorithm 1 Algorithm for goal selection.</bold>
</p><p specific-use="line">
<monospace>1: <italic toggle="yes">Q</italic><sub><italic toggle="yes">g</italic></sub>: goal value where <inline-formula id="pcbi.1013054.e069"><alternatives><graphic xlink:href="pcbi.1013054.e069.jpg" id="pcbi.1013054.e069g" position="anchor"/><mml:math id="M69" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#x02208;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> {cat, hat, car}</monospace>
</p><p specific-use="line">
<monospace>2: <inline-formula id="pcbi.1013054.e070"><alternatives><graphic xlink:href="pcbi.1013054.e070.jpg" id="pcbi.1013054.e070g" position="anchor"/><mml:math id="M70" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>: softmax temperatures for goal and action selection</monospace>
</p><p specific-use="line">
<monospace>3: <italic toggle="yes">c</italic><sub><italic toggle="yes">g</italic></sub>: goal preservation/choice kernel</monospace>
</p><p specific-use="line">
<monospace>4: <italic toggle="yes">c</italic>: cost of switching between goals</monospace>
</p><p specific-use="line">
<monospace>5: <italic toggle="yes">G</italic>: current goal (none at start)</monospace>
</p><p specific-use="line">
<monospace>6: <bold>for</bold> block in (1, <italic toggle="yes">num_blocks</italic>) <bold>do</bold></monospace>
</p><p specific-use="line">
<monospace>7: &#x02003; Initialize card probabilities</monospace>
</p><p specific-use="line">
<monospace>8: &#x02003; <bold>for</bold> round in (1, <italic toggle="yes">num_rounds</italic>) <bold>do</bold></monospace>
</p><p specific-use="line">
<monospace>9: &#x02003;&#x02003; Get goal values (<inline-formula id="pcbi.1013054.e071"><alternatives><graphic xlink:href="pcbi.1013054.e071.jpg" id="pcbi.1013054.e071g" position="anchor"/><mml:math id="M71" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>) <inline-formula id="pcbi.1013054.e072"><alternatives><graphic xlink:href="pcbi.1013054.e072.jpg" id="pcbi.1013054.e072g" position="anchor"/><mml:math id="M72" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x022b3;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> Using</monospace>
</p><p specific-use="line">
<monospace>&#x02003;&#x02003; TD-momentum/prospective</monospace>
</p><p specific-use="line">
<monospace>10: &#x02003;&#x02003; Calculate advantage of current goal G:</monospace>
</p><p specific-use="line">
<monospace>&#x02003;&#x02003; <inline-formula id="pcbi.1013054.e073"><alternatives><graphic xlink:href="pcbi.1013054.e073.jpg" id="pcbi.1013054.e073g" position="anchor"/><mml:math id="M73" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="normal">max</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula></monospace>
</p><p specific-use="line">
<monospace>11: &#x02003;&#x02003; Choose to stay with/switch from the current goal:</monospace>
</p><p specific-use="line">
<monospace>&#x02003; &#x02003; <inline-formula id="pcbi.1013054.e074"><alternatives><graphic xlink:href="pcbi.1013054.e074.jpg" id="pcbi.1013054.e074g" position="anchor"/><mml:math id="M74" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="normal">stay</mml:mi><mml:mo>;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>c</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>
<inline-formula id="pcbi.1013054.e075"><alternatives><graphic xlink:href="pcbi.1013054.e075.jpg" id="pcbi.1013054.e075g" position="anchor"/><mml:math id="M75" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x022b3;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> Choose goal</monospace>
</p><p specific-use="line">
<monospace>12: &#x02003;&#x02003; In case of a switch, chose the new goal according to</monospace>
</p><p specific-use="line">
<monospace>&#x02003;&#x02003; goal values:</monospace>
</p><p specific-use="line">
<monospace>&#x02003;&#x02003; <inline-formula id="pcbi.1013054.e076"><alternatives><graphic xlink:href="pcbi.1013054.e076.jpg" id="pcbi.1013054.e076g" position="anchor"/><mml:math id="M76" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></monospace>
</p><p specific-use="line">
<monospace>13: &#x02003;&#x02003; Update current goal <italic toggle="yes">G</italic></monospace>
</p><p specific-use="line">
<monospace>14: &#x02003;&#x02003; Update choice kernel of goal selection</monospace>
</p><p specific-use="line">
<monospace>15: &#x02003;&#x02003; Report the goal <italic toggle="yes">G</italic> every third round</monospace>
</p><p specific-use="line">
<monospace>16: &#x02003;&#x02003; Choose to act according to the current goal or explore</monospace>
</p><p specific-use="line">
<monospace>&#x02003;&#x02003; <inline-formula id="pcbi.1013054.e077"><alternatives><graphic xlink:href="pcbi.1013054.e077.jpg" id="pcbi.1013054.e077g" position="anchor"/><mml:math id="M77" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">stay</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>c</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>
<inline-formula id="pcbi.1013054.e078"><alternatives><graphic xlink:href="pcbi.1013054.e078.jpg" id="pcbi.1013054.e078g" position="anchor"/><mml:math id="M78" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x022b3;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> Choose action</monospace>
</p><p specific-use="line">
<monospace>17: &#x02003;&#x02003; In case of a switch, explore a new action according to</monospace>
</p><p specific-use="line">
<monospace>&#x02003;&#x02003; goal values:</monospace>
</p><p specific-use="line">
<monospace>&#x02003;&#x02003; <inline-formula id="pcbi.1013054.e079"><alternatives><graphic xlink:href="pcbi.1013054.e079.jpg" id="pcbi.1013054.e079g" position="anchor"/><mml:math id="M79" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></monospace>
</p><p specific-use="line">
<monospace>18: &#x02003;&#x02003; Observe outcome</monospace>
</p><p specific-use="line">
<monospace>19: &#x02003;&#x02003; Update goal-outcome probabilities <inline-formula id="pcbi.1013054.e080"><alternatives><graphic xlink:href="pcbi.1013054.e080.jpg" id="pcbi.1013054.e080g" position="anchor"/><mml:math id="M80" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x022b3;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> For</monospace>
</p><p specific-use="line">
<monospace>&#x02003;&#x02003; prospective/hybrid/TD-persistence</monospace>
</p><p specific-use="line">
<monospace>20: &#x02003;&#x02003; Update goal values</monospace>
</p><p specific-use="line">
<monospace>21: &#x02003; <bold>end for</bold></monospace>
</p><p specific-use="line">
<monospace>22: <bold>end for</bold></monospace>
</p><p>Once the values of individual goals are computed, the advantage of the current goal, <italic toggle="yes">A</italic><sub><italic toggle="yes">G</italic></sub>, over its alternatives is</p><disp-formula id="pcbi.1013054.e095"><alternatives><graphic xlink:href="pcbi.1013054.e095.jpg" id="pcbi.1013054.e095g" position="anchor"/><mml:math id="M95" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="normal">max</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(23)</label></disp-formula><p>The agent chooses to stay on the current goal (target suit) with a probability <inline-formula id="pcbi.1013054.e096"><alternatives><graphic xlink:href="pcbi.1013054.e096.jpg" id="pcbi.1013054.e096g" position="anchor"/><mml:math id="M96" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">stay</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>,</p><disp-formula id="pcbi.1013054.e097"><alternatives><graphic xlink:href="pcbi.1013054.e097.jpg" id="pcbi.1013054.e097g" position="anchor"/><mml:math id="M97" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">stay</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>c</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(24)</label></disp-formula><p>where <italic toggle="yes">c</italic> is the switching cost, <inline-formula id="pcbi.1013054.e098"><alternatives><graphic xlink:href="pcbi.1013054.e098.jpg" id="pcbi.1013054.e098g" position="anchor"/><mml:math id="M98" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is the softmax temperature to scale the advantage. <italic toggle="yes">c</italic><sub><italic toggle="yes">G</italic></sub> is the choice kernel where commitment towards the goal increases the more it is chosen in the past as updated through the delta-rule,</p><disp-formula id="pcbi.1013054.e099"><alternatives><graphic xlink:href="pcbi.1013054.e099.jpg" id="pcbi.1013054.e099g" position="anchor"/><mml:math id="M99" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(25)</label></disp-formula><p>where <italic toggle="yes">I</italic><sub><italic toggle="yes">G</italic></sub> is the indicator that the goal is chosen in the round.</p><p>When the agent decides to switch away from the current goal with probability (1&#x02212;<italic toggle="yes">p</italic><sub><italic toggle="yes">stay</italic></sub>), it selects a new goal from the two alternate goals depending on the values of the two alternate goals ( <inline-formula id="pcbi.1013054.e100"><alternatives><graphic xlink:href="pcbi.1013054.e100.jpg" id="pcbi.1013054.e100g" position="anchor"/><mml:math id="M100" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>)</p><p>Once the goal is chosen, the participant chooses to perform an action in accordance with the goal (choosing the cards) with a probability, <inline-formula id="pcbi.1013054.e101"><alternatives><graphic xlink:href="pcbi.1013054.e101.jpg" id="pcbi.1013054.e101g" position="anchor"/><mml:math id="M101" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">stay</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, which is modulated by a separate softmax term <inline-formula id="pcbi.1013054.e102"><alternatives><graphic xlink:href="pcbi.1013054.e102.jpg" id="pcbi.1013054.e102g" position="anchor"/><mml:math id="M102" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.</p><p>We chose separate softmax terms for actions and goals to allow goals to change at a different rate than actions. Participants can switch actions to explore other offers while staying on the same high level goal, and can choose to switch goals when the value of an alternative goal exceeds that of the current one. This framing can account for both the choices and the goal probes reported by the participants. Algorithm 1 shows the pseudocode for the common goal and action selection mechanism.</p></sec><sec id="sec029"><title>Comparisons with simpler versions: Rescorla-Wagner + choice kernel.</title><p>We contrasted the models with a much simpler Rescorla-Wagner model embedded with a choice kernel.</p><disp-formula id="pcbi.1013054.e103"><alternatives><graphic xlink:href="pcbi.1013054.e103.jpg" id="pcbi.1013054.e103g" position="anchor"/><mml:math id="M103" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(26)</label></disp-formula><disp-formula id="pcbi.1013054.e104"><alternatives><graphic xlink:href="pcbi.1013054.e104.jpg" id="pcbi.1013054.e104g" position="anchor"/><mml:math id="M104" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(27)</label></disp-formula><p>where <italic toggle="yes">Q</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>) is the expected value of token rewards for each goal, <italic toggle="yes">r</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>) is the binary reward of receiving a token, <italic toggle="yes">c</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>) is the choice kernel for the goal, <italic toggle="yes">I</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>) is the binary indicator value for the goal chosen each round.</p><p>The goal selection probability is given as</p><disp-formula id="pcbi.1013054.e105"><alternatives><graphic xlink:href="pcbi.1013054.e105.jpg" id="pcbi.1013054.e105g" position="anchor"/><mml:math id="M105" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(28)</label></disp-formula></sec><sec id="sec030"><title>Model fitting procedure.</title><p>We fit both participant card selections and goal probes in all models except for the Rescorla-Wagner model, where we just fit participants&#x02019; choices (where there is no notion of an extended goal).</p><p>The likelihood of a goal probe is estimated only in the rounds where goal probes are presented in the task (i.e., every third round), whereas the likelihood of an action is estimated in every round. In rounds where the participant does not report their goal, the goal remains latent, and the model estimates selection probabilities across all possible goals. Goal and action likelihoods are then computed by integrating over these latent goal probabilities. The full likelihood estimation process for goal and action selections in the task is summarized as follows.</p><list list-type="order"><list-item><p>Estimating goal selection likelihood:<list list-type="simple"><list-item><p>i. If the participant&#x02019;s goal is probed in round <italic toggle="yes">k</italic> (say the goal reported is <italic toggle="yes">cat</italic>)<disp-formula id="pcbi.1013054.e106"><alternatives><graphic xlink:href="pcbi.1013054.e106.jpg" id="pcbi.1013054.e106g" position="anchor"/><mml:math id="M106" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>;</mml:mo><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>;</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(29)</label></disp-formula></p></list-item><list-item><p>ii. For round <italic toggle="yes">k</italic>&#x02009;+&#x02009;1, the latent goal probabilities are estimated given latent goal in <italic toggle="yes">k</italic>:<disp-formula id="pcbi.1013054.e107"><alternatives><graphic xlink:href="pcbi.1013054.e107.jpg" id="pcbi.1013054.e107g" position="anchor"/><mml:math id="M107" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(30)</label></disp-formula></p><p>Conditional probability of choosing a goal <italic toggle="yes">g</italic> in round <italic toggle="yes">t</italic> given the previous goal <italic toggle="yes">G</italic> in round <italic toggle="yes">t</italic>&#x02013;1 is defined as:<disp-formula id="pcbi.1013054.e108"><alternatives><graphic xlink:href="pcbi.1013054.e108.jpg" id="pcbi.1013054.e108g" position="anchor"/><mml:math id="M108" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mspace width="1em"/><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">stay</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>g</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>G</mml:mi><mml:mspace width="1em"/><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">switch</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p><p>where <italic toggle="yes">p</italic><sub><italic toggle="yes">stay</italic></sub> is estimated as described in Equation (20), and the softmax is computed over the two alternative goal values.</p></list-item><list-item><p>iii. For round <italic toggle="yes">k</italic>&#x02009;+&#x02009;2, the latent goal probabilities are estimated given latent goal probabilities in <italic toggle="yes">k</italic>&#x02009;+&#x02009;1:<disp-formula id="pcbi.1013054.e109"><alternatives><graphic xlink:href="pcbi.1013054.e109.jpg" id="pcbi.1013054.e109g" position="anchor"/><mml:math id="M109" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>g</mml:mi></mml:msub><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(31)</label></disp-formula></p></list-item><list-item><p>iv. The goal is then probed again in round <italic toggle="yes">k</italic>&#x02009;+&#x02009;3, with the goal selection probabilities computed given the latent goal probabilities in <italic toggle="yes">k</italic>&#x02009;+&#x02009;2. This estimation is used to compute the likelihood of goal selection for the participant&#x02019;s goal probe in round <italic toggle="yes">k</italic>&#x02009;+&#x02009;3.<disp-formula id="pcbi.1013054.e110"><alternatives><graphic xlink:href="pcbi.1013054.e110.jpg" id="pcbi.1013054.e110g" position="anchor"/><mml:math id="M110" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>g</mml:mi></mml:msub><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(32)</label></disp-formula></p></list-item><list-item><p>v. The goal probabilities in round <inline-formula id="pcbi.1013054.e111"><alternatives><graphic xlink:href="pcbi.1013054.e111.jpg" id="pcbi.1013054.e111g" position="anchor"/><mml:math id="M111" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mn>3</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> are aligned with the latest goal probe (as in step (a)).</p></list-item></list>
</p></list-item><list-item><p>Estimating action selection likelihood:<disp-formula id="pcbi.1013054.e112"><alternatives><graphic xlink:href="pcbi.1013054.e112.jpg" id="pcbi.1013054.e112g" position="anchor"/><mml:math id="M112" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:mo movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(33)</label></disp-formula></p></list-item></list><p>One limitation of this procedure is that we hold the goal probe reported by the participant as the true value of the latent goal variable. However, the goal reports of participants could be noisy in disengaged participants. The participant goal probe statistics reported in S2A and S2B Fig can alleviate some of these concerns given the high congruence of goal action seen in the participant data.</p><p>TD-persistence has 5 free parameters, TD-momentum and prospective agent have 6, and hybrid agent has 7. For each individual participant, we estimate the parameters for each of the competing models to maximize the likelihood of their action choices and goal probes by employing a tree-structured parzen estimator (TPE) sampler, a Bayesian optimization algorithm implemented in optuna [<xref rid="pcbi.1013054.ref045" ref-type="bibr">45</xref>], an open source hyperparameter optimization framework in Python. We estimated the parameters that maximized the likelihood over 100 random starts.</p><p>We followed the same procedure for fitting behavior and for recovering parameters and models.</p></sec><sec id="sec031"><title>Parameter recovery.</title><p>For parameter recovery analysis, we sampled 1000 parameter vectors from a range of parameters (<inline-formula id="pcbi.1013054.e113"><alternatives><graphic xlink:href="pcbi.1013054.e113.jpg" id="pcbi.1013054.e113g" position="anchor"/><mml:math id="M113" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mrow/><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mn>1.0</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>). We generate synthetic behavior for each parameter vector using the TD-momentum model and recover the parameters by fitting the model to the simulated data. We optimized both the likelihoods of selection of the goals and the actions while fitting the data, where the simulated agent reported the goal every third round as the participants did. We plotted the fitted parameters and the generated parameters and reported correlations (see <xref rid="pcbi.1013054.s005" ref-type="supplementary-material">S4 Fig</xref>). Most of the parameters are recoverable with high correlation coefficients. However, low discount factors closer to 0.5 do not recover well; however, the discount factors of the participant fits are mostly in the range <inline-formula id="pcbi.1013054.e114"><alternatives><graphic xlink:href="pcbi.1013054.e114.jpg" id="pcbi.1013054.e114g" position="anchor"/><mml:math id="M114" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.9</mml:mn><mml:mo>,</mml:mo><mml:mn>1.0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, so we considered the range <inline-formula id="pcbi.1013054.e115"><alternatives><graphic xlink:href="pcbi.1013054.e115.jpg" id="pcbi.1013054.e115g" position="anchor"/><mml:math id="M115" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mn>1.0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for parameter recovery.</p></sec><sec id="sec032"><title>Model recovery.</title><p>We performed a model recovery analysis on the competing models. We sampled 100 random parameter vectors from parameter ranges derived from individual participant fits. For each of the four algorithms (TD-momentum, TD-persistence, prospective, hybrid), we generated choice data using the sampled parameters. We then fit the generated data to each of the four models and identify the winning model through BIC score comparisons. We plotted the confusion matrices where the rows correspond to the simulated model and the columns correspond to the recovered model (see <xref rid="pcbi.1013054.s006" ref-type="supplementary-material">S5 Fig</xref>). The left confusion matrix indicates the probability of fitted model given the simulated model, whereas the right matrix indicates the probability of simulated model given the fitted model. In all cases, we observed that the data generating model provides the best explanation for the choice data. However, there appears to be some confusion between the prospective and hybrid models.</p></sec><sec id="sec033"><title>Normative simulations.</title><p>For the normative account, we assumed three starting positions for the availability of three options: <inline-formula id="pcbi.1013054.e116"><alternatives><graphic xlink:href="pcbi.1013054.e116.jpg" id="pcbi.1013054.e116g" position="anchor"/><mml:math id="M116" display="inline" overflow="scroll"><mml:mrow><mml:mi>~</mml:mi><mml:mi>&#x1d4b0;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1013054.e117"><alternatives><graphic xlink:href="pcbi.1013054.e117.jpg" id="pcbi.1013054.e117g" position="anchor"/><mml:math id="M117" display="inline" overflow="scroll"><mml:mrow><mml:mi>~</mml:mi><mml:mi>&#x1d4b0;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1013054.e118"><alternatives><graphic xlink:href="pcbi.1013054.e118.jpg" id="pcbi.1013054.e118g" position="anchor"/><mml:math id="M118" display="inline" overflow="scroll"><mml:mrow><mml:mi>~</mml:mi><mml:mi>&#x1d4b0;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0.7</mml:mn><mml:mo>,</mml:mo><mml:mn>0.8</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. We simulated two variants: token contingencies that gradually changed through Gaussian random walks (<inline-formula id="pcbi.1013054.e119"><alternatives><graphic xlink:href="pcbi.1013054.e119.jpg" id="pcbi.1013054.e119g" position="anchor"/><mml:math id="M119" display="inline" overflow="scroll"><mml:mrow><mml:mi>~</mml:mi><mml:mi>&#x1d4a9;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.025</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) and abrupt switching of contingencies with probabilities shuffling every 30 rounds.</p><p>We computed the optimal performance achieved by each algorithm with parameters estimated by Bayesian optimization.</p></sec></sec><sec sec-type="supplementary-material" id="sec034"><title>Supporting information</title><supplementary-material id="pcbi.1013054.s001" position="float" content-type="local-data"><label>S1 Appendix</label><caption><title>Experimenting with variants of the prospective model.</title><p>(PDF)</p></caption><media xlink:href="pcbi.1013054.s001.pdf"/></supplementary-material><supplementary-material id="pcbi.1013054.s002" position="float" content-type="local-data"><label>S1 Fig</label><caption><title>Goal selections with no conflict.</title><p><bold>A.</bold> When the dominant and maximum progress suits are the same, participants predominantly choose the dominant/maximum progress option because it is devoid of goal conflict between accrued progress and the current rate of progress. <bold>B.</bold> Similar pattern of preference is shown when suits are classified as prospective and retrospective (based on predictions of the prospective agent optimized for the task).</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s002.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s003" position="float" content-type="local-data"><label>S2 Fig</label><caption><title>A. Goal-action congruence.</title><p>Figure shows the frequency distribution of the proportion of rounds where the participant performed action in congruence with the objective probe in the next round. <bold>A. Switches after goal probes</bold> Frequency distribution of the proportion of the total number of switches made by participants that occur in the round following the goal probe.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s003.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s004" position="float" content-type="local-data"><label>S3 Fig</label><caption><title>A. Proportion of dominant choice traced with task progress.</title><p>Blocks are split into groups of 3 to track progress throughout the task. There is no indication of increased persistence at the end of the task (last three blocks) compared to the beginning (first three blocks), except in experiment 2 where there is a marginal decrease in preference towards the dominant suit at the end. <bold>B.</bold> Proportion of the choice of dominant suit with no grouping of blocks. Oscillations in dominant choice proportion correspond to the block condition encountered.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s004.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s005" position="float" content-type="local-data"><label>S4 Fig</label><caption><title>TD Momentum parameters are recoverable.</title><p>1000 randomly generated parameter vectors were used to generate synthetic behavior and were recovered by fitting the model (recovery analysis in experiment 1).</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s005.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s006" position="float" content-type="local-data"><label>S5 Fig</label><caption><title>Competing computational hypotheses are individually recoverable.</title><p>Data generating model offers the best explanation for all competing hypotheses (recovery analysis in experiment 1).</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s006.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s007" position="float" content-type="local-data"><label>S6 Fig</label><caption><title>The simple variant of prospection with symmetric learning rates outperforms other variants (model fits for experiment 1).</title><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s007.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s008" position="float" content-type="local-data"><label>S7 Fig</label><caption><title>Optimal performance of models.</title><p>Number of suits collected by each agent broken down by block type when their parameters are optimized for task performance. Prospective and TD-persistence agents perform the best, and the retrospective agent gives the lowest performance. TD momentum performs better than the retrospective agent but is short of the other agents.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s008.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s009" position="float" content-type="local-data"><label>S8 Fig</label><caption><title>Heterogeneity in strategies in the version with explicit instructions.</title><p>40% of participants in the version with explicit instructions about the current block type were better accounted for by TD-persistence, with the rest 60% favoring TD-momentum according to BIC criteria. Lower retrospective bias and higher task performance is observed in the group that is better accounted by TD-persistence. Retrospective bias of the participants explained by TD-persistence follows the similar pattern as demonstrated by simulations of the algorithm.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s009.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s010" position="float" content-type="local-data"><label>S9 Fig</label><caption><title>Learning effects within blocks.</title><p>Figure shows the participants&#x02019; dominant suit choice probability in the first and second halves of the block across all experiments replicated by TD-momentum.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s010.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s011" position="float" content-type="local-data"><label>S10 Fig</label><caption><title>Model predictions of the prospective agent.</title><p>Prospective agent captures general trends in preferences towards maximum progress suit and switching patterns in the task.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s011.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s012" position="float" content-type="local-data"><label>S11 Fig</label><caption><title>Model predictions of the hybrid agent.</title><p>Hybrid agent captures general trends in preferences towards maximum progress suit and switching patterns in the task.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s012.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s013" position="float" content-type="local-data"><label>S12 Fig</label><caption><title>Model predictions of the TD-persistence agent.</title><p>TD-persistence agent does not capture preferences towards maximum progress and also shows increased preference towards switching away from the retrospective agent in contrast to the participants.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s013.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s014" position="float" content-type="local-data"><label>S13 Fig</label><caption><title>Model predictions of the Rescorla-Wagner agent.</title><p>Rescorla-Wagner agent neither captures preferences towards maximum progress nor patterns of switching seen in participants.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s014.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s015" position="float" content-type="local-data"><label>S14 Fig</label><caption><title>Experiment 4: Goal selection patterns in hybrid and prospective agents.</title><p>Both hybrid and prospective agents show a preference for the cat suit (smallest target) across conditions.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s015.tif"/></supplementary-material><supplementary-material id="pcbi.1013054.s016" position="float" content-type="local-data"><label>S15 Fig</label><caption><title>Experiment 4: Goal selection patterns comparing optimal prospective agent and behavior-fitted prospective agents.</title><p>Optimal prospective agent prefers optimal suit in each condition. Prospective agent fitted to participant data displays bias towards the cat suit.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1013054.s016.tif"/></supplementary-material></sec></sec></body><back><ack><p>We thank Antonio Rangel, Colin Camerer, Caltech SDN graduate program students, and O&#x02019;Doherty lab members for their valuable feedback and support throughout the development of the manuscript.</p></ack><ref-list><title>References</title><ref id="pcbi.1013054.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Molinaro</surname><given-names>G</given-names></name>, <name><surname>Collins</surname><given-names>AGE</given-names></name>. <article-title>A goal-centric outlook on learning</article-title>. <source>Trends Cogn Sci</source>. <year>2023</year>;<volume>27</volume>(<issue>12</issue>):<fpage>1150</fpage>&#x02013;<lpage>64</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.tics.2023.08.011</pub-id>
<pub-id pub-id-type="pmid">37696690</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>S</given-names></name>, <name><surname>Zhao</surname><given-names>M</given-names></name>, <name><surname>Tang</surname><given-names>N</given-names></name>, <name><surname>Zhao</surname><given-names>Y</given-names></name>, <name><surname>Zhou</surname><given-names>J</given-names></name>, <name><surname>Shen</surname><given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Intention beyond desire: spontaneous intentional commitment regulates conflicting desires</article-title>. <source>Cognition</source>. <year>2023</year>;<volume>238</volume>:<fpage>105513</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cognition.2023.105513</pub-id>
<pub-id pub-id-type="pmid">37331323</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref003"><label>3</label><mixed-citation publication-type="book"><name><surname>Molinaro</surname><given-names>G</given-names></name>, <name><surname>Colas</surname><given-names>C</given-names></name>, <name><surname>Oudeyer</surname><given-names>P-Y</given-names></name>, <name><surname>Collins</surname><given-names>A</given-names></name>. <source>Latent learning progress drives autonomous goal selection in human 7reinforcement learning</source>. <publisher-name>Center for Open Science</publisher-name>. <year>2024</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.31234/osf.io/5dtx3</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Poli</surname><given-names>F</given-names></name>, <name><surname>Meyer</surname><given-names>M</given-names></name>, <name><surname>Mars</surname><given-names>RB</given-names></name>, <name><surname>Hunnius</surname><given-names>S</given-names></name>. <article-title>Contributions of expected learning progress and perceptual novelty to curiosity-driven exploration</article-title>. <source>Cognition</source>. <year>2022</year>;<volume>225</volume>:<fpage>105119</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cognition.2022.105119</pub-id>
<pub-id pub-id-type="pmid">35421742</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref005"><label>5</label><mixed-citation publication-type="book"><name><surname>Sukhov</surname><given-names>N</given-names></name>, <name><surname>Dubey</surname><given-names>R</given-names></name>, <name><surname>Duke</surname><given-names>A</given-names></name>, <name><surname>Griffiths</surname><given-names>T</given-names></name>. <source>When to keep trying and when to let go: benchmarking optimal quitting</source>. <publisher-name>Center for Open Science</publisher-name>. <year>2023</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.31234/osf.io/gjucy</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Freyd</surname><given-names>JJ</given-names></name>, <name><surname>Finke</surname><given-names>RA</given-names></name>. <article-title>Representational momentum.</article-title>
<source>J Exp Psychol: Learn Memory Cognit</source>. <year>1984</year>;<volume>10</volume>(<issue>1</issue>):<fpage>126</fpage>&#x02013;<lpage>32</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/0278-7393.10.1.126</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Hubbard</surname><given-names>TL</given-names></name>. <article-title>Representational momentum and related displacements in spatial memory: a review of the findings</article-title>. <source>Psychon Bull Rev</source>. <year>2005</year>;<volume>12</volume>(<issue>5</issue>):<fpage>822</fpage>&#x02013;<lpage>51</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3758/bf03196775</pub-id>
<pub-id pub-id-type="pmid">16524000</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Kerzel</surname><given-names>D</given-names></name>. <article-title>Representational momentum beyond internalized physics</article-title>. <source>Curr Dir Psychol Sci</source>. <year>2005</year>;<volume>14</volume>(<issue>4</issue>):<fpage>180</fpage>&#x02013;<lpage>4</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/j.0963-7214.2005.00360.x</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Nevin</surname><given-names>JA</given-names></name>, <name><surname>Mandell</surname><given-names>C</given-names></name>, <name><surname>Atak</surname><given-names>JR</given-names></name>. <article-title>The analysis of behavioral momentum</article-title>. <source>J Exp Anal Behav</source>. <year>1983</year>;<volume>39</volume>(<issue>1</issue>):<fpage>49</fpage>&#x02013;<lpage>59</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1901/jeab.1983.39-49</pub-id>
<pub-id pub-id-type="pmid">16812312</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Plaud</surname><given-names>JJ</given-names></name>, <name><surname>Gaither</surname><given-names>GA</given-names></name>, <name><surname>Lawrence</surname><given-names>JB</given-names></name>. <article-title>Operant schedule transformations and human behavioral momentum</article-title>. <source>J Behav Ther Exp Psychiatry</source>. <year>1997</year>;<volume>28</volume>(<issue>3</issue>):<fpage>169</fpage>&#x02013;<lpage>79</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/s0005-7916(97)00007-4</pub-id>
<pub-id pub-id-type="pmid">9327296</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Mace</surname><given-names>FC</given-names></name>, <name><surname>Hock</surname><given-names>ML</given-names></name>, <name><surname>Lalli</surname><given-names>JS</given-names></name>, <name><surname>West</surname><given-names>BJ</given-names></name>, <name><surname>Belfiore</surname><given-names>P</given-names></name>, <name><surname>Pinter</surname><given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Behavioral momentum in the treatment of noncompliance</article-title>. <source>J Appl Behav Anal</source>. <year>1988</year>;<volume>21</volume>(<issue>2</issue>):<fpage>123</fpage>&#x02013;<lpage>41</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1901/jaba.1988.21-123</pub-id>
<pub-id pub-id-type="pmid">2971034</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Nevin</surname><given-names>JA</given-names></name>, <name><surname>Shahan</surname><given-names>TA</given-names></name>. <article-title>Behavioral momentum theory: equations and applications</article-title>. <source>J Appl Behav Anal</source>. <year>2011</year>;<volume>44</volume>(<issue>4</issue>):<fpage>877</fpage>&#x02013;<lpage>95</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1901/jaba.2011.44-877</pub-id>
<pub-id pub-id-type="pmid">22219536</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Nevin</surname><given-names>JA</given-names></name>, <name><surname>Grace</surname><given-names>RC</given-names></name>. <article-title>Behavioral momentum and the law of effect</article-title>. <source>Behav Brain Sci</source>. <year>2000</year>;<volume>23</volume>(<issue>1</issue>):<fpage>73</fpage>&#x02013;<lpage>90</lpage>; discussion 90-130. <comment>doi: </comment><pub-id pub-id-type="doi">10.1017/s0140525x00002405</pub-id>
<pub-id pub-id-type="pmid">11303339</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref014"><label>14</label><mixed-citation publication-type="confproc"><name><surname>Sutskever</surname><given-names>I</given-names></name>, <name><surname>Martens</surname><given-names>J</given-names></name>, <name><surname>Dahl</surname><given-names>G</given-names></name>, <name><surname>Hinton</surname><given-names>G</given-names></name>. <conf-name>On the importance of initialization and momentum in deep learning</conf-name>. In: <conf-name>Proceedings of the International Conference on Machine Learning</conf-name>. <year>2013</year>.</mixed-citation></ref><ref id="pcbi.1013054.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Rutledge</surname><given-names>RB</given-names></name>, <name><surname>Skandali</surname><given-names>N</given-names></name>, <name><surname>Dayan</surname><given-names>P</given-names></name>, <name><surname>Dolan</surname><given-names>RJ</given-names></name>. <article-title>A computational and neural model of momentary subjective well-being</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2014</year>;<volume>111</volume>(<issue>33</issue>):<fpage>12252</fpage>&#x02013;<lpage>7</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.1407535111</pub-id>
<pub-id pub-id-type="pmid">25092308</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Eldar</surname><given-names>E</given-names></name>, <name><surname>Rutledge</surname><given-names>RB</given-names></name>, <name><surname>Dolan</surname><given-names>RJ</given-names></name>, <name><surname>Niv</surname><given-names>Y</given-names></name>. <article-title>Mood as representation of momentum</article-title>. <source>Trends Cogn Sci</source>. <year>2016</year>;<volume>20</volume>(<issue>1</issue>):<fpage>15</fpage>&#x02013;<lpage>24</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.tics.2015.07.010</pub-id>
<pub-id pub-id-type="pmid">26545853</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Briki</surname><given-names>W</given-names></name>, <name><surname>Markman</surname><given-names>KD</given-names></name>. <article-title>Psychological momentum: the phenomenology of goal pursuit</article-title>. <source>Soc Personal Psych</source>. <year>2018</year>;<volume>12</volume>(<issue>9</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/spc3.12412</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Markman</surname><given-names>KD</given-names></name>, <name><surname>Guenther</surname><given-names>CL</given-names></name>. <article-title>Psychological momentum: intuitive physics and naive beliefs</article-title>. <source>Pers Soc Psychol Bull</source>. <year>2007</year>;<volume>33</volume>(<issue>6</issue>):<fpage>800</fpage>&#x02013;<lpage>12</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1177/0146167207301026</pub-id>
<pub-id pub-id-type="pmid">17488872</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Honey</surname><given-names>CJ</given-names></name>, <name><surname>Mahabal</surname><given-names>A</given-names></name>, <name><surname>Bellana</surname><given-names>B</given-names></name>. <article-title>Psychological momentum</article-title>. <source>Curr Dir Psychol Sci</source>. <year>2023</year>;<volume>32</volume>(<issue>4</issue>):<fpage>284</fpage>&#x02013;<lpage>92</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1177/09637214221143053</pub-id>
<pub-id pub-id-type="pmid">37786409</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Constantino</surname><given-names>SM</given-names></name>, <name><surname>Daw</surname><given-names>ND</given-names></name>. <article-title>Learning the opportunity cost of time in a patch-foraging task</article-title>. <source>Cogn Affect Behav Neurosci</source>. <year>2015</year>;<volume>15</volume>(<issue>4</issue>):<fpage>837</fpage>&#x02013;<lpage>53</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3758/s13415-015-0350-y</pub-id>
<pub-id pub-id-type="pmid">25917000</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Kendall</surname><given-names>RK</given-names></name>, <name><surname>Wikenheiser</surname><given-names>AM</given-names></name>. <article-title>Quitting while you&#x02019;re ahead: patch foraging and temporal cognition.</article-title>. <source>Behav Neurosci</source>. <year>2022</year>;<volume>136</volume>(<issue>5</issue>):<fpage>467</fpage>&#x02013;<lpage>78</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/bne0000526</pub-id><pub-id pub-id-type="pmid">35834190</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Charnov</surname><given-names>EL</given-names></name>. <article-title>Optimal foraging, the marginal value theorem</article-title>. <source>Theor Populat Biol</source>. <year>1976</year>;<volume>9</volume>(<issue>2</issue>):<fpage>129</fpage>&#x02013;<lpage>36</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0040-5809(76)90040-x</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Arkes</surname><given-names>HR</given-names></name>, <name><surname>Blumer</surname><given-names>C</given-names></name>. <article-title>The psychology of sunk cost</article-title>. <source>Organiz Behav Hum Decis Process</source>. <year>1985</year>;<volume>35</volume>(<issue>1</issue>):<fpage>124</fpage>&#x02013;<lpage>40</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0749-5978(85)90049-4</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Arkes</surname><given-names>HR</given-names></name>, <name><surname>Ayton</surname><given-names>P</given-names></name>. <article-title>The sunk cost and Concorde effects: are humans less rational than lower animals?</article-title>. <source>Psychol Bulletin</source>. <year>1999</year>;<volume>125</volume>(<issue>5</issue>):<fpage>591</fpage>&#x02013;<lpage>600</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/0033-2909.125.5.591</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Sweis</surname><given-names>BM</given-names></name>, <name><surname>Abram</surname><given-names>SV</given-names></name>, <name><surname>Schmidt</surname><given-names>BJ</given-names></name>, <name><surname>Seeland</surname><given-names>KD</given-names></name>, <name><surname>MacDonald AW</surname><given-names>3rd</given-names></name>, <name><surname>Thomas</surname><given-names>MJ</given-names></name>, <etal>et al</etal>. <article-title>Sensitivity to &#x0201c;sunk costs&#x0201d; in mice, rats, and humans</article-title>. <source>Science</source>. <year>2018</year>;<volume>361</volume>(<issue>6398</issue>):<fpage>178</fpage>&#x02013;<lpage>81</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.aar8644</pub-id>
<pub-id pub-id-type="pmid">30002252</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Holton</surname><given-names>E</given-names></name>, <name><surname>Grohn</surname><given-names>J</given-names></name>, <name><surname>Ward</surname><given-names>H</given-names></name>, <name><surname>Manohar</surname><given-names>SG</given-names></name>, <name><surname>O&#x02019;Reilly</surname><given-names>JX</given-names></name>, <name><surname>Kolling</surname><given-names>N</given-names></name>. <article-title>Goal commitment is supported by vmPFC through selective attention</article-title>. <source>Nat Hum Behav</source>. <year>2024</year>;<volume>8</volume>(<issue>7</issue>):<fpage>1351</fpage>&#x02013;<lpage>65</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41562-024-01844-5</pub-id>
<pub-id pub-id-type="pmid">38632389</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Frederick</surname><given-names>S</given-names></name>, <name><surname>Loewenstein</surname><given-names>G</given-names></name>, <name><surname>O&#x02019;donoghue</surname><given-names>T</given-names></name>. <article-title>Time discounting and time preference: a critical review</article-title>. <source>J Econ Literat</source>. <year>2002</year>;<volume>40</volume>(<issue>2</issue>):<fpage>351</fpage>&#x02013;<lpage>401</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1257/jel.40.2.351</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Kahneman</surname><given-names>D</given-names></name>, <name><surname>Knetsch</surname><given-names>JL</given-names></name>, <name><surname>Thaler</surname><given-names>RH</given-names></name>. <article-title>Anomalies: the endowment effect, loss aversion, and status quo bias</article-title>. <source>J Econ Perspect</source>. <year>1991</year>;<volume>5</volume>(<issue>1</issue>):<fpage>193</fpage>&#x02013;<lpage>206</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1257/jep.5.1.193</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref029"><label>29</label><mixed-citation publication-type="other">Tsitsiklis JN, Roy BV. <source>Analysis of temporal-diffference learning with function approximation. advances in neural information processing systems</source>. <year>1996</year>. p. <fpage>9</fpage>.</mixed-citation></ref><ref id="pcbi.1013054.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Pine</surname><given-names>A</given-names></name>, <name><surname>Seymour</surname><given-names>B</given-names></name>, <name><surname>Roiser</surname><given-names>JP</given-names></name>, <name><surname>Bossaerts</surname><given-names>P</given-names></name>, <name><surname>Friston</surname><given-names>KJ</given-names></name>, <name><surname>Curran</surname><given-names>HV</given-names></name>, <etal>et al</etal>. <article-title>Encoding of marginal utility across time in the human brain</article-title>. <source>J Neurosci</source>. <year>2009</year>;<volume>29</volume>(<issue>30</issue>):<fpage>9575</fpage>&#x02013;<lpage>81</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1126-09.2009</pub-id>
<pub-id pub-id-type="pmid">19641120</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Garrett</surname><given-names>N</given-names></name>, <name><surname>Daw</surname><given-names>ND</given-names></name>. <article-title>Biased belief updating and suboptimal choice in foraging decisions</article-title>. <source>Nat Commun</source>. <year>2020</year>;<volume>11</volume>(<issue>1</issue>):<fpage>3417</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41467-020-16964-5</pub-id>
<pub-id pub-id-type="pmid">32647271</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>A</surname><given-names>RR</given-names></name>. <article-title>A theory of Pavlovian conditioning: variations in the effectiveness of reinforcement and non-reinforcement</article-title>. <source>Classical conditioning, current research and theory</source>. <year>1972</year>;<volume>2</volume>:<fpage>64</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="pcbi.1013054.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Wilson</surname><given-names>RC</given-names></name>, <name><surname>Collins</surname><given-names>AG</given-names></name>. <article-title>Ten simple rules for the computational modeling of behavioral data</article-title>. <source>Elife</source>. <year>2019</year>;<volume>8</volume>:e49547. <comment>doi: </comment><pub-id pub-id-type="doi">10.7554/eLife.49547</pub-id>
<pub-id pub-id-type="pmid">31769410</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref034"><label>34</label><mixed-citation publication-type="book"><name><surname>Goffman</surname><given-names>E</given-names></name>. <article-title>The presentation of self in everyday life</article-title>. <source>Social theory re-wired</source>. <publisher-name>Routledge</publisher-name>. <year>2023</year>. p. <fpage>450</fpage>&#x02013;<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.4324/9781003320609-59</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Brandst&#x000e4;tter</surname><given-names>V</given-names></name>, <name><surname>Bernecker</surname><given-names>K</given-names></name>. <article-title>Persistence and disengagement in personal goal pursuit</article-title>. <source>Annu Rev Psychol</source>. <year>2022</year>;<volume>73</volume>:<fpage>271</fpage>&#x02013;<lpage>99</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1146/annurev-psych-020821-110710</pub-id>
<pub-id pub-id-type="pmid">34280324</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Wrosch</surname><given-names>C</given-names></name>, <name><surname>Scheier</surname><given-names>MF</given-names></name>, <name><surname>Miller</surname><given-names>GE</given-names></name>, <name><surname>Schulz</surname><given-names>R</given-names></name>, <name><surname>Carver</surname><given-names>CS</given-names></name>. <article-title>Adaptive self-regulation of unattainable goals: goal disengagement, goal reengagement, and subjective well-being</article-title>. <source>Pers Soc Psychol Bull</source>. <year>2003</year>;<volume>29</volume>(<issue>12</issue>):<fpage>1494</fpage>&#x02013;<lpage>508</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1177/0146167203256921</pub-id>
<pub-id pub-id-type="pmid">15018681</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Hogarth</surname><given-names>L</given-names></name>, <name><surname>Chase</surname><given-names>HW</given-names></name>, <name><surname>Baess</surname><given-names>K</given-names></name>. <article-title>Impaired goal-directed behavioural control in human impulsivity</article-title>. <source>Q J Exp Psychol (Hove)</source>. <year>2012</year>;<volume>65</volume>(<issue>2</issue>):<fpage>305</fpage>&#x02013;<lpage>16</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/17470218.2010.518242</pub-id>
<pub-id pub-id-type="pmid">21077008</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref038"><label>38</label><mixed-citation publication-type="journal"><name><surname>Gustavson</surname><given-names>DE</given-names></name>, <name><surname>Miyake</surname><given-names>A</given-names></name>, <name><surname>Hewitt</surname><given-names>JK</given-names></name>, <name><surname>Friedman</surname><given-names>NP</given-names></name>. <article-title>Genetic relations among procrastination, impulsivity, and goal-management ability: implications for the evolutionary origin of procrastination</article-title>. <source>Psychol Sci</source>. <year>2014</year>;<volume>25</volume>(<issue>6</issue>):<fpage>1178</fpage>&#x02013;<lpage>88</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1177/0956797614526260</pub-id>
<pub-id pub-id-type="pmid">24705635</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>Dickson</surname><given-names>JM</given-names></name>, <name><surname>Moberly</surname><given-names>NJ</given-names></name>, <name><surname>O&#x02019;Dea</surname><given-names>C</given-names></name>, <name><surname>Field</surname><given-names>M</given-names></name>. <article-title>Goal fluency, pessimism and disengagement in depression</article-title>. <source>PLoS One</source>. <year>2016</year>;<volume>11</volume>(<issue>11</issue>):e0166259. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0166259</pub-id>
<pub-id pub-id-type="pmid">27902708</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref040"><label>40</label><mixed-citation publication-type="journal"><name><surname>Koppe</surname><given-names>K</given-names></name>, <name><surname>Rothermund</surname><given-names>K</given-names></name>. <article-title>Let it go: Depression facilitates disengagement from unattainable goals</article-title>. <source>J Behav Ther Exp Psychiatry</source>. <year>2017</year>;<volume>54</volume>:<fpage>278</fpage>&#x02013;<lpage>84</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jbtep.2016.10.003</pub-id>
<pub-id pub-id-type="pmid">27783964</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref041"><label>41</label><mixed-citation publication-type="journal"><name><surname>Barkley</surname><given-names>RA</given-names></name>. <article-title>Behavioral inhibition, sustained attention, and executive functions: constructing a unifying theory of ADHD</article-title>. <source>Psychol Bull</source>. <year>1997</year>;<volume>121</volume>(<issue>1</issue>):<fpage>65</fpage>&#x02013;<lpage>94</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/0033-2909.121.1.65</pub-id>
<pub-id pub-id-type="pmid">9000892</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref042"><label>42</label><mixed-citation publication-type="journal"><name><surname>Dickson</surname><given-names>JM</given-names></name>, <name><surname>Johnson</surname><given-names>S</given-names></name>, <name><surname>Huntley</surname><given-names>CD</given-names></name>, <name><surname>Peckham</surname><given-names>A</given-names></name>, <name><surname>Taylor</surname><given-names>PJ</given-names></name>. <article-title>An integrative study of motivation and goal regulation processes in subclinical anxiety, depression and hypomania</article-title>. <source>Psychiatry Res</source>. <year>2017</year>;<volume>256</volume>:<fpage>6</fpage>&#x02013;<lpage>12</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.psychres.2017.06.002</pub-id>
<pub-id pub-id-type="pmid">28618249</pub-id>
</mixed-citation></ref><ref id="pcbi.1013054.ref043"><label>43</label><mixed-citation publication-type="journal"><name><surname>Mogg</surname><given-names>K</given-names></name>, <name><surname>Bradley</surname><given-names>BP</given-names></name>. <article-title>Attentional bias in generalized anxiety disorder versus depressive disorder</article-title>. <source>Cogn Ther Res</source>. <year>2005</year>;<volume>29</volume>(<issue>1</issue>):<fpage>29</fpage>&#x02013;<lpage>45</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10608-005-1646-y</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref044"><label>44</label><mixed-citation publication-type="journal"><name><surname>Sutton</surname><given-names>RS</given-names></name>, <name><surname>Precup</surname><given-names>D</given-names></name>, <name><surname>Singh</surname><given-names>S</given-names></name>. <article-title>Between MDPs and semi-MDPs: a framework for temporal abstraction in reinforcement learning</article-title>. <source>Artif Intell</source>. <year>1999</year>;<volume>112</volume>(1&#x02013;2):<fpage>181</fpage>&#x02013;<lpage>211</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/s0004-3702(99)00052-1</pub-id></mixed-citation></ref><ref id="pcbi.1013054.ref045"><label>45</label><mixed-citation publication-type="confproc"><name><surname>Akiba</surname><given-names>T</given-names></name>, <name><surname>Sano</surname><given-names>S</given-names></name>, <name><surname>Yanase</surname><given-names>T</given-names></name>, <name><surname>Ohta</surname><given-names>T</given-names></name>, <name><surname>Koyama</surname><given-names>M</given-names></name>. <conf-name>Optuna</conf-name>. In: <conf-name>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &#x00026; Data Mining</conf-name>. <publisher-name>ACM</publisher-name>. <year>2019</year>. p. <fpage>2623</fpage>&#x02013;<lpage>31</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3292500.3330701</pub-id></mixed-citation></ref></ref-list></back><sub-article article-type="author-comment" id="pcbi.1013054.r001" specific-use="rebutted-decision-letter-unavailable"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013054.r001</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">9 Aug 2024</named-content>
</p></body></sub-article><sub-article article-type="aggregated-review-documents" id="pcbi.1013054.r002" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013054.r002</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Latham</surname><given-names>Peter E.</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="author"><name><surname>Marinazzo</surname><given-names>Daniele</given-names></name><role>Section Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Latham, Marinazzo</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Latham, Marinazzo</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1013054" id="rel-obj002" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">28 Sep 2024</named-content>
</p><p>Dear Ms Aenugu,</p><p>Thank you very much for submitting your manuscript "Building momentum: A computational model of persistence in long-term goals" for consideration at PLOS Computational Biology.</p><p>As with all papers reviewed by the journal, your manuscript was reviewed by members of the editorial board and by several independent reviewers. In light of the reviews (below this email), we would like to invite the resubmission of a significantly-revised version that takes into account the reviewers' comments.</p><p>We cannot make any decision about publication until we have seen the revised manuscript and your response to the reviewers' comments. Your revised manuscript is also likely to be sent to reviewers for further evaluation.</p><p>When you are ready to resubmit, please upload the following:</p><p>[1] A letter containing a detailed list of your responses to the review comments and a description of the changes you have made in the manuscript.&#x000a0;Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out.</p><p>[2] Two versions of the revised manuscript: one with either highlights or tracked changes denoting where the text has been changed; the other a clean version (uploaded as the manuscript file).</p><p>Important additional instructions are given below your reviewer comments.</p><p>Please prepare and submit your revised manuscript within 60 days. If you anticipate any delay, please let us know the expected resubmission date by replying to this email. Please note that revised manuscripts received after the 60-day due date may require evaluation and peer review similar to newly submitted manuscripts.</p><p>Thank you again for your submission. We hope that our editorial process has been constructive so far, and we welcome your feedback at any time. Please don't hesitate to contact us if you have any questions or comments.</p><p>Sincerely,</p><p>Peter E. Latham</p><p>Academic Editor</p><p>PLOS Computational Biology</p><p>Daniele Marinazzo</p><p>Section Editor</p><p>PLOS Computational Biology</p><p>***********************</p><p>Reviewer's Responses to Questions</p><p>
<bold>Comments to the Authors:</bold>
</p><p>
<bold>Please note here if the review is uploaded as an attachment.</bold>
</p><p>Reviewer #1:&#x000a0;See attachment.</p><p>Reviewer #2:&#x000a0;Please see the attachment.</p><p>**********</p><p>
<bold>Have the authors made all data and (if applicable) computational code underlying the findings in their manuscript fully available?</bold>
</p><p>The <ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/materials-and-software-sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data and code underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data and code should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data or code &#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.</p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;None</p><p>**********</p><p>PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>
<underline>Figure Files:</underline>
</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com</ext-link>.&#x000a0;PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <underline><email>figures@plos.org</email></underline>.</p><p>
<underline>Data Requirements:</underline>
</p><p>Please note that, as a condition of publication, PLOS' data policy requires that you make available all data used to draw the conclusions outlined in your manuscript. Data must be deposited in an appropriate repository, included within the body of the manuscript, or uploaded as supporting information. This includes all numerical values that were used to generate graphs, histograms etc.. For an example in PLOS Biology see here: <ext-link xlink:href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5" ext-link-type="uri">http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5</ext-link>.</p><p>
<underline>Reproducibility:</underline>
</p><p>To enhance the reproducibility of your results, we recommend that you deposit your laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. Additionally, PLOS ONE offers an option to publish peer-reviewed clinical study protocols. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link></p><supplementary-material id="pcbi.1013054.s017" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">PCOMPBIOL-D-24-01340_Review_Pezzotta.pdf</named-content></p></caption><media xlink:href="pcbi.1013054.s017.pdf"/></supplementary-material><supplementary-material id="pcbi.1013054.s018" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">PLOS Comp Bio 2024 Building Momentum.pdf</named-content></p></caption><media xlink:href="pcbi.1013054.s018.pdf"/></supplementary-material></body></sub-article><sub-article article-type="author-comment" id="pcbi.1013054.r003"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013054.r003</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1013054" id="rel-obj003" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">1 Dec 2024</named-content>
</p><supplementary-material id="pcbi.1013054.s019" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Goal_selection_manuscript_plos_cb_address_to_reviewers.pdf</named-content></p></caption><media xlink:href="pcbi.1013054.s019.pdf"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pcbi.1013054.r004" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013054.r004</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Latham</surname><given-names>Peter E.</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="author"><name><surname>Marinazzo</surname><given-names>Daniele</given-names></name><role>Section Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Latham, Marinazzo</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Latham, Marinazzo</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1013054" id="rel-obj004" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">13 Feb 2025</named-content>
</p><p><!--<div style="color: rgb(51, 51, 51); font-family: sans-serif, Arial, Verdana, "Trebuchet MS"; font-size: 13px;">-->PCOMPBIOL-D-24-01340R1</p><p>Building momentum: A computational model of persistence in long-term goals</p><p>PLOS Computational Biology</p><p>Dear Dr. Aenugu,</p><p>I called this a major revision, but it seems that i's more about writing than the science. So hopefully it will be straightforward. Hopefully. ;)</p><p>Peter</p><p>--formal letter follows</p><p>Thank you for submitting your manuscript to PLOS Computational Biology. After careful consideration, we feel that it has merit but does not fully meet PLOS Computational Biology's publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>&#x0200b;Please submit your revised manuscript within 60 days Apr 15 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at ploscompbiol@plos.org. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pcompbiol/" ext-link-type="uri">https://www.editorialmanager.com/pcompbiol/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:</p><p>* A rebuttal letter that responds to each point raised by the editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'. This file does not need to include responses to formatting updates and technical items listed in the 'Journal Requirements' section below.</p><p>* A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p><p>* An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p><p>If you would like to make changes to your financial disclosure, competing interests statement, or data availability statement, please make these updates within the submission form at the time of resubmission. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Peter E. Latham</p><p>Academic Editor</p><p>PLOS Computational Biology</p><p>Daniele Marinazzo</p><p>Section Editor</p><p>PLOS Computational Biology</p><p>
<bold>Journal Requirements:</bold>
</p><p>1) Regarding Figures 1 and 9 :Thank you for indicating&#x000a0;that "We redid the images using open-source resources that you recommended (<ext-link xlink:href="https://openclipart.org/" ext-link-type="uri">https://openclipart.org/</ext-link>)." Please add this information in the figures legends.</p><p>
<bold>Reviewers' comments:</bold>
</p><p>Reviewer's Responses to Questions</p><p>Reviewer #1:&#x000a0;I would like to compliment and thank the authors for taking great care in addressing most of the questions from both us reviewers.</p><p>However, I feel that the task contingencies are still not explained clearly, which has been a source of a major misunderstanding on my side -- about which I had an exchange with the editor.</p><p>My original question --probably unclearly formulated-- was about whether or not the *card* chosen by the participant had any effect on the performance.</p><p>What I meant by *card*, confusingly, was the *token represented by the card* (car, hat, cat), and not the specific picture on the card (mat, wardrobe, key, etc).</p><p>For instance, in Fig 1, in the first block of Exp 1 one has</p><p>p(car) = 0.8</p><p>p(cat) = 0.2</p><p>p(hat) = 0.2</p><p>According to my understanding, this meant that whichever card (the actual card, with the token-related symbol on it) the participant picked, the probability of obtaining a token (say, "car") would always be the same (0.8 for the "car").</p><p>Instead, as I am aware the editor confirmed with you, these probabilities should be expressed more rigorously as *conditional probabilities* as follows:</p><p>p(car token|car related card) = 0.8</p><p>p(car token|cat related card) = 0</p><p>p(car token|mat related card) = 0</p><p>p(cat token|car related card) = 0</p><p>p(cat token|cat related card) = 0.2</p><p>p(cat token|mat related card) = 0</p><p>p(mat token|car related card) = 0</p><p>p(mat token|cat related card) = 0</p><p>p(mat token|mat related card) = 0.2</p><p>In the new text, we read:</p><p>"Participants receive tokens upon flipping cards with a given probability. If they</p><p>receive the token for a suit type in a round, it goes into its designated slots and the</p><p>participant proceeds to play another round."</p><p>A whole paragraph later (!), we have a description of the probabilities:</p><p>"Furthermore, we designed blocks in the game so that there is a dominant suit in</p><p>each block (counterbalanced across all suit types), such that flipping its cards gives</p><p>tokens with the highest probability (in Figure 1D, the cat suit is the dominant one</p><p>with 0.8 probability in block N ). The other two suits have identically inferior token</p><p>probabilities (both hat and car with 0.2 probability)."</p><p>It currently takes 5 (!) paragraphs to get to the to these probabilities.</p><p>They should come immediately (right after "Participants receive tokens upon flipping cards with a given probability") with the help of equations, and also with a clear graphical representation of the *conditional* probabilities in Fig. 1.</p><p>This points back to one of the first comment I made in the first round of revisions: the exposition should be more succinct and rely more on mathematical formulas.</p><p>I expect the readers of PLOS Computational Biology to be generally well versed in math, and this should not be an obstacle; rather, I feel it would help most readers go through the setup and the results.</p><p>Currently, I feel that the exposition is too verbose, at the cost of readability.</p><p>For this reason, I would strongly recommend to make further edits in this direction, to make justice to an otherwise interesting and solid work.</p><p>Reviewer #2:&#x000a0;I am pleased that the authors addressed most of the reviewers&#x02019; points in their new submission, which I find much improved compared to the original one. The current manuscript provides a clearer description of the methodologies, more adequately integrates existing research, and controls for alternative explanations thanks to newly added analyses. However, a couple of points remain to be addressed.</p><p>One point that remains unclear is regarding the fitted models&#x02019; likelihood with respect to participants&#x02019; goals. In particular, the authors affirm that in rounds where participants do not explicitly declare goals the model selects goals by itself based on their relative values. Such goals are then used to estimate the likelihood of participants&#x02019; chosen actions, which are known. However, the participants&#x02019; goals are latent variables, meaning standard model fitting methods based on likelihood may be inappropriate unless integrating over the latent variables across all trials (see e.g., Rmus et al., 2024 Plos Comp Bio). The authors should thus resort to alternative, likelihood-free model fitting methods, or explain why their approach works in the context of their task.</p><p>Moreover, is it correct that the model (algorithm 1) decides whether to switch goals based on the values of the current and the best alternative goal, but that in the case of a switch, the model could select the third available goal? If so, could the authors please justify this choice?</p><p>I also have some minor suggestions.</p><p>In some figures with model simulations, e.g., 2 and 4, the authors add the &#x0201c;optimal&#x0201d; specification to prospective and retrospective models to specify these models were not fit on participants&#x02019; behavior. However, I believe optimized would be a more appropriate term, especially for the retrospective model, since it is not optimal for task performance.</p><p>In addition, could the authors please state somewhere how many iterations per real participant were run by the models in order to produce simulated data?</p><p>On page 14, the authors state the prospective agent shows a &#x0201c;clear&#x0201d; interaction of performance with block type. However, I think &#x0201c;significant&#x0201d; would be a more appropriate adjective.</p><p>In general, I think the authors would benefit from additional proofreading and editing, as I noticed several typos/inconsistencies that should be corrected before the final submission.</p><p>**********</p><p>
<bold>Have the authors made all data and (if applicable) computational code underlying the findings in their manuscript fully available?</bold>
</p><p>The <ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/materials-and-software-sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data and code underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data and code should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data or code &#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.</p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;None</p><p>**********</p><p>PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>
<bold>Figure resubmission:</bold>
</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at figures@plos.org. Please note that Supporting Information files do not need this step. If there are other&#x000a0;versions of&#x000a0;figure files still present in your&#x000a0;submission file inventory at resubmission, please replace them with the PACE-processed versions.</p><p>
<bold>Reproducibility:</bold>
</p><p>To enhance the reproducibility of your results, we recommend that authors of applicable studies deposit laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. Additionally, PLOS ONE offers an option to publish peer-reviewed clinical study protocols. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link></p></body></sub-article><sub-article article-type="author-comment" id="pcbi.1013054.r005"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013054.r005</article-id><title-group><article-title>Author response to Decision Letter 2</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1013054" id="rel-obj005" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">8 Apr 2025</named-content>
</p><supplementary-material id="pcbi.1013054.s020" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">PlosCB_reviewers_revision_2.pdf</named-content></p></caption><media xlink:href="pcbi.1013054.s020.pdf"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pcbi.1013054.r006" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1013054.r006</article-id><title-group><article-title>Decision Letter 2</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Latham</surname><given-names>Peter E.</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="author"><name><surname>Marinazzo</surname><given-names>Daniele</given-names></name><role>Section Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Latham, Marinazzo</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Latham, Marinazzo</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1013054" id="rel-obj006" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">14 Apr 2025</named-content>
</p><p>Dear Ms Aenugu,</p><p>We are pleased to inform you that your manuscript 'Building momentum: A computational model of persistence toward long-term goals' has been provisionally accepted for publication in PLOS Computational Biology.</p><p>Before your manuscript can be formally accepted you will need to complete some formatting changes, which you will receive in a follow up email. A member of our team will be in touch with a set of requests.</p><p>Please note that your manuscript will not be scheduled for publication until you have made the required changes, so a swift response is appreciated.</p><p>IMPORTANT: The editorial review process is now complete. PLOS will only permit corrections to spelling, formatting or significant scientific errors from this point onwards. Requests for major changes, or any which affect the scientific understanding of your work, will cause delays to the publication date of your manuscript.</p><p>Should you, your institution's press office or the journal office choose to press release your paper, you will automatically be opted out of early publication. We ask that you notify us now if you or your institution is planning to press release the article. All press must be co-ordinated with PLOS.</p><p>Thank you again for supporting Open Access publishing; we are looking forward to publishing your work in PLOS Computational Biology.&#x000a0;</p><p>Best regards,</p><p>Peter E. Latham</p><p>Academic Editor</p><p>PLOS Computational Biology</p><p>Daniele Marinazzo</p><p>Section Editor</p><p>PLOS Computational Biology</p><p>***********************************************************</p><p>Reviewer's Responses to Questions</p><p>
<bold>Comments to the Authors:</bold>
</p><p>
<bold>Please note here if the review is uploaded as an attachment.</bold>
</p><p>Reviewer #1:&#x000a0;I am pleased to see that the authors have addressed all reviewers' comments and questions. The task is much more clearly explained now. I would recommend publication in Plos Comp Bio.</p><p>Reviewer #2:&#x000a0;I am satisfied with how the authors have addressed the points I raised and have no further comments.</p><p>**********</p><p>
<bold>Have the authors made all data and (if applicable) computational code underlying the findings in their manuscript fully available?</bold>
</p><p>The <ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/materials-and-software-sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data and code underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data and code should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data or code &#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.</p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;None</p><p>**********</p><p>PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p></body></sub-article></article>